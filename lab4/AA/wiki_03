<doc id="50546680" url="https://en.wikipedia.org/wiki?curid=50546680" title="Distributed tree search">
Distributed tree search

Distributed tree search (DTS) algorithm is a class of algorithms for searching values in an efficient and distributed manner. Their purpose is to iterate through a tree by working along multiple branches in parallel and merging the results of each branch into one common solution, in order to minimize time spent searching for a value in a tree-like data structure.

The original paper was written in 1988 by Chris Ferguson and Richard E. Korf, from the University of California's Computer Science Department. They used multiple other chess AIs to develop this wider range algorithm.

The Distributed Tree Search Algorithm (also known as Korf–Ferguson algorithm) was created to solve the following problem: "Given a tree with non-uniform branching factor and depth, search it in parallel with an arbitrary number of processors as fast as possible."

The top-level part of this algorithm is general and does not use a particular existing type of tree-search, but it can be easily specialized to fit any type of non-distributed tree-search.

DTS consists of using multiple processes, each with a node and a set of processors attached, with the goal of searching the sub-tree below the said node. Each process then divides itself into multiple coordinated sub-processes which recursively divide themselves again until an optimal way to search the tree has been found based on the number of processors available to each process. Once a process finishes, DTS dynamically reassigns the processors to other processes as to keep the efficiency to a maximum through good load-balancing, especially in irregular trees.

Once a process finishes searching, it recursively sends and merges a resulting signal to its parent-process, until all the different sub-answers have been merged and the entire problem has been solved.

DTS is only applicable under two major conditions: the data structure to search through is a tree, and the algorithm can make use of at least one computation unit (Although it cannot be considered as distributed if there is only one).

One major example of the everyday use of DTS is network routing. The Internet can be seen as a tree of IP addresses, and an analogy to a routing protocol could be how post offices work in the real world. Since there are over 4.3 billion IP addresses currently, society heavily depends on the time the data takes to find its way to its destination. As such, IP-routing divides the work into multiple sub-units which each have different scales of calculation capabilities and use each other's result to find the route in a very efficient manner. This is an instance of DTS that affects over 43% of the world's population, for reasons going from entertainment to national security.

Although DTS is currently one of the most widely used algorithms, many of its applications have alternatives to them which could potentially develop into more efficient, less resource-demanding solutions, were they more researched.

One of the more controversial examples is Big-Data processing. In applications like Google Search Engine, Facebook, YouTube, search needs to be optimized to keep waiting time inside a reasonable window. This could be achieved through the plain use of DTS, but other algorithms are used in place (for example data-hashing in SQL databases), or in conjunction (Facebook's Haystack algorithm groups parallel tree-search, data-hashing and memory-ordering/sorting).

One of the more important limits of DTS is the fact that it requires a tree as input. Trees are a sub-instance of a data structure known as Graphs, which means every Graph can be converted into a tree. Although there currently exists no better way to search through trees than Korf-Ferguson's algorithm, each task has different particularities and in most cases, there will exist more efficient data structures to represent the problem and solve it than through tree-search. And so there exist instances of tree structures with cycles that cannot possibly be faster than a graph-search on the same structure with the same processing power.

There are few controversies around Korf-Ferguson's DTS algorithm, since it is recognized as very complete, but simple. It is very often used as a stepping stone for students to discover the fundamentals and key concepts of distributed problem-solving.

The most important challenge to this algorithmic concept was an article by Kröll B, « Balanced Distributed Search Trees Do Not Exist », which does not attack the veracity or current efficiency of the algorithm, but rather the fact that DTS itself, no matter how many improvements are made to it (for example balancing the input tree before-hand), will never be able to reach optimal resolution-time. This opens a new view point: are too many resources used into the completion of DTS, which blocks new algorithms with higher efficiency-potential from getting researched and developed? Another limit of DTS is the fact that no matter how efficient the division, coordination and merging of the solutions is, it will always be limited by the material number or processors and their processing power. Until recently, this was admitted as being a limit to nearly every computation, but new-generation algorithms like Euclideon might one day be able to crush DFS's efficiency through processing-power-independent problem resolution.




</doc>
<doc id="51017812" url="https://en.wikipedia.org/wiki?curid=51017812" title="DONE">
DONE

The Data-based Online Nonlinear Extremumseeker (DONE) algorithm is a black-box optimization algorithm.
DONE models the unknown cost function and attempts to find an optimum of the underlying function.
The DONE algorithm is suitable for optimizing costly and noisy functions and does not require derivatives.
An advantage of DONE over similar algorithms, such as Bayesian optimization, is that the computational cost per iteration is independent of the number of function evaluations.

The DONE algorithm was first proposed by Hans Verstraete and Sander Wahls. The algorithm fits a surrogate model based on random Fourier features and then uses a well-known L-BFGS algorithm to find an optimum of the surrogate model.

DONE was first demonstrated for maximizing the signal in optical coherence tomography measurements, but has since then been applied to various other applications. For example, it was used to help extending the field of view in light sheet fluorescence microscopy.


</doc>
<doc id="50959785" url="https://en.wikipedia.org/wiki?curid=50959785" title="KHOPCA clustering algorithm">
KHOPCA clustering algorithm

KHOPCA is an adaptive clustering algorithm originally developed for dynamic networks. KHOPCA (formula_1-hop clustering algorithm) provides a fully distributed and localized approach to group elements such as nodes in a network according to their distance from each other. KHOPCA operates proactively through a simple set of rules that defines clusters, which are optimal with respect to the applied distance function.

KHOPCA's clustering process explicitly supports joining and leaving of nodes, which makes KHOPCA suitable for highly dynamic networks. However, it has been demonstrated that KHOPCA also performs in static networks.

Besides applications in ad hoc and wireless sensor networks, KHOPCA can be used in localization and navigation problems, networked swarming, and real-time data clustering and analysis.

KHOPCA (formula_1-hop clustering algorithm) operates proactively through a simple set of rules that defines clusters with variable formula_1-hops. A set of local rules describes the state transition between nodes. A node's weight is determined only depending on the current state of its neighbors in communication range. Each node of the network is continuously involved in this process. As result, formula_1-hop clusters are formed and maintained in static as well as dynamic networks.

KHOPCA does not require any predetermined initial configuration. Therefore, a node can potentially choose any weight (between formula_5 and formula_6). However, the choice of the initial configuration does influence the convergence time.

The prerequisites in the start configuration for the application of the rules are the following.
The following rules describe the state transition for a node formula_9 with weight formula_25. These rules have to be executed on each node in the order described here.

The first rule has the function of constructing an order within the cluster. This happens through a node formula_9 detects the direct neighbor with the highest weight formula_27, which is higher than the node's own weight formula_25. If such a direct neighbor is detected, the node formula_9 changes its own weight to be the weight of the highest weight within the neighborhood subtracted by 1. Applied iteratively, this process creates a top-to-down hierarchical cluster structure.
if max(W(N(n))) > w_n

The second rule deals with the situation where nodes in a neighborhood are on the minimum weight level. This situation can happen if, for instance, the initial configuration assigns the minimum weight to all nodes. If there is a neighborhood with all nodes having the minimum weight level, the node formula_9 declares itself as cluster center. Even if coincidently all nodes declare themselves as cluster centers, the conflict situation will be resolved by one of the other rules.
if max(W(N(n)) == MIN & w_n == MIN

The third rule describes situations where nodes with leveraged weight values, which are not cluster centers, attract surrounding nodes with lower weights. This behavior can lead to fragmented clusters without a cluster center. In order to avoid fragmented clusters, the node with higher weight value is supposed to successively decrease its own weight with the objective to correct the fragmentation by allowing the other nodes to reconfigure according to the rules. 
if max(W(N(n))) <= w_n && w_n != MAX

The fourth rule resolves the situation where two cluster centers connect in 1-hop neighborhood and need to decide which cluster center should continue its role as cluster center. Given any specific criterion (e.g., device ID, battery power), one cluster center remains while the other cluster center is hierarchized in 1-hop neighborhood to that new cluster center. The choice of the specific criterion to resolve the decision-making depends on the used application scenario and on the available information. 
if max(W(N(n)) == MAX && w_n == MAX

An exemplary sequence of state transitions applying the described four rules is illustrated below.

KHOPCA acting in a dynamic 2-D simulation. The geometry is based on a geometric random graph; all existing links are drawn in this network.

KHOPCA also works in a dynamic 3-D environment. The cluster connections are illustrated with bold lines.

It has been demonstrated that KHOPCA terminates after a finite number of state transitions in static networks.


</doc>
<doc id="51386092" url="https://en.wikipedia.org/wiki?curid=51386092" title="Certifying algorithm">
Certifying algorithm

In theoretical computer science, a certifying algorithm is an algorithm that outputs, together with a solution to the problem it solves, a proof that the solution is correct. A certifying algorithm is said to be "efficient" if the combined runtime of the algorithm and a proof checker is slower by at most a constant factor than the best known non-certifying algorithm for the same problem.

The proof produced by a certifying algorithm should be in some sense simpler than the algorithm itself, for otherwise any algorithm could be considered certifying (with its output verified by running the same algorithm again). Sometimes this is formalized by requiring that a verification of the proof take less time than the original algorithm, while for other problems (in particular those for which the solution can be found in linear time) simplicity of the output proof is considered in a less formal sense. For instance, the validity of the output proof may be more apparent to human users than the correctness of the algorithm, or a checker for the proof may be more amenable to formal verification.

Implementations of certifying algorithms that also include a checker for the proof generated by the algorithm may be considered to be more reliable than non-certifying algorithms. For, whenever the algorithm is run, one of three things happens: it produces a correct output (the desired case), it detects a bug in the algorithm or its implication (undesired, but generally preferable to continuing without detecting the bug), or both the algorithm and the checker are faulty in a way that masks the bug and prevents it from being detected (undesired, but unlikely as it depends on the existence of two independent bugs).

Many examples of problems with checkable algorithms come from graph theory.
For instance, a classical algorithm for testing whether a graph is bipartite would simply output a Boolean value: true if the graph is bipartite, false otherwise. In contrast, a certifying algorithm might output a 2-coloring of the graph in the case that it is bipartite, or a cycle of odd length if it is not. Any graph is bipartite if and only if it can be 2-colored, and non-bipartite if and only if it contains an odd cycle. Both checking whether a 2-coloring is valid and checking whether a given odd-length sequence of vertices is a cycle may be performed more simply than testing bipartiteness.

Analogously, it is possible to test whether a given directed graph is acyclic by a certifying algorithm that outputs either a topological order or a directed cycle. It is possible to test whether an undirected graph is a chordal graph by a certifying algorithm that outputs either an elimination ordering (an ordering of all vertices such that, for every vertex, the neighbors that are later in the ordering form a clique) or a chordless cycle. And it is possible to test whether a graph is planar by a certifying algorithm that outputs either a planar embedding or a Kuratowski subgraph.

The extended Euclidean algorithm for the greatest common divisor of two integers and is certifying: it outputs three integers (the divisor), , and , such that . This equation can only be true of multiples of the greatest common divisor, so testing that is the greatest common divisor may be performed by checking that divides both and and that this equation is correct.



</doc>
<doc id="51411922" url="https://en.wikipedia.org/wiki?curid=51411922" title="Algorithmic paradigm">
Algorithmic paradigm

An algorithmic paradigm or algorithm design paradigm is a generic model or framework which underlies the design of a class of algorithms. An algorithmic paradigm is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program.





</doc>
<doc id="52242050" url="https://en.wikipedia.org/wiki?curid=52242050" title="Multiplicative weight update method">
Multiplicative weight update method

The multiplicative weights update method is an algorithmic technique most commonly used for decision making and prediction, and also widely deployed in game theory and algorithm design. The simplest use case is the problem of prediction from expert advice, in which a decision maker needs to iteratively decide on an expert whose advice to follow. The method assigns initial weights to the experts (usually identical initial weights), and updates these weights multiplicatively and iteratively according to the feedback of how well an expert performed: reducing it in case of poor performance, and increasing it otherwise. It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving linear programs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory.

"Multiplicative weights" implies the iterative rule used in algorithms derived from the multiplicative weight update method. It is given with different names in the different fields where it was discovered or rediscovered.

The earliest known version of this technique was in an algorithm named "fictitious play" which was proposed in game theory in the early 1950s. Grigoriadis and Khachiyan applied a randomized variant of "fictitious play" to solve two-player zero-sum games efficiently using the multiplicative weights algorithm. In this case, player allocates higher weight to the actions that had a better outcome and choose his strategy relying on these weights. In machine learning, Littlestone applied the earliest form of the multiplicative weights update rule in his famous winnow algorithm, which is similar to Minsky and Papert's earlier perceptron learning algorithm. Later, he generalized the winnow algorithm to weighted majority algorithm. Freund and Schapire followed his steps and generalized the winnow algorithm in the form of hedge algorithm.

The multiplicative weights algorithm is also widely applied in computational geometry such as Clarkson's algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find set covers for hypergraphs with small VC dimension.

In operation research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.

In computer science field, some researchers have previously observed the close relationships between multiplicative update algorithms used in different contexts. Young discovered the similarities between fast LP algorithms and Raghavan's method of pessimistic estimators for derandomization of randomized rounding algorithms; Klivans and Servedio linked boosting algorithms in learning theory to proofs of Yao's XOR Lemma; Garg and Khandekar defined a common framework for convex optimization problems that contains Garg-Konemann and Plotkin-Shmoys-Tardos as subcases.

A binary decision needs to be made based on n experts’ opinions to attain an associated payoff. In the first round, all experts’ opinions have the same weight. The decision maker will make the first decision based on the majority of the experts' prediction. Then, in each successive round, the decision maker will repeatedly update the weight of each expert's opinion depending on the correctness of his prior predictions. Real life examples includes predicting if it is rainy tomorrow or if the stock market will go up or go down.

Given a sequential game played between an adversary and an aggregator who is advised by N experts, the goal is for the aggregator to make as few mistakes as possible. Assume there is an expert among the N experts who always gives the correct prediction. In the halving algorithm, only the consistent experts are retained. Experts who make mistakes will be dismissed. For every decision, the aggregator decides by taking a majority vote among the remaining experts. Therefore, every time the aggregator makes a mistake, at least half of the remaining experts are dismissed. The aggregator makes at most mistakes.

Unlike halving algorithm which dismisses experts who have made mistakes, weighted majority algorithm discounts their advice. Given the same "expert advice" setup, suppose we have n decisions, and we need to select one decision for each loop. In each loop, every decision incurs a cost. All costs will be revealed after making the choice. The cost is 0 if the expert is correct, and 1 otherwise. this algorithm's goal is to limit its cumulative losses to roughly the same as the best of experts.
The very first algorithm that makes choice based on majority vote every iteration does not work since the majority of the experts can be wrong consistently every time. The weighted majority algorithm corrects above trivial algorithm by keeping a weight of experts instead of fixing the cost at either 1 or 0. This would make fewer mistakes compared to halving algorithm.

If formula_10, the weight of the expert's advice will remain the same. When formula_11 increases, the weight of the expert's advice will decrease. Note that some researchers fix formula_12 in weighted majority algorithm.

After formula_6 steps, let formula_14 be the number of mistakes of expert i and formula_15 be the number of mistakes our algorithm has made. Then we have the following bound for every formula_16:

In particular, this holds for i which is the best expert. Since the best expert will have the least formula_14, it will give the best bound on the number of mistakes made by the algorithm as a whole.

Given the same setup with N experts. Consider the special situation where the proportions of experts predicting positive and negative, counting the weights, are both close to 50%. Then, there might be a tie. Following the weight update rule in weighted majority algorithm, the predictions made by the algorithm would be randomized. The algorithm calculates the probabilities of experts predicting positive or negatives, and then makes a random decision based on the computed fraction:

predict 

where 

The number of mistakes made by the randomized weighted majority algorithm is bounded as: 

where formula_22 and formula_23.

Note that only the learning algorithm is randomized. The underlying assumption is that the examples and experts' predictions are not random. The only randomness is the randomness where the learner makes his own prediction.
In this randomized algorithm, formula_24 if formula_25. Compared to weighted algorithm, this randomness halved the number of mistakes the algorithm is going to make. However, it is important to note that in some research, people define formula_12 in weighted majority algorithm and allow formula_27 in randomized weighted majority algorithm.

The multiplicative weights method is usually used to solve a constrained optimization problem. Let each expert be the constraint in the problem, and the events represent the points in the area of interest. The punishment of the expert corresponds to how well its corresponding constraint is satisfied on the point represented by an event.

Suppose we were given the distribution formula_28 on experts. Let formula_29 = payoff matrix of a finite two-player zero-sum game, with formula_30 rows.

When the row player formula_31 uses plan formula_16 and the column player formula_33 uses plan formula_34, the payoff of player formula_33 is formula_36≔formula_37, assuming formula_38.

If player formula_31 chooses action formula_16 from a distribution formula_28 over the rows, then the expected result for player formula_33 selecting action formula_34 is formula_44.

To maximize formula_45, player formula_33 is should choose plan formula_34. Similarly, the expected payoff for player formula_48 is formula_49. Choosing plan formula_16 would minimize this payoff. By John Von Neumann's Min-Max Theorem, we obtain:

where P and i changes over the distributions over rows, Q and j changes over the columns.

Then, let formula_52 denote the common value of above quantities, also named as the "value of the game". Let formula_53 be an error parameter. To solve the zero-sum game bounded by additive error of formula_54,

So there is an algorithm solving zero-sum game up to an additive factor of δ using O(/formula_57) calls to ORACLE, with an additional processing time of O(n) per call

Bailey and Piliouras showed that although the time average behavior of multiplicative weights update converges to Nash equilibria in zero-sum games the day-to-day (last iterate) behavior diverges away from it.

In machine learning, Littlestone and Warmuth generalized the winnow algorithm to the weighted majority algorithm. Later, Freund and Schapire generalized it in the form of hedge algorithm. AdaBoost Algorithm formulated by Yoav Freund and Robert Schapire also employed the Multiplicative Weight Update Method.

Based on current knowledge in algorithms, multiplicative weight update method was first used in Littlestone's winnow algorithm. It is used in machine learning to solve a linear program.

Given formula_58 labeled examples formula_59 where formula_60 are feature vectors, and formula_61 are their labels.

The aim is to find non-negative weights such that for all examples, the sign of the weighted combination of the features matches its labels. That is, require that formula_62 for all formula_34. Without loss of generality, assume the total weight is 1 so that they form a distribution. Thus, for notational convenience, redefine formula_64 to be formula_65, the problem reduces to finding a solution to the following LP:

This is general form of LP.

The hedge algorithm is similar to the weighted majority algorithm. However, their exponential update rules are different.
It is generally used to solve the problem of binary allocation in which we need to allocate different portion of resources into N different options. The loss with every option is available at the end of every iteration. The goal is to reduce the total loss suffered for a particular allocation. The allocation for the following iteration is then revised, based on the total loss suffered in the current iteration using multiplicative update.

Assume the learning rate formula_69 and for formula_70, formula_71 is picked by Hedge. Then for all experts formula_16,

Initialization: Fix an formula_69. For each expert, associate the weight formula_75 ≔1
For t=1,2,…,T:

This algorithm maintains a set of weights formula_80 over the training examples. On every iteration formula_3, a distribution formula_71 is computed by normalizing these weights. This distribution is fed to the weak learner WeakLearn which generates a hypothesis formula_83 that (hopefully) has small error with respect to the distribution. Using the new hypothesis formula_83, AdaBoost generates the next weight vector formula_85. The process repeats. After T such iterations, the final hypothesis formula_86 is the output. The hypothesis formula_86 combines the outputs of the T weak hypotheses using a weighted majority vote.

Given a formula_109 matrix formula_29 and formula_111, is there a formula_112 such that formula_113?

Using the oracle algorithm in solving zero-sum problem, with an error parameter formula_115, the output would either be a point formula_112 such that formula_117 or a proof that formula_112 does not exist, i.e., there is no solution to this linear system of inequalities.

Given vector formula_119, solves the following relaxed problem

If there exists a x satisfying (1), then x satisfies (2) for all formula_121. The contrapositive of this statement is also true.
Suppose if oracle returns a feasible solution for a formula_122, the solution formula_112 it returns has bounded width formula_124.
So if there is a solution to (1), then there is an algorithm that its output x satisfies the system (2) up to an additive error of formula_125. The algorithm makes at most formula_126 calls to a width-bounded oracle for the problem (2). The contrapositive stands true as well. The multiplicative updates is applied in the algorithm in this case.

Multiplicative weights update is the discrete-time variant of the replicator equation (replicator dynamics), which is a commonly used model in evolutionary game theory. It converges to Nash equilibrium when applied to a congestion game.

In operations research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.

The multiplicative weights algorithm is also widely applied in computational geometry, such as Clarkson's algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find Set Covers for hypergraphs with small VC dimension.


</doc>
<doc id="52280151" url="https://en.wikipedia.org/wiki?curid=52280151" title="Incremental learning">
Incremental learning

In computer science, incremental learning is a method of machine learning in which input data is continuously used to extend the existing model's knowledge i.e. to further train the model. It represents a dynamic technique of supervised learning and unsupervised learning that can be applied when training data becomes available gradually over time or its size is out of system memory limits. Algorithms that can facilitate incremental learning are known as incremental machine learning algorithms.

Many traditional machine learning algorithms inherently support incremental learning.
Other algorithms can be adapted to facilitate incremental learning. 
Examples of incremental algorithms include
decision trees
(IDE4,
ID5R),
decision rules,
artificial neural networks
(RBF networks,
Learn++,
Fuzzy ARTMAP,
TopoART, and
IGNG) or
the incremental SVM.

The aim of incremental learning is for the learning model to adapt to new data without forgetting its existing knowledge, it does not retrain the model. Some incremental learners have built-in some parameter or assumption that controls the relevancy of old data, while others, called stable incremental machine learning algorithms, learn representations of the training data that are not even partially forgotten over time. Fuzzy ART and TopoART are two examples for this second approach.

Incremental algorithms are frequently applied to data streams or big data, addressing issues in data availability and resource scarcity respectively. Stock trend prediction and user profiling are some examples of data streams where new data becomes continuously available. Applying incremental learning to big data aims to produce faster classification or forecasting times.



</doc>
<doc id="214269" url="https://en.wikipedia.org/wiki?curid=214269" title="Emergent algorithm">
Emergent algorithm

An emergent algorithm is an algorithm that exhibits emergent behavior. In essence an emergent algorithm implements a set of simple "building block" behaviors that when combined exhibit more complex behaviors. One example of this is the implementation of fuzzy motion controllers used to adapt robot movement in response to environmental obstacles.

An emergent algorithm has the following characteristics: 


Other examples of emergent algorithms and models include cellular automata, artificial neural networks and swarm intelligence systems (ant colony optimization, bees algorithm, etc.).



</doc>
<doc id="1773852" url="https://en.wikipedia.org/wiki?curid=1773852" title="Gutmann method">
Gutmann method

The Gutmann method is an algorithm for securely erasing the contents of computer hard disk drives, such as files. Devised by Peter Gutmann and Colin Plumb and presented in the paper "Secure Deletion of Data from Magnetic and Solid-State Memory" in July 1996, it involved writing a series of 35 patterns over the region to be erased.

The selection of patterns assumes that the user does not know the encoding mechanism used by the drive, so it includes patterns designed specifically for three types of drives. A user who knows which type of encoding the drive uses can choose only those patterns intended for their drive. A drive with a different encoding mechanism would need different patterns.

Most of the patterns in the Gutmann method were designed for older MFM/RLL encoded disks. Gutmann himself has noted that more modern drives no longer use these older encoding techniques, making parts of the method irrelevant. He said "In the time since this paper was published, some people have treated the 35-pass overwrite technique described in it more as a kind of voodoo incantation to banish evil spirits than the result of a technical analysis of drive encoding techniques".

Since about 2001, some ATA IDE and SATA hard drive manufacturer designs include support for the ATA Secure Erase standard, obviating the need to apply the Gutmann method when erasing an entire drive. However, a 2011 research found that 4 out of 8 manufacturers did not implement ATA Secure Erase correctly.

One standard way to recover data that has been overwritten on a hard drive is to capture and process the analog signal obtained from the drive's read/write head prior to this analog signal being digitized. This analog signal will be close to an ideal digital signal, but the differences will reveal important information. By calculating the ideal digital signal and then subtracting it from the actual analog signal, it is possible to amplify the obtained difference signal and use it to determine what had previously been written on the disk.

For example:

This can then be done again to see the previous data written:

However, even when overwriting the disk repeatedly with random data it is theoretically possible to recover the previous signal. The permittivity of a medium changes with the frequency of the magnetic field. This means that a lower frequency field will penetrate deeper into the magnetic material on the drive than a high frequency one. So a low frequency signal will, in theory, still be detectable even after it has been overwritten hundreds of times by a high frequency signal.

The patterns used are designed to apply alternating magnetic fields of various frequencies and various phases to the drive surface and thereby approximate degaussing the material below the surface of the drive.

An overwrite session consists of a lead-in of four random write patterns, followed by patterns 5 to 31 (see rows of table below), executed in a random order, and a lead-out of four more random patterns.

Each of patterns 5 to 31 was designed with a specific magnetic media encoding scheme in mind, which each pattern targets. The drive is written to for all the passes even though the table below only shows the bit patterns for the passes that are specifically targeted at each encoding scheme. The end result should obscure any data on the drive so that only the most advanced physical scanning (e.g., using a magnetic force microscope) of the drive is likely to be able to recover any data. 

The series of patterns is as follows:

Encoded bits shown in bold are what should be present in the ideal pattern, although due to the encoding the complementary bit is actually present at the start of the track.

The delete function in most operating systems simply marks the space occupied by the file as reusable (removes the pointer to the file) without immediately removing any of its contents. At this point the file can be fairly easily recovered by numerous recovery applications. However, once the space is overwritten with other data, there is no known way to use software to recover it. It cannot be done with software alone since the storage device only returns its current contents via its normal interface. Gutmann claims that intelligence agencies have sophisticated tools, including magnetic force microscopes, which together with image analysis, can detect the previous values of bits on the affected area of the media (for example hard disk).

Daniel Feenberg of the National Bureau of Economic Research, an American private nonprofit research organization, criticized Gutmann's claim that intelligence agencies are likely to be able to read overwritten data, citing a lack of evidence for such claims. Nevertheless, some published government security procedures consider a disk overwritten once to still be sensitive.

Gutmann himself has responded to some of these criticisms and also criticized how his algorithm has been abused in an epilogue to his original paper, in which he states:



</doc>
<doc id="45655492" url="https://en.wikipedia.org/wiki?curid=45655492" title="Wiener connector">
Wiener connector

In mathematics applied to the study of networks, the Wiener connector, named in honor of chemist Harry Wiener who first introduced the Wiener Index, is a means of maximizing efficiency in connecting specified "query vertices" in a network. Given a connected, undirected graph and a set of query vertices in a graph, the minimum Wiener connector is an induced subgraph that connects the query vertices and minimizes the sum of shortest path distances among all pairs of vertices in the subgraph. In combinatorial optimization, the minimum Wiener connector problem is the problem of finding the minimum Wiener connector. It can be thought of as a version of the classic Steiner tree problem (one of Karp's 21 NP-complete problems), where instead of minimizing the size of the tree, the objective is to minimize the distances in the subgraph.

The minimum Wiener connector was first presented by Ruchansky, et al. in 2015.

The minimum Wiener connector has applications in many domains where there is a graph structure and an interest in learning about connections between sets of individuals. For example, given a set of patients infected with a viral disease, which other patients should be checked to find the culprit? Or given a set of proteins of interest, which other proteins participate in pathways with them?

The Wiener index is the sum of shortest path distances in a (sub)graph. Using formula_1 to denote the shortest path between formula_2 and formula_3, the Wiener index of a (sub)graph formula_4, denoted formula_5, is defined as

The minimum Wiener connector problem is defined as follows. Given an undirected and unweighted graph with vertex set formula_7 and edge set formula_8 and a set of query vertices formula_9, find a connector formula_10 of minimum Wiener index. More formally, the problem is to compute
that is, find a connector formula_12 that minimizes the sum of shortest paths in formula_12.

The minimum Wiener connector problem is related to the Steiner tree problem. In the former, the objective function in the minimization is the Wiener index of the connector, whereas in the latter, the objective function is the sum of the weights of the edges in the connector. The optimum solutions to these problems may differ, given the same graph and set of query vertices. In fact, a solution for the Steiner tree problem may be arbitrarily bad for the minimum Wiener connector problem; the graph on the right provides an example.

The problem is NP-hard, and does not admit a polynomial-time approximation scheme unless P = NP. This can be proven using the inapproximability of vertex cover in bounded degree graphs. Although there is no polynomial-time approximation scheme, there is a polynomial-time constant-factor approximation—an algorithm that finds a connector whose Wiener index is within a constant multiplicative factor of the Wiener index of the optimum connector. In terms of complexity classes, the minimum Wiener connector problem is in APX but is not in PTAS unless P = NP.

An exhaustive search over all possible subsets of vertices to find the one that induces the connector of minimum Wiener index yields an algorithm that finds the optimum solution in formula_14 time (that is, exponential time) on graphs with "n" vertices. In the special case that there are exactly two query vertices, the optimum solution is the shortest path joining the two vertices, so the problem can be solved in polynomial time by computing the shortest path. In fact, for any fixed constant number of query vertices, an optimum solution can be found in polynomial time.

There is a constant-factor approximation algorithm for the minimum Wiener connector problem that runs in time formula_15 on a graph with "n" vertices, "m" edges, and "q" query vertices, roughly the same time it takes to compute shortest-path distances from the query vertices to every other vertex in the graph. The central approach of this algorithm is to reduce the problem to the vertex-weighted Steiner tree problem, which admits a constant-factor approximation in particular instances related to the minimum Wiener connector problem.

The minimum Wiener connector behaves like betweenness centrality.

When the query vertices belong to the same community, the non-query vertices that form the minimum Wiener connector tend to belong to the same community and have high centrality within the community. Such vertices are likely to be influential vertices playing leadership roles in the community. In a social network, these influential vertices might be good users for spreading information or to target in a viral marketing campaign.

When the query vertices belong to different communities, the non-query vertices that form the minimum Wiener connector contain vertices adjacent to edges that bridge the different communities. These vertices span a structural hole in the graph and are important.

The minimum Wiener connector is useful in applications in which one wishes to learn about the relationship between a set of vertices in a graph. For example,


</doc>
<doc id="39045480" url="https://en.wikipedia.org/wiki?curid=39045480" title="Automate This">
Automate This

Automate This: How Algorithms Came to Rule Our World is a book written by Christopher Steiner and published by Penguin Group. Steiner begins his study of algorithms on Wall Street in the 1980s but also provides examples from other industries. For example, he explains the history of Pandora Radio and the use of algorithms in music identification. He expresses concern that such use of algorithms may lead to the homogenization of music over time. Steiner also discusses the algorithms that eLoyalty (now owned by Mattersight Corporation following divestiture of the technology) was created by dissecting 2 million speech patterns and can now identify a caller's personality style and direct the caller with a compatible customer support representative.

Steiner's book shares both the warning and the opportunity that algorithms bring to just about every industry in the world, and the pros and cons of the societal impact of automation (e.g. impact on employment).



</doc>
<doc id="46902242" url="https://en.wikipedia.org/wiki?curid=46902242" title="Lancichinetti–Fortunato–Radicchi benchmark">
Lancichinetti–Fortunato–Radicchi benchmark

Lancichinetti–Fortunato–Radicchi benchmark is an algorithm that generates benchmark networks (artificial networks that resemble real-world networks). They have "a priori" known communities and are used to compare different community detection methods. The advantage of the benchmark over other methods is that it accounts for the heterogeneity in the distributions of node degrees and of community sizes.

The node degrees and the community sizes are distributed according to a power law, with different exponents. The benchmark assumes that both the degree and the community size have power law distributions with different exponents, formula_1 and formula_2, respectively. formula_3 is the number of nodes and the average degree is formula_4. There is a mixing parameter formula_5, which is the average fraction of neighboring nodes of a node that do not belong to any community that the benchmark node belongs to. This parameter controls the fraction of edges that are between communities. Thus, it reflects the amount of noise in the network. At the extremes, when formula_6 all links are within community links, if formula_7 all links are between nodes belonging to different communities.

One can generate the benchmark network using the following steps.

Step 1: Generate a network with nodes following a power law distribution with exponent formula_1 and choose extremes of the distribution formula_9 and formula_10 to get desired average degree is formula_11.

Step 2: formula_12 fraction of links of every node is with nodes of the same community, while fraction formula_5 is with the other nodes.

Step 3: Generate community sizes from a power law distribution with exponent formula_2. The sum of all sizes must be equal to formula_3. The minimal and maximal community sizes formula_16 and formula_17 must satisfy the definition of community so that every non-isolated node is in at least in one community:

Step 4: Initially, no nodes are assigned to communities. Then, each node is randomly assigned to a community. As long as the number of neighboring nodes within the community does not exceed the community size a new node is added to the community, otherwise stays out. In the following iterations the “homeless” node is randomly assigned to some community. If that community is complete, i.e. the size is exhausted, a randomly selected node of that community must be unlinked. Stop the iteration when all the communities are complete and all the nodes belong to at least one community.

Step 5: Implement rewiring of nodes keeping the same node degrees but only affecting the fraction of internal and external links such that the number of links outside the community for each node is approximately equal to the mixing parameter formula_5.

Consider a partition into communities that do not overlap. The communities of randomly chosen nodes in each iteration follow a formula_21 distribution that represents the probability that a randomly picked node is from the community formula_22. Consider a partition of the same network that was predicted by some community finding algorithm and has formula_23 distribution. The benchmark partition has formula_24 distribution.
The joint distribution is formula_25. The similarity of these two partitions is captured by the normalized mutual information.

If formula_27 the benchmark and the detected partitions are identical, and if formula_28 then they are independent of each other.


</doc>
<doc id="53596792" url="https://en.wikipedia.org/wiki?curid=53596792" title="FGLM algorithm">
FGLM algorithm

FGLM is one of the main algorithms in computer algebra, named after its designers, Faugère, Gianni, Lazard and Mora. They introduced their algorithm in 1993. The input of the algorithm is a Gröbner basis of a zero-dimensional ideal in the ring of polynomials over a field with respect to a monomial order and a second monomial order; As its output, it returns a Gröbner basis of the ideal with respect to the second ordering. The algorithm is a fundamental tool in computer algebra and has been implemented in most of the computer algebra systems. The complexity of FGLM is "O"("nD"), where "n" is the number of variables of the polynomials and D is the degree of the ideal. There are several generalization and various application for FGLM.


</doc>
<doc id="53471572" url="https://en.wikipedia.org/wiki?curid=53471572" title="Monte Carlo polarization">
Monte Carlo polarization

In analytic business theory Monte Carlo Polarization is an opinion generation algorithm for a given prototype or design idea. 
The algorithm expands on traditional Monte Carlo aggregation which operates by placing candidates together and selecting a subset at random.
Each member of this subset is then asked for an opinion usually by filling out a form.
A resultant opinion scalar can be generated by application of the Softmax function over the generated form set.
However Monte Carlo Polarization goes a step further and attempts to construct the subset with the greatest standard deviation in response, referred to as the form data response eigen-norm vector scalar.
The idea of Monte-Carlo Polarization was firstly invented in Athens, (more commonly known as Thens), by Errikos Babudopoulos in 1978, but was mostly used in research in the 1990s by famous mathematicians, such as Grigori Perelman, in proving the soul conjecture.

The origins of Monte Carlo polarization came from the following observations made in early 1922:
Where the validity of an opinion is defined by the Emotional Intelligence Hierarchical metric space, using the obvious distance function.

Given an array "A" of "n" elements with values or records "A" ... "A", sorted such that "A" ≤ ... ≤ "A", and target value "T", the following subroutine uses Monte Carlo Polarization to find the index of "T" in "A".

Extract the emotional category isofunctor morphismvector and append this to the business manifesto.

Note: This can be done in matrix form. This is left as an exercise for the reader.

Although this a very recent cutting-edge technique, it has seen a couple of variations upon the basic algorithm in the last couple of month, most notably JSON driven resolution methods. the basic idea is that instead of supplying the algorithm with "n" records, it is more useful to provide the algorithm with emotional meta-data to guide its search and improve its complexity beyond the usual logarithmic bounds and this by a factor of "log(n)/2". It allows to select intermediate "m" values for the search index and skew them towards the wanted emotional value in the initial records.

The acceleration structures permitted by the Monte Carlo Polarization consist mainly in BVH and EBVH hierarchies. The logical subdivision of the kernel space leads to a logarithmic complexity, which is key to the scalability of the sentient analysis tools.

A key application is the direct targeting of hidden nodes in neural networks. By applying a Monte Carlo Polarization filter to the input layer of the neural system, hidden layers will be systematically and dynamically selected based on user-defined characteristics. Only the specified layers and units will receive and process the data.

Compared to standard drop-off methods, Monte Carlo Polarization is both more effective and more secure. Instead of having all nodes receiving the data and selecting output from a subset, the unnecessary nodes are directly filtered out. The result is a greatly increased level of accuracy and protection, as unreliable and malicious nodes will be left out, and a higher degree of efficiency.

The neural system that is created using the aforementioned method is the basis for many Computer Vision projects. A specific highlight is the American web-animated sitcom "F is for Family". 

Monte Carlo polarization can be easily deployed through Node.js.

The library provides a basic implementation of Monte Carlo polarization, and shows the kernel space learning algorithm applied to session tokens.

The native support of JSON files by NodeJS's JavaScript language is an example of the application of JSON Driven Monte Carlo Polarization.

Being a cutting edge technology, researchers are experimenting the expandability of the current technology to support Asynchronous Transport Protocol for JSON, and to provide an API for classic AJAX (Asynchronous Javascript and XML) interface by tunneling the data through Socket.IO packets secured by blockchain technology.


</doc>
<doc id="313384" url="https://en.wikipedia.org/wiki?curid=313384" title="Long division">
Long division

In arithmetic, long division is a standard division algorithm suitable for dividing multi-digit numbers that is simple enough to perform by hand. It breaks down a division problem into a series of easier steps.

As in all division problems, one number, called the dividend, is divided by another, called the divisor, producing a result called the quotient. It enables computations involving arbitrarily large numbers to be performed by following a series of simple steps. The abbreviated form of long division is called short division, which is almost always used instead of long division when the divisor has only one digit. Chunking (also known as the partial quotients method or the hangman method) is a less mechanical form of long division prominent in the UK which contributes to a more holistic understanding about the division process.

While related algorithms have existed since the 12th century AD, the specific algorithm in modern use was introduced by Henry Briggs 1600 AD.

Inexpensive calculators and computers have become the most common way to solve division problems, eliminating a traditional mathematical exercise, and decreasing the educational opportunity to show how to do so by paper and pencil techniques. (Internally, those devices use one of a variety of division algorithms, the faster ones of which relies on approximations and multiplications to achieve the tasks). In the United States, long division has been especially targeted for de-emphasis, or even elimination from the school curriculum, by reform mathematics, though traditionally introduced in the 4th or 5th grades.

In English-speaking countries, long division does not use the division slash or obelus signs but instead constructs a tableau. The divisor is separated from the dividend by a right parenthesis or vertical bar ; the dividend is separated from the quotient by a vinculum (i.e., overbar). The combination of these two symbols is sometimes known as a long division symbol or division bracket. It developed in the 18th century from an earlier single-line notation separating the dividend from the quotient by a left parenthesis.

The process is begun by dividing the left-most digit of the dividend by the divisor. The quotient (rounded down to an integer) becomes the first digit of the result, and the remainder is calculated (this step is notated as a subtraction). This remainder carries forward when the process is repeated on the following digit of the dividend (notated as 'bringing down' the next digit to the remainder). When all digits have been processed and no remainder is left, the process is complete.

An example is shown below, representing the division of 500 by 4 (with a result of 125).
A more detailed breakdown of the steps goes as follows:


If the last remainder when we ran out of dividend digits had been something other than 0, there would have been two possible courses of action:


In this example, the decimal part of the result is calculated by continuing the process beyond the units digit, "bringing down" zeros as being the decimal part of the dividend.

This example also illustrates that, at the beginning of the process, a step that produces a zero can be omitted. Since the first digit 1 is less than the divisor 4, the first step is instead performed on the first two digits 12. Similarly, if the divisor were 13, one would perform the first step on 127 rather than 12 or 1.


A divisor of any number of digits can be used. In this example, 1260257 is to be divided by 37. First the problem is set up as follows:

Digits of the number 1260257 are taken until a number greater than or equal to 37 occurs. So 1 and 12 are less than 37, but 126 is greater. Next, the greatest multiple of 37 less than or equal to 126 is computed. So 3 × 37 = 111 < 126, but 4 × 37 > 126. The multiple 111 is written underneath the 126 and the 3 is written on the top where the solution will appear:

Note carefully which place-value column these digits are written into. The 3 in the quotient goes in the same column (ten-thousands place) as the 6 in the dividend 1260257, which is the same column as the last digit of 111.

The 111 is then subtracted from the line above, ignoring all digits to the right:

Now the digit from the next smaller place value of the dividend is copied down and appended to the result 15:

The process repeats: the greatest multiple of 37 less than or equal to 150 is subtracted. This is 148 = 4 × 37, so a 4 is added to the top as the next quotient digit. Then the result of the subtraction is extended by another digit taken from the dividend:

The greatest multiple of 37 less than or equal to 22 is 0 × 37 = 0. Subtracting 0 from 22 gives 22, we often don't write the subtraction step. Instead, we simply take another digit from the dividend:

The process is repeated until 37 divides the last line exactly:

For non-decimal currencies (such as the British £sd system before 1971) and measures (such as avoirdupois) mixed mode division must be used. Consider dividing 50 miles 600 yards into 37 pieces:

Each of the four columns is worked in turn. Starting with the miles: 50/37 = 1 remainder 13. No further division is
possible, so perform a long multiplication by 1,760 to convert miles to yards, the result is 22,880 yards. Carry this to the top of the yards column and add it to the 600 yards in the dividend giving 23,480. Long division of 23,480 / 37 now proceeds as normal yielding 634 with remainder 22. The remainder is multiplied by 3 to get feet and carried up to the feet column. Long division of the feet gives 1 remainder 29 which is then multiplied by twelve to get 348 inches. Long division continues with the final remainder of 15 inches being shown on the result line.

When the quotient is not an integer and the division process is extended beyond the decimal point, one of two things can happen:


China, Japan, Korea use the same notation as English-speaking nations including India. Elsewhere, the same general principles are used, but the figures are often arranged differently.

In Latin America (except Argentina, Bolivia, Mexico, Colombia, Paraguay, Venezuela, Uruguay and Brazil), the calculation is almost exactly the same, but is written down differently as shown below with the same two examples used above. Usually the quotient is written under a bar drawn under the divisor. A long vertical line is sometimes drawn to the right of the calculations.

and

In Mexico, the English-speaking world notation is used, except that only the result of the subtraction is annotated and the calculation is done mentally, as shown below:

In Bolivia, Brazil, Paraguay, Venezuela, Quebec, Colombia, and Peru, the European notation (see below) is used, except that the quotient is not separated by a vertical line, as shown below:

Same procedure applies in Mexico, Uruguay and Argentina, only the result of the subtraction is annotated and the calculation is done mentally.

In Spain, Italy, France, Portugal, Lithuania, Romania, Turkey, Greece, Belgium, Belarus, Ukraine, and Russia, the divisor is to the right of the dividend, and separated by a vertical bar. The division also occurs in the column, but the quotient (result) is written below the divider, and separated by the horizontal line. The same method is used in Iran and Mongolia.

In Cyprus, as well as in France, a long vertical bar separates the dividend and subsequent subtractions from the quotient and divisor, as in the below of 6359 divided by 17, which is 374 with a remainder of 1.

Decimal numbers are not divided directly, the dividend and divisor are multiplied by a power of ten so that the division involves two whole numbers. Therefore, if one were dividing 12,7 by 0,4 (commas being used instead of decimal points), the dividend and divisor would first be changed to 127 and 4, and then the division would proceed as above.

In Austria, Germany and Switzerland, the notational form of a normal equation is used. <dividend> : <divisor> = <quotient>, with the colon ":" denoting a binary infix symbol for the division operator (analogous to "/" or "÷"). In these regions the decimal separator is written as a comma. (cf. first section of Latin American countries above, where it's done virtually the same way):

The same notation is adopted in Denmark, Norway, Bulgaria, North Macedonia, Poland, Croatia, Slovenia, Hungary, Czech Republic, Slovakia, Vietnam and in Serbia.

In the Netherlands, the following notation is used:

Every natural number n can be uniquely represented in an arbitrary number base formula_1 as a sequence of digits formula_2 where formula_3, where formula_4 is the number of digits in formula_5. The value of n in terms of its digits and the base is
Let formula_5 be the dividend and formula_8 be the divisor, where formula_9 is the number of digits in formula_8. If formula_11, then formula_12 and formula_13. Otherwise, we iterate from formula_14, before stopping.

For each iteration formula_15, let formula_16 be the quotient extracted so far, formula_17 be the intermediate dividend, formula_18 be the intermediate remainder, formula_19 be the next digit of the original dividend, and formula_20 be the next digit of the quotient. By definition of digits in base formula_21, formula_22. All values are natural numbers. We initiate 
the first formula_9 digits of formula_5.

With every iteration, the three equations are true:
There only exists one such formula_20 such that formula_31.
The final quotient is formula_32 and the final remainder is formula_33

In base 10, using the example above with formula_34 and formula_35, the initial values formula_36 and formula_37.

Thus, formula_38 and formula_39.

In base 16, with formula_40 and formula_41, the initial values are formula_36 and formula_43.

Thus, formula_44 and formula_45.

If one doesn't have the addition, subtraction, or multiplication tables for base formula_21 memorised, then this algorithm still works if the numbers are converted to decimal and at the end are converted back to base formula_21. For example, with the above example, 
and 
with formula_50. The initial values are formula_36 and formula_52.
Thus, formula_53 and formula_54.

This algorithm can be done using the same kind of pencil-and-paper notations as shown in above sections.

If the quotient is not constrained to be an integer, then the algorithm does not terminate for formula_55. Instead, if formula_55 then formula_57 by definition. If the remainder formula_18 is equal to zero at any iteration, then the quotient is a formula_21-adic fraction, and is represented as a finite decimal expansion in base formula_21 positional notation. Otherwise, it is still a rational number but not a formula_21-adic rational, and is instead represented as an infinite repeating decimal expansion in base formula_21 positional notation.

Calculation within the binary number system is simpler, because each digit in the course can only be 1 or 0 - no multiplication is needed as multiplication by either either results in the same number or zero.

If this were on a computer, multiplication by 10 can be represented by a bit shift of 1 to the left, and finding formula_20 reduces down to the logical operation formula_64, where true = 1 and false = 0. With every iteration formula_65, the following operations are done:

For example, with formula_81 and formula_82, the initial values are formula_36 and formula_84.

Thus, formula_85 and formula_86.

On each iteration, the most time-consuming task is to select formula_20. We know that there are formula_21 possible values, so we can find formula_20 using formula_90 comparisons. Each comparison will require evaluating formula_91. Let formula_4 be the number of digits in the dividend formula_5 and formula_9 be the number of digits in the divisor formula_8. The number of digits in formula_96. The multiplication of formula_97 is therefore formula_98, and likewise the subtraction of formula_91. Thus it takes formula_100 to select formula_20. The remainder of the algorithm are addition and the digit-shifting of formula_16 and formula_18 to the left one digit, and so takes time formula_104 and formula_98 in base formula_21, so each iteration takes formula_107, or just formula_108. For all formula_109 digits, the algorithm takes time formula_110, or formula_111 in base formula_21.

Long division of integers can easily be extended to include non-integer dividends, as long as they are rational. This is because every rational number has a recurring decimal expansion. The procedure can also be extended to include divisors which have a finite or terminating decimal expansion (i.e. decimal fractions). In this case the procedure involves multiplying the divisor and dividend by the appropriate power of ten so that the new divisor is an integer – taking advantage of the fact that "a" ÷ "b" = ("ca") ÷ ("cb") – and then proceeding as above.

A generalised version of this method called polynomial long division is also used for dividing polynomials (sometimes using a shorthand version called synthetic division).




</doc>
<doc id="4104986" url="https://en.wikipedia.org/wiki?curid=4104986" title="How to Solve it by Computer">
How to Solve it by Computer

How to Solve it by Computer is a computer science book by R. G. Dromey, first published by Prentice-Hall in 1982.
It is occasionally used as a textbook, especially in India.

It is an introduction to the "why"s of algorithms and data structures.
Features of the book:

The very fundamental algorithms portrayed by this book are mostly presented in Pseudocode and/or Pascal notation.



</doc>
<doc id="54625345" url="https://en.wikipedia.org/wiki?curid=54625345" title="Right to explanation">
Right to explanation

In the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to explanation (or right to "an" explanation) is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be "Credit bureau X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for."

Some such legal rights already exist, while the scope of a general "right to explanation" is a matter of ongoing debate.

Credit score in the United States – more generally, credit actions – have a well-established right to explanation. Under the Equal Credit Opportunity Act (Regulation B of the Code of Federal Regulations),
Title 12, Chapter X, Part 1002, §1002.9, creditors are required to notify applicants of action taken in certain circumstances, and such notifications must provide specific reasons, as detailed in §1002.9(b)(2):

The official interpretation of this section details what types of statements are acceptable.

Credit agencies and data analysis firms such as FICO comply with this regulation by providing a list of reasons (generally at most 4, per interpretation of regulations), consisting of a numeric (as identifier) and an associated explanation, identifying the main factors affecting a credit score. An example might be:

The European Union General Data Protection Regulation (enacted 2016, taking effect 2018), extends the automated decision-making rights in the 1995 Data Protection Directive to provide a legally disputed form of a right to an explanation, stated as such in Recital 71: "[the data subject should have] the right ... to obtain an explanation of the decision reached". In full:

However, the extent to which the regulations themselves provide a "right to explanation" is heavily debated. There are two main strands of criticism. There are significant legal issues with the right as found in Article 22 — as recitals are not binding, and the right to an explanation is not mentioned in the binding articles of the text, having been removed during the legislative process. In addition, there are significant restrictions on the types of automated decisions that are covered — which must be both "solely" based on automated processing, and have legal or similarly significant effects — which significantly limits the range of automated systems and decisions to which the right would apply. In particular, the right is unlikely to apply in many of the cases of algorithmic controversy that have been picked up in the media.

A second potential source of such a right has been pointed to in Article 15, the "right of access by the data subject". This restates a similar provision from the 1995 Data Protection Directive, allowing the data subject access to "meaningful information about the logic involved" in the same significant, solely automated decision-making, found in Article 22. Yet this too suffers from alleged challenges that relate to the timing of when this right can be drawn upon, as well as practical challenges that mean it may not be binding in many cases of public concern.

In France the 2016 "Loi pour une République numérique" (Digital Republic Act or "loi numérique") amends the country's administrative code to introduce a new provision for the explanation of decisions made by public sector bodies about individuals. It notes that where there is "a decision taken on the basis of an algorithmic treatment", the rules that define that treatment and its “principal characteristics” must be communicated to the citizen upon request, where there is not an exclusion (e.g. for national security or defence). These should include the following:
Scholars have noted that this right, while limited to administrative decisions, goes beyond the GDPR right to explicitly apply to decision support rather than decisions "solely" based on automated processing, as well as provides a framework for explaining specific decisions. Indeed, the GDPR automated decision-making rights in the European Union, one of the places a "right to an explanation" has been sought within, find their origins in French law in the late 1970s.

Some argue that a "right to explanation" is at best unnecessary, at worst harmful, and threatens to stifle innovation. Specific criticisms include: favoring human decisions over machine decisions; being redundant with existing laws; and focusing on process over outcome.

More fundamentally, many algorithms used in machine learning are not easily explainable. For example, the output of a deep neural network depends on many layers of computations, connected in a complex way, and no one input or computation may be a dominant factor. The field of Explainable AI seeks to provide better explanations from existing algorithms, and algorithms that are more easily explainable, but it is a young and active field.

Similarly, human decisions often cannot be easily explained: they may be based on intuition or a "gut feeling" that is hard to put into words. Some would argue that machines should not be required to meet a higher standard than humans.




</doc>
<doc id="55206702" url="https://en.wikipedia.org/wiki?curid=55206702" title="Seidel's algorithm">
Seidel's algorithm

Seidel's algorithm is an algorithm designed by Raimund Seidel in 1992 for the all-pairs-shortest-path problem for undirected, unweighted, connected graphs. It solves the problem in formula_1 expected time for a graph with formula_2 vertices, where formula_3 is the exponent in the complexity formula_4 of formula_5 matrix multiplication. If only the distances between each pair of vertices are sought, the same time bound can be achieved in the worst case. Even though the algorithm is designed for connected graphs, it can be applied individually to each connected component of a graph with the same running time overall. There is an exception to the expected running time given above for computing the paths: if formula_6 the expected running time becomes formula_7.

The core of the algorithm is a procedure that computes the length of the shortest-paths between any pair of vertices.
This can be done in formula_1 time in the worst case. Once the lengths are computed, the paths can be reconstructed using a Las Vegas algorithm whose expected running time is formula_1 for formula_10 and formula_7 for formula_6.

The Python code below assumes the input graph is given as a formula_13 formula_14-formula_15 adjacency matrix formula_16 with zeros on the diagonal. It defines the function APD which returns a matrix with entries formula_17 such that formula_17 is the length of the shortest path between the vertices formula_19 and formula_20. The matrix class used can be any matrix class implementation supporting the multiplication, exponentiation, and indexing operators (for example numpy.matrix).
def apd(A, n: int):
The base case tests whether the input adjacency matrix describes a complete graph, in which case all shortest paths have length formula_15.

Algorithms for undirected and directed graphs with weights from a finite universe formula_22 also exist. The best known algorithm for the directed case is in time formula_23 by Zwick in 1998. This algorithm uses rectangular matrix multiplication instead of square matrix multiplication. Better upper bounds can be obtained if one uses the best rectangular matrix multiplication algorithm available instead of achieving rectangular multiplication via multiple square matrix multiplications. The best known algorithm for the undirected case is in time formula_24 by Shoshan and Zwick in 1999. The original implementation of this algorithm was erroneous and has been corrected by Eirinakis, Williamson, and Subramani in 2016.


</doc>
<doc id="1881722" url="https://en.wikipedia.org/wiki?curid=1881722" title="External memory algorithm">
External memory algorithm

In computing, external memory algorithms or out-of-core algorithms are algorithms that are designed to process data that are too large to fit into a computer's main memory at once. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory (auxiliary memory) such as hard drives or tape drives, or when memory is on a computer network. External memory algorithms are analyzed in the external memory model.

External memory algorithms are analyzed in an idealized model of computation called the external memory model (or I/O model, or disk access model). The external memory model is an abstract machine similar to the RAM machine model, but with a cache in addition to main memory. The model captures the fact that read and write operations are much faster in a cache than in main memory, and that reading long contiguous blocks is faster than reading randomly using a disk read-and-write head. The running time of an algorithm in the external memory model is defined by the number of reads and writes to memory required. The model was introduced by Alok Aggarwal and Jeffrey Vitter in 1988. The external memory model is related to the cache-oblivious model, but algorithms in the external memory model may know both the block size and the cache size. For this reason, the model is sometimes referred to as the cache-aware model.

The model consists of a processor with an internal memory or cache of size , connected to an unbounded external memory. Both the internal and external memory are divided into blocks of size . One input/output or memory transfer operation consists of moving a block of contiguous elements from external to internal memory, and the running time of an algorithm is determined by the number of these input/output operations.

Algorithms in the external memory model take advantage of the fact that retrieving one object from external memory retrieves an entire block of size formula_1. This property is sometimes referred to as locality.

Searching for an element among formula_2 objects is possible in the external memory model using a B-tree with branching factor formula_1. Using a B-tree, searching, insertion, and deletion can be achieved in formula_4 time (in Big O notation). Information theoretically, this is the minimum running time possible for these operations, so using a B-tree is asymptotically optimal.

External sorting is sorting in an external memory setting. External sorting can be done via distribution sort, which is similar to quicksort, or via a formula_5-way merge sort. Both variants achieve the asymptotically optimal runtime of formula_6 to sort objects. This bound also applies to the Fast Fourier Transform in the external memory model.

The permutation problem is to rearrange formula_2 elements into a specific permutation. This can either be done either by sorting, which requires the above sorting runtime, or inserting each element in order and ignoring the benefit of locality. Thus, permutation can be done in formula_8 time.

The external memory model captures the memory hierarchy, which is not modeled in other common models used in analyzing data structures, such as the random access machine, and is useful for proving lower bounds for data structures. The model is also useful for analyzing algorithms that work on datasets too big to fit in internal memory.

A typical example is geographic information systems, especially digital elevation models, where the full data set easily exceeds several gigabytes or even terabytes of data.

This methodology extends beyond general purpose CPUs and also includes GPU computing as well as classical digital signal processing. In general-purpose computing on graphics processing units (GPGPU), powerful graphics cards (GPUs) with little memory (compared with the more familiar system memory, which is most often referred to simply as RAM) are utilized with relatively slow CPU-to-GPU memory transfer (when compared with computation bandwidth).

An early use of the term "out-of-core" as an adjective is in 1962 in reference to "devices" that are other than the core memory of an IBM 360. An early use of the term "out-of-core" with respect to "algorithms" appears in 1971.




</doc>
<doc id="56036557" url="https://en.wikipedia.org/wiki?curid=56036557" title="Proof of authority">
Proof of authority

Proof of authority (PoA) is an algorithm used with blockchains that delivers comparatively fast transactions through a consensus mechanism based on identity as a stake.

In PoA-based networks, transactions and blocks are validated by approved accounts, known as validators. Validators run software allowing them to put transactions in blocks. The process is automated and does not require validators to be constantly monitoring their computers. It, however, does require maintaining the computer (the authority node) uncompromised. The term was coined by Gavin Wood, co-founder of Ethereum and Parity Technologies.

With PoA, individuals earn the right to become validators, so there is an incentive to retain the position that they have gained. By attaching a reputation to identity, validators are incentivized to uphold the transaction process, as they do not wish to have their identities attached to a negative reputation. This is considered more robust than PoS (proof-of-stake) - PoS, while a stake between two parties may be even, it does not take into account each party’s total holdings. This means that incentives can be unbalanced. 
On the other hand, PoA only allows non-consecutive block approval from any one validator, meaning that the risk of serious damage is centralized to the authority node.

PoA is suited for both private networks and public networks, like POA Network, where trust is distributed.


</doc>
<doc id="47937215" url="https://en.wikipedia.org/wiki?curid=47937215" title="The Master Algorithm">
The Master Algorithm

The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.

The book outlines five tribes of machine learning: inductive reasoning, connectionism, evolutionary computation, Bayes' theorem and analogical modelling. The author explains these tribes to the reader by referring to more understandable processes of logic, connections made in the brain, natural selection, probability and similarity judgements. Throughout the book, it is suggested that each different tribe has the potential to contribute to a unifying "master algorithm".

Towards the end of the book the author pictures a "master algorithm" in the near future, where machine learning algorithms asymptotically grow to a perfect understanding of how the world and people in it work. Although the algorithm doesn't yet exist, he briefly reviews his own invention of the Markov logic network.

In 2016 Bill Gates recommended the book, alongside Nick Bostrom's "Superintelligence", as one of two books everyone should read to understand AI. In 2018 the book was noted to be on Chinese President Xi Jinping's bookshelf.

A computer science educator stated in "Times Higher Education" that the examples are clear and accessible. In contrast, "The Economist" agreed Domingos "does a good job" but complained that he "constantly invents metaphors that grate or confuse". "Kirkus Reviews" praised the book, stating "Readers unfamiliar with logic and computer theory will have a difficult time, but those who persist will discover fascinating insights."

A "New Scientist" review called it "compelling but rather unquestioning".



</doc>
<doc id="56463048" url="https://en.wikipedia.org/wiki?curid=56463048" title="Algorithms of Oppression">
Algorithms of Oppression

Algorithms of Oppression: How Search Engines Reinforce Racism is a 2018 book by Safiya Umoja Noble in the fields of information science, machine learning, and human-computer interaction.

Noble is a professor at the University of California, Los Angeles in the Department of Information Studies. She is a Co-Director of the Critical Internet Inquiry Center and also works with African American Studies and Gender Studies. Her best-selling book, "Algorithms Of Oppression", has been featured in the Los Angeles Review of Books, New York Public Library 2018 Best Books for Adults, and Bustle’s magazine 10 Books about Race to Read Instead of Asking a Person of Color to Explain Things to You. Her work markets the ways that digital media impacts issues of race, gender, culture, and technology. 

"Algorithms of Oppression" is a text based on over six years of academic research on Google search algorithms. Noble argues that search algorithms become racist because they reflect the biases and values of the people who create them. These algorithms can then have negative biases against women of color and other marginalized populations, while also affecting Internet users in general by leading to "racial and gender profiling, misrepresentation, and even economic redlining." She mentions the issue of technological redlining, that profiles users.

In Chapter 1 of "Algorithms of Oppression", Safiya Noble explores how Google search’s auto suggestion feature is demoralizing. On September 18, 2011 a mother googled “black girls” attempting to find fun activities to show her stepdaughter and nieces. To her surprise, the results encompassed websites and images of porn. This result encloses the data failures specific to people of color and women which Noble coins algorithmic oppression. Noble also adds that as a society we must have a feminist lens, with racial awareness to understand the “problematic positions about the benign instrumentality of technologies.”

Noble also discusses how Google can remove the human curation from the first page of results to eliminate any potential racial slurs or inappropriate imaging. Another example discussed in this text is a public dispute of the results that were returned when “jew” was searched on Google. The results included a number of anti-Semitic pages and Google claimed little ownership for the way it provided these identities. Google instead encouraged people to use “jews” or “Jewish people” and claimed the actions of White supremacist groups are out of Google’s control. Unless pages are unlawful, Google will allow its algorithm to continue to act without removing pages.

Noble reflects on AdWords which is Google's advertising tool and how this tool can add to the biases on Google. Adwords allows anyone to advertise on Google’s search pages and is highly customizable. First, Google ranks ads on relevance and then displays the ads on pages which is believes are relevant to the search query taking place. An advertiser can also set a maximum amount of money per day to spend on advertising. The more you spend on ads, the higher probability your ad will be closer to the top. Therefore, if an advertiser is passionate about his/her topic but is controversial it may be the first to appear on a Google search. 

Critical reception for "Algorithms of Oppression" has been largely positive. In the "Los Angeles Review of Books", Emily Drabinski writes, "What emerges from these pages is the sense that Google’s algorithms of oppression comprise just one of the hidden infrastructures that govern our daily lives, and that the others are likely just as hard-coded with white supremacy and misogyny as the one that Noble explores." In "PopMatters," Hans Rollman describes writes that "Algorithms of Oppression" "demonstrate[s] that search engines, and in particular Google, are not simply imperfect machines, but systems designed by humans in ways that replicate the power structures of the western countries where they are built, complete with all the sexism and racism that are built into those structures." In "Booklist," reviewer Lesley Williams states, "Noble’s study should prompt some soul-searching about our reliance on commercial search engines and about digital social equity."

In early February 2018, "Algorithms of Oppression" received press attention when the official Twitter account for the Institute of Electrical and Electronics Engineers expressed criticism of the book, citing that the thesis of the text, based on the text of the book's official blurb on commercial sites, could not be reproduced. IEEE's outreach historian, Alexander Magoun, later revealed that he had not read the book, and issued an apology.




</doc>
<doc id="23632960" url="https://en.wikipedia.org/wiki?curid=23632960" title="Sardinas–Patterson algorithm">
Sardinas–Patterson algorithm

In coding theory, the Sardinas–Patterson algorithm is a classical algorithm for determining in polynomial time whether a given variable-length code is uniquely decodable, named after August Albert Sardinas and George W. Patterson, who published it in 1953. The algorithm carries out a systematic search for a string which admits two different decompositions into codewords. As Knuth reports, the algorithm was rediscovered about ten years later in 1963 by Floyd, despite the fact that it was at the time already well known in coding theory.

Consider the code formula_1. This code, which is based on an example by Berstel, is an example of a code which is not uniquely decodable, since the string

can be interpreted as the sequence of codewords

but also as the sequence of codewords

Two possible decodings of this encoded string are thus given by "cdb" and "babe".

In general, a codeword can be found by the following idea: In the first round, we choose two codewords formula_2 and formula_3 such that formula_2 is a prefix of formula_3, that is,
formula_6 for some "dangling suffix" formula_7. If one tries first formula_8 and formula_9, the dangling suffix is formula_10. If we manage to find two sequences formula_11 and formula_12 of codewords such that
formula_13, then we are finished: For then the string formula_14 can alternatively be decomposed as formula_15, and we have found the desired string having at least two different decompositions into codewords.

In the second round, we try out two different approaches: the first trial is to look for a codeword that has "w" as prefix. Then we obtain a new dangling suffix "w"', with which we can continue our search. If we eventually encounter a dangling suffix that is itself a codeword (or the empty word), then the search will terminate, as we know there exists a string with two decompositions. The second trial is to seek for a codeword that is itself a prefix of "w". In our example, we have formula_10, and the sequence "1" is a codeword. We can thus also continue with "w'=0" as the new dangling suffix.

The algorithm is described most conveniently using quotients of formal languages. In general, for two sets of strings "D" and "N", the (left) quotient formula_17 is defined as the residual words obtained from "D" by removing some prefix in "N". Formally, formula_18. Now let formula_19 denote the (finite) set of codewords in the given code.

The algorithm proceeds in rounds, where we maintain in each round not only one dangling suffix as described above, but the (finite) set of all potential dangling suffixes. Starting with round formula_20, the set of potential dangling suffixes will be denoted by formula_21. The sets formula_21 are defined inductively as follows:

formula_23. Here, the symbol formula_24 denotes the empty word.

formula_25, for all formula_26.

The algorithm computes the sets formula_21 in increasing order of formula_28. As soon as one of the formula_21 contains a word from "C" or the empty word, then the algorithm terminates and answers that the given code is not uniquely decodable. Otherwise, once a set formula_21
equals a previously encountered set formula_31 with <math>j, then the algorithm would enter in principle an endless loop. Instead of continuing endlessly, it answers that the given code is uniquely decodable.

Since all sets formula_21 are sets of suffixes of a finite set of codewords, there are only finitely many different candidates for formula_21. Since visiting one of the sets for the second time will cause the algorithm to stop, the algorithm cannot continue endlessly and thus must always terminate. More precisely, the total number of dangling suffixes that the algorithm considers is at most equal to the total of the lengths of the codewords in the input, so the algorithm runs in polynomial time as a function of this input length. By using a suffix tree to speed the comparison between each dangling suffix and the codewords, the time for the algorithm can be bounded by O("nk"), where "n" is the total length of the codewords and "k" is the number of codewords. The algorithm can be implemented using a pattern matching machine. The algorithm can also be implemented to run on a nondeterministic Turing machine that uses only logarithmic space; the problem of testing unique decipherability is NL-complete, so this space bound is optimal.

A proof that the algorithm is correct, i.e. that it always gives the correct answer, is found in the textbooks by Salomaa and by Berstel et al.






</doc>
<doc id="55213052" url="https://en.wikipedia.org/wiki?curid=55213052" title="Hub labels">
Hub labels

In computer science, hub labels or the hub-labelling algorithm is a method that consumes much fewer resources than the lookup table but is still extremely fast for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.

This method allows at the most with two SELECT statements and the analysis of two strings to compute the shortest path between two vertices of a graph.
For a graph that is oriented like a road graph, this technique requires the prior computation of two tables from structures constructed using the method of the contraction hierarchies. 
In the end, these two computed tables will have as many rows as nodes present within the graph. For each row (each node), a label will be calculated.

A label is a string containing the distance information between the current node (the node of the row) and all the other nodes that can be reached with an ascending search on the relative multi-level structure. The advantage of these distances is that they all represent the shortest paths. 

So, for future queries, the search of a shortest path will start from the source on the first table and the destination on the second table, from which it will be search within the labels for the common nodes with the associated distance information. Only the smallest sum of distances will be kept as the shortest path result.


</doc>
<doc id="54117020" url="https://en.wikipedia.org/wiki?curid=54117020" title="Unrestricted algorithm">
Unrestricted algorithm

An unrestricted algorithm is an algorithm for the computation of a mathematical function that puts no restrictions on the range of the argument or on the precision that may be demanded in the result. The idea of such an algorithm was put forward by C. W. Clenshaw and F. W. J. Olver in a paper published in 1980.

In the problem of developing algorithms for computing, as regards the values of a real-valued function of a real variable (e.g., "g"["x"] in "restricted" algorithms), the error that can be tolerated in the result is specified in advance. An interval on the real line would also be specified for values when the values of a function are to be evaluated. Different algorithms may have to be applied for evaluating functions outside the interval. An unrestricted algorithm envisages a situation in which a user may stipulate the value of "x" and also the precision required in "g"("x") quite arbitrarily. The algorithm should then produce an acceptable result without failure.


</doc>
<doc id="632487" url="https://en.wikipedia.org/wiki?curid=632487" title="List of algorithm general topics">
List of algorithm general topics

This is a list of algorithm general topics. 




</doc>
<doc id="18568" url="https://en.wikipedia.org/wiki?curid=18568" title="List of algorithms">
List of algorithms

The following is a list of algorithms along with one-line descriptions for each.
































































</doc>
<doc id="57373227" url="https://en.wikipedia.org/wiki?curid=57373227" title="Krauss wildcard-matching algorithm">
Krauss wildcard-matching algorithm

In computer science, the Krauss wildcard-matching algorithm is a pattern matching algorithm. Based on the wildcard syntax in common use, e.g. in the Microsoft Windows command-line interface, the algorithm provides a non-recursive mechanism for matching patterns in software applications, based on syntax simpler than that typically offered by regular expressions.

The algorithm is based on a history of development, correctness and performance testing, and programmer feedback that began with an unsuccessful search for a reliable non-recursive algorithm for matching wildcards. An initial algorithm, implemented in a single while loop, quickly prompted comments from software developers, leading to improvements. Ongoing comments and suggestions culminated in a revised algorithm still implemented in a single while loop but refined based on a collection of test cases and a performance profiler. The experience tuning the single while loop using the profiler prompted development of a two-loop strategy that achieved further performance gains, particularly in situations involving empty input strings or input containing no wildcard characters. The two-loop algorithm is available for use by the open-source software development community, under the terms of the Apache License v. 2.0, and is accompanied by test case code.

The algorithm made available under the Apache license is implemented in both pointer-based C++ and portable C++ (implemented without pointers). The test case code, also available under the Apache license, can be applied to any algorithm that provides the pattern matching operations below. The implementation as coded is unable to handle multibyte character sets and poses problems when the text being searched may contain multiple incompatible character sets.

The algorithm supports three pattern matching operations:


The original algorithm has been ported to the DataFlex programming language by Larry Heiges for use with Data Access Worldwide code library. It has been posted on GitHub in modified form as part of a log file reader. The 2014 algorithm is part of the Unreal Model Viewer built into the Epic Games Unreal Engine game engine.



</doc>
<doc id="57504451" url="https://en.wikipedia.org/wiki?curid=57504451" title="Algorithms and Combinatorics">
Algorithms and Combinatorics

Algorithms and Combinatorics () is a book series in mathematics, and particularly in combinatorics and the design and analysis of algorithms. It is published by Springer Science+Business Media, and was founded in 1987.

, the books published in this series include:


</doc>
<doc id="57506816" url="https://en.wikipedia.org/wiki?curid=57506816" title="Hall circles">
Hall circles

Hall circles (also known as M-circles and N-circles) are a graphical tool in control theory used to obtain values of a closed-loop transfer function from the Nyquist plot (or the Nichols plot) of the associated open-loop transfer function. Hall circles have been introduced in control theory by Albert C. Hall in his thesis.

Consider a closed-loop linear control system with open-loop transfer function given by transfer function formula_1 and with a unit gain in the feedback loop. The closed-loop transfer function is given by formula_2. 

To check the stability of "T"("s"), it is possible to use the Nyquist stability criterion with the Nyquist plot of the open-loop transfer function "G"("s"). Note, however, that only the Nyquist plot of "G"("s") does not give the actual values of "T"("s"). To get this information from the G(s)-plane, Hall proposed to construct the locus of points in the "G"("s")-plane such that "T"("s") has constant magnitude and the also the locus of points in the "G"("s")-plane such that "T"("s") has constant phase angle.

Given a positive real value "M" representing a fixed magnitude, and denoting G(s) by "z", the points satisfying formula_3are given by the points "z" in the "G"("s")-plane such that the ratio of the distance between "z" and 0 and the distance between "z" and -1 is equal to "M". The points "z" satisfying this locus condition are circles of Apollonius, and this locus is known in the context of control systems as "M-circles".

Given a positive real value "N" representing a phase angle, the points satisfying formula_4are given by the points z in the "G"("s")-plane such that the angle between -1 and z and the angle between 0 and z is constant. In other words, the angle opposed to the line segment between -1 and 0 must be constant. This implies that the points z satisfying this locus condition are arcs of circles, and this locus is known in the context of control systems as "N-circles".

To use the Hall circles, a plot of M and N circles is done over the Nyquist plot of the open-loop transfer function. The points of the intersection between these graphics give the corresponding value of the closed-loop transfer function.

Hall circles are also used with the Nichols plot and in this setting, are also known as Nichols chart. Rather than overlaying directly the Hall circles over the Nichols plot, the points of the circles are transferred to a new coordinate system where the ordinate is given by formula_5 and the abscissa is given by formula_6. The advantage of using Nichols chart is that adjusting the gain of the open loop transfer function directly reflects in up and down translation of the Nichols plot in the chart.




</doc>
<doc id="52773150" url="https://en.wikipedia.org/wiki?curid=52773150" title="Algorithmic transparency">
Algorithmic transparency

Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services, the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.

The phrases "algorithmic transparency" and "algorithmic accountability" are sometimes used interchangeably – especially since they were coined by the same people – but they have subtly different meanings. Specifically, "algorithmic transparency" states that the inputs to the algorithm and the algorithm's use itself must be known, but they need not be fair. "Algorithmic accountability" implies that the organizations that use algorithms must be accountable for the decisions made by those algorithms, even though the decisions are being made by a machine, and not by a human being.

Current research around algorithmic transparency interested in both societal effects of accessing remote services running algorithms., as well as mathematical and computer science approaches that can be used to achieve algorithmic transparency In the United States, the Federal Trade Commission's Bureau of Consumer Protection studies how algorithms are used by consumers by conducting its own research on algorithmic transparency and by funding external research. In the European Union, the data protection laws that came into effect in May 2018 include a "right to explanation" of decisions made by algorithms, though it is unclear what this means.



</doc>
<doc id="58006810" url="https://en.wikipedia.org/wiki?curid=58006810" title="Abramov's algorithm">
Abramov's algorithm

In mathematics, particularly in computer algebra, Abramov's algorithm computes all rational solutions of a linear recurrence equation with polynomial coefficients. The algorithm was published by Sergei A. Abramov in 1989. 

The main concept in Abramov's algorithm is a universal denominator. Let formula_1 be a field of characteristic zero. The "dispersion" formula_2 of two polynomials formula_3 is defined asformula_4where formula_5 denotes the set of non-negative integers. Therefore the dispersion is the maximum formula_6 such that the polynomial formula_7 and the formula_8-times shifted polynomial formula_9 have a common factor. It is formula_10 if such a formula_8 does not exist. The dispersion can be computed as the largest non-negative integer root of the resultant formula_12. Let formula_13 be a recurrence equation of order formula_14 with polynomial coefficients formula_15, polynomial right-hand side formula_16 and rational sequence solution formula_17. It is possible to write formula_18 for two relatively prime polynomials formula_3. Let formula_20 andformula_21where formula_22 denotes the falling factorial of a function. Then formula_23 divides formula_24. So the polynomial formula_24 can be used as a denominator for all rational solutions formula_26 and hence it is called a universal denominator.

Let again formula_13 be a recurrence equation with polynomial coefficients and formula_24 a universal denominator. After substituting formula_29 for an unknown polynomial formula_30 and setting formula_31 the recurrence equation is equivalent toformula_32As the formula_33 cancel this is a linear recurrence equation with polynomial coefficients which can be solved for an unknown polynomial solution formula_34. There are algorithms to find polynomial solutions. The solutions for formula_34 can then be used again to compute the rational solutions formula_36. 

The homogeneous recurrence equation of order formula_46formula_47over formula_48 has a rational solution. It can be computed by considering the dispersionformula_49This yields the following universal denominator:formula_50andformula_51Multiplying the original recurrence equation with formula_52 and substituting formula_53 leads toformula_54This equation has the polynomial solution formula_55 for an arbitrary constant formula_56. Using formula_57 the general rational solution isformula_58for arbitrary formula_56.


</doc>
<doc id="58462412" url="https://en.wikipedia.org/wiki?curid=58462412" title="Join-based tree algorithms">
Join-based tree algorithms

In computer science, join-based tree algorithms are a class of algorithms for self-balancing binary search trees.
The algorithmic framework is based on a single operation "join". Under this framework, the "join" operation captures all balancing criteria of different balancing schemes, and all other functions "join" have generic implementation across different balancing schemes. The "join-based algorithms" can be applied to at least four balancing schemes: AVL trees, red-black trees, weight-balanced trees and treaps.

The "join"formula_1 operation takes as input two binary balanced trees formula_2 and formula_3 of the same balancing scheme, and a key formula_4, and outputs a new balanced binary tree formula_5 whose in-order traversal is the in-order traversal of formula_2, then formula_4 then the in-order traversal of formula_3. In particular, if the trees are search trees, which means that the in-order of the trees maintain a total ordering on keys, it must satisfy the condition that all keys in formula_2 are smaller than formula_4 and all keys in formula_3 are greater than formula_4.

The "join" operation was first defined by Tarjan on red-black trees, which runs in worst-case logarithmic time. Later Sleator and Tarjan described a "join" algorithm for splay trees which runs in amortized logarithmic time. Later Adams extended "join" to weight-balanced trees and used it for fast set-set functions including union, intersection and set difference. In 1998, Blelloch and Reid-Miller extended "join" on treaps, and proved the bound of the set functions to be formula_13 for two trees of size formula_14 and formula_15, which is optimal in the comparison model. They also brought up parallelism in Adams' algorithm by using a divide-and-conquer scheme. In 2016, Blelloch et al. formally proposed the join-based algorithms, and formalized the "join" algorithm for four different balancing schemes: AVL trees, red-black trees, weight-balanced trees and treaps. In the same work they proved that Adams' algorithms on union, intersection and difference are work-optimal on all the four balancing schemes.

The function "join"formula_16 considers rebalancing the tree, and thus depends on the input balancing scheme. If the two trees are balanced, "join" simply creates a new node with left subtree , root and right subtree . Suppose that is heavier (this "heavier" depends on the balancing scheme) than (the other case is symmetric). "Join" follows the right spine of until a node which is balanced with . At this point a new node with left child , root and right child is created to replace c. The new node may invalidate the balancing invariant. This can be fixed with rotations.

The following is the "join" algorithms on different balancing schemes.

The "join" algorithm for AVL trees:

Here formula_17 of a node formula_18 the height of formula_18. expose(v)=(l,k,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4, and the right child formula_23. Node(l,k,r) means to create a node of left child formula_21, key formula_4, and right child formula_23.

The "join" algorithm for red-black trees:

Here formula_27 of a node formula_18 means twice the black height of a black node, and the twice the black height of a red node. expose(v)=(l,⟨k,c⟩,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4, the color of the node formula_32 and the right child formula_23. Node(l,⟨k,c⟩,r) means to create a node of left child formula_21, key formula_4, color formula_32 and right child formula_23.

The "join" algorithm for weight-balanced trees:

Here balanceformula_38 means two weights formula_39 and formula_40 are balanced. expose(v)=(l,k,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4 and the right child formula_23. Node(l,k,r) means to create a node of left child formula_21, key formula_4 and right child formula_23.

In the following, expose(v)=(l,k,r) means to extract a tree node formula_18's left child formula_21, the key of the node formula_4 and the right child formula_23. Node(l,k,r) means to create a node of left child formula_21, key formula_4 and right child formula_23. right(formula_18) and left(formula_18) extracts the right child and the left child of a tree nodeformula_18, respectively. formula_58 extract the key of a node formula_18. "formula_60" means that two statements formula_61 and formula_62 can run in parallel.

To split a tree into two trees, those smaller than key "x", and those larger than key "x", we first draw a path from the root by inserting "x" into the tree. After this insertion, all values less than "x" will be found on the left of the path, and all values greater than "x" will be found on the right. By applying "Join", all the subtrees on the left side are merged bottom-up using keys on the path as intermediate nodes from bottom to top to form the left tree, and the right part is asymmetric. For some applications, "Split" also returns a boolean value denoting if "x" appears in the tree. The cost of "Split" is formula_63, order of the height of the tree. 

The split algorithm is as follows:

This function is defined similarly as "join" but without the middle key. It first splits out the last key formula_4 of the left tree, and then join the rest part of the left tree with the right tree with formula_4.
The algorithm is as follows:

The cost is formula_63 for a tree of size formula_67.

The insertion and deletion algorithms, when making use of "join" can be independent of balancing schemes. For an insertion, the algorithm compares the key to be inserted with the key in the root, inserts it to the left/right subtree if the key is smaller/greater than the key in the root, and joins the two subtrees back with the root. A deletion compares the key to be deleted with the key in the root. If they are equal, return join2 on the two subtrees. Otherwise, delete the key from the corresponding subtree, and join the two subtrees back with the root.
The algorithms are as follows:

Both insertion and deletion requires formula_63 time if formula_69.

Several set operations have been defined on weight-balanced trees: union, intersection and set difference. The union of two weight-balanced trees and representing sets and , is a tree that represents . The following recursive function computes this union:

Similarly, the algorithms of intersection and set-difference are as follows:

The complexity of each of union, intersection and difference is formula_70 for two weight-balanced trees of sizes formula_14 and formula_15. This complexity is optimal in terms of the number of comparisons. More importantly, since the recursive calls to union, intersection or difference are independent of each other, they can be executed in parallel with a parallel depth formula_73. When formula_74, the join-based implementation applies the same computation as in a single-element insertion or deletion if the root of the larger tree is used to split the smaller tree.

The algorithm for building a tree can make use of the union algorithm, and use the divide-and-conquer scheme:

This algorithm costs formula_75 work and has formula_76 depth. A more-efficient algorithm makes use of a parallel sorting algorithm.

This algorithm costs formula_75 work and has formula_63 depth assuming the sorting algorithm has formula_75 work and formula_63 depth.

This function selects all entries in a tree satisfying an indicator formula_81, and return a tree containing all selected entries. It recursively filters the two subtrees, and join them with the root if the root satisfies formula_81, otherwise "join2" the two subtrees.

This algorithm costs work formula_83 and depth formula_63 on a tree of size formula_67, assuming formula_81 has constant cost.

The join-based algorithms are applied to support interface for sets, maps, and augmented maps in libarays such as Hackage, SML/NJ, and PAM.



</doc>
<doc id="73415" url="https://en.wikipedia.org/wiki?curid=73415" title="Sieve of Eratosthenes">
Sieve of Eratosthenes

In mathematics, the sieve of Eratosthenes is a simple and ingenious ancient algorithm for finding all prime numbers up to any given limit.

It does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the first prime number, . The multiples of a given prime are generated as a sequence of numbers starting from that prime, with constant difference between them that is equal to that prime. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.

The earliest known reference to the sieve (, "kóskinon Eratosthénous") is in Nicomachus of Gerasa's "Introduction to Arithmetic", which describes it and attributes it to Eratosthenes of Cyrene, a Greek mathematician.

One of a number of prime number sieves, it is one of the most efficient ways to find all of the smaller primes. It may be used to find primes in arithmetic progressions.

A prime number is a natural number that has exactly two distinct natural number divisors: the number 1 and itself.

To find all the prime numbers less than or equal to a given integer by Eratosthenes' method:


The main idea here is that every value given to will be prime, because if it were composite it would be marked as a multiple of some other, smaller prime. Note that some of the numbers may be marked more than once (e.g., 15 will be marked both for 3 and 5).

As a refinement, it is sufficient to mark the numbers in step 3 starting from , as all the smaller multiples of will have already been marked at that point. This means that the algorithm is allowed to terminate in step 4 when is greater than . 

Another refinement is to initially list odd numbers only, , and count in increments of from in step 3, thus marking only odd multiples of . This actually appears in the original algorithm. This can be generalized with wheel factorization, forming the initial list only from numbers coprime with the first few primes and not just from odds (i.e., numbers coprime with 2), and counting in the correspondingly adjusted increments so that only such multiples of are generated that are coprime with those small primes, in the first place.

To find all the prime numbers less than or equal to 30, proceed as follows.

First, generate a list of integers from 2 to 30:

The first number in the list is 2; cross out every 2nd number in the list after 2 by counting up from 2 in increments of 2 (these will be all the multiples of 2 in the list):

The next number in the list after 2 is 3; cross out every 3rd number in the list after 3 by counting up from 3 in increments of 3 (these will be all the multiples of 3 in the list):

The next number not yet crossed out in the list after 3 is 5; cross out every 5th number in the list after 5 by counting up from 5 in increments of 5 (i.e. all the multiples of 5):

The next number not yet crossed out in the list after 5 is 7; the next step would be to cross out every 7th number in the list after 7, but they are all already crossed out at this point, as these numbers (14, 21, 28) are also multiples of smaller primes because 7 × 7 is greater than 30. The numbers not crossed out at this point in the list are all the prime numbers below 30:

The sieve of Eratosthenes can be expressed in pseudocode, as follows:

This algorithm produces all primes not greater than . It includes a common optimization, which is to start enumerating the multiples of each prime from . The time complexity of this algorithm is , provided the array update is an operation, as is usually the case.

As Sorenson notes, the problem with the sieve of Eratosthenes is not the number of operations it performs but rather its memory requirements. For large , the range of primes may not fit in memory; worse, even for moderate , its cache use is highly suboptimal. The algorithm walks through the entire array , exhibiting almost no locality of reference.

A solution to these problems is offered by "segmented" sieves, where only portions of the range are sieved at a time. These have been known since the 1970s, and work as follows:


If is chosen to be , the space complexity of the algorithm is , while the time complexity is the same as that of the regular sieve.

For ranges with upper limit so large that the sieving primes below as required by the page segmented sieve of Eratosthenes cannot fit in memory, a slower but much more space-efficient sieve like the sieve of Sorenson can be used instead.

An incremental formulation of the sieve generates primes indefinitely (i.e., without an upper bound) by interleaving the generation of primes with the generation of their multiples (so that primes can be found in gaps between the multiples), where the multiples of each prime are generated directly by counting up from the square of the prime in increments of (or for odd primes). The generation must be initiated only when the prime's square is reached, to avoid adverse effects on efficiency. It can be expressed symbolically under the dataflow paradigm as

Primes can also be produced by iteratively sieving out the composites through [[Trial division|divisibility testing]] by sequential primes, one prime at a time. It is not the sieve of Eratosthenes but is often confused with it, even though the sieve of Eratosthenes directly generates the composites instead of testing for them. Trial division has worse theoretical [[Analysis of algorithms|complexity]] than that of the sieve of Eratosthenes in generating ranges of primes.

When testing each prime, the "optimal" trial division algorithm uses all prime numbers not exceeding its square root, whereas the sieve of Eratosthenes produces each composite from its prime factors only, and gets the primes "for free", between the composites. The widely known 1975 [[functional programming|functional]] sieve code by [[David Turner (computer scientist)|David Turner]] is often presented as an example of the sieve of Eratosthenes but is actually a sub-optimal trial division sieve.

The work performed by this algorithm is almost entirely the operations to cull the composite number representations which for the basic non-optimized version is the sum of the range divided by each of the primes up to that range or
where is the sieving range in this and all further analysis.

By rearranging Mertens' second theorem, this is equal to as approaches infinity, where M is the Meissel–Mertens constant of about ...

The optimization of starting at the square of each prime and only culling for primes less than the square root changes the "" in the above expression to (or ) and not culling until the square means that the sum of the base primes each minus two is subtracted from the operations. As the sum of the first primes is and the prime number theorem says that is approximately , then the sum of primes to is , and therefore the sum of base primes to is expressed as a factor of . The extra offset of two per base prime is , where is the prime-counting function in this case, or ; expressing this as a factor of as are the other terms, this is . Combining all of this, the expression for the number of optimized operations without wheel factorization is

For the wheel factorization cases, there is a further offset of the operations not done of
where is the highest wheel prime and a constant factor of the whole expression is applied which is the fraction of remaining prime candidates as compared to the repeating wheel circumference. The wheel circumference is
and it can easily be determined that this wheel factor is
as is the fraction of remaining candidates for the highest wheel prime, , and each succeeding smaller prime leaves its corresponding fraction of the previous combined fraction.

Combining all of the above analysis, the total number of operations for a sieving range up to including wheel factorization for primes up to is approximately

To show that the above expression is a good approximation to the number of composite number cull operations performed by the algorithm, following is a table showing the actually measured number of operations for a practical implementation of the sieve of Eratosthenes as compared to the number of operations predicted from the above expression with both expressed as a fraction of the range (rounded to four decimal places) for different sieve ranges and wheel factorizations (Note that the last column is a maximum practical wheel as to the size of the wheel gaps Look Up Table - almost 10 million values):

The above table shows that the above expression is a very good approximation to the total number of culling operations for sieve ranges of about a hundred thousand (10) and above.

The sieve of Eratosthenes is a popular way to benchmark computer performance. As can be seen from the above by removing all constant offsets and constant factors and ignoring terms that tend to zero as n approaches infinity, the time complexity of calculating all primes below in the random access machine model is operations, a direct consequence of the fact that the prime harmonic series asymptotically approaches . It has an exponential time complexity with regard to input size, though, which makes it a pseudo-polynomial algorithm. The basic algorithm requires of memory.

The bit complexity of the algorithm is bit operations with a memory requirement of .

The normally implemented page segmented version has the same operational complexity of as the non-segmented version but reduces the space requirements to the very minimal size of the segment page plus the memory required to store the base primes less than the square root of the range used to cull composites from successive page segments of size .

A special rarely if ever implemented segmented version of the sieve of Eratosthenes, with basic optimizations, uses operations and bits of memory.

To show that the above approximation in complexity is not very accurate even for about as large as practical a range, the following is a table of the estimated number of operations as a fraction of the range rounded to four places, the calculated ratio for a factor of ten change in range based on this estimate, and the factor based on the estimate for various ranges and wheel factorizations (the combo column uses a frequently practically used pre-cull by the maximum wheel factorization but only the 2/3/5/7 wheel for the wheel factor as the full factorization is difficult to implement efficiently for page segmentation):

The above shows that the estimate is not very accurate even for maximum practical ranges of about 10. One can see why it does not match by looking at the computational analysis above and seeing that within these practical sieving range limits, there are very significant constant offset terms such that the very slowly growing term does not get large enough so as to make these terms insignificant until the sieving range approaches infinity – well beyond any practical sieving range. Within these practical ranges, these significant constant offsets mean that the performance of the Sieve of Eratosthenes is much better than one would expect just using the asymptotic time complexity estimates by a significant amount, but that also means that the slope of the performance with increasing range is steeper than predicted as the benefit of the constant offsets becomes slightly less significant.

One should also note that in using the calculated operation ratios to the sieve range, it must be less than about 0.2587 in order to be faster than the often compared sieve of Atkin if the operations take approximately the same time each in CPU clock cycles, which is a reasonable assumption for the one huge bit array algorithm. Using that assumption, the sieve of Atkin is only faster than the maximally wheel factorized sieve of Eratosthenes for ranges of over 10 at which point the huge sieve buffer array would need about a quarter of a terabyte (about 250 gigabytes) of RAM memory even if bit packing were used. An analysis of the page segmented versions will show that the assumption that the time per operation stays the same between the two algorithms does not hold for page segmentation and that the sieve of Atkin operations get slower much faster than the sieve of Eratosthenes with increasing range. Thus for practical purposes, the maximally wheel factorized Sieve of Eratosthenes is faster than the Sieve of Atkin although the Sieve of Atkin is faster for lesser amounts of wheel factorization.

Using big O notation is also not the correct way to compare practical performance of even variations of the Sieve of Eratosthenes as it ignores constant factors and offsets that may be very significant for practical ranges: The sieve of Eratosthenes variation known as the Pritchard wheel sieve has an performance, but its basic implementation requires either a "one large array" algorithm which limits its usable range to the amount of available memory else it needs to be page segmented to reduce memory use. When implemented with page segmentation in order to save memory, the basic algorithm still requires about bits of memory (much more than the requirement of the basic page segmented sieve of Eratosthenes using bits of memory). Pritchard's work reduced the memory requirement to the limit as described above the table, but the cost is a fairly large constant factor of about three in execution time to about three quarters the sieve range due to the complex computations required to do so. As can be seen from the above table for the basic sieve of Eratosthenes, even though the resulting wheel sieve has performance and an acceptable memory requirement, it will never be faster than a reasonably Wheel Factorized basic sieve of Eratosthenes for any practical sieving range by a factor of about two. Other than that it is quite complex to implement, it is rarely practically implemented because it still uses more memory than the basic Sieve of Eratosthenes implementations described here as well as being slower for practical ranges. It is thus more of an intellectual curiosity than something practical.

Euler's proof of the zeta product formula contains a version of the sieve of Eratosthenes in which each composite number is eliminated exactly once. The same sieve was rediscovered and observed to take linear time by . It, too, starts with a list of numbers from 2 to in order. On each step the first element is identified as the next prime and the results of multiplying this prime with each element of the list are marked in the list for subsequent deletion. The initial element and the marked elements are then removed from the working sequence, and the process is repeated:
Here the example is shown starting from odds, after the first step of the algorithm. Thus, on the th step all the remaining multiples of the th prime are removed from the list, which will thereafter contain only numbers coprime with the first primes (cf. wheel factorization), so that the list will start with the next prime, and all the numbers in it below the square of its first element will be prime too.

Thus, when generating a bounded sequence of primes, when the next identified prime exceeds the square root of the upper limit, all the remaining numbers in the list are prime. In the example given above that is achieved on identifying 11 as next prime, giving a list of all primes less than or equal to 80.

Note that numbers that will be discarded by a step are still used while marking the multiples in that step, e.g., for the multiples of 3 it is , , , , ..., , ..., so care must be taken dealing with this.




</doc>
<doc id="775" url="https://en.wikipedia.org/wiki?curid=775" title="Algorithm">
Algorithm

In mathematics and computer science, an algorithm () is a finite sequence of well-defined, computer-implementable instructions, typically to solve a class of problems or to perform a computation. Algorithms are always unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks.

As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.

The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, was used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC. Greek mathematicians later used algorithms in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as Al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.

The word "algorithm" itself is derived from the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī, Latinized "Algoritmi". A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define "effective calculability" or "effective method". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.

The word 'algorithm' has its roots in Latinizing the name of Persian mathematician Muhammad ibn Musa al-Khwarizmi in the first steps to "algorismus". Al-Khwārizmī (, c. 780–850) was a Persian mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarazm', a region that was part of Greater Iran and is now in Uzbekistan.

About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century under the title "Algoritmi de numero Indorum". This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, "algorismus", English 'algorism', the corruption of his name, simply meant the "decimal number system". In the 15th century, under the influence of the Greek word ἀριθμός 'number' ("cf." 'arithmetic'), the Latin word was altered to "algorithmus", and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.

In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that "algorithm" took on the meaning that it has in modern English.

Another early use of the word is from 1240, in a manual titled "Carmen de Algorismo" composed by Alexandre de Villedieu. It begins with:
which translates to:
The poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.

An informal definition could be "a set of rules that precisely defines a sequence of operations", which would include all computer programs, including programs that do not perform numeric calculations, and (for example) any prescribed bureaucratic procedure.
In general, a program is only an algorithm if it stops eventually.

A prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.

No human being can write fast enough, or long enough, or small enough† ( †"smaller and smaller without limit … you'd be trying to write on molecules, on atoms, on electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give "explicit instructions for determining the nth member of the set", for arbitrary finite "n". Such instructions are to be given quite explicitly, in a form in which "they could be followed by a computing machine", or by a "human who is capable of carrying out only very elementary operations on symbols."

An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an "arbitrary" "input" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as "y = m + n" (i.e., two arbitrary "input variables" "m" and "n" that produce an output "y"), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):

The concept of "algorithm" is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of "algorithm" that suits both concrete (in some sense) and abstract usage of the term.

Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):

Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem.

Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.

For some of these computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).

Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom"—an idea that is described more formally by "flow of control".

So far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of "memory" as a scratchpad. An example of such an assignment can be found below.

For some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.

Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.

There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see Turing machine for more).

Representations of algorithms can be classed into three accepted levels of Turing machine description, as follows:

For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.

Algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.

One of the most important aspects of algorithm design lies in the creation of algorithm that has an efficient run-time, also known as its Big O.

Typical steps in the development of algorithms:

Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.

In computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended "target" computer(s) to produce "output" from given (perhaps null) "input". An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.

""Elegant" (compact) programs, "good" (fast) programs ": The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:

Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant—such a proof would solve the Halting problem (ibid).

"Algorithm versus function computable by an algorithm": For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that "It is ... important to distinguish between the notion of "algorithm", i.e. procedure and the notion of "function computable by algorithm", i.e. mapping yielded by procedure. The same function may have several different algorithms".

Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.

"Computers (and computors), models of computation": A computer (or human "computor") is a restricted type of machine, a "discrete deterministic mechanical device" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable "locations", (ii) discrete, indistinguishable "counters" (iii) an agent, and (iv) a list of instructions that are "effective" relative to the capability of the agent.

Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three "assignment" (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general "types" of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT. However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is a convenience; it can be constructed by initializing a dedicated location to zero e.g. the instruction " Z ← 0 "; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.

"Simulation of an algorithm: computer (computor) language": Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can "effectively" execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.

This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).

But what model should be used for the simulation? Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of "simulation" enters". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky's "decrement").

"Structured programming, canonical structures": Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.

"Canonical flowchart symbols": The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.

One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:

"High-level description:"

"(Quasi-)formal description:"
Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:

Euclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his "Elements". Euclid poses the problem thus: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed of units": a counting number, a positive integer not including zero. To "measure" is to place a shorter measuring length "s" successively ("q" times) along longer length "l" until the remaining portion "r" is less than the shorter length "s". In modern words, remainder "r" = "l" − "q"×"s", "q" being the quotient, or remainder "r" is the "modulus", the integer-fractional part left over after the division.

For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be “proper”; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).

Euclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the "greatest". While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number "1" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.

Only a few instruction "types" are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.

The following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length "s" from the remaining length "r" until "r" is less than "s". The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:

INPUT:

E0: [Ensure "r" ≥ "s".]

E1: [Find remainder]: Until the remaining length "r" in R is less than the shorter length "s" in S, repeatedly subtract the measuring number "s" in S from the remaining length "r" in R.

E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.

E3: [Interchange "s" and "r"]: The nut of Euclid's algorithm. Use remainder "r" to measure what was previously smaller number "s"; L serves as a temporary location.

OUTPUT:

DONE:

 The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction is the assignment instruction symbolized by ←.

"How "Elegant" works": In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co-loops", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S (Difference = Minuend − Subtrahend), the minuend can become "s" (the new measuring length) and the subtrahend can become the new "r" (the length to be measured); in other words the "sense" of the subtraction reverses.

The following version can be used with object-oriented languages:
Does an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.

But "exceptional cases" must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S? Ditto for "Elegant": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if "negative" numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).

"Proof of program correctness by use of mathematical induction": Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's algorithm, and he proposes "a general method applicable to proving the validity of any algorithm". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.

"Elegance (compactness) versus goodness (speed)": With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions. However, "Inelegant" is "faster" (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: "Elegant" does "two" conditional tests in every subtraction loop, whereas "Inelegant" only does one. As the algorithm (usually) requires many loop-throughs, "on average" much time is wasted doing a "B = 0?" test that is needed only after the remainder is computed.

"Can the algorithms be improved?": Once the programmer judges a program "fit" and "effective"—that is, it computes the function intended by its author—then the question becomes, can it be improved?

The compactness of "Inelegant" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.

The speed of "Elegant" can be improved by moving the "B=0?" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.

It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O("n"), using the big O notation with "n" as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of "O(1)", if the space required to store the input numbers is not counted, or O("n") if it is counted.

Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.

The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.

Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.

To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.

There are various ways to classify algorithms, each with its own merits.

One way to classify algorithms is by implementation means.


Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:


For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:


Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.

Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.

Algorithms can be classified by the amount of time they need to complete compared to their input size:

Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.

The adjective "continuous" when applied to the word "algorithm" can mean:

Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.

Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).

The earliest evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm. During the Hammurabi dynasty circa 1800-1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.

Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus circa 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the "Introduction to Arithmetic" by Nicomachus, and the Euclidean algorithm, which was first described in "Euclid's Elements" (c. 300 BC).

Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.

Muhammad ibn Mūsā al-Khwārizmī, a Persian mathematician, wrote the "Al-jabr" in the 9th century. The terms "algorism" and "algorithm" are derived from the name al-Khwārizmī, while the term "algebra" is derived from the book "Al-jabr". In Europe, the word "algorithm" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):

The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in "A Manuscript On Deciphering Cryptographic Messages". He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.

"The clock": Bolter credits the invention of the weight-driven clock as "The key invention [of Europe in the Middle Ages]", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. "The accurate automatic machine" led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.

"Logical machines 1870 – Stanley Jevons' "logical abacus" and "logical machine"": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a "Logical Machine"" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc] ...". With this machine he could analyze a "syllogism or any other simple logical argument".

This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 "Symbolic Logic", turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's "abacus" ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".

"Jacquard loom, Hollerith punch cards, telegraphy and telephony – the electromechanical relay": Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as "dots and dashes" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.

"Telephone-switching networks" of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".

Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" "open" and "closed"):

"Symbols and rules": In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's "The principles of arithmetic, presented by a new method" (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language".

But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic. ... in which we see a " 'formula language', that is a "lingua characterica", a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).

"The paradoxes": At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.

"Effective calculability": In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus a finely honed definition of "general recursion" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine"—in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine". S.C. Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I", and a few years later Kleene's renaming his Thesis "Church's Thesis" and proposing "Turing's Thesis".

Emil Post (1936) described the actions of a "computer" (human being) as follows:

His symbol space would be

Alan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'". Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.

Turing—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.

Turing's reduction yields the following:
"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:

A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:

J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):

Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his "An Unsolvable Problem of Elementary Number Theory" (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper "On Formally Undecidable Propositions of Principia Mathematica and Related Systems I" (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.

Stephen C. Kleene defined as his now-famous "Thesis I" known as the Church–Turing thesis. But he did this in the following context (boldface in original):

A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.




</doc>
<doc id="6770335" url="https://en.wikipedia.org/wiki?curid=6770335" title="Car–Parrinello molecular dynamics">
Car–Parrinello molecular dynamics

Car–Parrinello molecular dynamics or CPMD refers to either a method used in molecular dynamics (also known as the Car–Parrinello method) or the computational chemistry software package used to implement this method.

The CPMD method is related to the more common Born–Oppenheimer molecular dynamics (BOMD) method in that the quantum mechanical effect of the electrons is included in the calculation of energy and forces for the classical motion of the nuclei. However, whereas BOMD treats the electronic structure problem within the time-"independent" Schrödinger equation, CPMD explicitly includes the electrons as active degrees of freedom, via (fictitious) dynamical variables.

The software is a parallelized plane wave / pseudopotential implementation of density functional theory, particularly designed for "ab initio" molecular dynamics.

The Car–Parrinello method is a type of molecular dynamics, usually employing periodic boundary conditions, planewave basis sets, and density functional theory, proposed by Roberto Car and Michele Parrinello in 1985, who were subsequently awarded the Dirac Medal by ICTP in 2009.

In contrast to Born–Oppenheimer molecular dynamics wherein the nuclear (ions) degree of freedom are propagated using ionic forces which are calculated at each iteration by approximately solving the electronic problem with conventional matrix diagonalization methods, the Car–Parrinello method explicitly introduces the electronic degrees of freedom as (fictitious) dynamical variables, writing an extended Lagrangian for the system which leads to a system of coupled equations of motion for both ions and electrons. In this way an explicit electronic minimization at each time step, as done in Born-Oppenheimer MD, is not needed: after an initial standard electronic minimization, the fictitious dynamics of the electrons keeps them on the electronic ground state corresponding to each new ionic configuration visited along the dynamics, thus yielding accurate ionic forces. In order to maintain this adiabaticity condition, it is necessary that the fictitious mass of the electrons is chosen small enough to avoid a significant energy transfer from the ionic to the electronic degrees of freedom. This small fictitious mass in turn requires that the equations of motion are integrated using a smaller time step than the one (1–10 fs) commonly used in Born–Oppenheimer molecular dynamics.

In CPMD the core electrons are usually described by a pseudopotential and the wavefunction of the valence electrons are approximated by a plane wave basis set.

The ground state electronic density (for fixed nuclei) is calculated self-consistently, usually using the density functional theory method. Then, using that density, forces on the nuclei can be computed, to update the trajectories (using, e.g. the Verlet integration algorithm). In addition, however, the coefficients used to obtain the electronic orbital functions can be treated as a set of extra spatial dimensions, and trajectories for the orbitals can be calculated in this context.

CPMD is an approximation of the Born–Oppenheimer MD (BOMD) method. In BOMD, the electrons' wave function must be minimized via matrix diagonalization at every step in the trajectory. CPMD uses fictitious dynamics to keep the electrons close to the ground state, preventing the need for a costly self-consistent iterative minimization at each time step. The fictitious dynamics relies on the use of a fictitious electron mass (usually in the range of 400 – 800 a.u.) to ensure that there is very little energy transfer from nuclei to electrons, i.e. to ensure adiabaticity. Any increase in the fictitious electron mass resulting in energy transfer would cause the system to leave the ground-state BOMD surface.

where "E"[{"ψ"},{R}] is the Kohn–Sham energy density functional, which outputs energy values when given Kohn–Sham orbitals and nuclear positions.

where "δ" is the Kronecker delta.

The equations of motion are obtained by finding the stationary point of the Lagrangian under variations of "ψ" and R, with the orthogonality constraint.

where Λ is a Lagrangian multiplier matrix to comply with the orthonormality constraint.

In the formal limit where "μ" → 0, the equations of motion approach Born–Oppenheimer molecular dynamics.





</doc>
<doc id="59538271" url="https://en.wikipedia.org/wiki?curid=59538271" title="Newest vertex bisection">
Newest vertex bisection

Newest Vertex Bisection is an algorithmic method to locally refine triangulations. It is widely used in computational science, numerical simulation, and computer graphics. The advantage of newest vertex bisection is that it allows local refinement of triangulations without degenerating the shape of the triangles after repeated usage.

In newest vertex bisection, whenever a triangle is to be split into smaller triangles, it will be bisected by drawing a line from the newest vertex to the midpoint of the edge opposite to that vertex. That midpoint becomes the newest vertex of the two newer triangles. One can show that repeating this procedure for a given triangulation leads to triangles that belong to only a finite number of similarity classes.

Generalizations of newest vertex bisection to dimension three and higher are known. Newest vertex bisection is used in local mesh refinement for adaptive finite element methods, where it is an alternative to red-green refinement and uniform mesh refinement.


</doc>
<doc id="59892172" url="https://en.wikipedia.org/wiki?curid=59892172" title="Neural Style Transfer">
Neural Style Transfer

Neural Style Transfer (NST) refers to a class of software algorithms that manipulate digital images, or videos, to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks in order to perform the image transformation. Common uses for NST are the creation of artificial artwork from photographs, for example by transferring the appearance of famous paintings to user supplied photographs. Several notable mobile apps use NST techniques for this purpose, including DeepArt and Prisma.

NST is an example of image stylization, a problem studied for over two decades within the field of non-photorealistic rendering. Prior to NST, the transfer of image style was performed using machine learning techniques based on image analogy. Given a training pair of images–a photo and an artwork depicting that photo–a transformation could be learned and then applied to create a new artwork from a new photo, by analogy. The drawback of this method is that such a training pair rarely exists in practice. For example, original source material (photos) are rarely available for famous artworks.

NST requires no such pairing; only a single example of artwork is needed for the algorithm to transfer its style.

NST was first published in the paper "A Neural Algorithm of Artistic Style" by Gatys et al., originally released to ArXiv 2015, and subsequently accepted by the peer-reviewed Computer Vision and Pattern Recognition (CVPR) in 2016.

The core innovation of NST is the use of deep learning to disentangle the representation of the content (structure) of an image, from the appearance (style) in which it is depicted. The original paper used a convolutional neural network (CNN) VGG-19 architecture that has been pre-trained to perform object recognition using the ImageNet dataset.

The process of NST assumes an input image formula_1 and an example style image formula_2.

The image formula_1 is fed through the CNN, and network activations are sampled at a late convolution layer of the VGG-19 architecture. Let formula_4 be the resulting output sample, called the 'content' of the input formula_1.

The style image formula_2 is then fed through the same CNN, and network activations are sampled at the early to middle layers of the CNN. These activations are encoded into a Gramian matrix representation, call it formula_7 to denote the 'style' of formula_2.

The goal of NST is to synthesize an output image formula_9 that exhibits the content of formula_1 applied with the style of formula_2, i.e. formula_12 and formula_13.

An iterative optimization (usually gradient descent) then gradually updates formula_9 to minimize the loss function error:

formula_15,

where formula_16 is the L2 distance. The constant formula_17 controls the level of the stylization effect.

Image formula_9 is initially approximated by adding a small amount of white noise to input image formula_1 and feeding it through the CNN. Then we successively backpropagate this loss through the network with the CNN weights fixed in order to update the pixels of formula_9. After several thousand epochs of training, an formula_9 (hopefully) emerges that matches the style of formula_2 and the content of formula_1.

Algorithms are typically implemented for GPUs, so that training takes a few minutes.

NST has also been extended to videos.

Subsequent work improved the speed of NST for images.

In a paper by Fei-Fei Li et al. adopted a different regularized loss metric and accelerated method for training to produce results in real time (three times faster than Gatys). Their idea was to use not the "pixel-based loss" defined above but rather a 'perceptual loss' measuring the differences between higher level layers within the CNN. They used a symmetric encoder-decoder CNN. Training uses a similar loss function to the basic NST method but also regularizes the output for smoothness using a total variation (TV) loss. Once trained, the network may be used to transform an image into the style used during training, using a single feed-forward pass of the network. However the network is restricted to the single style in which it has been trained.

In a work by Chen Dongdong et al. they explored the fusion of optical flow information into feedforward networks in order to improve the temporal coherence of the output.

Most recently, feature transform based NST methods have been explored for fast stylization that are not coupled to single specific style and enable user-controllable "blending" of styles, for example the Whitening and Coloring Transform (WCT).


</doc>
<doc id="59730114" url="https://en.wikipedia.org/wiki?curid=59730114" title="Parallel external memory">
Parallel external memory

In computer science, a parallel external memory (PEM) model is a cache-aware, external-memory abstract machine. It is the parallel-computing analogy to the single-processor external memory (EM) model. In a similar way, it is the cache-aware analogy to the parallel random-access machine (PRAM). The PEM model consists of a number of processors, together with their respective private caches and a shared main memory.

The PEM model is a combination of the EM model and the PRAM model. The PEM model is a computation model which consists of formula_1 processors and a two-level memory hierarchy. This memory hierarchy consists of a large external memory (main memory) of size formula_2 and formula_1 small internal memories (caches). The processors share the main memory. Each cache is exclusive to a single processor. A processor can't access another’s cache. The caches have a size formula_4 which is partitioned in blocks of size formula_5. The processors can only perform operations on data which are in their cache. The data can be transferred between the main memory and the cache in blocks of size formula_5.

The complexity measure of the PEM model is the I/O complexity, which determines the number of parallel blocks transfers between the main memory and the cache. During a parallel block transfer each processor can transfer a block. So if formula_1 processors load parallelly a data block of size formula_5 form the main memory into their caches, it is considered as an I/O complexity of formula_9 not formula_10. A program in the PEM model should minimize the data transfer between main memory and caches and operate as much as possible on the data in the caches.

In the PEM model, there is no direct communication network between the P processors. The processors have to communicate indirectly over the main memory. If multiple processors try to access the same block in main memory concurrently read/write conflicts occur. Like in the PRAM model, three different variations of this problem are considered:
The following two algorithms solve the CREW and EREW problem if formula_11 processors write to the same block simultaneously.
A first approach is to serialize the write operations. Only one processor after the other writes to the block. This results in a total of formula_1 parallel block transfers. A second approach needs formula_13 parallel block transfers and an additional block for each processor. The main idea is to schedule the write operations in a binary tree fashion and gradually combine the data into a single block. In the first round formula_1 processors combine their blocks into formula_15 blocks. Then formula_15 processors combine the formula_15 blocks into formula_18. This procedure is continued until all the data is combined in one block.

Let formula_19 be a vector of d-1 pivots sorted in increasing order. Let formula_20 be an unordered set of N elements. A d-way partition of formula_20 is a set formula_22 , where formula_23 and formula_24 for formula_25. formula_26 is called the i-th bucket. The number of elements in formula_26 is greater than formula_28 and smaller than formula_29. In the following algorithm the input is partitioned into N/P-sized contiguous segments formula_30 in main memory. The processor i primarily works on the segment formula_31. The multiway partitioning algorithm (codice_1) uses a PEM prefix sum algorithm to calculate the prefix sum with the optimal formula_32 I/O complexity. This algorithm simulates an optimal PRAM prefix sum algorithm.

If the vector of formula_43 pivots M and the input set A are located in contiguous memory, then the d-way partitioning problem can be solved in the PEM model with formula_44 I/O complexity. The content of the final buckets have to be located in contiguous memory.

The selection problem is about finding the k-th smallest item in an unordered list formula_20 of size formula_2.
The following code makes use of codice_2 which is a PRAM optimal sorting algorithm which runs in formula_47, and codice_3, which is a cache optimal single-processor selection algorithm.

Under the assumption that the input is stored in contiguous memory, codice_4 has an I/O complexity of:

formula_59

Distribution sort partitions an input list formula_20 of size formula_2 into formula_62 disjoint buckets of similar size. Every bucket is then sorted recursively and the results are combined into a fully sorted list.

If formula_63 the task is delegated to a cache-optimal single-processor sorting algorithm.

Otherwise the following algorithm is used:

The I/O complexity of codice_5 is:

formula_96

where

formula_97

If the number of processors is chosen that formula_98and formula_99 the I/O complexity is then:

formula_100

Where formula_101 is the time it takes to sort formula_2 items with formula_1 processors in the PEM model.



</doc>
<doc id="588615" url="https://en.wikipedia.org/wiki?curid=588615" title="Ant colony optimization algorithms">
Ant colony optimization algorithms

In computer science and operations research, the ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems which can be reduced to finding good paths through graphs. Artificial Ants stand for multi-agent methods inspired by the behavior of real ants. 
The pheromone-based communication of biological ants is often the predominant paradigm used. Combinations of Artificial Ants and local search algorithms have become a method of choice for numerous optimization tasks involving some sort of graph, e.g., vehicle routing and internet routing. The burgeoning activity in this field has led to conferences dedicated solely to Artificial Ants, and to numerous commercial applications by specialized companies such as AntOptima.

As an example, Ant colony optimization is a class of optimization algorithms modeled on the actions of an ant colony. Artificial 'ants' (e.g. simulation agents) locate optimal solutions by moving through a parameter space representing all possible solutions. Real ants lay down pheromones directing each other to resources while exploring their environment. The simulated 'ants' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate better solutions. One variation on this approach is the bees algorithm, which is more analogous to the foraging patterns of the honey bee, another social insect.

This algorithm is a member of the ant colony algorithms family, in swarm intelligence methods, and it constitutes some metaheuristic optimizations. Initially proposed by Marco Dorigo in 1992 in his PhD thesis, the first algorithm was aiming to search for an optimal path in a graph, based on the behavior of ants seeking a path between their colony and a source of food. The original idea has since diversified to solve a wider class of numerical problems, and as a result, several problems have emerged, drawing on various aspects of the behavior of ants. From a broader perspective, ACO performs a model-based search and shares some similarities with estimation of distribution algorithms.

In the natural world, ants of some species (initially) wander randomly, and upon finding food return to their colony while laying down pheromone trails. If other ants find such a path, they are likely not to keep travelling at random, but instead to follow the trail, returning and reinforcing it if they eventually find food (see Ant communication).

Over time, however, the pheromone trail starts to evaporate, thus reducing its attractive strength. The more time it takes for an ant to travel down the path and back again, the more time the pheromones have to evaporate. A short path, by comparison, gets marched over more frequently, and thus the pheromone density becomes higher on shorter paths than longer ones. Pheromone evaporation also has the advantage of avoiding the convergence to a locally optimal solution. If there were no evaporation at all, the paths chosen by the first ants would tend to be excessively attractive to the following ones. In that case, the exploration of the solution space would be constrained. The influence of pheromone evaporation in real ant systems is unclear, but it is very important in artificial systems.

The overall result is that when one ant finds a good (i.e., short) path from the colony to a food source, other ants are more likely to follow that path, and positive feedback eventually leads to many ants following a single path. The idea of the ant colony algorithm is to mimic this behavior with "simulated ants" walking around the graph representing the problem to solve.

New concepts are required since “intelligence” is no longer centralized but can be found throughout all minuscule objects. Anthropocentric concepts have been known to lead to the production of IT systems in which data processing, control units and calculating forces are centralized. These centralized units have continually increased their performance and can be compared to the human brain. The model of the brain has become the ultimate vision of computers. Ambient networks of intelligent objects and, sooner or later, a new generation of information systems which are even more diffused and based on nanotechnology, will profoundly change this concept. Small devices that can be compared to insects do not dispose of a high intelligence on their own. Indeed, their intelligence can be classed as fairly limited. It is, for example, impossible to integrate a high performance calculator with the power to solve any kind of mathematical problem into a biochip that is implanted into the human body or integrated in an intelligent tag which is designed to trace commercial articles. However, once those objects are interconnected they dispose of a form of intelligence that can be compared to a colony of ants or bees. In the case of certain problems, this type of intelligence can be superior to the reasoning of a centralized system similar to the brain.

Nature offers several examples of how minuscule organisms, if they all follow the same basic rule, can create a form of collective intelligence on the macroscopic level. Colonies of social insects perfectly illustrate this model which greatly differs from human societies. This model is based on the co-operation of independent units with simple and unpredictable behavior. They move through their surrounding area to carry out certain tasks and only possess a very limited amount of information to do so. A colony of ants, for example, represents numerous qualities that can also be applied to a network of ambient objects. Colonies of ants have a very high capacity to adapt themselves to changes in the environment as well as an enormous strength in dealing with situations where one individual fails to carry out a given task. This kind of flexibility would also be very useful for mobile networks of objects which are perpetually developing. Parcels of information that move from a computer to a digital object behave in the same way as ants would do. They move through the network and pass from one knot to the next with the objective of arriving at their final destination as quickly as possible.

Pheromone-based communication is one of the most effective ways of communication which is widely observed in nature. Pheromone is used by social insects such as
bees, ants and termites; both for inter-agent and agent-swarm communications. Due to its feasibility, artificial pheromones have been adopted in multi-robot and swarm robotic systems. Pheromone-based communication was implemented by different means such as chemical or physical (RFID tags, light, sound) ways. However, those implementations were not able to replicate all the aspects of pheromones as seen in nature.

Using projected light was presented in an 2007 IEEE paper by Garnier, Simon, et al. as an experimental setup to study pheromone-based communication with micro autonomous robots. Another study that proposed a novel pheromone communication method, "COSΦ", for a swarm robotic system is based on precise and fast visual localization. 
The system allows simulation of a virtually unlimited number of different pheromones and provides the result of their interaction as a gray-scale image on a horizontal LCD screen that the robots move on. In order to demonstrate the pheromone communication method, Colias autonomous micro robot was deployed as the swarm robotic platform.

In the ant colony optimization algorithms, an artificial ant is a simple computational agent that searches for good solutions to a given optimization problem. To apply an ant colony algorithm, the optimization problem needs to be converted into the problem of finding the shortest path on a weighted graph. In the first step of each iteration, each ant stochastically constructs a solution, i.e. the order in which the edges in the graph should be followed. In the second step, the paths found by the different ants are compared. The last step consists of updating the pheromone levels on each edge.

Each ant needs to construct a solution to move through the graph. To select the next edge in its tour, an ant will consider the length of each edge available from its current position, as well as the corresponding pheromone level. At each step of the algorithm, each ant moves from a state formula_1 to state formula_2, corresponding to a more complete intermediate solution. Thus, each ant formula_3 computes a set formula_4 of feasible expansions to its current state in each iteration, and moves to one of these in probability. For ant formula_3, the probability formula_6 of moving from state formula_1 to state formula_2 depends on the combination of two values, the "attractiveness" formula_9 of the move, as computed by some heuristic indicating the "a priori" desirability of that move and the "trail level" formula_10 of the move, indicating how proficient it has been in the past to make that particular move. The "trail level" represents a posteriori indication of the desirability of that move.

In general, the formula_3th ant moves from state formula_1 to state formula_2 with probability

formula_14

where

formula_10 is the amount of pheromone deposited for transition from state formula_1 to formula_2, 0 ≤ formula_18 is a parameter to control the influence of formula_10, formula_9 is the desirability of state transition formula_21 ("a priori" knowledge, typically formula_22, where formula_23 is the distance) and formula_24 ≥ 1 is a parameter to control the influence of formula_9. formula_26 and formula_27 represent the trail level and attractiveness for the other possible state transitions.

Trails are usually updated when all ants have completed their solution, increasing or decreasing the level of trails corresponding to moves that were part of "good" or "bad" solutions, respectively. An example of a global pheromone updating rule is

formula_28

where formula_10 is the amount of pheromone deposited for a state transition formula_21, formula_31 is the "pheromone evaporation coefficient" and formula_32 is the amount of pheromone deposited by formula_3th ant, typically given for a TSP problem (with moves corresponding to arcs of the graph) by

formula_34

where formula_35 is the cost of the formula_3th ant's tour (typically length) and formula_37 is a constant.

Here are some of the most popular variations of ACO algorithms.

The Ant System is the first ACO algorithm. This algorithm corresponds to the one presented above. It was developed by Dorigo.

In the Ant Colony System algorithm, the original Ant System was modified in three aspects: (i) the edge selection is biased towards exploitation (i.e. favoring the probability of selecting the shortest edges with a large amount of pheromone); (ii) while building a solution, ants change the pheromone level of the edges they are selecting by applying a local pheromone updating rule; (iii) at the end of each iteration, only the best ant is allowed to update the trails by applying a modified global pheromone updating rule. 

In this algorithm, the global best solution deposits pheromone on its trail after every iteration (even if this trial has not been revisited), along with all the other ants.

This algorithm controls the maximum and minimum pheromone amounts on each trail. Only the global best tour or the iteration best tour are allowed to add pheromone to its trail. To avoid stagnation of the search algorithm, the range of possible pheromone amounts on each trail is limited to an interval [τ,τ]. All edges are initialized to τ to force a higher exploration of solutions. The trails are reinitialized to τ when nearing stagnation.

All solutions are ranked according to their length. Only a fixed number of the best ants in this iteration are allowed to update their trials. The amount of pheromone deposited is weighted for each solution, such that solutions with shorter paths deposit more pheromone than the solutions with longer paths.

The pheromone deposit mechanism of COAC is to enable ants to search for solutions collaboratively and effectively. By using an orthogonal design method, ants in the feasible domain can explore their chosen regions rapidly and efficiently, with enhanced global search capability and accuracy. The orthogonal design method and the adaptive radius adjustment method can also be extended to other optimization algorithms for delivering wider advantages in solving practical problems.

It is a recursive form of ant system which divides the whole search domain into several sub-domains and solves the objective on these subdomains. The results from all the subdomains are compared and the best few of them are promoted for the next level. The subdomains corresponding to the selected results are further subdivided and the process is repeated until an output of desired precision is obtained. This method has been tested on ill-posed geophysical inversion problems and works well.

For some versions of the algorithm, it is possible to prove that it is convergent (i.e., it is able to find the global optimum in finite time). The first evidence of convergence for an ant colony algorithm was made in 2000, the graph-based ant system algorithm, and later on for the ACS and MMAS algorithms. Like most metaheuristics, it is very difficult to estimate the theoretical speed of convergence. A performance analysis of a continuous ant colony algorithm with respect to its various parameters (edge selection strategy, distance measure metric, and pheromone evaporation rate) showed that its performance and rate of convergence are sensitive to the chosen parameter values, and especially to the value of the pheromone evaporation rate. In 2004, Zlochin and his colleagues showed that COAC-type algorithms could be assimilated methods of stochastic gradient descent, on the cross-entropy and estimation of distribution algorithm. They proposed these metaheuristics as a "research-based model".

Ant colony optimization algorithms have been applied to many combinatorial optimization problems, ranging from quadratic assignment to protein folding or routing vehicles and a lot of derived methods have been adapted to dynamic problems in real variables, stochastic problems, multi-targets and parallel implementations.
It has also been used to produce near-optimal solutions to the travelling salesman problem. They have an advantage over simulated annealing and genetic algorithm approaches of similar problems when the graph may change dynamically; the ant colony algorithm can be run continuously and adapt to changes in real time. This is of interest in network routing and urban transportation systems.

The first ACO algorithm was called the ant system and it was aimed to solve the travelling salesman problem, in which the goal is to find the shortest round-trip to link a series of cities. The general algorithm is relatively simple and based on a set of ants, each making one of the possible round-trips along the cities. At each stage, the ant chooses to move from one city to another according to some rules:






To optimize the form of antennas, ant colony algorithms can be used. As example can be considered antennas RFID-tags based on ant colony algorithms (ACO)., loopback and unloopback vibrators 10×10

The ACO algorithm is used in image processing for image edge detection and edge linking.
The graph here is the 2-D image and the ants traverse from one pixel depositing pheromone.The movement of ants from one pixel to another is directed by the local variation of the image's intensity values. This movement causes the highest density of the pheromone to be deposited at the edges.

The following are the steps involved in edge detection using ACO:

"Step1: Initialization:"Randomly place formula_38 ants on the image formula_39 where formula_40 . Pheromone matrix formula_41 are initialized with a random value. The major challenge in the initialization process is determining the heuristic matrix.

There are various methods to determine the heuristic matrix. For the below example the heuristic matrix was calculated based on the local statistics:
the local statistics at the pixel position (i,j).

formula_42

Where formula_43 is the image of size formula_44
formula_45,which is a normalization factor

formula_46

formula_47 can be calculated using the following functions:formula_48formula_49formula_50formula_51The parameter formula_52 in each of above functions adjusts the functions’ respective shapes."Step 2 Construction process:"The ant's movement is based on 4-connected pixels or 8-connected pixels. The probability with which the ant moves is given by the probability equation formula_53"Step 3 and Step 5 Update process:"The pheromone matrix is updated twice. in step 3 the trail of the ant (given by formula_54 ) is updated where as in step 5 the evaporation rate of the trail is updated which is given by the below equation.formula_55, where formula_56 is the pheromone decay coefficient formula_57

"Step 7 Decision Process:"Once the K ants have moved a fixed distance L for N iteration, the decision whether it is an edge or not is based on the threshold T on the pheromone matrixτ. Threshold for the below example is calculated based on Otsu's method.

Image Edge detected using ACO:The images below are generated using different functions given by the equation (1) to (4).


With an ACO algorithm, the shortest path in a graph, between two points A and B, is built from a combination of several paths. It is not easy to give a precise definition of what algorithm is or is not an ant colony, because the definition may vary according to the authors and uses. Broadly speaking, ant colony algorithms are regarded as populated metaheuristics with each solution represented by an ant moving in the search space. Ants mark the best solutions and take account of previous markings to optimize their search. They can be seen as probabilistic multi-agent algorithms using a probability distribution to make the transition between each iteration. In their versions for combinatorial problems, they use an iterative construction of solutions. According to some authors, the thing which distinguishes ACO algorithms from other relatives (such as algorithms to estimate the distribution or particle swarm optimization) is precisely their constructive aspect. In combinatorial problems, it is possible that the best solution eventually be found, even though no ant would prove effective. Thus, in the example of the Travelling salesman problem, it is not necessary that an ant actually travels the shortest route: the shortest route can be built from the strongest segments of the best solutions. However, this definition can be problematic in the case of problems in real variables, where no structure of 'neighbours' exists. The collective behaviour of social insects remains a source of inspiration for researchers. The wide variety of algorithms (for optimization or not) seeking self-organization in biological systems has led to the concept of "swarm intelligence", which is a very general framework in which ant colony algorithms fit.

There is in practice a large number of algorithms claiming to be "ant colonies", without always sharing the general framework of optimization by canonical ant colonies. In practice, the use of an exchange of information between ants via the environment (a principle called "stigmergy") is deemed enough for an algorithm to belong to the class of ant colony algorithms. This principle has led some authors to create the term "value" to organize methods and behavior based on search of food, sorting larvae, division of labour and cooperative transportation.


The inventors are Frans Moyson and Bernard Manderick. Pioneers of the field include Marco Dorigo, Luca Maria Gambardella.

Chronology of ant colony optimization algorithms.




</doc>
<doc id="60034541" url="https://en.wikipedia.org/wiki?curid=60034541" title="Miller's recurrence algorithm">
Miller's recurrence algorithm

Miller's recurrence algorithm is a procedure for calculating a rapidly decreasing solution of a linear recurrence relation developed by J. C. P. Miller. It was originally developed to compute tables of the modified Bessel function but also applies to Bessel functions of the first kind and has other applications such as computation of the coefficients of Chebyshev expansions of other special functions.

Many families of special functions satisfy a recurrence relation that relates the values of the functions of different orders with common argument formula_1.

The modified Bessel functions of the first kind formula_2 satisfy the recurrence relation
However, the modified Bessel functions of the second kind formula_4 also satisfy the same recurrence relation

The first solution decreases rapidly with formula_6. The second solution increases rapidly with formula_6. Miller's algorithm provides a numerically stable procedure to obtain the decreasing solution.

To compute the terms of a recurrence formula_8 through formula_9 according to Miller's algorithm, one first chooses a value formula_10 much larger than formula_11 and computes a trial solution taking initial conditionformula_12 to an arbitrary non-zero value (such as 1) and taking formula_13 and later terms to be zero. Then the recurrence relation is used to successively compute trial values for formula_14, formula_15 down to formula_8. Noting that a second sequence obtained from the trial sequence by multiplication by a constant normalizing factor will still satisfy the same recurrence relation, one can then apply a separate normalizing relationship to determine the normalizing factor that yields the actual solution.

In the example of the modified Bessel functions, a suitable normalizing relation is a summation involving the even terms of the recurrence: 
where the infinite summation becomes finite due to the approximation that formula_13 and later terms are zero.

Finally, it is confirmed that the approximation error of the procedure is acceptable by repeating the procedure with a second choice of formula_10 larger than the initial choice and confirming that the second set of results for formula_8 through formula_9 agree within the first set within the desired tolerance. Note that to obtain this agreement, the value of formula_10 must be large enough such that the term formula_12 is small compared to the desired tolerance.

In contrast to Miller's algorithm, attempts to apply the recurrence relation in the forward direction starting from known values of formula_24 and formula_25 obtained by other methods will fail as rounding errors introduce components of the rapidly increasing solution.

Olver and Gautschi analyses the error propagation of the algorithm in detail.

For Bessel functions of the first kind, the equivalent recurrence relation and normalizing relationship are :

The algorithm is particularly efficient in applications that require the values of the Bessel functions for all orders formula_28 for each value of formula_1 compared to direct independent computations of formula_30 separate functions.


</doc>
<doc id="60327286" url="https://en.wikipedia.org/wiki?curid=60327286" title="Whitehead's algorithm">
Whitehead's algorithm

Whitehead's algorithm is a mathematical algorithm in group theory for solving the automorphic equivalence problem in the finite rank free group "F". The algorithm is based on a classic 1936 paper of J. H. C. Whitehead. It is still unknown (except for the case "n" = 2) if Whitehead's algorithm has polynomial time complexity.

Let formula_1 be a free group of rank formula_2 with a free basis formula_3. The automorphism problem, or the automorphic equivalence problem for formula_4 asks, given two freely reduced words formula_5 whether there exists an automorphism formula_6 such that formula_7.

Thus the automorphism problem asks, for formula_5 whether formula_9.
For formula_5 one has formula_9 if and only if formula_12, where formula_13 are conjugacy classes in formula_4 of formula_15 accordingly. Therefore, the automorphism problem for formula_4 is often formulated in terms of formula_17-equivalence of conjugacy classes of elements of formula_4.

For an element formula_19, formula_20 denotes the freely reduced length of formula_21 with respect to formula_22, and formula_23 denotes the cyclically reduced length of formula_21 with respect to formula_22. For the automorphism problem, the length of an input formula_21 is measured as formula_20 or as formula_23, depending on whether one views formula_21 as an element of formula_4 or as defining the corresponding conjugacy class formula_31 in formula_4.

The automorphism problem for formula_4 was algorithmically solved by J. H. C. Whitehead in a classic 1936 paper, and his solution came to be known as Whitehead's algorithm. Whitehead used a topological approach in his paper. Namely, consider the 3-manifold formula_34, the connected sum of formula_35 copies of formula_36. Then formula_37, and, moreover, up to a quotient by a finite normal subgroup isomorphic to formula_38, the mapping class group of formula_39 is equal to formula_17; see. Different free bases of formula_4 can be represented by isotopy classes of "sphere systems" in formula_39, and the cyclically reduced form of an element formula_43, as well as the Whitehead graph of formula_31, can be "read-off" from how a loop in general position representing formula_31 intersects the spheres in the system. Whitehead moves can be represented by certain kinds of topological "swapping" moves modifying the sphere system.

Subsequently, Rapaport, and later, based on her work, Higgins and Lyndon, gave a purely combinatorial and algebraic re-interpretation of Whitehead's work and of Whitehead's algorithm. The exposition of Whitehead's algorithm in the book of Lyndon and Schupp is based on this combinatorial approach. Culler and Vogtmann, in their 1986 paper that introduced the Outer space, gave a hybrid approach to Whitehead's algorithm, presented in combinatorial terms but closely following Whitehead's original ideas.

Our exposition regarding Whitehead's algorithm mostly follows Ch.I.4 in the book of Lyndon and Schupp, as well as.

The automorphism group formula_46 has a particularly useful finite generating set formula_47 of Whitehead automorphisms or Whitehead moves. Given formula_48 the first part of Whitehead's algorithm consists of iteratively applying Whitehead moves to formula_49 to take each of them to an ``automorphically minimal" form, where the cyclically reduced length strictly decreases at each step. Once we find automorphically these minimal forms formula_50 of formula_49, we check if formula_52. If formula_53 then formula_49 are not automorphically equivalent in formula_4.

If formula_52, we check if there exists a finite chain of Whitehead moves taking formula_57 to formula_58 so that the cyclically reduced length remains constant throughout this chain. The elements formula_49 are not automorphically equivalent in formula_4 if and only if such a chain exists.

Whitehead's algorithm also solves the "search automorphism problem" for formula_4. Namely, given formula_48, if Whitehead's algorithm concludes that formula_9, the algorithm also outputs an automorphism formula_64 such that formula_65. Such an element formula_64 is produced as the composition of a chain of Whitehead moves arising from the above procedure and taking formula_21 to formula_68.

A Whitehead automorphism, or Whitehead move, of formula_4 is an automorphism formula_70 of formula_4 of one of the following two types:

(i) There is a permutation formula_72 of formula_73 such that for formula_74

(ii) There is an element formula_77, called the multiplier, such that for every formula_78

Often, for a Whitehead automorphism formula_84, the corresponding outer automorphism in formula_17 is also called a Whitehead automorphism or a Whitehead move.
Let formula_86.

Let formula_87 be a homomorphism such that
Then formula_76 is actually an automorphism of formula_90, and, moreover, formula_76 is a Whitehead automorphism of the second kind, with the multiplier formula_92.

Let formula_93 be a homomorphism such that
Then formula_95 is actually an inner automorphism of formula_90 given by conjugation by formula_97, and, moreover, formula_95is a Whitehead automorphism of the second kind, with the multiplier formula_99.

For formula_43, the conjugacy class formula_31 is called automorphically minimal if for every formula_64 we have formula_103. 
Also, a conjugacy class formula_31 is called Whitehead minimal if for every Whitehead move formula_70 we have formula_106.

Thus, by definition, if formula_31 is automorphically minimal then it is also Whitehead minimal. It turns out that the converse is also true.

The following statement is referred to as Whitehead's "Peak Reduction Lemma", see Proposition 4.20 in and Proposition 1.2 in:

Let formula_43. Then the following hold:

(1) If formula_31 is not automorphically minimal, then there exists a Whitehead automorphism formula_70 such that formula_111.

(2) Suppose that formula_31 is automorphically minimal, and that another conjugacy class formula_113 is also automorphically minimal. Then formula_9 if and only if formula_115 and there exists a finite sequence of Whitehead moves formula_116 such that
and

Part (1) of the Peak Reduction Lemma implies that a conjugacy class formula_31 is Whitehead minimal if and only if it is automorphically minimal.

The automorphism graph formula_120 of formula_4 is a graph with the vertex set being the set of conjugacy classes formula_122 of elements formula_123. Two distinct vertices formula_124 are adjacent in formula_120 if formula_126 and there exists a Whitehead automorphism formula_76 such that formula_128. For a vertex formula_122 of formula_120, the connected component of formula_122 in formula_120 is denoted formula_133.

For formula_134 with cyclically reduced form formula_57, the Whitehead graph formula_136 is a labelled graph with the vertex set formula_137, where for formula_138 there is an edge joining formula_139 and formula_140 with the label or "weight" formula_141 which is equal to the number of distinct occurrences of subwords formula_142 read cyclically in formula_57. (In some versions of the Whitehead graph one only includes the edges with formula_144.)

If formula_84 is a Whitehead automorphism, then the length change formula_146 can be expressed as a linear combination, with integer coefficients determined by formula_76, of the weights formula_141 in the Whitehead graph formula_136. See Proposition 4.6 in Ch. I of. This fact plays a key role in the proof of Whitehead's peak reduction result.

Whitehead's minimization algorithm, given a freely reduced word formula_43, finds an automorphically minimal formula_151 such that formula_152

This algorithm proceeds as follows. Given formula_43, put formula_154. If formula_155 is already constructed, check if there exists a Whitehead automorphism formula_84 such that formula_157. (This condition can be checked since the set of Whitehead automorphisms of formula_4 is finite.) If such formula_76 exists, put formula_160 and go to the next step. If no such formula_76 exists, declare that formula_162 is automorphically minimal, with formula_163, and terminate the algorithm.

Part (1) of the Peak Reduction Lemma implies that the Whitehead's minimization algorithm terminates with some formula_164, where formula_165, and that then formula_166 is indeed automorphically minimal and satisfies formula_167.

Whitehead's algorithm for the automorphic equivalence problem, given formula_168 decides whether or not formula_9.

The algorithm proceeds as follows. Given formula_168, first apply the Whitehead minimization algorithm to each of formula_49 to find automorphically minimal formula_172 such that formula_173 and formula_174. If formula_175, declare that formula_176 and terminate the algorithm. Suppose now that formula_177. Then check if there exists a 
finite sequence of Whitehead moves formula_116 such that

and

This condition can be checked since the number of cyclically reduced words of length formula_181 in formula_4 is finite. More specifically, using the breadth-first approach, one constructs the connected components formula_183 of the automorphism graph and checks if formula_184.

If such a sequence exists, declare that formula_9, and terminate the algorithm. If no such sequence exists, declare that formula_186 and terminate the algorithm.

The Peak Reduction Lemma implies that Whitehead's algorithm correctly solves the automorphic equivalence problem in formula_187. Moreover, if formula_9, the algorithm actually produces (as a composition of Whitehead moves) an automorphism formula_64 such that formula_65.





</doc>
<doc id="60378307" url="https://en.wikipedia.org/wiki?curid=60378307" title="Broadcast (parallel pattern)">
Broadcast (parallel pattern)

Broadcast is a collective communication primitive in parallel programming to distribute programming instructions or data to nodes in a cluster it is the reverse operation of reduce. The broadcast operation is widely used in parallel algorithms, such as matrix-vector multiplication, Gaussian elimination and shortest paths.

The Message Passing Interface implements broadcast in codice_1.

A message formula_1of length n should be distributed from one node to all other formula_2 nodes.

formula_3is the time it takes to send one byte.

formula_4is the time it takes for a message to travel to another node, independent of its length.

Therefore, the time to send a package from one node to another is formula_5.

formula_6 is the number of nodes and the number of processors.

With Binomial Tree Broadcast the whole message is sent at once. Each node that has already received the message sends it on further. This grows exponentially as each time step the amount of sending nodes is doubled. The algorithm is ideal for short messages but falls short with longer ones as during the time when the first transfer happens and only one node is busy.

Sending a message to all nodes takes formula_7 time which results in a runtime of formula_8
Message M

id := node number
p := number of nodes

if id > 0 
for (i := ceil(log_2(id)) - 1; i >= 0; i--)

The message is split up into formula_9 packages and send piecewise from node formula_10 to node formula_11. The time needed to distribute the first message piece is formula_12 whereby formula_13 is the time needed to send a package from one processor to another.

Sending a whole message takes formula_14.

Optimal is to choose formula_15 resulting in a runtime of approximately formula_16

The run time is dependent on not only message length but also the number of processors that play roles. This approach shines when the length of the message is much larger than the amount of processors.
Message M := [m_1, m_2, ... ,m_n]
id = node number

for (i := 1; i <= n; i++) in parallel

This algorithm combines Binomial Tree Broadcast and Linear Pipeline Broadcast, which makes the algorithm work well for both short and long messages. The aim is to have as many nodes work as possible while maintaining the ability to send short messages quickly. A good approach is to use Fibonacci trees for splitting up the tree, which are a good choice as a message cannot be sent to both children at the same time. This results in a binary tree structure.

We will assume in the following that communication is full-duplex. The Fibonacci tree structure has a depth of about formula_17whereby formula_18the golden ratio.

The resulting runtime is formula_19. Optimal is formula_20.

This results in a runtime of formula_21.
Message M := [m_1, m_2, ... ,m_k]

for i = 1 to k 

This algorithm aims to improve on some disadvantages of tree structure models with pipelines. Normally in tree structure models with pipelines (see above methods), leaves receive just their data and cannot contribute to send and spread data.

The algorithm concurrently uses two binary trees to communicate over. Those trees will be called tree A and B. Structurally in binary trees there are relatively more leave nodes than inner nodes. Basic Idea of this algorithm is to make a leaf node of tree A be an inner node of tree B. It has also the same technical function in opposite side from B to A tree. This means, two packets are sent and received by inner nodes and leaves in different steps.

The number of steps needed to construct construct two parallel-working binary trees is dependent on the amount of processors. Like with other structures one processor can is the root node who sends messages to two trees. It is not necessary to set a root node, because it is not hard to recognize that the direction of sending messages in binary tree is normally top to bottom. There is no limitation on the number of processors to build two binary trees. Let the height of the combined tree be . Tree A and B can have a height of formula_22. Especially, if the number of processors correspond to formula_23, we can make both sides trees and a root node.

To construct this model efficiently and easily with a fully built tree, we can use two methods called "Shifting" and "Mirroring" to get second tree. Let assume tree A is already modelled and tree B is supposed to be constructed based on tree A. We assume that we have formula_24 processors ordered from 0 to formula_25.

The "Shifting" method, first copies tree A and moves every node one position to the left to get tree B. The node, which will be located on -1, becomes a child of processor formula_26.

"Mirroring" is ideal for an even number of processors. With this method tree B can be more easily constructed by tree A, because there are no structural transformations in order to create the new tree. In addition, a symmetric process makes this approach simple. This method can also handle an odd number of processors, in this case, we can set processor formula_25 as root node for both trees. For the remaining processors "Mirroring" can be used.

We need to find a schedule in order to make sure that no processor has to send or receive two messages from two trees in a step. The edge, is a communication connection to connect two nodes, and can be labelled as either 0 or 1 to make sure that every processor can alternate between 0 and 1-labelled edges. The edges of and can be colored with two colors (0 and 1) such that


In every even step the edges with 0 are activated and edges with 1 are activated in every odd step.

In this case the number of packet k is divided in half for each tree. Both trees are working together the total number of packets formula_28 (upper tree + bottom tree)

In each binary tree sending a message to another nodes takes formula_29 steps until a processor has at least a packet in step formula_30. Therefore, we can calculate all steps as formula_31.
The resulting run time is formula_32. (Optimal formula_33)

This results in a run time of formula_34.

In this section, another broadcasting algorithm with an underlying telephone communication model will be introduced. A Hypercube creates network system with formula_35. Every node is represented by binary formula_36 depending on the number of dimensions. Fundamentally ESBT(Edge-disjoint Spanning Binomial Trees) is based on hypercube graphs, pipelining(formula_37 messages are divided by formula_38 packets) and binomial trees. The Processor formula_39 cyclically spreads packets to roots of ESBTs. The roots of ESBTs broadcast data with binomial tree. To leave all of formula_38 from formula_41, formula_38 steps are required, because all packets are distributed by formula_43. It takes another d steps until the last leaf node receives the packet. In total formula_44 steps are necessary to broadcast formula_37 message through ESBT.

The resulting run time is formula_46. formula_47.

This results in a run time of formula_48.



</doc>
<doc id="59742671" url="https://en.wikipedia.org/wiki?curid=59742671" title="DSatur">
DSatur

DSatur is a graph colouring algorithm put forward by Daniel Brélaz in 1979. Similarly to the greedy colouring algorithm, DSatur colours the vertices of a graph one after another, expending a previously unused colour when needed. Once a new vertex has been coloured, the algorithm determines which of the remaining uncoloured vertices has the highest number of colours in its neighbourhood and colours this vertex next. Brélaz defines this number as the "degree of saturation" of a given vertex. The contraction of the degree of saturation forms the name of the algorithm. DSatur is a heuristic graph colouring algorithm, yet produces exact results for bipartite, cycle, and wheel graphs. DSatur has also been referred to as saturation LF in the literature.

Define the degree of saturation of a vertex as the number of different colours in its neighbourhood. Given a simple, undirected graph "G" compromising vertex set "V" and edge set "E":


The worst-case complexity of DSatur is "Ο"("n"), however in practical some additional expenses result from the need for holding the degree of saturation of the uncoloured vertices. DSatur has been proven to be exact for bipartite graphs, as well as for cycle and wheel graphs. In an empirical comparison by Lewis 2015, DSatur produced significantly better vertex colourings than the greedy algorithm on random graphs with edge probability "p" = 0.5 at varying number of vertices, while in turn producing significantly worse colourings than the Recursive Largest First algorithm.


</doc>
<doc id="60842845" url="https://en.wikipedia.org/wiki?curid=60842845" title="Enumeration algorithm">
Enumeration algorithm

In computer science, an enumeration algorithm is an algorithm that enumerates the answers to a computational problem. Formally, such an algorithm applies to problems that take an input and produce a list of solutions, similarly to function problems. For each input, the enumeration algorithm must produce the list of all solutions, without duplicates, and then halt. The performance of an enumeration algorithm is measured in terms of the time required to produce the solutions, either in terms of the total time required to produce all solutions, or in terms of the maximal delay between two consecutive solutions and in terms of a preprocessing time, counted as the time before outputting the first solution. This complexity can be expressed in terms of the size of the input, the size of each individual output, or the total size of the set of all outputs, similarly to what is done with output-sensitive algorithms.

An enumeration problem formula_1 is defined as a relation formula_2 over strings of an arbitrary alphabet formula_3:

formula_4

An algorithm solves formula_1 if for every input formula_6 the algorithm produces the (possibly infinite) sequence formula_7 such that formula_7 has no duplicate and formula_9 if and only if formula_10. The algorithm should halt if the sequence formula_7 is finite.

Enumeration problems have been studied in the context of computational complexity theory, and several complexity classes have been introduced for such problems.

A very general such class is EnumP, the class of problems for which the correctness of a possible output can be checked in polynomial time in the input and output. Formally, for such a problem, there must exist an algorithm A which takes as input the problem input "x", the candidate output "y", and solves the decision problem of whether "y" is a correct output for the input "x", in polynomial time in "x" and "y". For instance, this class contains all problems that amount to enumerating the witnesses of a problem in the class NP.

Other classes that have been defined include the following. In the case of problems that are also in EnumP, these problems are ordered from least to most specific




The notion of enumeration algorithms is also used in the field of computability theory to define some high complexity classes such as RE, the class of all recursively enumerable problems. This is the class of sets for which there exist an enumeration algorithm that will produce all elements of the set: the algorithm may run forever if the set is infinite, but each solution must be produced by the algorithm after a finite time.


</doc>
<doc id="61070746" url="https://en.wikipedia.org/wiki?curid=61070746" title="Snap rounding">
Snap rounding

Snap rounding is a method of approximating line segment locations by creating a grid and placing each point in the centre of a cell (pixel) of the grid. The method preserves certain topological properties of the arrangement of line segments.

Drawbacks include the potential interpolation of additional vertices in line segments (lines become polylines), the arbitrary closeness of a point to a non-incident edge, and arbitrary numbers of intersections between input line-segments. The 3 dimensional case is worse, with a polyhedral subdivision of complexity becoming complexity "O"(n).

There are more refined algorithms to cope with some of these issues, for example "iterated snap rounding" guarantees a "large" separation between points and non-incident edges.


Conversely there are undesirable properties:




</doc>
<doc id="61176336" url="https://en.wikipedia.org/wiki?curid=61176336" title="Block swap algorithms">
Block swap algorithms

Block swap algorithms is the simple art of swapping two elements of an array in computer algorithms. It is also simple to swap two non-overlapping regions of an array of equal size. However, it is not simple to swap two non-overlapping regions of an array in-place that are next to each other, but are of unequal sizes. Three algorithms are known to accomplish this: Bentley's Juggling, Gries-Mills, and Reversal. All three algorithms are linear time "O"("n") (see Time Complexity).

The reversal algorithm is the simplest to explain, using rotations. A rotation is an in-place reversal of array elements. This method swaps two elements of an array from outside in within a range. The rotation works for an even number of elements or an odd number of array elements. The reversal algorithm uses three in-place rotations to accomplish an in-place block swap:

Gries-Mills and Reversal algorithms perform better than Bentley's Juggling, because of their cache-friendly memory access pattern behavior.

The Reversal algorithm parallelizes well, because rotations can be split into sub-regions, which can be rotated independently of others.


</doc>
<doc id="61186810" url="https://en.wikipedia.org/wiki?curid=61186810" title="Pan–Tompkins algorithm">
Pan–Tompkins algorithm

The Pan–Tompkins algorithm is commonly used to detect QRS complexes in electrocardiographic signals (ECG). The QRS complex represents the ventricular depolarization and the main spike visible in an ECG signal (see figure). This feature makes it particularly suitable for measuring heart rate, the first way to assess the heart health state. In the first derivation of Einthoven of a physiological heart, the QRS complex is composed by a downward deflection (Q wave), a high upward deflection (R wave) and a final downward deflection (S wave).

The Pan–Tompkins algorithm applies a series of filters to highlight the frequency content of this rapid heart depolarization and removes the background noise. Then, it squares the signal to amplify the QRS contribute. Finally, it applies adaptive thresholds to detect the peaks of the filtered signal. The algorithm was proposed by Jiapu Pan and Willis J. Tompkins in 1985, in the journal IEEE Transactions on Biomedical Engineering. The performance of the method was tested on an annotated arrhythmia database (MIT/BIH) and evaluated also in presence of noise. Pan and Tompkins reported that the 99.3 percent of QRS complexes was correctly detected.

As a first step, a band-pass filter is applied to increase the signal-to-noise ratio. A filter bandwidth of 5-15 Hz is suggested to maximize the QRS contribute and reduce muscle noise, baseline wander, powerline interference and the P wave/T wave frequency content. In the original algorithm proposed in 1985, the band-pass filter was obtained with a low-pass filter and a high-pass filter in cascade to reduce the computational cost and allow a real-time detection, while ensuring a 3 dB passband in the 5–12 Hz frequency range, reasonably close to the design goal.

For a signal sampled at a frequency of 200 Hz, Pan and Tompkins suggested the filters with the following transfer functions formula_1 in an updated version of their article:


As a third step, a derivative filter is applied to provide information about the slope of the QRS. For a signal sampled at 200 Hz, Pan and Tompkins suggested the following transfer function:

formula_4for a 5-point derivative filter with gain of 0.1 and a processing delay of 2 samples.

The filtered signal is squared to enhance the dominant peaks (QRSs) and reduce the possibility of erroneously recognizing a T wave as an R peak. Then, a moving average filter is applied to provide information about the duration of the QRS complex. The number of samples to average is chosen in order to average on windows of 150 ms. The signal so obtained is called integrated signal.

In order to detect a QRS complex, the local peaks of the integrated signal are found. A peak is defined as the point in which the signal changes direction (from an increasing direction to a decreasing direction). After each peak, no peak can be detected in the next 200 ms (ie. the lockout time). This is a physiological constraint due to the refractory period during which ventricular depolarization cannot occur even in the presence of a stimulus.

Each fiducial mark is considered as a potential QRS. To reduce the possibility of wrongly selecting a noise peak as a QRS, each peak amplitude is compared to a threshold ("Threshold") that takes into account the available information about already detected QRS and the noise level:

formula_5

where "NoiseLevel" is the running estimate of the noise level in the integrated signal and "SignalLevel" is the running estimate of the signal level in the integrated signal.

The threshold is automatically updated after detecting a new peak, based on its classification as signal or noise peak:

formula_6(if "PEAK" is a signal peak)

formula_7(if "PEAK" is a noise peak)

where "PEAK" is the new peak found in the integrated signal.

At the beginning of the QRS detection, a 2 seconds learning phase is needed to initialize "SignalLevel" and "NoiseLevel" as a percentage of the maximum and average amplitude of the integrated signal, respectively.

If a new "PEAK" is under the "Threshold", the noise level is updated. If "PEAK" is above the "Threshold", the algorithm implements a further check before confirming the peak as a true QRS, taking into consideration the information provided by the bandpass filtered signal.

In the filtered signal the peak corresponding to the one evaluated on the integrated signal is searched and compared with a threshold, calculated in a similar way to the previous step:

formula_8

formula_9(if "PEAK" is a signal peak)

formula_10(if "PEAK" is a noise peak)

where the final F stands for filtered signal.

The algorithm takes into account the possibility of setting too high values of "ThresholdI" and "ThresholdI" A check is performed to continuously assess the RR intervals (namely the temporal interval between two consecutively QRS peaks) to overcome this issue. The average RR is computed in two ways to consider both regular and irregular heart rhythm. In the first method "RRaverage1" is computed as the mean of the last RR intervals. In the second method "RRaverage2" is computed as the mean of the last RR intervals that fell between the limits specified as:

formula_11

formula_12

If no QRS is detected in a window of 166% of the average RR ("RRaverage1" or "RRaverage2", if the heart rhythm is regular or irregular, respectively)"," the algorithm adds the maximal peak in the window as a potential QRS and classify it considering half the values of the thresholds (both "ThresholdI and ThresholdI"). This check is implemented because the temporal distance between two consecutive beats cannot physiologically change more quickly than this.

The algorithm takes particularly into consideration the possibility of a false detection of T waves. If a potential QRS falls up to a 160 ms window after the refractory periody from the last correctly detected QRS complex, the algorithm evaluates if it could be a T wave with particular high amplitude. In this case, its slope is compared to the one of the precedent QRS complex. If the slope is less than half the previous one, the current QRS is recognized as a T wave and discarded, and it also updates the "NoiseLevel" (both in the filtered signal and the integrated signal).

Once the QRS complex is successfully recognized, the heart rate is computed as a function of the distance in seconds between two consecutive QRS complexes (or R peaks):

formula_13

where bpm stands for beats per minute. The HR is often used to compute the heart rate variability (HRV) a measure of the variability of the time interval between heartbeats. HRV is often used in the clinical field to diagnose and monitor pathological conditions and their treatment, but also in the affective computing research to study new methods to assess the emotional state of people.



</doc>
<doc id="61379828" url="https://en.wikipedia.org/wiki?curid=61379828" title="Berlekamp–Rabin algorithm">
Berlekamp–Rabin algorithm

In number theory, Berlekamp's root finding algorithm, also called the Berlekamp–Rabin algorithm, is the probabilistic method of finding roots of polynomials over a field formula_1. The method was discovered by Elwyn Berlekamp in 1970 as an auxiliary to the algorithm for polynomial factorization over finite fields. The algorithm was later modified by Rabin for arbitrary finite fields in 1979. The method was also independently discovered before Berlekamp by other researchers.

The method was proposed by Elwyn Berlekamp in his 1970 work on polynomial factorization over finite fields. His original work lacked a formal correctness proof and was later refined and modified for arbitrary finite fields by Michael Rabin. In 1986 René Peralta proposed a similar algorithm for finding square roots in formula_1. In 2000 Peralta's method was generalized for cubic equations.

Let formula_3 be an odd prime number. Consider the polynomial formula_4 over the field formula_1 of remainders modulo formula_3. The algorithm should find all formula_7 in formula_1 such that formula_9 in formula_1.

Let formula_11. Finding all roots of this polynomial is equivalent to finding its factorization into linear factors. To find such factorization it is sufficient to split the polynomial into any two non-trivial divisors and factorize them recursively. To do this, consider the polynomial formula_12 where formula_13 is some any element of formula_1. If one can represent this polynomial as the product formula_15 then in terms of the initial polynomial it means that formula_16, which provides needed factorization of formula_17.

Due to Euler's criterion, for every monomial formula_19 exactly one of following properties holds:


Thus if formula_28 is not divisible by formula_20, which may be checked separately, then formula_28 is equal to the product of greatest common divisors formula_31 and formula_32.

The property above leads to the following algorithm:


If formula_17 is divisible by some non-linear primitive polynomial formula_45 over formula_1 then when calculating formula_40 with formula_48 and formula_49 one will obtain a non-trivial factorization of formula_50, thus algorithm allows to find all roots of arbitrary polynomials over formula_1.

Consider equation formula_52 having elements formula_53 and formula_54 as its roots. Solution of this equation is equivalent to factorization of polynomial formula_55 over formula_1. In this particular case problem it is sufficient to calculate only formula_57. For this polynomial exactly one of the following properties will hold:


In the third case GCD is equal to either formula_63 or formula_64. It allows to write the solution as formula_65.

Assume we need to solve the equation formula_66. For this we need to factorize formula_67. Consider some possible values of formula_13:



A manual check shows that, indeed, formula_80 and formula_81.

The algorithm finds factorization of formula_28 in all cases except for ones when all numbers formula_83 are quadratic residues or non-residues simultaneously. According to theory of cyclotomy, the probability of such an event for the case when formula_84 are all residues or non-residues simultaneously (that is, when formula_85 would fail) may be estimated as formula_86 where formula_87 is the number of distinct values in formula_84. In this way even for the worst case of formula_89 and formula_90, the probability of error may be estimated as formula_91 and for modular square root case error probability is at most formula_92.

Let a polynomial have degree formula_93. We derive the algorithm's complexity as follows:


Thus the whole procedure may be done in formula_101. Using the fast Fourier transform and Half-GCD algorithm, the algorithm's complexity may be improved to formula_105. For the modular square root case, the degree is formula_106, thus the whole complexity of algorithm in such case is bounded by formula_107 per iteration.


</doc>
<doc id="58536963" url="https://en.wikipedia.org/wiki?curid=58536963" title="Bartels–Stewart algorithm">
Bartels–Stewart algorithm

In numerical linear algebra, the Bartels–Stewart algorithm is used to numerically solve the Sylvester matrix equation formula_1. Developed by R.H. Bartels and G.W. Stewart in 1971, it was the first numerically stable method that could be systematically applied to solve such equations. The algorithm works by using the real Schur decompositions of formula_2 and formula_3 to transform formula_1 into a triangular system that can then be solved using forward or backward substitution. In 1979, G. Golub, C. Van Loan and S. Nash introduced an improved version of the algorithm, known as the Hessenberg–Schur algorithm. It remains a standard approach for solving Sylvester equations when formula_5 is of small to moderate size.

Let formula_6, and assume that the eigenvalues of formula_2 are distinct from the eigenvalues of formula_3. Then, the matrix equation formula_1 has a unique solution. The Bartels–Stewart algorithm computes formula_5 by applying the following steps: 

1.Compute the real Schur decompositions

The matrices formula_13 and formula_14 are block-upper triangular matrices, with diagonal blocks of size formula_15 or formula_16.

2. Set formula_17

3. Solve the simplified system formula_18, where formula_19. This can be done using forward substitution on the blocks. Specifically, if formula_20, then

where formula_22is the formula_23th column of formula_24. When formula_25, columns formula_26 should be concatenated and solved for simultaneously. 

4. Set formula_27

Using the QR algorithm, the real Schur decompositions in step 1 require approximately formula_28 flops, so that the overall computational cost is formula_29. 

In the special case where formula_30 and formula_31 is symmetric, the solution formula_5 will also be symmetric. This symmetry can be exploited so that formula_24 is found more efficiently in step 3 of the algorithm.

The Hessenberg–Schur algorithm replaces the decomposition formula_34 in step 1 with the decomposition formula_35, where formula_36 is an upper-Hessenberg matrix. This leads to a system of the form formula_37 that can be solved using forward substitution. The advantage of this approach is that formula_35 can be found using Householder reflections at a cost of formula_39 flops, compared to the formula_40 flops required to compute the real Schur decomposition of formula_2. 

The subroutines required for the Hessenberg-Schur variant of the Bartels–Stewart algorithm are implemented in the SLICOT library. These are used in the MATLAB control system toolbox.

For large systems, the formula_42 cost of the Bartels–Stewart algorithm can be prohibitive. When formula_2 and formula_3 are sparse or structured, so that linear solves and matrix vector multiplies involving them are efficient, iterative algorithms can potentially perform better. These include projection-based methods, which use Krylov subspace iterations, methods based on the alternating direction implicit (ADI) iteration, and hybridizations that involve both projection and ADI. Iterative methods can also be used to directly construct low rank approximations to formula_5 when solving formula_46. 


</doc>
<doc id="52300160" url="https://en.wikipedia.org/wiki?curid=52300160" title="Shapiro–Senapathy algorithm">
Shapiro–Senapathy algorithm

Gene regulation is the main genetic program through which an organism controls its normal functions. Thus, any error in this program caused by mutations will alter the normal state and lead to disease. RNA splicing is increasingly realized to be at the center of gene regulation in eukaryotic organisms, including all animals and plants. In this context, Dr. Periannan Senapathy has pioneered research in the biology of RNA splicing, and provided tenable solutions for why genes are split, how splice junction sequences originated, and why exons are very short and introns are very long (Split-Gene Theory). Based on these findings, he has provided an algorithm known as the Shapiro & Senapathy algorithm (S&S) for predicting the splice sites, exons and genes in animals and plants. This algorithm has the ability to discover disease-causing mutations in splice junctions in cancerous and non-cancerous diseases that is being used in major research institutions around the world.

The S&S algorithm has been cited in ~3,000 publications in clinical genomics on finding splicing mutations in thousands of diseases including many different forms of cancer and non-cancer diseases. It has been the basis of many leading software tools, such as Human Splicing Finder, Splice-site Analyzer Tool, dbass (Ensembl), Alamut and SROOGLE, which are cited by approx. 1,500 additional citations. The S&S algorithm has thus significantly impacted the field of medicine, and is increasingly applied in today's disease research, pharmacogenomics, and Precision Medicine, as up to 50% of all diseases and ADRs (Adverse Drug Reactions) are now thought to be caused by RNA splicing mutations.

Using the S&S algorithm, scientists have identified mutations and genes that cause numerous cancers, inherited disorders, immune deficiency diseases and neurological disorders. In addition, mutations in various drug metabolizing genes that cause ADRs to different drugs that are used to treat different diseases, including cancer chemotherapeutic drugs, have been identified. S&S is also used in detecting the “cryptic” splice sites that are not authentic sites used in the normal splicing of gene transcripts, and the mutations in which cause numerous diseases. The details are provided in the following sections.

By using the S&S algorithm, mutations and genes that cause many different forms of cancer have been discovered. For example, genes causing commonly occurring cancers including breast cancer, ovarian cancer, colorectal cancer, leukemia, head and neck cancers, prostate cancer, retinoblastoma, squamous cell carcinoma, gastrointestinal cancer, melanoma, liver cancer, Lynch syndrome, skin cancer, and neurofibromatosis have been found.  In addition, splicing mutations in genes causing less commonly known cancers including gastric cancer, gangliogliomas, Li-Fraumeni syndrome, Loeys–Dietz syndrome, Osteochondromas (bone tumor), Nevoid basal cell carcinoma syndrome, and Pheochromocytomas have been identified.

Specific mutations in different splice sites in various genes causing breast cancer (e.g., BRCA1, PALB2), ovarian cancer (e.g.,  SLC9A3R1, COL7A1, HSD17B7), colon cancer (e.g., APC, MLH1, DPYD), colorectal cancer (e.g., COL3A1, APC, HLA-A), skin cancer (e.g., COL17A1, XPA, POLH), and Fanconi anemia (e.g., FANC, FANA) have been uncovered. The mutations in the donor and acceptor splice sites in different genes causing a variety of cancers that have been identified by S&S are shown in Table 1.

Table 1. Mutations in the donor and acceptor splice sites in different genes

Specific mutations in different splice sites in various genes that cause inherited disorders, including, for example, Type 1 diabetes (e.g., PTPN22, TCF1 (HCF-1A)), hypertension (e.g., LDL, LDLR, LPL), marfane syndrome (e.g., FBN1, TGFBR2, FBN2), cardiac diseases (e.g., COL1A2, MYBPC3, ACTC1), eye disorders (e.g., EVC, VSX1) have been uncovered. Few example mutations in the donor and acceptor splice sites in different genes causing a variety of inherited disorders identified using S&S are shown in Table 2.

Table 2. Mutations in the donor and acceptor splice sites in different genes causing inherited disorders

More than 100 immune system disorders affect humans, including inflammatory bowel diseases, multiple sclerosis, systemic lupus erythematosus, bloom syndrome, familial cold autoinflammatory syndrome, and dyskeratosis congenita. The Shapiro–Senapathy algorithm has been used to discover genes and mutations involved in many immune disorder diseases, including Ataxia telangiectasia, B-cell defects, Epidermolysis bullosa, and X-linked agammaglobulinemia.

Xeroderma pigmentosum, an autosomal recessive disorder is caused by faulty proteins formed due to new preferred splice donor site identified using S&S algorithm and resulted in defective nucleotide excision repair.

Type I Bartter syndrome (BS) is caused by mutations in the gene SLC12A1. S&S algorithm helped in disclosing the presence of two novel heterozygous mutations c.724 + 4A > G in intron 5 and c.2095delG in intron 16 leading to complete exon 5 skipping.

Mutations in the MYH gene, which is responsible for removing the oxidatively damaged DNA lesion are cancer-susceptible in the individuals. The IVS1+5C plays a causative role in the activation of a cryptic splice donor site and the alternative splicing in intron 1, S&S algorithm shows, guanine (G) at the position of IVS+5 is well conserved (at the frequency of 84%) among primates. This also supported the fact that the G/C SNP in the conserved splice junction of the MYH gene causes the alternative splicing of intron 1 of the β type transcript.

Splice site scores were calculated according to S&S to find EBV infection in X-linked lymphoproliferative disease. Identification of Familial tumoral calcinosis (FTC) is an autosomal recessive disorder characterized by ectopic calcifications and elevated serum phosphate levels and it is because of aberrant splicing.

Applying the S&S technology platform in modern clinical genomics research hasadvance diagnosis and treatment of human diseases.

In the modern era of Next Generation Sequencing (NGS) technology, S&S is applied in clinical practice extensively. Clinicians and molecular diagnostic laboratories apply S&S using various computational tools including HSF, SSF, and Alamut. It is aiding in the discovery of genes and mutations in patients whose disease are stratified or when the disease in a patient is unknown based on clinical investigations.

In this context, S&S has been applied on cohorts of patients in different ethnic groups with various cancers and inherited disorders. A few examples are given below.


Dr. Senapathy's original objective in developing a method for identifying splice sites was to find complete genes in raw uncharacterized genomic sequence that could be used in the human genome project. In the landmark paper with this objective, he described the basic method for identifying the splice sites within a given sequence based on the Position Weight Matrix (PWM) of the splicing sequences in different eukaryotic organism groups for the first time. He also created the first exon detection method by defining the basic characteristics of an exon as the sequence bounded by an acceptor and a donor splice sites that had S&S scores above a threshold, and by an ORF that was mandatory for an exon. An algorithm for finding complete genes based on the identified exons was also described by Dr. Senapathy for the first time.

Dr. Senapathy demonstrated that only deleterious mutations in the donor or acceptor splice sites that would drastically make the protein defective would reduce the splice site score (later known as the Shapiro–Senapathy score), and other non-deleterious variations would not reduce the score. The S&S method was adapted for researching the cryptic splice sites caused by mutations leading to diseases. This method for detecting deleterious splicing mutations in eukaryotic genes has been used extensively in disease research in the humans, animals and plants over the past three decades, as described above.  

The basic method for splice site identification, and for defining exons and genes was subsequently used by researchers in finding splice sites, exons and eukaryotic genes in a variety of organisms. These methods also formed the basis of all subsequent tools development for discovering genes in uncharacterized genomic sequences. It also was used in a different computational approaches including machine learning and neural network, and in alternative splicing research.

The Shapiro–Senapathy algorithm has been used to determine the various aberrant splicing mechanisms in genes due to deleterious mutations in the splice sites, which cause numerous diseases. Deleterious splice site mutations impair the normal splicing of the gene transcripts, and thereby make the encoded protein defective. A mutant splice site can become “weak” compared to the original site, due to which the mutated splice junction becomes unrecognizable by the spliceosomal machinery. This can lead to the skipping of the exon in the splicing reaction, resulting in the loss of that exon in the spliced mRNA (exon-skipping). On the other hand, a partial or complete intron could be included in the mRNA due to a splice site mutation that makes it unrecognizable (intron inclusion). A partial exon-skipping or intron inclusion can lead to premature termination of the protein from the mRNA, which will become defective leading to diseases. The S&S has thus paved the way to determine the mechanisms by which a deleterious mutation could lead to a defective protein, resulting in different diseases depending on which gene is affected.

An example of splicing aberration (exon skipping) caused by a mutation in the donor splice site in the exon 8 of MLH1 gene that led to colorectal cancer is given below. This example shows that a mutation in a splice site within a gene can lead to a profound effect in the sequence and structure of the mRNA, and the sequence, structure and function of the encoded protein, leading to disease.

The proper identification of splice sites has to be highly precise as the consensus splice sequences are very short and there are many other sequences similar to the authentic splice sites within gene sequences, which are known as cryptic, non-canonical, or pseudo splice sites. When an authentic or real splice site is mutated, any cryptic splice sites present close to the original real splice site could be erroneously used as authentic site, resulting in an aberrant mRNA. The erroneous mRNA may include a partial sequence from the neighboring intron or lose a partial exon, which may result in a premature stop codon. The result may be a truncated protein that would have lost its function completely.

Shapiro–Senapathy algorithm can identify the cryptic splice sites, in addition to the authentic splice sites. Cryptic sites can often be stronger than the authentic sites, with a higher S&S score. However, due to the lack of an accompanying complementary donor or acceptor site, this cryptic site will not be active or used in a splicing reaction. When a neighboring real site is mutated to become weaker than the cryptic site, then the cryptic site may be used instead of the real site, resulting in a cryptic exon and an aberrant transcript.

Numerous diseases have been caused by cryptic splice site mutations or usage of cryptic splice sites due to the mutations in authentic splice sites.

S&S has also been used in RNA splicing research in many animals and plants.

The mRNA splicing plays a fundamental role in gene functional regulation. Very recently, it has been shown that A to G conversions at splice sites can lead to mRNA mis-splicing in Arabidopsis. The splicing and exon–intron junction prediction coincided with the GT/AG rule (S&S) in the Molecular characterization and evolution of carnivorous sundew (Drosera rotundifolia L.) class V b-1,3-glucanase. Unspliced (LSDH) and spliced (SSDH) transcripts of NAD+ dependent sorbitol dehydroge nase (NADSDH) of strawberry (Fragaria ananassa Duch., cv. Nyoho) were investigated for phytohormonal treatments.

Ambra1 is a positive regulator of autophagy, a lysosome-mediated degradative process involved both in physiological and pathological conditions. Nowadays, this function of Ambra1 has been characterized only in mammals and zebrafish. Diminution of "rbm24a" or "rbm24b" gene products by morpholino knockdown resulted in significant disruption of somite formation in mouse and zebrafish. Dr.Senapathy algorithm used extensively to study intron-exon organization of fut8 genes.The intron-exon boundaries of "Sf"9 "fut8" were in agreement with the consensus sequence for the splicing donor and acceptor sites concluded using S&S.

The motivation for Dr. Senapathy to develop a method for the detection of splice junctions came from his split-gene theory. If primordial DNA sequences had a random nucleotide organization, the random distribution of stop codons would allow only very short Open Reading Frames (ORFs), as three stop codons out of 64 codons would result in an average ORF of ~60 bases. When Senapathy tested this in random DNA sequences, not only this was proven to be true, but the longest ORFs even in very long DNA sequences was found to be ~600 bases above which no ORFs existed. If so, a long coding sequence of even 1,200 bases (the average coding sequence length of genes from living organisms), and longer coding sequences of 6,000 bases (many of which occur in living organisms) will not occur in a primordial random sequence. Thus, genes had to occur in pieces in a split form, with short coding sequences (ORFs) that became exons, interrupted by very long random sequences that became introns. When the eukaryotic DNA was tested for ORF length distribution, it exactly matched that from random DNA, with very short ORFs that matched the lengths of exons, and very long introns as predicted, supporting the split gene theory.

If this split gene theory was true, then the ends of these ORFs that had a stop codon by nature would have become the ends of exons that would occur within introns, and that would define the splice junctions. When this hypothesis was tested, the almost all splice junctions in eukaryotic genes were found to contain stop codons exactly at the ends of introns, bordering the exons. In fact, these stop codons were found to form the “canonical” AG:GT splicing sequence, with the three stop codons occurring as part of the strong consensus signals. The Nobel Laureate Dr. Marshall Nirenberg, who deciphered the codons, stated that these findings strongly showed that the split gene theory for the origin of introns and the split structure of genes must be valid, and communicated the paper to the PNAS. New Scientist covered this publication in “A long explanation for introns”.

This basic split gene theory led to the hypothesis that the splice junctions originated from the stop codons.  Besides the codon CAG, only TAG, which is a stop codon, was found at the ends of introns. Surprisingly, all three stop codons (TGA, TAA and TAG) were found after one base (G) at the start of introns. These stop codons are shown in the consensus canonical donor splice junction as AG:GT(A/G)GGT, wherein the TAA and TGA are the stop codons, and the additional TAG is also present at this position. The canonical acceptor splice junction is shown as (C/T)AG:GT, in which TAG is the stop codon. These consensus sequence clearly show the presence of the stop codons at the ends of introns bordering the exons in all eukaryotic genes.  Dr. Marshall Nirenberg again stated that these observations fully supported the split gene theory for the origin of splice junction sequences from stop codons, who was the referee for this paper. New Scientist covered this publication in “Exons, Introns and Evolution”.

Dr. Senapathy wanted to detect the splice junctions in random DNA based on the consensus splice signal sequences, as he found that there were many sequences resembling splice sites that were not the real splice sites within genes. This Position Weight Matrix method turned out to be a highly accurate algorithm to detect the real splice sites and the cryptic sites in genes. He also formulated the first exon detection method, based on the requirement for splice junctions at the ends of exons, and the requirement for an Open Reading Frame that would contain the exon. This exon detection method also turned to be highly accurate, detecting most of the exons with few false positives and false negatives. He extended this approach to define a complete split gene in a eukaryotic genomic sequence. Thus, the PWM based algorithm turned out to be very sensitive to not only detect the real splice sites and cryptic sites, but also to detect mutated splice sites that are deleterious as opposed to non-deleterious splicing mutations.

The stop codons within splice junctions turned out to be the strongest bases in splice junctions of eukaryotic genes, when tested using the PWMs of the consensus sequences. In fact, it was shown that mutations in these bases were the cause of diseases compared to other bases, as these three of the four bases (base 1, 3 and 4) of the canonical AG:GT were part of the stop codons. Senapathy showed that, when these canonical bases were mutated, the splice site score became weak, causing splicing aberrations in the splicing process and translation of the mRNA (as described under the diseases section above). Although the value of the splice site detection method in discovering genes with splicing mutations that caused disease has been realized over the years, its importance in clinical medicine is increasingly realized in the Next Generation Sequencing era over the past five years, with its incorporation in several tools based on the S&S algorithm.

Dr. Senapathy is currently the President and CSO of Genome International Corporation (GIC), a genomics R&D company based in Madison, WI. His team has developed several databases and tools for the analysis of splice junctions, including EuSplice, AspAlt, ExDom and RoBust. AspAlt was commended by Biotechniques, which stated that it solved a difficult problem for scientists in the comparative analysis and visualization of alternative splicing across different genomes. GIC has most recently developed the clinical genomics analysis platform Genome Explorer.



</doc>
<doc id="61982153" url="https://en.wikipedia.org/wiki?curid=61982153" title="Magic state distillation">
Magic state distillation

Magic state distillation is a process that takes in multiple noisy quantum states and outputs a smaller number of more reliable quantum states. It is considered by many experts to be one of the leading proposals for achieving fault tolerant quantum computation. Magic state distillation has also been used to argue that quantum contextuality may be the "magic ingredient" responsible for the power of quantum computers. Magic state distillation was first proposed by Sergey Bravyi and Alexei Kitaev in 2005. A related proposal was given by Emanuel Knill in 2005.

Thanks to the Gottesman–Knill theorem, it is known that some quantum operations (operations in the Clifford algebra) can be perfectly simulated in polynomial time on a probabilistic classical computer. In order to achieve universal quantum computation, a quantum computer must be able to perform operations outside this set. Magic state distillation achieves this, in principle, by concentrating the usefulness of imperfect resources, represented by mixed states, into states that are conducive for performing operations that are difficult to simulate classically. 

A variety of magic state distillation routines with various advantages have been proposed since Bravyi and Kitaev's original protocol was published.

The Clifford group consists of a set of formula_1-qubit operations generated by the gates (where H is Hadamard and S is formula_2) called Clifford gates. The Clifford group generates stabilizer states which can be efficiently simulated classically, as shown by the Gottesman–Knill theorem. This set of gates with a non-Clifford operation is universal for quantum computation.

Magic states are purified from formula_1 copies of a mixed state formula_4. These states are typically provided via an ancilla to the circuit. The magic state (for the formula_5 gate) isformula_6 where formula_7. By combining (copies of) magic states with Clifford gates, can be used to make a non-Clifford gate. Since Clifford gates combined with a non-Clifford gate are universal for quantum computation, magic states combined with Clifford gates are also universal.

The first magic state distillation algorithm; invented by Sergey Bravyi and Alexei Kitaev is a follows.


</doc>
<doc id="26978338" url="https://en.wikipedia.org/wiki?curid=26978338" title="Gale–Shapley algorithm">
Gale–Shapley algorithm

In mathematics, economics, and computer science, the Gale–Shapley algorithm (also known as the deferred acceptance algorithm) is an algorithm for finding a solution to the stable matching problem, named for David Gale and Lloyd Shapley.
It takes polynomial time, and the time is linear in the size of the input to the algorithm. Depending on how it is used, it can find either the solution that is optimal for the participants on one side of the matching, or for the participants on the other side. It is a truthful mechanism from the point of view of the participants for whom it provides the optimal solution.

The stable matching problem, in its most basic form, takes as input equal numbers of two types of participants ( men and women, or medical students and internships, for example), and an ordering for each participant giving their preference for whom to be matched to among the participants of the other type. A matching is "not" stable if:

In other words, a matching is stable when there does not exist any match ("A", "B") which both prefer each other to their current partner under the matching.
A stable matching always exists, and the algorithmic problem solved by the Gale–Shapley algorithm is to find one.

In 1962, David Gale and Lloyd Shapley proved that, for any equal number of men and women, it is always possible to solve the SMP and make all marriages stable. They presented an algorithm to do so.

The Gale–Shapley algorithm involves a number of "rounds" (or "iterations"):

The runtime complexity of this algorithm is formula_1 where formula_2 is the number of men or women.

This algorithm guarantees that:


 algorithm stable_matching is

The existence of different stable matchings raises the question: which matching is returned by the Gale-Shapley algorithm? Is it the matching better for men, for women, or the intermediate one? In the above example, the algorithm converges in a single round on the men-optimal solution because each woman receives exactly one proposal, and therefore selects that proposal as her best choice, ensuring that each man has an accepted offer, ending the match.

This is a general fact: the Gale-Shapley algorithm in which men propose to women "always" yields a stable matching that is the "best for all men" among all stable matchings. Similarly, if the women propose then the resulting matching is the "best for all women" among all stable matchings. These two matchings are the top and bottom elements of a larger structure on all stable matchings, the lattice of stable matchings.

To see this, consider the illustration at the right. Let A be the matching generated by the men-proposing algorithm, and B an alternative stable matching that is better for at least one man, say "m". Suppose "m" is matched in B to a woman "w", which he prefers to his match in A. This means that in A, "m" has visited "w", but she rejected him (this is denoted by the green arrow from "m" to "w"). "w" rejected him in favor of some man that she prefers, say "m". So in B, "w" is matched to "m" but "yearns" (wants but unmatched) to "m" (this is denoted by the red arrow from "w" to "m").

Since B is a stable matching, "m" must be matched in B to some woman he prefers to "w", say "w". This means that in A, "m" has visited "w" before arriving at "w", which means that "w" has rejected him. By similar considerations, and since the graph is finite, we must eventually have a directed cycle in which each man was rejected in A by the next woman in the cycle, who rejected him in favor of the next man in the cycle. But this is impossible since such "cycle of rejections" cannot start anywhere: suppose by contradiction that it starts at e.g. "m" - the first man rejected by his adjacent woman ("w"). By assumption, this rejection happens only after "m" comes to "w". But this can happen only after "w" rejects "m" - contradiction to "m" being the first.

The GS algorithm is a truthful mechanism from the point of view of men (the proposing side). I.e, no man can get a better matching for himself by misrepresenting his preferences. Moreover, the GS algorithm is even "group-strategy proof" for men, i.e., no coalition of men can coordinate a misrepresentation of their preferences such that all men in the coalition are strictly better-off. However, it is possible for some coalition to misrepresent their preferences such that some men are better-off and the other men retain the same partner.

The GS algorithm is non-truthful for the women (the reviewing side): each woman may be able to misrepresent her preferences and get a better match.





</doc>
<doc id="31818344" url="https://en.wikipedia.org/wiki?curid=31818344" title="Irish logarithm">
Irish logarithm

Irish logarithms were a system of number manipulation invented by Percy Ludgate for machine multiplication. The system used a combination of mechanical cams as look-up tables and mechanical addition to sum pseudo-logarithmic indices to produce partial products, which were then added to produce results. The technique is similar to Zech logarithms (also known as Jacobi logarithms), but uses a system of indices original to Ludgate.

Ludgate's algorithm compresses the multiplication of two single decimal numbers into two table lookups (to convert the digits into indices), the addition of the two indices to create a new index which is input to a second lookup table that generates the output product. Because both lookup tables are one-dimensional, and the addition of indices is simple to implement mechanically, this allows a less complex mechanism than would be needed to implement a two-dimensional 10x10 multiplication lookup table.

The following is an implementation of Ludgate's Irish logarithms algorithm in Python:

Table 1 is taken from Ludgate's original paper; given the first table, the contents of Table 2 can be trivially derived from Table 1 and the definition of the algorithm. Note that the last third of the second table is entirely zeros; this can potentially be exploited to further simplify a mechanical implementation.




</doc>
<doc id="47893974" url="https://en.wikipedia.org/wiki?curid=47893974" title="Behavior selection algorithm">
Behavior selection algorithm

In artificial intelligence, a behavior selection algorithm, or action selection algorithm, is an algorithm that selects appropriate behaviors or actions for one or more intelligent agents. In game artificial intelligence, it selects behaviors or actions for one or more non-player characters. Common behavior selection algorithms include:


In application programming, run-time selection of the behavior of a specific method is referred to as the strategy design pattern.



</doc>
<doc id="6008483" url="https://en.wikipedia.org/wiki?curid=6008483" title="Convolution reverb">
Convolution reverb

In audio signal processing, convolution reverb is a process used for digitally simulating the reverberation of a physical or virtual space through the use of software profiles; a piece of software (or algorithm) that creates a simulation of an audio environment. It is based on the mathematical convolution operation, and uses a pre-recorded audio sample of the impulse response of the space being modeled. To apply the reverberation effect, the impulse-response recording is first stored in a digital signal-processing system. This is then convolved with the incoming audio signal to be processed. 

An impulse response is a recording of the reverberation that is caused by an acoustic space when an ideal impulse is played. However, an ideal impulse is a mathematical construct, and cannot exist in reality, as it would have to be infinitesimally narrow in time. Therefore, approximations have to be used: the sound of an electric spark, starter pistol shot or the bursting of a balloon, for instance. A recording of this approximated ideal impulse may be used directly as an impulse response. Techniques involving starter pistols and balloons are sometimes referred to as transient methods, and the response is contained at the beginning of the recording in an impulse. 

Another technique, referred to as the sine sweep method, covers the entire audible frequency range, which can result in a broader-range, and higher-quality, impulse response. This involves the use of a longer sound to excite a space (typically a sine sweep), which is then put through a process of deconvolution to produce an impulse response. This approach has the advantage that such sounds are less susceptible to distortion; however, it requires more sophisticated processing to produce a usable impulse response. 

A third approach involves using maximum-length sequences. This uses a constant-power signal instead of an impulse, so does not require as much dynamic range when recording.

The transfer function (or frequency response) of a system can be measured using any sound that covers the frequency spectrum. For example, to sample the acoustic properties of a larger space such as a small church or cathedral, the space can simply be excited using white noise, with the result recorded both near the source, and somewhere else in the space.

The coefficients of a finite impulse response can then be generated as the inverse Fourier Transform of the cross-correlation of the output of the system with the auto-correlation of the input to the system. This is difficult in practice because such sequences are highly susceptible to distortion.

The primary goal of a convolution reverb is to sample real spaces, in order to simulate the acoustics of the sampled space. A straightforward and simple mono example of capturing an impulse response would be to set up a microphone in a concert hall and to place the microphone in the centre of the auditorium. Next, produce a very brief pulse (often an electric spark) of sound, and record everything that the microphone picks up, which includes both the original sound and the response of the room to it. The recorded take would then be cleanly edited and loaded into the convolution processor. This convolution can be applied as part of a signal processing chain.

It is also possible to sample the impulse response of a reverberation unit, instead of sampling a real space. Thus, it is possible to use a convolution reverb in place of a hardware machine. The techniques used to sample a reverberation unit are the same as the ones used to sample real spaces.

In electronic music convolution is the imposition of a spectral or rhythmic structure on a sound. Often this envelope or structure is taken from another sound. The convolution of two signals is the filtering of one through the other. See applications of convolution



</doc>
<doc id="63087276" url="https://en.wikipedia.org/wiki?curid=63087276" title="Plotting algorithms for the Mandelbrot set">
Plotting algorithms for the Mandelbrot set

]
There are many programs and algorithms used to plot the Mandelbrot set and other fractals, some of which are described in fractal-generating software. These programs use a variety of algorithms to determine the color of individual pixels efficiently. 

=Escape time algorithm=

The simplest algorithm for generating a representation of the Mandelbrot set is known as the "escape time" algorithm. A repeating calculation is performed for each "x", "y" point in the plot area and based on the behavior of that calculation, a color is chosen for that pixel.

In both the unoptimized and optimized escape time algorithms, the "x" and "y" locations of each point are used as starting values in a repeating, or iterating calculation (described in detail below). The result of each iteration is used as the starting values for the next. The values are checked during each iteration to see whether they have reached a critical "escape" condition, or "bailout". If that condition is reached, the calculation is stopped, the pixel is drawn, and the next "x", "y" point is examined. For some starting values, escape occurs quickly, after only a small number of iterations. For starting values very close to but not in the set, it may take hundreds or thousands of iterations to escape. For values within the Mandelbrot set, escape will never occur. The programmer or user must choose how many iterations–or how much "depth"–they wish to examine. The higher the maximal number of iterations, the more detail and subtlety emerge in the final image, but the longer time it will take to calculate the fractal image.

Escape conditions can be simple or complex. Because no complex number with a real or imaginary part greater than 2 can be part of the set, a common bailout is to escape when either coefficient exceeds 2. A more computationally complex method that detects escapes sooner, is to compute distance from the origin using the Pythagorean theorem, i.e., to determine the absolute value, or "modulus", of the complex number. If this value exceeds 2, or equivalently, when the sum of the squares of the real and imaginary parts exceed 4, the point has reached escape. More computationally intensive rendering variations include the Buddhabrot method, which finds escaping points and plots their iterated coordinates.

The color of each point represents how quickly the values reached the escape point. Often black is used to show values that fail to escape before the iteration limit, and gradually brighter colors are used for points that escape. This gives a visual representation of how many cycles were required before reaching the escape condition.

To render such an image, the region of the complex plane we are considering is subdivided into a certain number of pixels. To color any such pixel, let formula_1 be the midpoint of that pixel. We now iterate the critical point 0 under formula_2, checking at each step whether the orbit point has modulus larger than 2. When this is the case, we know that formula_1 does not belong to the Mandelbrot set, and we color our pixel according to the number of iterations used to find out. Otherwise, we keep iterating up to a fixed number of steps, after which we decide that our parameter is "probably" in the Mandelbrot set, or at least very close to it, and color the pixel black.

In pseudocode, this algorithm would look as follows. The algorithm does not use complex numbers and manually simulates complex-number operations using two real numbers, for those who do not have a complex data type. The program may be simplified if the programming language includes complex-data-type operations.

Here, relating the pseudocode to formula_1, formula_5 and formula_2:
and so, as can be seen in the pseudocode in the computation of "x" and "y":

To get colorful images of the set, the assignment of a color to each value of the number of executed iterations can be made using one of a variety of functions (linear, exponential, etc.). One practical way, without slowing down calculations, is to use the number of executed iterations as an entry to a palette initialized at startup. If the color table has, for instance, 500 entries, then the color selection is "n" mod 500, where "n" is the number of iterations.

The code in the previous section uses an unoptimized inner while loop for clarity. In the unoptimized version, one must perform five multiplications per iteration. To reduce the number of multiplications the following code for the inner while loop may be used instead:

The above code works via some algebraic simplification of the complex multiplication:

formula_12

Using the above identity, the number of multiplications can be reduced to three instead of five.

The above inner while loop can be further optimized by expanding "w" to
formula_13

Substituting "w" into formula_14 yields
formula_15
and hence calculating "w" is no longer needed.

The further optimized pseudocode for the above is:

Note that in the above pseudocode, formula_16 seems, at the surface, to increase the number of multiplications by 1, but, since 2 is the multiplier, the code can be optimized via a left bit-shift or formula_17.

=Coloring algorithms=

In addition to plotting the set, a variety of algorithms have been developed to efficiently color the set in an aesthetically pleasing way.

A more complex coloring method involves using a histogram which pairs each pixel with said pixel's maximum iteration count before escape / bailout . This method will equally distribute colors to the same overall area, and, importantly, is independent of the maximum number of iterations chosen.

This algorithm has four passes. The first pass involves calculating the iteration counts associated with each pixel (but without any pixels being plotted). These are stored in an array which we'll call IterationCounts[x][y], where x and y are the x and y coordinates of said pixel on the screen respectively.

The first step of the second pass is to create an array of size "n", which is the maximum iteration count. We'll call that array NumIterationsPerPixel . Next, one must iterate over the array of pixel-iteration count pairs, IterationCounts[][], and retrieve each pixel's saved iteration count, "i", via e.g. "i" = IterationCounts[x][y]. After each pixel's iteration count "i" is retrieved, it is necessary to index the NumIterationsPerPixel by "i" and increment the indexed value (which is initially zero) -- e.g. NumIterationsPerPixel["i"] = NumIterationsPerPixel["i"] + 1 .

The third pass iterates through the NumIterationsPerPixel array and adds up all the stored values, saving them in "total". The array index represents the number of pixels that reached that iteration count before bailout. 

After this, the fourth pass begins and all the values in the IterationCounts array are indexed, and, for each iteration count "i", associated with each pixel, the count is added to a global sum of all the iteration counts from 1 to "i" in the NumIterationsPerPixel array. . This value is then normalized by dividing the sum by the "total" value computed earlier.

Finally, the computed value is used, e.g. as an index to a color palette.

This method may be combined with the smooth coloring method below for more aesthetically pleasing images.

The escape time algorithm is popular for its simplicity. However, it creates bands of color, which, as a type of aliasing, can detract from an image's aesthetic value. This can be improved using an algorithm known as "normalized iteration count", which provides a smooth transition of colors between iterations. The algorithm associates a real number formula_18 with each value of "z" by using the connection of the iteration number with the potential function. This function is given by

where "z" is the value after "n" iterations and "P" is the power for which "z" is raised to in the Mandelbrot set equation ("z" = "z" + "c", "P" is generally 2).

If we choose a large bailout radius "N" (e.g., 10), we have that

for some real number formula_21, and this is

and as "n" is the first iteration number such that |"z"| > "N", the number we subtract from "n" is in the interval <nowiki>[0, 1)</nowiki>.

For the coloring we must have a cyclic scale of colors (constructed mathematically, for instance) and containing "H" colors numbered from 0 to "H" − 1 ("H" = 500, for instance). We multiply the real number formula_21 by a fixed real number determining the density of the colors in the picture, take the integral part of this number modulo "H", and use it to look up the corresponding color in the color table.

For example, modifying the above pseudocode and also using the concept of linear interpolation would yield
=Advanced plotting algorithms=
In addition to the simple and slow escape time algorithms already discussed, there are many other more advanced algorithms that can be used to speed up the plotting process.

One can compute the distance from point "c" (in exterior or interior) to nearest point on the boundary of the Mandelbrot set.

The proof of the connectedness of the Mandelbrot set in fact gives a formula for the uniformizing map of the complement of formula_24 (and the derivative of this map). By the Koebe quarter theorem, one can then estimate the distance between the midpoint of our pixel and the Mandelbrot set up to a factor of 4.

In other words, provided that the maximal number of iterations is sufficiently high, one obtains a picture of the Mandelbrot set with the following properties:

The distance estimate "b" of a pixel "c" (a complex number) from the Mandelbrot set is given by

where 

The idea behind this formula is simple: When the equipotential lines for the potential function formula_37 lie close, the number formula_38 is large, and conversely, therefore the equipotential lines for the function formula_39 should lie approximately regularly.

From a mathematician's point of view, this formula only works in limit where "n" goes to infinity, but very reasonable estimates can be found with just a few additional iterations after the main loop exits.

Once "b" is found, by the Koebe 1/4-theorem, we know that there is no point of the Mandelbrot set with distance from "c" smaller than "b/4".

The distance estimation can be used for drawing of the boundary of the Mandelbrot set, see the article Julia set. In this approach, pixels that are sufficiently close to M are drawn using a different color. This creates drawings where the thin "filaments" of the Mandelbrot set can be easily seen. This technique is used to good effect in the B&W images of Mandelbrot sets in the books "The Beauty of Fractals" and "The Science of Fractal Images".

Here is a sample B&W image rendered using Distance Estimates:
Distance Estimation can also be used to render 3D images of Mandelbrot and Julia sets

It is also possible to estimate the distance of a limitly periodic (i.e., inner) point to the boundary of the Mandelbrot set. The estimate is given by

where

Analogous to the exterior case, once "b" is found, we know that all points within the distance of "b"/4 from "c" are inside the Mandelbrot set.

There are two practical problems with the interior distance estimate: first, we need to find formula_49 precisely, and second, we need to find formula_41 precisely.
The problem with formula_49 is that the convergence to formula_49 by iterating formula_43 requires, theoretically, an infinite number of operations.
The problem with any given formula_41 is that, sometimes, due to rounding errors, a period is falsely identified to be an integer multiple of the real period (e.g., a period of 86 is detected, while the real period is only 43=86/2). In such case, the distance is overestimated, i.e., the reported radius could contain points outside the Mandelbrot set.
One way to improve calculations is to find out beforehand whether the given point lies within the cardioid or in the period-2 bulb. Before passing the complex value through the escape time algorithm, first check that:

where "x" represents the real value of the point and "y" the imaginary value. The first two equations determine that the point is within the cardioid, the last the period-2 bulb.

The cardioid test can equivalently be performed without the square root:

3rd- and higher-order buds do not have equivalent tests, because they are not perfectly circular. However, it is possible to find whether the points are within circles inscribed within these higher-order bulbs, preventing many, though not all, of the points in the bulb from being iterated.

To prevent having to do huge numbers of iterations for points inside the set, one can perform periodicity checking. Check whether a point reached in iterating a pixel has been reached before. If so, the pixel cannot diverge and must be in the set.

Periodicity checking is, of course, a trade-off. The need to remember points costs memory and "data management" instructions, whereas it saves "computational" instructions.

However, checking against only one previous iteration can detect many periods with little performance overhead. For example, within the while loop of the pseudocode above, make the following modifications:

The above code stores away a new x and y value on every 20:th iteration, thus it can detect periods that are up to 20 points long.

It can be shown that if a solid shape can be drawn on the Mandelbrot set, with all the border colors being the same, then the shape can be filled in with that color. This is a result of the Mandelbrot set being simply connected. Border tracing works by following the lemniscates of the various iteration levels (colored bands) all around the set, and then filling the entire band at once. This can be a good speed increase, because it means that large numbers of points can be skipped. Note that border tracing can't be used to identify bands of pixels outside the set if the plot computes DE (Distance Estimate) or potential (fractional iteration) values.

Border tracing is especially beneficial for skipping large areas of a plot that are parts of the Mandelbrot set (in M), since determining that a pixel is in M requires computing the maximum number of iterations.

Below is an example of a Mandelbrot set rendered using border tracing:

This is a 400x400 pixel plot using simple escape-time rendering with a maximum iteration count of 1000 iterations. It only had to compute 6.84% of the total iteration count that would have been required without border tracing. It was rendered using a slowed-down rendering engine to make the rendering process slow enough to watch, and took 6.05 seconds to render. The same plot took 117.0 seconds to render with border tracing turned off with the same slowed-down rendering engine.

Note that even when the settings are changed to calculate fractional iteration values (which prevents border tracing from tracing non-Mandelbrot points) the border tracing algorithm still renders this plot in 7.10 seconds because identifying Mandelbrot points always requires the maximum number of iterations. The higher the maximum iteration count, the more costly it is to identify Mandelbrot points, and thus the more benefit border tracing provides.

That is, even if the outer area uses smooth/continuous coloring then border tracing will still speed up the costly inner area of the Mandelbrot set. Unless the inner area also uses some smooth coloring method, for instance interior distance estimation.

An older and simpler to implement method than border tracing is to use rectangles. There are several variations of the rectangle method. All of them are slower than border tracing because they end up calculating more pixels.

The basic method is to calculate the border pixels of a box of say 8x8 pixels. If the entire box border has the same color, then just fill in the 36 pixels (6x6) inside the box with the same color, instead of calculating them. (Mariani's algorithm.)

A faster and slightly more advanced variant is to first calculate a bigger box, say 25x25 pixels. If the entire box border has the same color, then just fill the box with the same color. If not, then split the box into four boxes of 13x13 pixels, reusing the already calculated pixels as outer border, and sharing the inner "cross" pixels between the inner boxes. Again, fill in those boxes that has only one border color. And split those boxes that don't, now into four 7x7 pixel boxes. And then those that "fail" into 4x4 boxes. (Mariani-Silver algorithm.)

Even faster is to split the boxes in half instead of into four boxes. Then it might be optimal to use boxes with a 1.4:1 aspect ratio, so they can be split like how A3 papers are folded into A4 and A5 papers. (The DIN approach.)

One variant just calculates the corner pixels of each box. However this causes damaged pictures more often than calculating all box border pixels. Thus it only works reasonably well if only small boxes of say 6x6 pixels are used, and no recursing in from bigger boxes. (Fractint method.)

As with border tracing, rectangle checking only works on areas with one discrete color. But even if the outer area uses smooth/continuous coloring then rectangle checking will still speed up the costly inner area of the Mandelbrot set. Unless the inner area also uses some smooth coloring method, for instance interior distance estimation.

The horizontal symmetry of the Mandelbrot set allows for portions of the rendering process to be skipped upon the presence of the real axis in the final image. However, regardless of the portion that gets mirrored, the same number of points will be rendered.

Julia sets have symmetry around the origin. This means that quadrant 1 and quadrant 3 are symmetric, and quadrants 2 and quadrant 4 are symmetric. Supporting symmetry for both Mandelbrot and Julia sets requires handling symmetry differently for the two different types of graphs.

Escape-time rendering of Mandelbrot and Julia sets lends itself extremely well to parallel processing. On multi-core machines the area to be plotted can be divided into a series of rectangular areas which can then be provided as a set of tasks to be rendered by a pool of rendering threads. This is an embarrassingly parallel computing problem. (Note that one gets the best speed-up by first excluding symmetric areas of the plot, and then dividing the remaining unique regions into rectangular areas.)

Here is a short video showing the Mandelbrot set being rendered using multithreading and symmetry, but without boundary following:

Finally, here is a video showing the same Mandelbrot set image being rendered using multithreading, symmetry, and boundary following:

Very highly magnified images require more than the standard 64–128 or so bits of precision that most hardware floating-point units provide, requiring renderers to use slow "BigNum" or "arbitrary-precision" math libraries to calculate. However, this can be sped up by the exploitation of perturbation theory. Given

as the iteration, and a small epsilon and delta, it is the case that

or

so if one defines

one can calculate a single point (e.g. the center of an image) using high-precision arithmetic ("z"), giving a "reference orbit", and then compute many points around it in terms of various initial offsets delta plus the above iteration for epsilon, where epsilon-zero is set to 0. For most iterations, epsilon does not need more than 16 significant figures, and consequently hardware floating-point may be used to get a mostly accurate image. There will often be some areas where the orbits of points diverge enough from the reference orbit that extra precision is needed on those points, or else additional local high-precision-calculated reference orbits are needed. By measuring the orbit distance between the reference point and the point calculated with low precision, it can be detected that it is not possible to calculate the point correctly, and the calculation can be stopped. These incorrect points can later be re-calculated e.g. from another closer reference point.

Further, it is possible to approximate the starting values for the low-precision points with a truncated Taylor series, which often enables a significant amount of iterations to be skipped.
Renderers implementing these techniques are publicly available and offer speedups for highly magnified images by around two orders of magnitude.

An alternate explanation of the above:

For the central point in the disc formula_76 and its iterations formula_77, and an arbitrary point in the disc formula_78 and its iterations formula_79, it is possible to define the following iterative relationship:

With formula_81. Successive iterations of formula_82 can be found using the following:

Now from the original definition:

It follows that:

As the iterative relationship relates an arbitrary point to the central point by a very small change formula_89, then most of the iterations of formula_82 are also small and can be calculated using floating point hardware.

However, for every arbitrary point in the disc it is possible to calculate a value for a given formula_91 without having to iterate through the sequence from formula_92, by expressing formula_82 as a power series of formula_89.

With formula_96.

Now given the iteration equation of formula_97, it is possible to calculate the coefficients of the power series for each formula_82:

Therefore, it follows that:

The coefficients in the power series can be calculated as iterative series using only values from the central point's iterations formula_106, and do not change for any arbitrary point in the disc. If formula_89 is very small, formula_82 should be calculable to sufficient accuracy using only a few terms of the power series. As the Mandelbrot Escape Contours are 'continuous' over the complex plane, if a points escape time has been calculated, then the escape time of that points neighbours should be similar. Interpolation of the neighbouring points should provide a good estimation of where to start in the formula_82 series.

Further, separate interpolation of both real axis points and imaginary axis points should provide both an upper and lower bound for the point being calculated. If both results are the same (i.e. both escape or dot not escape) then the difference formula_110 can be used to recuse until both an upper and lower bound can be established. If floating point hardware can be used to iterate the formula_97 series, then there exists a relation between how many iterations can be achieved in the time it takes to use BigNum software to compute a given formula_82. If the difference between the bounds is greater than the number of iterations, it is possible to perform binomial search using BigNum software, successively halving the gap until it becomes more time efficient to find the escape value using floating point hardware.

=References=


</doc>
<doc id="63148459" url="https://en.wikipedia.org/wiki?curid=63148459" title="Sliding DFT">
Sliding DFT

The sliding DFT is a recursive algorithm to compute
successive STFTs of input data frames that are a single sample
apart (hopsize-1).

Starting with a DFT at time n,

formula_1

The DFT for time n+1 can be computed as

formula_2

and recursively thereafter as

formula_3

with

formula_4


</doc>
<doc id="23515853" url="https://en.wikipedia.org/wiki?curid=23515853" title="Collective operation">
Collective operation

Collective operations are building blocks for interaction patterns, that are often used in SPMD algorithms in the parallel programming context. Hence, there is an interest in efficient realizations of these operations.

A realization of the collective operations is provided by the Message Passing Interface (MPI).

In all asymptotic runtime functions, we denote the latency formula_1, the communication cost per word formula_2, the number of processing units formula_3 and the input size per node formula_4. In cases where we have initial messages on more than one node we assume that all local messages are of the same size. To address individual processing units we use formula_5. 

If we do not have an equal distribution, i.e. node formula_6 has a message of size formula_7, we get an upper bound for the runtime by setting formula_8. 

A distributed memory model is assumed. The concepts are similar for the shared memory model. However, shared memory systems can provide hardware support for some operations like broadcast () for example, which allows convenient concurrent read . Thus, new algorithmic possibilities can become available.

The broadcast pattern is used to distribute data from one processing unit to all processing units, which is often needed in SPMD parallel programs to dispense input or global values. Broadcast can be interpreted as an inverse version of the reduce pattern (). Initially only root formula_9 with formula_10 formula_11 stores message formula_12. During broadcast formula_12 is sent to the remaining processing units, so that eventually formula_12 is available to all processing units.

Since an implementation by means of a sequential for-loop with formula_15 iterations becomes a bottleneck, divide-and-conquer approaches are common. One possibility is to utilize a binomial tree structure with the requirement that formula_3 has to be a power of two. When a processing unit is responsible for sending formula_12 to processing units formula_18, it sends formula_12 to processing unit formula_20 and delegates responsibility for the processing units formula_21 to it, while its own responsibility is cut down to formula_22.

Binomial trees have a problem with long messages formula_12. The receiving unit of formula_12 can only propagate the message to other units, after it received the whole message. In the meantime, the communication network is not utilized. Therefore pipelining on binary trees is used, where formula_12 is split into an array of formula_26 packets of size formula_27. The packets are then broadcast one after another, so that data is distributed fast in the communication network.

Pipelined broadcast on balanced binary tree is possible in formula_28.

The reduce pattern is used to collect data or partial results from different processing units and to combine them into a global result by a chosen operator. Reduction can be seen as an inverse version of broadcast (). Given formula_3 processing units, message formula_30 is on processing unit formula_6 initially. All formula_30 are aggregated by formula_33 and the result is eventually stored on formula_34. The reduction operator formula_33 must be associative at least. Some algorithms require a commutative operator with a neutral element. Operators like formula_36, formula_37, formula_38 are common.

Since reduce can be interpreted as an inverse broadcast, equal implementation considerations apply (). For pipelining on binary trees the message must be representable as a vector of smaller object for component-wise reduction.

Pipelined reduce on a balanced binary tree is possible in formula_28.

The all-reduce pattern is used, if the result of a reduce operation () must be distributed to all processing units. Given formula_3 processing units, message formula_30 is on processing unit formula_6 initially. All formula_30 are aggregated by an operator formula_33 and the result is eventually stored on all formula_6. Analog to the reduce operation, the operator formula_33 must be at least associative.

All-reduce can be interpreted as a reduce operation with a subsequent broadcast (). For long messages a corresponding implementation is suitable, whereas for short messages, the latency can be reduced by using a hypercube () topology, if formula_3 is a power of two.

All-reduce is possible in formula_28, since reduce and broadcast are possible in formula_28 with pipelining on balanced binary trees.

The prefix-sum or scan operation is used to collect data or partial results from different processing units and to compute intermediate results by an operator, which are stored on those processing units. It can be seen as a generalization of the reduce operation (). Given formula_3 processing units, message formula_30 is on processing unit formula_6. The operator formula_33 must be at least associative, whereas some algorithms require also a commutative operator and a neutral element. Common operators are formula_36, formula_37 and formula_38. Eventually processing unit formula_6 stores the prefix sum formula_58formula_59. In the case of the so-called exclusive prefix sum, processing unit formula_6 stores the prefix sum formula_61formula_59. Some algorithms require to store the overall sum at each processing unit in addition to the prefix sums.

For short messages, this can be achieved with a hypercube topology if formula_3 is a power of two. For long messages, the hypercube (, ) topology is not suitable, since all processing units are active in every step and therefore pipelining can't be used. A binary tree topology is better suited for arbitrary formula_3 and long messages ().

Prefix-sum on a binary tree can be implemented with an upward and downward phase. In the upward phase reduction is performed, while the downward phase is similar to broadcast, where the prefix sums are computed by sending different data to the left and right children. With this approach pipelining is possible, because the operations are equal to reduction () and broadcast ().

Pipelined prefix sum on a binary tree is possible in formula_28.
The barrier as a collective operation is a generalization of the concept of a barrier, that can be used in distributed computing. When a processing unit calls barrier, it waits until all other processing units have called barrier as well. Barrier is thus used to achieve global synchronization in distributed computing.

One way to implement barrier is to call all-reduce () with an empty/ dummy operand. We know the runtime of All-reduce is formula_66. Using a dummy operand reduces size formula_4 to a constant factor and leads to a runtime of formula_68.

The gather communication pattern is used to store data from all processing units on a single processing unit. Given formula_3 processing units, message formula_30 on processing unit formula_6. For a fixed processing unit formula_72, we want to store the message formula_73 on formula_72. Gather can be thought of as a reduce operation () that uses the concatenation operator. This works due to the fact that concatenation is associative. By using the same binomial tree reduction algorithm we get a runtime of formula_75. We see that the asymptotic runtime is similar to the asymptotic runtime of reduce formula_28, but with the addition of a factor p to the term formula_77. This additional factor is due to the message size increasing in each step as messages get concatenated. Compare this to reduce where message size is a constant for operators like formula_37.

The all-gather communication pattern is used to collect data from all processing units and to store the collected data on all processing units. Given formula_3 processing units formula_6, message formula_30 initially stored on formula_6, we want to store the message formula_73 on each formula_72.

It can be thought of in multiple ways. The first is as an all-reduce operation () with concatenation as the operator, in the same way that gather can be represented by reduce. The second is as a gather-operation followed by a broadcast of the new message of size formula_85. With this we see that all-gather in formula_75 is possible.

The scatter communication pattern is used to distribute data from one processing unit to all the processing units. It differs from broadcast, in that it does not send the same message to all processing units. Instead it splits the message and delivers one part of it to each processing unit.

Given formula_3 processing units formula_6, a fixed processing unit formula_72 that holds the message formula_90. We want to transport the message formula_30 onto formula_6. The same implementation concerns as for gather () apply. This leads to an optimal runtime in formula_75. 

All-to-all is the most general communication pattern. For formula_94, message formula_95 is the message that is initially stored on node formula_96 and has to be delivered to node formula_97. We can express all communication primitives that do not use operators through all-to-all. For example, broadcast of message formula_12 from node formula_99 is emulated by setting formula_100 for formula_101 and setting formula_102 empty for formula_103.

Assuming we have a fully connected network, the best possible runtime for all-to-all is in formula_104 . This is achieved through formula_3 rounds of direct message exchange. For formula_3 power of 2, in communication round formula_26 , node formula_6 exchanges messages with node formula_109 .

If the message size is small and latency dominates the communication, a hypercube algorithm can be used to distribute the messages in time formula_110 .

This table gives an overview over the best known asymptotic runtimes, assuming we have free choice of network topology.

Example topologies we want for optimal runtime are binary tree, binomial tree, hypercube.

In practice, we have to adjust to the available physical topologies, e.g. dragonfly, fat tree, grid network (references other topologies, too).

More information under Network topology.

For each operation, the optimal algorithm can depend on the input sizes formula_4. For example, broadcast for short messages is best implemented using a binomial tree whereas for long messages a pipelined communication on a balanced binary tree is optimal.

The complexities stated in the table depend on the latency formula_1 and the communication cost per word formula_2 in addition to the number of processing units formula_3 and the input message size per node formula_4. The "# senders" and "# receivers" columns represent the number of senders and receivers that are involved in the operation respectively. The "# messages" column lists the number of input messages and the "Computations?" column indicates if any computations are done on the messages or if the messages are just delivered without processing. "Complexity" gives the asymptotic runtime complexity of an optimal implementation under free choice of topology.


</doc>
<doc id="63442371" url="https://en.wikipedia.org/wiki?curid=63442371" title="Regulation of algorithms">
Regulation of algorithms

Regulation of algorithms, or algorithmic regulation, is the creation of laws, rules and public sector policies for promotion and regulation of algorithms, particularly in artificial intelligence and machine learning. For the subset of AI algorithms, the term regulation of artificial intelligence is used. The regulatory and policy landscape for artificial intelligence (AI) is an emerging issue in jurisdictions globally, including in the European Union. Regulation of AI is considered necessary to both encourage AI and manage associated risks, but challenging. Another emerging topic is the regulation of blockchain algorithms and is mentioned alongside with regulation of AI algorithms. Many countries have enacted regulations of high frequency trades, which is shifting due to technological progress into the realm of AI algorithms. 

The motivation for regulation of algorithms is the apprehension of losing control over the algorithms, whose impact on human life increases. Multiple countries have already introduced regulations in case of automated credit score calculation—right to explanation is mandatory for those algorithms. 

In 2017 Elon Musk advocated regulation of algorithms in the context of the existential risk from artificial general intelligence. According to NPR, the Tesla CEO was "clearly not thrilled" to be advocating for government scrutiny that could impact his own industry, but believed the risks of going completely without oversight are too high: "Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation." 

In response, some politicians expressed skepticism about the wisdom of regulating a technology that is still in development. Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO Brian Krzanich has argued that artificial intelligence is in its infancy and that it is too early to regulate the technology. Instead of trying to regulate the technology itself, some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty. One suggestion has been for the development of a global governance board to regulate AI development. In 2020, the European Union published its draft strategy paper for promoting and regulating AI.

AI law and regulations can be divided into three main topics, namely governance of autonomous intelligence systems, responsibility and accountability for the systems, and privacy and safety issues. The development of public sector strategies for management and regulation of AI has been increasingly deemed necessary at the local, national, and international levels and in a variety of fields, from public service management to law enforcement, the financial sector, robotics, the military, and international law.

In the United States, on January 7, 2019, following an Executive Order on 'Maintaining American Leadership in Artificial Intelligence', the White House’s Office of Science and Technology Policy released a draft "Guidance for Regulation of Artificial Intelligence Applications", which includes ten principles for United States agencies when deciding whether and how to regulate AI. In response, the National Institute of Standards and Technology has released a position paper, the National Security Commission on Artificial Intelligence has published an interim report, and the Defense Innovation Board has issued recommendations on the ethical use of AI. 

In 2016, China published a position paper questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U.N. Security Council to broach the issue, and leading to proposals for global regulation. In the United States, steering on regulating security-related AI is provided by the National Security Commission on Artificial Intelligence.

Blockchain systems provide transparent and fixed records of transactions and hereby contradict the very nature of the European GDPR.

In 1942, author Isaac Asimov addressed regulation of algorithms by introducing the fictional Three Laws of Robotics:
In Frank Herbert's "Dune" universe, thinking machines is a collective term for artificial intelligence, which have been completely destroyed and banned after a revolt known as Butlerian Jihad:
JIHAD, BUTLERIAN: (see also Great Revolt) — the crusade against computers, thinking machines, and conscious robots begun in 201 B.G. and concluded in 108 B.G. Its chief commandment remains in the O.C. Bible as "Thou shalt not make a machine in the likeness of a human mind."



</doc>
<doc id="63151790" url="https://en.wikipedia.org/wiki?curid=63151790" title="Newman–Janis algorithm">
Newman–Janis algorithm

In general relativity, the Newman-Janis algorithm (NJA) is a complexification technique for finding exact solutions to the Einstein field equations. In 1964, Newman and Janis showed that the Kerr metric could be obtained from the Schwarzschild metric by means of a coordinate transformation and allowing the radial coordinate to take on complex values. Originally, no clear reason for why the algorithm works was known .

In 1998, Drake and Szekeres gave a detailed explanation of the success of the algorithm and proved the uniqueness of certain solutions. In particular, the only perfect fluid solution generated by NJA is the Kerr metric and the only Petrov type D solution is the Kerr-Newman metric .

The algorithm works well on f(R) and Einstein-Maxwell-Dilaton theories, but doesn't return expected results on Braneworld and Born-Infield theories.



</doc>
<doc id="5584249" url="https://en.wikipedia.org/wiki?curid=5584249" title="Gallery of named graphs">
Gallery of named graphs

Some of the finite structures considered in graph theory have names, sometimes inspired by the graph's topology, and sometimes after their discoverer. A famous example is the Petersen graph, a concrete graph on 10 vertices that appears as a minimal example or counterexample in many different contexts.

The strongly regular graph on "v" vertices and rank "k" is usually denoted srg("v,k",λ,μ).

A symmetric graph is one in which there is a symmetry (graph automorphism) taking any ordered pair of adjacent vertices to any other ordered pair; the Foster census lists all small symmetric 3-regular graphs. Every strongly regular graph is symmetric, but not vice versa.

The complete graph on formula_1 vertices is often called the "formula_1-clique" and usually denoted formula_3, from German "komplett".

The complete bipartite graph is usually denoted formula_4. For formula_5 see the section on star graphs. The graph formula_6 equals the 4-cycle formula_7 (the square) introduced below. 

The cycle graph on formula_1 vertices is called the "n-cycle" and usually denoted formula_9. It is also called a "cyclic graph", a "polygon" or the "n-gon". Special cases are the "triangle" formula_10, the "square" formula_7, and then several with Greek naming "pentagon" formula_12, "hexagon" formula_13, etc.

The friendship graph "F" can be constructed by joining "n" copies of the cycle graph "C" with a common vertex.

In graph theory, the term fullerene refers to any 3-regular, planar graph with all faces of size 5 or 6 (including the external face). It follows from Euler's polyhedron formula, "V" – "E" + "F" = 2 (where "V", "E", "F" indicate the number of vertices, edges, and faces), that there are exactly 12 pentagons in a fullerene and "h" = "V"/2 – 10 hexagons. Therefore "V" = 20 + 2"h"; "E" = 30 + 3"h". Fullerene graphs are the Schlegel representations of the corresponding fullerene compounds.

An algorithm to generate all the non-isomorphic fullerens with a given number of hexagonal faces has been developed by G. Brinkmann and A. Dress. G. Brinkmann also provided a freely available implementation, called fullgen.

The complete graph on four vertices forms the skeleton of the tetrahedron, and more generally the complete graphs form skeletons of simplices. The hypercube graphs are also skeletons of higher-dimensional regular polytopes.

A snark is a bridgeless cubic graph that requires four colors in any proper edge coloring. The smallest snark is the Petersen graph, already listed above.

A star "S" is the complete bipartite graph "K". The star "S" is called the claw graph.

The wheel graph "W" is a graph on "n" vertices constructed by connecting a single vertex to every vertex in an ("n" − 1)-cycle.


</doc>
<doc id="28297874" url="https://en.wikipedia.org/wiki?curid=28297874" title="Named graph">
Named graph

Named graphs are a key concept of Semantic Web architecture in which a set of Resource Description Framework statements (a graph) are identified using a URI, allowing descriptions to be made of that set of statements such as context, provenance information or other such metadata.

Named graphs are a simple extension of the RDF data model through which graphs can be created but the model lacks an effective means of distinguishing between them once published on the Web at large.

One conceptualization of the Web is as a graph of document nodes identified with URIs and connected by hyperlink arcs which are expressed within the HTML documents. By doing an HTTP GET on a URI (usually via a Web browser), a somehow-related document may be retrieved. This "follow your nose" approach also applies to RDF documents on the Web in the form of Linked Data, where typically an RDF syntax is used to express data as a series of statements, and URIs within the RDF point to other resources. This Web of data has been described by Tim Berners-Lee as the "Giant Global Graph".

Named graphs are a formalization of the intuitive idea that the contents of an RDF document (a graph) on the Web can be considered to be named by the URI of the document. This considerably simplifies techniques for managing chains of provenance for pieces of data and enabling fine-grained access control to the source data. Additionally trust can be managed through the publisher applying a digital signature to the data in the named graph. (Support for these facilities was originally intended to come from RDF reification, however that approach proved problematic.)

While named graphs may appear on the Web as simple linked documents (i.e. Linked Data), they are also very useful for managing sets of RDF data within an RDF store. In particular, the scope of a SPARQL query may be limited to a specific set of named graphs.

Assume the following (Turtle) RDF document has been placed in a SPARQL-capable store with the name .

"This data has been written in a more verbose form than necessary to show the triple structures"

The homepage of the person with the email address can be obtained using the SPARQL query:

The FROM NAMED here identifies the target graph for the query.

Prior to the publication of the papers describing named graphs, there was considerable discussion about fulfilling their role within a store by using an arity above that of RDF triple statements: where "triples" have the form "<subject> <predicate> <object>", "quads" would have a form along the lines of "<subject> <predicate> <object> <context>". Named graphs can be represented this way, as "<subject> <predicate> <object> <graphname>", with the advantage that the "<graphname>" part will be a URI, giving the quad Web-global scope compared to arbitrary local statement names. This way of representing quads resp. quad-statements was incorporated in the specification of N-Quads.

A paper from the WWW 2005 conference by Carroll et al. includes a formal definition of named graphs.

There is currently no specification for named graphs in themselves beyond that described in Carroll et al. (2005) and Carroll and Stickler (2004) (which includes syntaxes for representing named graphs), but they do form part of the SPARQL Protocol and RDF Query Language specification.



</doc>
<doc id="8997770" url="https://en.wikipedia.org/wiki?curid=8997770" title="List of graphs">
List of graphs

This partial list of graphs contains definitions of graphs and graph families which are known by particular names, but do not have a Wikipedia article of their own.

For collected definitions of graph theory terms that do not refer to individual graph types, such as "vertex" and "path", see Glossary of graph theory. For links to existing articles about particular kinds of graphs, see .

A gear graph, denoted "G" is a graph obtained by inserting an extra vertex between each pair of adjacent vertices on the perimeter of a wheel graph "W". Thus, "G" has 2"n"+1 vertices and 3"n" edges. Gear graphs are examples of squaregraphs, and play a key role in the forbidden graph characterization of squaregraphs. Gear graphs are also known as cogwheels and bipartite wheels.

A grid graph is a unit distance graph corresponding to the square lattice, so that it is isomorphic to the graph having a vertex corresponding to every pair of integers ("a", "b"), and an edge connecting ("a", "b") to ("a"+1, "b") and ("a", "b"+1). The finite grid graph "G" is an "m"×"n" rectangular graph isomorphic to the one obtained by restricting the ordered pairs to the range 0 ≤ "a" < "m", 0 ≤ "b" < "n". Grid graphs can be obtained as the Cartesian product of two paths: "G" = "P" × "P". Every grid graph is a median graph.

A helm graph, denoted H is a graph obtained by attaching a single edge and node to each node of the outer circuit of a wheel graph W.

A lobster graph is a tree in which all the vertices are within distance 2 of a central path. Compare "caterpillar".

The web graph "W" is a graph consisting of "r" concentric copies of the cycle graph "C", with corresponding vertices connected by "spokes". Thus "W" is the same graph as "C", and "W" is a prism.

A web graph has also been defined as a prism graph "Y", with the edges of the outer cycle removed.

Gallery of named graphs


</doc>
<doc id="19726159" url="https://en.wikipedia.org/wiki?curid=19726159" title="Table of the largest known graphs of a given diameter and maximal degree">
Table of the largest known graphs of a given diameter and maximal degree

In graph theory, the degree diameter problem is the problem of finding the largest possible graph for a given maximum degree and diameter. The Moore bound sets limits on this, but for many years mathematicians in the field have been interested in a more precise answer. The table below gives current progress on this problem (excluding the case of degree 2, where the largest graphs are cycles with an odd number of vertices).

Below is the table of the vertex numbers for the best-known graphs (as of October 2008) in the undirected degree diameter problem for graphs of degree at most 3 ≤ "d" ≤ 16 and diameter 2 ≤ "k" ≤ 10. Only a few of the graphs in this table (marked in bold) are known to be optimal (that is, largest possible). The remainder are merely the largest so far discovered, and thus finding a larger graph that is closer in order (in terms of the size of the vertex set) to the Moore bound is considered an open problem. Some general constructions are known for values of "d" and "k" outside the range shown in the table.
The following table is the key to the colors in the table presented above:



</doc>
<doc id="557931" url="https://en.wikipedia.org/wiki?curid=557931" title="Graph (abstract data type)">
Graph (abstract data type)

In computer science, a graph is an abstract data type that is meant to implement the undirected graph and directed graph concepts from the field of graph theory within mathematics.

A graph data structure consists of a finite (and possibly mutable) set of "vertices" (also called "nodes" or "points"), together with a set of unordered pairs of these vertices for an undirected graph or a set of ordered pairs for a directed graph. These pairs are known as "edges" (also called "links" or "lines"), and for a directed graph are also known as "arrows". The vertices may be part of the graph structure, or may be external entities represented by integer indices or references.

A graph data structure may also associate to each edge some "edge value", such as a symbolic label or a numeric attribute (cost, capacity, length, etc.).

The basic operations provided by a graph data structure "G" usually include:

Structures that associate values to the edges usually also provide:

Different data structures for the representation of graphs are used in practice:

The following table gives the time complexity cost of performing various operations on graphs, for each of these representations, with |"V" | the number of vertices and |"E" | the number of edges. In the matrix representations, the entries encode the cost of following an edge. The cost of edges that are not present are assumed to be ∞.

Adjacency lists are generally preferred because they efficiently represent sparse graphs. An adjacency matrix is preferred if the graph is dense, that is the number of edges |"E" | is close to the number of vertices squared, |"V" |, or if one must be able to quickly look up if there is an edge connecting two vertices.

The parallelization of graph problems faces significant challenges: Data-driven computations, unstructured problems, poor locality and high data access to computation ratio. The graph representation used for parallel architectures plays a significant role in facing those challenges. Poorly chosen representations may unnecessarily drive up the communication cost of the algorithm, which will decrease its scalability. In the following, shared and distributed memory architectures are considered.

In the case of a shared memory model, the graph representations used for parallel processing are the same as in the sequential case, since parallel read-only access to the graph representation (e.g. an adjacency list) is efficient in shared memory.

In the distributed memory model, the usual approach is to partition the vertex set formula_1 of the graph into formula_2 sets formula_3. Here, formula_2 is the amount of available processing elements (PE). The vertex set partitions are then distributed to the PEs with matching index, additionally to the corresponding edges. Every PE has its own subgraph representation, where edges with an endpoint in another partition require special attention. For standard communication interfaces like MPI, the ID of the PE owning the other endpoint has to be identifiable. During computation in a distributed graph algorithms, passing information along these edges implies communication.

Partitioning the graph needs to be done carefully - there is a trade-off between low communication and even size partitioning But partitioning a graph is a NP-hard problem, so it is not feasible to calculate them. Instead, the following heuristics are used.

1D partitioning: Every processor gets formula_5 vertices and the corresponding outgoing edges. This can be understood as a row-wise or column-wise decomposition of the adjacency matrix. For algorithms operating on this representation, this requires an All-to-All communication step as well as formula_6 message buffer sizes, as each PE potentially has outgoing edges to every other PE.

2D partitioning: Every processor gets a submatrix of the adjacency matrix. Assume the processors are aligned in a rectangle formula_7, where formula_8 and formula_9 are the amount of processing elements in each row and column, respectively. Then each processor gets a submatrix of the adjacency matrix of dimension formula_10. This can be visualized as a checkerboard pattern in a matrix. Therefore, each processing unit can only have outgoing edges to PEs in the same row and column. This bounds the amount of communication partners for each PE to formula_11 out of formula_7 possible ones.




</doc>
<doc id="1748701" url="https://en.wikipedia.org/wiki?curid=1748701" title="Smith graph">
Smith graph

In the mathematical field of graph theory, a Smith graph is either of two kinds of graph.




</doc>
<doc id="319589" url="https://en.wikipedia.org/wiki?curid=319589" title="History of anime">
History of anime

The history of anime can be traced back to the start of the 20th century, with the earliest verifiable films dating from 1906
. Before the advent of film, Japan already had a rich tradition of entertainment with colourful painted figures moving across the projection screen in 写し絵 (Utushi-e), a particular Japanese type of magic lantern show popular in the 19th century. Possibly inspired by European phantasmagoria shows, utushi-e showmen used mechanical slides and developed lightweight wooden projectors (furo) that were handheld so that several performers could each control the motions of different projected figures.

The first generation of animators in the late 1910s included Ōten Shimokawa, Jun'ichi Kōuchi and Seitaro Kitayama, commonly referred to as the "fathers" of anime. Propaganda films, such as "Momotarō no Umiwashi" (1943) and "" (1945), the latter being the first anime feature film, were made during World War II. During the 1970s, anime developed further, with the inspiration of Disney animators, separating itself from its Western roots, and developing distinct genres such as mecha and its super robot subgenre. Typical shows from this period include "Astro Boy", "Lupin III" and "Mazinger Z". During this period several filmmakers became famous, especially Hayao Miyazaki and Mamoru Oshii.

In the 1980s, anime became mainstream in Japan, experiencing a boom in production with the rise in popularity of anime like "Gundam", "Macross", "Dragon Ball", and genres such as real robot, space opera and cyberpunk. "Space Battleship Yamato" and "The Super Dimension Fortress Macross" also achieved worldwide success after being adapted respectively as "Star Blazers" and "Robotech".

The film "Akira" set records in 1988 for the production costs of an anime film and went on to become an international success, with a live action edition planned for 2021. Later, in 2004, the same creators produced "Steamboy", which took over as the most expensive anime film. "Spirited Away" shared the first prize at the 2002 Berlin Film Festival and won the 2003 Academy Award for Best Animated Feature, while "" was featured at the 2004 Cannes Film Festival.

According to Natsuki Matsumoto, the first animated film produced in Japan may have stemmed from as early as 1907. Known as , from its depiction of a boy in a sailor suit drawing the characters for "katsudō shashin", the film was first found in 2005. It consists of fifty frames stencilled directly onto a strip of celluloid. This claim has not been verified though and predates the first known showing of animated films in Japan. The date and first film publicly displayed is another source of contention: while no Japanese-produced animation is definitively known to date before 1917, the possibility exists that other films entered Japan and that no known records have surfaced to prove a showing prior to 1912. Film titles have surfaced over the years, but none have been proven to predate this year. The first foreign animation is known to have been found in Japan in 1910, but it is not clear if the film was ever shown in a cinema or publicly displayed at all. Yasushi Watanabe found a film known as in the records of the company. The description matches James Blackton's "Humorous Phases of Funny Faces", though academic consensus on whether or not this is a true animated film is disputed. According to Kyokko Yoshiyama, the first animated film called was shown in Japan at the in Tokyo sometime in 1911. Yoshiyama did not refer to the film as "animation" though. The first confirmed animated film shown in Japan was "Les Exploits de Feu Follet" by Émile Cohl on April 15, 1912. While speculation and other "trick films" have been found in Japan, it is the first recorded account of a public showing of a two-dimensional animated film in Japanese cinema. During this time, German animations marketed for home release were distributed in Japan.

Few complete animations made during the beginnings of Japanese animation have survived. The reasons vary, but many are of commercial nature. After the clips had been run, reels (being property of the cinemas) were sold to smaller cinemas in the country and then disassembled and sold as strips or single frames. The first anime that was produced in Japan was made sometime in 1917, but there it is disputed which title was the first to get that honour. It has been confirmed that was made sometime during February 1917. At least two unconfirmed titles were reported to have been made the previous month.

The first anime short-films were made by three leading figures in the industry. Ōten Shimokawa was a political caricaturist and cartoonist who worked for the magazine "Tokyo Puck". He was hired by Tenkatsu to do an animation for them. Due to medical reasons, he was only able to do five movies, including "Imokawa Mukuzo Genkanban no Maki" (1917), before he returned to his previous work as a cartoonist. Another prominent animator in this period was Jun'ichi Kōuchi. He was a caricaturist and painter, who also had studied watercolour painting. In 1912, he also entered the cartoonist sector and was hired for an animation by Kobayashi Shokai later in 1916. He is viewed as the most technically advanced Japanese animator of the 1910s. His works include around 15 movies. The third was Seitaro Kitayama, an early animator who made animations on his own and was not hired by larger corporations. He eventually founded his own animation studio, the Kitayama Eiga Seisakujo, which was later closed due to lack of commercial success. He utilized the chalkboard technique, and later paper animation, with and without pre-printed backgrounds. The works of these two latter pioneers include "Namakura Gatana" ("An Obtuse Sword", 1917) and a 1918 film "Urashima Tarō" which were believed to have been discovered together at an antique market in 2007. However, this "Urashima Tarō" was later proved to most likely be a different film of the same story than the 1918 one by Kitayama, which, as of October 2017, remains undiscovered.

Yasuji Murata, Hakuzan Kimura, Sanae Yamamoto and Noburō Ōfuji were students of Kitayama Seitaro and worked at his film studio. Kenzō Masaoka, another important animator, worked at a smaller animation studio. In 1923, the Great Kantō earthquake destroyed most of the Kitayama studio and the residing animators spread out and founded studios of their own.

Prewar animators faced several difficulties. First, they had to compete with foreign producers such as Disney, which were influential on both audiences and producers. Foreign films had already made a profit abroad, and could be undersold in the Japanese market, priced lower than what domestic producers needed to break even. Japanese animators thus had to work cheaply, in small companies with only a handful of employees, which then made it difficult to compete in terms of quality with foreign product that was in color, with sound, and promoted by much bigger companies. Until the mid-1930s, Japanese animation generally used cutout animation instead of cel animation because the celluloid was too expensive. This resulted in animation that could seem derivative, flat (since motion forward and backward was difficult) and without detail. But just as postwar Japanese animators were able to turn limited animation into a plus, so masters such as Yasuji Murata and Noburō Ōfuji were able to perform wonders that they made with cutout animation.

Animators such as Kenzō Masaoka and Mitsuyo Seo, however, did attempt to bring Japanese animation up to the level of foreign work by introducing cel animation, sound, and technology such as the multiplane camera. Masaoka created the first talkie anime, "Chikara to Onna no Yo no Naka", released in 1933, and the first anime made entirely using cel animation, "The Dance of the Chagamas" (1934). Seo was the first to use the multiplane camera in "Ari-chan" in 1941.

Such innovations, however, were difficult to support purely commercially, so prewar animation depended considerably on sponsorship, as animators often concentrated on making PR films for companies, educational films for the government, and eventually works of propaganda for the military. During this time, censorship and school regulations discouraged film-viewing by children, so anime which could possess educational value were supported and encouraged by the Monbusho (the Ministry of Education). This proved important for producers that had experienced obstacles releasing their work in regular theaters. Animation had found a place in scholastic, political and industrial use.

In the 1930s, the Japanese government began enforcing cultural nationalism. This also lead to a strict censorship and control of published media. Many animators were urged to produce animations which enforced the Japanese spirit and national affiliation. Some movies were shown in newsreel theaters, especially after the Film Law of 1939 promoted documentary and other educational films. Such support helped boost the industry, as bigger companies formed through mergers and prompted major live-action studios such as Shochiku to begin producing animation. It was at Shochiku that such masterworks as Kenzō Masaoka's "Kumo to Chūrippu" were produced. Wartime reorganization of the industry, however, merged the feature film studios into three big companies.

More animated films were commissioned by the military, showing the sly, quick Japanese people winning against enemy forces. In 1941, "Princess Iron Fan" was made in China, as Asia's first full-length animated feature. In 1943, Geijutsu Eigasha produced Mitsuyo Seo's "Momotaro's Sea Eagles" with help from the Navy. Shochiku then made Japan's first full-length animated feature, Seo's "Momotaro's Divine Sea Warriors" in 1945, again with the help of the Navy.

Toei Animation and Mushi Production was founded and produced the first color anime feature film in 1958, "Hakujaden" ("The Tale of the White Serpent", 1958).
It was released in the US in 1961 as well as "Panda and the Magic Serpent". After the success of the project, Toei released a new feature-length animation annually.

Toei's style was characterized by an emphasis on each animator bringing his own ideas to the production. The most extreme example of this is Isao Takahata's film "" (1968). "Hols" is often seen as the first major break from the normal anime style and the beginning of a later movement of "auteuristic" or "progressive anime" which would eventually involve directors such as Hayao Miyazaki (creator of "Spirited Away") and Mamoru Oshii.

A major contribution of Toei's style to modern anime was the development of the "money shot". This cost-cutting method of animation allows for emphasis to be placed on important shots by animating them with more detail than the rest of the work (which would often be limited animation). Toei animator Yasuo Ōtsuka began to experiment with this style and developed it further as he went into television. In the 1980s, Toei would later lend its talent to companies like Sunbow Productions, Marvel Productions, DiC Entertainment, Murakami-Wolf-Swenson, Ruby Spears and Hanna Barbera, producing several animated cartoons for America during this period. Other studios like TMS Entertainment, were also being used in the 1980s, which lead to Asian studios being used more often to animate foreign productions, but the companies involved still produced anime for their native Japan.

Osamu Tezuka established Mushi Production in 1961, after Tezuka's contract with Toei Animation expired. The studio pioneered TV animation in Japan, and was responsible for such successful TV series as "Astro Boy", "Kimba the White Lion", "Gokū no Daibōken" and "Princess Knight".

Mushi Production also produced the first anime to be broadcast in the United States (on NBC in 1963), although Osamu Tezuka would complain about the restrictions on US television, and the alterations necessary for broadcast.

The 1960s brought anime to television and in America. The first anime film to be broadcast was "Three Tales" in 1960. The following year saw the premiere of Japan's first animated television series, "Instant History", although it did not consist entirely of animation. Osamu Tezuka's "Tetsuwan Atom" ("Astro Boy") is often miscredited as the first anime television series, premiering on January 1, 1963. "Astro Boy" was highly influential to other anime in the 1960s, and was followed by a large number of anime about robots or space.

1963 introduced "Sennin Buraku" as the first "late night" anime and Toei Doga's first anime television series "Wolf Boy Ken". Mushi Pro continued to produce more anime television and met success with titles such as "Kimba the White Lion" in 1965. What is noted as the first magical girl anime, "Sally the Witch", began broadcast in 1966. The original "Speed Racer" anime television began in 1967, and was brought to the West with great success. At the same time, an anime adaptation of Tezuka's "Princess Knight" aired, making it one of very few "shoujo" anime of the decade. The first anime adaptation of Shotaro Ishinomori's manga "Cyborg 009" was created in 1968, following the film adaptation two years prior.

The long-running "Sazae-san" anime also began in 1969 and continues today with excess of 6,500 episodes broadcast as of 2014. With an audience share of 25% the series is still the most-popular anime broadcast.
During the 1970s, the Japanese film market shrank due to competition from television. This reduced Toei animation's staff and many animators went to studios such as A Pro and Telecom animation. Mushi Production went bankrupt (though the studio was revived 4 years later), its former employees founding studios such as Madhouse and Sunrise. Many young animators were thrust into the position of director, and the injection of young talent allowed for a wide variety of experimentation. One of the earliest successful television productions in the early 1970s was "Tomorrow's Joe" (1970), a boxing anime which has become iconic in Japan. 1971 saw the first installment of the "Lupin III" anime. Contrary to the franchise's currently popularity, the first series ran for 23 episodes before being cancelled. The second series (starting in 1977) saw considerably more success, spanning 155 episodes over three years.

Another example of this experimentation is Isao Takahata's 1974 television series "Heidi, Girl of the Alps". This show was originally a hard sell because it was a simple realistic drama aimed at children, and most TV networks thought children needed something more fantastic to draw them in. "Heidi" was an international success, popular in many European countries, and so successful in Japan that it allowed for Hayao Miyazaki and Takahata to start a series of literary-based anime ("World Masterpiece Theater"). Miyazaki and Takahata left Nippon Animation in the late 1970s. Two of Miyazaki's critically acclaimed productions during the 1970s were "Future Boy Conan" (1978) and "Lupin III: The Castle of Cagliostro" (1979).

During this period, Japanese animation reached continental Europe with productions aimed at European and Japanese children, with the most-pronounced examples being the aforementioned "Heidi" but also "Barbapapa" and "Vicky the Vikings". Italy, Spain and France grew an interest in Japan's output, which was offered for a low price.

Another genre known as mecha came into being at this time. Some early works include "Mazinger Z" (1972–1974), "Science Ninja Team Gatchaman" (1972–1974), "Space Battleship Yamato" (1974–75) and "Mobile Suit Gundam" (1979–80).

As a contrast to the action-oriented shows, shows for a female audience such as "Candy Candy" and "Rose of Versailles" earned high popularity on Japanese television and later in other parts of the world.

By 1978, over fifty shows were aired on television.

The shift towards space operas became more pronounced with the commercial success of "Star Wars" (1977). This allowed for the space opera "Space Battleship Yamato" (1974) to be revived as a theatrical film. "Mobile Suit Gundam" (1979) was also unsuccessful and revived as a theatrical film in 1982. The success of the theatrical versions of "Yamato" and "Gundam" is seen as the beginning of the anime boom of the 1980s, and of "Japanese Cinema's Second Golden Age".

A subculture in Japan, whose members later called themselves "otaku", began to develop around animation magazines such as "Animage" and "Newtype". These magazines formed in response to the overwhelming fandom that developed around shows such as "Yamato" and "Gundam" in the late 1970s and early 1980s.

In the United States, the popularity of "Star Wars" had a similar, though much smaller, effect on the development of anime. "Gatchaman" was reworked and edited into "Battle of the Planets" in 1978 and again as "G-Force" in 1986. "Space Battleship Yamato" was reworked and edited into "Star Blazers" in 1979. The "Macross" series began with "The Super Dimension Fortress Macross" (1982), which was adapted into English as the first arc of "Robotech" (1985), which was created from three separate anime titles: "The Super Dimension Fortress Macross", "Super Dimension Cavalry Southern Cross" and "Genesis Climber Mospeada". The sequel to "Mobile Suit Gundam", "Mobile Suit Zeta Gundam" (1985), became the most successful real robot space opera in Japan, where it managed an average television rating of 6.6% and a peak of 11.7%.

The "otaku" subculture became more pronounced with Mamoru Oshii's adaptation of Rumiko Takahashi's popular manga "Urusei Yatsura" (1981). "Yatsura" made Takahashi a household name and Oshii would break away from fan culture and take a more auteuristic approach with his 1984 film "". This break with the "otaku" subculture would allow Oshii to experiment further.

The "otaku" subculture had some effect on people who were entering the industry around this time. The most famous of these people were the amateur production group Daicon Films which would become Gainax. Gainax began by making films for the Daicon science fiction conventions and were so popular in the "otaku" community that they were given a chance to helm the biggest-budgeted anime film (at that time), "" (1987).

One of the most-influential anime of all time, "Nausicaä of the Valley of the Wind" (1984), was made during this period. The film gave extra prestige to anime allowing for many experimental and ambitious projects to be funded shortly after its release. It also allowed director Hayao Miyazaki and his longtime colleague Isao Takahata to create their own studio under the supervision of former "Animage" editor Toshio Suzuki. This studio would become known as Studio Ghibli and its first film was "" (1986), one of Miyazaki's most-ambitious films.

The success of "Dragon Ball" (1986) introduced the martial arts genre and became incredibly influential in the Japanese Animation industry. It influenced many more martial arts anime and manga series' including "YuYu Hakusho" (1990), "One Piece" (1999), "Naruto" (2002), and "One Punch Man" (2015).

The 1980s brought anime to the home video market in the form of original video animation (OVA). The first OVA was Mamoru Oshii's "Dallos" (1983–1984). Shows such as "Patlabor" had their beginnings in this market and it proved to be a way to test less-marketable animation against audiences. The OVA allowed for the release of pornographic anime such as "Cream Lemon" (1984); the first "hentai" OVA was actually the little-known Wonder Kids studio's "Lolita Anime", also released in 1984.

The 1980s also saw the amalgamation of anime with video games. The airing of "Red Photon Zillion" (1987) and subsequent release of its companion game, is considered to have been a marketing ploy by Sega to promote sales of their newly released Master System in Japan.

Sports anime, as it is now known, made its debut in 1983 with an anime adaptation of Yoichi Takahashi's soccer manga Captain Tsubasa, which became the first worldwide successful sports anime. Its themes and stories were a formula that would be used in many sports series that followed, such as "Slam Dunk", "Prince of Tennis" and "Eyeshield 21".

The late 1980s saw an increasing number of high-budget and experimental films. In 1985, Toshio Suzuki helped put together funding for Oshii's experimental film "Angel's Egg" (1985). Theatrical releases became more ambitious, each film trying to outclass or outspend its predecessors, taking cues from "Nausicaä"s popular and critical success. "Night on the Galactic Railroad" (1985), "Tale of Genji" (1986), and "Grave of the Fireflies" (1988) were all ambitious films based on important literary works in Japan. Films such as "Char's Counterattack" (1988) and "Arion" (1986) were lavishly budgeted spectacles. This period of lavish budgeting and experimentation would reach its zenith with two of the most-expensive anime film productions ever: "" (1987) and "Akira" (1988). Studio Ghibli's "Kiki's Delivery Service" (1989) was the top-grossing film for 1989, earning over $40 million at the box office.

Despite the commercial failure of "Akira" in Japan, it brought with it a much larger international fan base for anime. When shown overseas, the film became a cult hit and, eventually, a symbol of the medium for the West. The domestic failure and international success of "Akira", combined with the bursting of the bubble economy and Osamu Tezuka's death in 1989, brought a close to the 1980s era of anime.

In 1993, the first airing of "JoJo's Bizarre Adventure" in anime form was released. This marked the beginning of an on and off airing schedule of one of the most critically acclaimed anime series'. The series has a very western influence in the way the characters are portrayed and act. Later, "JoJo's" has gone on to inspire many western looking anime, such as "Cowboy Bebop".

In 1995, Hideaki Anno wrote and directed the controversial anime "Neon Genesis Evangelion". This show became popular in Japan among anime fans and became known to the general public through mainstream media attention. It is believed that Anno originally wanted the show to be the ultimate "otaku" anime, designed to revive the declining anime industry, but midway through production he also made it into a heavy critique of the subculture. It culminated in the successful but controversial film "The End of Evangelion" which grossed over $10 million in 1997. The many violent and sexual scenes in "Evangelion" caused TV Tokyo to increase censorship of anime content. As a result, when "Cowboy Bebop" was first broadcast in 1998, it was shown heavily edited and only half the episodes were aired; it too gained heavy popularity both in and outside of Japan.

"Evangelion" started a series of so-called "post-"Evangelion"" or "organic" mecha shows. Most of these were giant robot shows with some kind of religious or complex plot. These include "RahXephon", "Brain Powerd", and "Gasaraki". It also led to late-night experimental anime shows. Starting with "Serial Experiments Lain" (1998), late night became a forum for experimental anime such as "Boogiepop Phantom" (2000), "Texhnolyze" (2003) and "Paranoia Agent" (2004). Experimental anime films were also released in the 1990s, most notably the cyberpunk thriller "Ghost in the Shell" (1995), which had a strong influence on "The Matrix". "Ghost in the Shell", alongside "Evangelion" and the neo-noir space western "Cowboy Bebop", helped further increase the awareness of anime in international markets.

The late 1990s also saw a brief revival of the super robot genre that had decreased in popularity due to the rise of real robot and psychological mecha shows like "Gundam", "Macross", and "Evangelion". The revival of the super robot genre began with "Brave Exkaiser" in 1990, and led to remakes and sequels of 1970s super robot shows like "Getter Robo Go" and "Tetsujin-28 go FX". There were very few popular super robot shows produced after this, until "Tengen Toppa Gurren Lagann" in 2007.

Alongside its super robot counterpart, the real robot genre was also declining during the 1990s. Though several "Gundam" shows were produced during this decade, very few of them were successful. The only "Gundam" shows in the 1990s which managed an average television rating over 4% in Japan were "Mobile Fighter G Gundam" (1994) and "New Mobile Report Gundam Wing" (1995). It wasn't until "Mobile Suit Gundam SEED" in 2002 that the real robot genre regained its popularity.

In 1997, Hayao Miyazaki's "Princess Mononoke" became the most-expensive anime film up until that time, costing $20 million to produce. Miyazaki personally checked each of the 144,000 cels in the film, and is estimated to have redrawn parts of 80,000 of them.

By 1998, over one hundred anime shows were aired on television in Japan, including a popular series based on the "Pokémon" video game franchise. Other 1990s anime series which gained international success were "Dragon Ball Z," "Sailor Moon," and "Digimon"; the success of these shows brought international recognition to the martial arts superhero genre, the magical girl genre, and the action-adventure genre, respectively. In particular, "Dragon Ball Z" and "Sailor Moon" were dubbed into more than a dozen languages worldwide. Another large success was the anime "One Piece", based on the best-selling manga of all time, which is still ongoing.

The ""Evangelion"-era" trend continued into the 2000s with "Evangelion"-inspired mecha anime such as "RahXephon" (2002) and "Zegapain" (2006) – "RahXephon" was also intended to help revive 1970s-style mecha designs.

The real robot genre (including the "Gundam" and "Macross" franchises), which had declined during the 1990s, was revived in 2002 with the success of shows such as "Mobile Suit Gundam SEED" (2002), "Eureka Seven" (2005), "Code Geass: Lelouch of the Rebellion" (2006), "Mobile Suit Gundam 00" (2007), and "Macross Frontier" (2008).

The 1970s-style super robot genre revival began with "GaoGaiGar" in 1997 and continued into the 2000s, with several remakes of classic series such as "Getter Robo" and "Dancougar", as well as original titles created in the super robot mold like "Godannar" and "Gurren Lagann." "Gurren Lagann" in particular combined the super robot genre with elements from 1980s real robot shows, as well as 1990s "post-"Evangelion"" shows. "Gurren Lagann" received both the "best television production" and "best character design" awards from the Tokyo International Anime Fair in 2008. This eventually culminated in the release of "Shin Mazinger" in 2009, a full-length revival of the first super robot series, "Mazinger Z".

An art movement started by Takashi Murakami that combined Japanese pop-culture with postmodern art called Superflat began around this time. Murakami asserts that the movement is an analysis of post-war Japanese culture through the eyes of the "otaku" subculture. His desire is also to get rid of the categories of 'high' and 'low' art making a flat continuum, hence the term 'superflat'. His art exhibitions have gained popularity overseas and have influenced a handful of anime creators, particularly those from Studio 4 °C. 

The experimental late night anime trend popularized by "Serial Experiments Lain" also continued into the 2000s with experimental anime such as "Boogiepop Phantom" (2000), "Texhnolyze" (2003), "Paranoia Agent" (2004), "Gantz" (2004), and "Ergo Proxy" (2006)

In addition to these experimental trends, the 2000s were also characterized by an increase of moe-style art and bishōjo and bishōnen character design. There was a rising presence and popularity of genres such as romance, harem and slice of life.

Anime based on eroge and visual novels increased in popularity in the 2000s, building on a trend started in the late 1990s by such works as "Sentimental Journey" (1998) and "To Heart" (1999). Examples of such works include "Green Green" (2003), "SHUFFLE!" (2006), "Kanon" (2002 and 2006), "Fate/Stay Night" (2006), "Higurashi no Naku Koro ni" (2006), "" (2007), "True Tears" (2008), and "Clannad" (2008 and 2009).

Many shows have been adapted from manga and light novels, including popular titles such as "Yu-Gi-Oh!" (2000), "Inuyasha" (2000), "Naruto" (2002), "Fullmetal Alchemist" (2003), "Monster" (2004), "Bleach" (2004), "Rozen Maiden" (2005), "Aria the Animation" (2005), "Shakugan no Shana" (2005), "Pani Poni Dash!" (2005), "Death Note" (2006), "Mushishi" (2006), "Sola" (2007), "The Melancholy of Haruhi Suzumiya" (2006), "Lucky Star" (2007), "Toradora!" (2008–09), "K-On!" (2009), "Bakemonogatari" (2009), and "Fairy Tail" (2009); these shows typically last several years and achieve large fanbases. Nevertheless, original anime titles continue to be produced with the same success.

The 2000s marked a trend of emphasis of the "otaku" subculture. A notable critique of this "otaku" subculture is found in the 2006 anime "Welcome to the N.H.K.", which features a "hikikomori" (socially withdrawn) protagonist and explores the effects and consequences of various Japanese sub-cultures, such as "otaku", lolicon, internet suicide, massively multiplayer online games and multi-level marketing.

In contrast to the above-mentioned phenomenon, there have been more productions of late-night anime for a non-"otaku" audience as well. The first concentrated effort came from Fuji TV's Noitamina block. The 30-minute late-Thursday timeframe was created to showcase productions for young women of college age, a demographic that watches very little anime. The first production "Honey and Clover" was a particular success, peaking at a 5% TV rating in Kantou, very strong for late-night anime. The block has been running uninterrupted since April 2005 and has yielded many successful productions unique in the modern anime market.

There have been revivals of American cartoons such as "Transformers" which spawned four new series, "" in 2000, "" in 2003, "" in 2004, and "" in 2005. In addition, an anime adaptation of the G.I Joe series was produced titled "".

The revival of earlier anime series was seen in the forms of "" (2006) and "Dragon Ball Z Kai" (2009). Later series also started receiving revivals in the late 2000s and early 2010s, such as with Studio Khara's "Rebuild of Evangelion" tetralogy (2007–), and new adaptations of Masamune Shirow's manga "Appleseed XIII" (2011) and "" (2013–2016).

The decade also dawned a revival of high-budget feature-length anime films, such as "Millennium Actress" (2001), "Metropolis" (2001), "Appleseed" (2001), "Paprika" (2006), and the most expensive of all being "Steamboy" (2004) which cost $26 million to produce. Satoshi Kon established himself alongside Otomo and Oshii as one of the premier directors of anime film, before his premature death at the age of 46. Other younger film directors, such as Mamoru Hosoda, director of "The Girl Who Leapt Through Time" (2006) and "Summer Wars" (2009), also began to reach prominence.

During this decade, anime feature films were nominated for and won major international film awards for the first time in the industry's history. In 2002, "Spirited Away", a Studio Ghibli production directed by Hayao Miyazaki, won the Golden Bear at the Berlin International Film Festival and in 2003 at the 75th Academy Awards it won the Academy Award for Best Animated Feature. It was the first non-American film to win the award and is one of only two to do so. It has also become the highest grossing anime film, with a worldwide box office of US$274 million.

Following the launch of Toonami on Cartoon Network and later Adult Swim, anime saw a giant rise in the North American market. Kid-friendly anime such as "Pokémon", "Yu-Gi-Oh!", "Digimon", "Doraemon", "Bakugan", "Beyblade", and the 4Kids Entertainment adaptation of "One Piece" have all received varying levels of success. This era also saw the rise of anime-influenced animation, most notably "" and its sequel "The Legend of Korra", "Ben 10", "Chaotic", "Samurai Jack", "The Boondocks", "RWBY" and "Teen Titans".

At the 2004 Cannes Film Festival, "", directed by Mamoru Oshii, was in competition for the Palme d'Or and in 2006, at the 78th Academy Awards, "Howl's Moving Castle", another Studio Ghibli-produced film directed by Hayao Miyazaki, was nominated for Best Animated Feature. "5 Centimeters Per Second", directed by Makoto Shinkai, won the inaugural Asia Pacific Screen Award for Best Animated Feature Film in 2007, and so far, anime films have been nominated for the award every year.

By 2004, over two hundred shows were aired on television.

In 2012, the Toonami block in the US was relaunched as an adult-oriented action block on Adult Swim, bringing uncut anime to a far wider audience. In addition to re-releasing older shows, the block (as well as Adult Swim itself) oversees the premiere of English dubbed releases for various new shows, including: "Durarara!!" (2010), "Deadman Wonderland" (2011), "Hunter x Hunter" (2011), "Sword Art Online" (2012), "Attack on Titan" (2013), "Kill la Kill" (2013), "Space Dandy" (2014), "Akame ga Kill!" (2014), "Parasyte -the maxim-" (2014), "One Punch Man" (2015), "My Hero Academia" (2016), "" (2017), and "Black Clover" (2017)

On September 6, 2013 Hayao Miyazaki announced that "The Wind Rises" (2013) would be his last film, and on August 3, 2014 it was announced that Studio Ghibli was "temporarily halting production" following the release of "When Marnie Was There" (2014), further substantiating the finality of Miyazaki's retirement. The disappointing sales of Isao Takahata's comeback film "The Tale of Princess Kaguya" (2013) has also been cited as a factor. Several prominent staffers, including producer Yoshiaki Nishimura and director Hiromasa Yonebayashi, left to form their own Studio Ponoc, premièring with "Mary and the Witch's Flower" (2017). Both Ghibli and Miyazaki subsequently went back into production for the up-coming film "How Do You Live?", while Takahata died on April 5, 2018 of lung cancer.

Additionally, various international anime distribution companies, such as ADV Films, Bandai Entertainment, and Geneon Entertainment, were shut down due to poor revenue, with their assets spun into new companies like Sentai Filmworks or given to other companies.

Both "Attack on Titan" and "The Wind Rises" reflect a national debate surrounding the reinterpretation of Article 9 of the Constitution of Japan, with Miyazaki's pacifism in the film coming under fire from the political right, while "Attack on Titan" has been accused of promoting militarism by people in neighboring Asian countries, despite being intended to show the haunting, hopeless aspects of conflict. The mecha anime genre (as well as Japanese kaiju films) received a Western homage with the 2013 film "Pacific Rim" directed by Guillermo del Toro.

Western streaming services such as Netflix and Amazon Prime are increasingly becoming involved in the production and licensing of anime.

In 2015, an all record-high of three hundred forty anime series aired on television.




</doc>
<doc id="800" url="https://en.wikipedia.org/wiki?curid=800" title="Anime">
Anime

The earliest commercial Japanese animation dates to 1917, and Japanese anime production has since continued to increase steadily. The characteristic anime art style emerged in the 1960s with the works of Osamu Tezuka and spread internationally in the late 20th century, developing a large domestic and international audience. Anime is distributed theatrically, by way of television broadcasts, directly to home media, and over the Internet. It is classified into numerous genres targeting diverse broad and niche audiences.

Anime is a diverse art form with distinctive production methods and techniques that have been adapted over time in response to emergent technologies. It combines graphic art, characterization, cinematography, and other forms of imaginative and individualistic techniques. The production of anime focuses less on the animation of movement and more on the realism of settings as well as the use of camera effects, including panning, zooming, and angle shots. Being hand-drawn, anime is separated from reality by a crucial gap of fiction that provides an ideal path for escapism that audiences can immerse themselves into with relative ease. Diverse art styles are used, and character proportions and features can be quite varied, including characteristically large or realistically sized emotive eyes.

The anime industry consists of over 430 production studios, including major names like Studio Ghibli, Gainax, and Toei Animation. Despite comprising only a fraction of Japan's domestic film market, anime makes up a majority of Japanese DVD and Blu-ray sales. It has also seen international success after the rise of English-dubbed and subbed programming. This rise in international popularity has resulted in non-Japanese productions using the anime art style. Whether these works are anime-influenced animation or proper anime is a subject for debate amongst fans. Japanese anime accounted for 60% of the world's animated television shows . In 2015, the 44th President of the United States, Barack Obama, referred to anime as one of the loved ones of American youth.

Anime is an art form, specifically animation, that includes all genres found in cinema, but it can be mistakenly classified as a genre. In Japanese, the term "anime" is used as a blanket term to refer to all forms of animation from around the world. In English, "anime" () is more restrictively used to denote a "Japanese-style animated film or television entertainment" or as "a style of animation created in Japan".

The etymology of the word "anime" is disputed. The English term "animation" is written in Japanese "katakana" as ("animēshon", ) and is ("anime") in its shortened form. The pronunciation of "anime" in Japanese differs from pronunciations in other languages such as Standard English (pronunciation: ), which has different vowels and stress with regards to Japanese, where each mora carries equal stress. As with a few other Japanese words such as "saké", "Pokémon", and "Kobo Abé," English-language texts sometimes spell "anime" as "animé" (as in French), with an acute accent over the final "e", to cue the reader to pronounce the letter, not to leave it silent as Standard English orthography may suggest.

Some sources claim that "anime" derives from the French term for animation "dessin animé", but others believe this to be a myth derived from the French popularity of the medium in the late 1970s and 1980s. In English, "anime"—when used as a common noun—normally functions as a mass noun. (For example: "Do you watch anime?" or "How much anime have you collected?") Prior to the widespread use of "anime", the term "Japanimation" was prevalent throughout the 1970s and 1980s. In the mid-1980s, the term "anime" began to supplant "Japanimation". In general, the latter term now only appears in period works where it is used to distinguish and identify Japanese animation.

The word "anime" has also been criticised, e.g. in 1987, when Hayao Miyazaki stated that he despised the truncated word "anime" because to him it represented the desolation of the Japanese animation industry. He equated the desolation with animators lacking motivation and with mass-produced, overly expressionistic products relying upon a fixed iconography of facial expressions and protracted and exaggerated action scenes but lacking depth and sophistication in that they do not attempt to convey emotion or thought.

The first format of anime was theatrical viewing which originally began with commercial productions in 1917. Originally the animated flips were crude and required played musical components before adding sound and vocal components to the production. On July 14, 1958, Nippon Television aired "Mogura no Abanchūru" ("Mole's Adventure"), both the first televised and first color anime to debut. It was not until the 1960s when the first televised series were broadcast and it has remained a popular medium since. Works released in a direct to video format are called "original video animation" (OVA) or "original animation video" (OAV); and are typically not released theatrically or televised prior to home media release. The emergence of the Internet has led some animators to distribute works online in a format called "original net anime" (ONA).

The home distribution of anime releases were popularized in the 1980s with the VHS and LaserDisc formats. The VHS NTSC video format used in both Japan and the United States is credited as aiding the rising popularity of anime in the 1990s. The Laser Disc and VHS formats were transcended by the DVD format which offered the unique advantages; including multiple subtitling and dubbing tracks on the same disc. The DVD format also has its drawbacks in its usage of region coding; adopted by the industry to solve licensing, piracy and export problems and restricted region indicated on the DVD player. The Video CD (VCD) format was popular in Hong Kong and Taiwan, but became only a minor format in the United States that was closely associated with bootleg copies.

Japanese animation began in the early 20th century, when Japanese filmmakers experimented with the animation techniques also pioneered in France, Germany, the United States and Russia. A claim for the earliest Japanese animation is "Katsudō Shashin", an undated and private work by an unknown creator. In 1917, the first professional and publicly displayed works began to appear. Animators such as Ōten Shimokawa and Seitarou Kitayama produced numerous works, with the oldest surviving film being Kouchi's "Namakura Gatana", a two-minute clip of a samurai trying to test a new sword on his target only to suffer defeat. The 1923 Great Kantō earthquake resulted in widespread destruction to Japan's infrastructure and the destruction of Shimokawa's warehouse, destroying most of these early works.

By the 1930s animation was well established in Japan as an alternative format to the live-action industry. It suffered competition from foreign producers and many animators—like Noburō Ōfuji and Yasuji Murata—still worked in cheaper cutout animation rather than cel animation. Other creators, Kenzō Masaoka and Mitsuyo Seo, nonetheless made great strides in animation technique; they benefited from the patronage of the government, which employed animators to produce educational shorts and propaganda. The first talkie anime was "Chikara to Onna no Yo no Naka", produced by Masaoka in 1933. By 1940, numerous anime artists' organizations had risen, including the Shin Mangaha Shudan and Shin Nippon Mangaka. The first feature-length animated film was "Momotaro's Divine Sea Warriors" directed by Seo in 1944 with sponsorship by the Imperial Japanese Navy.

The success of The Walt Disney Company's 1937 feature film "Snow White and the Seven Dwarfs" profoundly influenced many Japanese animators. The 1950s saw a proliferation of short, animated advertisements made in Japan for television broadcasting. In the 1960s, manga artist and animator Osamu Tezuka adapted and simplified many Disney animation techniques to reduce costs and to limit the number of frames in productions. He intended this as a temporary measure to allow him to produce material on a tight schedule with inexperienced animation staff. "Three Tales", aired in 1960, was the first anime shown on television. The first anime television series was "Otogi Manga Calendar", aired from 1961 to 1964.

The 1970s saw a surge of growth in the popularity of "manga", Japanese comic books and graphic novels, many of which were later animated. The work of Osamu Tezuka drew particular attention: he has been called a "legend" and the "god of manga". His work—and that of other pioneers in the field—inspired characteristics and genres that remain fundamental elements of anime today. The giant robot genre (known as "mecha" outside Japan), for instance, took shape under Tezuka, developed into the Super Robot genre under Go Nagai and others, and was revolutionized at the end of the decade by Yoshiyuki Tomino who developed the Real Robot genre. Robot anime like the "Gundam" and "The Super Dimension Fortress Macross" series became instant classics in the 1980s, and the robot genre of anime is still one of the most common in Japan and worldwide today. In the 1980s, anime became more accepted in the mainstream in Japan (although less than manga), and experienced a boom in production. Following a few successful adaptations of anime in overseas markets in the 1980s, anime gained increased acceptance in those markets in the 1990s and even more at the turn of the 21st century. In 2002, "Spirited Away", a Studio Ghibli production directed by Hayao Miyazaki won the Golden Bear at the Berlin International Film Festival and in 2003 at the 75th Academy Awards it won the Academy Award for Best Animated Feature. 

Anime differs greatly from other forms of animation by its diverse art styles, methods of animation, its production, and its process. Visually, anime is a diverse art form that contains a wide variety of art styles, differing from one creator, artist, and studio. While no one art style predominates anime as a whole, they do share some similar attributes in terms of animation technique and character design.

Anime follows the typical production of animation, including storyboarding, voice acting, character design, and cel production ("Shirobako", itself a series, highlights many of the aspects involved in anime production). Since the 1990s, animators have increasingly used computer animation to improve the efficiency of the production process. Artists like Noburō Ōfuji pioneered the earliest anime works, which were experimental and consisted of images drawn on blackboards, stop motion animation of paper cutouts, and silhouette animation. Cel animation grew in popularity until it came to dominate the medium. In the 21st century, the use of other animation techniques is mostly limited to independent short films, including the stop motion puppet animation work produced by Tadahito Mochinaga, Kihachirō Kawamoto and Tomoyasu Murata. Computers were integrated into the animation process in the 1990s, with works such as "Ghost in the Shell" and "Princess Mononoke" mixing cel animation with computer-generated images. Fuji Film, a major cel production company, announced it would stop cel production, producing an industry panic to procure cel imports and hastening the switch to digital processes.

Prior to the digital era, anime was produced with traditional animation methods using a pose to pose approach. The majority of mainstream anime uses fewer expressive key frames and more in-between animation.

Japanese animation studios were pioneers of many limited animation techniques, and have given anime a distinct set of conventions. Unlike Disney animation, where the emphasis is on the movement, anime emphasizes the art quality and let limited animation techniques make up for the lack of time spent on movement. Such techniques are often used not only to meet deadlines but also as artistic devices. Anime scenes place emphasis on achieving three-dimensional views, and backgrounds are instrumental in creating the atmosphere of the work. The backgrounds are not always invented and are occasionally based on real locations, as exemplified in "Howl's Moving Castle" and "The Melancholy of Haruhi Suzumiya". Oppliger stated that anime is one of the rare mediums where putting together an all-star cast usually comes out looking "tremendously impressive".

The cinematic effects of anime differentiates itself from the stage plays found in American animation. Anime is cinematically shot as if by camera, including panning, zooming, distance and angle shots to more complex dynamic shots that would be difficult to produce in reality. In anime, the animation is produced before the voice acting, contrary to American animation which does the voice acting first; this can cause lip sync errors in the Japanese version.

Body proportions of human anime characters tend to accurately reflect the proportions of the human body in reality. The height of the head is considered by the artist as the base unit of proportion. Head heights can vary, but most anime characters are about seven to eight heads tall. Anime artists occasionally make deliberate modifications to body proportions to produce super deformed characters that feature a disproportionately small body compared to the head; many super deformed characters are two to four heads tall. Some anime works like "Crayon Shin-chan" completely disregard these proportions, in such a way that they resemble caricatured Western cartoons.

A common anime character design convention is exaggerated eye size. The animation of characters with large eyes in anime can be traced back to Osamu Tezuka, who was deeply influenced by such early animation characters as Betty Boop, who was drawn with disproportionately large eyes. Tezuka is a central figure in anime and manga history, whose iconic art style and character designs allowed for the entire range of human emotions to be depicted solely through the eyes. The artist adds variable color shading to the eyes and particularly to the cornea to give them greater depth. Generally, a mixture of a light shade, the tone color, and a dark shade is used. Cultural anthropologist Matt Thorn argues that Japanese animators and audiences do not perceive such stylized eyes as inherently more or less foreign. However, not all anime characters have large eyes. For example, the works of Hayao Miyazaki are known for having realistically proportioned eyes, as well as realistic hair colors on their characters.

Hair in anime is often unnaturally lively and colorful or uniquely styled. The movement of hair in anime is exaggerated and "hair action" is used to emphasize the action and emotions of characters for added visual effect. Poitras traces hairstyle color to cover illustrations on manga, where eye-catching artwork and colorful tones are attractive for children's manga. Despite being produced for a domestic market, anime features characters whose race or nationality is not always defined, and this is often a deliberate decision, such as in the "Pokémon" animated series.

Anime and manga artists often draw from a common canon of iconic facial expression illustrations to denote particular moods and thoughts. These techniques are often different in form than their counterparts in Western animation, and they include a fixed iconography that is used as shorthand for certain emotions and moods. For example, a male character may develop a nosebleed when aroused. A variety of visual symbols are employed, including sweat drops to depict nervousness, visible blushing for embarrassment, or glowing eyes for an intense glare.

The opening and credits sequences of most anime television episodes are accompanied by Japanese pop or rock songs, often by reputed bands. They may be written with the series in mind, but are also aimed at the general music market, and therefore often allude only vaguely or not at all to the themes or plot of the series. Pop and rock songs are also sometimes used as incidental music ("insert songs") in an episode, often to highlight particularly important scenes.

Anime are often classified by target demographic, including , , and a diverse range of genres targeting an adult audience. Shoujo and shounen anime sometimes contain elements popular with children of both sexes in an attempt to gain crossover appeal. Adult anime may feature a slower pace or greater plot complexity that younger audiences may typically find unappealing, as well as adult themes and situations. A subset of adult anime works featuring pornographic elements are labeled "R18" in Japan, and are internationally known as "hentai" (originating from ). By contrast, some anime subgenres incorporate "ecchi", sexual themes or undertones without depictions of sexual intercourse, as typified in the comedic or harem genres; due to its popularity among adolescent and adult anime enthusiasts, the inclusion of such elements is considered a form of fan service. Some genres explore homosexual romances, such as "yaoi" (male homosexuality) and "yuri" (female homosexuality). While often used in a pornographic context, the terms "yaoi" and "yuri" can also be used broadly in a wider context to describe or focus on the themes or the development of the relationships themselves.

Anime's genre classification differs from other types of animation and does not lend itself to simple classification. Gilles Poitras compared the labeling "Gundam 0080" and its complex depiction of war as a "giant robot" anime akin to simply labeling "War and Peace" a "war novel". Science fiction is a major anime genre and includes important historical works like Tezuka's "Astro Boy" and Yokoyama's "Tetsujin 28-go". A major subgenre of science fiction is mecha, with the "Gundam" metaseries being iconic. The diverse fantasy genre includes works based on Asian and Western traditions and folklore; examples include the Japanese feudal fairytale "InuYasha", and the depiction of Scandinavian goddesses who move to Japan to maintain a computer called Yggdrasil in "Ah! My Goddess". Genre crossing in anime is also prevalent, such as the blend of fantasy and comedy in "Dragon Half", and the incorporation of slapstick humor in the crime anime film "Castle of Cagliostro". Other subgenres found in anime include magical girl, harem, sports, martial arts, literary adaptations, medievalism, and war.

The animation industry consists of more than 430 production companies with some of the major studios including Toei Animation, Gainax, Madhouse, Gonzo, Sunrise, Bones, TMS Entertainment, Nippon Animation, P.A.Works, Studio Pierrot and Studio Ghibli. Many of the studios are organized into a trade association, The Association of Japanese Animations. There is also a labor union for workers in the industry, the Japanese Animation Creators Association. Studios will often work together to produce more complex and costly projects, as done with Studio Ghibli's "Spirited Away". An anime episode can cost between US$100,000 and US$300,000 to produce. In 2001, animation accounted for 7% of the Japanese film market, above the 4.6% market share for live-action works. The popularity and success of anime is seen through the profitability of the DVD market, contributing nearly 70% of total sales. According to a 2016 article on Nikkei Asian Review, Japanese television stations have bought over worth of anime from production companies "over the past few years", compared with under from overseas. There has been a rise in sales of shows to television stations in Japan, caused by late night anime with adults as the target demographic. This type of anime is less popular outside Japan, being considered "more of a niche product". "Spirited Away" (2001) is the all-time highest-grossing film in Japan. It was also the highest-grossing anime film worldwide until it was overtaken by Makoto Shinkai's 2016 film "Your Name". Anime films represent a large part of the highest-grossing Japanese films yearly in Japan, with 6 out of the top 10 in 2014, in 2015 and also in 2016.

Anime has to be licensed by companies in other countries in order to be legally released. While anime has been licensed by its Japanese owners for use outside Japan since at least the 1960s, the practice became well-established in the United States in the late 1970s to early 1980s, when such TV series as "Gatchaman" and "Captain Harlock" were licensed from their Japanese parent companies for distribution in the US market. The trend towards American distribution of anime continued into the 1980s with the licensing of titles such as "Voltron" and the 'creation' of new series such as "Robotech" through use of source material from several original series.

In the early 1990s, several companies began to experiment with the licensing of less children-oriented material. Some, such as A.D. Vision, and Central Park Media and its imprints, achieved fairly substantial commercial success and went on to become major players in the now very lucrative American anime market. Others, such as AnimEigo, achieved limited success. Many companies created directly by Japanese parent companies did not do as well, most releasing only one or two titles before completing their American operations.

Licenses are expensive, often hundreds of thousands of dollars for one series and tens of thousands for one movie. The prices vary widely; for example, "" cost only $91,000 to license while "Kurau Phantom Memory" cost $960,000. Simulcast Internet streaming rights can be cheaper, with prices around $1,000-$2,000 an episode, but can also be more expensive, with some series costing more than per episode.

The anime market for the United States was worth approximately $2.74 billion in 2009. Dubbed animation began airing in the United States in 2000 on networks like The WB and Cartoon Network's Adult Swim. In 2005, this resulted in five of the top ten anime titles having previously aired on Cartoon Network. As a part of localization, some editing of cultural references may occur to better follow the references of the non-Japanese culture. The cost of English localization averages US$10,000 per episode.

The industry has been subject to both praise and condemnation for fansubs, the addition of unlicensed and unauthorized subtitled translations of anime series or films. Fansubs, which were originally distributed on VHS bootlegged cassettes in the 1980s, have been freely available and disseminated online since the 1990s. Since this practice raises concerns for copyright and piracy issues, fansubbers tend to adhere to an unwritten moral code to destroy or no longer distribute an anime once an official translated or subtitled version becomes licensed. They also try to encourage viewers to buy an official copy of the release once it comes out in English, although fansubs typically continue to circulate through file sharing networks. Even so, the laid back regulations of the Japanese animation industry tends to overlook these issues, allowing it to grow underground and thus increasing the popularity until there is a demand for official high quality releases for animation companies. This has led to an increase in global popularity with Japanese animations, reaching $40 million in sales in 2004.

Legal international availability of anime on the Internet has changed in recent years, with simulcasts of series available on websites like Crunchyroll.

Japan External Trade Organization (JETRO) valued the domestic anime market in Japan at (), including from licensed products, in 2005. JETRO reported sales of overseas anime exports in 2004 to be (). JETRO valued the anime market in the United States at (), including in home video sales and over from licensed products, in 2005. JETRO projected in 2005 that the worldwide anime market, including sales of licensed products, would grow to (). The anime market in China was valued at in 2017, and is projected to reach by 2020.

The anime industry has several annual awards which honor the year's best works. Major annual awards in Japan include the Ōfuji Noburō Award, the Mainichi Film Award for Best Animation Film, the Animation Kobe Awards, the Japan Media Arts Festival animation awards, the Tokyo Anime Award and the Japan Academy Prize for Animation of the Year. In the United States, anime films compete in the Crunchyroll Anime Awards. There were also the American Anime Awards, which were designed to recognize excellence in anime titles nominated by the industry, and were held only once in 2006. Anime productions have also been nominated and won awards not exclusively for anime, like the Academy Award for Best Animated Feature or the Golden Bear.

Anime has become commercially profitable in Western countries, as demonstrated by early commercially successful Western adaptations of anime, such as "Astro Boy" and "Speed Racer". Early American adaptions in the 1960s made Japan expand into the continental European market, first with productions aimed at European and Japanese children, such as "Heidi", "Vicky the Viking" and "Barbapapa", which aired in various countries. Particularly Italy, Spain and France grew an interest into Japan's output, due to its cheap selling price and productive output. In fact, Italy imported the most anime outside of Japan. These mass imports influenced anime popularity in South American, Arabic and German markets.

The beginning of 1980 saw the introduction of Japanese anime series into the American culture. In the 1990s, Japanese animation slowly gained popularity in America. Media companies such as Viz and Mixx began publishing and releasing animation into the American market. The 1988 film "Akira" is largely credited with popularizing anime in the Western world during the early 1990s, before anime was further popularized by television shows such as "Pokémon" and "Dragon Ball Z" in the late 1990s. The growth of the Internet later provided Western audiences an easy way to access Japanese content. This is especially the case with net services such as Netflix and Crunchyroll. As a direct result, various interests surrounding Japan have increased.

Anime clubs gave rise to anime conventions in the 1990s with the "anime boom", a period marked by increased popularity of anime. These conventions are dedicated to anime and manga and include elements like cosplay contests and industry talk panels. Cosplay, a portmanteau for "costume play", is not unique to anime and has become popular in contests and masquerades at anime conventions. Japanese culture and words have entered English usage through the popularity of the medium, including "otaku", an unflattering Japanese term commonly used in English to denote a fan of anime and manga. Another word that has arisen describing fans in the United States is "wapanese" meaning White individuals who desire to be Japanese, or later known as "weeaboo" for individuals who demonstrate a strong interest in Japanese anime subculture, which is a term that originated from abusive content posted on the popular bulletin board website 4chan.org. Anime enthusiasts have produced fan fiction and fan art, including computer wallpapers and anime music videos.

As of the 2010s, many anime fans use online communities and databases such as MyAnimeList to discuss anime and track their progress watching respective series.

One of the key points that made anime different from a handful of the Western cartoons is the potential for visceral content. Once the expectation that the aspects of visual intrigue or animation being just for children is put aside, the audience can realize that themes involving violence, suffering, sexuality, pain, and death can all be storytelling elements utilized in anime as much as other types of media. However, as anime itself became increasingly popular, its styling has been inevitably the subject of both satire and serious creative productions. "South Park"s "Chinpokomon" and "Good Times with Weapons" episodes, Adult Swim's "Perfect Hair Forever", and Nickelodeon's "Kappa Mikey" are Western examples of satirical depictions of Japanese culture and anime, but anime tropes have also been satirized by some anime, such as "KonoSuba".

Traditionally only Japanese works have been considered anime, but some works have sparked debate for blurring the lines between anime and cartoons, such as the American anime style production "". These anime styled works have become defined as anime-influenced animation, in an attempt to classify all anime styled works of non-Japanese origin. Some creators of these works cite anime as a source of inspiration, for example the French production team for "Ōban Star-Racers" that moved to Tokyo to collaborate with a Japanese production team. When anime is defined as a "style" rather than as a national product it leaves open the possibility of anime being produced in other countries, but this has been contentious amongst fans, with John Oppliger stating, "The insistence on referring to original American art as Japanese "anime" or "manga" robs the work of its cultural identity."

A U.A.E.-Filipino produced TV series called "Torkaizer" is dubbed as the "Middle East's First Anime Show", and is currently in production and looking for funding. Netflix has produced multiple anime series in collaboration with Japanese animation studios, and in doing so, has offered a more accessible channel for distribution to Western markets.

The web-based series "RWBY", produced by Texas-based company Rooster Teeth, is produced using an anime art style, and the series has been described as "anime" by multiple sources. For example, "Adweek", in the headline to one of its articles, described the series as "American-made anime", and in another headline, "The Huffington Post" described it as simply "anime", without referencing its country of origin. In 2013, Monty Oum, the creator of "RWBY", said “Some believe just like Scotch needs to be made in Scotland, an American company can’t make anime. I think that’s a narrow way of seeing it. Anime is an art form, and to say only one country can make this art is wrong." "RWBY" has been released in Japan with a Japanese language dub; the CEO of Rooster Teeth, Matt Hullum, commented "This is the first time any American-made anime has been marketed to Japan. It definitely usually works the other way around, and we're really pleased about that."

In Japanese culture and entertainment, media mix is a strategy to disperse content across multiple representations: different broadcast media, gaming technologies, cell phones, toys, amusement parks, and other methods. It is the Japanese term for a transmedia franchise. The term gained its circulation in late 1980s, but the origins of the strategy can be traced back to the 1960s with the proliferation of anime, with its interconnection of media and commodity goods.

A number of anime media franchises have gained considerable global popularity, and are among the world's highest-grossing media franchises. "Pokémon" in particular is the highest-grossing media franchise of all time, bigger than "Star Wars" and "Marvel Cinematic Universe". Other anime media franchises among the world's top 15 highest-grossing media franchises include "Hello Kitty", "Gundam", and "Dragon Ball", while the top 30 also includes "Fist of the North Star", "Yu-Gi-Oh" and "Evangelion".




</doc>
<doc id="53407486" url="https://en.wikipedia.org/wiki?curid=53407486" title="Anime in hip hop">
Anime in hip hop

Anime in hip hop is a recent phenomenon in which anime and hip hop, two vastly different subcultures, have collided to form a new subgenre in today's globalised popular culture. 

Many notable rappers such as RZA (of the Wu-Tang Clan), Kanye West, and Frank Ocean have taken inspiration from anime when creating their music. In mid-2015 Canadian rapper Sese, aka Lord Frieza, caught the attention of this subgenre when he released his mixtape "The Frieza Saga", which was entirely inspired by "Dragon Ball Z". "One of my friends was talking about how epic the battles on "Dragon Ball Z" were and then we started talking about how crazy the parallels between hip-hop and the show are," Sese explains.

Since the early 1960s, anime has become increasingly more profitable in Western countries. The growth of the Internet provided Western audiences an easy way to access Japanese content. This has seemingly influenced many anime creators to incorporate more Western culture in their productions. The Western market has influenced the creation of many popular hip-hop inspired anime titles such as "Afro Samurai", "Samurai Champloo", "Tokyo Tribes", "PaRappa the Rapper", and "Detroit Metal City".



</doc>
<doc id="58128778" url="https://en.wikipedia.org/wiki?curid=58128778" title="2.5D musical">
2.5D musical

A (abbreviated 2.5D musical), also known as an anime musical, is a type of modern Japanese musical theatre production based exclusively on popular Japanese anime, manga, or video games. The term "2.5D musical" was coined to describe stories presented in a two-dimensional medium being brought to real life.

Approximately 70 2.5D musicals were produced in 2013 and attracted at least 1.6 million people, most of them young women in their teens and 20s. 2.5D musicals are often seen as the starting point of many young actors in Japan.

2.5D musicals are defined through make-up and costuming that accurately depicts the actor as the original character, along with exaggerated acting that mimics the expressions in the original work. It also include special effects and stunts that reenact the setting and tone of the original work. Directors of the musicals are usually the ones who write the lyrics to the songs. With the evolution of technology, some of the modern 2.5D musicals uses projection mapping, in which backgrounds and special effects are projected onto the stage and screens. According to the Japan 2.5-Dimensional Musical Association, the term not only applies to musicals, but also plays, comedies, and dramas.

The first successful manga-based musical production was "The Rose of Versailles" in 1974 by the Takarazuka Revue. At the time, these plays were simply known as "musicals" or "anime musicals." Around the 1990s, a number of musicals and small stage skits produced were based on anime and manga series aimed at elementary school girls, such as "Sailor Moon", "Akazukin Chacha", and "Hime-chan's Ribbon", which performed moderately well, but were not popular and were known as . However, in 2000, "Hunter x Hunter" was considered revolutionary for the time because the voice cast for the original anime series had also played the characters onstage.

Japanese media-based musicals rose to popularity in 2003 with "Musical: The Prince of Tennis" through word-of-mouth and social media, which soon became a starting point for many up-and-coming actors. The shows attracted more than 2 million people during its run and was notable for using stage effects to simulate a tennis match, and it was popular enough to include its first overseas shows in South Korea and Taiwan in 2008. After its success, many productions based on anime, manga, and video games soon followed, some of the well-documented ones including "Naruto", "Yowamushi Pedal", "Hyper Projection Engeki Haikyu!!" among others. Unlike productions featuring the Takarazuka Revue, which are supported by fans of the troupe, these musicals mainly draw anime and manga fans and other audiences that usually do not see plays on a regular basis.

The term "2.5D musical" was codified in 2014 when the initial director of "Musical: The Prince of Tennis", Makoto Matsuda, first established the Japan 2.5-Dimensional Musical Association. At first, despite the success of "Musical: The Prince of Tennis", he did not consider it a formal stage production on par with most modern theater performances imported from Western works such as Broadway productions. However, after a group of South Korean musical professionals had acknowledged "Black Butler"'s production value and standards in performing arts, Matsuda decided to bring the genre worldwide. Plays certified by the Japan 2.5-Dimensional Musical Association offer theater glasses that contain subtitles in four other languages for people who do not speak Japanese. Since 2014, many 2.5D musicals have also been performed abroad in places like China, Taiwan, the United States, and parts of Europe.

In 2018, "2.5D Musical Studies" was added as a program at the Tokyo School of Anime. In April 2018, actor Kenta Suga, who has starred as Gaara in "Naruto" and Hinata in "Hyper Projection Engeki Haikyu!!", was appointed as overseas ambassador by the Japan 2.5-Dimensional Musical Association, succeeding Ryo Kato. By the end of 2018, the 2.5D musical market had increased by 44.9% over the previous year, grossing .



</doc>
<doc id="8011421" url="https://en.wikipedia.org/wiki?curid=8011421" title="Hiroshima International Animation Festival">
Hiroshima International Animation Festival

The International Animation Festival Hiroshima is a biennial animation festival hosted in Hiroshima, Japan.
The festival was found in 1985 by "Association International du Film d'Animation" or "ASIFA" as "International Animation Festival for the World Peace". The city of Hiroshima was one of the sites of nuclear bombings in 1945 at the end of World War II and it was chosen to inspire thoughts of unity through the arts. The festival is now considered one of the most respected animated festivals, along with Annecy International Animated Film Festival, Ottawa International Animation Festival, and Zagreb World Festival of Animated Films.

The first two festivals were held in odd years (1985 and 1987); since 1990, the festival has been held biennially in even years. In 2008, the 12th Festival takes place for 5 days (Aug. 7-Aug. 11). The city of Hiroshima co-hosts the festival, which takes place in Aste Plaza near the Hiroshima Peace Memorial Park at the center of Hiroshima city.

The founding of the festival is largely credited to Sayoko Kinoshita and her late husband Renzo Kinoshita. The married couple were renowned figures in the independent animation world and also founders of ASIFA's Japan chapter. Sayoko Kinoshita has been the festival director since the first festival and is now also the president of ASIFA.

In the festival's first year in 1985, the Grand Prize was awarded to "Broken Down Film" by Osamu Tezuka and the animator became one of the members of the jury for the following festival. This cycle has often repeated and many of the grand prize winners have become judges for the following festival.

In 2010, the Festival had nearly 1,937 entries from 57 countries and regions, and had more than 34,000 participants. The next festival will take place in 2020 and submissions for entries will begin in late 2019. 






</doc>
<doc id="60029105" url="https://en.wikipedia.org/wiki?curid=60029105" title="Madame Butterfly's Illusion">
Madame Butterfly's Illusion

A Japanese woman reflects on her ill-fated marriage to an American naval officer. She ultimately concludes that "it is better to die with honor than live in shame."

Outside of his work as a dentist, Arai collaborated with a small group of friends (particularly Tobiishi Nakaya) to create roughly one short animated film each year between 1939 and 1947. All of these films are in the style of silhouette animation and many, including "Madame Butterfly's Illusion" (1940), "Jakku to mame no ki" ("Jack and the Beanstalk", 1941), and "Kaguyahime" ("Princess Kaguya", 1942), are based on popular tales. It is believed that Arai - despite his talents as an animator - stopped creating films soon after World War II because of Tobiishi's untimely death.

The film's Puccini-esque score was composed and performed by renowned Japanese opera singer Tamaki Miura.

Arai is the focus of Minami Masatoki's short documentary Arai Wagorō: Kage-e Animēshon no Sekai (Wagorō Arai: His World of Silhouette Animation, 2013), which screened at the Hiroshima International Animation Festival in 2016.

A 35mm print of the film is held in the National Film Archive of Japan and was screened by NFAJ in 2017 alongside animated films by Ōfuji Noburō and Murata Yasuji.


</doc>
<doc id="61778781" url="https://en.wikipedia.org/wiki?curid=61778781" title="List of anime distributed in India">
List of anime distributed in India

This is a list of anime distributed in India. These anime series and films have been shown and have achieved varying levels of popularity in India. In India, most anime can be seen televised on channels, with most channels such as Toonami India, Cartoon Network, Pogo, Nickelodeon, Disney XD, Marvel HQ, Sonic Nickelodeon, Hungama TV, Disney Channel airing anime targeted toward children audiences and with channel such as Animax India airing anime targeted toward young adults to adult audiences, Animax India was later ceased broadcasting. Anime can also be viewed on streaming websites such as Crunchyroll, Amazon Prime Video and Netflix.



English version

Japanese version



</doc>
<doc id="61051062" url="https://en.wikipedia.org/wiki?curid=61051062" title="Marie &amp; Gali">
Marie &amp; Gali

Marie & Gali () is a Japanese anime created by Izumi Todo. It aired on NHK from March 30, 2009, until March 22, 2011, for a total of 70 episodes of five minutes each. It was the first anime (in over three decades) produced by Toei Animation to be broadcast on the NHK-E educational channel since 1978's Captain Future. 
A sequel, "Marie & Gali ver.2.0" aired from March 30, 2010, until March 22, 2011.

The anime follows Marika, a student who loves the gothic lolita aesthetic, but falls asleep as soon as someone starts talking about science. One day, after a nap on the train, she discovers that her stuffed animal, Pet, is able to move and ends up in the strange city of Galihabara. Here, Marika meets the astronomer Galileo and other famous scientists, and with a bit of their help, starts to appreciate science little by little.

The theme of the first season is , sung by , while that of the second season is , Sung by Chiba Chiemi and Inoue Marina (VAs of Marika and Norika).

A video game, was developed by DORASU for the Nintendo DS and released in Japan on April 15, 2010.



</doc>
<doc id="35783863" url="https://en.wikipedia.org/wiki?curid=35783863" title="Prize book">
Prize book

In the fields of bibliography and bookselling, prize books (also known as prize bindings), are a category of finely bound books once given as prizes and awards in educational institutions primarily in England, Ireland, and the Netherlands. This tradition flourished in Latin schools in continental Europe from the mid-17th century until the introduction of publishers bindings after about 1830. Books continued to be given as prizes at academic ceremonies, but the only distinguishing feature are typically special inscriptions and/or bookplates.

In the United Kingdom, the tradition of special prize bindings persisted until the mid-20th century with the demise of traditional hand binding. Most titles consist of classical works in the humanities published in unbound print runs for this purpose. Schools would contract with a local bindery to prepare prize editions stamped or embossed with the logo of the school. The editions are often made to resemble the fine bindings of the 18th century and are prized by booksellers for their classical appearance. They are often mistaken as ex-school library books but can be distinguished by a prize inscription, lack of call number, and general higher quality of the binding.



</doc>
<doc id="35953231" url="https://en.wikipedia.org/wiki?curid=35953231" title="Outline of books">
Outline of books

The following outline is provided as an overview of and topical guide to books:
Book – set of written, printed, illustrated, or blank sheets, made of ink, paper, parchment, or other materials, usually fastened together to hinge at one side.

A book is a medium for a collection of words and/or pictures to represent knowledge, often manifested in bound paper and ink, or in electronic format such as e-books.

Books can be described as all of the following:

Physical types of books not to be confused with literary genres or types of literature.



Book design – the common structural parts of a book include:




History of books






Lists of books – list of book lists (bibliographies) on Wikipedia 


New York Review of Books – an American magazine containing lietary criticism, and discussions of the contents of various books.





</doc>
<doc id="3778" url="https://en.wikipedia.org/wiki?curid=3778" title="Book">
Book

A book is a medium for recording information in the form of writing or images, typically composed of many pages (made of papyrus, parchment, vellum, or paper) bound together and protected by a cover. The technical term for this physical arrangement is "codex" (plural, "codices"). In the history of hand-held physical supports for extended written compositions or records, the codex replaces its immediate predecessor, the scroll. A single sheet in a codex is a leaf, and each side of a leaf is a page.

As an intellectual object, a book is prototypically a composition of such great length that it takes a considerable investment of time to compose and a still considerable, though not so extensive, investment of time to read. This sense of book has a restricted and an unrestricted sense. In the restricted sense, a book is a self-sufficient section or part of a longer composition, a usage that reflects the fact that, in antiquity, long works had to be written on several scrolls, and each scroll had to be identified by the book it contained. So, for instance, each part of Aristotle's "Physics" is called a book. In the unrestricted sense, a book is the compositional whole of which such sections, whether called books or chapters or parts, are parts.

The intellectual content in a physical book need not be a composition, nor even be called a book. Books can consist only of drawings, engravings, or photographs, or such things as crossword puzzles or cut-out dolls. In a physical book, the pages can be left blank or can feature an abstract set of lines as support for ongoing entries, e.g., an account book, an appointment book, an autograph book, a notebook, a diary, or a sketchbook. Some physical books are made with pages thick and sturdy enough to support other physical objects, like a scrapbook or photograph album. Books may be distributed in electronic form as e-books and other formats.

Although in ordinary academic parlance a monograph is understood to be a specialist academic work, rather than a reference work on a single scholarly subject, in library and information science "monograph" denotes more broadly any non-serial publication complete in one volume (book) or a finite number of volumes (even a novel like Proust's seven-volume "In Search of Lost Time"), in contrast to serial publications like a magazine, journal, or newspaper. An avid reader or collector of books is a bibliophile or colloquially, "bookworm". A shop where books are bought and sold is a bookshop or bookstore. Books are also sold elsewhere. Books can also be borrowed from libraries. Google has estimated that as of 2010, approximately 130,000,000 distinct titles had been published. In some wealthier nations, the sale of printed books has decreased because of the increased usage of e-books.

The word book comes from Old English "bōc", which in turn comes from the Germanic root "*bōk-", cognate to "beech". Similarly, in Slavic languages (for example, Russian, Bulgarian, Macedonian) "буква" (bukva—"letter") is cognate with "beech". In Russian, Serbian and Macedonian, the word "букварь" (bukvar') or "буквар" (bukvar) refers specifically to a primary school textbook that helps young children master the techniques of reading and writing.
It is thus conjectured that the earliest Indo-European writings may have been carved on beech wood. Similarly, the Latin word "codex", meaning a book in the modern sense (bound and with separate leaves), originally meant "block of wood".

When writing systems were created in ancient civilizations, a variety of objects, such as stone, clay, tree bark, metal sheets, and bones, were used for writing; these are studied in epigraphy.

A tablet is a physically robust writing medium, suitable for casual transport and writing. Clay tablets were flattened and mostly dry pieces of clay that could be easily carried, and impressed with a stylus. They were used as a writing medium, especially for writing in cuneiform, throughout the Bronze Age and well into the Iron Age. Wax tablets were pieces of wood covered in a coating of wax thick enough to record the impressions of a stylus. They were the normal writing material in schools, in accounting, and for taking notes. They had the advantage of being reusable: the wax could be melted, and reformed into a blank.

The custom of binding several wax tablets together (Roman "pugillares") is a possible precursor of modern bound (codex) books. The etymology of the word "codex" (block of wood) also suggests that it may have developed from wooden wax tablets.

Scrolls can be made from papyrus, a thick paper-like material made by weaving the stems of the papyrus plant, then pounding the woven sheet with a hammer-like tool until it is flattened. Papyrus was used for writing in Ancient Egypt, perhaps as early as the First Dynasty, although the first evidence is from the account books of King Neferirkare Kakai of the Fifth Dynasty (about 2400 BC). Papyrus sheets were glued together to form a scroll. Tree bark such as lime and other materials were also used.

According to Herodotus (History 5:58), the Phoenicians brought writing and papyrus to Greece around the 10th or 9th century BC. The Greek word for papyrus as writing material ("biblion") and book ("biblos") come from the Phoenician port town Byblos, through which papyrus was exported to Greece. From Greek we also derive the word tome (), which originally meant a slice or piece and from there began to denote "a roll of papyrus". "Tomus" was used by the Latins with exactly the same meaning as "volumen" (see also below the explanation by Isidore of Seville).

Whether made from papyrus, parchment, or paper, scrolls were the dominant form of book in the Hellenistic, Roman, Chinese, Hebrew, and Macedonian cultures. The more modern codex book format form took over the Roman world by late antiquity, but the scroll format persisted much longer in Asia.

Isidore of Seville (d. 636) explained the then-current relation between codex, book and scroll in his Etymologiae (VI.13): "A codex is composed of many books; a book is of one scroll. It is called codex by way of metaphor from the trunks ("codex") of trees or vines, as if it were a wooden stock, because it contains in itself a multitude of books, as it were of branches." Modern usage differs.

A codex (in modern usage) is the first information repository that modern people would recognize as a "book": leaves of uniform size bound in some manner along one edge, and typically held between two covers made of some more robust material. The first written mention of the codex as a form of book is from Martial, in his Apophoreta at the end of the first century, where he praises its compactness. However, the codex never gained much popularity in the pagan Hellenistic world, and only within the Christian community did it gain widespread use. This change happened gradually during the 3rd and 4th centuries, and the reasons for adopting the codex form of the book are several: the format is more economical, as both sides of the writing material can be used; and it is portable, searchable, and easy to conceal. A book is much easier to read, to find a page that you want, and to flip through. A scroll is more awkward to use. The Christian authors may also have wanted to distinguish their writings from the pagan and Judaic texts written on scrolls. In addition, some metal books were made, that required smaller pages of metal, instead of an impossibly long, unbending scroll of metal. A book can also be easily stored in more compact places, or side by side in a tight library or shelf space.

The fall of the Roman Empire in the 5th century AD saw the decline of the culture of ancient Rome. Papyrus became difficult to obtain due to lack of contact with Egypt, and parchment, which had been used for centuries, became the main writing material. Parchment is a material made from processed animal skin and used—mainly in the past—for writing on.
Parchment is most commonly made of calfskin, sheepskin, or goatskin. It was historically used for writing documents, notes, or the pages of a book. Parchment is limed, scraped and dried under tension. It is not tanned, and is thus different from leather. This makes it more suitable for writing on, but leaves it very reactive to changes in relative humidity and makes it revert to rawhide if overly wet.

Monasteries carried on the Latin writing tradition in the Western Roman Empire. Cassiodorus, in the monastery of Vivarium (established around 540), stressed the importance of copying texts. St. Benedict of Nursia, in his "Rule of Saint Benedict" (completed around the middle of the 6th century) later also promoted reading. The "Rule of Saint Benedict" (Ch. ), which set aside certain times for reading, greatly influenced the monastic culture of the Middle Ages and is one of the reasons why the clergy were the predominant readers of books. The tradition and style of the Roman Empire still dominated, but slowly the peculiar medieval book culture emerged.
Before the invention and adoption of the printing press, almost all books were copied by hand, which made books expensive and comparatively rare. Smaller monasteries usually had only a few dozen books, medium-sized perhaps a few hundred. By the 9th century, larger collections held around 500 volumes and even at the end of the Middle Ages, the papal library in Avignon and Paris library of the Sorbonne held only around 2,000 volumes.

The "scriptorium" of the monastery was usually located over the chapter house. Artificial light was forbidden for fear it may damage the manuscripts. There were five types of scribes:
The bookmaking process was long and laborious. The parchment had to be prepared, then the unbound pages were planned and ruled with a blunt tool or lead, after which the text was written by the scribe, who usually left blank areas for illustration and rubrication. Finally, the book was bound by the bookbinder.
Different types of ink were known in antiquity, usually prepared from soot and gum, and later also from gall nuts and iron vitriol. This gave writing a brownish black color, but black or brown were not the only colors used. There are texts written in red or even gold, and different colors were used for illumination. For very luxurious manuscripts the whole parchment was colored purple, and the text was written on it with gold or silver (for example, Codex Argenteus).

Irish monks introduced spacing between words in the 7th century. This facilitated reading, as these monks tended to be less familiar with Latin. However, the use of spaces between words did not become commonplace before the 12th century. It has been argued that the use of spacing between words shows the transition from semi-vocalized reading into silent reading.

The first books used parchment or vellum (calfskin) for the pages. The book covers were made of wood and covered with leather. Because dried parchment tends to assume the form it had before processing, the books were fitted with clasps or straps. During the later Middle Ages, when public libraries appeared, up to the 18th century, books were often chained to a bookshelf or a desk to prevent theft. These chained books are called "libri catenati".

At first, books were copied mostly in monasteries, one at a time. With the rise of universities in the 13th century, the Manuscript culture of the time led to an increase in the demand for books, and a new system for copying books appeared. The books were divided into unbound leaves ("pecia"), which were lent out to different copyists, so the speed of book production was considerably increased. The system was maintained by secular stationers guilds, which produced both religious and non-religious material.

Judaism has kept the art of the scribe alive up to the present. According to Jewish tradition, the Torah scroll placed in a synagogue must be written by hand on parchment and a printed book would not do, though the congregation may use printed prayer books and printed copies of the Scriptures are used for study outside the synagogue. A sofer "scribe" is a highly respected member of any observant Jewish community.

People of various religious (Jews, Christians, Zoroastrians, Muslims) and ethnic backgrounds (Syriac, Coptic, Persian, Arab etc.) in the Middle East also produced and bound books in the Islamic Golden Age (mid 8th century to 1258), developing advanced techniques in Islamic calligraphy, miniatures and bookbinding. A number of cities in the medieval Islamic world had book production centers and book markets. Yaqubi (d. 897) says that in his time Baghdad had over a hundred booksellers. Book shops were often situated around the town's principal mosque as in Marrakesh, Morocco, that has a street named "Kutubiyyin" or book sellers in English and the famous Koutoubia Mosque is named so because of its location in this street.

The medieval Muslim world also used a method of reproducing reliable copies of a book in large quantities known as check reading, in contrast to the traditional method of a single scribe producing only a single copy of a single manuscript. In the check reading method, only "authors could authorize copies, and this was done in public sessions in which the copyist read the copy aloud in the presence of the author, who then certified it as accurate." With this check-reading system, "an author might produce a dozen or more copies from a single reading," and with two or more readings, "more than one hundred copies of a single book could easily be produced." By using as writing material the relatively cheap paper instead of parchment or papyrus the Muslims, in the words of Pedersen "accomplished a feat of crucial significance not only to the history of the Islamic book, but also to the whole world of books".

In woodblock printing, a relief image of an entire page was carved into blocks of wood, inked, and used to print copies of that page. This method originated in China, in the Han dynasty (before 220 AD), as a method of printing on textiles and later paper, and was widely used throughout East Asia. The oldest dated book printed by this method is "The Diamond Sutra" (868 AD). The method (called woodcut when used in art) arrived in Europe in the early 14th century. Books (known as block-books), as well as playing-cards and religious pictures, began to be produced by this method. Creating an entire book was a painstaking process, requiring a hand-carved block for each page; and the wood blocks tended to crack, if stored for long. The monks or people who wrote them were paid highly.

The Chinese inventor Bi Sheng made movable type of earthenware c. 1045, but there are no known surviving examples of his printing. Around 1450, in what is commonly regarded as an independent invention, Johannes Gutenberg invented movable type in Europe, along with innovations in casting the type based on a matrix and hand mould. This invention gradually made books less expensive to produce, and more widely available.

Early printed books, single sheets and images which were created before 1501 in Europe are known as incunables or "incunabula". "A man born in 1453, the year of the fall of Constantinople, could look back from his fiftieth year on a lifetime in which about eight million books had been printed, more perhaps than all the scribes of Europe had produced since Constantine founded his city in AD 330."

Steam-powered printing presses became popular in the early 19th century. These machines could print 1,100 sheets per hour, but workers could only set 2,000 letters per hour. Monotype and linotype typesetting machines were introduced in the late 19th century. They could set more than 6,000 letters per hour and an entire line of type at once. There have been numerous improvements in the printing press. As well, the conditions for freedom of the press have been improved through the gradual relaxation of restrictive censorship laws. See also intellectual property, public domain, copyright. In mid-20th century, European book production had risen to over 200,000 titles per year.

Throughout the 20th century, libraries have faced an ever-increasing rate of publishing, sometimes called an information explosion. The advent of electronic publishing and the internet means that much new information is not printed in paper books, but is made available online through a digital library, on CD-ROM, in the form of e-books or other online media. An on-line book is an e-book that is available online through the internet. Though many books are produced digitally, most digital versions are not available to the public, and there is no decline in the rate of paper publishing. There is an effort, however, to convert books that are in the public domain into a digital medium for unlimited redistribution and infinite availability. This effort is spearheaded by Project Gutenberg combined with Distributed Proofreaders. There have also been new developments in the process of publishing books. Technologies such as POD or "print on demand", which make it possible to print as few as one book at a time, have made self-publishing (and vanity publishing) much easier and more affordable. On-demand publishing has allowed publishers, by avoiding the high costs of warehousing, to keep low-selling books in print rather than declaring them out of print.

The methods used for the printing and binding of books continued fundamentally unchanged from the 15th century into the early 20th century. While there was more mechanization, a book printer in 1900 had much in common with Gutenberg. Gutenberg's invention was the use of movable metal types, assembled into words, lines, and pages and then printed by letterpress to create multiple copies. Modern paper books are printed on papers designed specifically for printed books. Traditionally, book papers are off-white or low-white papers (easier to read), are opaque to minimise the show-through of text from one side of the page to the other and are (usually) made to tighter caliper or thickness specifications, particularly for case-bound books. Different paper qualities are used depending on the type of book: Machine finished coated papers, woodfree uncoated papers, coated fine papers and special fine papers are common paper grades.

Today, the majority of books are printed by offset lithography. When a book is printed, the pages are laid out on the plate so that after the printed sheet is folded the pages will be in the correct sequence. Books tend to be manufactured nowadays in a few standard sizes. The sizes of books are usually specified as "trim size": the size of the page after the sheet has been folded and trimmed. The standard sizes result from sheet sizes (therefore machine sizes) which became popular 200 or 300 years ago, and have come to dominate the industry. British conventions in this regard prevail throughout the English-speaking world, except for the USA. The European book manufacturing industry works to a completely different set of standards.

Modern bound books are organized according to a particular format called the book's "layout". Although there is great variation in layout, modern books tend to adhere to as set of rules with regard to what the parts of the layout are and what their content usually includes. A basic layout will include a "front cover", a "back cover" and the book's content which is called its "body copy" or "content pages". The front cover often bears the book's title (and subtitle, if any) and the name of its author or editor(s). The "inside front cover" page is usually left blank in both hardcover and paperback books. The next section, if present, is the book's "front matter", which includes all textual material after the front cover but not part of the book's content such as a foreword, a dedication, a table of contents and publisher data such as the book's edition or printing number and place of publication. Between the body copy and the back cover goes the "end matter" which would include any indices, sets of tables, diagrams, glossaries or lists of cited works (though an edited book with several authors usually places cited works at the end of each authored chapter). The "inside back cover" page, like that inside the front cover, is usually blank. The "back cover" is the usual place for the book's ISBN and maybe a photograph of the author(s)/ editor(s), perhaps with a short introduction to them. Also here often appear plot summaries, barcodes and excerpted reviews of the book.

Some books, particularly those with shorter runs (i.e. fewer copies) will be printed on sheet-fed offset presses, but most books are now printed on web presses, which are fed by a continuous roll of paper, and can consequently print more copies in a shorter time. As the production line circulates, a complete "book" is collected together in one stack, next to another, and another web press carries out the folding itself, delivering bundles of "signatures" (sections) ready to go into the gathering line. Note that the pages of a book are printed two at a time, not as one complete book. Excess numbers are printed to make up for any spoilage due to make-readies or test pages to assure final print quality.

A "make-ready" is the preparatory work carried out by the pressmen to get the printing press up to the required quality of impression. Included in make-ready is the time taken to mount the plate onto the machine, clean up any mess from the previous job, and get the press up to speed. As soon as the pressman decides that the printing is correct, all the make-ready sheets will be discarded, and the press will start making books. Similar make readies take place in the folding and binding areas, each involving spoilage of paper.

After the signatures are folded and gathered, they move into the bindery. In the middle of last century there were still many trade binders – stand-alone binding companies which did no printing, specializing in binding alone. At that time, because of the dominance of letterpress printing, typesetting and printing took place in one location, and binding in a different factory. When type was all metal, a typical book's worth of type would be bulky, fragile and heavy. The less it was moved in this condition the better: so printing would be carried out in the same location as the typesetting. Printed sheets on the other hand could easily be moved. Now, because of increasing computerization of preparing a book for the printer, the typesetting part of the job has flowed upstream, where it is done either by separately contracting companies working for the publisher, by the publishers themselves, or even by the authors. Mergers in the book manufacturing industry mean that it is now unusual to find a bindery which is not also involved in book printing (and vice versa).

If the book is a hardback its path through the bindery will involve more points of activity than if it is a paperback. Unsewn binding, is now increasingly common. The signatures of a book can also be held together by "Smyth sewing" using needles, "McCain sewing", using drilled holes often used in schoolbook binding, or "notch binding", where gashes about an inch long are made at intervals through the fold in the spine of each signature. The rest of the binding process is similar in all instances. Sewn and notch bound books can be bound as either hardbacks or paperbacks.

"Making cases" happens off-line and prior to the book's arrival at the binding line. In the most basic case-making, two pieces of cardboard are placed onto a glued piece of cloth with a space between them into which is glued a thinner board cut to the width of the spine of the book. The overlapping edges of the cloth (about 5/8" all round) are folded over the boards, and pressed down to adhere. After case-making the stack of cases will go to the foil stamping area for adding decorations and type.

Recent developments in book manufacturing include the development of digital printing. Book pages are printed, in much the same way as an office copier works, using toner rather than ink. Each book is printed in one pass, not as separate signatures. Digital printing has permitted the manufacture of much smaller quantities than offset, in part because of the absence of make readies and of spoilage. One might think of a web press as printing quantities over 2000, quantities from 250 to 2000 being printed on sheet-fed presses, and digital presses doing quantities below 250. These numbers are of course only approximate and will vary from supplier to supplier, and from book to book depending on its characteristics. Digital printing has opened up the possibility of print-on-demand, where no books are printed until after an order is received from a customer.

In the 2000s, due to the rise in availability of affordable handheld computing devices, the opportunity to share texts through electronic means became an appealing option for media publishers. Thus, the "e-book" was made. The term e-book is a contraction of "electronic book"; it refers to a book-length publication in digital form. An e-book is usually made available through the internet, but also on CD-ROM and other forms. E-Books may be read either via a computing device with an LED display such as a traditional computer, a smartphone or a tablet computer; or by means of a portable e-ink display device known as an e-book reader, such as the Sony Reader, Barnes & Noble Nook, Kobo eReader, or the Amazon Kindle. E-book readers attempt to mimic the experience of reading a print book by using this technology, since the displays on e-book readers are much less reflective.

Book design is the art of incorporating the content, style, format, design, and sequence of the various components of a book into a coherent whole. In the words of Jan Tschichold, book design "though largely forgotten today, methods and rules upon which it is impossible to improve have been developed over centuries. To produce perfect books these rules have to be brought back to life and applied." Richard Hendel describes book design as "an arcane subject" and refers to the need for a context to understand what that means. Many different creators can contribute to book design, including graphic designers, artists and editors.

The size of a modern book is based on the printing area of a common flatbed press. The pages of type were arranged and clamped in a frame, so that when printed on a sheet of paper the full size of the press, the pages would be right side up and in order when the sheet was folded, and the folded edges trimmed.

The most common book sizes are:

Sizes smaller than 16mo are:

Small books can be called "booklets".

Sizes larger than quarto are:

The largest extant medieval manuscript in the world is Codex Gigas 92 × 50 × 22 cm. The world's largest book is made of stone and is in Kuthodaw Pagoda (Burma).

A common separation by content are fiction and non-fiction books. This simple separation can be found in most collections, libraries, and bookstores.

Many of the books published today are fiction, meaning that they are in-part or completely untrue. Historically, paper production was considered too expensive to be used for entertainment. An increase in global literacy and print technology led to the increased publication of books for the purpose of entertainment, and allegorical social commentary. Most fiction is additionally categorized by genre.

The novel is the most common form of fiction book. Novels are stories that typically feature a plot, setting, themes and characters. Stories and narrative are not restricted to any topic; a novel can be whimsical, serious or controversial. The novel has had a tremendous impact on entertainment and publishing markets. A novella is a term sometimes used for fiction prose typically between 17,500 and 40,000 words, and a novelette between 7,500 and 17,500. A short story may be any length up to 10,000 words, but these word lengths vary.

Comic books or graphic novels are books in which the story is illustrated. The characters and narrators use speech or thought bubbles to express verbal language.

In a library, a reference book is a general type of non-fiction book which provides information as opposed to telling a story, essay, commentary, or otherwise supporting a point of view. An almanac is a very general reference book, usually one-volume, with lists of data and information on many topics. An encyclopedia is a book or set of books designed to have more in-depth articles on many topics. A book listing words, their etymology, meanings, and other information is called a dictionary. A book which is a collection of maps is an atlas. A more specific reference book with tables or lists of data and information about a certain topic, often intended for professional use, is often called a handbook. Books which try to list references and abstracts in a certain broad area may be called an index, such as "Engineering Index", or abstracts such as chemical abstracts and biological abstracts.
Books with technical information on how to do something or how to use some equipment are called instruction manuals. Other popular how-to books include cookbooks and home improvement books.

Students typically store and carry textbooks and schoolbooks for study purposes. Elementary school pupils often use workbooks, which are published with spaces or blanks to be filled by them for study or homework. In US higher education, it is common for a student to take an exam using a blue book.

There is a large set of books that are made only to write private ideas, notes, and accounts. These books are rarely published and are typically destroyed or remain private. Notebooks are blank papers to be written in by the user. Students and writers commonly use them for taking notes. Scientists and other researchers use lab notebooks to record their notes. They often feature spiral coil bindings at the edge so that pages may easily be torn out.

Address books, phone books, and calendar/appointment books are commonly used on a daily basis for recording appointments, meetings and personal contact information.

Books for recording periodic entries by the user, such as daily information about a journey, are called logbooks or simply logs. A similar book for writing the owner's daily private personal events, information, and ideas is called a diary or personal journal.

Businesses use accounting books such as journals and ledgers to record financial data in a practice called bookkeeping.

There are several other types of books which are not commonly found under this system. Albums are books for holding a group of items belonging to a particular theme, such as a set of photographs, card collections, and memorabilia. One common example is stamp albums, which are used by many hobbyists to protect and organize their collections of postage stamps. Such albums are often made using removable plastic pages held inside in a ringed binder or other similar holder. Picture books are books for children with pictures on every page and less text (or even no text).

Hymnals are books with collections of musical hymns that can typically be found in churches. Prayerbooks or missals are books that contain written prayers and are commonly carried by monks, nuns, and other devoted followers or clergy. Lap books are a learning tool created by students.

A leveled book collection is a set of books organized in levels of difficulty from the easy books appropriate for an emergent reader to longer more complex books adequate for advanced readers. Decodable readers or books are a specialized type of leveled books that use decodable text only including controlled lists of words, sentences and stories consistent with the letters and phonics that have been taught to the emergent reader. New sounds and letters are added to higher level decodable books, as the level of instruction progresses, allowing for higher levels of accuracy, comprehension and fluency.

Hardcover books have a stiff binding. Paperback books have cheaper, flexible covers which tend to be less durable. An alternative to paperback is the glossy cover, otherwise known as a dust cover, found on magazines, and comic books. Spiral-bound books are bound by spirals made of metal or plastic. Examples of spiral-bound books include teachers' manuals and puzzle books (crosswords, sudoku).

Publishing is a process for producing pre-printed books, magazines, and newspapers for the reader/user to buy.

Publishers may produce low-cost, pre-publication copies known as galleys or 'bound proofs' for promotional purposes, such as generating reviews in advance of publication. Galleys are usually made as cheaply as possible, since they are not intended for sale.

Private or personal libraries made up of non-fiction and fiction books, (as opposed to the state or institutional records kept in archives) first appeared in classical Greece. In the ancient world, the maintaining of a library was usually (but not exclusively) the privilege of a wealthy individual. These libraries could have been either private or public, i.e. for people who were interested in using them. The difference from a modern public library lies in the fact that they were usually not funded from public sources. It is estimated that in the city of Rome at the end of the 3rd century there were around 30 public libraries. Public libraries also existed in other cities of the ancient Mediterranean region (for example, Library of Alexandria). Later, in the Middle Ages, monasteries and universities had also libraries that could be accessible to general public. Typically not the whole collection was available to public, the books could not be borrowed and often were chained to reading stands to prevent theft.

The beginning of modern public library begins around 15th century when individuals started to donate books to towns. The growth of a public library system in the United States started in the late 19th century and was much helped by donations from Andrew Carnegie. This reflected classes in a society: The poor or the middle class had to access most books through a public library or by other means while the rich could afford to have a private library built in their homes. In the United States the Boston Public Library 1852 "Report of the Trustees" established the justification for the public library as a tax-supported institution intended to extend educational opportunity and provide for general culture.

The advent of paperback books in the 20th century led to an explosion of popular publishing. Paperback books made owning books affordable for many people. Paperback books often included works from genres that had previously been published mostly in pulp magazines. As a result of the low cost of such books and the spread of bookstores filled with them (in addition to the creation of a smaller market of extremely cheap used paperbacks) owning a private library ceased to be a status symbol for the rich.

In library and booksellers' catalogues, it is common to include an abbreviation such as "Crown 8vo" to indicate the paper size from which the book is made.

When rows of books are lined on a book holder, bookends are sometimes needed to keep them from slanting.

During the 20th century, librarians were concerned about keeping track of the many books being added yearly to the Gutenberg Galaxy. Through a global society called the International Federation of Library Associations and Institutions (IFLA), they devised a series of tools including the International Standard Bibliographic Description (ISBD). Each book is specified by an International Standard Book Number, or ISBN, which is unique to every edition of every book produced by participating publishers, worldwide. It is managed by the ISBN Society. An ISBN has four parts: the first part is the country code, the second the publisher code, and the third the title code. The last part is a check digit, and can take values from 0–9 and X (10). The EAN Barcodes numbers for books are derived from the ISBN by prefixing 978, for Bookland, and calculating a new check digit.

Commercial publishers in industrialized countries generally assign ISBNs to their books, so buyers may presume that the ISBN is part of a total international system, with no exceptions. However, many government publishers, in industrial as well as developing countries, do not participate fully in the ISBN system, and publish books which do not have ISBNs. A large or public collection requires a catalogue. Codes called "call numbers" relate the books to the catalogue, and determine their locations on the shelves. Call numbers are based on a Library classification system. The call number is placed on the spine of the book, normally a short distance before the bottom, and inside. Institutional or national standards, such as ANSI/NISO Z39.41 - 1997, establish the correct way to place information (such as the title, or the name of the author) on book spines, and on "shelvable" book-like objects, such as containers for DVDs, video tapes and software.
One of the earliest and most widely known systems of cataloguing books is the Dewey Decimal System. Another widely known system is the Library of Congress Classification system. Both systems are biased towards subjects which were well represented in US libraries when they were developed, and hence have problems handling new subjects, such as computing, or subjects relating to other cultures. Information about books and authors can be stored in databases like online general-interest book databases. Metadata, which means "data about data" is information about a book. Metadata about a book may include its title, ISBN or other classification number (see above), the names of contributors (author, editor, illustrator) and publisher, its date and size, the language of the text, its subject matter, etc.


Aside from the primary purpose of reading them, books are also used for other ends:

Once the book is published, it is put on the market by the distributors and the bookstores. Meanwhile, his promotion comes from various media reports. Book marketing is governed by the law in many states.

In recent years, the book had a second life in the form of reading aloud. This is called public readings of published works, with the assistance of professional readers (often known actors) and in close collaboration with writers, publishers, booksellers, librarians, leaders of the literary world and artists.

Many individual or collective practices exist to increase the number of readers of a book. Among them:

This form of the book chain has hardly changed since the eighteenth century, and has not always been this way. Thus, the author has asserted gradually with time, and the copyright dates only from the nineteenth century. For many centuries, especially before the invention of printing, each freely copied out books that passed through his hands, adding if necessary his own comments. Similarly, bookseller and publisher jobs have emerged with the invention of printing, which made the book an industrial product, requiring structures of production and marketing.

The invention of the Internet, e-readers, tablets, and projects like Wikipedia and Gutenberg, are likely to strongly change the book industry in the years to come.

Paper was first made in China as early as 200 BC, and reached Europe through Muslim territories. At first made of rags, the industrial revolution changed paper-making practices, allowing for paper to be made out of wood pulp. Papermaking in Europe began in the 11th century, although vellum was also common there as page material up until the beginning of the 16th century, vellum being the more expensive and durable option. Printers or publishers would often issue the same publication on both materials, to cater to more than one market.

Paper made from wood pulp became popular in the early 20th century, because it was cheaper than linen or abaca cloth-based papers. Pulp-based paper made books less expensive to the general public. This paved the way for huge leaps in the rate of literacy in industrialised nations, and enabled the spread of information during the Second Industrial Revolution.

Pulp paper, however, contains acid which eventually destroys the paper from within. Earlier techniques for making paper used limestone rollers, which neutralized the acid in the pulp. Books printed between 1850 and 1950 are primarily at risk; more recent books are often printed on acid-free or alkaline paper. Libraries today have to consider mass deacidification of their older collections in order to prevent decay.

Stability of the climate is critical to the long-term preservation of paper and book material. Good air circulation is important to keep fluctuation in climate stable. The HVAC system should be up to date and functioning efficiently. Light is detrimental to collections. Therefore, care should be given to the collections by implementing light control. General housekeeping issues can be addressed, including pest control. In addition to these helpful solutions, a library must also make an effort to be prepared if a disaster occurs, one that they cannot control. Time and effort should be given to create a concise and effective disaster plan to counteract any damage incurred through "acts of God" therefore an emergency management plan should be in place.





</doc>
<doc id="55487307" url="https://en.wikipedia.org/wiki?curid=55487307" title="Book clasp">
Book clasp

A book clasp is a leather or metal element attached to the medieval and early modern book covers, used to protect the book from the penetration of dust and light.


</doc>
<doc id="44543003" url="https://en.wikipedia.org/wiki?curid=44543003" title="Book restoration">
Book restoration

Book restoration is the renewal and repair of books. Techniques include cleaning; mending and filling damaged pages; restitching and rebinding. The first substantial work on the subject was Alfred Bonnardot's "Essai sur l'art de Restaurer les Estampes et les Livres" which was first published in Paris in 1846. Further significant developments occurred as a result of specific events including the 1904 fire at the Turin National University Library and the 1966 flood of the Arno which damaged over a million items in the National Central Library in Florence.

In France, conservators specialized in graphic arts and books are trained at the Institut National du Patrimoine (The National Institute of Cultural Heritage). Their mission is to intervene when heritage resources are threatened or deteriorated for several reasons. The conservator prevents works of art from disappearing or loses its purpose whilst analyzing the complex stage of its material history and the cause of alteration.


</doc>
<doc id="614236" url="https://en.wikipedia.org/wiki?curid=614236" title="Bookworm (insect)">
Bookworm (insect)

Bookworm is a general name for any insect that is said to bore through books.

The damage to books that is commonly attributed to "bookworms" is, in truth, not caused by any species of worm. Often, the larvae of various types of insects including beetles, moths and cockroaches, which may bore or chew through books seeking food, are responsible. Some such larvae exhibit a superficial resemblance to worms and are the likely inspiration for the term, though they are not true worms. In other cases, termites, carpenter ants, and woodboring beetles will first infest wooden bookshelves and later feed on books placed upon the shelves, attracted by the wood-pulp paper used in most commercial book production.

True book-borers are uncommon. The primary food sources for many "bookworms" are the leather or cloth bindings of a book, the glue used in the binding process, or molds and fungi that grow on or inside books. When the pages themselves are attacked, a gradual encroachment across the surface of one page or a small number of pages is typical, rather than the boring of holes through the entire book (see images on right).

The term has come to have a second, idiomatic use, indicative of a person who reads a great deal or to perceived excess: someone who devours books metaphorically.

The booklouse, also known as a paperlouse, is a soft-bodied, wingless insect in the order Psocoptera (usually "Trogium pulsatorium"), typically 1 mm or less in length. Booklice feed on microscopic molds and other organic matter found in or on aging items that have been stored in places that lack the climate control necessary to inhibit organic growth. Areas of archives, libraries, and museums that are cool, damp, dark, and generally undisturbed are common sites for such growth, generating a food source which subsequently attracts booklice. Booklice will also attack bindings, glue, and paper.

Despite their name, booklice are not considered to be true lice, as they do not feed on a living host.

By the 20th century, bookbinding materials had developed a high resistance against damage by various types of book-boring insects. Many museums and archives in possession of materials vulnerable to booklouse damage employ pest control methods to manage existing infestations and make use of climate control to prevent the growth of potential booklouse food sources.

Of the quarter million species of beetles, some adults damage books by eating paper and binding materials themselves. However, their larvae do the most damage. Typically eggs are laid on the books's edges and spine. Upon hatching, they bore into, and sometimes even through, the book.





These beetles have been known to feed on leather bindings.



Termites are the most devastating type of book eating pest. They will eat almost every part of a book including paper, cloth, and cardboard, not to mention the damage that can be done to shelves. Termites can make entire collections unusable before the infestation is even noticed.
Some species of ants can damage books in a way that is similar to termites.

Moths that feed on cloth will also feed on bookbindings, decaying organic material (which includes paper), and mold.




Bookdamaging cockroach species chew away at the starch in cloth bindings and paper. Their droppings can also harm books.



These insects consume portions of book that contain polysaccharides. Paper that is slightly ragged at the edges is usually the work of silverfish.


Pseudoscorpions love old dusty books where they can find their prey: The booklouse.

Pesticides can be used to protect books from these insects, but they are often made with harsh chemicals that make them an unattractive option. Museums and universities that want to keep their archives bookworm free without using pesticides often turn towards temperature control. Books can be stored at low temperatures that keep eggs from hatching, or placed in a deep-freezer to kill larvae and adults. The idea was taken from commercial food storage practices, as they are often dealing with the same pests.

The term is also used idiomatically to describe an avid or voracious reader, an indiscriminate or uncritical reader, or a bibliophile. In its earliest iterations, it had a negative connotation, "e.g.", an idler who would rather read than participate in the world around him or a person who pays too much attention to formal rules and book learning. Over the years its meaning has drifted in a more positive direction.




</doc>
<doc id="63175972" url="https://en.wikipedia.org/wiki?curid=63175972" title="Oregon Battle of the Books">
Oregon Battle of the Books

Oregon Battle of the Books (or OBOB) is a trivia-style reading competition where students grades 3 to 12 compete against one another by answering a series of questions based upon a list of books they are given at the beginning of the year. The list is around 15 books long and questions will be asked of any of the books for that age division in "Jeopardy!" style.

For the preliminary tournament it starts off in pool play style tournaments, then the next round is in the style of stepladder or bracket play. 

Students can choose to participate in the local, district, regional, and state level of the program.

The organization is a volunteer-based group supported by the Oregon Association of School Libraries (or OASL). The idea was sparked by the Chicago Public Library who in the mid-1900s had a radio show with a similar concept. In the 1970s, author Sybilla Cook introduced the idea in Oregon, but it was not until 2006 that it got funding from the Oregon State Library. 

For the 2018-19 school year, Alex Gino's "George" was one of the 16 novels selected for students to read. It is a novel about a 10-year-old transgender girl who goes by the name of Melissa. It is a coming of age novel targeted for students aged 8 to 12. Despite all the praise the book received from its initial release, it was featured two years in a row on the American Library Association's list titled "10 Most Challenged Books" for years 2016 and 2017.

Two school districts, Hermiston School District and Cascade School District, both decided to drop from the competition. They claimed that the content of the book was too mature for the students but mentioned that it had nothing to do with transgender rights.


</doc>
<doc id="11780487" url="https://en.wikipedia.org/wiki?curid=11780487" title="Hokusai Manga">
Hokusai Manga

The is a collection of sketches of various subjects by the Japanese artist Hokusai. Subjects of the sketches include landscapes, flora and fauna, everyday life and the supernatural. The word "manga" in the title does not refer to the contemporary story-telling "manga", as the sketches in the work are not connected to each other. Block-printed in three colours (black, gray and pale flesh), the Manga comprise literally thousands of images in 15 volumes, the first published in 1814, when the artist was 55. The final three volumes were published posthumously, two of them assembled by their publisher from previously unpublished material. The final volume was made up of previously published works, some not even by Hokusai, and is not considered authentic by art historians.

The preface to the first volume of the work, written by , a minor artist of Nagoya, suggests that the publication of the work may be aided by Hokusai's pupils. Part of the preface reads:
The final volume is considered spurious by some art historians.

The traditional view holds that, after the outburst of production, Hokusai carefully selected and redrew the sketches, arranging them into the patterns we see today. However, Michener (1958:30-34) argues that the pattern of the images on a particular plate were arranged by the wood carvers and publishers, not by the artist himself.

The first volume of 'Manga' (Defined by Hokusai as 'Brush gone wild'), was an art instruction book published to aid his troubled finances. Shortly after he removed the text and republished it. The Manga evidence a dedication to artistic realism in portrayal of people and the natural world. The work was an immediate success, and the subsequent volumes soon followed. The work became known to the West since Philipp Franz von Siebold's lithographed paraphrases of some of the sketches appeared in his "Nippon: Archiv Zur Beschreibung von Japon" in 1831. The work began to circulate in the West soon after Matthew C. Perry's entry into Japan in 1854.





</doc>
<doc id="11258836" url="https://en.wikipedia.org/wiki?curid=11258836" title="Chōjū-jinbutsu-giga">
Chōjū-jinbutsu-giga

, commonly shortened to , is a famous set of four picture scrolls, or "emakimono", belonging to Kōzan-ji temple in Kyoto, Japan. The "Chōjū-giga" scrolls are also referred to as Scrolls of Frolicking Animals and Scrolls of Frolicking Animals and Humans in English. Some think that Toba Sōjō created the scrolls; however, it seems clear from the style that more than one artist is involved. The right-to-left reading direction of "Chōjū-jinbutsu-giga " is traditional in East Asia, and is still common in Japan. "Chōjū-jinbutsu-giga " is also credited as the oldest work of manga. The scrolls are now entrusted to the Kyoto National Museum and Tokyo National Museum.

The scrolls are the earliest in a linear monochrome drawing style that was to continue in use in Japanese painting (as they are all done with the usual writing and painting brush, they count as painting). 

As opened, the first scroll illustrates anthropomorphic rabbits and monkeys bathing and getting ready for a ceremony, a monkey thief runs from animals with sticks and knocks over a frog from the lively ceremony. Further on, the rabbits and monkeys are playing and wrestling while another group of animals participate in a funeral and frog prays to Buddha as the scroll closes.

The scrolls were also adapted into several novels published by Geijutsuhiroba, the first book simply compiled the scrolls into one publication, now out of print. One of the books participated as part of the company's "Fine Arts Log" series as well as some were exclusive to certain exhibitions. Other companies like Misuzu Shobo and Shibundō also published books based on the "Chōjū-jinbutsu-giga" emakimono.

Although "Chōjū-jinbutsu-giga" is sometimes credited as the first manga, there have been some disputes with the "Yomiuri Shimbun" newspaper. Seiki Hosokibara pointed to the "Shigisan-engi" scrolls as the first manga, and Kanta Ishida explained that the scrolls should be treated as masterpieces in their own right.

The "Chōjū-jinbutsu-giga" emakimono, belonging to the Kōzan-ji temple in Kyoto, Japan as an ancient cultural property, are usually thought to have been painted in the mid-12th century, whereas the third and fourth scrolls may well date from the 13th century.

Most think Toba Sōjō created "Chōjū-jinbutsu-giga", who created a painting a lot like "Chōjū-jinbutsu-giga"; however, it is hard to verify this claim. The drawings of "Chōjū-jinbutsu-giga" are making fun of Japanese priests in the creator's time period, characterising them as toads, rabbits and monkeys. "Chōjū-jinbutsu-giga" is read and rolled out from which can still be seen in manga and Japanese books. "Chōjū-jinbutsu-giga" is credited as being the oldest work of manga in Japan, and is a national treasure as well as many Japanese animators believe it is also the origin of Japanese animated movies. In "Chōjū-jinbutsu-giga" the animals were drawn with very expressive faces and also sometimes used "speed lines", a technique used in manga til this day. Emakimono like "Chōjū-jinbutsu-giga" and many others barely were seen in the public until they made their way into popular culture, with many common people imitating the style. Emakimono emerged very popular in the city of Ōtsu, Shiga, and being dubbed "Ōtsu-e" after its popularity in the city around the 17th century. The first two scrolls are entrusted to the Tokyo National Museum, and the second two are entrusted to the Kyoto National Museum. The scrolls currently on display at Kōzan-ji are reproductions.

The first scroll, which is considered the most famous, depicts various animals (frogs, rabbits and monkeys) frolicking as if they were human. There is no writing on any of the scrolls; they consist of pictures only. The first scroll is also the largest, with a length of 11 meters (36 ft) and 30 cm (1 ft) wide. As the first scroll is opened, rabbits and monkeys are bathing and swimming in a lake, moving on past the mountains, cliffs and trees are rabbits and frogs making bow and arrows. Further more, more rabbits and frogs are bringing pots and boxes to a (currently) unknown event. Frogs and rabbits pass by monks with their cattle (wild boar, sika deer) and a monkey runs away, supposedly stealing, and being chased by a rabbit with a long stick, further more a frog is lying on the floor who could have possibly been knocked over by the thief. Nearby, a celebration has started with two frogs dancing, and a group of animals having a conversation. Not too far from the celebration are animals wrestling and fighting and two monkeys holding a box. Far from the celebration are a group of animals at a funeral and a frog praying in front of a frog shaped Budai as the scroll closes.

Four publications based on "Chōjū-jinbutsu-giga" have been released by the publisher Geijutsuhiroba. The first "Chōjū-jinbutsu-giga" book published by the company was which simply compiled the "Chōjū-giga" emakimono into one publication, released on February 2003, now out of print. A publication made for the anniversary of "Chōjū-giga" entitled was released on October 11, 2007 as a part of the series. All four scrolls were published in actual size in their boxset publication entitled "Chōjū Jinbutsu Giga" released June 10, 2008. Exclusive to the Suntory Museum of Art exhibition of "Chōjū-giga", the same company released a book entitled . "Chōjū-jinbutsu-giga" was also released in a "shinsōbon" (deluxe edition) by Misuzu Shobo. In 1991 a book by Shibundō entitled "Chōjū-jinbutsu-giga to Okoe: Emaki" was published and written by Nobuo Tsuji.

The "Yomiuri Shimbun" newspaper with Kanta Ishida discussed different theories of what really is the "first manga". Manga artist, Seiki Hosokibara pointed to "Shigisan-engi" as the first manga in history. Ishida said that the scrolls be treated as masterpieces in their own right, and not be cubby-holed as just the origin of manga and they have no connection with contemporary manga and the domestic works people are familiar with today.







</doc>
<doc id="12930350" url="https://en.wikipedia.org/wiki?curid=12930350" title="Japan Media Arts Festival">
Japan Media Arts Festival

The Japan Media Arts Festival is an annual festival held since 1997 by Japan's Agency for Cultural Affairs. The festival begins with an open competition and culminates with the awarding of several prizes and an exhibition.

Based on judging by a jury of artistic peers, awards are given in four categories: Art (formerly called Non-Interactive Digital Art), Entertainment (formerly called Interactive Art; including video games and websites), animation, and manga. Within each category, one Grand Prize, four Excellence Prizes, and (since 2002) one Encouragement Prize are awarded. 




</doc>
<doc id="18985" url="https://en.wikipedia.org/wiki?curid=18985" title="Manga">
Manga

In Japan, people of all ages read manga. The medium includes works in a broad range of genres: action, adventure, business and commerce, comedy, detective, drama, historical, horror, mystery, romance, science fiction and fantasy, erotica ("hentai"), sports and games, and suspense, among others. Many manga are translated into other languages. Since the 1950s, manga has become an increasingly major part of the Japanese publishing industry. By 1995, the manga market in Japan was valued at (), with annual sales of 1.9billion manga books and manga magazines in Japan (equivalent to 15issues per person). Manga have also gained a significant worldwide audience. In 2008, in the U.S. and Canada, the manga market was valued at $175 million. Manga represented 38% of the French comics market , equivalent to approximately ten times that of the United States, and was valued at about ($million). In Europe and the Middle East, the market was valued at $250 million in 2012.

Manga stories are typically printed in black-and-white—due to time constraints, artistic reasons (as coloring could lessen the impact of the artwork) and to keep printing costs low—although some full-color manga exist (e.g., "Colorful"). In Japan, manga are usually serialized in large manga magazines, often containing many stories, each presented in a single episode to be continued in the next issue. Collected chapters are usually republished in "tankōbon" volumes, frequently but not exclusively paperback books. A manga artist ("mangaka" in Japanese) typically works with a few assistants in a small studio and is associated with a creative editor from a commercial publishing company. If a manga series is popular enough, it may be animated after or during its run. Sometimes, manga are based on previous live-action or animated films.

Manga-influenced comics, among original works, exist in other parts of the world, particularly in Algeria ("DZ-manga"), China, Hong Kong, Taiwan ("manhua"), and South Korea ("manhwa").

The word "manga" comes from the Japanese word 漫画, composed of the two kanji 漫 (man) meaning "whimsical or impromptu" and 画 (ga) meaning "pictures". The same term is the root of the Korean word for comics, "manhwa", and the Chinese word "manhua".

The word first came into common usage in the late 18th century with the publication of such works as Santō Kyōden's picturebook "Shiji no yukikai" (1798), and in the early 19th century with such works as Aikawa Minwa's "Manga hyakujo" (1814) and the celebrated "Hokusai Manga" books (1814–1834) containing assorted drawings from the sketchbooks of the famous ukiyo-e artist Hokusai. Rakuten Kitazawa (1876–1955) first used the word "manga" in the modern sense.

In Japanese, "manga" refers to all kinds of cartooning, comics, and animation. Among English speakers, "manga" has the stricter meaning of "Japanese comics", in parallel to the usage of "anime" in and outside Japan. The term "ani-manga" is used to describe comics produced from animation cels.

The history of manga is said to originate from scrolls dating back to the 12th century, and it is believed they represent the basis for the right-to-left reading style. During the Edo period (1603–1867), "Toba Ehon" embedded the concept of manga. The word itself first came into common usage in 1798, with the publication of works such as Santō Kyōden's picturebook "Shiji no yukikai" (1798), and in the early 19th century with such works as Aikawa Minwa's "Manga hyakujo" (1814) and the "Hokusai Manga" books (1814–1834). Adam L. Kern has suggested that "kibyoshi", picture books from the late 18th century, may have been the world's first comic books. These graphical narratives share with modern manga humorous, satirical, and romantic themes. Some works were mass-produced as serials using woodblock printing.

Writers on manga history have described two broad and complementary processes shaping modern manga. One view represented by other writers such as Frederik L. Schodt, Kinko Ito, and Adam L. Kern, stress continuity of Japanese cultural and aesthetic traditions, including pre-war, Meiji, and pre-Meiji culture and art. The other view, emphasizes events occurring during and after the Allied occupation of Japan (1945–1952), and stresses U.S. cultural influences, including U.S. comics (brought to Japan by the GIs) and images and themes from U.S. television, film, and cartoons (especially Disney).

Regardless of its source, an explosion of artistic creativity occurred in the post-war period, involving manga artists such as Osamu Tezuka ("Astro Boy") and Machiko Hasegawa ("Sazae-san"). "Astro Boy" quickly became (and remains) immensely popular in Japan and elsewhere, and the anime adaptation of "Sazae-san" drawing more viewers than any other anime on Japanese television in 2011. Tezuka and Hasegawa both made stylistic innovations. In Tezuka's "cinematographic" technique, the panels are like a motion picture that reveals details of action bordering on slow motion as well as rapid zooms from distance to close-up shots. This kind of visual dynamism was widely adopted by later manga artists. Hasegawa's focus on daily life and on women's experience also came to characterize later "shōjo manga". Between 1950 and 1969, an increasingly large readership for manga emerged in Japan with the solidification of its two main marketing genres, "shōnen manga" aimed at boys and "shōjo manga" aimed at girls.

In 1969 a group of female manga artists (later called the "Year 24 Group", also known as "Magnificent 24s") made their "shōjo" manga debut ("year 24" comes from the Japanese name for the year 1949, the birth-year of many of these artists). The group included Moto Hagio, Riyoko Ikeda, Yumiko Ōshima, Keiko Takemiya, and Ryoko Yamagishi. Thereafter, primarily female manga artists would draw "shōjo" for a readership of girls and young women. In the following decades (1975–present), "shōjo" manga continued to develop stylistically while simultaneously evolving different but overlapping subgenres. Major subgenres include romance, superheroines, and "Ladies Comics" (in Japanese, "redisu" , "redikomi" , and "josei" ).

Modern "shōjo" manga romance features love as a major theme set into emotionally intense narratives of self-realization. With the superheroines, "shōjo" manga saw releases such as Pink Hanamori's "Mermaid Melody Pichi Pichi Pitch" Reiko Yoshida's "Tokyo Mew Mew", And, Naoko Takeuchi's "Pretty Soldier Sailor Moon", which became internationally popular in both manga and anime formats. Groups (or "sentais") of girls working together have also been popular within this genre. Like Lucia, Hanon, and Rina singing together, and Sailor Moon, Sailor Mercury, Sailor Mars, Sailor Jupiter, and Sailor Venus working together.

Manga for male readers sub-divides according to the age of its intended readership: boys up to 18 years old ("shōnen" manga) and young men 18 to 30 years old ("seinen" manga); as well as by content, including action-adventure often involving male heroes, slapstick humor, themes of honor, and sometimes explicit sex. The Japanese use different kanji for two closely allied meanings of "seinen"— for "youth, young man" and for "adult, majority"—the second referring to pornographic manga aimed at grown men and also called "seijin" ("adult" ) manga. "Shōnen", "seinen", and "seijin" manga share a number of features in common.

Boys and young men became some of the earliest readers of manga after World War II. From the 1950s on, "shōnen" manga focused on topics thought to interest the archetypal boy, including subjects like robots, space-travel, and heroic action-adventure. Popular themes include science fiction, technology, sports, and supernatural settings. Manga with solitary costumed superheroes like Superman, Batman, and Spider-Man generally did not become as popular.

The role of girls and women in manga produced for male readers has evolved considerably over time to include those featuring single pretty girls ("bishōjo") such as Belldandy from "Oh My Goddess!", stories where such girls and women surround the hero, as in "" and "Hanaukyo Maid Team", or groups of heavily armed female warriors ("sentō bishōjo")

With the relaxation of censorship in Japan in the 1990s, an assortment of explicit sexual material appeared in manga intended for male readers, and correspondingly continued into the English translations. However, in 2010 the Tokyo Metropolitan Government passed a bill to restrict such content.

The "gekiga" style of storytelling—thematically somber, adult-oriented, and sometimes deeply violent—focuses on the day-in, day-out grim realities of life, often drawn in a gritty and unvarnished fashion. "Gekiga" such as Sampei Shirato's 1959–1962 "Chronicles of a Ninja's Military Accomplishments" ("Ninja Bugeichō") arose in the late 1950s and 1960s partly from left-wing student and working-class political activism and partly from the aesthetic dissatisfaction of young manga artists like Yoshihiro Tatsumi with existing manga.

In Japan, manga constituted an annual 40.6 billion yen (approximately US$395 million) publication-industry by 2007. In 2006 sales of manga books made up for about 27% of total book-sales, and sale of manga magazines, for 20% of total magazine-sales. The manga industry has expanded worldwide, where distribution companies license and reprint manga into their native languages.

Marketeers primarily classify manga by the age and gender of the target readership. In particular, books and magazines sold to boys ("shōnen") and girls ("shōjo") have distinctive cover-art, and most bookstores place them on different shelves. Due to cross-readership, consumer response is not limited by demographics. For example, male readers may subscribe to a series intended for female readers, and so on. Japan has manga cafés, or "manga kissa" ("kissa" is an abbreviation of "kissaten"). At a "manga kissa", people drink coffee, read manga and sometimes stay overnight.

The Kyoto International Manga Museum maintains a very large website listing manga published in Japanese.

Manga magazines usually have many series running concurrently with approximately 20–40 pages allocated to each series per issue. Other magazines such as the anime fandom magazine "Newtype" featured single chapters within their monthly periodicals. Other magazines like "Nakayoshi" feature many stories written by many different artists; these magazines, or "anthology magazines", as they are also known (colloquially "phone books"), are usually printed on low-quality newsprint and can be anywhere from 200 to more than 850 pages thick. Manga magazines also contain one-shot comics and various four-panel "yonkoma" (equivalent to comic strips). Manga series can run for many years if they are successful. Manga artists sometimes start out with a few "one-shot" manga projects just to try to get their name out. If these are successful and receive good reviews, they are continued. Magazines often have a short life.

After a series has run for a while, publishers often collect the episodes together and print them in dedicated book-sized volumes, called "tankōbon". These can be hardcover, or more usually softcover books, and are the equivalent of U.S. trade paperbacks or graphic novels. These volumes often use higher-quality paper, and are useful to those who want to "catch up" with a series so they can follow it in the magazines or if they find the cost of the weeklies or monthlies to be prohibitive. "Deluxe" versions have also been printed as readers have gotten older and the need for something special grew. Old manga have also been reprinted using somewhat lesser quality paper and sold for 100 yen (about $1 U.S. dollar) each to compete with the used book market.

Kanagaki Robun and Kawanabe Kyōsai created the first manga magazine in 1874: "Eshinbun Nipponchi". The magazine was heavily influenced by "Japan Punch", founded in 1862 by Charles Wirgman, a British cartoonist. "Eshinbun Nipponchi" had a very simple style of drawings and did not become popular with many people. "Eshinbun Nipponchi" ended after three issues. The magazine "Kisho Shimbun" in 1875 was inspired by "Eshinbun Nipponchi", which was followed by "Marumaru Chinbun" in 1877, and then "Garakuta Chinpo" in 1879. "Shōnen Sekai" was the first "shōnen" magazine created in 1895 by Iwaya Sazanami, a famous writer of Japanese children's literature back then. "Shōnen Sekai" had a strong focus on the First Sino-Japanese War.

In 1905 the manga-magazine publishing boom started with the Russo-Japanese War, "Tokyo Pakku" was created and became a huge hit. After "Tokyo Pakku" in 1905, a female version of "Shōnen Sekai" was created and named "Shōjo Sekai", considered the first "shōjo" magazine. "Shōnen Pakku" was made and is considered the first children's manga magazine. The children's demographic was in an early stage of development in the Meiji period. "Shōnen Pakku" was influenced from foreign children's magazines such as "Puck" which an employee of Jitsugyō no Nihon (publisher of the magazine) saw and decided to emulate. In 1924, "Kodomo Pakku" was launched as another children's manga magazine after "Shōnen Pakku". During the boom, "Poten" (derived from the French "potin") was published in 1908. All the pages were in full color with influences from "Tokyo Pakku" and "Osaka Puck". It is unknown if there were any more issues besides the first one. "Kodomo Pakku" was launched May 1924 by Tokyosha and featured high-quality art by many members of the manga artistry like Takei Takeo, Takehisa Yumeji and Aso Yutaka. Some of the manga featured speech balloons, where other manga from the previous eras did not use speech balloons and were silent.

Published from May 1935 to January 1941, "Manga no Kuni" coincided with the period of the Second Sino-Japanese War (1937–1945). "Manga no Kuni" featured information on becoming a mangaka and on other comics industries around the world. "Manga no Kuni" handed its title to "Sashie Manga Kenkyū" in August 1940.

"Dōjinshi", produced by small publishers outside of the mainstream commercial market, resemble in their publishing small-press independently published comic books in the United States. Comiket, the largest comic book convention in the world with around 500,000 visitors gathering over three days, is devoted to "dōjinshi". While they most often contain original stories, many are parodies of or include characters from popular manga and anime series. Some "dōjinshi" continue with a series' story or write an entirely new one using its characters, much like fan fiction. In 2007, "dōjinshi" sales amounted to 27.73 billion yen (US$245 million). In 2006 they represented about a tenth of manga books and magazines sales.

Thanks to the advent of the internet, there have been new ways for aspiring mangaka to upload and sell their manga online. Before, there were two main ways in which a mangaka's work could be published: taking their manga drawn on paper to a publisher themselves, or submitting their work to competitions run by magazines.

In recent years, there has been a rise in manga released digitally. Web manga, as it's known in Japan, has a seen an increase thanks in part to image hosting websites where anyone can upload pages from their works for free. Although released digitally, almost all web manga stick to the conventional black-and-white format despite some never getting physical publications. Pixiv is the most popular site where a host of amateur and professional works get published on the site. It has grown to be the most visited site for artwork in Japan. Twitter has also become a popular place for web manga with many artists releasing pages weekly on their accounts in the hopes of their works getting picked up or published professionally. One of the best examples of an amateur work becoming professional is One-Punch Man which was released online and later got a professional remake released digitally and an anime adaptation soon there after.

Many of the big print publishers have also released digital only magazines and websites where web manga get published alongside their serialized magazines. Shogakukan for instance has two websites, "Sunday Webry" and "Ura Sunday", that release weekly chapters for web manga and even offer contests for mangaka to submit their work. Both Sunday Webry and Ura Sunday have become one of the top web manga sites in Japan. Some have even released apps that teach how to draw professional manga and learn how to create them. Weekly Shōnen Jump released "Jump Paint", an app that guides users on how to make their own manga from making storyboards to digitally inking lines. It also offers more than 120 types of pen tips and more than 1,000 screentones for artists to practice. Kodansha has also used the popularity of web manga to launch more series and also offer better distribution of their officially translated works under Kodansha Comics thanks in part to the titles being released digitally first before being published physically.

The rise web manga has also been credited to smartphones and computers as more and more readers read manga on their phones rather than from a print publication. While paper manga has seen a decrease overtime, digital manga have been growing in sales each year. The Research Institute for Publications reports that sales of digital manga books excluding magazines jumped 27.1 percent to ¥146 billion in 2016 from the year before while sales of paper manga saw a record year-on-year decline of 7.4 percent to ¥194.7 billion. They have also said that if the digital and paper keep the same growth and drop rates, web manga will exceed their paper counterparts.

While webtoons have caught on in popularity as a new medium for comics in Asia, Japan has been slow to adopt webtoons as the traditional format and print publication still dominate the way manga is created and consumed. Despite this, one of the biggest webtoon publishers in the world, Comico, has had success in the traditional Japanese manga market. Comico was launched by NHN Japan, the Japanese subsidiary of Korean company, NHN Entertainment. As of now, there are only two webtoon publishers that publish Japanese webtoons: Comico and Naver Webtoon (under the name "XOY" in Japan). Kakao has also had success by offering licensed manga and translated Korean webtoons with their service Piccoma. All three companies credit their success to the webtoon pay model where users can purchase each chapter individually instead of having to buy the whole book while also offering some chapters for free for a period of time allowing anyone to read a whole series for free if they wait long enough. The added benefit of having all of their titles in color and some with special animations and effects have also helped them succeed. Some popular Japanese webtoons have also gotten anime adaptations and print releases, the most notable being "ReLIFE" and "Recovery of an MMO Junkie".

By 2007, the influence of manga on international comics had grown considerably over the past two decades. "Influence" is used here to refer to effects on the comics markets outside Japan and to aesthetic effects on comics artists internationally.

Traditionally, manga stories flow from top to bottom and from right to left. Some publishers of translated manga keep to this original format. Other publishers mirror the pages horizontally before printing the translation, changing the reading direction to a more "Western" left to right, so as not to confuse foreign readers or traditional comics-consumers. This practice is known as "flipping". For the most part, criticism suggests that flipping goes against the original intentions of the creator (for example, if a person wears a shirt that reads "MAY" on it, and gets flipped, then the word is altered to "YAM"), who may be ignorant of how awkward it is to read comics when the eyes must flow through the pages and text in opposite directions, resulting in an experience that's quite distinct from reading something that flows homogeneously. If the translation is not adapted to the flipped artwork carefully enough it is also possible for the text to go against the picture, such as a person referring to something on their left in the text while pointing to their right in the graphic. Characters shown writing with their right hands, the majority of them, would become left-handed when a series is flipped. Flipping may also cause oddities with familiar asymmetrical objects or layouts, such as a car being depicted with the gas pedal on the left and the brake on the right, or a shirt with the buttons on the wrong side, but these issues are minor when compared to the unnatural reading flow, and some of them could be solved with an adaptation work that goes beyond just translation and blind flipping.

Manga has influenced European cartooning in a way that is somewhat different from in the U.S. Broadcast anime in France and Italy opened the European market to manga during the 1970s. French art has borrowed from Japan since the 19th century (Japonism) and has its own highly developed tradition of bande dessinée cartooning. In France, beginning in the mid-1990s, manga has proven very popular to a wide readership, accounting for about one-third of comics sales in France since 2004. According to the Japan External Trade Organization, sales of manga reached $212.6 million within France and Germany alone in 2006. France represents about 50% of the European market and is the second worldwide market, behind Japan. In 2013, there were 41 publishers of manga in France and, together with other Asian comics, manga represented around 40% of new comics releases in the country, surpassing Franco-Belgian comics for the first time. European publishers marketing manga translated into French include Asuka, Casterman, Glénat, Kana, and Pika Édition, among others. European publishers also translate manga into Dutch, German, Italian, and other languages. In 2007, about 70% of all comics sold in Germany were manga.

Manga publishers based in the United Kingdom include Gollancz and Titan Books. Manga publishers from the United States have a strong marketing presence in the United Kingdom: for example, the Tanoshimi line from Random House.

Manga made their way only gradually into U.S. markets, first in association with anime and then independently. Some U.S. fans became aware of manga in the 1970s and early 1980s. However, anime was initially more accessible than manga to U.S. fans, many of whom were college-age young people who found it easier to obtain, subtitle, and exhibit video tapes of anime than translate, reproduce, and distribute "tankōbon"-style manga books. One of the first manga translated into English and marketed in the U.S. was Keiji Nakazawa's "Barefoot Gen", an autobiographical story of the atomic bombing of Hiroshima issued by Leonard Rifas and Educomics (1980–1982). More manga were translated between the mid-1980s and 1990s, including "Golgo 13" in 1986, "Lone Wolf and Cub" from First Comics in 1987, and "Kamui", "Area 88", and "Mai the Psychic Girl", also in 1987 and all from Viz Media-Eclipse Comics. Others soon followed, including "Akira" from Marvel Comics' Epic Comics imprint, "Nausicaä of the Valley of the Wind" from Viz Media, and "Appleseed" from Eclipse Comics in 1988, and later "Iczer-1" (Antarctic Press, 1994) and Ippongi Bang's "F-111 Bandit" (Antarctic Press, 1995).

In the 1980s to the mid-1990s, Japanese animation, like "Akira", "Dragon Ball", "Neon Genesis Evangelion", and "Pokémon", made a bigger impact on the fan experience and in the market than manga. Matters changed when translator-entrepreneur Toren Smith founded Studio Proteus in 1986. Smith and Studio Proteus acted as an agent and translator of many Japanese manga, including Masamune Shirow's "Appleseed" and Kōsuke Fujishima's "Oh My Goddess!", for Dark Horse and Eros Comix, eliminating the need for these publishers to seek their own contacts in Japan.
Simultaneously, the Japanese publisher Shogakukan opened a U.S. market initiative with their U.S. subsidiary Viz, enabling Viz to draw directly on Shogakukan's catalogue and translation skills.

Japanese publishers began pursuing a U.S. market in the mid-1990s due to a stagnation in the domestic market for manga. The U.S. manga market took an upturn with mid-1990s anime and manga versions of Masamune Shirow's "Ghost in the Shell" (translated by Frederik L. Schodt and Toren Smith) becoming very popular among fans. An extremely successful manga and anime translated and dubbed in English in the mid-1990s was "Sailor Moon". By 1995–1998, the "Sailor Moon" manga had been exported to over 23 countries, including China, Brazil, Mexico, Australia, North America and most of Europe. In 1997, Mixx Entertainment began publishing "Sailor Moon", along with CLAMP's "Magic Knight Rayearth", Hitoshi Iwaaki's "Parasyte" and Tsutomu Takahashi's "Ice Blade" in the monthly manga magazine "MixxZine". Mixx Entertainment, later renamed Tokyopop, also published manga in trade paperbacks and, like Viz, began aggressive marketing of manga to both young male and young female demographics.

During this period, Dark Horse Manga was a major publisher of translated manga. In addition to "Oh My Goddess!", the company published "Akira", "Astro Boy", "Berserk", "Blade of the Immortal", "Ghost in the Shell", "Lone Wolf and Cub", Yasuhiro Nightow's "Trigun" and "Blood Blockade Battlefront", "Gantz", Kouta Hirano's "Hellsing" and "Drifters", "Blood+", "Multiple Personality Detective Psycho", "FLCL", "Mob Psycho 100", and "Oreimo". The company received 13 Eisner Award nominations for its manga titles, and three of the four manga creators admitted to The Will Eisner Award Hall of Fame — Osamu Tezuka, Kazuo Koike, and Goseki Kojima — were published in Dark Horse translations. 

In the following years, manga became increasingly popular, and new publishers entered the field while the established publishers greatly expanded their catalogues. The "Pokémon" manga "Electric Tale of Pikachu" issue #1 sold over 1million copies in the United States, making it the best-selling single comic book in the United States since 1993. By 2008, the U.S. and Canadian manga market generated $175 million in annual sales. Simultaneously, mainstream U.S. media began to discuss manga, with articles in "The New York Times", "Time" magazine, "The Wall Street Journal", and "Wired" magazine. As of 2017, manga distributor Viz Media is the largest publisher of graphic novels and comic books in the United States, with a 23% share of the market. BookScan sales show that manga is one of the fastest-growing areas of the comic book and narrative fiction markets. From January 2019 to May 2019, the manga market grew 16%, compared to the overall comic book market's 5% growth. The NPD Group noted that, compared to other comic book readers, manga readers are younger (76% under 30) and more diverse, including a higher female readership (16% higher than other comic books).

A number of artists in the United States have drawn comics and cartoons influenced by manga. As an early example, Vernon Grant drew manga-influenced comics while living in Japan in the late 1960s and early 1970s. Others include Frank Miller's mid-1980s "Ronin", Adam Warren and Toren Smith's 1988 "The Dirty Pair", Ben Dunn's 1987 "Ninja High School" and "Manga Shi 2000" from Crusade Comics (1997).

By the 21st century several U.S. manga publishers had begun to produce work by U.S. artists under the broad marketing-label of manga. In 2002 I.C. Entertainment, formerly Studio Ironcat and now out of business, launched a series of manga by U.S. artists called "Amerimanga". In 2004 eigoMANGA launched the "Rumble Pak" and "Sakura Pakk" anthology series. Seven Seas Entertainment followed suit with "World Manga". Simultaneously, TokyoPop introduced original English-language manga (OEL manga) later renamed "Global Manga".

Francophone artists have also developed their own versions of manga ("manfra"), like Frédéric Boilet's "la nouvelle manga". Boilet has worked in France and in Japan, sometimes collaborating with Japanese artists.

The Japanese manga industry grants a large number of awards, mostly sponsored by publishers, with the winning prize usually including publication of the winning stories in magazines released by the sponsoring publisher. Examples of these awards include:

The Japanese Ministry of Foreign Affairs has awarded the International Manga Award annually since May 2007.

Kyoto Seika University in Japan has offered a highly competitive course in manga since 2000. Then, several established universities and vocational schools (専門学校: "Semmon gakkou") established a .

Shuho Sato, who wrote "Umizaru" and "Say Hello to Black Jack", has created some controversy on Twitter. Sato says, "Manga school is meaningless because those schools have very low success rates. Then, I could teach novices required skills on the job in three months. Meanwhile, those school students spend several million yen, and four years, yet they are good for nothing." and that, "For instance, Keiko Takemiya, the then professor of Seika Univ., remarked in the Government Council that 'A complete novice will be able to understand where is "Tachikiri" (i.e., margin section) during four years.' On the other hand, I would imagine that, It takes about thirty minutes to completely understand that at work."




</doc>
<doc id="52222979" url="https://en.wikipedia.org/wiki?curid=52222979" title="Heta-uma">
Heta-uma

Heta-uma ( or ) is a Japanese underground manga movement started in the 1970 decade with the magazine "Garo". "Heta-uma" can be translated as "bad but good", designating a work which looks poorly drawn, but with an aesthetically conscious quality, opposed to the polished look of mainstream manga.

Some of "heta-uma"'s main artists are Teruhiko Yumura (pen name "King Terry"), Yoshikazu Ebisu and Takashi Nemoto.


</doc>
<doc id="4330437" url="https://en.wikipedia.org/wiki?curid=4330437" title="History of manga">
History of manga

The history of manga is said to originate from scrolls dating back to the 12th century, and it is believed they represent the basis for the right-to-left reading style. The word first came into common usage in the late 18th century. Manga is a Japanese term that can be translated as "comic"; Historians and writers on manga history have described two broad and complementary processes shaping modern manga. Their views differ in the relative importance they attribute to the role of cultural and historical events following World War II versus the role of pre-war, Meiji, and pre-Meiji Japanese culture and art.

One view represented by other writers such as Frederik L. Schodt, Kinko Ito, and Adam L. Kern, stress continuity of Japanese cultural and aesthetic traditions, including pre-war, Meiji, and pre-Meiji culture and art. The other view, emphasizes events occurring during and after the Allied occupation of Japan (1945–1952), and stresses that manga was strongly shaped by United States cultural influences, including US comics brought to Japan by the GIs and by images and themes from US television, film, and cartoons (especially Disney). According to Sharon Kinsella, the booming Japanese publishing industry helped create a consumer-oriented society in which publishing giants like Kodansha could shape popular taste.

Manga is said to originate from scrolls dating back to the 12th and 13th centuries. During the Edo period (1603-1867), another book of drawings, "Toba Ehon", embedded the concept of manga. The word first came into common usage in the late 18th century with the publication of such works as Santō Kyōden's picturebook "Shiji no yukikai" (1798), and in the early 19th century with such works as Aikawa Minwa's "Manga hyakujo" (1814) and the celebrated "Hokusai Manga" books (1814–1834) containing assorted drawings from the sketchbooks of the famous ukiyo-e artist Hokusai who lived from 1760–1849. Rakuten Kitazawa (1876–1955) first used the word "manga" in the modern sense.
Writers stress continuity of Japanese cultural and aesthetic traditions as central to the history of manga. They include Frederik L. Schodt, Kinko Ito, Adam L. Kern, and Eric Peter Nash. Schodt points to the existence in the 13th century of illustrated picture scrolls like Chōjū-jinbutsu-giga that told stories in sequential images with humor and wit. Schodt also stresses continuities of aesthetic style and vision between ukiyo-e and shunga woodblock prints and modern manga (all three fulfill Eisner's criteria for sequential art). While there are disputes over whether Chōjū-jinbutsu-giga or Shigisan-engi was the first manga, both scrolls date back to about the same time period. However others like Isao Takahata, Studio Ghibli co-founder and director, contends there is no linkage with the scrolls and modern manga.

Schodt and Nash also see a particularly significant role for "kamishibai", a form of street theater where itinerant artists displayed pictures in a light box while narrating the story to audiences in the street. Torrance has pointed to similarities between modern manga and the Osaka popular novel between the 1890s and 1940, and argues that the development of widespread literacy in Meiji and post-Meiji Japan helped create audiences for stories told in words and pictures. Kinko Ito also roots manga historically in aesthetic continuity with pre-Meiji art, but she sees its post-WWII history as driven in part by consumer enthusiasm for the rich imagery and narrative of the newly developing manga tradition. Ito describes how this tradition has steadily produced new genres and markets, e.g., for girls' ("shōjo") manga in the late 1960s and for Ladies Comics ("redisu") in the 1980s.

Kern has suggested that "kibyoshi", picture books from the late 18th century, may have been the world's first comic books. These graphical narratives share with modern manga humorous, satirical, and romantic themes. Although Kern does not believe that "kibyoshi" were a direct forerunner of manga, for Kern the existence of "kibyoshi" nonetheless points to a Japanese willingness to mix words and pictures in a popular story-telling medium. The first recorded use of the term "manga" to mean "whimsical or impromptu pictures" comes from this tradition in 1798, which, Kern points out, predates Katsushika Hokusai's better known "Hokusai Manga" usage by several decades.

Illustrated magazines for Western expatriates introduced Western-style satirical cartoons to Japan in the late 19th century. New publications in both the Western and Japanese styles became popular, and at the end of the 1890s, American-style newspaper comics supplements began to appear in Japan, as well as some American comic strips. 1900 saw the debut of the "Jiji Manga" in the "Jiji Shinpō" newspaper—the first use of the word "manga" in its modern sense, and where, in 1902, Rakuten Kitazawa began the first modern Japanese comic strip. By the 1930s, comic strips were serialized in large-circulation monthly girls' and boys' magazine and collected into hardback volumes.

Similarly, Inoue sees manga as being a mixture of image- and word-centered elements, each pre-dating the Allied occupation of Japan. In his view, Japanese image-centered or "pictocentric" art ultimately derives from Japan's long history of engagement with Chinese graphic art, whereas word-centered or "logocentric" art, like the novel, was stimulated by social and economic needs of Meiji and pre-war Japanese nationalism for a populace unified by a common written language. Both fuse in what Inoue sees as a symbiosis in manga.

The roots of the wide-eyed look commonly associated with manga dates back to "shōjo" magazine illustrations during the late 19th to early 20th centuries. The most important illustrators associated with this style at the time were Yumeji Takehisa and particularly Jun'ichi Nakahara, who, influenced by his work as a doll creator, frequently drew female characters with big eyes in the early 20th century. This had a significant influence on early manga, particularly shōjo manga, evident in the work of influential manga artists such as Makoto Takahashi and Riyoko Ikeda.

However, other writers such as Takashi Murakami have stressed events after WWII, but Murakami sees Japan's surrender and the atomic bombing of Hiroshima and Nagasaki as having created long-lasting scars on the Japanese artistic psyche, which, in this view, lost its previously virile confidence in itself and sought solace in harmless and cute ("kawaii") images. However, Takayumi Tatsumi sees a special role for a transpacific economic and cultural transnationalism that created a postmodern and shared international youth culture of cartooning, film, television, music, and related popular arts, which was, for Tatsumi the crucible in which modern manga have developed, along with Norakuro

For Murakami and Tatsumi, trans-nationalism (or globalization) refers specifically to the flow of cultural and subcultural material from one nation to another. In their usage, the term does not refer to international corporate expansion, nor to international tourism, nor to cross-border international personal friendships, but to ways in which artistic, aesthetic, and intellectual traditions influence each other across national boundaries. An example of cultural trans-nationalism is the creation of "Star Wars" films in the US, their transformation into manga by Japanese artists, and the marketing of "Star Wars" manga to the US. Another example is the transfer of hip-hop culture from the US to Japan. Wong also sees a major role for trans-nationalism in the recent history of manga.

Thus, these scholars see the history of manga as involving historical continuities and discontinuities between the aesthetic and cultural past as it interacts with post-WWII innovation and trans-nationalism.

After World War II, Japanese artists subsequently gave life to their own style during the occupation (1945–1952) and post-occupation years (1952-early 1960s), when a previously militaristic and ultranationalist Japan was rebuilding its political and economic infrastructure. Although Allied occupation censorship policies specifically prohibited art and writing that glorified war and Japanese militarism, those policies did not prevent the publication of other kinds of material, including manga. Furthermore, the 1947 Japanese Constitution (Article 21) prohibited all forms of censorship. One result was the growth of artistic creativity in this period.

In the forefront of this period are two manga series and characters that influenced much of the future history of manga. These are Osamu Tezuka's "Mighty Atom" ("Astro Boy" in the United States; begun in 1951) and Machiko Hasegawa's "Sazae-san" (begun in 1946).

Astro Boy was both a superpowered robot and a naive little boy. Tezuka never explained why Astro Boy had such a highly developed social conscience nor what kind of robot programming could make him so deeply affiliative. Both seem innate to Astro Boy, and represent a Japanese sociality and community-oriented masculinity differing very much from the Emperor-worship and militaristic obedience enforced during the previous period of Japanese imperialism. "Astro Boy" quickly became (and remains) immensely popular in Japan and elsewhere as an icon and hero of a new world of peace and the renunciation of war, as also seen in Article 9 of the Japanese constitution. Similar themes occur in Tezuka's "New World" and "Metropolis".

By contrast, "Sazae-san" (meaning "Ms. Sazae") was drawn starting in 1946 by Machiko Hasegawa, a young woman artist who made her heroine a stand-in for millions of Japanese men and especially women rendered homeless by the war. Sazae-san does not face an easy or simple life, but, like Astro Boy, she too is highly affiliative and is deeply involved with her immediate and extended family. She is also a very strong character, in striking contrast to the officially sanctioned Neo-Confucianist principles of feminine meekness and obedience to the "good wife, wise mother" (, ) ideal taught by the previous military regime. Sazae-san faces the world with cheerful resilience, what Hayao Kawai calls a "woman of endurance." "Sazae-san" sold more than 62 million copies over the next half century.

Tezuka and Hasegawa were also both stylistic innovators. In Tezuka's "cinematographic" technique, the panels are like a motion picture that reveals details of action bordering on slow motion as well as rapid zooms from distance to close-up shots. More critically, Tezuka synchronised the placement of panel with the reader's viewing speed to simulate moving pictures. Hence in manga production as in film production, the person who decides the allocation of panels (Komawari) is credited as the author while most drawing are done by assistants. This kind of visual dynamism was widely adopted by later manga artists. Hasegawa's focus on daily life and on women's experience also came to characterize later shōjo manga.

Between 1950 and 1969, increasingly large audiences for manga emerged in Japan with the solidification of its two main marketing genres, "shōnen" manga aimed at boys and "shōjo" manga aimed at girls. Up to 1969, "shōjo" manga was drawn primarily by adult men for young female readers.

Two very popular and influential male-authored manga for girls from this period were Tezuka's 1953-1956 "Ribon no Kishi" ("Princess Knight" or "Knight in Ribbons") and Mitsuteru Yokoyama 1966 "Mahōtsukai Sarii" ("Little Witch Sally"). "Ribon no Kishi" dealt with the adventures of Princess Sapphire of a fantasy kingdom who had been born with male and female souls, and whose sword-swinging battles and romances blurred the boundaries of otherwise rigid gender roles. Sarii, the pre-teen princess heroine of "Mahōtsukai Sarii," came from her home in the magical lands to live on Earth, go to school, and perform a variety of magical good deeds for her friends and schoolmates. Yokoyama's "Mahōtsukai Sarii" was influenced by the US TV sitcom "Bewitched", but unlike Samantha, the main character of "Bewitched", a married woman with her own daughter, Sarii is a pre-teenager who faces the problems of growing up and mastering the responsibilities of forthcoming adulthood. "Mahōtsukai Sarii" helped create the now very popular "mahō shōjo" or "magical girl" subgenre of later manga. Both series were and still are very popular.

In 1969, a group of women manga artists later called the "Year 24 Group" (also known as "Magnificent 24s") made their "shōjo" manga debut (year 24 comes from the Japanese name for 1949, when many of these artists were born). The group included Hagio Moto, Riyoko Ikeda, Yumiko Ōshima, Keiko Takemiya, and Ryoko Yamagishi and they marked the first major entry of women artists into manga. Thereafter, "shōjo" manga would be drawn primarily by women artists for an audience of girls and young women.

In 1971, Ikeda began her immensely popular "shōjo" manga "Berusaiyu no Bara" ("The Rose of Versailles"), a story of Oscar François de Jarjayes, a cross-dressing woman who was a Captain in Marie Antoinette's Palace Guards in pre-Revolutionary France. In the end, Oscar dies as a revolutionary leading a charge of her troops against the Bastille. Likewise, Hagio Moto's work challenged Neo-Confucianist limits on women's roles and activities as in her 1975 "They Were Eleven", a "shōjo" science fiction story about a young woman cadet in a future space academy.

These women artists also created considerable stylistic innovations. In its focus on the heroine's inner experiences and feelings, "shōjo" manga are "picture poems" with delicate and complex designs that often eliminate panel borders completely to create prolonged, non-narrative extensions of time. All of these innovations – strong and independent female characters, intense emotionality, and complex design – remain characteristic of "shōjo" manga up to the present day.

In the following decades (1975–present), "shōjo" manga continued to develop stylistically while simultaneously evolving different but overlapping subgenres. Major subgenres have included romance, superheroines, and "Ladies Comics" (in Japanese, "redisu" レディース, "redikomi" レディコミ, and "josei" 女性 じょせい), whose boundaries are sometimes indistinguishable from each other and from "shōnen" manga.

In modern "shōjo" manga romance, love is a major theme set into emotionally intense narratives of self-realization. Japanese manga/anime critic Eri Izawa defines romance as symbolizing "the emotional, the grand, the epic; the taste of heroism, fantastic adventure, and the melancholy; passionate love, personal struggle, and eternal longing" set into imaginative, individualistic, and passionate narrative frameworks. These romances are sometimes long narratives that can deal with distinguishing between false and true love, coping with sexual intercourse, and growing up in an ambivalent world, themes inherited by subsequent animated versions of the story. These "coming of age" or "Bildungsroman" themes occur in both "shōjo" and "shōnen" manga.

In the "Bildungsroman", the protagonist must deal with adversity and conflict, and examples in "shōjo" manga of romantic conflict are common. They include Miwa Ueda's "Peach Girl",
Fuyumi Soryo's "Mars", and, for older readers, Moyoco Anno's "Happy Mania", Yayoi Ogawa's "Tramps Like Us", and Ai Yazawa's "Nana". In another "shōjo" manga "Bildungsroman" narrative device, the young heroine is transported to an alien place or time where she meets strangers and must survive on her own (including Hagio Moto's "They Were Eleven", Kyoko Hikawa's "From Far Away", Yû Watase's "", and Chiho Saito's "The World Exists For Me").

Yet another such device involves meeting unusual or strange people and beings, for example, Natsuki Takaya's "Fruits Basket"—one of the most popular "shōjo" manga in the United States—whose orphaned heroine Tohru must survive living in the woods in a house filled with people who can transform into the animals of the Chinese zodiac. In Harako Iida's "Crescent Moon", heroine Mahiru meets a group of supernatural beings, finally to discover that she herself too has a supernatural ancestry when she and a young tengu demon fall in love.

With the superheroines, "shōjo" manga continued to break away from neo-Confucianist norms of female meekness and obedience. Naoko Takeuchi's "Sailor Moon" ("Bishōjo Senshi Sēramūn": "Pretty Guardian Sailor Moon") is a sustained, 18-volume narrative about a group of young heroines simultaneously heroic and introspective, active and emotional, dutiful and ambitious. The combination proved extremely successful, and "Sailor Moon" became internationally popular in both manga and anime formats. Another example is CLAMP's "Magic Knight Rayearth," whose three young heroines, Hikaru, Umi, and Fuu, are magically transported to the world of Cefiro to become armed magical warriors in the service of saving Cefiro from internal and external enemies.

The superheroine subgenre also extensively developed the notion of teams ("sentai") of girls working together, like the "Sailor Senshi" in "Sailor Moon", the Magic Knights in "Magic Knight Rayearth", and the Mew Mew girls from Mia Ikumi's "Tokyo Mew Mew". By today, the superheroine narrative template has been widely used and parodied within the "shōjo" manga tradition (e.g., Nao Yazawa's "Wedding Peach" and "Hyper Rune" by Tamayo Akiyama) and outside that tradition, e.g., in "bishōjo" comedies like Kanan's "Galaxy Angel".

In the mid-1980s and thereafter, as girls who had read "shōjo" manga as teenagers matured and entered the job market, "shōjo" manga elaborated subgenres directed at women in their 20s and 30s. This "Ladies Comic" or "redisu"-"josei" subgenre has dealt with themes of young adulthood: jobs, the emotions and problems of sexual intercourse, and friendships or love among women.

Redisu manga retains many of the narrative stylistics of "shōjo" manga but has been drawn by and written for adult women. Redisu manga and art has often been, though not always, sexually explicit, but the content has characteristically been set into thematic narratives of pleasure and erotic arousal combined with emotional risk. Examples include Ryō Ramiya's "Luminous Girls", Masako Watanabe's "Kinpeibai" and the work of Shungicu Uchida Another subgenre of "shōjo"-"redisu" manga deals with emotional and sexual relationships among women ("akogare" and "yuri"), in work by Erica Sakurazawa, Ebine Yamaji, and Chiho Saito. Other subgenres of "shōjo"-"redisu" manga have also developed, e.g., fashion ("oshare") manga, like Ai Yazawa's "Paradise Kiss" and horror-vampire-gothic manga, like Matsuri Hino's "Vampire Knight", Kaori Yuki's "Cain Saga", and Mitsukazu Mihara's "DOLL", which interact with street fashions, costume play ("cosplay"), J-Pop music, and goth subcultures in various ways.

By the start of the 21st century, manga for women and girls thus represented a broad spectrum of material for pre- and early teenagers to material for adult women.

Manga for male readers can be characterized in different ways. One is by the age of its intended audience: boys up to 18 years old ("shōnen" manga) and young men 18 to 30 years old ("seinen" manga). Another approach is by content, including action-adventure often involving male heroes, slapstick humor, themes of honor, and sometimes explicit sex. Japanese uses different kanji for two closely allied meanings of "seinen"—青年 for "youth, young man" and 成年 for "adult, majority"—the second referring to pornographic manga aimed at grown men, also called "seijin" ("adult," 成人) manga. "Shōnen", "seinen", and "seijin" manga share a number of features in common.

Boys and young men were among the earliest readers of manga after World War II. From the 1950s on, "shōnen" manga focused on topics thought to interest the archetypical boy: sci-tech subjects like robots and space travel, and heroic action-adventure. Early sh"ōnen" and "seinen" manga narratives often portrayed challenges to the protagonist’s abilities, skills, and maturity, stressing self-perfection, austere self-discipline, sacrifice in the cause of duty, and honorable service to society, community, family, and friends.
Manga with solitary costumed superheroes like Superman, Batman, and Spider-Man did not become popular as a "shōnen" genre. An exception is Kia Asamiya's "", released in the US by DC Comics and in Japan by Kodansha. However, lone antiheroes occur in Takao Saito's "Golgo 13" and Koike and Kojima's "Lone Wolf and Cub". "Golgo 13" is about an assassin who puts his skills to the service of world peace and other social goals, and Ogami Itto, the swordsman-hero of "Lone Wolf and Cub", is a widower caring for his son Daigoro while he seeks vengeance against his wife's murderers. However, Golgo and Itto remain men throughout and neither of them ever displays superpowers. Instead, these stories "journey into the hearts and minds of men" by remaining on the plane of human psychology and motivation.

Many "shōnen" manga have science fiction and technology themes. Early examples in the robot subgenre included Tezuka’s "Astro Boy" (see above) and Fujiko F. Fujio's 1969 "Doraemon", about a robot cat and the boy he lives with, which was aimed at younger boys. The robot theme evolved extensively, from Mitsuteru Yokoyama's 1956 "Tetsujin 28-go" to later, more complex stories where the protagonist must not only defeat enemies, but learn to master himself and cooperate with the mecha he controls. Thus, in "Neon Genesis Evangelion" by Yoshiyuki Sadamoto, Shinji struggles against the enemy and against his father, and in "Vision of Escaflowne" by Katsu Aki, Van not only makes war against Dornkirk’s empire but must deal with his complex feelings for Hitomi, the heroine.

Sports themes are also popular in manga for male readers. These stories stress self-discipline, depicting not only the excitement of sports competition but also character traits the hero needs to transcend his limitations and to triumph. Examples include boxing (Tetsuya Chiba's 1968-1973 "Tomorrow's Joe" and Rumiko Takahashi's 1987 "One-Pound Gospel") and basketball (Takehiko Inoue’s 1990 "Slam Dunk").
Supernatural settings have been another source of action-adventure plots in shōnen and some shōjo manga in which the hero must master challenges. Sometimes the protagonist fails, as in Tsugumi Ohba and Takeshi Obata's "Death Note", where protagonist Light Yagami receives a notebook from a Death God ("shinigami") that kills anyone whose name is written in it, and, in a "shōjo" manga example, Hakase Mizuki's "The Demon Ororon", whose protagonist abandons his demonic kingship of Hell to live and die on earth. Sometimes the protagonist himself is supernatural, like Kohta Hirano's "Hellsing", whose vampire hero Alucard battles reborn Nazis hellbent on conquering England, but the hero may also be (or was) human, battling an ever-escalating series of supernatural enemies (Hiromu Arakawa's "Fullmetal Alchemist", Nobuyuki Anzai's "Flame of Recca", and Tite Kubo's "Bleach").
Military action-adventure stories set in the modern world, for example, about WWII, remained under suspicion of glorifying Japan’s Imperial history and have not become a significant part of the "shōnen" manga repertoire. Nonetheless, stories about fantasy or historical military adventure were not stigmatized, and manga about heroic warriors and martial artists have been extremely popular. Some are serious dramas, like Sanpei Shirato's "The Legend of Kamui" and "Rurouni Kenshin" by Nobuhiro Watsuki, but others contain strongly humorous elements, like Akira Toriyama's "Dragon Ball".

Although stories about modern war and its weapons do exist, they deal as much or more with the psychological and moral problems of war as they do with sheer shoot-'em-up adventure. Examples include Seiho Takizawa's "Who Fighter", a retelling of Joseph Conrad's story "Heart of Darkness" about a renegade Japanese colonel set in WWII Burma, Kaiji Kawaguchi's "The Silent Service", about a Japanese nuclear submarine, and Motofumi Kobayashi's "Apocalypse Meow", about the Vietnam War told in talking animal format. Other battle and fight-oriented manga sometimes focus on criminal and espionage conspiracies to be overcome by the protagonist, such as in "Crying Freeman" by Kazuo Koike and Ryoichi Ikegami, "City Hunter" by Hojo Tsukasa, and the "shōjo" series "From Eroica with Love" by Yasuko Aoike, a long-running crime-espionage story combining adventure, action, and humor (and another example of how these themes occur across demographics).

For manga critics Koji Aihara and Kentaro Takekuma, such battle stories endlessly repeat the same mindless themes of violence, which they sardonically label the "Shonen Manga Plot Shish Kebob", where fights follow fights like meat skewered on a stick. Other commentators suggest that fight sequences and violence in comics serve as a social outlet for otherwise dangerous impulses. Shōnen manga and its extreme warriorship have been parodied, for example, in Mine Yoshizaki's screwball comedy "Sgt. Frog" ("Keroro Gunso"), about a platoon of slacker alien frogs who invade the Earth and end up free-loading off the Hinata family in Tokyo.

In early "shōnen" manga, men and boys played all the major roles, with women and girls having only auxiliary places as sisters, mothers, and occasionally girlfriends. Of the nine cyborgs in Shotaro Ishinomori's 1964 "Cyborg 009", only one is female, and she soon vanishes from the action. Some recent "shōnen" manga virtually omit women, e.g., the martial arts story "Baki the Grappler" by Itagaki Keisuke and the supernatural fantasy "Sand Land" by Akira Toriyama. However, by the 1980s, girls and women began to play increasingly important roles in "shōnen" manga, for example, Toriyama's 1980 "Dr. Slump", whose main character is the mischievous and powerful girl robot Arale Norimaki.
The role of girls and women in manga for male readers has evolved considerably since Arale. One class is the pretty girl ("bishōjo"). Sometimes the woman is unattainable, but she is generally an object of the hero's emotional and sexual interest, like Belldandy from "Oh My Goddess!" by Kōsuke Fujishima and Shao-lin from "Guardian Angel Getten" by Minene Sakurano. In other stories, the hero is surrounded by such girls and women, as in "" by Ken Akamatsu and "Hanaukyo Maid Team" by Morishige. The male protagonist does not always succeed in forming a relationship with the woman, for example when Bright Honda and Aimi Komori fail to bond in "Shadow Lady" by Masakazu Katsura. In other cases, a successful couple's sexual activities are depicted or implied, like "Outlanders" by Johji Manabe. Other stories feature an initially naive hero subsequently learning how to deal and live with women emotionally and sexually, like Yota in "Video Girl Ai" by Masakazu Katsura, Train Man in "" by Hidenori Hara, and Makoto in "Futari Ecchi" by Katsu Aki. In erotic manga ("seijin" manga), often called "hentai" manga in the US, a sexual relationship is taken for granted and depicted explicitly, as in work by Toshiki Yui and in "Were-Slut" by Jiro Chiba and "Slut Girl" by Isutoshi. The result is a range of depictions of boys and men from naive to very sexually accustomed.

Heavily armed female warriors ("sentō bishōjo") represent another class of girls and women in manga for male readers. Some "sentō bishōjo" are battle cyborgs, like Alita from "Battle Angel Alita" by Yukito Kishiro, Motoko Kusanagi from Masamune Shirow's "Ghost in the Shell", and Chise from Shin Takahashi's "Saikano". Others are human, like Attim M-Zak from Hiroyuki Utatane's "Seraphic Feather", Johji Manabe's Karula Olzen from "Drakuun", and Alita Forland (Falis) from Sekihiko Inui's "Murder Princess".

As of 2013 national censorship laws and local ordinances remain in Japan and the public response to the publication of manga with sexual content or the depiction of nudity has been mixed. Series have an audience and sell well but their publication also encounters opposition. In the early 1990s the opposition resulted in the creation of "Harmful manga" lists and a shift in the publishing industry. By this time large publishers had created a general manga demand but the corollary is that they were also susceptible to public opinion in their markets. Faced with criticism from certain segments of the population and under pressure from industry groups to self-regulate, major publishing houses discontinued series, such as "Angel" and "1+2=Paradise", while smaller publication companies, not as susceptible to these forces, were able to fill the void.

With the relaxation of censorship in Japan after the early 1990s, various forms of graphically drawn sexual content appeared in manga intended for male readers that correspondingly occurred in English translations. These depictions ranged from partial to total nudity through implied and explicit sexual intercourse through sadomasochism (SM), incest, rape, and sometimes zoophilia (bestiality). In some cases, rape and lust-murder themes came to the forefront, as in "Urotsukidōji" by Toshio Maeda and "Blue Catalyst" from 1994 by Kei Taniguchi, but these extreme elements are not commonplace in either untranslated or translated manga.

"Gekiga" literally means "drama pictures" and refers to a form of "aesthetic realism" in manga. Gekiga style storytelling tends to be emotionally dark, adult-oriented, and sometimes deeply violent, focusing on the day-in, day-out realities of life, often drawn in gritty fashion. Gekiga arose in the late 1950s and 1960s partly from left-wing student and working class political activism and partly from the aesthetic dissatisfaction of young manga artists like Yoshihiro Tatsumi with existing manga. Examples include Sanpei Shirato's 1959–1962 "Chronicles of a Ninja's Military Accomplishments" ("Ninja Bugeichō"), the story of Kagemaru, the leader of a peasant rebellion in the 16th century, which dealt directly with oppression and class struggle, and Hiroshi Hirata's "Satsuma Gishiden", about uprisings against the Tokugawa shogunate.

Gekiga can be seen as the Japanese equivalent of the graphic novel culture happening in Europe (Hugo Pratt, Didier Comes, Jacques Tardi) and in the U.S. (Will Eisners "A Contract with God", Art Spiegelmans Maus, Robert Crumbs autobiographical works) and in South America (Alberto Breccia, Hector Oesterheld). For that reason typical graphic novel publishers as Drawn & Quarterly and Fantagraphics started publishing many English version of Japanese Gekiga highlights in more recent years.

As the social protest of these early years waned, gekiga shifted in meaning towards socially conscious, mature drama and towards the avant-garde. Examples include Koike and Kojima's "Lone Wolf and Cub" and "Akira", an apocalyptic tale of motorcycle gangs, street war, and inexplicable transformations of the children of a future Tokyo. Another example is Osamu Tezuka's 1976 manga "MW", a bitter story of the aftermath of the storage and possibly deliberate release of poison gas by U.S. armed forces based in Okinawa years after World War II. Gekiga and the social consciousness it embodies remain alive in modern-day manga. An example is "Ikebukuro West Gate Park" from 2001 by Ira Ishida and Sena Aritou, a story of street thugs, rape, and vengeance set on the social margins of the wealthy Ikebukuro district of Tokyo.





</doc>
<doc id="56646155" url="https://en.wikipedia.org/wiki?curid=56646155" title="Piccoma">
Piccoma

When the service was first launched, it offered a regular model of buying each individual manga and volume similar to other online shops but it has since moved to adopt the webtoon model where a user can purchase individual chapters and wait 24 hours to read some for free. Korean webtoons that are offered on Kakao's services (Daum Webtoon & KakaoPage) are offered through Piccoma in Japanese. Kakao Japan announced that it will start offering original Japanese, Korean, and Chinese webtoons for Piccoma in the summer of 2018.


</doc>
<doc id="62203986" url="https://en.wikipedia.org/wiki?curid=62203986" title="Kono Manga ga Sugoi!">
Kono Manga ga Sugoi!



</doc>
<doc id="3859951" url="https://en.wikipedia.org/wiki?curid=3859951" title="Manga outside Japan">
Manga outside Japan

Manga, or Japanese comics, have appeared in translation in many different languages in different countries. France represents about 40% of the European manga market and in 2011 manga represented 40% of the comics being published in the country. In 2007, 70% of the comics sold in Germany were manga. In the United States, manga comprises a small (but growing) industry, especially when compared to the inroads that Japanese animation or Japanese Video Games have made in the USA. One example of a manga publisher in the United States, VIZ Media, functions as the American affiliate of the Japanese publishers Shogakukan and Shueisha. The UK has fewer manga publishers than the U.S.
Since written Japanese fiction usually flows from right to left, manga artists draw and publish this way in Japan. When first translating various titles into Western languages, publishers reversed the artwork and layouts in a process known as "flipping", so that readers could follow the books from left-to-right. However, various creators (such as Akira Toriyama) did not approve of the modification of their work in this way, and requested that foreign versions retain the right-to-left format of the originals. Soon, due both to fan demand and to the requests of creators, more publishers began offering the option of right-to-left formatting, which has now become commonplace in North America. Left-to-right formatting has gone from the rule to the exception.

Translated manga often includes notes on details of Japanese culture that foreign audiences may not find familiar.

One company, TOKYOPOP (founded 1997), produces manga in the United States with the right-to-left format as a highly publicized point-of-difference.

The Chinese Ministry of Culture announced in 2015 that it has blacklisted 38 Japanese anime and manga titles from distribution in China, including popular series like Death Note and Attack on Titan online or in print, citing "scenes of violence, pornography, terrorism and crimes against public morality." 

Manga in India is published by VIZ Media.

Manga in Indonesia is published by Elex Media Komputindo, Acolyte, Level Comic, M&C and Gramedia, and has greatly influenced Indonesia's original comic industry.

The wide distribution of scanlations actually contributes to the growth of publication of bootleg manga, which is printed in lower quality. One of the most notable publisher is Seventh Heaven which publishes bootleg version of One Piece. Many popular titles, such as Bleach, Loki, Magister Nagi, Rose Hip Zero, and Kingdom Hearts, have been pirated, which draws controversy toward manga readers in Indonesia.

Manga in Pakistan is usually imported and sold in bookstores countrywide. It is published in English by VIZ Media, Yen Press and Seven Seas Entertainment.

Manga in the Philippines were imported from the US and were sold only in specialty stores and in limited copies. The first manga in Filipino language is Doraemon which was published by J-Line Comics and was then followed by Case Closed.

A few local publishing companies like VIVA-PSICOM Publishing feature manga created by local artists whose stories are usually based from popular written books from the writing site Wattpad and are read from left to right instead of the usual right-to-left format for Japanese manga. The very first commercial local manga is She Died, an adaptation of the book written by Wattpad writer HaveYouSeenThisGirl. The art was done by Enjelicious.

In 2015, VIVA-PSICOM Publishing has announced that they will start publishing manga titles in the Filipino language with the line-up starting with Hiro Mashima's Fairy Tail and Isayama Hajime's Attack on Titan.

In 2015, Boy's Love manga became popular through the introduction of BL manga by printing company BLACKink. Among the first BL titles to be printed were Poster Boy, Tagila, and Sprinters, all were written in Filipino. BL manga have become bestsellers in the top three bookstore companies in the Philippines since their introduction in 2015.

The company Chuang Yi publishes manga in English and Chinese in Singapore; some of Chuang Yi's English-language titles are also imported into Australia, New Zealand and the Philippines by Madman Entertainment. Singapore also has its own official Comics Society, led by manga artist Wee Tian Beng, illustrator of the Dream Walker series.

In Thailand, before 1992, almost all available manga were fast, unlicensed, poor quality bootlegs. However, due to copyright laws, this has changed and copyrights protect nearly all published manga. Thailand's prominent manga publishers include Nation Edutainment, Siam Inter Comics, Vibulkij, and Bongkoch.

Many parents in Thai society are not supportive of manga. In October 2005, there was a television programme broadcast about the dark side of manga with exaggerated details, resulted in many manga being banned. The programme received many complaints and issued an apology to the audience.

In 2015, Boy's Love manga have become popular in mainstream Thai consumers, leading to television series adapted from BL manga stories since 2016.

France has a particularly strong and diverse manga market. Many works published in France belong to genres not well represented outside Japan, such as too adult-oriented drama, or too experimental and "avant-garde" works. Early editors like Tonkam have published Hong-Kong authors (Andy Seto, Yu & Lau) or Korean authors (Kim Jae-hwan, Soo & Il, Wan & Weol and Hyun Se Lee) in their "manga" collection during 1995/1996 which is quite uncommon. Also, some Japanese authors, such as Jiro Taniguchi, are relatively unknown in other western countries but received much acclaim in France.

Since its introduction in the 1990s, "manga" publishing and "anime" broadcasting have become intertwined in France, where the most popular and exploited "shōnen", "shōjo" and "seinen" TV series were imported in their paper version. Therefore, Japanese books ("manga") were naturally and readily accepted by a large juvenile public who was already familiar with the series and received the "manga" as part of their own culture. A strong parallel backup was the emergence of Japanese video games, Nintendo/Sega, which were mostly based on "manga" and "anime" series.

Producer Jean Chalopin contacted some Japanese studios, such as Toei (who did "Grendizer"); and Tokyo Movie Shinsha, Studio Pierrot and Studio Junio produced French-Japanese series. Even though made completely in Japan by character-designers such as Shingo Araki, the first Chalopin production of this type, "Ulysses 31" took thematic inspiration from the Greek "Odyssey" and graphic influence from Stanley Kubrick's "". "Ulysses 31" went on sale in 1981, other shows produced by DiC Entertainment followed in 1982, "Jayce and the Wheeled Warriors", "Mysterious Cities of Gold", later "M.A.S.K.", etc. Such series were popular enough to allow the introduction of licensed products such as tee shirts, toys, stickers, mustard glass, mugs or "keshi". Also followed a wave of "anime" adaptations of European tales by Studio Pierrot and mostly by the Nippon Animation studio, "e.g." Johanna Spyri's "Heidi, Girl of the Alps" (1974), Waldemar Bonsels's "Maya the Honey Bee" (1975), Hector Malot's "" (1977), Cécile Aubry's "Belle and Sebastian" (1980), or Jules Verne's "Around the World with Willy Fog" (1983), notable adaptation of American works were Mark Twain's "Adventures of Tom Sawyer" (1980) and Alexander Key's "Future Boy Conan". Interesting cases are Alexandre Dumas, père's "The Three Musketeers" adapted to "Dogtanian and the Three Muskehounds" (1981) and Sir Arthur Conan Doyle's Sherlock Holmes become "Sherlock Hound" (1984), both turned human characters into anthropomorph animals.

Such anthropomorphism in tales comes from old and common storytelling traditions in both Japanese and French cultures, including the "Chōjū giga" "emaki" (the true origins of "manga") of Toba Sōjō (1053–1140), and the animal fables of Jean de La Fontaine (1621–1695). Changing humans to anthropomorphized dogs reflects a known form of Cynicism: etymologically speaking, the bite of the Cynic comes from the fact he is a dog ("cyno" means "dog" in Greek). The adaptations of these popular tales made easier the acceptance and assimilation of semi-Japanese cultural products in countries such as France, Italy or Spain. The localization including credits removal by Saban or DiC, was such that even today, twenty or thirty years later, most of French adults who have watched series like "Calimero" (1974) adapted from an Italian novel, "Wanpaku Omukashi Kum Kum" (1975), "Barbapapa" (1977) adapted from a French novel, or "Monchichi" (1980) as kids don't even know they were not local animation but "Japananimation" created in Japan.

In 1986 and 1987 three new private or privatized television channels appeared on French airwaves. An aggressive struggle for audience, especially on children television shows, started between the two public and the two private channels. After the private channels lost market share, they counter-attacked with a non-Japanese lineup, mostly American productions such as Hanna-Barbera. This ploy failed, and TF1 remained pre-eminent in children's TV shows with its Japanese licenses.

In 1991 French theaters showed an "anime" feature-film for the first time: Katsuhiro Otomo's "Akira", a teen-rated, SF movie supported by manga publisher Glénat. TF1 Video edited the video (VHS) version for the French market, and "Akira" quickly became an "anime" reference. However, Japanese animation genre became massively exploited by TV shows from the late 1980s onwards, most notably the cult Club Dorothée show (mostly dedicated to Toei anime and tokusatsu series). In fact, the commercial relationship between the Japanese studio and the French show producers were so good, that the French presenter was even featured in a Metal Hero Series episode as guest star.

Just as in a Japanese "manga" series magazine, the Club Dorothée audience voted by phone or minitel to select and rank their favourite series. Top-ranked series continued the following week, others stopped. The most popular series were "Dragon Ball" and later its sequel, "Dragon Ball Z", which became number one, and was nicknamed "le chouchou" ("the favorite") by the show presenter, Dorothée. As the series kept number one for several months, Dorothée invited Akira Toriyama (Toei Animation), creator of the series, on the TV show studio to introduce him to the French audience and award him a prize in the name of the TV show.

Saint Seiya was another "anime" series to achieve popularity in France. It showed more violence – directed towards an older audience – than the Nippon Animation studio "shōnen"/"shōjo" series of the 1970s and 1980s. Notable Toei and non-Toei "anime" series broadcast by that time on French TV included "Captain Tsubasa", "Robotech", "High School! Kimengumi" and "Kinnikuman". This cult TV show ran from 1987 to 1997.

Glénat published the first manga issued in France, "Akira", in 1990 — supported by the respected newspaper "Libération" and by the national TV channel Antenne 2. Followers included "Dragon Ball" (1993), "Appleseed" (1994), "Ranma 1/2" (1994) and five others. In the mid-1990s, "manga" magazines in B5 size like "Kameha" (Glénat) and "Manga Player" (MSE) were available.

At the same time a controversy arose among some parents. In particular, the conservative association "" started a media polemic about the undesirable contents, such as violence, portrayed in the Club Dorothée, a kids' TV show. By this time, a generational conflict had arisen between the young fans of "Japanimation" (in use until "anime" became mainstream) and the older "Japoniaiseries" (a pejorative pun for "Japonaiseries", literally "Japanese stuff" and "niaiseries", "simpleton stuff"). Ségolène Royal even published a book, "Le Ras le bol des bébés zappeurs" in which manga are described as decadent dangerous and violent. She hasn't changed her position on that topic yet. The same adult content controversy was applied to "hentai" manga, including the notorious, "forbidden", "Shin Angel" by U-Jin, published by pioneers such as Samourai Editions or Katsumi Editions and later to magazines. The first "hentai" series magazine, "Yoko", featured softcore series like Yuuki's "Tropical Eyes". It was first issued in late 1995. The same year, the noir and ultra-violent series, "Gunnm" (aka "Battle Angel Alita"), was serialized in a slim, monthly edition. Around the same period a hardcore version of Yoko magazine "Okaz" was issued.

In 1996 the production group of Club Dorothée, broadcast on private channel TF1, set up a cable/satellite channel dedicated to "manga" and "anime". The new channel changed its name to "Mangas" in 1998: the concepts of "anime" and "manga" have become intertwined in France, and "manga" actually became the mainstream generic term to designate the two media. The channel broadcasts former discontinued series from the Club Dorothée both to nostalgic adults and to new and younger generations.

In late 1999 respected newspapers such as "Le Monde" gave critical acclaim to Hiroyuki Okiura's "Jin-Roh", and in 2000, Hayao Miyazaki's "Princess Mononoke" became a commercial success.

In 2004, Mamoru Oshii's "" became the first animation finalist in the prestigious International Film Festival of Cannes, which demonstrates a radical perspective change and a social acceptance of Japanese "anime"/"manga". Since 2005, contemporary Japanese series such as "Naruto", "Initial D", "Great Teacher Onizuka", "Blue Gender" or "Gunslinger Girl" appeared on new, analog/digital terrestrial (public) and on satellite/broadband (private) channels. As the highly aggressive competition who raged once between, the sole two or three available channels no more exists in the new, vast, and segmented French TV offer, the "anime" is doing a revival in France. In 2011, 40% of the comics published in France were manga. In 2013, there were 41 publishers of manga in France and, together with other Asian comics, manga represented around 40% of new comics releases in the country, surpassing Franco-Belgian comics for the first time.

A surge in the growth of manga publishing circa 1996 coincided with the "Club Dorothée" show losing its audience – which eventually led to the show going off the air. Some early publishers like Glénat, adapted manga using the Western reading direction and its induced work of mirroring each panel and graphical signs, and also using a quality paper standard to the Franco-Belgian comics, while others, like J'ai Lu, were faithful to the original manga culture and not only kept the original, inverted, Japanese direction reading but also used a newspaper standard, cheap quality, paper just like in Japan. The Japanese manga was such an important cultural phenomenon that it quickly influenced French comics authors. A new "French manga" genre emerged, known as "La nouvelle manga" ("lit. the new manga") in reference to the French Nouvelle Vague.

Much like France, television had a large part in influencing the popularity of Japanese Manga, particularly with "Dragon Ball" and "Saint Seiya" showing up in the early nineties. Manga shook up the Spanish comics industry with new publishers taking in different directions with mostly publishing up manga instead of European comics.

Since late 1970s, Italy had many anime broadcast on private channels, taking a great popularity to Japanese animation. Unfortunately, many of these were censored on some channels and for this reason some anime were not taken very seriously. Many Manga drawn by Italian artists were also released, based on these censored versions. The publishing of Akira took an interest in older readers picking up other manga in the same vein. Italy had a high acceptance of comics with violence and nudity which contributed to this development. The very first un-flipped version of a manga was "Dragon Ball" released for the very first time in a tankōbon format.

Unlike its European neighbours, Germany never had a vibrant local comic production. A volume of "Barefoot Gen" was licensed in Germany in the 1980s, as was "Japan Inc.", published by small presses. "Akira"'s first volume was not very popular. Paul Malone attributes the wider distribution of manga in the late 1990s to the fledgling commercial television stations showing dubbed anime, which led to the popularity of manga. Malone also notes that the native German comics market collapsed at the end of the 1990s. Manga began outselling other comics in 2000.

With a few other series like "Appleseed" in the following years, the "manga movement" picked up speed with the publication of "Dragon Ball", an un-flipped German manga, in late 1996. In 2007, manga accounted for approximately 70–75% of all comics published in Germany.

In Portugal, manga has been published by Bertrand, Devir, Mangaline, Meribérica/Líber, Planeta DeAgostini and Texto Editora. The first manga published in Portugal were "Ranma ½" and "Spriggan", both in 1995. There is a magazine of manga-inspired Portuguese comics, "Banzai".

Comics never gained high popularity in Russia, only few Marvel's titles being a moderate success. Russian readers traditionally considered them children's literature, so the manga market developed late. A strong movement of anime fans helped to spread manga. The general director of Egmont Russia Lev Yelin commented that the most popular manga series in Japan are comics which "contain sex and violence", so they probably won't be published in Russia. A representative of Sakura Press (the licensor and publisher of "Ranma ½", "Gunslinger Girl" and some other titles) noted that although this niche is promising, it's hard to advance on the market, because "in Russia comics are considered children's literature". It is also impossible for publishers to predict the success or failure of any specific title. On the contrary, Rosmen's general director Mikhail Markotkin said the whole popularity of comics doesn't matter, as only artistic talent and good story make a successful project, and only such manga "will work" on the market.

The first officially licensed and published manga series in Russia was "Ranma ½". Sakura Press released the first volume in 2005. Since then several legal companies appeared, including Comics Factory and Comix-ART. Comix-ART, which is working in collaboration with Eksmo, one of the largest publishing houses in Russia, was the first company to publish Original English-language manga (usually called "manga" or just "comics"), such as "Bizenghast", "Shutterbox" and "Van Von Hunter".

Manga has been published in Poland since the 1990s when the owner (a Japanese person) of one of the biggest publishers (J.P.F.) translated Dragon Ball into Polish to practice the language. Later, he decided to publish his work. The publisher is known from series like Dragon Ball, One Piece, Bleach and many others including Junjo Ito horrors or well-known, old josei manga. Next to JPF, there are publishers such as Waneko or Studio JG known as the two other publishers making up the top three biggest in Poland. Waneko is well known for publishing the largest number of manga monthly and series like GTO, Kuroshitsuji, Pandora Hearts, and Bakuman. They are also very known for publishing less popular series like Bokura no Kiseki. Studio JG makes a lot of controversy by taking long breaks between manga volumes, leading many fans to express frustration at their attitude. They are known from series like Toradora, and Spice and Wolf. Behind that, there are publishers like Yumegari (though manhwa mainly), Kotori (known from Sword Art Light Novel and many yaoi manga), and Dango, which is the youngest of all Polish publishers. Dango is very much appreciated by fans due to good quality of volumes and the many extra free gadgets included. Yaoi manga sell well in Poland. Another publisher which deserves attention is Hanami, known for more mature manga like Monster and Pluto.

The growth of manga translation and publishing in the United States has been a slow progression over several decades. The earliest manga-derived series to be released in the United States was a redrawn American adaptation of Osamu Tezuka's "Astro Boy" published by Gold Key Comics starting in 1965.

In 1979, the Gold Key published the comic book Battle of Planets, based on a television series of the same name. Marvel published a series based Shogun Warriors, bringing characters of the mecha anime and manga series: Brave Raideen, Chodenji Robo Combattler V and Wakusei Robo Danguard Ace.

One of the first manga to be distributed in English in the US with its original artwork intact was Keiji Nakazawa's major work "Barefoot Gen" in 1978, which was originally translated and printed under the auspices of Project Gen in Japan (by volunteers) to spread Nakazawa's message to the world, and then sent overseas and distributed in the U.S. by New Society Publishers. The second volume was translated by Frederik Schodt and Jared Cook. In December 1982 the San Francisco-based publisher Educomics released a colorized and translated version of Keiji Nakazawa's "I Saw It". Four translated volumes of Barefoot Gen were initially distributed in the U.S. in the early 1980s, especially with the help of Alan Gleason, who served as the local coordinator for the Barefoot Gen project. Short works by several "Garo"-affiliated artists including Yoshiharu Tsuge and Terry Yumura appeared in May 1985 in "RAW"'s no. 7 "Tokyo Raw" special.

In 1987, Viz Comics, an American subsidiary of the Japanese publishers Shogakukan and Shueisha, began publishing translations of three manga series – "Area 88", "Mai the Psychic Girl", and "The Legend of Kamui" – in the U.S. in association with the American publisher Eclipse Comics. Viz went on to bring English translations of popular series such as "Ranma ½" and "Nausicaä of the Valley of the Wind" in the late 1980s and early 1990s. Some other American publishers released notable translations of Japanese comics in this period, such as First Comics' serialization of "Lone Wolf and Cub" which started in May 1987. However, the first manga to make a strong impression on American audiences was Katsuhiro Otomo's "Akira", which was brought to the United States in colorized form in 1988 by Epic Comics, a division of Marvel.

Throughout the 1990s, manga slowly gained popularity as Viz Media, Dark Horse and Mixx (now Tokyopop) released more titles for the US market. Both Mixx and Viz published manga anthologies: "MixxZine" (1997–1999) ran serialized manga such as "Sailor Moon", "Magic Knight Rayearth" and "Ice Blade", while Viz's "Animerica Extra" (1998–2004) featured series including "Fushigi Yugi", "Banana Fish" and "Utena: Revolutionary Girl". In 2002 Viz began publishing a monthly American edition of the famous Japanese "phone book"-style manga anthology "Shōnen Jump" featuring some of the most popular manga titles from Japan, including "Dragon Ball Z", "Naruto", "Bleach" and One Piece. Its circulation far surpassed that of previous American manga anthologies, reaching 180,000 in 2005. Also in 2005, Viz launched "Shojo Beat", a successful counterpart to "Shonen Jump" aimed at female readers.

In 2002, Tokyopop introduced its "100% Authentic Manga" line, which featured unflipped pages and were smaller in size than most other translated graphic novels. This allowed them be retailed at a price lower than that of comparable publications by Viz and others. The line was also made widely available in mainstream bookstores such as Borders and Barnes & Noble, which greatly increased manga's visibility among the book-buying public. After Tokyopop's success, most of the other manga companies switched to the smaller unflipped format and offered their titles at similar prices.

A large number of small companies in the United States publish manga. Several large publishers have also released, or expressed interest in releasing manga. Del Rey translated and published several Japanese series including "xxxHolic", "" and, "", while Harlequin has brought its Ginger Blossom line of manga, originally released only in Japan, to the United States as well.

In Australia and New Zealand, many popular Japanese- and Chinese-language manga and anime are distributed by Madman Entertainment.

Before the 1990s some trial marketing of manga took place in Brazil, including "Lone Wolf and Cub", the first one published in the country in 1988, "Mai, the Psychic Girl", "Akira", "Cobra", "Crying Freeman", and "The Legend of Kamui". The Brazilian "shōnen" market started in the mid-1990s with "Ranma ½" published by Animangá, although the publication did not prove successful (due to the fact that it was released in the American format and contained only two chapters per issue, roughly equivalent to one fourth of a "tankohon"). It was followed by "", released by Conrad in 1999, during the "Pokémon" boom.

In 2000, Conrad published "Saint Seiya" and "Dragon Ball" (both titles already well known, since the equivalent anime had been highly successful in the 1990s). After the success of these titles, Conrad released not only trendy manga like "One Piece", "Vagabond", "Neon Genesis Evangelion", and "Slam Dunk", but also classic manga like Osamu Tezuka titles (including "Adolf" and "Buddha"), "Nausicaä", and less known titles like "Bambi and Her Pink Gun" and "Sade".

In 2003, the Japanese-Brazilian company Japan Brazil Communication (JBC) started publishing manga, releasing "Rurouni Kenshin", "Magic Knight Rayearth", "Cardcaptor Sakura" and "Video Girl Ai". In 2009, JBC published Clamp titles like "X/1999", "Tsubasa Reservoir Chronicle" and "xxxHolic", and popular titles like "Inuyasha", "Negima!", "Fruits Basket", "Death Note", "Fullmetal Alchemist", "Yu-Gi-Oh!", "Shaman King", "Love Hina" and "Bakuman", having also picked up the publishing rights for "Ranma ½" and "Neon Genesis Evangelion" in the same year.

In 2004, Panini started publishing manga, with the release of "Peach Girl" and "". In 2012, Panini published the most popular manga in Brazil: "Naruto" and "Bleach", as well as titles like "Black Lagoon", "Highschool of the Dead", "Full Metal Panic!" and "Welcome to the N.H.K.". Panini has also, in 2012, acquired the publishing rights to "One Piece" in Brazil, continuing publication from where Conrad had stopped (Japanese volume 37) as well as reprinting earlier volumes in the original Japanese format.

Originally, Brazilian manga appeared with about half the size of a tankoubon (about 100 pages of stories and two to eight pages of extras), but almost all of the manga is released in the original format.

After years of negotiation, JBC finally released Sailor Moon in early 2014. The edition, regarded as the most important release ever done by the company, was reportedly praised by creator Naoko Takeuchi and Kodansha employees, thanks to its quality.

Another popular form of manga distribution outside Japan involves Internet scanlations (or "scanslations"). Typically, a small group of people scan the original version of a series with no current license in the language which they wish to translate it to, translate it, and freely distribute it; usually through the use of IRC or BitTorrent.

Manga has proved so popular that it has led to other companies such as Antarctic Press, Oni Press, Seven Seas Entertainment and Tokyopop, as well as long-established publishers like Marvel and Archie Comics, to release their own manga-inspired works that apply the same artistic stylings and story pacing commonly seen in Japanese manga. One of the first of these such works came in 1985 when Ben Dunn, founder of Antarctic Press, released "Mangazine" and "Ninja High School".

While Antarctic Press actively refers to its works as "American Manga", it does not source all of these manga-inspired works from the United States. Many of the artists working on Seven Seas Entertainment series such as "Last Hope (manga)" and "Amazing Agent Luna" are Filipino and TOKYOPOP has hired a variety of Korean and Japanese artists to work on titles such as "Warcraft" and "Princess Ai". Many of these works have been classified on the Internet with titles such as OEL Manga, MIC, and World Manga, although none of these terms have actually been used by manga companies to describe these works on the books themselves.

In Brazil, the popularity of manga is marked also by the large number of Japanese and descendants in the country. In the 1960s, some of Japanese descent, such as Minami Keizi and Claudio Seto, started using graphic influences, narratives or manga themes in their work in EDREL publisher founded by Keizi.

In Germany, as manga began outselling domestic comics in 2000, German publishers began supporting German creators of manga-styled comics. Jürgen Seebeck's "Bloody Circus" was not popular amongst German manga readers due to its European style, and other early German manga artists were affected by cancellations. After this, German publishers began focussing on female creators, due to the popularity of shōjo manga, leading to what Paul Malone describes as a "home-grown shōjo boom", and "more female German comics artists in print than ever before". However, genuinely manga-influenced stylistic conventions, such as sweatdrops, are employed to ensure "authenticity", original German works are flipped to read in a right-to-left style familiar to manga readers, author's afterwords and sidebars are common, and many German manga take place in Asia.

The Arabic language manga "Canary 1001" is by a group calling themselves Amateam, whose director is Wahid Jodar, from the United Arab Emirates. Another Arab language manga is "Gold Ring", by Qais Sedeki, from 2009, also from the United Arab Emirates. Both groups of artists use the word "manga" for their work.

In May 2010, Glenat Spain introduced their new line of works known as "Linea Gaijin" which showcases the works of several Spanish and Latin American comic book artists. This is an effort on the part of Glenat to bring fresh new content and breed a new generation of manga-inspired artists that grew up reading manga. The line began with titles such as "Bakemono", "Dos Espadas", and "Lettera" that were shown on the "Salón del Manga de Barcelona" in October 2010, but it would later introduce other works as well.




</doc>
<doc id="18839" url="https://en.wikipedia.org/wiki?curid=18839" title="Music">
Music

Music is an art form, and cultural activity, whose medium is sound. General definitions of music include common elements such as pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, meter, and articulation), dynamics (loudness and softness), and the sonic qualities of timbre and texture (which are sometimes termed the "color" of a musical sound). Different styles or types of music may emphasize, de-emphasize or omit some of these elements. Music is performed with a vast range of instruments and vocal techniques ranging from singing to rapping; there are solely instrumental pieces, solely vocal pieces (such as songs without instrumental accompaniment) and pieces that combine singing and instruments. The word derives from Greek μουσική ("mousike"; "art of the Muses").
See glossary of musical terminology.

In its most general form, the activities describing music as an art form or cultural activity include the creation of works of music (songs, tunes, symphonies, and so on), the criticism of music, the study of the history of music, and the aesthetic examination of music. Ancient Greek and Indian philosophers defined music as tones ordered horizontally as melodies and vertically as harmonies. Common sayings such as "the harmony of the spheres" and "it is music to my ears" point to the notion that music is often ordered and pleasant to listen to. However, 20th-century composer John Cage thought that any sound can be music, saying, for example, "There is no noise, only sound."

The creation, performance, significance, and even the definition of music vary according to culture and social context. Indeed, throughout history, some new forms or styles of music have been criticized as "not being music", including Beethoven's "Grosse Fuge" string quartet in 1825, early jazz in the beginning of the 1900s and hardcore punk in the 1980s. There are many types of music, including popular music, traditional music, art music, music written for religious ceremonies and work songs such as chanteys. Music ranges from strictly organized compositions–such as Classical music symphonies from the 1700s and 1800s, through to spontaneously played improvisational music such as jazz, and avant-garde styles of chance-based contemporary music from the 20th and 21st centuries.

Music can be divided into genres (e.g., country music) and genres can be further divided into subgenres (e.g., country blues and pop country are two of the many country subgenres), although the dividing lines and relationships between music genres are often subtle, sometimes open to personal interpretation, and occasionally controversial. For example, it can be hard to draw the line between some early 1980s hard rock and heavy metal. Within the arts, music may be classified as a performing art, a fine art or as an auditory art. Music may be played or sung and heard live at a rock concert or orchestra performance, heard live as part of a dramatic work (a music theater show or opera), or it may be recorded and listened to on a radio, MP3 player, CD player, smartphone or as film score or TV show.

In many cultures, music is an important part of people's way of life, as it plays a key role in religious rituals, rite of passage ceremonies (e.g., graduation and marriage), social activities (e.g., dancing) and cultural activities ranging from amateur karaoke singing to playing in an amateur funk band or singing in a community choir. People may make music as a hobby, like a teen playing cello in a youth orchestra, or work as a professional musician or singer. The music industry includes the individuals who create new songs and musical pieces (such as songwriters and composers), individuals who perform music (which include orchestra, jazz band and rock band musicians, singers and conductors), individuals who record music (music producers and sound engineers), individuals who organize concert tours, and individuals who sell recordings, sheet music, and scores to customers. Even once a song or piece has been performed, music critics, music journalists, and music scholars may assess and evaluate the piece and its performance.

The word derives from Greek μουσική ("mousike"; "art of the Muses"). In Greek mythology, the nine Muses were the goddesses who inspired literature, science, and the arts and who were the source of the knowledge embodied in the poetry, song-lyrics, and myths in the Greek culture. According to the "Online Etymological Dictionary", the term "music" is derived from "mid-13c., musike, from Old French "musique" (12c.) and directly from Latin "musica" "the art of music," also including poetry (also [the] source of Spanish "música", Italian "musica", Old High German "mosica", German "Musik", Dutch "muziek", Danish "musik")." This is derived from the "...Greek "mousike (techne)" "(art) of the Muses," from fem. of mousikos "pertaining to the Muses," from Mousa "Muse" (see muse (n.)). Modern spelling [dates] from [the] 1630s. In classical Greece, [the term "music" refers to] any art in which the Muses presided, but especially music and lyric poetry."

Music is composed and performed for many purposes, ranging from aesthetic pleasure, religious or ceremonial purposes, or as an entertainment product for the marketplace. When music was only available through sheet music scores, such as during the Classical and Romantic eras, music lovers would buy the sheet music of their favourite pieces and songs so that they could perform them at home on the piano. With the advent of sound recording, records of popular songs, rather than sheet music became the dominant way that music lovers would enjoy their favourite songs. With the advent of home tape recorders in the 1980s and digital music in the 1990s, music lovers could make tapes or playlists of their favourite songs and take them with them on a portable cassette player or MP3 player. Some music lovers create mix tapes of their favorite songs, which serve as a "self-portrait, a gesture of friendship, prescription for an ideal party... [and] an environment consisting solely of what is most ardently loved."

Amateur musicians can compose or perform music for their own pleasure, and derive their income elsewhere. Professional musicians are employed by a range of institutions and organisations, including armed forces (in marching bands, concert bands and popular music groups), churches and synagogues, symphony orchestras, broadcasting or film production companies, and music schools. Professional musicians sometimes work as freelancers or session musicians, seeking contracts and engagements in a variety of settings. There are often many links between amateur and professional musicians. Beginning amateur musicians take lessons with professional musicians. In community settings, advanced amateur musicians perform with professional musicians in a variety of ensembles such as community concert bands and community orchestras.

A distinction is often made between music performed for a live audience and music that is performed in a studio so that it can be recorded and distributed through the music retail system or the broadcasting system. However, there are also many cases where a live performance in front of an audience is also recorded and distributed. Live concert recordings are popular in both classical music and in popular music forms such as rock, where illegally taped live concerts are prized by music lovers. In the jam band scene, live, improvised jam sessions are preferred to studio recordings.

"Composition" is the act or practice of creating a song, an instrumental music piece, a work with both singing and instruments, or another type of music. In many cultures, including Western classical music, the act of composing also includes the creation of music notation, such as a sheet music "score", which is then performed by the composer or by other singers or musicians. In popular music and traditional music, the act of composing, which is typically called songwriting, may involve the creation of a basic outline of the song, called the lead sheet, which sets out the melody, lyrics and chord progression. In classical music, the composer typically orchestrates his or her own compositions, but in musical theatre and in pop music, songwriters may hire an arranger to do the orchestration. In some cases, a songwriter may not use notation at all, and instead compose the song in her mind and then play or record it from memory. In jazz and popular music, notable recordings by influential performers are given the weight that written scores play in classical music.

Even when music is notated relatively precisely, as in classical music, there are many decisions that a performer has to make, because notation does not specify all of the elements of music precisely. The process of deciding how to perform music that has been previously composed and notated is termed "interpretation". Different performers' interpretations of the same work of music can vary widely, in terms of the tempos that are chosen and the playing or singing style or phrasing of the melodies. Composers and songwriters who present their own music are interpreting their songs, just as much as those who perform the music of others. The standard body of choices and techniques present at a given time and a given place is referred to as performance practice, whereas interpretation is generally used to mean the individual choices of a performer.

Although a musical composition often uses musical notation and has a single author, this is not always the case. A work of music can have multiple composers, which often occurs in popular music when a band collaborates to write a song, or in musical theatre, when one person writes the melodies, a second person writes the lyrics, and a third person orchestrates the songs. In some styles of music, such as the blues, a composer/songwriter may create, perform and record new songs or pieces without ever writing them down in music notation. A piece of music can also be composed with words, images, or computer programs that explain or notate how the singer or musician should create musical sounds. Examples range from avant-garde music that uses graphic notation, to text compositions such as "Aus den sieben Tagen", to computer programs that select sounds for musical pieces. Music that makes heavy use of randomness and chance is called aleatoric music, and is associated with contemporary composers active in the 20th century, such as John Cage, Morton Feldman, and Witold Lutosławski. A more commonly known example of chance-based music is the sound of wind chimes jingling in a breeze.

The study of composition has traditionally been dominated by examination of methods and practice of Western classical music, but the definition of composition is broad enough to include the creation of popular music and traditional music songs and instrumental pieces as well as spontaneously improvised works like those of free jazz performers and African percussionists such as Ewe drummers.

In the 2000s, music notation typically means the written expression of music notes and rhythms on paper using symbols. When music is written down, the pitches and rhythm of the music, such as the notes of a melody, are notated. Music notation also often provides instructions on how to perform the music. For example, the sheet music for a song may state that the song is a "slow blues" or a "fast swing", which indicates the tempo and the genre. To read music notation, a person must have an understanding of music theory, harmony and the performance practice associated with a particular song or piece's genre.

Written notation varies with style and period of music. In the 2000s, notated music is produced as sheet music or, for individuals with computer scorewriter programs, as an image on a computer screen. In ancient times, music notation was put onto stone or clay tablets. To perform music from notation, a singer or instrumentalist requires an understanding of the rhythmic and pitch elements embodied in the symbols and the performance practice that is associated with a piece of music or a genre. In genres requiring musical improvisation, the performer often plays from music where only the chord changes and form of the song are written, requiring the performer to have a great understanding of the music's structure, harmony and the styles of a particular genre (e.g., jazz or country music).

In Western art music, the most common types of written notation are scores, which include all the music parts of an ensemble piece, and parts, which are the music notation for the individual performers or singers. In popular music, jazz, and blues, the standard musical notation is the lead sheet, which notates the melody, chords, lyrics (if it is a vocal piece), and structure of the music. Fake books are also used in jazz; they may consist of lead sheets or simply chord charts, which permit rhythm section members to improvise an accompaniment part to jazz songs. Scores and parts are also used in popular music and jazz, particularly in large ensembles such as jazz "big bands." In popular music, guitarists and electric bass players often read music notated in tablature (often abbreviated as "tab"), which indicates the location of the notes to be played on the instrument using a diagram of the guitar or bass fingerboard. Tabulature was also used in the Baroque era to notate music for the lute, a stringed, fretted instrument.

Musical improvisation is the creation of spontaneous music, often within (or based on) a pre-existing harmonic framework or chord progression. Improvisers use the notes of the chord, various scales that are associated with each chord, and chromatic ornaments and passing tones which may be neither chord tones not from the typical scales associated with a chord. Imorovisay on a song can be done with or without preparation. Improvisation is a major part of some types of music, such as blues, jazz, and jazz fusion, in which instrumental performers improvise solos, melody lines and accompaniment parts. 

In the Western art music tradition, improvisation was an important skill during the Baroque era and during the Classical era. In the Baroque era, performers improvised ornaments and basso continuo keyboard players improvised chord voicings based on figured bass notation. As well, the top soloists were expected to be able to improvise pieces such as preludes. In the Classical era, solo performers and singers improvised virtuoso cadenzas during concerts.

However, in the 20th and early 21st century, as "common practice" Western art music performance became institutionalized in symphony orchestras, opera houses and ballets, improvisation has played a smaller role, as more and more music was notated in scores and parts for musicians to play. At the same time, some 20th and 21st century art music composers have increasingly included improvisation in their creative work. In Indian classical music, improvisation is a core component and an essential criterion of performances.

Music theory encompasses the nature and mechanics of music. It often involves identifying patterns that govern composers' techniques and examining the language and notation of music. In a grand sense, music theory distills and analyzes the parameters or elements of music – rhythm, harmony (harmonic function), melody, structure, form, and texture. Broadly, music theory may include any statement, belief, or conception of or about music. People who study these properties are known as music theorists, and they typically work as professors in colleges, universities, and music conservatories. Some have applied acoustics, human physiology, and psychology to the explanation of how and why music is perceived. Music theorists publish their research in music theory journals and university press books.

Music has many different fundamentals or elements. Depending on the definition of "element" being used, these can include: pitch, beat or pulse, tempo, rhythm, melody, harmony, texture, style, allocation of voices, timbre or color, dynamics, expression, articulation, form and structure. The elements of music feature prominently in the music curriculums of Australia, UK and US. All three curriculums identify pitch, dynamics, timbre and texture as elements, but the other identified elements of music are far from universally agreed. Below is a list of the three official versions of the "elements of music":

In relation to the UK curriculum, in 2013 the term: "appropriate musical notations" was added to their list of elements and the title of the list was changed from the "elements of music" to the "inter-related dimensions of music". The inter-related dimensions of music are listed as: pitch, duration, dynamics, tempo, timbre, texture, structure and appropriate musical notations.

The phrase "the elements of music" is used in a number of different contexts. The two most common contexts can be differentiated by describing them as the "rudimentary elements of music" and the "perceptual elements of music".

In the 1800s, the phrases "the elements of music" and "the rudiments of music" were used interchangeably. The elements described in these documents refer to aspects of music that are needed in order to become a musician, Recent writers such as Espie Estrella seem to be using the phrase "elements of music" in a similar manner. A definition which most accurately reflects this usage is: "the rudimentary principles of an art, science, etc.: the elements of grammar." The UK's curriculum switch to the "inter-related dimensions of music" seems to be a move back to using the rudimentary elements of music.

Since the emergence of the study of psychoacoustics in the 1930s, most lists of elements of music have related more to how we "hear" music than how we learn to play it or study it. C.E. Seashore, in his book "Psychology of Music", identified four "psychological attributes of sound". These were: "pitch, loudness, time, and timbre" (p. 3). He did not call them the "elements of music" but referred to them as "elemental components" (p. 2). Nonetheless these elemental components link precisely with four of the most common musical elements: "Pitch" and "timbre" match exactly, "loudness" links with dynamics and "time" links with the time-based elements of rhythm, duration and tempo. This usage of the phrase "the elements of music" links more closely with "Webster's New 20th Century Dictionary" definition of an element as: "a substance which cannot be divided into a simpler form by known methods" and educational institutions' lists of elements generally align with this definition as well.

Although writers of lists of "rudimentary elements of music" can vary their lists depending on their personal (or institutional) priorities, the perceptual elements of music should consist of an established (or proven) list of discrete elements which can be independently manipulated to achieve an intended musical effect. It seems at this stage that there is still research to be done in this area.

Some styles of music place an emphasis on certain of these fundamentals, while others place less emphasis on certain elements. To give one example, while Bebop-era jazz makes use of very complex chords, including altered dominants and challenging chord progressions, with chords changing two or more times per bar and keys changing several times in a tune, funk places most of its emphasis on rhythm and groove, with entire songs based around a vamp on a single chord. While Romantic era classical music from the mid- to late-1800s makes great use of dramatic changes of dynamics, from whispering pianissimo sections to thunderous fortissimo sections, some entire Baroque dance suites for harpsichord from the early 1700s may use a single dynamic. To give another example, while some art music pieces, such as symphonies are very long, some pop songs are just a few minutes long.

Pitch is an aspect of a sound that we can hear, reflecting whether one musical sound, note or tone is "higher" or "lower" than another musical sound, note or tone. We can talk about the highness or lowness of pitch in the more general sense, such as the way a listener hears a piercingly high piccolo note or whistling tone as higher in pitch than a deep thump of a bass drum. We also talk about pitch in the precise sense associated with musical melodies, basslines and chords. Precise pitch can only be determined in sounds that have a frequency that is clear and stable enough to distinguish from noise. For example, it is much easier for listeners to discern the pitch of a single note played on a piano than to try to discern the pitch of a crash cymbal that is struck.
A melody (also called a "tune") is a series of pitches (notes) sounding in succession (one after the other), often in a rising and falling pattern. The notes of a melody are typically created using pitch systems such as scales or modes. Melodies also often contain notes from the chords used in the song. The melodies in simple folk songs and traditional songs may use only the notes of a single scale, the scale associated with the tonic note or key of a given song. For example, a folk song in the key of C (also referred to as C major) may have a melody that uses only the notes of the C major scale (the individual notes C, D, E, F, G, A, B and C; these are the "white notes" on a piano keyboard. On the other hand, Bebop-era jazz from the 1940s and contemporary music from the 20th and 21st centuries may use melodies with many chromatic notes (i.e., notes in addition to the notes of the major scale; on a piano, a chromatic scale would include all the notes on the keyboard, including the "white notes" and "black notes" and unusual scales, such as the whole tone scale (a whole tone scale in the key of C would contain the notes C, D, E, F, G and A). A low, deep musical line played by bass instruments such as double bass, electric bass or tuba is called a bassline.

Harmony refers to the "vertical" sounds of pitches in music, which means pitches that are played or sung together at the same time to create a chord. Usually this means the notes are played at the same time, although harmony may also be implied by a melody that outlines a harmonic structure (i.e., by using melody notes that are played one after the other, outlining the notes of a chord). In music written using the system of major-minor tonality ("keys"), which includes most classical music written from 1600 to 1900 and most Western pop, rock and traditional music, the key of a piece determines the scale used, which centres around the "home note" or tonic of the key. Simple classical pieces and many pop and traditional music songs are written so that all the music is in a single key. More complex Classical, pop and traditional music songs and pieces may have two keys (and in some cases three or more keys). Classical music from the Romantic era (written from about 1820–1900) often contains multiple keys, as does jazz, especially Bebop jazz from the 1940s, in which the key or "home note" of a song may change every four bars or even every two bars.

Rhythm is the arrangement of sounds and silences in time. Meter animates time in regular pulse groupings, called measures or bars, which in Western classical, popular and traditional music often group notes in sets of two (e.g., 2/4 time), three (e.g., 3/4 time, also known as Waltz time, or 3/8 time), or four (e.g., 4/4 time). Meters are made easier to hear because songs and pieces often (but not always) place an emphasis on the first beat of each grouping. Notable exceptions exist, such as the backbeat used in much Western pop and rock, in which a song that uses a measure that consists of four beats (called 4/4 time or common time) will have accents on beats two and four, which are typically performed by the drummer on the snare drum, a loud and distinctive-sounding percussion instrument. In pop and rock, the rhythm parts of a song are played by the rhythm section, which includes chord-playing instruments (e.g., electric guitar, acoustic guitar, piano, or other keyboard instruments), a bass instrument (typically electric bass or for some styles such as jazz and bluegrass, double bass) and a drum kit player.

Musical texture is the overall sound of a piece of music or song. The texture of a piece or sing is determined by how the melodic, rhythmic, and harmonic materials are combined in a composition, thus determining the overall nature of the sound in a piece. Texture is often described in regard to the density, or thickness, and range, or width, between lowest and highest pitches, in relative terms as well as more specifically distinguished according to the number of voices, or parts, and the relationship between these voices (see common types below). For example, a thick texture contains many 'layers' of instruments. One of these layers could be a string section, or another brass. The thickness also is affected by the amount and the richness of the instruments. Texture is commonly described according to the number of and relationship between parts or lines of music:

Music that contains a large number of independent parts (e.g., a double concerto accompanied by 100 orchestral instruments with many interweaving melodic lines) is generally said to have a "thicker" or "denser" texture than a work with few parts (e.g., a solo flute melody accompanied by a single cello).

Timbre, sometimes called "color" or "tone color" is the quality or sound of a voice or instrument. Timbre is what makes a particular musical sound different from another, even when they have the same pitch and loudness. For example, a 440 Hz A note sounds different when it is played on oboe, piano, violin or electric guitar. Even if different players of the same instrument play the same note, their notes might sound different due to differences in instrumental technique (e.g., different embouchures), different types of accessories (e.g., mouthpieces for brass players, reeds for oboe and bassoon players) or strings made out of different materials for string players (e.g., gut strings versus steel strings). Even two instrumentalists playing the same note on the same instrument (one after the other) may sound different due to different ways of playing the instrument (e.g., two string players might hold the bow differently).

The physical characteristics of sound that determine the perception of timbre include the spectrum, envelope and overtones of a note or musical sound. For electric instruments developed in the 20th century, such as electric guitar, electric bass and electric piano, the performer can also change the tone by adjusting equalizer controls, tone controls on the instrument, and by using electronic effects units such as distortion pedals. The tone of the electric Hammond organ is controlled by adjusting drawbars.

Expressive qualities are those elements in music that create change in music without changing the main pitches or substantially changing the rhythms of the melody and its accompaniment. Performers, including singers and instrumentalists, can add musical expression to a song or piece by adding phrasing, by adding effects such as vibrato (with voice and some instruments, such as guitar, violin, brass instruments and woodwinds), dynamics (the loudness or softness of piece or a section of it), tempo fluctuations (e.g., ritardando or accelerando, which are, respectively slowing down and speeding up the tempo), by adding pauses or fermatas on a cadence, and by changing the articulation of the notes (e.g., making notes more pronounced or accented, by making notes more legato, which means smoothly connected, or by making notes shorter).

Expression is achieved through the manipulation of pitch (such as inflection, vibrato, slides etc.), volume (dynamics, accent, tremolo etc.), duration (tempo fluctuations, rhythmic changes, changing note duration such as with legato and staccato, etc.), timbre (e.g. changing vocal timbre from a light to a resonant voice) and sometimes even texture (e.g. doubling the bass note for a richer effect in a piano piece). Expression therefore can be seen as a manipulation of all elements in order to convey "an indication of mood, spirit, character etc." and as such cannot be included as a unique perceptual element of music, although it can be considered an important rudimentary element of music.

In music, form describes how the overall structure or plan of a song or piece of music, and it describes the layout of a composition as divided into sections. In the early 20th century, Tin Pan Alley songs and Broadway musical songs were often in AABA 32 bar form, in which the A sections repeated the same eight bar melody (with variation) and the B section provided a contrasting melody and/or harmony for 8 bars. From the 1960s onward, Western pop and rock songs are often in verse-chorus form, which is based around a sequence of verse and chorus ("refrain") sections, with new lyrics for most verses and repeating lyrics for the choruses. Popular music often makes use of strophic form, sometimes in conjunction with the twelve bar blues.

In the tenth edition of "The Oxford Companion to Music", Percy Scholes defines musical form as "a series of strategies designed to find a successful mean between the opposite extremes of unrelieved repetition and unrelieved alteration." Examples of common forms of Western music include the fugue, the invention, sonata-allegro, canon, strophic, theme and variations, and rondo.

Scholes states that European classical music had only six stand-alone forms: simple binary, simple ternary, compound binary, rondo, air with variations, and fugue (although musicologist Alfred Mann emphasized that the fugue is primarily a method of composition that has sometimes taken on certain structural conventions.)

Where a piece cannot readily be broken down into sectional units (though it might borrow some form from a poem, story or programme), it is said to be through-composed. Such is often the case with a fantasia, prelude, rhapsody, etude (or study), symphonic poem, Bagatelle, impromptu, etc. Professor Charles Keil classified forms and formal detail as "sectional, developmental, or variational."

This form is built from a sequence of clear-cut units that may be referred to by letters but also often have generic names such as introduction and coda, exposition, development and recapitulation, verse, chorus or refrain, and bridge. Introductions and codas, when they are no more than that, are frequently excluded from formal analysis. All such units may typically be eight measures long. Sectional forms include:

This form is defined by its "unrelieved repetition" (AAAA...).


Medley, potpourri is the extreme opposite, that of "unrelieved variation": it is simply an indefinite sequence of self-contained sections (ABCD...), sometimes with repeats (AABBCCDD...). Examples include orchestral overtures, which are sometimes no more than a string of the best tunes of the musical theatre show or opera to come.

This form uses two sections (AB...), each often repeated (AABB...). In 18th-century Western classical music, "simple binary" form was often used for dances and carried with it the convention that the two sections should be in different musical keys but same rhythm, duration and tone. The alternation of two tunes gives enough variety to permit a dance to be extended for as long as desired.
b
This form has three parts. In Western classical music a simple ternary form has a third section that is a recapitulation of the first (ABA). Often, the first section is repeated (AABA). This approach was popular in the 18th-century operatic aria, and was called "da capo" (i.e. "repeat from the top") form. Later, it gave rise to the 32-bar song, with the B section then often referred to as the "middle eight". A song has more need than a dance of a self-contained form with a beginning and an end of course.

This form has a recurring theme alternating with different (usually contrasting) sections called "episodes". It may be asymmetrical (ABACADAEA) or symmetrical (ABACABA). A recurring section, especially the main theme, is sometimes more thoroughly varied, or else one episode may be a "development" of it. A similar arrangement is the ritornello form of the Baroque concerto grosso. Arch form (ABCBA) resembles a symmetrical rondo without intermediate repetitions of the main theme. It is normally used in a round.

Variational forms are those in which variation is an important formative element.

Theme and Variations: a theme, which in itself can be of any shorter form (binary, ternary, etc.), forms the only "section" and is repeated indefinitely (as in strophic form) but is varied each time (A, B, A, F, Z, A), so as to make a sort of sectional chain form. An important variant of this, much used in 17th-century British music and in the Passacaglia and Chaconne, was that of the ground bass – a repeating bass theme or "basso ostinato" over and around which the rest of the structure unfolds, often, but not always, spinning polyphonic or contrapuntal threads, or improvising divisions and descants. This is said by Scholes (1977) to be the form "par excellence" of unaccompanied or accompanied solo instrumental music. The Rondo is often found with sections varied (AABACABA) or (ABACABA).

Developmental forms are built directly from smaller units, such as motifs. A well-known Classical piece with a motif is Beethoven's fifth symphony, which starts with three short repeated notes and then a long note. In Classical pieces that are based on motifs, the motif is usually combined, varied and worked out in different ways, perhaps having a symmetrical or arch-like underpinning and a progressive development from beginning to end. By far the most important developmental form in Western classical music is Sonata form.

This form, also known as "sonata" form, first movement form, compound binary, ternary and a variety of other names, developed from the binary-formed dance movement described above but is almost always cast in a greater ternary form having the nominal subdivisions of "Exposition, Development" and "Recapitulation". Usually, but not always, the "A" parts (Exposition and Recapitulation, respectively) may be subdivided into two or three themes or theme groups which are taken asunder and recombined to form the "B" part (the development) – thus e. g. (AabB[dev. of a and/or b]Aab+coda). This developmental form is generally confined to certain sections of the piece, as to the middle section of the first movement of a sonata, though 19th-century composers such as Berlioz, Liszt and Wagner made valiant efforts to derive large-scale works purely or mainly from the motif.

Prehistoric music can only be theorized based on findings from paleolithic archaeology sites. Flutes are often discovered, carved from bones in which lateral holes have been pierced; these are thought to have been blown at one end like the Japanese shakuhachi. The Divje Babe flute, carved from a cave bear femur, is thought to be at least 40,000 years old. Instruments such as the seven-holed flute and various types of stringed instruments, such as the Ravanahatha, have been recovered from the Indus Valley Civilization archaeological sites. India has one of the oldest musical traditions in the world—references to Indian classical music ("marga") are found in the Vedas, ancient scriptures of the Hindu tradition. The earliest and largest collection of prehistoric musical instruments was found in China and dates back to between 7000 and 6600 BC. The "Hurrian Hymn to Nikkal", found on clay tablets that date back to approximately 1400 BC, is the oldest surviving notated work of music.

The ancient Egyptians credited one of their gods, Thoth, with the invention of music, with Osiris in turn used as part of his effort to civilize the world. The earliest material and representational evidence of Egyptian musical instruments dates to the Predynastic period, but the evidence is more securely attested in the Old Kingdom when harps, flutes and double clarinets were played. Percussion instruments, lyres and lutes were added to orchestras by the Middle Kingdom. Cymbals frequently accompanied music and dance, much as they still do in Egypt today. Egyptian folk music, including the traditional Sufi "dhikr" rituals, are the closest contemporary music genre to ancient Egyptian music, having preserved many of its features, rhythms and instruments.

Indian classical music is one of the oldest musical traditions in the world. The Indus Valley civilization has sculptures that show dance and old musical instruments, like the seven holed flute. Various types of stringed instruments and drums have been recovered from Harappa and Mohenjo Daro by excavations carried out by Sir Mortimer Wheeler. The Rigveda has elements of present Indian music, with a musical notation to denote the metre and the mode of chanting. Indian classical music (marga) is monophonic, and based on a single melody line or raga rhythmically organized through talas. "Silappadhikaram" by Ilango Adigal provides information about how new scales can be formed by modal shifting of the tonic from an existing scale. Hindi music was influenced by the Persian performance practices of the Afghan Mughals. Carnatic music, popular in the southern states, is largely devotional; the majority of the songs are addressed to the Hindu deities. There are also many songs emphasising love and other social issues.

Asian music covers the music cultures of Arabia, Central Asia, East Asia, South Asia, and Southeast Asia. Chinese classical music, the traditional art or court music of China, has a history stretching over around three thousand years. It has its own unique systems of musical notation, as well as musical tuning and pitch, musical instruments and styles or musical genres. Chinese music is pentatonic-diatonic, having a scale of twelve notes to an octave (5 + 7 = 12) as does European-influenced music. Persian music is the music of Persia and Persian language countries: "musiqi", the science and art of music, and "muzik", the sound and performance of music (Sakata 1983).

Knowledge of the biblical period is mostly from literary references in the Bible and post-biblical sources. Religion and music historian Herbert Lockyer, Jr. writes that "music, both vocal and instrumental, was well cultivated among the Hebrews, the New Testament Christians, and the Christian church through the centuries." He adds that "a look at the Old Testament reveals how God's ancient people were devoted to the study and practice of music, which holds a unique place in the historical and prophetic books, as well as the Psalter."

Music and theatre scholars studying the history and anthropology of Semitic and early Judeo-Christian culture have discovered common links in theatrical and musical activity between the classical cultures of the Hebrews and those of later Greeks and Romans. The common area of performance is found in a "social phenomenon called litany," a form of prayer consisting of a series of invocations or supplications. The "Journal of Religion and Theatre" notes that among the earliest forms of litany, "Hebrew litany was accompanied by a rich musical tradition:"

Genesis 4.21 indicated that Jubal is the "father of all such as handle the harp and pipe", the Pentateuch is nearly silent about the practice and instruction of music in the early life of Israel". In I Samuel 10, there are more depictions of "large choirs and orchestras". These large ensembles could only be run with extensive rehearsals. This had led some scholars to theorize that the prophet Samuel led a public music school to a wide range of students.

Music was an important part of social and cultural life in ancient Greece. Musicians and singers played a prominent role in Greek theater. Mixed-gender choruses performed for entertainment, celebration, and spiritual ceremonies. Instruments included the double-reed "aulos" and a plucked string instrument, the "lyre", principally the special kind called a "kithara". Music was an important part of education, and boys were taught music starting at age six. Greek musical literacy created a flowering of music development. Greek music theory included the Greek musical modes, that eventually became the basis for Western religious and classical music. Later, influences from the Roman Empire, Eastern Europe, and the Byzantine Empire changed Greek music. The Seikilos epitaph is the oldest surviving example of a complete musical composition, including musical notation, from anywhere in the world. The oldest surviving work written on the subject of music theory is "Harmonika Stoicheia" by Aristoxenus.

The medieval era (476 to 1400), which took place during the Middle Ages, started with the introduction of monophonic (single melodic line) chanting into Roman Catholic Church services. Musical notation was used since Ancient times in Greek culture, but in the Middle Ages, notation was first introduced by the Catholic church so that the chant melodies could be written down, to facilitate the use of the same melodies for religious music across the entire Catholic empire. The only European Medieval repertory that has been found in written form from before 800 is the monophonic liturgical plainsong chant of the Roman Catholic Church, the central tradition of which was called Gregorian chant. Alongside these traditions of sacred and church music there existed a vibrant tradition of secular song (non-religious songs). Examples of composers from this period are Léonin, Pérotin, Guillaume de Machaut, and Walther von der Vogelweide.

Renaissance music (c. 1400 to 1600) was more focused on secular (non-religious) themes, such as courtly love. Around 1450, the printing press was invented, which made printed sheet music much less expensive and easier to mass-produce (prior to the invention of the printing press, all notated music was hand-copied). The increased availability of sheet music helped to spread musical styles more quickly and across a larger area. Musicians and singers often worked for the church, courts and towns. Church choirs grew in size, and the church remained an important patron of music. By the middle of the 15th century, composers wrote richly polyphonic sacred music, in which different melody lines were interwoven simultaneously. Prominent composers from this era include Guillaume Dufay, Giovanni Pierluigi da Palestrina, Thomas Morley, and Orlande de Lassus. As musical activity shifted from the church to the aristocratic courts, kings, queens and princes competed for the finest composers. Many leading important composers came from the Netherlands, Belgium, and northern France. They are called the Franco-Flemish composers. They held important positions throughout Europe, especially in Italy. Other countries with vibrant musical activity included Germany, England, and Spain.

The Baroque era of music took place from 1600 to 1750, as the Baroque artistic style flourished across Europe; and during this time, music expanded in its range and complexity. Baroque music began when the first operas (dramatic solo vocal music accompanied by orchestra) were written. During the Baroque era, polyphonic contrapuntal music, in which multiple, simultaneous independent melody lines were used, remained important (counterpoint was important in the vocal music of the Medieval era). German Baroque composers wrote for small ensembles including strings, brass, and woodwinds, as well as for choirs and keyboard instruments such as pipe organ, harpsichord, and clavichord. During this period several major music forms were defined that lasted into later periods when they were expanded and evolved further, including the fugue, the invention, the sonata, and the concerto. The late Baroque style was polyphonically complex and richly ornamented. Important composers from the Baroque era include Johann Sebastian Bach ("Cello suites"), George Frideric Handel ("Messiah"), Georg Philipp Telemann and Antonio Lucio Vivaldi ("The Four Seasons").

The music of the Classical period (1730 to 1820) aimed to imitate what were seen as the key elements of the art and philosophy of Ancient Greece and Rome: the ideals of balance, proportion and disciplined expression. (Note: the music from the Classical period should not be confused with Classical music in general, a term which refers to Western art music from the 5th century to the 2000s, which includes the Classical period as one of a number of periods). Music from the Classical period has a lighter, clearer and considerably simpler texture than the Baroque music which preceded it. The main style was homophony, where a prominent melody and a subordinate chordal accompaniment part are clearly distinct. Classical instrumental melodies tended to be almost voicelike and singable. New genres were developed, and the fortepiano, the forerunner to the modern piano, replaced the Baroque era harpsichord and pipe organ as the main keyboard instrument (though pipe organ continued to be used in sacred music, such as Masses).

Importance was given to instrumental music. It was dominated by further development of musical forms initially defined in the Baroque period: the sonata, the concerto, and the symphony. Others main kinds were the trio, string quartet, serenade and divertimento. The sonata was the most important and developed form. Although Baroque composers also wrote sonatas, the Classical style of sonata is completely distinct. All of the main instrumental forms of the Classical era, from string quartets to symphonies and concertos, were based on the structure of the sonata. The instruments used chamber music and orchestra became more standardized. In place of the basso continuo group of the Baroque era, which consisted of harpsichord, organ or lute along with a number of bass instruments selected at the discretion of the group leader (e.g., viol, cello, theorbo, serpent), Classical chamber groups used specified, standardized instruments (e.g., a string quartet would be performed by two violins, a viola and a cello). The Baroque era improvised chord-playing of the continuo keyboardist or lute player was gradually phased out between 1750 and 1800.

One of the most important changes made in the Classical period was the development of public concerts. The aristocracy still played a significant role in the sponsorship of concerts and compositions, but it was now possible for composers to survive without being permanent employees of queens or princes. The increasing popularity of classical music led to a growth in the number and types of orchestras. The expansion of orchestral concerts necessitated the building of large public performance spaces. Symphonic music including symphonies, musical accompaniment to ballet and mixed vocal/instrumental genres such as opera and oratorio became more popular.

The best known composers of Classicism are Carl Philipp Emanuel Bach, Christoph Willibald Gluck, Johann Christian Bach, Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven and Franz Schubert. Beethoven and Schubert are also considered to be composers in the later part of the Classical era, as it began to move towards Romanticism.

Romantic music (c. 1810 to 1900) from the 19th century had many elements in common with the Romantic styles in literature and painting of the era. Romanticism was an artistic, literary, and intellectual movement was characterized by its emphasis on emotion and individualism as well as glorification of all the past and nature. Romantic music expanded beyond the rigid styles and forms of the Classical era into more passionate, dramatic expressive pieces and songs. Romantic composers such as Wagner and Brahms attempted to increase emotional expression and power in their music to describe deeper truths or human feelings. With symphonic tone poems, composers tried to tell stories and evoke images or landscapes using instrumental music. Some composers promoted nationalistic pride with patriotic orchestral music inspired by folk music. The emotional and expressive qualities of music came to take precedence over tradition.

Romantic composers grew in idiosyncrasy, and went further in the syncretism of exploring different art-forms in a musical context, (such as literature), history (historical figures and legends), or nature itself. Romantic love or longing was a prevalent theme in many works composed during this period. In some cases the formal structures from the classical period continued to be used (e.g., the sonata form used in string quartets and symphonies), but these forms were expanded and altered. In many cases, new approaches were explored for existing genres, forms, and functions. Also, new forms were created that were deemed better suited to the new subject matter. Composers continued to develop opera and ballet music, exploring new styles and themes.

In the years after 1800, the music developed by Ludwig van Beethoven and Franz Schubert introduced a more dramatic, expressive style. In Beethoven's case, short motifs, developed organically, came to replace melody as the most significant compositional unit (an example is the distinctive four note figure used in his Fifth Symphony). Later Romantic composers such as Pyotr Ilyich Tchaikovsky, Antonín Dvořák, and Gustav Mahler used more unusual chords and more dissonance to create dramatic tension. They generated complex and often much longer musical works. During the late Romantic period, composers explored dramatic chromatic alterations of tonality, such as extended chords and altered chords, which created new sound "colours". The late 19th century saw a dramatic expansion in the size of the orchestra, and the industrial revolution helped to create better instruments, creating a more powerful sound. Public concerts became an important part of well-to-do urban society. It also saw a new diversity in theatre music, including operetta, and musical comedy and other forms of musical theatre.

In the 19th century, one of the key ways that new compositions became known to the public was by the sales of sheet music, which middle class amateur music lovers would perform at home on their piano or other common instruments, such as violin. With 20th-century music, the invention of new electric technologies such as radio broadcasting and the mass market availability of gramophone records meant that sound recordings of songs and pieces heard by listeners (either on the radio or on their record player) became the main way to learn about new songs and pieces. There was a vast increase in music listening as the radio gained popularity and phonographs were used to replay and distribute music, because whereas in the 19th century, the focus on sheet music restricted access to new music to the middle class and upper-class people who could read music and who owned pianos and instruments, in the 20th century, anyone with a radio or record player could hear operas, symphonies and big bands right in their own living room. This allowed lower-income people, who would never be able to afford an opera or symphony concert ticket to hear this music. It also meant that people could hear music from different parts of the country, or even different parts of the world, even if they could not afford to travel to these locations. This helped to spread musical styles.

The focus of art music in the 20th century was characterized by exploration of new rhythms, styles, and sounds. The horrors of World War I influenced many of the arts, including music, and some composers began exploring darker, harsher sounds. Traditional music styles such as jazz and folk music were used by composers as a source of ideas for classical music. Igor Stravinsky, Arnold Schoenberg, and John Cage were all influential composers in 20th-century art music. The invention of sound recording and the ability to edit music gave rise to new subgenre of classical music, including the acousmatic and Musique concrète schools of electronic composition. Sound recording was also a major influence on the development of popular music genres, because it enabled recordings of songs and bands to be widely distributed. The introduction of the multitrack recording system had a major influence on rock music, because it could do much more than record a band's performance. Using a multitrack system, a band and their music producer could overdub many layers of instrument tracks and vocals, creating new sounds that would not be possible in a live performance.

Jazz evolved and became an important genre of music over the course of the 20th century, and during the second half of that century, rock music did the same. Jazz is an American musical artform that originated in the beginning of the 20th century in African American communities in the Southern United States from a confluence of African and European music traditions. The style's West African pedigree is evident in its use of blue notes, improvisation, polyrhythms, syncopation, and the swung note.

Rock music is a genre of popular music that developed in the 1960s from 1950s rock and roll, rockabilly, blues, and country music. The sound of rock often revolves around the electric guitar or acoustic guitar, and it uses a strong back beat laid down by a rhythm section. Along with the guitar or keyboards, saxophone and blues-style harmonica are used as soloing instruments. In its "purest form", it "has three chords, a strong, insistent back beat, and a catchy melody". The traditional rhythm section for popular music is rhythm guitar, electric bass guitar, drums. Some bands also have keyboard instruments such as organ, piano, or, since the 1970s, analog synthesizers. In the 1980s, pop musicians began using digital synthesizers, such as the DX-7 synthesizer, electronic drum machines such as the TR-808 and synth bass devices (such as the TB-303) or synth bass keyboards. In the 1990s, an increasingly large range of computerized hardware musical devices and instruments and software (e.g., digital audio workstations) were used. In the 2020s, soft synths and computer music apps make it possible for bedroom producers to create and record some types of music, such as electronic dance music in their own home, adding sampled and digital instruments and editing the recording digitally. In the 1990s, some bands in genres such as nu metal began including DJs in their bands. DJs create music by manipulating recorded music on record players or CD players, using a DJ mixer.

Performance is the physical expression of music, which occurs when a song is sung or when a piano piece, electric guitar melody, symphony, drum beat or other musical part is played by musicians. In classical music, a musical work is written in music notation by a composer and then it is performed once the composer is satisfied with its structure and instrumentation. However, as it gets performed, the interpretation of a song or piece can evolve and change. In classical music, instrumental performers, singers or conductors may gradually make changes to the phrasing or tempo of a piece. In popular and traditional music, the performers have a lot more freedom to make changes to the form of a song or piece. As such, in popular and traditional music styles, even when a band plays a cover song, they can make changes to it such as adding a guitar solo to or inserting an introduction.

A performance can either be planned out and rehearsed (practiced)—which is the norm in classical music, with jazz big bands and many popular music styles–or improvised over a chord progression (a sequence of chords), which is the norm in small jazz and blues groups. Rehearsals of orchestras, concert bands and choirs are led by a conductor. Rock, blues and jazz bands are usually led by the bandleader. A rehearsal is a structured repetition of a song or piece by the performers until it can be sung and/or played correctly and, if it is a song or piece for more than one musician, until the parts are together from a rhythmic and tuning perspective. Improvisation is the creation of a musical idea–a melody or other musical line–created on the spot, often based on scales or pre-existing melodic riffs.

Many cultures have strong traditions of solo performance (in which one singer or instrumentalist performs), such as in Indian classical music, and in the Western art-music tradition. Other cultures, such as in Bali, include strong traditions of group performance. All cultures include a mixture of both, and performance may range from improvised solo playing to highly planned and organised performances such as the modern classical concert, religious processions, classical music festivals or music competitions. Chamber music, which is music for a small ensemble with only a few of each type of instrument, is often seen as more intimate than large symphonic works.

Many types of music, such as traditional blues and folk music were not written down in sheet music; instead, they were originally preserved in the memory of performers, and the songs were handed down orally, from one musician or singer to another, or aurally, in which a performer learns a song "by ear". When the composer of a song or piece is no longer known, this music is often classified as "traditional" or as a "folk song". Different musical traditions have different attitudes towards how and where to make changes to the original source material, from quite strict, to those that demand improvisation or modification to the music. A culture's history and stories may also be passed on by ear through song.

In music, an "ornament" consists of added notes that provide decoration to a melody, bassline or other musical part. The detail included explicitly in the music notation varies between genres and historical periods. In general, art music notation from the 17th through the 19th centuries required performers to have a great deal of contextual knowledge about performing styles. For example, in the 17th and 18th centuries, music notated for solo performers typically indicated a simple, unadorned melody. Performers were expected to know how to add stylistically appropriate ornaments to add interest to the music, such as trills and turns. Different styles of music use different ornaments. A Baroque flute player might add mordents, which are short notes that are played before the main melody note, either above or below the main melody note. A blues guitarist playing electric guitar might use string bending to add expression; a heavy metal guitar player might use hammer-ons and pull-offs.

In the 19th century, art music for solo performers may give a general instruction such as to perform the music expressively, without describing in detail how the performer should do this. The performer was expected to know how to use tempo changes, accentuation, and pauses (among other devices) to obtain this "expressive" performance style. In the 20th century, art music notation often became more explicit and used a range of markings and annotations to indicate to performers how they should play or sing the piece. In popular music and traditional music styles, performers are expected to know what types of ornaments are stylistically appropriate for a given song or piece, and performers typically add them in an improvised fashion. One exception is note-for-note solos, in which some players precisely recreate a famous version of a solo, such as a guitar solo.

Philosophy of music is a subfield of philosophy. The philosophy of music is the study of fundamental questions regarding music. The philosophical study of music has many connections with philosophical questions in metaphysics and aesthetics.
Some basic questions in the philosophy of music are:


In ancient times, such as with the Ancient Greeks, the aesthetics of music explored the mathematical and cosmological dimensions of rhythmic and harmonic organization. In the 18th century, focus shifted to the experience of hearing music, and thus to questions about its beauty and human enjoyment ("plaisir" and "jouissance") of music. The origin of this philosophic shift is sometimes attributed to Alexander Gottlieb Baumgarten in the 18th century, followed by Immanuel Kant. Through their writing, the ancient term 'aesthetics', meaning sensory perception, received its present-day connotation. In the 2000s, philosophers have tended to emphasize issues besides beauty and enjoyment. For example, music's capacity to express emotion has been a central issue.

In the 20th century, important contributions were made by Peter Kivy, Jerrold Levinson, Roger Scruton, and Stephen Davies. However, many musicians, music critics, and other non-philosophers have contributed to the aesthetics of music. In the 19th century, a significant debate arose between Eduard Hanslick, a music critic and musicologist, and composer Richard Wagner regarding whether music can express meaning. Harry Partch and some other musicologists, such as Kyle Gann, have studied and tried to popularize microtonal music and the usage of alternate musical scales. Also many modern composers like La Monte Young, Rhys Chatham and Glenn Branca paid much attention to a scale called just intonation.

It is often thought that music has the ability to affect our emotions, intellect, and psychology; it can assuage our loneliness or incite our passions. The philosopher Plato suggests in "The Republic" that music has a direct effect on the soul. Therefore, he proposes that in the ideal regime music would be closely regulated by the state (Book VII).

There has been a strong tendency in the aesthetics of music to emphasize the paramount importance of compositional structure; however, other issues concerning the aesthetics of music include lyricism, harmony, hypnotism, emotiveness, temporal dynamics, resonance, playfulness, and color (see also musical development).

Modern music psychology aims to explain and understand musical behavior and experience. Research in this field and its subfields are primarily empirical; their knowledge tends to advance on the basis of interpretations of data collected by systematic observation of and interaction with human participants. In addition to its focus on fundamental perceptions and cognitive processes, music psychology is a field of research with practical relevance for many areas, including music performance, composition, education, criticism, and therapy, as well as investigations of human aptitude, skill, intelligence, creativity, and social behavior.

Cognitive neuroscience of music is the scientific study of brain-based mechanisms involved in the cognitive processes underlying music. These behaviours include music listening, performing, composing, reading, writing, and ancillary activities. It also is increasingly concerned with the brain basis for musical aesthetics and musical emotion. The field is distinguished by its reliance on direct observations of the brain, using such techniques as functional magnetic resonance imaging (fMRI), transcranial magnetic stimulation (TMS), magnetoencephalography (MEG), electroencephalography (EEG), and positron emission tomography (PET).

Cognitive musicology is a branch of cognitive science concerned with computationally modeling musical knowledge with the goal of understanding both music and cognition. The use of computer models provides an exacting, interactive medium in which to formulate and test theories and has roots in artificial intelligence and cognitive science.

This interdisciplinary field investigates topics such as the parallels between language and music in the brain. Biologically inspired models of computation are often included in research, such as neural networks and evolutionary programs. This field seeks to model how musical knowledge is represented, stored, perceived, performed, and generated. By using a well-structured computer environment, the systematic structures of these cognitive phenomena can be investigated.

Psychoacoustics is the scientific study of sound perception. More specifically, it is the branch of science studying the psychological and physiological responses associated with sound (including speech and music). It can be further categorized as a branch of psychophysics.

Evolutionary musicology concerns the "origins of music, the question of animal song, selection pressures underlying music evolution", and "music evolution and human evolution". It seeks to understand music perception and activity in the context of evolutionary theory. Charles Darwin speculated that music may have held an adaptive advantage and functioned as a protolanguage, a view which has spawned several competing theories of music evolution. An alternate view sees music as a by-product of linguistic evolution; a type of "auditory cheesecake" that pleases the senses without providing any adaptive function. This view has been directly countered by numerous music researchers.

An individual's culture or ethnicity plays a role in their music cognition, including their preferences, emotional reaction, and musical memory. Musical preferences are biased toward culturally familiar musical traditions beginning in infancy, and adults' classification of the emotion of a musical piece depends on both culturally specific and universal structural features. Additionally, individuals' musical memory abilities are greater for culturally familiar music than for culturally unfamiliar music.

Many ethnographic studies demonstrate that music is a participatory, community-based activity. Music is experienced by individuals in a range of social settings ranging from being alone to attending a large concert, forming a music community, which cannot be understood as a function of individual will or accident; it includes both commercial and non-commercial participants with a shared set of common values. Musical performances take different forms in different cultures and socioeconomic milieus. In Europe and North America, there is often a divide between what types of music are viewed as a "high culture" and "low culture." "High culture" types of music typically include Western art music such as Baroque, Classical, Romantic, and modern-era symphonies, concertos, and solo works, and are typically heard in formal concerts in concert halls and churches, with the audience sitting quietly in seats.

Other types of music—including, but not limited to, jazz, blues, soul, and country—are often performed in bars, nightclubs, and theatres, where the audience may be able to drink, dance, and express themselves by cheering. Until the later 20th century, the division between "high" and "low" musical forms was widely accepted as a valid distinction that separated out better quality, more advanced "art music" from the popular styles of music heard in bars and dance halls.

However, in the 1980s and 1990s, musicologists studying this perceived divide between "high" and "low" musical genres argued that this distinction is not based on the musical value or quality of the different types of music. Rather, they argued that this distinction was based largely on the socioeconomics standing or social class of the performers or audience of the different types of music. For example, whereas the audience for Classical symphony concerts typically have above-average incomes, the audience for a rap concert in an inner-city area may have below-average incomes. Even though the performers, audience, or venue where non-"art" music is performed may have a lower socioeconomic status, the music that is performed, such as blues, rap, punk, funk, or ska may be very complex and sophisticated.

When composers introduce styles of music that break with convention, there can be a strong resistance from academic music experts and popular culture. Late-period Beethoven string quartets, Stravinsky ballet scores, serialism, bebop-era jazz, hip hop, punk rock, and electronica have all been considered non-music by some critics when they were first introduced. Such themes are examined in the sociology of music. The sociological study of music, sometimes called sociomusicology, is often pursued in departments of sociology, media studies, or music, and is closely related to the field of ethnomusicology.

Women have played a major role in music throughout history, as composers, songwriters, instrumental performers, singers, conductors, music scholars, music educators, music critics/music journalists and other musical professions. As well, it describes music movements, events and genres related to women, women's issues and feminism. In the 2010s, while women comprise a significant proportion of popular music and classical music singers, and a significant proportion of songwriters (many of them being singer-songwriters), there are few women record producers, rock critics and rock instrumentalists. Although there have been a huge number of women composers in classical music, from the Medieval period to the present day, women composers are significantly underrepresented in the commonly performed classical music repertoire, music history textbooks and music encyclopedias; for example, in the "Concise Oxford History of Music", Clara Schumann is one of the only female composers who is mentioned.

Women comprise a significant proportion of instrumental soloists in classical music and the percentage of women in orchestras is increasing. A 2015 article on concerto soloists in major Canadian orchestras, however, indicated that 84% of the soloists with the Orchestre Symphonique de Montreal were men. In 2012, women still made up just 6% of the top-ranked Vienna Philharmonic orchestra. Women are less common as instrumental players in popular music genres such as rock and heavy metal, although there have been a number of notable female instrumentalists and all-female bands. Women are particularly underrepresented in extreme metal genres. In the 1960s pop-music scene, "[l]ike most aspects of the...music business, [in the 1960s,] songwriting was a male-dominated field. Though there were plenty of female singers on the radio, women ...were primarily seen as consumers:... Singing was sometimes an acceptable pastime for a girl, but playing an instrument, writing songs, or producing records simply wasn't done." Young women "...were not socialized to see themselves as people who create [music]."

Women are also underrepresented in orchestral conducting, music criticism/music journalism, music producing, and sound engineering. While women were discouraged from composing in the 19th century, and there are few women musicologists, women became involved in music education "...to such a degree that women dominated [this field] during the later half of the 19th century and well into the 20th century."

According to Jessica Duchen, a music writer for London's "The Independent", women musicians in classical music are "...too often judged for their appearances, rather than their talent" and they face pressure "...to look sexy onstage and in photos." Duchen states that while "[t]here are women musicians who refuse to play on their looks...the ones who do tend to be more materially successful."
According to the UK's Radio 3 editor, Edwina Wolstencroft, the music industry has long been open to having women in performance or entertainment roles, but women are much less likely to have positions of authority, such as being the leader of an orchestra. In popular music, while there are many women singers recording songs, there are very few women behind the audio console acting as music producers, the individuals who direct and manage the recording process. One of the most recorded artists is Asha Bhosle, an Indian singer best known as a playback singer in Hindi cinema.

The music that composers and songwriters make can be heard through several media; the most traditional way is to hear it live, in the presence of the musicians (or as one of the musicians), in an outdoor or indoor space such as an amphitheatre, concert hall, cabaret room, theatre, pub, or coffeehouse. Since the 20th century, live music can also be broadcast over the radio, television or the Internet, or recorded and listened to on a CD player or Mp3 player. 

Some musical styles focus on producing songs and pieces for a live performance, while others focus on producing a recording that mixes together sounds that were never played "live." Even in essentially live styles such as rock, recording engineers often use the ability to edit, splice and mix to produce recordings that may be considered "better" than the actual live performance. For example, some singers record themselves singing a melody and then record multiple harmony parts using overdubbing, creating a sound that would be impossible to do live.

Technology has had an influence on music since prehistoric times, when cave people used simple tools to bore holes into bone flutes 41,000 years ago. Technology continued to influence music throughout the history of music, as it enabled new instruments and music notation reproduction systems to be used, with one of the watershed moments in music notation being the invention of the printing press in the 1400s, which meant music scores no longer had to be hand copied. In the 19th century, music technology led to the development of a more powerful, louder piano and led to the development of new valves brass instruments. 

In the early 20th century (in the late 1920s), as talking pictures emerged in the early 20th century, with their prerecorded musical tracks, an increasing number of moviehouse orchestra musicians found themselves out of work. During the 1920s, live musical performances by orchestras, pianists, and theater organists were common at first-run theaters. With the coming of the talking motion pictures, those featured performances were largely eliminated. The American Federation of Musicians (AFM) took out newspaper advertisements protesting the replacement of live musicians with mechanical playing devices. One 1929 ad that appeared in the "Pittsburgh Press" features an image of a can labeled "Canned Music / Big Noise Brand / Guaranteed to Produce No Intellectual or Emotional Reaction Whatever"

Since legislation introduced to help protect performers, composers, publishers and producers, including the Audio Home Recording Act of 1992 in the United States, and the 1979 revised Berne Convention for the Protection of Literary and Artistic Works in the United Kingdom, recordings and live performances have also become more accessible through computers, devices and Internet in a form that is commonly known as Music-On-Demand.

In many cultures, there is less distinction between performing and listening to music, since virtually everyone is involved in some sort of musical activity, often in a communal setting. In industrialized countries, listening to music through a recorded form, such as sound recording on record or radio became more common than experiencing live performance, roughly in the middle of the 20th century. By the 1980s, watching a music videos was a popular way to listen to music, while also seeing the performers.

Sometimes, live performances incorporate prerecorded sounds. For example, a disc jockey uses disc records for scratching, and some 20th-century works have a solo for an instrument or voice that is performed along with music that is prerecorded onto a tape. Some pop bands use recorded backing tracks. Computers and many keyboards can be programmed to produce and play Musical Instrument Digital Interface (MIDI) music. Audiences can also "become" performers by participating in karaoke, an activity of Japanese origin centered on a device that plays voice-eliminated versions of well-known songs. Most karaoke machines also have video screens that show lyrics to songs being performed; performers can follow the lyrics as they sing over the instrumental tracks.

The advent of the Internet and widespread high-speed broadband access has transformed the experience of music, partly through the increased ease of access to recordings of music via streaming video and vastly increased choice of music for consumers. Chris Anderson, in his book "", suggests that while the traditional economic model of supply and demand describes scarcity, the Internet retail model is based on abundance. Digital storage costs are low, so a company can afford to make its whole recording inventory available online, giving customers as much choice as possible. It has thus become economically viable to offer music recordings that very few people are interested in. Consumers' growing awareness of their increased choice results in a closer association between listening tastes and social identity, and the creation of thousands of niche markets.

Another effect of the Internet arose with online communities and social media websites like YouTube and Facebook, a social networking service. These sites make it easier for aspiring singers and amateur bands to distribute videos of their songs, connect with other musicians, and gain audience interest. Professional musicians also use YouTube as a free publisher of promotional material. YouTube users, for example, no longer only download and listen to MP3s, but also actively create their own. According to Don Tapscott and Anthony D. Williams, in their book "Wikinomics", there has been a shift from a traditional consumer role to what they call a "prosumer" role, a consumer who both creates content and consumes. Manifestations of this in music include the production of mashes, remixes, and music videos by fans.

The music industry refers to the businesses connected with the creation and sale of music. It consists of songwriters and composers who create new songs and musical pieces, music producers and sound engineers who record songs and pieces, record labels and publishers that distribute recorded music products and sheet music internationally and that often control the rights to those products. Some music labels are "independent," while others are subsidiaries of larger corporate entities or international media groups. In the 2000s, the increasing popularity of listening to music as digital music files on MP3 players, iPods, or computers, and of trading music on file sharing websites or buying it online in the form of digital files had a major impact on the traditional music business. Many smaller independent CD stores went out of business as music buyers decreased their purchases of CDs, and many labels had lower CD sales. Some companies did well with the change to a digital format, though, such as Apple's iTunes, an online music store that sells digital files of songs over the Internet.

In spite of some international copyright treaties, determining which music is in the public domain is complicated by of national copyright laws that may be applicable. US copyright law formerly protected printed music published after 1923 for 28 years and with renewal for another 28 years, but the Copyright Act of 1976 made renewal automatic, and the Digital Millennium Copyright Act changed the calculation of the copyright term to 70 years after the death of the creator. Recorded sound falls under mechanical licensing, often covered by a confusing patchwork of state laws; most cover versions are licensed through the Harry Fox Agency. Performance rights may be obtained by either performers or the performance venue; the two major organizations for licensing are BMI and ASCAP. Two online sources for public domain music are IMSLP (International Music Score Library Project) and Choral Public Domain Library (CPDL).

The incorporation of some music or singing training into general education from preschool to post secondary education is common in North America and Europe. Involvement in playing and singing music is thought to teach basic skills such as concentration, counting, listening, and cooperation while also promoting understanding of language, improving the ability to recall information, and creating an environment more conducive to learning in other areas. In elementary schools, children often learn to play instruments such as the recorder, sing in small choirs, and learn about the history of Western art music and traditional music. Some elementary school children also learn about popular music styles. In religious schools, children sing hymns and other religious music. In secondary schools (and less commonly in elementary schools), students may have the opportunity to perform in some types of musical ensembles, such as choirs (a group of singers), marching bands, concert bands, jazz bands, or orchestras. In some school systems, music lessons on how to play instruments may be provided. Some students also take private music lessons after school with a singing teacher or instrument teacher. Amateur musicians typically learn basic musical rudiments (e.g., learning about musical notation for musical scales and rhythms) and beginner- to intermediate-level singing or instrument-playing techniques.

At the university level, students in most arts and humanities programs can receive credit for taking a few music courses, which typically take the form of an overview course on the history of music, or a music appreciation course that focuses on listening to music and learning about different musical styles. In addition, most North American and European universities have some types of musical ensembles that students in arts and humanities are able to participate in, such as choirs, marching bands, concert bands, or orchestras. The study of Western art music is increasingly common outside of North America and Europe, such as the Indonesian Institute of the Arts in Yogyakarta, Indonesia, or the classical music programs that are available in Asian countries such as South Korea, Japan, and China. At the same time, Western universities and colleges are widening their curriculum to include music of non-Western cultures, such as the music of Africa or Bali (e.g. Gamelan music).

Individuals aiming to become professional musicians, singers, composers, songwriters, music teachers and practitioners of other music-related professions such as music history professors, sound engineers, and so on study in specialized post-secondary programs offered by colleges, universities and music conservatories. Some institutions that train individuals for careers in music offer training in a wide range of professions, as is the case with many of the top U.S. universities, which offer degrees in music performance (including singing and playing instruments), music history, music theory, music composition, music education (for individuals aiming to become elementary or high school music teachers) and, in some cases, conducting. On the other hand, some small colleges may only offer training in a single profession (e.g., sound recording).

While most university and conservatory music programs focus on training students in classical music, there are a number of universities and colleges that train musicians for careers as jazz or popular music musicians and composers, with notable U.S. examples including the Manhattan School of Music and the Berklee College of Music. Two important schools in Canada which offer professional jazz training are McGill University and Humber College. Individuals aiming at careers in some types of music, such as heavy metal music, country music or blues are less likely to become professionals by completing degrees or diplomas in colleges or universities. Instead, they typically learn about their style of music by singing and/or playing in many bands (often beginning in amateur bands, cover bands and tribute bands), studying recordings available on CD, DVD and the Internet and working with already-established professionals in their style of music, either through informal mentoring or regular music lessons. Since the 2000s, the increasing popularity and availability of Internet forums and YouTube "how-to" videos have enabled many singers and musicians from metal, blues and similar genres to improve their skills. Many pop, rock and country singers train informally with vocal coaches and singing teachers.

Undergraduate university degrees in music, including the Bachelor of Music, the Bachelor of Music Education, and the Bachelor of Arts (with a major in music) typically take about four years to complete. These degrees provide students with a grounding in music theory and music history, and many students also study an instrument or learn singing technique as part of their program. Graduates of undergraduate music programs can seek employment or go on to further study in music graduate programs. Bachelor's degree graduates are also eligible to apply to some graduate programs and professional schools outside of music (e.g., public administration, business administration, library science, and, in some jurisdictions, teacher's college, law school or medical school).

Graduate music degrees include the Master of Music, the Master of Arts (in musicology, music theory or another music field), the Doctor of Philosophy (Ph.D.) (e.g., in musicology or music theory), and more recently, the Doctor of Musical Arts, or DMA. The Master of Music degree, which takes one to two years to complete, is typically awarded to students studying the performance of an instrument, education, voice (singing) or composition. The Master of Arts degree, which takes one to two years to complete and often requires a thesis, is typically awarded to students studying musicology, music history, music theory or ethnomusicology.

The PhD, which is required for students who want to work as university professors in musicology, music history, or music theory, takes three to five years of study after the master's degree, during which time the student will complete advanced courses and undertake research for a dissertation. The DMA is a relatively new degree that was created to provide a credential for professional performers or composers that want to work as university professors in musical performance or composition. The DMA takes three to five years after a master's degree, and includes advanced courses, projects, and performances. In Medieval times, the study of music was one of the Quadrivium of the seven Liberal Arts and considered vital to higher learning. Within the quantitative Quadrivium, music, or more accurately harmonics, was the study of rational proportions.

Musicology, the academic study of the subject of music, is studied in universities and music conservatories. The earliest definitions from the 19th century defined three sub-disciplines of musicology: systematic musicology, historical musicology, and comparative musicology or ethnomusicology. In 2010-era scholarship, one is more likely to encounter a division of the discipline into music theory, music history, and ethnomusicology. Research in musicology has often been enriched by cross-disciplinary work, for example in the field of psychoacoustics. The study of music of non-Western cultures, and the cultural study of music, is called ethnomusicology. Students can pursue the undergraduate study of musicology, ethnomusicology, music history, and music theory through several different types of degrees, including bachelor's degrees, master's degrees and PhD degrees.

Music theory is the study of music, generally in a highly technical manner outside of other disciplines. More broadly it refers to any study of music, usually related in some form with compositional concerns, and may include mathematics, physics, and anthropology. What is most commonly taught in beginning music theory classes are guidelines to write in the style of the common practice period, or tonal music. Theory, even of music of the common practice period, may take many other forms. Musical set theory is the application of mathematical set theory to music, first applied to atonal music. "Speculative music theory", contrasted with "analytic music theory", is devoted to the analysis and synthesis of music materials, for example tuning systems, generally as preparation for composition.

Zoomusicology is the study of the music of non-human animals, or the musical aspects of sounds produced by non-human animals. As George Herzog (1941) asked, "do animals have music?" François-Bernard Mâche's "Musique, mythe, nature, ou les Dauphins d'Arion" (1983), a study of "ornitho-musicology" using a technique of Nicolas Ruwet's "Langage, musique, poésie" (1972) paradigmatic segmentation analysis, shows that bird songs are organised according to a repetition-transformation principle. Jean-Jacques Nattiez (1990), argues that "in the last analysis, it is a human being who decides what is and is not musical, even when the sound is not of human origin. If we acknowledge that sound is not organised and conceptualised (that is, made to form music) merely by its producer, but by the mind that perceives it, then music is uniquely human."

In the West, much of the history of music that is taught deals with the Western civilization's art music, which is known as classical music. The history of music in non-Western cultures ("world music" or the field of "ethnomusicology"), which typically covers music from
Africa and Asia is also taught in Western universities. This includes the documented classical traditions of Asian countries outside the influence of Western Europe, as well as the folk or indigenous music of various other cultures. Popular or folk styles of music in non-Western countries varied widely from culture to culture, and from period to period. Different cultures emphasised different instruments, techniques, singing styles and uses for music. Music has been used for entertainment, ceremonies, rituals, religious purposes and for practical and artistic communication. Non-Western music has also been used for propaganda purposes, as was the case with Chinese opera during the Cultural Revolution.

There is a host of music classifications for non-Western music, many of which are caught up in the argument over the definition of music. Among the largest of these is the division between classical music (or "art" music), and popular music (or commercial music – including non-Western styles of rock, country, and pop music-related styles). Some genres do not fit neatly into one of these "big two" classifications, (such as folk music, world music, or jazz-related music).

As world cultures have come into greater global contact, their indigenous musical styles have often merged with other styles, which produces new styles. For example, the United States bluegrass style contains elements from Anglo-Irish, Scottish, Irish, German and African instrumental and vocal traditions, which were able to fuse in the United States' multi-ethnic "melting pot" society. Some types of world music contain a mixture of non-Western indigenous styles with Western pop music elements. Genres of music are determined as much by tradition and presentation as by the actual music. Some works, like George Gershwin's "Rhapsody in Blue", are claimed by both jazz and classical music, while Gershwin's "Porgy and Bess" and Leonard Bernstein's "West Side Story" are claimed by both opera and the Broadway musical tradition. Many current music festivals for non-Western music include bands and singers from a particular musical genre, such as world music.

Indian music, for example, is one of the oldest and longest living types of music, and is still widely heard and performed in South Asia, as well as internationally (especially since the 1960s). Indian music has mainly three forms of classical music, Hindustani, Carnatic, and Dhrupad styles. It has also a large repertoire of styles, which involve only percussion music such as the talavadya performances famous in South India.

Music therapy is an interpersonal process in which a trained therapist uses music and all of its facets—physical, emotional, mental, social, aesthetic, and spiritual—to help clients to improve or maintain their health. In some instances, the client's needs are addressed directly through music; in others they are addressed through the relationships that develop between the client and therapist. Music therapy is used with individuals of all ages and with a variety of conditions, including: psychiatric disorders, medical problems, physical disabilities, sensory impairments, developmental disabilities, substance abuse issues, communication disorders, interpersonal problems, and aging. It is also used to improve learning, build self-esteem, reduce stress, support physical exercise, and facilitate a host of other health-related activities. Music therapists may encourage clients to sing, play instruments, create songs, or do other musical activities.

One of the earliest mentions of music therapy was in Al-Farabi's (c. 872–950) treatise "Meanings of the Intellect", which described the therapeutic effects of music on the soul. Music has long been used to help people deal with their emotions. In the 17th century, the scholar Robert Burton's "The Anatomy of Melancholy" argued that music and dance were critical in treating mental illness, especially melancholia. He noted that music has an "excellent power ...to expel many other diseases" and he called it "a sovereign remedy against despair and melancholy." He pointed out that in Antiquity, Canus, a Rhodian fiddler, used music to "make a melancholy man merry, ...a lover more enamoured, a religious man more devout." In the Ottoman Empire, mental illnesses were treated with music. In November 2006, Dr. Michael J. Crawford and his colleagues also found that music therapy helped schizophrenic patients.

Albert Einstein had a lifelong love of music (particularly the works of Bach and Mozart), once stating that life without playing music would be inconceivable to him. In some interviews Einstein even attributed much of his scientific intuition to music, with his son Hans recounting that "whenever he felt that he had come to the end of the road or into a difficult situation in his work, he would take refuge in music, and that would usually resolve all his difficulties." Something in the music, according to Michele and Robert Root-Bernstein in "Psychology Today", "would guide his thoughts in new and creative directions." It has been said that Einstein considered Mozart's music to reveal a universal harmony that Einstein believed existed in the universe, "as if the great Wolfgang Amadeus did not 'create' his beautifully clear music at all, but simply discovered it already made. This perspective parallels, remarkably, Einstein’s views on the ultimate simplicity of nature and its explanation and statement via essentially simple mathematical expressions." A review suggests that music may be effective for improving subjective sleep quality in adults with insomnia symptoms. Music is also being used in clinical rehabilitation of cognitive and motor disorders.





</doc>
<doc id="100151" url="https://en.wikipedia.org/wiki?curid=100151" title="Index of music articles">
Index of music articles

This is an alphabetical index of articles related to music.

7-limit tuning -
15 equal temperament -
17 equal temperament -
19 equal temperament -
22 equal temperament -
23 equal temperament -
31 equal temperament -
34 equal temperament -
41 equal temperament -
53 equal temperament -
58 equal temperament -
72 equal temperament -
96 equal temperament -
20th-century classical music -
20th-century music -
21st-century classical music -
833 cents scale

A - 
A♭ -
A♯ - 
A major -
A minor -
A-flat major -
A-flat minor -
A-sharp minor -
A cappella -
A capriccio -
A due -
A12 scale -
Abbreviation -
Abendmusik -
Absolute music -
Absolute pitch -
Accent (music) -
Accentus -
Accidental -
Accompaniment -
Acoustic enhancement -
Acoustic scale -
Ad libitum -
Adagio -
Added tone chord -
Additive rhythm -
Adonai malakh mode -
Aeolian dominant scale -
Aeolian mode -
Aesthetics of music -
Air -
Air à boire -
Air de cour -
Ajam (maqam) -
Akebono scale -
Albanian opera -
Alberti bass -
Album -
Aleatoric music -
Algaita -
Algerian scale -
Algorithmic composition -
All-interval tetrachord -
All-interval twelve-tone row -
Alla breve -
Allegro -
Allemande -
Alpha scale -
Alta cappella -
Altered chord -
Altered scale -
Alternate bass -
Alternatim -
Altissimo -
Alto -
Ambassel scale -
Ambrosian chant -
Ambitus (music) -
American Music Awards -
Amusia -
Anacrusis -
Ancient Celtic music -
Ancient music -
Andalusian cadence -
Andalusian classical music -
Andamento -
Andean music -
Anhemitonic scale -
Answer song -
Anthem -
Antiphon -
Arab tone system -
Arabic maqam -
Arabic music -
Arch form -
Archlute -
Arghul -
Aria -
Aria di sorbetto -
Arioso -
Arpeggio -
Arrangement -
Ars antiqua -
Ars nova -
Ars subtilior -
Arsis and thesis -
Art music -
Art song -
Articulation -
Artificial harmonic -
Atonality -
Attacco -
Audio mixing -
Auditory illusion -
Augmentation -
Augmented fifth -
Augmented octave -
Augmented second -
Augmented seventh -
Augmented sixth -
Augmented third -
Augmented triad -
Augmented tuning -
Augmented unison -
Ausmultiplikation -
Authentic performance -
Avant-garde music -
Avaz (music) -
Azione teatrale

B -
B♭ -
B♯ - 
B major -
B minor -
B-flat major -
B-flat minor -
BACH motif -
Back beat -
Background music -
Bagatelle (music) -
Bagpipes -
Balalaika -
Ballad opera -
Ballade
Ballata -
Ballet (music) -
Band (music) -
Bar (music) -
Bar form -
Barcarolle -
Bariolage -
Baritenor -
Baritone -
Baritone violin -
Baroque music -
Baroque music of the British Isles -
Baroque orchestra -
Baroque violin -
Baryton -
Basque music -
Bass (sound) -
Bass (voice type) -
Bass arpeggiation -
Bass note -
Bass run -
Bass saxophone -
Bass song -
Bass violin -
Bass-baritone -
Bassanello -
Bassist -
Bassline -
Basso continuo -
Basso profondo -
Bassoon -
Baton (conducting) -
Battaglia (music) -
Bayati (maqam) -
Bayaty-Shiraz (mode) -
Bayreuth canon -
Bayreuth Festival -
Beam -
Beat (music) -
Beatmatching -
Bebung -
Beethoven and C minor -
Beethoven's compositional method -
Beethoven's musical style -
Bel canto -
Bell pattern -
Bell tone -
Belting (music) -
Berber music -
Berceuse -
Bergamask -
Beste (Turkish music) -
Beta scale -
Bicinium -
Bimodality -
Binary form -
Beneventan chant -
Biomusic -
Biomusicology -
Birds in music -
Bisector (music) -
Blind octave -
Block chord -
Blue note -
Blues -
Blues ballad -
Bohlen–Pierce scale -
Bologna School of music -
Boogie woogie -
Border ballad -
Borrowed chord -
Bouzouki -
Bowed clavier -
Boy soprano -
Braille music -
Brass instrument -
Brass quintet -
Brass tablature -
Bravura -
Break (music) -
Breath mark -
Breedsma -
Bridge (music) -
Brindisi (music) -
British opera -
Broken consort -
Bruckner rhythm -
Burgundian School -
Burletta -
Bushi (music) -
Byzantine lyra -
Byzantine music

C - 
C♯ - 
C major -
C minor -
C-flat major -
C-sharp major -
C-sharp minor - 
Cabaletta - 
Cadence -
Cadenza -
Caesura -
Call and response (music) -
Calypso music -
Cambiata -
Camerata (music) -
Canción -
Canon (music) -
Canntaireachd -
Cantabile -
Cantastoria -
Cantata -
Canticle -
Cantiga -
Cantillation -
Cantore al liuto -
Cantus coronatus -
Cantus firmus -
Canzona -
Canzone Napoletana -
Canzonetta -
Capriccio (music) -
Carnatic music -
Carnival song -
Cassation (music) -
Castrato -
Catalogue aria -
Catalogues of classical compositions -
Cauda -
Cavatina -
Celesta -
Cell (music) -
Cello -
Cello da spalla -
Cellone -
Cello sonata -
Celtic chant -
Celtic harp -
Celtic music -
Cent (music) -
Central American music -
Chaconne -
Chahargah (mode) -
Chamber music -
Chamber opera -
Changing tones -
Chanson -
Charlemagne and church music -
Cheironomy -
Chest of viols -
Chest voice -
Chiaroscuro (music) -
Chiavette -
Child singer -
Chitarra Italiana -
Choir -
Choral concerto -
Choral symphony -
Chorale -
Chorale motet -
Chorale partita -
Chord -
Chord chart -
Chord substitution -
Chording -
Chordioid -
Chord progression -
Chord-scale system -
Chordal space -
Chroma feature -
Chromatic chord -
Chromatic circle -
Chromatic fantasia -
Chromatic fourth -
Chromatic genus -
Chromatic hexachord -
Chromatic mediant -
Chromatic scale -
Chromaticism -
Church music -
Church music in Scotland -
Cibell -
Cimbasso -
Circle of fifths -
Circular breathing -
Circus music -
Clapping -
Clarinet -
Clarinet trio -
Classical music -
Classical music in Scotland -
Classical music written in collaboration -
Classical period (music) -
Clausula (music) -
Clavichord -
Clef -
Clinic (music) -
Close and open harmony -
Closely related key -
Cloud (music) -
Coda -
Cognitive musicology -
Col legno -
Colascione -
Cologne School (music) -
Color (medieval music) -
Coloratura soprano -
Colored music notation -
Colorist (music) -
Colotomy -
Collaborative piano -
Combination tone -
Combinatoriality -
Comic opera -
Comma (music) -
Common tone (chord) -
Comping -
Complement (music) -
Complexe sonore -
Compound metre -
Composer -
Composer tributes (classical music) -
Comprimario -
Concert -
Concert aria -
Concert band -
Concertmaster -
Concert performance -
Concert pitch -
Concert version -
Concertato -
Concertino (composition) -
Concerto -
Concerto for Orchestra -
Conclusion (music) -
Conducting -
Conductorless orchestra -
Conjunto -
Consonance -
Consort of instruments -
Consort song (musical) -
Constant spectrum melody -
Constant structure -
Contemporary classical music -
Contemporary harpsichord -
Contenance angloise -
Counting (music) -
Contrabass oboe -
Contrabassophone -
Contraforte -
Contralto -
Contrapuntal motion -
Contrast (music) -
Convenienze -
Coptic music -
Copula (music) -
Copyist -
Cor anglais -
Corelli cadence -
Cornamuse -
Cornett -
Count off -
Counter-melody -
Counterpoint -
Countertenor -
Country house opera -
Country music -
Courante -
Court music in Scotland -
Cover version -
Cretan lyra -
Critical edition (opera) -
Cross-dressing in music and opera -
Cross motif -
Cross-strung harp -
Crusade song -
Cubase -
Cue note -
Culture in music cognition -
Curtain call -
Cut-out score -
Cycle (music) -
Cyclic form -
Cyclic set -
Cymbal

D - 
D♭ -
D♯ -
D-sharp minor -
D major -
D minor -
D-flat major -
D-flat minor -
D♯ - 
Da capo -
Da capo aria -
Dal segno -
Dance music -
Dance and music of Latin America -
Darmstadt School -
Daseian notation -
Dastgah -
Dates of classical music eras -
Decet (music) -
Definite pitch -
Definition of music -
Degenerate music -
Degree (music) -
Delta scale -
Derived row -
Descant -
Descending tetrachord -
Deutsch's scale illusion -
Diapason -
Diaschisma -
"Diatessaron" -
Diatonic and chromatic -
Diatonic hexachord -
Diatonic scale -
Diatonic set theory -
Diegetic music -
Diesis -
Digital sheet music -
Diminished fourth -
Diminished octave -
Diminished second -
Diminished seventh -
Diminished seventh chord -
Diminished sixth -
Diminished third -
Diminished triad -
Diminished tuning -
Diminution -
Discant -
Distance model -
Ditone -
Ditonic scale -
Divertimento -
Divisi -
Division viol -
DJ mix -
Dodeka music notation -
Dominant -
Dominant seventh sharp ninth chord -
Dorian mode -
Dorian ♭2 scale -
Dotted note -
Double bass -
Double drumming -
Double-Function Form -
Double stop -
Double tonic -
Double variation -
Double whole note -
Dramma giocoso -
Dream chord -
Dresden amen -
Dromoi -
Drone -
Drone music -
Drum -
Drum kit -
Drum rudiment -
DSCH motif -
Dubreq Stylophone -
Duet -
Duettino -
Dulab -
Dulcian -
Dumka (musical genre) -
Duodecet -
Duodrama -
Duration (music) -
Dyad (music) -
Dynamic tonality -
Dynamics

E - 
E♭ -
E major -
E minor -
E-flat major -
E-flat minor -
Ear training -
Early music -
Early music festivals -
Early music of the British Isles -
Early music revival -
Earworm -
Echos -
Eclecticism in music -
Ecomusicology -
Educational music -
Eighth note -
Ekphonetic notation -
Elastic scoring -
Electronic music -
Electric piano -
Electronic musical instrument -
Elements of music -
Eleventh -
Eleventh chord -
Emancipation of the dissonance -
Embodied music cognition -
Encore -
English art song -
English bagpipes -
English cadence -
English Musical Renaissance -
Enharmonic -
Enharmonic keyboard -
Enharmonic scale -
Enigmatic scale -
Ensalada (music) -
Entr'acte -
Entrainment -
Environmentalism in music -
Epidiapente -
Equal temperament -
Equivalence class (music) -
Escala nordestina -
Estampie -
Étude -
Ethnomusicology -
Euclidean rhythm -
Euouae -
Evolutionary musicology -
Exposition (music) -
Expression pedal -
Extension (music) -
Eye movement in music reading -
Eye music

F - 
F♯ - 
F major -
F minor -
F-flat major -
F-sharp major -
F-sharp minor -
F+ (pitch) -
Factor (chord) -
Faking (Western classical music) -
Falset (music) - 
Falsetto -
Falsettone -
False relation -
Falsobordone -
Fandango -
Fantasia (music) -
Farandole -
Farsa -
Fasıl -
Fauxbourdon -
Feedback -
Fermata -
Festa teatrale -
Fifteenth -
Fifth (chord) -
Figure (music) -
Figured bass -
Filk music -
Fill (music) -
Film score -
Finale (music) -
Finale (software) -
Finalis -
Fingering (music) -
Finger vibrato -
Finnish tango -
Fioritura -
First inversion -
First Viennese School -
The Five (composers) -
Five-finger exercise -
Five-limit tuning -
Five string violin -
Flamenco mode -
Flat (music) -
Flugelhorn -
Flute -
Flute choir -
Flute quartet -
Folia -
Folk music -
Formalism (music) -
Formula composition -
Forte -
Fortepiano -
Fortepiano (musical dynamic) -
Fortspinnung -
Four note group -
Four-part harmony -
Fragmentation (music) -
Franco-Flemish School -
Frankfurt Group -
French classical music -
French horn -
French opera -
French overture -
French pop music -
Frequency -
Friction idiophone -
Fugue -
Function (music) -
Fundamental structure -
Furniture music -
Futurism (music)

G -
G♭ -
G♯ - 
G major -
G minor -
G-flat major -
G-sharp major -
G-sharp minor -
G run -
Gagaku -
Galliard - 
Gallican chant -
Gamelan -
Gamelan notation -
Gamma scale - 
Gavotte -
Gebrauchsmusik -
Geisslerlieder -
Generalized keyboard -
Generative music -
Generative theory of tonal music -
Generic and specific intervals -
German organ schools -
Ghost note -
Gigue -
Giovane scuola -
Glissando -
Glockenspiel -
Gong -
Gongche notation -
GNU LilyPond -
Grace note -
Grand motet -
Grand opera -
Graphic notation (music) -
Grave (music) -
Greek musical instruments -
Gregorian chant -
Gregorian mode -
Groove -
Group of Eight (music) -
Group piano -
Grupo de los cuatro -
Grupo renovación -
Guidonian hand -
Gymel

Hagiopolitan Octoechos -
Half diminished scale -
Half note -
Half-time (music) -
Hammond organ -
Hang (instrument)
Harmonic -
Harmonic major scale -
Harmonic rhythm -
Harmonic scale -
Harmonic series (music) -
Harmonic seventh -
Harmonization -
Harmony -
Harp -
Harpsichord -
Harpsichord-viola -
Hauptstimme -
Haute-contre -
Head (music) -
Head voice -
Helmholtz pitch notation -
Hemiola -
Heptatonic scale -
Hexachord -
Hexatonic scale -
Heyrati -
Hip hop -
Hirajōshi scale -
Hirtenschalmei -
Historically informed performance -
History of lute-family instruments -
History of music -
History of music publishing -
History of sonata form -
History of the harpsichord -
History of the violin -
Holdrian comma -
Humayun (mode) -
Homophony -
Homotonal -
Hornbostel-Sachs -
Hundred twenty-eighth note -
Hungarian minor scale -
Hurrian songs -
Hymn -
Hyperrealism (music) -
Hypoaeolian mode -
Hypoionian mode -
Hypophrygian mode

ii–V–I progression -
Imitation (music) -
Implication-Realization -
Impressionism in music -
Impromptu -
Improvisation -
In scale -
Incidental music -
Incipit -
Incomplete repetition -
Incomposite interval -
Indefinite pitch -
Indeterminacy (music) -
Indian Classical Music -
Industrial music
Inganno -
Inharmonicity -
In scale -
Insects in music -
Insen scale -
Instrumental idiom -
Instrumentation (music) -
Intabulation -
Interactive music -
Interdominant -
Intermedio -
Intermezzo -
Interpolation (classical music) -
Interval -
Interval class -
Interval ratio -
Interval vector -
Intonation -
Introduction (music) -
Intuitive music -
Inversion (music) -
Ionian mode -
Irmos -
Irrational rhythm -
Irregular resolution -
Islamic music -
Ison (music) -
Isorhythm -
Istrian scale -
Italian opera -
Italian overture -
Iwato scale

Japanese mode -
Japanese musical scales -
Jazz -
Jazz standard -
Jins -
Jubilus -
Just intonation

Kamancheh -
Kammersänger -
Kapellmeister - 
Karabakh Shikastasi (mode) -
[[Karaoke] -
[[Kawala]] -
[[Key (music)|Key]] -
[[Key signature]] -
[[Key signature names and translations]] -
[[Keyboard bass]] -
[[Keyboard instrument]] -
[[Keyboard tablature]] -
[[Keyboardist]] -
[[Khrennikov's Seven]] -
[[Klang (music)]] -
[[Klangfarbenmelodie]] -
[[Kleisma]] -
[[Korean court music]] -
[[Kortholt]] -
[[Krakebs]] -
[[Kwitra]]

[[L'istesso tempo]] -
[[Lacuna (music)]] -
[[Lament bass]] -
[[Landini cadence]] -
[[Larghetto]] -
[[Layali]] -
[[Leading-tone]] -
[[Lead instrument]] -
[[Lead sheet]] -
[[Ledger line]] -
[[Legato]] -
[[Leitmotif]] -
[[Les Six]] -
[[Letter notation]] -
[[Level (music)]] -
[[Libretto]] -
[[Lied]] -
[[Ligature (music)|Ligature]] -
[[Lilting]] -
[[Limit (music)]] -
[[Linear#Music|Linear]] -
[[Linear progression]] -
[[Lining out]] -
[[Linzer Orgeltabulatur]] -
[[Lisztomania]] -
[[Literaturoper]] -
[[Live electronic music]] -
[[Locrian mode]] -
[[Longa (music)]] -
[[Ludomusicology]] -
[[Luri music]] -
[[Lute]] -
[[Lute song]] -
[[Lydian augmented scale]] -
[[Lydian cadence]] -
[[Lydian mode]] -
[[Lyra viol]]

[[Madrigal (music)|Madrigal]] -
[[Madrigal (Trecento)]] -
[[Madrigal comedy]] -
[[Madrigale spirituale]] - 
[[Maestoso]] -
[[Maestro]] -
[[Magic chord]] -
[[Major chord]] -
[[Major fourth and minor fifth]] -
[[Major limma]] -
[[Major Locrian scale]] -
[[Major second]] -
[[Major scale]] -
[[Major seventh]] -
[[Major sixth]] -
[[Major third]] -
[[Mandola]] -
[[Mannheim school]] -
[[Manualism (hand music)]] -
[[Manuscript paper]] -
[[Māori music]] -
[[March (music)]] -
[[Marching band]] -
[[Mariachi]] -
[[Marimba]] -
[[Martial music]] -
[[Masonic music]] -
[[Mass (music)]] -
[[Matrix (music)]] -
[[Maxima (music)]] -
[[Maximal evenness]] -
[[Mazurka]] -
[[Meantone temperament]] -
[[Bar (music)|Measure]] -
[[Mediant]] -
[[Medieval music]] -
[[Melharmony]] -
[[Melisma]] -
[[Melodic expectation]] -
[[Melodic fission]] -
[[Melodic motion]] -
[[Melodic pattern]] -
[[Melody]] -
[[Melody type]] -
[[Mensural notation]] -
[[Mensurstrich]] -
[[Messa di voce]] -
[[Method (music)]] -
[[Metre (hymn)]] -
[[Metre (music)]] -
[[Metric modulation]] -
[[Metronome]] -
[[Mezzo-soprano]] -
[[Micropolyphony]] -
[[Microsound]] -
[[Microtonal music]] -
[[Middle Eastern music]] -
[[Military band]] -
[[Millioctave]] -
[[Miming in instrumental performance]] -
[[Minimal music]] -
[[Minnesang]] -
[[Minor chord]] -
[[Minor diatonic semitone]] -
[[Minor second]] -
[[Minor scale]] -
[[Minor seventh]] -
[[Minor sixth]] -
[[Minor third]] -
[[Minuet]] -
[[Missing fundamental]] -
[[Mistuning]] -
[[Mix tape]] -
[[Mixed-interval chord]] -
[[Audio mixing (recorded music)|Mixing]] -
[[Mixolydian mode]] -
[[Modal frame]] -
[[Modal voice]] -
[[Mode (music)|Mode]] -
[[Modernism (music)]] -
[[Modes of limited transposition]] -
[[Modified Stave Notation]] -
[[Modular music]] -
[[Modulation (music)]] -
[[Modus (medieval music)]] -
[[Moment form]] -
[[Monad (music)]] -
[[Money note]] -
[[Monodrama]] -
[[Monody]] -
[[Monophony]] -
[[Monotonic scale]] -
[[Motet]] -
[[Motet-chanson]] -
[[Motif (music)]] -
[[Mouthpiece (brass)]] -
[[Mouthpiece (woodwind)]] -
[[Movement (music)]] -
[[Mozarabic chant]] -
[[Mozart and G minor]] -
[[Mozart effect]] -
[[Multiphonic]] -
[[Multiplication (music)]] -
[[Muqam]] -
[[Museme]] -
[[Music]] -
[[Music acquisition]] -
[[Music alignment]] -
[[Music and artificial intelligence]] -
[[Music and emotion]] -
[[Music and mathematics]] -
[[Music and politics]] -
[[Music appreciation]] -
[[Music archaeology]] -
[[Music as a coping strategy]] -
[[Music box]] -
[[Music community]] -
[[Music criticism]] -
[[Music drama]] -
[[Music education]] -
[[Music Encoding Initiative]] -
[[Music engraving]] -
[[Music examination]] -
[[Music festival]] -
[[Music genre]] -
[[Music history]] -
[[Music journalism]] -
[[Music industry]] -
[[Music in early modern Scotland]] -
[[Music in Medieval England]] -
[[Music in Medieval Scotland]] -
[[Music in Paris]] -
[[Music in psychological operations]] -
[[Music in space]] -
[[Music in Tatarstan]] -
[[Music learning theory]] -
[[Music lesson]] -
[[Music librarianship]] -
[[Music manuscript]] -
[[Musico]] -
[[Music of Afghanistan]] -
[[Music of Albania]] -
[[Music of Algeria]] -
[[Music of Ancient Greece]] -
[[Music of Andorra]] -
[[Music of Argentina]] -
[[Music of Armenia]] -
[[Music of Australia]] -
[[Music of Austria]] -
[[Music of Badakhshan]] -
[[Music of Bahrain]] -
[[Music of Barbados]] -
[[Music of Belarus]] -
[[Music of Belgium]] -
[[Music of Belize]] -
[[Music of Bermuda]] -
[[Music of Bolivia]] -
[[Music of Bosnia and Herzegovina]] -
[[Music of Brazil]] -
[[Music of Brunei]] -
[[Music of Bulgaria]] -
[[Music of Canada]] -
[[Music of Catalonia]] -
[[Music of Chile]] -
[[Music of Colombia]] -
[[Music of Costa Rica]] -
[[Music of Croatia]] -
[[Music of Cuba]] -
[[Music of Cyprus]] -
[[Music of Denmark]] -
[[Music of Easter Island]] -
[[Music of Ecuador]] -
[[Music of Egypt]] -
[[Music of El Salvador]] -
[[Music of Estonia]] -
[[Music of Fiji]] -
[[Music of Finland]] -
[[Music of France]] -
[[Music of French Guiana]] -
[[Music of Georgia (country)]] -
[[Music of Germany]]-
[[Music of Guadeloupe]] -
[[Music of Guatemala]] -
[[Music of Guyana]] -
[[Music of Haiti]] -
[[Music of Hawaii]] -
[[Music of Honduras]] -
[[Music of Hungary]] -
[[Music of Iceland]] -
[[Music of India]] -
[[Music of Indonesia]] -
[[Music of Iraq]] -
[[Music of Ireland]] -
[[Music of Jamaica]] -
[[Music of Jordan]] -
[[Music of Kazakhstan]] -
[[Music of Kenya]] -
[[Music of Kuwait]] -
[[Music of Latvia]] -
[[Music of Lebanon]] -
[[Music of Libya]] -
[[Music of Liechtenstein]] -
[[Music of Lithuania]] -
[[Music of Luxembourg]] -
[[Music of Malta]] -
[[Music of Martinique]] -
[[Music of Mauritius]] -
[[Music of Mesopotamia]] -
[[Music of Mexico]] -
[[Music of Moldova]] -
[[Music of Monaco]] -
[[Music of Mongolia]] -
[[Music of Montenegro]] -
[[Music of Morocco]] -
[[Music of Myanmar]] -
[[Music of Namibia]] -
[[Music of New Zealand]] -
[[Music of Nicaragua]] -
[[Music of Niue]] -
[[Music of North Macedonia]] -
[[Music of Norway]] -
[[Music of Oman]] -
[[Music of Palestine]] -
[[Music of Panama]] -
[[Music of Paraguay]] -
[[Music of Peru]] -
[[Music of Polynesia]] -
[[Music of Portugal]] -
[[Music of Puerto Rico]] -
[[Music of Qatar]] -
[[Music of Russia]] -
[[Music of San Marino]] -
[[Music of Samoa]] -
[[Music of Saudi Arabia]] -
[[Music of Scotland]] -
[[Music of Scotland in the eighteenth century]] -
[[Music of Scotland in the nineteenth century]] -
[[Music of Serbia]] -
[[Music of Seychelles]] -
[[Music of Singapore]] -
[[Music of Slovakia]] -
[[Music of Slovenia]] -
[[Music of South Korea]] -
[[Music of Sweden]] -
[[Music of Switzerland]] -
[[Music of Syria]] -
[[Music of Thailand]] -
[[Music of Thessaly]] -
[[Music of the Bahamas]] -
[[Music of the Cayman Islands]] -
[[Music of the Channel Islands]] -
[[Music of the Comoros]] -
[[Music of the Czech Republic]] -
[[Music of the Dominican Republic]] -
[[Music of the Faroe Islands]] -
[[Music of the Federated States of Micronesia]] -
[[Music of the Lesser Antilles]] -
[[Music of the Maldives]] -
[[Music of the Netherlands]] -
[[Music of the Philippines]] -
[[Music of the Trecento]] -
[[Music of the Turks and Caicos Islands]] -
[[Music of the United Arab Emirates]] -
[[Music of the United Kingdom]] -
[[Music of Tibet]] -
[[Music of Tokelau]] -
[[Music of Tonga]] -
[[Music of Tunisia]] -
[[Music of Turkey]] -
[[Music of Ukraine]] -
[[Music of Uruguay]] -
[[Music of Vatican City]] -
[[Music of Venezuela]] -
[[Music of Vienna]] -
[[Music of Vietnam]] -
[[Music of Western Sahara]] -
[[Music of Yemen]] -
[[Music of Zambia]] -
[[Music of Zimbabwe]] -
[[Music piracy]] -
[[Music psychology]] -
[[Music publisher (sheet music)]] -
[[Music-related memory]] -
[[Music school]] -
[[Music-specific disorders]] -
[[Music stand]] -
[[Music technology]] -
[[Music theory]] -
[[Music therapy]] -
[[Music tourism]] -
[[Music transposer]] -
[[Music venue]] -
[[MusicWriter]] -
[[Musica ficta]] -
[[Musica poetica]] -
[[Musica reservata]] -
[[Musical acoustics]] -
[[Musical argument]] -
[[Musical composition]] -
[[Musical cryptogram]] -
[[Musical development]] -
[[Musical ensemble]] -
[[Musical expression]] -
[[Musical form]] -
[[Musical gesture]] -
[[Musical hallucinations]] -
[[Musical historicism]] -
[[Musical instrument]] -
[[Musical keyboard]] -
[[Musical literacy]] -
[[Musical notation]] -
[[Musical note]] -
[[Musical phrasing]] -
[[Musical prefix]] -
[[Musical semantics]] -
[[Musical setting]] -
[[Musical similarity]] -
[[Musical syntax]] -
[[Musical system of ancient Greece]] -
[[Musical technique]] -
[[Musical tone]] -
[[Musical tuning]] -
[[Musicality]] -
[[Musician]] -
[[Musicology]] -
[[Musique mesurée]] -
[[Mute (music)]] -
[[Mystic chord]]

[[Nashville Number System]] -
[[Natural (music)]] -
[[Neapolitan scale]] -
[[Neapolitan School]] -
[[Nenano]] -
[[Neobyzantine Octoechos]] -
[[Neoconservative postmodernism]] -
[[Neo-Medieval music]] -
[[Neue Deutsche Härte]] -
[[Neume]] -
[[Neuroscience of music]] -
[[Neutral interval]] -
[[Neutral sixth]] -
[[New-age music]] -
[[New German School]] -
[[New interfaces for musical expression]] -
[[New Music Manchester]] -
[[New musicology]] -
[[New Venice School]] -
[[Niente]] -
[[Ninth]] -
[[Ninth chord]] -
[[Nocturne]] -
[[Noise music]] -
[[Nonchord tone]] -
[[Nonet (music)]] -
[[Non-lexical vocables in music]] -
[[Notehead]] -
[[Note nere]] -
[[Notes inégales]] -
[[Notre-Dame school]] -
[[Novelette (music)]] -
[[Novelty song]] -
[[Number (music)]] -
[[Numbered musical notation]] -
[[Number opera]] - 
[[Numerical sight-singing]]

[[Obbligato]] -
[[Oboe]] -
[[Oboe da caccia]] -
[[Octatonic scale]] -
[[Octave]] -
[[Octave glissando]] -
[[Octave illusion]] -
[[Octet (music)]] -
[[Octoechos]] -
[["Ode-to-Napoleon" hexachord]] -
[[Oeldorf Group]] -
[[Offstage instrument or choir part in classical music]] -
[[Oltremontani]] -
[[Open chord]] -
[[Opera]] -
[[Opéra-ballet]] -
[[Opera buffa]] -
[[Opéra comique]] -
[[Opéra féerie]] -
[[Opera house]] -
[[Opera in Arabic]] -
[[Opera in English]] -
[[Opera in German]] -
[[Opera in Scotland]] -
[[Opera semiseria]] -
[[Opera seria]] -
[[Operetta]] -
[[Optical music recognition]] -
[[Opus number]] -
[[Oratorio]] -
[[Orchestra]] -
[[Orchestral enhancement]] -
[[Orchestra hit]] -
[[Orchestra pit]] -
[[Orchestration]] -
[[Orff Schulwerk]] -
[[Organ (music)|Organ]] -
[[Organ tablature]] -
[[Organetto]] -
[[Organology]] -
[[Organum]] -
[[Oriental riff]] -
[[Origin of the harp in Europe]] -
[[Origins of opera]] -
[[Oriscus]] -
[[Ornament (music)]] -
[[Orwell comma]] -
[[Ostinato]] -
[[Ossia]] -
[[Otonality and Utonality]] -
[[Ottoman classical music]] -
[[Oversinging]] -
[[Overtone]] -
[[Overture]]

[[Pandiatonicism]] -
[[Pandura]] -
[[Papadic Octoechos]] -
[[Parallel and counter parallel]] -
[[Parallel harmony]] -
[[Parallel key]] -
[[Paraphrase mass]] -
[[Pardessus de viole]] -
[[Parlour music]] -
[[Parody mass]] -
[[Parody music]] -
[[Parsons code]] -
[[Part (music)]] -
[[Partbook]]-
[[Partimento]] -
[[Partita]] -
[[Pasodoble]] -
[[Passacaglia]] -
[[Passaggio]] -
[[Passing chord]] -
[[Passion music]] -
[[Pasticcio]] -
[[Pastorale]] -
[[Pastorale héroïque]] -
[[Patter song]] -
[[Pattern completion]] -
[[Pavane]] -
[[Pedal keyboard]] -
[[Pedal point]] -
[[Pedal tone]] -
[[Pelog]] -
[[Pensato]] -
[[Pentachord]] -
[[Pentatonic scale]] -
[[Percussion instrument]] -
[[Percussion notation]] -
[[Perfect fourth]] -
[[Perfect fifth]] -
[[Period (music)]] -
[[Permutation (music)]] -
"[[Perpetuum mobile]]" -
[[Persian scale]] -
[[Persian traditional music]] -
[[Pervading imitation]] -
[[Pesante]] -
[[Petasti]] -
[[Petrushka chord]] -
[[Philosophy of music]] -
[[Phonograph]] -
[[Phrase (music)]] -
[[Phrygian dominant scale]] -
[[Phrygian mode]] -
[[Piano]] -
[[Piano ballade]] -
[[Piano concerto]] -
[[Piano duet]] -
[[Piano extended technique]] -
[[Piano four hands]] -
[[Piano history and musical performance]] -
[[Piano pedagogy]] -
[[Piano pedals]] -
[[Piano piece]] -
[[Piano quartet]] -
[[Piano quintet]] -
[[Piano sextet]] -
[[Piano six hands]] -
[[Piano solo]] -
[[Piano sonata]] -
[[Piano trio]] -
[[Piano-vocal score]] -
[[Pianto]] -
[[Picardy third]] -
[[Piccolo]] -
[[Piccolo heckelphone]] -
[[Piccolo oboe]] -
[[Piccolo trumpet]] -
[[Pièce d'occasion]] -
[[Pierrot ensemble]] -
[[Pipe band]] -
[[Pipe organ]] -
[[Piston valve]] -
[[Pitch (music)|Pitch]] -
[[Pitch axis theory]] -
[[Pitch class]] -
[[Pitch class space]] -
[[Pitch of brass instruments]] -
[[Pitch space]] -
[[Pit orchestra]] -
[[Pizzicato]] -
[[Plainsong]] -
[[Playing by ear]] -
[[Polish opera]] -
[[Polychord]] -
[[Polyphonic Era]] -
[[Polyphony]] -
[[Polyrhythm]] -
[[Polystylism]] -
[[Polytempo]] -
[[Polytonality]] -
[[Pop music]] -
[[Popular music]] -
[[Portato]] -
[[Positive organ]] -
[[Postminimalism]] -
[[Postmodern music]] -
[[Post-tonal music theory]] -
[[Potpourri (music)]] -
[[Power chord]] -
[[Precomposition]] -
[[Predominant chord]] -
[[Prehistoric music]] -
[[Prelude (music)]] -
[[Prelude and fugue]] -
[[Preparation (music)]] -
[[Prima donna]] -
[[Primary tone]] -
[[Primary triad]] -
[[Principal (music)]] -
[[Privileged pattern]] -
[[Process music]] -
[[Program music]] -
[[Progressive music]] -
[[Progressive tonality]] -
[[Projected set]] -
[[Prolation]] -
[[Prolation canon]] -
[[Prolongation]] -
[[Promenade concert]] -
[[Prompter (opera)]] -
[[Prosody (music)]] -
[[Pro Tools]] -
[[Protein music]] -
[[Protest song]] -
[[Pseudo-octave]] -
[[Psychedelic music]] -
[[Psychoacoustics]] -
[[Psychoanalysis and music]] -
[[Psychology of music preference]] -
[[Ptolemy's intense diatonic scale]] -
[[Public domain music]] -
[[Pulse (music)|Pulse]] -
[[Punctualism]] -
[[Pygmy music]] -
[[Pyknon]] -
[[Pythagorean comma]] -
[[Pythagorean interval]] -
[[Pythagorean tuning]]

[[Qanun (instrument)]] -
[[Quartal and quintal harmony]] -
[[Quarter-comma meantone]] -
[[Quarter note]] -
[[Quarter tone]] -
[[Quartet]] -
[[Quintet]] -
[[Quintuple meter]] -
[[Quintus (vocal music)]] -
[[Quodlibet]]

[[Rackett]] -
[[Radio opera]] -
[[Raga]] -
[[Rage aria]] -
[[Range (music)]] -
[[Ragisma]] -
[[Rast (mode)]] -
[[Rastrum]] -
[[Rauschpfeife]] -
[[Reason (program)]] -
[[Rebec]] -
[[Recapitulation (music)]] -
[[Recitative]] -
[[Reciting tone]] -
[[Record label]] -
[[Reduction (music)]] -
[[Reed contrabass]] -
[[Reggae]] -
[[Register (music)]] -
[[Registration (organ)]] -
[[Regular diatonic tuning]] -
[[Rehearsal letter]] -
[[Relative key]] -
[[Relative pitch]] -
[[Tension (music)|Relaxation]] -
[[Religious music]] -
[[Remix]] -
[[Renaissance music]] -
[[Repeat sign]] -
[[Répertoire International des Sources Musicales]] -
[[Répétiteur]] -
[[Repetition (music)]] -
[[Requiem]] -
[[Reprise]] -
[[Rescue opera]] -
[[Research in music education]] -
[[Resolution (music)]] -
[[Resonance]] -
[[Rest (music)]] -
[[Retrograde (music)]] -
[[Retrograde inversion]] -
[[Rhaita]] -
[[Rhapsody (music)]] -
[[Rhythm]] -
[[Rhythm in Arabic music]] -
[[Rhythm section]] -
[[Rhythmic gesture]] -
[[Rhythmic mode]] -
[[Rhythmic unit]] -
[[Ricercar]] -
[[Riddim]] -
[[Riff]] -
[[Ripieno]] -
[[Ripieno concerto]] -
[[Ritornello]] -
[[Ritsu and ryo scales]] -
[[Rock music]] -
[[Rock music in France]] -
[[Rock music in Portugal]] -
[[Rock music in Russia]] -
[[Roman School]] -
[[Romantic music]] -
[[Romantische Oper]] -
[[Rondalla]] -
[[Rondellus]] -
[[Rondo]] -
[[Rondò]] -
[[Root (chord)]] -
[[Rosegarden]] -
[[Rosette (music)]] -
[[Rotary valve]] -
[[Rothphone]] -
[[Roulade (music)]] -
[[Round (music)]] -
[[Rubab (instrument)]] -
[[Rule of the octave]] -
[[Rumba]] -
[[Russian classical music]] -
[[Russian opera]] -
[[Russian romance]]

[[Sacher hexachord]] -
[[Schenkerian analysis]] -
[[Schoenberg hexachord]] -
[[Sacred Harp]] -
[[Sainete]] -
[[Saint Martial school]] -
[[Sámi music]] -
[[Sampling (music)]] -
[[Sarabande]] -
[[Sarrusophone]] -
[[Satz]] -
[[Savart]] -
[[Savoy opera]] -
[[Saxophone]] -
[[Scale (music)]] -
[[Scale of harmonics]] -
[[Scale-step]] -
[[Scherzo]] -
[[Schisma]] -
[[Schismatic temperament]] -
[[Science fiction opera]] -
[[Scientific pitch notation]] -
[[Scordatura]] -
[[Scratching]] -
[[Scroll (music)]] -
[[Second inversion]] -
[[Second Viennese School]] -
[[Seconda pratica]] -
[[Secondary dominant]] -
[[Section (music)]] -
[[Secular music]] -
[[Secundal]] -
[[Segah]] -
[[Segue (music)]] -
[[Seikilos epitaph]] -
[[Semicomma]] -
[[Semi-contrabassoon]] -
[[Semitone]] -
[[Sensitive style]] -
[[Sentence (music)]] -
[[Septet]] -
[[Septimal chromatic semitone]] -
[[Septimal comma]] -
[[Septimal diatonic semitone]] -
[[Septimal diesis]] -
[[Septimal kleisma]] -
[[Septimal major third]] -
[[Septimal meantone temperament]] -
[[Septimal minor third]] -
[[Septimal quarter tone]] -
[[Septimal semicomma]] -
[[Septimal third tone]] -
[[Septimal tritone]] -
[[Septimal whole tone]] -
[[Sequence (music)]] -
[[Sequence (musical form)]] -
[[Serenade]] -
[[Serialism]] -
[[Seventh (chord)]] -
[[Seventh chord]] -
[[Seventh octave]] -
[[Sextet]] -
[[Sextuple metre]] -
[[Shakuhachi musical notation]] -
[[Shape note]] -
[[Sharawadji effect]] -
[[Sharp (music)]] -
[[Sheet music]] -
[[Shepard tone]] -
[[Shinto music]] -
[[Shorthand for orchestra instrumentation]] -
[[Shur (mode)]] -
[[Shushtar (mode)]] -
[[Sibelius (scorewriter)]] -
[[Siciliana]] -
[[Siffernotskrift]] -
[[Sigah]] -
[[Similarity relation (music)]] -
[[Simplified music notation]] -
[[Sinfonia]] -
[[Sinfonia concertante]] -
[[Singakademie]] -
[[Singing]] -
[[Singing school]] -
[[Single (music)]] -
[[Singspiel]] -
[[Sight-reading]] -
[[Simple meter]] -
[[Simultaneity (music)|Simultaneity]] -
[[Sixteenth note]] -
[[Major sixth|Sixth]] -
[[Sixth chord]] -
[[Sketch (music)]] -
[[Slash chord]] -
[[Slide (musical ornament)]] -
[[Slide trumpet]] -
[[Slur (music)]] -
[[Social history of the piano]] -
[[Sociomusicology]] -
[[Soft pedal]] -
[[Soggetto cavato]] -
[[Solfège]] -
[[Solita forma]] -
[[Solmization]] -
[[Solo (music)|Solo]] -
[[Solo tuning]] -
[[Sonata]] -
[[Sonata cycle]] -
[[Sonata da chiesa]] -
[[Sonata form]] -
[[Sonata theory]] -
[[Sonatina]] -
[[Song]] -
[[Song cycle]] -
[[Song structure]] -
[[Sonorism]] -
[[Sopranissimo]] -
[[Sopranist]] -
[[Soprano]] -
[[Soprano sfogato]] -
[[Sordun]] -
[[Sori (music)]] -
[[Sostenuto]] -
[[Sotto voce (music)]] -
[[Soul music]] -
[[Sound]] -
[[Sound and music computing]] -
[[Sound hole]] -
[[Sound icon]] -
[[Sound quality]] -
[[Sound recording and reproduction]] -
[[Sound sculpture]] -
[[Sound reinforcement system|Sound system]] -
[[Spaltklang]] -
[[Spanish opera]] -
[[Spatial music]] -
[[Spectral music]] -
[[Spiccato]] -
[[Spieloper]] -
[[Spinto]] -
[[Spinto soprano]] -
[[Spiral array model]] -
[[Spontaneous composition]] -
[[Sprechgesang]] -
[[Squillo]] -
[[Stab (music)]] -
[[Staccato]] -
[[Staff (music)]] -
[[Stagione]] -
[[Stem (music)]] -
[[Stentato]] -
[[Steps and skips]] -
[[Stile concitato]] -
[[Stile rappresentativo]] -
[[Sting (musical phrase)]] -
[[Sting (percussion)]] -
[[Stochastic]] -
[[Stopped note]] -
[[Strain (music)]] -
[[Strähle's construction]] -
[[Stretto]] -
[[String instrument]] -
[[String octet]] -
[[String piano]] -
[[String quartet]] -
[[String quintet]] -
[[String resonance]] -
[[String section]] -
[[String sextet]] -
[[String trio]] -
[[Strophic form]] -
[[Style brisé]] -
[[Sub-bass]] -
[[Subdominant]] -
[[Submediant]] -
[[Subminor and supermajor]] -
[[Subtonic]] -
[[Suite (music)|Suite]] -
[[Sung-through]] -
[[Superius]] -
[[Supertonic]] -
[[Suspension (music)]] -
[[Sustain pedal]] -
[[Suzuki method]] -
[[Swing (jazz performance style)|Swing]] -
[[Symmetric scale]] -
[[Symmetry#In music|Symmetry]] -
[[Sympathetic string]] -
[[Sympathy (music)]] -
[[Symphonic music in Iran]] -
[[Symphonic poem]] -
[[Symphony]] -
[[Syncopation]] -
[[Synthesizer]] -
[[Synthetic mode]] -
[[Synthetic scale]] -
[[Syntonic comma]] -
[[Syriac sacral music]] -
[[Systematic musicology]] -
[[Systems music]]

[[Tablature]] -
[[Table canon]] - 
[[Tacet]] -
[[Tafelmusik]] -
[[Tambourine]] -
[[Tarantella]] -
[[Tarantella Napoletana]] -
[[Tasto solo]] -
[[Tatum (music)]] -
[[Musical temperament|Temperament]] -
[[Temperament ordinaire]] -
[[Tempo]] -
[[Tempo rubato]] -
[[Tenor]] -
[[Tenor violin]] -
[[Tenore contraltino]] -
[[Tenore di grazia]] -
[[Tenoroon]] -
[[Tension (music)]] -
[[Tenuto]] -
[[Ternary form]] -
[[Tertian]] -
[[Tertium major]] -
[[Terzschritt]] -
[[Tessitura]] -
[[Tetrachord]] -
[[Tetrad (music)]] -
[[Tetratonic scale]] -
[[Text declamation]] -
[[Theatre music]] -
[[Thematic transformation]] -
[[Theorbo]] -
[[Theoretical key]] -
Theosophy and music -
[[Third (chord)]] -
[[Thirteenth]] -
[[Thirty-second note]] -
[[Thirty-two-bar form]] -
[[Three-hand effect]] -
[[Three-key exposition]] -
[[Through-composed]] -
[[Thumb position]] -
[[Tie (music)]] -
[[Tiento]] -
[[Timbre]] -
[[Timbre composition]] -
[[Timeline of trends in Italian music]] -
[[Time point]] -
[[Time signature]] -
[[Time unit box system]] -
[[Timing (music)]] -
[[Timpani]] -
[[Tiorbino]] -
[[Toccata]] -
[[Tonality]] -
[[Tonality diamond]] -
[[Tonality flux]] -
[[Tone (musical instrument)|Tone]] -
[[Tone Clock]] -
[[Tone cluster]] -
[[Tone row]] -
[[Tongan music notation]] -
[[Tonic (music)]] -
[[Tonicization]] -
[[Tonic Sol-fa]] -
[[Tonnetz]] -
[[Tonus peregrinus]] -
[[Total chromatic]] -
[[Totalism]] -
[[Trance music]] -
[[Traditional Gaelic music]] -
[[Tragédie en musique]] -
"[[Traité de l'harmonie réduite à ses principes naturels]]" -
[[Transcription (music)]] -
[[Transformation (music)]] -
[[Transformational theory]] -
[[Transition from Renaissance to Baroque in instrumental music]] -
[[Transposing instrument]] -
[[Transposition (music)]] -
"[[Treatise on Instrumentation]]" -
[[Treble (sound)]] -
[[Treble flute]] -
[[Treble voice]] -
[[Tremolo]] -
[[Triad (music)]] -
[[Triangle (musical instrument)]] -
[[Trill (music)|Trill]] -
[[Triple contrabass viol]] -
[[Tritone]] -
[[Tritone paradox]] -
[[Tritone substitution]] -
[[Tritonic scale]] -
[[Triumphal march]] -
[[Trombone]] -
[[Troubadour]] -
[[Trouvère]] -
[[Trumpet]] -
[[Trumpet voluntary]] -
[[TTBB]] -
[[Tuba]] -
[[Tuna (music)]] -
[[Musical tuning|Tuning]] -
[[Tuplet]] -
[[Turkish Five]] -
[[Turkish folk music]] -
[[Turkish music (style)]] -
[[Tutti]] -
[[Twelve-tone technique]] -
[[Two hundred fifty-sixth note]] -
[[Tydorel]]

[[Ukrainian Dorian scale]] -
[[Undertone series]] -
[[Unfigured bass]] -
[[Unfolding (music)]] -
[[Unified field]] -
[[Unison]] -
[[Universal key]]

[[Variation (music)]] -
[[Venetian polychoral style]] -
[[Venetian School (music)]] -
[[Verismo]] -
[[Vernacular music]] -
[[Verrophone]] -
[[Verse anthem]] -
[[Verse–chorus form]] -
[[Verso sciolto]] -
[[Vertical viola]] -
[[Vibraphone]] -
[[Vibrato]] -
[[Video game music]] -
[[Vienna horn]] -
[[Vienna New Year's Concert]] -
[[Viennese trichord]] -
[[Villanella]] -
[[Viol]] -
[[Viola]] -
[[Viola bastarda]] -
[[Viola d'amore]] -
[[Viola organista]] -
[[Viola pomposa]] -
[[Viola profonda]] -
[[Violin]] -
[[Violin family]] -
[[Violin octet]] -
[[Violin sonata]] -
[[Violino piccolo]] -
[[Violotta]] -
[[Virelai]] -
[[Virtuoso]] -
[[Vocal fry register]] -
[[Vocal music]] -
[[Vocal pedagogy]] -
[[Vocal range]] -
[[Vocal register]] - 
[[Vocal weight]] -
[[Vogel's Tonnetz]] -
[[Voice classification in non-classical music]] -
[[Voice crossing]] -
[[Voice exchange]] -
[[Voicing (music)]] -
[[Voluntary (music)]]

[[Wagner tuba]] -
[[Wah-wah (music)|Wah-wah]] -
[[Waltz (music)]] -
[[War of the Romantics]] -
[[Well temperament]] -
[[White voice]] -
[[Wienerlied]] -
[[Whole note]] -
[[Wind instrument]] -
[[Wind quartet]] -
[[Wind quintet]] -
[[Women in music]] -
[[Woodwind doubler]] -
[[Woodwind instrument]] -
[[Woodwind quartet]] -
[[Wolf interval]] -
[[Word painting]] -
[[Wordless functional analysis]]

[[Xenharmonic]] -
[[Xhosa music]] -
[[Xylophone]]

[[Yo scale]] -
[[Yoruba music]]

[[Zarzuela]] -
[[Zeitoper]] -
[[Zhonghu]] -
[[Zither]] -
[[Znamenny chant]] -
[[Zoomusicology]] -
[[Zukra]]

[[List of chord progressions|Chord progressions]] -
[[List of chords|Chords]] -
[[Chronological lists of classical composers]] -
[[List of classical and art music traditions|Classical and art music traditions]] -
[[List of classical music concerts with an unruly audience response|Classical music concerts with an unruly audience response]] -
[[Lists of composers|Composers]] -
[[List of compositions in just intonation|Compositions in just intonation]] -
[[List of cultural and regional genres of music|Cultural and regional genres of music]] -
[[List of intervals in 5-limit just intonation|Intervals in 5-limit just intonation]] -
[[List of Italian musical terms used in English|Italian musical terms used in English]] -
[[List of major/minor compositions|Major/minor compositions]] -
[[List of major opera composers|Major opera composers]]
[[List of meantone intervals|Meantone intervals]] -
[[List of music museums|Music museums]] -
[[List of music software|Music software]] -
[[List of music styles|Music styles]] -
[[List of music theorists|Music theorists]] -
[[List of musical genres by era|Musical genres by era]] -
[[List of musical instruments|Musical instruments]] -
[[List of musical instruments by Hornbostel–Sachs number|Musical instruments by Hornbostel–Sachs number]] -
[[List of musical scales and modes|Musical scales and modes]] -
[[List of musical symbols|Musical symbols]] -
[[List of musicology topics|Musicology topics]] -
[[List of opera librettists|Opera librettists]]
[[List of ornaments|Ornaments]] -
[[List of performances of French grand operas at the Paris Opéra|Performances of French grand operas at the Paris Opéra]] -
[[List of period instruments|Period instruments]] -
[[List of pipe organ stops|Pipe organ stops]] -
[[List of pitch intervals|Pitch intervals]] -
[[List of principal conductors by orchestra|Principal conductors by orchestra]] -
[[List of program music]] -
[[List of quarter tone pieces|Quarter tone pieces]] -
[[List of styles of music: A–F|Styles of music: A–F]] -
[[List of styles of music: G–M|Styles of music: G–M]] -
[[List of styles of music: N–R|Styles of music: N–R]] -
[[List of styles of music: S–Z|Styles of music: S–Z]] -
[[List of symphony orchestras|Symphony orchestras]] -
[[List of tone rows and series|Tone rows and series]] 


[[Category:Wikipedia indexes|Music]]
[[Category:Music-related lists|*Index of music articles]]
[[Category:Music]]

</doc>
<doc id="706956" url="https://en.wikipedia.org/wiki?curid=706956" title="Nuremberg Charter">
Nuremberg Charter

The Charter of the International Military Tribunal – Annex to the Agreement for the prosecution and punishment of the major war criminals of the European Axis (usually referred to as the Nuremberg Charter or London Charter) was the decree issued by the European Advisory Commission on 8 August 1945 that set down the rules and procedures by which the Nuremberg trials were to be conducted.

The charter stipulated that crimes of the European Axis Powers could be tried. Three categories of crimes were defined: crimes against peace, war crimes, and crimes against humanity. Article 8 of the charter also stated that holding an official position was no defense to war crimes. Obedience to orders could only be considered in mitigation of punishment if the Tribunal determined that justice so required.

The criminal procedure used by the Tribunal was closer to civil law than to common law, with a trial before a panel of judges rather than a jury trial and with wide allowance for hearsay evidence. Defendants who were found guilty could appeal the verdict to the Allied Control Council. In addition, they would be permitted to present evidence in their defense and to cross-examine witnesses.

The Charter was developed by the European Advisory Commission under the authority of the Moscow Declaration: Statement on Atrocities, which was agreed at the Moscow Conference (1943). It was drawn up in London, following the surrender of Germany on VE Day. It was drafted by Robert H. Jackson, Robert Falco, and Iona Nikitchenko of the European Advisory Commission and issued on 8 August 1945.

The Charter and its definition of crimes against peace was also the basis of the Finnish law, approved by the Finnish parliament on 11 September 1945, that enabled the war-responsibility trials in Finland.

The Agreement for the prosecution and punishment of the major war criminals of the European Axis and the annexed Charter were formally signed by France, the Soviet Union, the United Kingdom, and the United States on 8 August 1945. The Agreement and Charter were subsequently ratified by 20 other Allied states.




</doc>
<doc id="11205710" url="https://en.wikipedia.org/wiki?curid=11205710" title="Amnesty law">
Amnesty law

An amnesty law is any law that retroactively exempts a select group of people, usually military leaders and government leaders, from criminal liability for crimes committed.
Most allegations involve human rights abuses and crimes against humanity.

Many countries have been plagued by revolutions, coups and civil war. After such turmoil the leaders of the outgoing regime that want, or are forced, to restore democracy in their country are confronted with possible litigation regarding the "counterinsurgency" actions taken during their reign. It is not uncommon for people to make allegations of human rights abuse and crimes against humanity. To overcome the hazard of facing prosecution, many countries have absolved those involved of their alleged crimes.

Amnesty laws are often also equally problematic to the opposing side as a cost-benefit problem: Is bringing the old leadership to justice worth extending the conflict or rule of the previous regime, with an accompanying increase in suffering and casualties, as the old regime refuses to let go of power?

Victims, their families and human rights organisations—"e.g.," Amnesty International, Human Rights Watch, Humanitarian Law Project—have opposed such laws through demonstrations and litigation, their argument being that an amnesty law violates local constitutional law and international law by upholding impunity.

Providing amnesty for “international crimes”—which include crimes against humanity, war crimes and genocide—is increasingly considered to be prohibited by international law. This understanding is drawn from the obligations set out in human rights treaties, the decisions of international and regional courts and the law emerging from long-standing state practice (customary international law). International, regional and national courts have increasingly overturned general amnesties. And recent peace agreements have largely avoided granting amnesty for serious crimes. With that in mind, the International Criminal Court was established to ensure that perpetrators do not evade command responsibility for their crimes should the local government fail to prosecute.

The Belfast Guidelines on Amnesty and Accountability set out a framework to evaluate the legality and legitimacy of amnesties in accordance with the multiple legal obligations faced by states undergoing conflict or political transition. They have been collectively authored by a group of international human rights and conflict resolution experts led by Louise Mallinder and Tom Hadden at the Transitional Justice Institute.

Afghanistan has adopted a law precluding prosecution for war crimes committed in conflicts in previous decades.

The Afghan government adopted the Action Plan for Peace, Justice, and Reconciliation in December 2005, and hotly debated the plan's focus on criminal accountability. Later, Parliament adopted a bill that provided a nearly blanket amnesty for all those involved in the Afghan conflict.

The drafting of the amnesty bill was pioneered by some of the former commanders known to have committed human rights abuses and who felt threatened by the sudden emphasis on accountability. 
Although this bill was never formally recognized as law, it has had major political significance, serving as a clear signal of some human rights violators’ continuing power.

A decree by the President in 2006 makes prosecution impossible for human rights abuses, and even muzzle open debate by criminalizing public discussion about the nation's decade-long conflict.

The National Commission for Forced Disappearances (CONADEP), led by writer Ernesto Sabato, was created in 1983. Two years later, the "Juicio a las Juntas" (Trial of the Juntas) largely succeeded in proving the crimes of the various "juntas" which had formed the self-styled National Reorganization Process. Most of the top officers who were tried were sentenced to life imprisonment: Jorge Rafael Videla, Emilio Eduardo Massera, Roberto Eduardo Viola, Armando Lambruschini, Raúl Agosti, Rubén Graffigna, Leopoldo Galtieri, Jorge Anaya and Basilio Lami Dozo. However, Raúl Alfonsín's government voted two amnesty laws in order to avoid the escalation of trials against militaries involved in human rights abuses: the 1986 "Ley de Punto Final" and the 1987 "Ley de Obediencia Debida". President Carlos Menem then pardoned the leaders of the "junta" and the surviving commanders of the armed leftist guerrilla organizations in 1989–1990. Following persistent activism by the Mothers of the Plaza de Mayo and other associations, the amnesty laws were overturned by the Argentine Supreme Court nearly twenty years later, in June 2005. However, the ruling wasn't applied to the guerrilla leaders, who remained at large.

In the 1980s, incompetent economic management and ballooning domestic graft, including the draining of funds from parastatals, combined with a continent-wide economic crisis, effectively bankrupted the economy. The government turned to the Bretton Woods institutions for support, which required the implementation of unpopular economic austerity measures. In 1988, when France refused to meet the budgetary shortfall, the three main banks, all state-owned, collapsed and the government was unable to pay teachers, civil servants and soldiers their salaries, nor students their grants. This caused domestic opposition to mushroom, rendering the country ‘virtually ungovernable’.20 The World Bank and the InternationalMonetary Fund (IMF) refused to provide emergency assistance because of Benin's failure to adhere to prior agreements.21 Kérékou convened a national conference to discuss the country's future course, bringing together representatives of all sectors of Beninese society, including ‘teachers, students, the military, government officials, religious authorities, non-governmental organizations, more than 50 political parties, ex-presidents, labor unions, business interests, farmers, and dozens of local development organizations’.22 Kérékou believed that he could retain control of the 488 delegates. Instead, when it met in February 1990, the convention declared itself sovereign, redefined the powers of the presidency, reducing Kérékou to a figurehead role, and appointed Nicéphore Soglo, a former World Bank staff member, to act as executive prime minister. In exchange for a full pardon for any crimes he may have committed, Kérékou peacefully ceded power. By March 1991, the Beninese electorate had ratified a new constitution and democratically elected Soglo president.

In 1979, Brazil's military dictatorship—which suppressed young political activists and trade unionists—passed an amnesty law. This law allowed exiled activists to return, but was also used to shield human rights violators from prosecution. Perpetrators of human rights abuses during Brazil's 1964 to 1985 military dictatorship have yet to face criminal justice.
In 2010, the Inter-American Court of Human Rights declared Brazil's amnesty law illegal because of the provisions that “prevent the investigation and punishment of serious human rights violations” and ordered the nation to begin exploring the gross human rights violations of the past. 
On April 9, 2014, a bill (#237/2013) that would modify this law to exclude human rights violations committed by state agents was approved by the Brazilian senate.

When Augusto Pinochet was arrested in London as part of a failed extradition to Spain, which was demanded by magistrate Baltasar Garzón, a bit more information concerning Condor was revealed. One of the lawyers who asked for his extradition talked about an attempt to assassinate Carlos Altamirano, leader of the Chilean Socialist Party: Pinochet would have met Italian terrorist Stefano Delle Chiaie in Madrid in 1975, during Franco's funeral, in order to have him murdered. But as with Bernardo Leighton, who was shot in Rome in 1975 after a meeting the same year in Madrid between Stefano Delle Chiaie, Michael Townley and anti-Castrist Virgilio Paz Romero, the plan ultimately failed.

Chilean judge Juan Guzmán Tapia would eventually make jurisprudence concerning "permanent kidnapping" crime: since the bodies of the victims could not be found, he deemed that the kidnapping may be said to continue, therefore refusing to grant to the military the benefices of the statute of limitation. This helped indict Chilean militaries who were benefitting from a 1978 self-amnesty decree.

In November 2005 an amnesty law was adopted regarding offences committed between August 1996 and June 2003.

President Joseph Kabila put an Amnesty Law into effect in May 2009. This law forgives combatants for war-related violence in the eastern provinces of North and South Kivu committed between June 2003 and May 2009 – excluding genocide, war crimes international crimes against humanity. Although of limited temporal and geographic scope, by granting amnesty for many crimes perpetuated by rebel groups, Congolese armed forces, militias, and police, there is a risk that the law may perpetuate the DRC's culture of impunity.

Following the twelve-year-long civil war an amnesty law was passed in 1993. The law modified a previous amnesty law which was passed in 1992. In 2016, however, the amnesty law was overturned by the El Salvador Supreme Court.

The Indemnity and Oblivion Act was passed in 1660, as part of rebuilding during the English Restoration after the English Civil War. It was jokingly referred to as producing "indemnity for the King's enemies and oblivion for the King's friends".

The so-called "Alan Turing law" is a proposed amnesty law for men convicted of consensual homosexual sex prior to the passing of the Sexual Offences Act 1967. The Government has announced that the amnesty will be introduced in England and Wales as an amendment to the Policing and Crime Bill 2016.

An amnesty law for crimes perpetrated before March 28, 1991, was enacted in 1991 after which the militias (with the important exception of Hezbollah) were dissolved, and the Lebanese Armed Forces began to slowly rebuild themselves as Lebanon's only major non-sectarian institution.

On 14 June 1995, President Alberto Fujimori signed a bill granting amnesty for any human rights abuses or other criminal acts committed from May 1982 to 14 June 1995 as part of the counterinsurgency war by military, police, and civilians.

The amnesty laws created a new challenge for the human rights movement in Peru. They thwarted the demands for truth and justice that thousands of family members of victims of political violence have been making since the 1980s. Thus, after the fall of Alberto Fujimori in 2001, the Inter-American Court ruled that the amnesty laws 26.479 and 26.492 were invalid because they were incompatible with the American Convention on Human Rights. The court later specified that the ruling was applicable to all Peruvian cases.

A bill absolved anyone convicted for committing political crimes. Among them
those who were convicted of having assassinated a constitutional court judge in 1993.

On 7 July 1999, the "Lomé Peace Agreement" was signed. Along with a cease-fire agreement between the government of Alhaji Ahmad Tejan Kabbah and the Revolutionary United Front (RUF) it contained proposals to "expunge responsibility for all offences including international crimes, otherwise known as delict jus gentium such as crimes against humanity, war crimes, genocide, torture and other serious violations of international humanitarian law."

Following the end of apartheid South Africa decided not to prosecute but instead created the Truth and Reconciliation Commission (TRC). Its aim was to investigate and elucidate the crimes committed during the apartheid regime while not indicting in an attempt to make the alleged perpetrators more compliant to cooperate.

The TRC offered of “amnesty for truth” to perpetrators of human rights abuses during the apartheid era. This enabled abusers to confess their actions to the TRC in order to be granted amnesty. It aroused much controversy in the country and internationally.

In 1977, the first democratic government elected after Franco's death passed the Law 46/1977, of amnesty, which exempted of responsibility to everyone who committed any offence for political reasons prior to this date. This law allowed the commutation of sentences of those accused to attack the dictatorship and secured that those crimes committed during the Francoism would not be prosecuted.

During the War on Terror, the United States enacted the Military Commissions Act of 2006 in an attempt to regulate the legal procedures involving illegal combatants. Part of the act was an amendment which retroactively rewrote the War Crimes Act effectively making policy makers (i.e., politicians and military leaders) and those applying policy (i.e., CIA interrogators and U.S. soldiers) no longer subject to legal prosecution under U.S. law for acts defined as war crimes before the amendment was passed. Because of that, critics describe the MCA as an amnesty law for crimes committed in the War on Terror.

Uruguay granted the former military regime amnesty for the violations of human rights during their regime.



</doc>
<doc id="68762" url="https://en.wikipedia.org/wiki?curid=68762" title="Rome Statute of the International Criminal Court">
Rome Statute of the International Criminal Court

The Rome Statute of the International Criminal Court (often referred to as the International Criminal Court Statute or the Rome Statute) is the treaty that established the International Criminal Court (ICC). It was adopted at a diplomatic conference in Rome, Italy on 17 July 1998 and it entered into force on 1 July 2002. As of November 2019, 123 states are party to the statute. Among other things, the statute establishes the court's functions, jurisdiction and structure.

The Rome Statute established four core international crimes: genocide, crimes against humanity, war crimes, and the crime of aggression. Those crimes "shall not be subject to any statute of limitations". Under the Rome Statute, the ICC can only investigate and prosecute the four core international crimes in situations where states are "unable" or "unwilling" to do so themselves; the jurisdiction of the court is complementary to jurisdictions of domestic courts. The court has jurisdiction over crimes only if they are committed in the territory of a state party or if they are committed by a national of a state party; an exception to this rule is that the ICC may also have jurisdiction over crimes if its jurisdiction is authorized by the United Nations Security Council.

The Rome Statute established four core international crimes: (I) Genocide, (II) Crimes against humanity, (III) War crimes, and (IV) Crime of aggression. Following years of negotiation, aimed at establishing a permanent international tribunal to prosecute individuals accused of genocide and other serious international crimes, such as crimes against humanity, war crimes and crimes of aggression, the United Nations General Assembly convened a five-week diplomatic conference in Rome in June 1998 "to finalize and adopt a convention on the establishment of an international criminal court".

A five-week diplomatic conference was convened in Rome in June 1998 "to finalize and adopt a convention on the establishment of an international criminal court". On 17 July 1998, the Rome Statute was adopted by a vote of 120 to 7, with 21 countries abstaining. By agreement, there was no officially recorded vote of each delegation's vote regarding the adoption of the Rome Statute. Therefore, there is some dispute over the identity of the seven countries that voted against the treaty. It is certain that the People's Republic of China, Israel, and the United States were three of the seven because they have publicly confirmed their negative votes; India, Indonesia, Iraq, Libya, Qatar, Russia, Saudi Arabia, Sudan, and Yemen have been identified by various observers and commentators as possible sources for the other four negative votes, with Iraq, Libya, Qatar, and Yemen being the four most commonly identified. Israel's vote against was publicly declared as being due to the inclusion in the list of a war crimes of “the action of transferring population into occupied territory”.

On 11 April 2002, ten countries ratified the statute at the same time at a special ceremony held at the United Nations headquarters in New York City, bringing the total number of signatories to sixty, which was the minimum number required to bring the statute into force, as defined in Article 126. The treaty entered into force on 1 July 2002; the ICC can only prosecute crimes committed on or after that date. The statute was modified in 2010 after the Review Conference in Kampala, Uganda, but the amendments to the statute that were adopted at that time are not effective yet.

The Rome Statute is the result of multiple attempts for the creation of a supranational and international tribunal. At the end of the 19th century, the international community took the first steps towards the institution of permanent courts with supranational jurisdiction. With the Hague International Peace Conferences, representatives of the most powerful nations made an attempt to harmonize laws of war and to limit the use of technologically advanced weapons. After World War I and even more after the heinous crimes committed during World War II, it became a priority to prosecute individuals responsible for egregious crimes so serious that they needed to be exemplified by being referred to as "crimes against humanity". In order to re-affirm basic principles of democratic civilisation, the alleged criminals were not executed in public squares or sent to torture camps, but instead treated as criminals: with a regular trial, the right to defense and the presumption of innocence. The Nuremberg trials marked a crucial moment in legal history, and after that, some treaties that led to the drafting of the Rome Statute were signed.

UN General Assembly Resolution n. 260 9 December 1948, the Convention on the Prevention and Punishment of the Crime of Genocide, was the first step towards the establishment of an international permanent criminal tribunal with jurisdiction on crimes yet to be defined in international treaties. In the resolution there was a hope for an effort from the Legal UN commission in that direction. The General Assembly, after the considerations expressed from the commission, established a committee to draft a statute and study the related legal issues. In 1951 a first draft was presented; a second draft followed in 1955 but there were a number of delays, officially due to the difficulties in the definition of the crime of aggression, that were only solved with diplomatic assemblies in the years following the statute's coming into force. The geopolitical tensions of the Cold War also contributed to the delays.

Trinidad and Tobago asked the General Assembly in December 1989 to re-open the talks for the establishment of an international criminal court and in 1994 presented a draft Statute. The General Assembly created an "ad hoc" committee for the International Criminal Court and, after hearing the conclusions, a Preparatory Committee that worked for two years (1996–1998) on the draft. Meanwhile, the United Nations created the "ad hoc" tribunals for the former Yugoslavia (ICTY) and for Rwanda (ICTR) using statutes—and amendments due to issues raised during pre-trial or trial stages of the proceedings—that are quite similar to the Rome Statute.

During its 52nd session the UN General Assembly decided to convene a diplomatic conference for the establishment of the International Criminal Court, held in Rome 15 June–17 July 1998 to define the treaty, entered into force on 1 July 2002. This Rome Conference was attended by representatives from 161 member states, along with observers from various other organizations, intergovernmental organizations and agencies, and non-governmental organizations (including many human rights groups) and was held at the headquarters of the Food and Agriculture Organization of the United Nations, located about 4 km away from the Vatican (one of the states represented).

The states parties held a Review Conference in Kampala, Uganda from 31 May to 11 June 2010. The Review Conference adopted a definition of the crime of aggression, thereby allowing the ICC to exercise jurisdiction over the crime for the first time. It also adopted an expansion of the list of war crimes.

The Rome Statute outlines the ICC's structure and areas of jurisdiction. The ICC can prosecute individuals (but not states or organizations) for four kinds of crimes: genocide, crimes against humanity, war crimes, and the crime of aggression. These crimes are detailed in Articles 6, 7, 8, and 8 "bis" of the Rome Statute, respectively. They must have been committed after 1 July 2002, when the Rome Statute came into effect.

The ICC has jurisdiction over these crimes in three cases: first, if they took place on the territory of a State Party; second, if they were committed by a national of a State Party; or third, if the crimes were referred to the Prosecutor by the UN Security Council. The ICC may begin an investigation before issuing a warrant if the crimes were referred by the UN Security Council or if a State Party requests an investigation. Otherwise, the Prosecutor must seek authorization from a Pre-Trial Chamber of three judges to begin an investigation "proprio motu" (on its own initiative). The only type of immunity the ICC recognizes is that it cannot prosecute those under 18 when the crime was committed. In particular, no officials – not even a head of state – are immune from prosecution.

The Rome Statute established three bodies: the ICC itself, the Assembly of States Parties (ASP), and the Trust Fund for Victims. The ASP has two subsidiary bodies. These are the Permanent Secretariat, established in 2003, and an elected Bureau which includes a president and vice-president. The ICC itself has four organs: the Presidency (with mostly administrative responsibilities); the Divisions (the Pre-Trial, Trial, and Appeals judges); the Office of the Prosecutor; and the Registry (whose role is to support the other three organs). The functions of these organs are detailed in Part 4 of the Rome Statute.

Any amendment to the Rome Statute requires the support of a two-thirds majority of the states parties, and an amendment (except those amending the list of crimes) will not enter into force until it has been ratified by seven-eighths of the states parties. A state party which has not ratified such an amendment may withdraw with immediate effect. Any amendment to the list of crimes within the jurisdiction of the court will only apply to those states parties that have ratified it. It does not need a seven-eighths majority of ratifications.





</doc>
<doc id="1155020" url="https://en.wikipedia.org/wiki?curid=1155020" title="War Crimes Act of 1996">
War Crimes Act of 1996

The War Crimes Act of 1996 is a law that defines a war crime to include a "grave breach of the Geneva Conventions", specifically noting that "grave breach" should have the meaning defined in any convention (related to the laws of war) to which the United States is a party. The definition of "grave breach" in some of the Geneva Conventions have text that extend additional protections, but all the Conventions share the following text in common: "... committed against persons or property protected by the Convention: willful killing, torture or inhuman treatment, including biological experiments, willfully causing great suffering or serious injury to body or health."

The law applies if either the victim or the perpetrator is a national of the United States or a member of the U.S. Armed Forces. The penalty may be life imprisonment or death. The death penalty is only invoked if the conduct resulted in the death of one or more victims.

The act was passed with overwhelming majorities by the United States Congress and signed into law by President Bill Clinton.

The law criminalized breaches of the Geneva Conventions so that the United States could prosecute war criminals, specifically North Vietnamese soldiers who tortured U.S. military personnel during the Vietnam War. The Department of Defense "fully support[ed] the purposes of the bill," recommending that it be expanded to include a longer list of war crimes. Because the United States generally followed the Conventions, the military recommended making breaches by U.S. military personnel war crimes as well "because doing so set a high standard for others to follow." The bill passed by unanimous consent in the Senate and by a voice vote in the House, showing that it was entirely uncontroversial at the time.

Ten years later, the United States Supreme Court ruled in "Hamdan v. Rumsfeld" that Common Article 3 of the Geneva Conventions applied to the War on Terrorism, with the unstated implication that any interrogation technique that violated Common Article 3 constituted war crimes. The possibility that American officials and military personnel could be prosecuted for war crimes for committing the "outrages upon personal dignity, in particular humiliating and degrading treatment" prohibited by the Conventions led to a series of proposals to make such actions legal in certain circumstances, which resulted in the Military Commissions Act of 2006.

White House officials were concerned that they and other U.S. officials could be prosecuted under the War Crimes Act for the U.S. treatment of detainees after the September 11 attacks for violations of the Geneva Conventions. In a January 2002 memorandum to the president, then-White House Counsel Alberto Gonzales authored a controversial memo that explored whether Common Article 3 of the Geneva Conventions applied to Al Qaeda and Taliban combatants captured during the war in Afghanistan and held in detention facilities around the world, including Camp X-Ray in Guantanamo Bay, Cuba. The memo made several arguments both for and against providing Common Article 3's protections to Al Qaeda and Taliban combatants. He concluded that Common Article 3 was outdated and ill-suited for dealing with captured Al Qaeda and Taliban combatants. He described as "quaint" the provisions that require providing captured Al Qaeda and Taliban combatants "commissary privileges, scrip, athletic uniforms, and scientific instruments". He also argued that existing military regulations and instructions from the President were more than adequate to ensure that the principles of the Geneva Conventions would be applied. He also argued that undefined language in the Geneva Conventions, such as "outrages upon personal dignity" and "inhuman treatment", could make officials and military leaders subject to the War Crimes Act of 1996 if mistreatment was discovered.

The adoption of the Military Commissions Act of 2006 marked grave abuses of Common Article 3 to only include torture, cruel or inhumane treatment, murder, mutilation or maiming, intentionally causing serious bodily harm, rape, sexual assault or abuse, and the taking of hostages, thereby limiting the scope of the original law.




</doc>
<doc id="470452" url="https://en.wikipedia.org/wiki?curid=470452" title="War Crimes Act 1991">
War Crimes Act 1991

The War Crimes Act 1991 is an Act of the Parliament of the United Kingdom. It confers jurisdiction on courts in the United Kingdom to try people for war crimes committed in Nazi Germany or German-occupied territory during the Second World War by people who were not British citizens at the time, but have since become British citizens or residents. The legislation was enacted, as there then were no provisions to allow the extradition of British residents, or naturalised citizens to face trial for war crimes in third countries. Other countries, such as the United States have used civil, rather than criminal proceedings, to resolve this issue by revoking citizenship of suspects, facilitating their deportation.

The Act was rejected by the House of Lords, and so it was passed with the authority of only the House of Commons, under the provisions of the Parliament Acts 1911 and 1949. The Parliament Acts are rarely invoked: the War Crimes Act was only the fourth statute since 1911 enacted under their provisions, and the first since the Parliament Act 1949. The War Crimes Act remains the only time that the Parliament Acts were invoked by a Conservative government.

To date only one person, Anthony Sawoniuk, has been convicted under the Act. In 1999, he was sentenced to life imprisonment for murder during his involvement with the collaborationist Belarusian Auxiliary Police. He died in jail in 2005.

The first person to be charged, however, was fellow Belarusian officer, Symon Serafinowicz Sr. His trial commenced in 1997 for the murder of three unnamed Jews committed as during his role as Police chief in Mir. At this stage, he was in the advanced stages of dementia and was declared medically unfit. He died later that year.


</doc>
<doc id="5275960" url="https://en.wikipedia.org/wiki?curid=5275960" title="Coalition for International Justice">
Coalition for International Justice

The Coalition for International Justice (CIJ) was an international, non-profit organization based in both Washington D.C. and The Hague that supported the international war crimes tribunals for Rwanda and the former Yugoslavia, and criminal and transitional justice initiatives for East Timor, Sierra Leone, Cambodia, and Sudan. CIJ initiated and conducted advocacy and public education campaigns, targeting decision-makers in Washington and other capitals, media, and the public.

In the field, CIJ provided practical assistance on legal, technical, and outreach matters to the tribunals and other justice initiatives. From 2000-2003, CIJ conducted a substantial rule of law project in East Timor. Most recently, in July 2004, CIJ assembled an international team of professionals who conducted over 1,200 interviews in Chad with refugees who had fled the conflict in Darfur, Sudan.

On March 31, 2006, the Coalition for Justice closed operations. In a letter from its board of directors, they noted that CIJ "was not intended to become a permanent institution."


</doc>
<doc id="25717545" url="https://en.wikipedia.org/wiki?curid=25717545" title="Report of the International Commission on the Balkan Wars">
Report of the International Commission on the Balkan Wars

The Report of the International Commission to Inquire into the Causes and Conduct of the Balkan Wars is a document published in the Washington D.C. in 1914 by the Carnegie Endowment for International Peace.

The International Commission consisted of university professors and other prominent individuals from France, Great Britain, United States, Germany, Austria and Russia. Among the members of the Commission there were three Nobel Prize winners.

The Commission went to the Balkans at the beginning of August 1913 and remained until the end of September. After returning to Paris all the material was processed and released in the form of a detailed report. The report speaks of the numerous violations of international conventions and war crimes committed during the Balkan Wars. The information collected was published by the Endowment in the early summer of 1914, but was soon overshadowed by the beginning of the First World War.



</doc>
<doc id="15115376" url="https://en.wikipedia.org/wiki?curid=15115376" title="Superior orders">
Superior orders

Superior orders, often known as the Nuremberg defense, just following orders, or by the German phrase Befehl ist Befehl ("an order is an order"), is a plea in a court of law that a person—whether a member of the military, law enforcement, a firefighting force, or the civilian population—not be held guilty for actions ordered by a superior officer or an official. 
The superior orders plea is often regarded as the complement to command responsibility. 

One of the most noted uses of this plea, or defense, was by the accused in the 1945–1946 Nuremberg trials, such that it is also called the "Nuremberg defense". The Nuremberg trials were a series of military tribunals, held by the main victorious Allies after World War II, most notable for the prosecution of prominent members of the political, military, and economic leadership of the defeated Nazi Germany. These trials, under the London Charter of the International Military Tribunal that set them up, established that the defense of superior orders was no longer enough to escape punishment, but merely enough to lessen punishment. 

Historically, the plea of superior orders has been used both before and after the Nuremberg Trials, with a notable lack of consistency in various rulings. 

Apart from the specific plea of superior orders, discussions about how the general concept of superior orders ought to be used, or ought not to be used, have taken place in various arguments, rulings and statutes that have not necessarily been part of "after the fact" war crimes trials, strictly speaking. Nevertheless, these discussions and related events help to explain the evolution of the specific plea of superior orders and the history of its usage.

In 1474, in the trial of Peter von Hagenbach by an ad hoc tribunal of the Holy Roman Empire, the first known "international" recognition of commanders' obligations to act lawfully occurred. Hagenbach offered the defense that he was just following orders, but this defense was rejected and he was convicted of war crimes and beheaded.

Specifically, Hagenbach was put on trial for atrocities committed under his command but not by him directly, during the occupation of Breisach. This was the earliest modern European example of the doctrine of command responsibility. Since he was convicted for crimes "he as a knight was deemed to have a duty to prevent", Hagenbach defended himself by arguing that he was only following orders from the Duke of Burgundy, Charles the Bold, to whom the Holy Roman Empire had given Breisach.

During the Second Boer War, three Australian officers (Morant, Handcock and Witton) were charged and tried for a number of murders, including those of prisoners who had surrendered. A significant part of the defense was that they were acting under orders issued by Lord Kitchener to "take no prisoners". However, these orders were verbal, were denied by Kitchener and his staff, and could not be validated in court, resulting in a guilty verdict against all three men.

On June 4, 1921, the legal doctrine of superior orders was used during the German Military Trials that took place after World War I: One of the most famous of these trials was the matter of Lieutenant Karl Neumann, who was a U-boat captain responsible for the sinking of the hospital ship the "Dover Castle". Even though he frankly admitted to having sunk the ship, he stated that he had done so on the basis of orders supplied to him by the German Admiralty and so he could not be held liable for his actions. The Reichsgericht, then Germany's supreme court, acquitted him, accepting the defense of superior orders as a grounds to escape criminal liability. Further, that very court had this to say in the matter of superior orders: "... that all civilized nations recognize the principle that a subordinate is covered by the orders of his superiors."

Many accused of war crimes were acquitted on a similar defense, creating immense dissatisfaction among the Allies. This is thought to be one of the main causes for the specific removal of this defense in the August 8, 1945 London Charter of the International Military Tribunal. The removal has been attributed to the actions of Robert H. Jackson, a Justice of the United States Supreme Court, who was appointed Chief Prosecutor at the Nuremberg trials.

On the other hand, when the defendants could not reasonably claim that they did not know their orders were clearly illegal, the defense was ineffective. For instance, Lieutenants Dithmar and Boldt were ordered to fire on lifeboats, obeyed the order, and were found guilty in the same German Military Trials

On October 8, 1945, Anton Dostler was the first German general to be tried for war crimes by a US military tribunal at the Royal Palace in Caserta. He was accused of ordering the execution of 15 captured US soldiers of Operation Ginny II in Italy in March 1944. He admitted to ordering the execution but said that he could not be held responsible because he was following orders from his superiors. The execution of the prisoners of war in Italy, ordered by Dostler, was an implementation of Adolf Hitler's Commando Order of 1942, which required the immediate execution of all Allied commandos, whether they were in proper uniforms or not, without trial if they were apprehended by German forces. The tribunal rejected the defense of Superior Orders and found Dostler guilty of war crimes. He was sentenced to death and executed by a firing squad on December 1, 1945, in Aversa.

The Dostler case became a precedent for the principle that was used in the Nuremberg Trials of German generals, officials, and Nazi leaders beginning in November 1945: using superior orders as a defense does not relieve officers from responsibility of carrying out illegal orders and their liability to be punished in court. The principle was codified in Principle IV of the Nuremberg Principles, and similar principles were found in sections of the Universal Declaration of Human Rights.

In 1945 and 1946, during the Nuremberg trials the issue of superior orders again arose. Before the end of World War II, the Allies suspected such a defense might be employed and issued the London Charter of the International Military Tribunal (IMT), which explicitly stated that following an unlawful order is not a valid defense against charges of war crimes.

Thus, under Nuremberg Principle IV, "defense of superior orders" is not a defense for war crimes, although it might be a mitigating factor that could influence a sentencing authority to lessen the penalty. Nuremberg Principle IV states:

The fact that a person acted pursuant to order of his Government or of a superior does not relieve him from responsibility under international law, provided a moral choice was in fact possible to him.

During the Nuremberg Trials, Wilhelm Keitel, Alfred Jodl, and other defendants unsuccessfully used the defense. They contended that while they knew Adolf Hitler's orders were unlawful, or at least had reason to believe they were unlawful, their place was not to question, but to obey. They claimed they were compelled to do so by the "Führerprinzip" (leader principle) that governed the Nazi regime, as well as their own oath of allegiance to Hitler. In most cases, the tribunal found that the defendants' offenses were so egregious that obedience to superior orders could not be considered a mitigating factor.

The German military law since 1872 said that while the superior is ("solely") responsible for his order, the subordinate "is" to be punished for his participation in it if he either transgressed the order on his own account, or if he knew the order to be criminal. For many of their offenses (e.g., killing a non-combatant without trial) the Nazis did not bother to (or were too reluctant to) legalize them by a formal law, so, the prosecutors at Nuremberg could have argued that the defendants broke German law to begin with. However, this line of argumentation was infrequently used in the trials.

The trials gained so much attention that the "superior orders defense" has subsequently become interchangeable with the label "Nuremberg defense", a legal defense that essentially states that defendants were "only following orders" ("Befehl ist Befehl", literally "an order is an order") and so are not responsible for their crimes.

However, US General Telford Taylor, who had served as Chief Counsel for the United States during the Nuremberg trials, employed the term "Nuremberg defense" in a different sense. He applied it not to the defense offered by the Nuremberg defendants but to a justification put forward by those who refused to take part in military action (specifically America's involvement in the Vietnam War) that they believed to be criminal.

The defense of superior orders again arose in the 1961 trial of Nazi war criminal Adolf Eichmann in Israel, as well as the trial of Alfredo Astiz of Argentina, the latter responsible for a large number of disappearances and kidnappings that took place during that country's last civil-military dictatorship (1976-1983), which forced a State-sponsored terrorism upon the population, resulting in what (to several sources) amounted to a genocide.

In the 1950s and 1960s the use of "Befehlsnotstand" (), a concept in which a certain action is ordered which violates law but where the refusal to carry out such an order would lead to drastic consequences for the person refusing to carry out the order, as a defence in war crimes trials in Germany was quite successful as it protected the accused from punishment. With the formation of the Central Office of the State Justice Administrations for the Investigation of National Socialist Crimes this changed as a historical research by the organisation revealed that refusing an unlawful order did not result in punishment.

In 1957, the Israeli legal system established the concept of a 'blatantly illegal order' to explain when a military order (or in general, a security-related order) should be followed, and when an order "must not" be followed. The concept is explained in 1957 by the infamous Kafr Qasim massacre ruling.

The Kafr Qasim trial considered for the first time the issue of when Israeli security personnel are required to disobey illegal orders. The judges decided that soldiers do not have the obligation to examine each and every order in detail as to its legality, nor were they entitled to disobey orders merely on a subjective feeling that they might be illegal. On the other hand, some orders were manifestly illegal, and these must be disobeyed. Judge Benjamin Halevy's words, still much-quoted today, were that "The distinguishing mark of a manifestly illegal order is that above such an order should fly, like a black flag, a warning saying: 'Prohibited!'"

Captain (res.) Itai Haviv, a signatory of the 'courage to refuse' letter of 2002 tells of his unhappiness about his service for the Israeli Defense Forces (IDF) and says "For 35 years a black flag was proudly hanging over our heads, but we have refused to see it". A translation note explains the "Black Flag" principle but adds "In the 45 years that passed since [the ruling], not even a single soldier was protected by a military court for refusing to obey a command because it was a 'black flag' command."

Following the My Lai Massacre in 1968, the defense was employed during the court martial of William Calley. Some have argued that the outcome of the My Lai Massacre courts martial was a reversal of the laws of war that were set forth in the Nuremberg and Tokyo War Crimes Tribunals. Secretary of the Army Howard Callaway was quoted in the "New York Times" as stating that Calley's sentence was reduced because Calley believed that what he did was a part of his orders. Calley used the exact phrase "just following orders" when another American soldier, Hugh Thompson, confronted him about the ongoing massacre.

In "United States v. Keenan", the accused was found guilty of murder after he obeyed an order to shoot and kill an elderly Vietnamese citizen. The Court of Military Appeals held that "the justification for acts done pursuant to orders does not exist if the order was of such a nature that a man of ordinary sense and understanding would know it to be illegal". The soldier who gave the order, Corporal Luczko, was acquitted by reason of insanity.

The provision containing the superior orders defense can be found as a defense to international crimes in the Rome Statute of the International Criminal Court. (The Rome Statute was agreed upon in 1998 as the foundational document of the International Criminal Court, established to try those individuals accused of serious international crimes.) Article 33, titled "Superior orders and prescription of law",
states:

There are two interpretations of this Article:


Nuremberg Principle IV, and its reference to an individual's responsibility, was at issue in Canada in the case of "Hinzman v. Canada." Jeremy Hinzman was a U.S. Army deserter who claimed refugee status in Canada as a conscientious objector, one of many Iraq War resisters. Hinzman's lawyer, (at that time Jeffry House), had previously raised the issue of the legality of the Iraq War as having a bearing on their case. The Federal Court ruling was released on March 31, 2006, and denied the refugee status claim. In the decision, Justice Anne L. Mactavish addressed the issue of personal responsibility:
An individual must be involved at the policy-making level to be culpable for a crime against peace ... the ordinary foot soldier is not expected to make his or her own personal assessment as to the legality of a conflict. Similarly, such an individual cannot be held criminally responsible for fighting in support of an illegal war, assuming that his or her personal war-time conduct is otherwise proper.
On Nov 15, 2007, a quorum of the Supreme Court of Canada made of Justices Michel Bastarache, Rosalie Abella, and Louise Charron refused an application to have the Court hear the case on appeal, without giving reasons.

In June 2006, during the Iraq War, Ehren Watada refused to go to Iraq on account of his belief that the Iraq war was a crime against peace (waging a war of aggression for territorial aggrandizement), which he believed could make him liable for prosecution under the command responsibility doctrine. In this case, the judge ruled that soldiers, in general, are not responsible for determining whether the order to go to war itself is a lawful order – but are only responsible for those orders resulting in a specific application of military force, such as an order to shoot civilians, or to treat POWs inconsistently with the Geneva Conventions. This is consistent with the Nuremberg defense, as only the civilian and military principals of the Axis were charged with crimes against peace, while subordinate military officials were not so charged. It is often the case in modern warfare that while subordinate military officials are not held liable for their actions, neither are their superiors, as was the case with Calley's immediate superior Captain Ernest Medina.

Based on this principle, international law developed the concept of individual criminal liability for war crimes, which resulted in the current doctrine of command responsibility.

Note: Yellow rows indicate the use of the "precise" plea of Superior Orders in a war crimes trial - as opposed to events regarding the "general" concept of Superior Orders.

The superior orders defense is still used with the following rationale in the following scenario: An "order" may come from one's superior at the level of "national" law. But according to Nuremberg Principle IV, such an order is sometimes "unlawful" according to "international" law. Such an "unlawful order" presents a legal dilemma from which there is no legal escape: On one hand, a person who "refuses" such an unlawful order faces the possibility of legal punishment "at the national level" for refusing orders. On the other hand, a person who "accepts" such an unlawful order faces the possibility of legal punishment "at the international level" (e.g. Nuremberg Trials) for committing unlawful acts.

Nuremberg Principle II responds to that dilemma by stating: "The fact that internal law does not impose a penalty for an act which constitutes a crime under international law does not relieve the person who committed the act from responsibility under international law."

The above scenario might present a "legal" dilemma, but Nuremberg Principle IV speaks of "a "moral" choice" as being just as important as "legal" decisions: It states: "The fact that a person acted pursuant to order of his Government or of a superior does not relieve him from responsibility under international law, provided a "moral" choice was in fact possible to him".

In "moral choices" or ethical dilemmas an ethical decision is often made by appealing to a "higher ethic" such as ethics in religion or secular ethics. One such "higher ethic" found in many religions and in secular ethics, is the "ethic of reciprocity", or "Golden Rule". It states that one has a right to just treatment, and therefore has a reciprocal responsibility to ensure justice for others. "Higher ethics", such as those, could be used by an individual to solve the "legal" dilemma presented by the superior orders defense.

Although messengers are not usually responsible for the message that their superior sends with them, the Babylonian Talmud (3rd to 5th century corpus of Jewish law) states, "There is no messenger in a case of sin." Joseph Telushkin interprets precept to mean that "if a person is sent to perform an evil act, he cannot defend his behavior by saying he was only acting as another's messenger. ... [T]he person who carries out the evil act bears responsibility for the evil he or she does." This is because God's law (i.e. morality) supersedes human law.

Another argument against the use of the superior orders defense is that it does not follow the traditional legal definitions and categories established under criminal law. Under criminal law, a principal is any actor who is primarily responsible for a criminal offense. Such an actor is distinguished from others who may also be subject to criminal liability as accomplices, accessories or conspirators. (See also the various degrees of liability: absolute liability, strict liability, and mens rea.)

The common argument in this matter, is that every individual under orders should be bound by law to immediately relieve of command a superior officer who gives an obviously unlawful order to their troops. This represents a rational check to be put in place versus organizational command hierarchies.

Nuremberg Principle IV, the international law that counters the superior orders defense, is legally supported by the jurisprudence found in certain articles in the Universal Declaration of Human Rights that deal indirectly with conscientious objection. It is also supported by the principles found in paragraph 171 of the Handbook on Procedures and Criteria for Determining Refugee Status, which was issued by the Office of the United Nations High Commissioner for Refugees (UNHCR). Those principles deal with the conditions under which conscientious objectors can apply for refugee status in another country if they face persecution in their own country for refusing to participate in an illegal war.





</doc>
<doc id="15164989" url="https://en.wikipedia.org/wiki?curid=15164989" title="Convention on the Non-Applicability of Statutory Limitations to War Crimes and Crimes Against Humanity">
Convention on the Non-Applicability of Statutory Limitations to War Crimes and Crimes Against Humanity

The Convention on the Non-Applicability of Statutory Limitations to War Crimes and Crimes Against Humanity was adopted and opened for signature, ratification and accession by United Nations General Assembly resolution 2391 (XXIII) of 26 November 1968.
Pursuant to the provisions of its Article VIII (90 days following the deposit of the tenth ratification), it came into force on 11 November 1970.

The Convention provides that no signatory state may apply statutory limitations to:

As of January 2015, the Convention has 55 state parties, which includes 54 UN member states and the State of Palestine.



</doc>
<doc id="12640235" url="https://en.wikipedia.org/wiki?curid=12640235" title="War crimes trial">
War crimes trial

A war crimes trial is the trial of persons charged with criminal violation of the laws and customs of war and related principles of international law committed during armed conflict.

The trial of Peter von Hagenbach by an ad hoc tribunal of the Holy Roman Empire in 1474, was the first “international” war crimes trials and also of command responsibility. Hagenbach was put on trial for atrocities committed during the occupation of Breisach, found guilty, and beheaded. Since he was convicted for crimes, "he as a knight was deemed to have a duty to prevent", although Hagenbach defended himself by arguing that he was only following orders from the Duke of Burgundy, Charles the Bold, to whom the Holy Roman Empire had given Breisach.

In 1865, Henry Wirz, a Confederate officer, was held accountable and hanged for appalling conditions at Andersonville Prison where many Union soldiers died during the American Civil War.

After World War I, a small number of German personnel were tried by a German court in the Leipzig War Crimes Trials for crimes allegedly committed during that war.

Article 227 of the Treaty of Versailles, the peace treaty between Germany and the Allied Powers after the First World War, “publicly arraign[ed] William II of Hohenzollern, formerly German Emperor, for a supreme offence against international morality and the sanctity of treaties.” The former Kaiser had escaped to the Netherlands, however, and despite demands for his extradition having been made, the Dutch refused to surrender him, and he was not brought to trial. Germany, as a signatory to the treaty, thus was placed on notice as to what might occur in the event of a subsequent war.
After World War II, the phrase referred usually to the trials of German and Japanese leaders in courts established by the victorious Allied nations.

The most important of these trials were held in Nuremberg, Germany, under the authority of two legal instruments. One, the London Charter was signed by representatives of the United States, Great Britain, France, and the USSR in London on August 8, 1945; the other, Law No. 10, was promulgated by the Allied Control Council in Berlin on December 20, 1945.

The London Charter provided for the establishment of the International Military Tribunal, composed of one judge and one alternate judge from each of the signatory nations, to try war criminals. Under the London Charter, the crimes charged against defendants fell into three categories: crimes against peace, that is, crimes involving the planning, initiating, and waging a war of aggression; war crimes, that is, violations of the laws and customs of war as embodied in the Hague Conventions and generally recognized by military forces of civilized nations; and crimes against humanity, such as the extermination of racial, ethnic, and religious groups and other such atrocities against civilians.

On October 8, 1945, Anton Dostler was the first German general to be tried for war crimes by a U.S. military tribunal at the Royal Palace of Caserta in Caserta. He was accused of ordering the killing of 15 captured U.S. soldiers of Operation Ginny II in Italy in March 1944. He admitted into ordering the execution but said that he cannot be held responsible because he was just following orders from his superiors. The execution of 15 U.S. prisoners of war in Italy ordered by Dostler was an implementation of Hitler's Commando Order of 1942 which required the immediate execution of all Allied commandos, whether in proper uniforms or not, without trial if apprehended by German forces. The tribunal rejected the defense of Superior Orders and found Dostler guilty of war crimes. He was sentenced to death and executed by a firing squad on December 1, 1945, in Aversa.

The Dostler case became precedent for the Nuremberg trials of German generals, officials, and Nazi leaders beginning in November 1945 that using Superior orders as a defense does not relieve officers from responsibility of carrying out illegal orders and liable to be punished in court. This principle was codified in Principle IV of the Nuremberg Principles and similar principle were found in sections of the Universal Declaration of Human Rights.

On October 18, 1945, the chief prosecutors lodged an indictment with the tribunal charging 24 individuals with a variety of crimes and atrocities, including the deliberate instigation of aggressive wars, extermination of racial and religious groups, murder and mistreatment of prisoners of war, and the murder, mistreatment, and deportation of hundreds of thousands of inhabitants of countries occupied by Germany during the war.

Among the accused were the Nationalist Socialist leaders Hermann Göring and Rudolf Hess, the diplomat Joachim von Ribbentrop, the munitions maker Gustav Krupp von Bohlen und Halbach, Marshal Wilhelm Keitel, Grand Admiral Erich Raeder and 18 other military leaders and civilian officials. Seven organizations that formed part of the basic structure of the Nazi government were also charged as criminal. These organizations included the SS (Schutzstaffel, "Defense Corps"), the Gestapo (Geheime Staatspolizei, "Secret State Police"), and the SA (Sturmabteilung, "Storm Troops"), as well as the General Staff and High Command of the German armed forces.

The trial began on November 20, 1945. Much of the evidence submitted by the prosecution consisted of original military, diplomatic, and other government documents that fell into the hands of the Allied forces after the collapse of the German government.

The judgment of the International Military Tribunal was handed down on September 30-October 1, 1946. Among notable features of the decision was the conclusion, in accordance with the London Agreement, that to plan or instigate an aggressive war is a crime under the principles of international law. The tribunal rejected the contention of the defense that such acts had not previously been defined as crimes under international law and that therefore the condemnation of the defendants would violate the principle of justice prohibiting ex post facto punishments. As with the Dostler case, it also rejected the contention of a number of the defendants that they were not legally responsible for their acts because they performed the acts under the orders of superior authority, stating that ""the true test . . . is not the existence of the order but whether moral choice (in executing it) was in fact possible"."

With respect to war crimes and crimes against humanity, the tribunal found overwhelming evidence of a systematic rule of violence, brutality, and terrorism by the German government in the territories occupied by its forces. Millions of persons were murdered in Nazi concentration camps, many of which were equipped with gas chambers for the extermination of Jews, Gypsies, and members of other ethnic or religious groups. Under the slave-labor policy of the German government, at least 5 million persons had been forcibly deported from their homes to Germany. Many of them died because of inhumane treatment. The tribunal also found that atrocities had been committed on a large scale and as a matter of official policy.

Of the seven indicted organizations, the tribunal declared criminal the Leadership Corps of the National Socialist Party, the SS, the SD (Sicherheitsdienst, "Security Service"), and the Gestapo.

In May 1993, during the Yugoslav Wars following the massive war crimes, and acts of "ethnic cleansing" in the former Yugoslavia by Bosnian-Serb forces, the United Nations established the International Criminal Tribunal for the Former Yugoslavia, to try war criminals of all nationalities. The crimes indicted included grave breaches of the Geneva Conventions, war crimes, crimes against humanity, and genocide; it was the first tribunal in which sexual assault was prosecuted as a war crime. The ICTY was the first international war crimes tribunal since the Nuremberg Trials. Ultimately, nearly 161 individuals were indicted in the ICTY: 68% of Serb ethnicity. Croatian-Serb, Bosnian-Serb, Serbian, and Bosnian-Croat officials were convicted of crimes against humanity, and Bosnian-Serb leaders of genocide.

In 1994, the UN opened the International Criminal Tribunal for Rwanda following the April–June genocide in that country of Hutu nationals.

The tribunals, while effective in prosecution of individuals, proved to be a costly venture, and exposed the need for a permanent tribunal, which was eventually known as the International Criminal Court.




</doc>
<doc id="2939738" url="https://en.wikipedia.org/wiki?curid=2939738" title="United States Senate Committee on the Philippines">
United States Senate Committee on the Philippines

The Committee on the Philippines was a standing committee of the United States Senate from 1899 to 1921. The committee was established by Senate resolution on December 15, 1899, to oversee administration of the Philippines, which Spain had ceded to the United States as part of the settlement of the Spanish–American War. The committee was established by Senate resolution on December 15, 1899, even though the peace treaty signed in Paris on December 10, 1898, had not yet been ratified.

In 1921, the Committee was terminated and jurisdiction over legislative matters concerning the Philippines was transferred to the newly created Committee on Territories and Insular Possessions.

At the time of the creation of the committee, the Philippines were in a state of civil turmoil that greatly concerned the Senate, where a debate raged between those who wished to extend U.S. sovereignty over the Filipinos and the anti-imperialists. Like the Committee on the Pacific Islands and Puerto Rico, the Committee on the Philippines focused on primarily on legal and economic matters, such as Philippine independence, administration of the islands by the U.S. Philippine Commission, and trade issues. Matters relating to the suppression of the Philippine insurrection were often referred to the Senate Foreign Relations Committee.


Henry Cabot Lodge, Republican of Massachusetts, was the committee's first chairman, serving until 1911. During this time, the committee was informally known as the "Lodge Committee." In 1902, under Chairman Lodge, the committee carried out an investigation into allegations of war crimes in the Philippine–American War. The hearings commenced on January 31, 1902 and adjourned on June 28, 1902. They were closed to the public, except for three press associations. The final report came to 3,000 pages.

A remark to a "Manila News" reporter by newly promoted Brigadier General Jacob H. Smith on November 4, 1901, triggered the hearings, which eventually led to Smith's own court-martial and conviction. Smith said that he intended to set the entire island of Samar ablaze, and would probably wipe out most of the population.

Senator George Frisbie Hoar had been demanding an investigation after increasing evidence of U.S. military war crimes in the Philippine–American War. Hoar introduced a resolution to establish a select committee to conduct the investigation on January 13, 1902. However, Chairman Lodge argued that the hearings would be better conducted by the existing Committee on the Philippines. Anti-imperialists in the Senate feared a whitewash, because Lodge had been avoiding investigating mounting allegations of war crimes so much so that the U.S. Senate Committee on the Philippines had been inactive for several months.

During the time of the committee investigation, the minority on the committee consisted of Democratic and Republican anti-imperialists, led by Senator Hoar, while the majority was dominated by imperialists, led by Chairman Lodge. Hearings often degenerated into shouting matches between the imperialists and anti-imperialists. Nothing came of the hearings.

Eugene Hale was the only other Republican to vote with Hoar against the Treaty of Paris

Governor William Howard Taft had arrived in the Philippines in June 1900 with the Taft Commission to set up a civilian government. Taft was the first to testify in the Lodge Committee. As a lawyer, Taft would be assumed to have been a safe witness, but he conceded under questioning that "the torturing of natives by so-called water-cure and other methods" had been used "on some occasions to extract information"..."There are some amusing instances of Filipinos who came in and said they would not say anything unless tortured; that they must have an excuse for what they proposed to say." As Miller writes, "Very few died from the water cure, a mild form of torture."

Taft was immediately followed by three pro-imperialist witnesses. General Robert P. Hughes, chief of staff to General Elwell S. Otis, testified for two weeks in March 1902. In his testimony, Hughes conceded that Filipino houses were burned indiscriminately as a strategy to eliminate shelters and hiding places for guerrillas and as a deterrent. During questioning, Senator Charles Dietrich followed up by asking Hughes to estimate the value of these houses. Hughes said they only took a few days to build, and cost between $1.50 and $4.00. Senator Joseph Rawlins continued the questioning:
"Rawlins:" If these shacks were of no consequence what was the utility of their destruction?<br>
"Hughes:" The destruction was a punishment. They permitted these people to come in there and conceal themselves and they gave no sign. It is always--<br>
"Rawlins:" The punishment in that case would fall, not upon the men, who could go elsewhere, but mainly upon the women and little children.<br>
"Hughes:" The women and children are part of the family, and where you wish to inflict a punishment you can punish the man probably worse in that way than in any other.<br>
"Rawlins:" But is that within the ordinary rules of civilized warfare? Of course you could exterminate the family which would be still worse punishment.<br>
"Hughes:" These people are not civilized.<br>
"Rawlins:" But is that within the ordinary rules of civilized warfare?<br>
"Hughes:" No; I think it is not.<br>
"Dietrich:" In order to carry on civilized warfare both sides have to engage in such warfare.<br>
"Hughes:" Yes sir; certainly that is the point, I think that if I am allowed to go on I will come to a place where I shall have something to say that will bear directly on the subject.<br>
Senator Hale commented that the war had become less and less civilized with each successive commander, to which Hughes agreed saying "from summer to summer, the conduct of the war was sterner, stiffer, as you call it."
David Prescott Barrows, school director in the Philippines testified, claiming that anti-imperialist factions in the press had grossly distorted the situation. For example, concentration camps and the water cure were explained in the press as "more terrible than they are." He claimed Filipinos in the camps were "there of their own volition", for they "are pleased with it, because they are permitted to lead an easier life--much easier than at home." He went on to claim that alleged torture via the water cure "injured no one." While stating his belief that the natives had benefited from the war, Barrows stated he did not "wish to assent to the proposition that war is a good thing ... but where you have a war existing, it is, I think, better to go ahead and pursue it rigorously and finish it."
A fourth witness, General Elwell Stephen Otis testified the week of March 20, claiming here had been no warfare in the Philippines for the past two years. Senator Hale questions that statement, saying "there have been a good many fights since." Otis alleged any such fighting was due to "robbers", and that he and his men "were laughed at by the Spaniards and European officers for the humanity that we exercised." The committee proceeded to take a two-week break before continuing with hearings.

Major Cornelius Gardener, a West Point graduate serving as provincial governor of Tayabas, the province next to Batangas, submitted a report to the committee, which Chairman Lodge laid before the committee on April 10, 1902.

The Committee on the Philippines refused to subpoena Major Cornelius Gardener in a May 1, party-line vote. Committee members Senator Thomas Patterson decried this move in a speech on the Senate floor. Senator Benjamin Tillman, a Democrat from South Carolina, similarly objected to this move, claiming information was being "smothered."

Democrats on the committee pressed Lodge to call as witnesses Emilio Aguinaldo, the Filipino general and independence leader, and several others they thought necessary for the committee to hear. In addition to Mr. Aguinaldo, Mr. Rawlings proposed calling Apolinario Mabini, one of Aguinaldo's principal advisors; Sixto Lopez, an advocate for Philippine independence; Judge Pío del Pilar, General Torres, Howard W. Bray, an Englishman, who has spent many years in the Philippines, Robert M. Collins, and Harold Martin, both Associated Press correspondents. The committee refused this request as well as one that would have sent a subcommittee to the Philippines to collect testimony.

Instead, Mr. Lodge subpoenaed several veterans from a so-called "safe list" supplied by Secretary of War Elihu Root. However, when the soldiers appeared, they began to lecture the committee on the necessity of shooting and burning all Filipinos because of their "inability to appreciate human kindness."

Sergeant L. E. Hallock, Private William J. Gibbs, George C. Boardman, Captain Lee Hall, Richard Thomas O'Brien all testified to what they had seen during their service in the Philippines, including torture of Filipino prisoners, including use of the water cure, murder of natives, and other harassment.

Chairman Lodge countered with details of the murder of Private O'Herne. The witness said that in June 1900, O'Herne, with two other members of the company, had been sent to Iloilo for mail, and that on their return, on June 30, they were ambushed by 100 natives, and O'Herne's companions captured. O'Herne had made a dash to get away, and after escaping from the attacking party, had fallen in with other natives supposed to be friendly, but that instead of proving to be so they had devoted the entire next day to his torture and death, beginning at daylight by cutting him with bolos and then roasting him all day by a slow fire, not finishing up until night. All these details had, the witness said, been gathered from the confessions of the men to whom they had given the cure. Sergeant Hallock described the torture of around a dozen natives at the town of Leon, Panay. He said they were captured and tortured in order to secure information of the murder of Private O'Herne.

Corporal Richard O'Brien, testified he had been present at Igbaras when the water cure was administered to the Presidente (or chief) of that town. "There was a Spanish woman in town--a woman of education--who was attacked by the American officers." The witness said he could not give the names of the officers, adding that he had not witnessed the incident, but that the woman's husband was his authority for the statement." O'Brien further testified that there was an "unwritten law out there to take no prisoners." He said "dum dum" bullets, or expanding bullets, were issued in the regular way with other ammunition. He had seen them strike a man and take the top of this head off.

General MacArthur testified before the committee twice. On April 13, he initially discussed the short war with the Spaniards and the American cooperation with the Filipinos. Then later that month, he testified again, regarding the capture of Emilio Aguinaldo. The General testified he had used deception to capture Mr. Aguinaldo, saying "I am responsible in that matter in every way and particular. It was one of the deceptions frequently practiced in war, and whatever deception attached thereto, I take." He attributed the plan to Gen. Funston, but said he (MacArthur) was responsible for approving the plant. However, he insisted doing so did not violate the rules of civilized warfare. MacArthur also distanced himself from any alleged orders of General Jacob H. Smith to turn Samar a howling wilderness.

MacArthur said that absolute chaos would result should the Filipinos be given complete independence and the United States entirely withdraw from the islands. Aguinaldo also had told him it would be impossible at this stage of their evolution for his own people to establish a stable independent Government. He said Aguinaldo was at the time of the conversation a "qualified prisoner", but that there was no coercion or duress resorted to extract the statement.

In regards to the death toll in the Philippines, he said, "The destruction is simply incident to war, and of course embraces a very small percentage of the total population, which is dense." In response, Senator Patterson noted that the death toll in one province was nearly a third. Gen. MacArthur spoke of the capture of papers from high Filipino officials in which the information was contained that, if President McKinley should be re-elected, the insurgents would surrender to the authority of the United States.

Colonel Arthur L. Wagner, the Army's chief public relations officer, had spent two and a half years in the Philippines. Wagner testified in May, where he was questioned about concentration camps in the Philippines, 31 including deaths at the camps. In one camp, it was reported that the people were assembled according to villages, so that the people in all cases would have their old neighbors near them. So far as he had been able to observe, there was no evidence of want among the people there congregated. Moreover, they were surprisingly contented. Such camps, he insisted, were created to "protect friendly natives from the insurgents" and to "assure them an adequate food supply", while also teaching them "proper sanitary standards." People were limited to travel within 300 to 800 yards of the camp, beyond which was a so-called "dead line" that anyone caught crossing would be shot, though he claimed the standing order was not to shoot any helpless persons, or any others if the shooting could be avoided.

Colonel Wagner said that one of the principal purposes of concentrating the native people in the Philippines was to protect them against the Ladrones, which had been admirably accomplished. Another object of the camps had been that of facilitating the collection of the rice supplies in order to starve out the Ladrones and guerrillas. The result had been that hostile parties had practically disappeared and their leader, Malvar, had been captured. The policy had been necessary to "protect life and property, and he did not see how any other policy could have been successful. He said that the people were fed and given medical supplies, and the sanitation of the camps was looked after. He insisted that American camps in the Philippines no more could be compared to Valeriano Weyler's reconcentrado camps in Cuba than mercy could be compared to cruelty.

Over loud Republican protests, Senator Culberson began to read a letter from one of J. Franklin Bell's officers, which had been quoted in the Senate by Mr. Bacon, in which the officer described a concentration camp as a "suburb of hell." The chair ruled that unless the senator identified the author, who had asked to remain anonymous, it was "hearsay evidence" and directed the witness not to comment on it. But Culberson had already read part of the letter:

What a farce it all is ... this little spot of black sogginess is a reconcentrado pen, with a dead line outside, beyond which everything living is shot ... Upon arrival, I found 30 cases of smallpox, and average fresh ones of five a day, which practically have to be turned out to die. At nightfall crowds of huge vampire bats softly swirl out of their orgies over the dead. Mosquitos work in relays. This corpse-carcass stench wafts in and combined with some lovely municipal odors besides makes it slightly unpleasant here.

Col. Wagner said he had no personal knowledge of the tortures of the natives in the Philippines, but he gave several instances in which he had heard reports of torture. In most of these it was found on examination that the reports either were untrue or exaggerated.

Wagner said that he knew that one village had been burned because the citizens would not give information of the murderers of a native friendly to the United States.

After intense cross examination, Wagner agreed that some "innocents" had suffered in the Philippines, but he added that the same was true of every war and that it was an injustice as old as man. "The Almighty destroyed Sodom, notwithstanding the fact there were a few just people in that community." Senator Albert Beveridge replied, "I was thinking of that instance of Sodom and Gomorrah."

Senator Albert Beveridge published a separate senate document containing his views on the committee, published as Senate Document 422 in the 57th Congress, 1st session. Historian Miller criticized this secondary publication, calling it a "deceitful cut and paste job ... gleaning from the record anything that remotely supported his conclusion that the war was one of the most humane ones in history ... [Beveridge felt that] the Lodge committee had destroyed the malicious fiction of "the slanders of the Army".





</doc>
<doc id="7696135" url="https://en.wikipedia.org/wiki?curid=7696135" title="Controversies of the Polish–Soviet War">
Controversies of the Polish–Soviet War

Controversies of the Polish-Soviet War, fought in 1919–20, concerning the behaviour of the military forces and crimes they committed. Each side charged the other with violations of international law in an effort to sway public opinion in the West, which was felt to be important for both sides.

During this war between two countries experiencing great socioeconomic difficulties, and often unable to care for their own populations, the treatment of prisoners of war was far from adequate, with tens of thousands on both sides dying of communicable diseases. Between 16,000 to 20,000 of Soviet POWs – out of 80,000 – died in Polish camps; and about 20,000 – out of 51,000 – Polish POWs died in Soviet and Lithuanian camps.. Russian professor Matveyev from Moscow National University pointed another numbers that within 60,000 up to 83,500 Russian POWs died in Polish prisoner camps out of 206.877 Soviet Russian POWs.

After 1922 Polish and Russian prisoners were exchanged between the two sides. Ekaterina Peshkova, the chairwoman of the organization "Assistance to Political Prisoners" (Pompolit, "Помощь политическим заключенным", Помполит). was recognised by the Polish Red Cross for her participation in the exchange of POWs after the Polish-Soviet War.

The Polish side claimed that during the Soviet retreat from Berdychiv, Kiev and Zhytomyr mass hostage-taking of civilians occurred, with hostages forced to go with the Red Army all the way to the rear of the front. Similar claims were made that when returning to Berdychiv the Bolsheviks threw out the sick and wounded from the hospital "disregarding the lives and honor of the medical personnel" and that in general the Soviet advance into Ukraine was characterized by mass killing of civilians and the burning of entire villages, especially by Budyonny's cossacks, designed to terrorise the Ukrainian population.

In January 1918 in Cichinicze near Mohylow Bolsheviks shot patients and personnel of a Polish hospital.

Behind Polish lines, Soviet forces hanged suspected enemies on the spot. Ultimately, in the pacification of Ukraine that began during the Soviet counteroffensive in 1920 and which would not end until 1922, the Soviets would take tens of thousands of Ukrainian lives. On 7 June, the same day Budyonny's Cossacks, spreading terror in the rear of recently broken Polish frontlines, burned a hospital in Berdychiv, with 600 patients and International Red Cross nuns inside.

There were also accusations against Stanisław Bułak-Bałachowicz, a former officer of the Imperial Russian and Bolsheviks armies, who switched sides in the conflict and became a general in Poland. Although Bułak-Bałachowicz was regarded as a national hero to Belarusians in Poland for protecting them against Bolshevik terror, and his refusal to kill peasants on orders from Soviets, he is said to have behaved like an absolute ruler in the territories controlled by his troops, even conducting public executions As one Polish officer wrote in a letter to his wife: "This is a person without ideology. The bandit and the murderer and his comrades – subordinates are just like that. They know no shame and are similar to barbarians... I witnessed throwing the cut-off heads of Bolsheviks under his feet... The massacre of Bolsheviks was horrific".

In 1919, Russian Jews were caught in the middle of a civil war, and became the victims of warring Red and White Russian, Ukrainian and Polish forces, among others, resulting in the loss of an estimated 100,000 Jewish lives. White Russian troops led by Denikin staged pogroms against Jews in practically every town he captured. In Ukraine at this time, murders of Jews took place on an unprecedented scale, second only to the Holocaust years of World War II.

Isaac Babel, a war correspondent embedded with the Red Army, in his 1920 diary wrote down many first-hand accounts of atrocities committed by both sides against Jews (Most of them were retreating Red Army in Ukrainian Front). On 5 April 1919 in Pinsk, a Polish officer, after hearing reports that Jewish inhabitants of the city were preparing to riot, panicked and ordered the execution of thirty-five Jews (Pinsk massacre). Similar hostilities, resulting in fewer casualties, took place in other towns. In Lida soldiers stopped several elderly Jews and cut off their beards with sabres and knives. During the pillage of Lida, Jewish homes were looted and 30 Jews were killed. Violence against Jews caused a major uproar and condemnation in the Polish Parliament. Ignacy Daszyński, leader of the Polish Socialist Party called all soldiers that committed acts of violence against the Jewish population "hooligans in uniform". However, Minister of War General Józef Leśniewski, in his written reply to the speaker of Parliament, defended anti-Jewish violence by Polish units in Lida, referring to Jews as a Communist-minded community and stating that the Polish Army had the right to kill their adversaries.

The ordeal of Jewish victims at the hands of the Poles could not be equated with the massacres committed by Denikin's troops.

However, reports of these incidents caused the United States to send a commission led by Henry Morgenthau, Sr. and Sir Stuart M. Samuel to investigate. According to the findings of this Anglo-American Investigating Commission, a total of about 300 Jews lost their lives in all incidents involving Poles. The commission also found that the Polish military and civil authorities did their best to prevent such incidents and their recurrence in the future. The Morgenthau report stated that some forms of discrimination against Jews was of political rather than anti-Semitic nature and specifically avoided use of the term "pogrom," noting that the term was used to apply to a wide range of excesses, and had no specific definition.

Sociologist Tadeusz Piotrowski noted that the Morgenthau Report admitted that the word "pogrom" was inapplicable to the conditions existing within a war zone. Richard C. Lukas argues that in some places, Jews had made themselves vulnerable by collaborating with Poland's Lithuanian and Soviet enemies.

Both sides in the conflict raised complaints about property destruction in diplomatic notes addressed to the Entente. One note raised by the Soviet side stated that during the Soviet advance the retreating Poles engaged in "vengeful vandalism", as in Borisov where the Poles, following their retreat, shelled the city with artillery from the opposite bank of the Berezina River "killing hundreds of people and leaving thousands without shelter."

Another joint diplomatic note issued by Soviet Ukraine and Soviet Russia to the Entente blamed the Poles for heavily damaging Kiev, including its civilian infrastructure and art, such as St Volodymyr's Cathedral, a charge the Poles denied, admitting only to the destruction of the Kiev bridges, in order to slow down the Red Army. That particular note seems to be based on a telegram by Leon Trotsky, who later admitted that the information he received on St Volodymyr's Cathedral was incorrect.

Around the same time, two days after breaking through the Polish front line – Budyonny's 1st Army destroyed the bridges in Zhytomyr, wrecked the railway station and burned various buildings; Budyonny's troops would continue to spread terror and wreck infrastructure over the coming month in western Ukraine and eastern Poland, to delay the Polish army and disrupt its logistics.


</doc>
<doc id="41274192" url="https://en.wikipedia.org/wiki?curid=41274192" title="Tokyo Charter">
Tokyo Charter

The International Military Tribunal for the Far East Charter (IMTFE Charter), also known as the Tokyo Charter, was the decree issued by General Douglas MacArthur, Supreme Commander for the Allied Powers in Allied-occupied Japan, on January 19th, 1946 that set down the laws and procedures by which the Tokyo Trials were to be conducted. The charter was issued months following the surrender of Japan on September 2nd, 1945, which brought World War II to an end.

Modeled after the Nuremberg Charter, the Tokyo Charter stipulated that crimes of the Japanese could be tried. Three categories of crimes were defined: crimes against peace, war crimes, and crimes against humanity. Article 6 of the Tokyo Charter also stated that holding an official position or acting pursuant to order of his government or of a superior was no defense to war crimes, but that such circumstances may be considered in mitigation of punishment if the Tribunal determines that justice so requires.

Like the Nuremberg Trials, the criminal procedure used by the Tokyo Trials was closer to civil law than to common law, with a trial before a panel of judges rather than a jury trial and with wide allowance for hearsay evidence. Defendants who were found guilty could appeal the verdict to the Allied Council for Japan. In addition, they would be permitted to present evidence in their defense and to cross-examine witnesses.

Unlike the Nuremberg Charter, the Tokyo Charter was not part of a treaty or agreement among the Allies but it was substantially the same as the Nuremberg Charter. A major exception was that Emperor Hirohito was excluded from being tried for crimes against peace, war crimes, and crimes against humanity.

The Tokyo Charter differs from the Nuremberg Charter, however, in two ways. The Tokyo Charter adds the categories of persons to be held responsible and it does not make "persecution" subject to "religious" grounds. The first variance is only in the drafting of Article 5(c) of the Tokyo Charter defining crimes against humanity since the same responsibility basis exists in the Charter thought not in Article 6(c) of the Nuremberg Charter. The second variance is because the Nazi crimes against the Jews did not have a counterpart in the Asian conflict.

The Nuremberg and Tokyo Charters were applicable only to major criminals, leaving other criminals to be tried by the Allies. In Germany, the Allies acted pursuant to Control Council Law No. 10 (CCL 10) in their respective zones of occupation. But they also relied on their military and national tribunals, where they applied their own laws. There was no counterpart in Japan to CCL 10 because the United States was the sole occupying power of Japan, whereas Germany was occupied by the four major Allies (United States, Great Britain, France, and the Soviet Union). The same legal issues pertaining to Article 6(c) of the Nuremberg Charter also apply to Article 5(c) of the Tokyo Charter.




</doc>
<doc id="61099" url="https://en.wikipedia.org/wiki?curid=61099" title="Crimes against humanity">
Crimes against humanity

Crimes against humanity are certain acts that are purposely committed as part of a widespread or systematic attack directed against any civilian or an identifiable part of a civilian population. The first prosecution for crimes against humanity took place at the Nuremberg trials. Crimes against humanity have since been prosecuted by other international courts (for example, the International Criminal Tribunal for the former Yugoslavia, the International Criminal Tribunal for Rwanda and the International Criminal Court) as well as in domestic prosecutions. The law of crimes against humanity has primarily developed through the evolution of customary international law. Crimes against humanity are not codified in an international convention, although there is currently an international effort to establish such a treaty, led by the Crimes Against Humanity Initiative.

Unlike war crimes, crimes against humanity can be committed during peace or war. They are not isolated or sporadic events, but are part either of a government policy (although the perpetrators need not identify themselves with this policy) or of a wide practice of atrocities tolerated or condoned by a government or a de facto authority. War crimes, murder, massacres, dehumanization, genocide, ethnic cleansing, deportations, unethical human experimentation, extrajudicial punishments including summary executions, use of weapons of mass destruction, state terrorism or state sponsoring of terrorism, death squads, kidnappings and forced disappearances, use of child soldiers, unjust imprisonment, enslavement, torture, rape, political repression, racial discrimination, religious persecution and other human rights abuses may reach the threshold of crimes against humanity if they are part of a widespread or systematic practice.

The term "crimes against humanity" is potentially ambiguous because of the ambiguity of the word "humanity", which can mean humankind (all human beings collectively) or the value of humanness. The history of the term shows that the latter sense is intended.

There were several bilateral treaties in 1814 that foreshadowed the multilateral treaty of Final Act of the Congress of Vienna (1815) that used wording expressing condemnation of the slave trade using moral language. For example, the Treaty of Paris (1814) between Britain and France included the wording "principles of natural justice"; and the British and United States plenipotentiaries stated in the Treaty of Ghent (1814) that the slave trade violated the "principles of humanity and justice".

The multilateral "Declaration of the Powers, on the Abolition of the Slave Trade, of 8 February 1815" (Which also formed of the Final Act of the Congress of Vienna of the same year) included in its first sentence the concept of the "principles of humanity and universal morality" as justification for ending a trade that was "odious in its continuance".

The term "crimes against humanity" was used by George Washington Williams in a pamphlet published in 1890 to describe the practices of Leopold II of Belgium's administration of the Congo Free State. In treaty law, the term originated in the Second Hague Convention of 1899 preamble and was expanded in the Fourth Hague Convention of 1907 preamble and their respective regulations, which were concerned with the codification of new rules of international humanitarian law. The preamble of the two Conventions referenced the "laws of humanity" as an expression of underlying inarticulated humanistic values. The term is part of what is known as the Martens Clause.

On May 24, 1915, the Allied Powers, Britain, France, and Russia, jointly issued a statement explicitly charging for the first time ever another government of committing "a crime against humanity". An excerpt from this joint statement reads:

At the conclusion of the war, an international war crimes commission recommended the creation of a tribunal to try "violations of the laws of humanity". However, the US representative objected to references to "law of humanity" as being imprecise and insufficiently developed at that time and the concept was not pursued. 

Nonetheless, a UN report in 1948 referred to the usage of the term "crimes against humanity" in regard to the Armenian massacres as a precedent to the Nürnberg and Tokyo Charters. On May 15, 1948, the Economic and Social Council presented a 384-pages report prepared by the United Nations War Crimes Commission (UNWCC), set up in London (October 1943) to collect and collate information on war crimes and war criminals. The report was in compliance to the request by the UN Secretary-General to make arrangements for "the collection and publication of information concerning human rights arising from trials of war criminals, quislings and traitors, and in particular from the Nürnberg and Tokyo Trials." The report had been prepared by members of the Legal Staff of the Commission. The report is highly topical in regard to the Armenian Genocide, not only because it uses the 1915 events as a historic example, but also as a precedent to the Articles 6 (c) and 5 (c) of the Nuremberg and Tokyo Charters, and thereby as a precursor to the then newly adopted UN Genocide Convention, differentiating between war crimes and crimes against humanity. By refereeing to the information collected during WWI and put forward by the 1919 Commission of Responsibilities, the report entitled "Information Concerning Human Rights Arising from Trials of War Criminals" used the Armenian case as a vivid example of committed crimes by a state against its own citizens. The report also noted that while the Paris Peace Treaties with Germany, Austria, Hungary and Bulgaria, did not include any reference to "laws of humanity", instead basing the charges on violations of "laws and customs of war", The Sèvres Peace Treaty with Turkey did so. In addition to the Articles 226–228, concerning customs of war (corresponding to Articles 228–230 of the Treaty of Versailles), the Sèvres Treaty also contained an additional Article 230, obviously in compliance with the Allied ultimatum of May 24, 1915 in regard to committed "crimes against humanity and civilization".

After the Second World War, the London Charter of the International Military Tribunal set down the laws and procedures by which the Nuremberg trials were to be conducted. The drafters of this document were faced with the problem of how to respond to the Holocaust and the grave crimes committed by the Nazi regime. A traditional understanding of war crimes gave no provision for crimes committed by a power on its own citizens. Therefore, Article 6 of the Charter was drafted to include not only traditional war crimes and crimes against peace, but also "crimes against humanity", defined as
Under this definition, crimes against humanity could only be punished insofar as they could be connected somehow to war crimes or crimes against peace. The jurisdictional limitation was explained by the American chief representative to the London Conference, Robert H. Jackson, who pointed out that it "has been a general principle from time immemorial that the internal affairs of another government are not ordinarily our business". Thus, "it is justifiable that we interfere or attempt to bring retribution to individuals or to states only because the concentration camps and the deportations were in pursuance of a common plan or enterprise of making an unjust war". The judgement of the first Nuremberg trial found that "the policy of persecution, repression and murder of civilians" and persecution of Jews within Germany before the outbreak of war in 1939 were not crimes against humanity, because as "revolting and horrible as many of these crimes were, it has not been satisfactorily proved that they were done in execution of, or in connection with," war crimes or crimes against peace. The subsequent Nuremberg trials were conducted under Control Council Law No. 10 which included a revised definition of crimes against humanity with a wider scope.

The International Military Tribunal for the Far East (IMTFE), also known as the Tokyo Trial, was convened to try the leaders of the Empire of Japan for three types of crimes: "Class A" (crimes against peace), "Class B" (war crimes), and "Class C" (crimes against humanity), committed during the Second World War.

The legal basis for the trial was established by the Charter of the International Military Tribunal for the Far East (CIMTFE) that was proclaimed on 19 January 1946. The tribunal convened on May 3, 1946, and was adjourned on November 12, 1948.

In the Tokyo Trial, Crimes against Humanity (Class C) was not applied for any suspect. Prosecutions related to the Nanking Massacre were categorised as infringements upon the Laws of War.

A panel of eleven judges presided over the IMTFE, one each from victorious Allied powers (United States, Republic of China, Soviet Union, United Kingdom, the Netherlands, Provisional Government of the French Republic, Australia, New Zealand, Canada, British India, and the Philippines).

The different types of crimes which may constitute crimes against humanity differs between definitions both internationally and on the domestic level. Isolated inhumane acts of a certain nature committed as part of a widespread or systematic attack may instead constitute grave infringements of human rights, or – depending on the circumstances – war crimes, but are not classified as crimes against humanity.

The systematic persecution of one racial group by another, such as occurred during the South African apartheid government, was recognized as a crime against humanity by the United Nations General Assembly in 1976. The Charter of the United Nations (Article 13, 14, 15) makes actions of the General Assembly advisory to the Security Council. In regard to apartheid in particular, the UN General Assembly has not made any findings, nor have apartheid-related trials for crimes against humanity been conducted.

Neither the Nuremberg or Tokyo Charters contained an explicit provision recognizing sexual and gender-based crimes as war crimes or crimes against humanity, although Control Council Law No. 10 recognized rape as a crime against humanity. The statutes of the International Criminal Tribunal for the former Yugoslavia and the International Criminal Tribunal for Rwanda both included rape as a crime against humanity. The ICC is the first international instrument expressly to include various forms of sexual and gender-based crimes including rape, sexual slavery, enforced prostitution, forced pregnancy, enforced sterilisation, and other forms of sexual violence as both an underlying act of crimes against humanity and war crime committed in international and/or non-international armed conflicts. As an example, the events of Khojaly and Khatyn can be shown that the world strongly condemns. International institutions have asked for a ransom to avoid such incidents. There are hundreds of massacres, thousands of prisoners and wounded in these incidents.

In 2008, the U.N. Security Council adopted resolution 1820, which noted that "rape and other forms of sexual violence can constitute war crimes, crimes against humanity or a constitutive act with respect to genocide".

Unlike genocide and war crimes, which have been widely recognized and prohibited in international criminal law since the establishment of the Nuremberg principles, there has never been a comprehensive convention on crimes against humanity, even though such crimes are continuously perpetrated worldwide in numerous conflicts and crises. There are eleven international texts defining crimes against humanity, but they all differ slightly as to their definition of that crime and its legal elements.

In 2008, the Crimes Against Humanity Initiative was launched by Professor Leila Nadya Sadat at the Whitney R. Harris World Law Institute to address this gap in international law. The Initiative represents the first concerted effort to address the gap that exists in international criminal law by enumerating a comprehensive international convention on crimes against humanity.

On July 30, 2013, the United Nations International Law Commission voted to include the topic of crimes against humanity in its long-term program of work. In July 2014, the Commission moved this topic to its active programme of work based largely on a report submitted by Sean D. Murphy. Professor Sean D. Murphy, the United States’ Member on the United Nations’ International Law Commission, has been named the Special Rapporteur for Crimes Against Humanity. Sean D. Murphy attended the 2008 Experts' Meeting held by the Crimes Against Humanity Initiative prior to this appointment.

There is some debate on what the status of crimes against humanity under customary international law is. M. Cherif Bassiouni argues that crimes against humanity are part of "jus cogens" and as such constitute a non-derogable rule of international law.

The United Nations has been primarily responsible for the prosecution of crimes against humanity since it was chartered in 1948.

After Nuremberg, there was no international court with jurisdiction over crimes against humanity for almost 50 years. Work continued on developing the definition of crimes against humanity at the United Nations, however. In 1947, the International Law Commission was charged by the United Nations General Assembly with the formulation of the principles of international law recognized and reinforced in the Nuremberg Charter and judgment, and with drafting a 'code of offenses against the peace and security of mankind'. Completed fifty years later in 1996, the Draft Code defined crimes against humanity as various inhumane acts, i.e., "murder, extermination, torture, enslavement, persecution on political, racial, religious or ethnic grounds, institutionalized discrimination, arbitrary deportation or forcible transfer of population, arbitrary imprisonment, rape, enforced prostitution and other inhuman acts committed in a systematic manner or on a large scale and instigated or directed by a Government or by any organization or group." This definition differs from the one used in Nuremberg, where the criminal acts were to have been committed "before or during the war", thus establishing a nexus between crimes against humanity and armed conflict.

A report on the 2008–09 Gaza War by Richard Goldstone accused Palestinian and Israeli forces of possibly committing a crime against humanity. In 2011, Goldstone said that he no longer believed that Israeli forces had targeted civilians or committed a crime against humanity.

On 21 March 2013, at its 22nd session, the United Nations Human Rights Council established the Commission of Inquiry on human rights in the Democratic People's Republic of Korea (DPRK). The Commission is mandated to investigate the systematic, widespread and grave violations of human rights in the Democratic People's Republic of Korea, with a view to ensuring full accountability, in particular for violations which may amount to crimes against humanity. The Commission dealt with matters relating to crimes against humanity on the basis of definitions set out by customary international criminal law and in the Rome Statute of the International Criminal Court. The 2014 Report by the commission found "the body of testimony and other information it received establishes that crimes against humanity have been committed in the Democratic People's Republic of Korea, pursuant to policies established at the highest level of the State... These crimes against humanity entail extermination, murder, enslavement, torture, imprisonment, rape, forced abortions and other sexual violence, persecution on political, religious, racial and gender grounds, the forcible transfer of populations, the enforced disappearance of persons and the inhumane act of knowingly causing prolonged starvation. The commission further finds that crimes against humanity are ongoing in the Democratic People's Republic of Korea because the policies, institutions and patterns of impunity that lie at their heart remain in place." Additionally, the commission found that crimes against humanity have been committed against starving populations, particularly during the 1990s, and are being committed against persons from other countries who were systematically abducted or denied repatriation, in order to gain labour and other skills for the Democratic People's Republic of Korea.

UN Security Council Resolution 1674, adopted by the United Nations Security Council on 28 April 2006, "reaffirms the provisions of paragraphs 138 and 139 of the 2005 World Summit Outcome Document regarding the responsibility to protect populations from genocide, war crimes, ethnic cleansing and crimes against humanity". The resolution commits the Council to action to protect civilians in armed conflict.

In 2008 the U.N. Security Council adopted resolution 1820, which noted that "rape and other forms of sexual violence can constitute war crimes, crimes against humanity or a constitutive act with respect to genocide".

According to the United Nations Security Council resolution 1970 (2011) concerning Libya, any direct or indirect trade of arms to the Libyan Arab Jamahiriya, in the form of supply, transfer, or sale should be prevented by the member nations. The arms embargo restricts the supply of arms, weapons, military vehicles, spare parts, technical assistance, finances, along with the provision of armed mercenaries, with origins of a country other than the one providing. 

However, the United Nations claimed in its November 2019 report that the United Arab Emirates, Jordan and Turkey are violating the arms embargo imposed on Libya under the 1970 resolution. An airstrike on the migrant detention center in Tripoli in July 2019, believed to have been carried out by the United Arab Emirates, can be amounted as a war crime, as stated by the United Nations. The airstrike was deadlier than the 2011 militarized uprising that overthrew the regime of Muammar Gaddafi.

After the Nuremberg and Tokyo trials of 1945–1946, the next international tribunal with jurisdiction over crimes against humanity was not established for another five decades. In response to atrocities committed in the 1990s, multiple ad hoc tribunals were established with jurisdiction over crimes against humanity. The statutes of the International Criminal Court, the International Criminal Tribunals for the Former Yugolavia and for Rwanda each contain different definitions of crimes against humanity.

In 1993, the UN Security Council established the International Criminal Tribunal for the former Yugoslavia (ICTY), with jurisdiction to investigate and prosecute three international crimes which had taken place in the former Yugoslavia: genocide, war crimes, and crimes against humanity. Article 5 of the ICTY Statute states that 

This definition of crimes against humanity revived the original ‘Nuremberg’ nexus with armed conflict, connecting crimes against humanity to both international and non-international armed conflict. It also expanded the list of criminal acts used in Nuremberg to include imprisonment, torture and rape. Cherif Bassiouni has argued that this definition was necessary as the conflict in the former Yugoslavia was considered to be a conflict of both an international and non-international nature. Therefore, this adjusted definition of crimes against humanity was necessary to afford the tribunal jurisdiction over this crime.

The UN Security Council established the International Criminal Tribunal for Rwanda in 1994 following the Rwandan genocide. Under the ICTR Statute, the link between crimes against humanity and an armed conflict of any kind was dropped. Rather, the requirement was added that the inhumane acts must be part of a "systematic or widespread attack against any civilian population on national, political, ethnic, racial or religious grounds." Unlike the conflict in the former Yugoslavia, the conflict in Rwanda was deemed to be non-international, so crimes against humanity would likely not have been applicable if the nexus to armed conflict had been maintained.

In 2002, the International Criminal Court (ICC) was established in The Hague (Netherlands) and the Rome Statute provides for the ICC to have jurisdiction over genocide, crimes against humanity and war crimes. The definition of what is a "crime against humanity" for ICC proceedings has significantly broadened from its original legal definition or that used by the UN. Essentially, the Rome Statute employs the same definition of crimes against humanity that the ICTR Statute does, minus the requirement that the attack was carried out ‘on national, political, ethnic, racial or religious grounds’. In addition, the Rome Statute definition offers the most expansive list of specific criminal acts that may constitute crimes against humanity to date.

Article 7 of the treaty stated that:

The Rome Statute Explanatory Memorandum states that crimes against humanity
To fall under the Rome Statute, a crime against humanity which is defined in Article 7.1 must be "part of a widespread or systematic attack directed against any civilian population". Article 7.2.a states "For the purpose of paragraph 1: "Attack directed against any civilian population means a course of conduct involving the multiple commission of acts referred to in paragraph 1 against any civilian population, pursuant to or in furtherance of a State or organizational policy to commit such attack." This means that an individual crime on its own, or even a number of such crimes, would not fall under the Rome Statute unless they were the result of a State policy or an organizational policy. This was confirmed by Luis Moreno Ocampo in an open letter publishing his conclusions about allegations of crimes committed during the invasion of Iraq in March 2003 which might fall under the ICC. In a section entitled "Allegations concerning Genocide and Crimes against Humanity" he states that "the available information provided no reasonable indicator of the required elements for a crime against humanity," i.e. 'a widespread or systematic attack directed against any civilian population'".

The ICC can only prosecute crimes against humanity in situations under which it has jurisdiction. The ICC only has jurisdiction over crimes contained in its statute – genocide, war crimes and crimes against humanity – which have been committed on the territory of a State party to the Rome Statute, when a non-party State refers a situation within its country to the court or when the United Nation Security Council refers a case to the ICC. In 2005 the UN referred to the ICC the situation in Darfur. This referral resulted in an indictment of Sudanese President Omar al-Bashir for genocide, crimes against humanity and war crimes in 2008. When the ICC President reported to the UN regarding its progress handling these crimes against humanity case, Judge Phillipe Kirsch said "The Court does not have the power to arrest these persons. That is the responsibility of States and other actors. Without arrests, there can be no trials.

The Committee of Ministers of the Council of Europe on 30 April 2002 issued a recommendation to the member states, on the protection of women against violence. In the section "Additional measures concerning violence in conflict and post-conflict situations", states in paragraph 69 that member states should: "penalize rape, sexual slavery, forced pregnancy, enforced sterilization or any other form of sexual violence of comparable gravity as an intolerable violation of human rights, as crimes against humanity and, when committed in the context of an armed conflict, as war crimes;"

In the Explanatory Memorandum on this recommendation when considering paragraph 69:
The Holodomor has been recognized as a crime against humanity by the European Parliament.

Sources say the 20th century can be considered the bloodiest period in global history. Millions of civilian infants, children, adults, and elderly people died in warfare. One civilian perished for every combatant killed. Efforts of the International Committee of the Red Cross, humanitarian laws, and rules of warfare were not able to stop these crimes against humanity. These terminologies were invented since previous vocabulary was not enough to describe these offenses. War criminals did not fear prosecution, apprehension, or imprisonment before World War II. Britain's Prime Minister Winston Churchill favored the outright execution of war criminals. The United States was more lenient and called for a just trial. The British Government was convinced to institute the Nuremberg Trial which left several legacies. These are worldwide jurisdiction for severe war crimes are, creation of international war crime tribunals, judicial procedures that documented history of colossal crimes effectively, and success of UN courts in holding impartial trials.

The UN pointed out the Rome Statute of the International Criminal Court (ICC) specifically Article 7 (Crimes against Humanity), which defines large-scale acts of violence against a locality's civilian populace. These acts consist of murder; annihilation; enslavement; bondage; forced removal of the population; imprisonment or deprivation of physical liberty that violates international laws; maltreatment; forced prostitution and rape; discrimination and tyranny against certain groups; apartheid (racial discrimination and segregation); and, other inhumane acts. A publication from Trial International mentioned that crimes against humanity have been collated starting in 1990. These were the 1993 Statute of the International Criminal Tribunal for Yugoslavia, 1994 Statute of the International Tribunal for Rwanda, and 1998 Rome Statute of the International Criminal Court. The latter contains the latest and most extensive list of detailed crimes against civilians.





</doc>
<doc id="20712850" url="https://en.wikipedia.org/wiki?curid=20712850" title="Rathlin Island massacre">
Rathlin Island massacre

The Rathlin Island massacre took place on Rathlin Island, off the coast of Ireland on 26 July 1575, when more than 600 Scots and Irish were killed.

Rathlin Island was used as a sanctuary because of its natural defences and rocky shores; when the wind blew from the west, in earlier times it was almost impossible to land. It was also respected as a hiding place, as it was the one-time abode of St. Columba. Installing themselves in Rathlin Castle, the MacDonnells of Antrim made Rathlin their base for resistance to the Enterprise of Ulster. Their military leader, Sorley Boy MacDonnell ("Scottish Gaelic": Somhairle Buidhe Mac Domhnaill) and other Scots had thought it prudent to send their wives, children, elderly, and sick to Rathlin Island for safety.

Acting on the instructions of Sir Henry Sidney and the Earl of Essex, Sir Francis Drake and Sir John Norreys took the castle by storm. Drake used two cannons to batter the castle and when the walls gave in, Norreys ordered direct attack on 25 July, and the Garrison surrendered. Norreys set the terms of surrender, whereupon the constable, his family, and one of the hostages were given safe passage and all other defending soldiers were killed, and on 26 July 1575, Norreys' forces hunted the old, sick, very young and women who were hiding in the caves. Despite the surrender, they killed all the 200 defenders and more than 400 civilian men, women and children. Drake was also charged with the task of preventing any Scottish reinforcement vessels reaching the Island.

The entire family of Sorley Boy MacDonnell perished in the massacre. Essex, who ordered the killings, boasted in a letter to Francis Walsingham, the Queen's secretary and spymaster, that Sorley Boy MacDonnell watched the massacre from the mainland helplessly and was "like to run mad from sorrow".

Norreys stayed on the island and tried to rebuild the walls of the castle so that the English might use the structure as a fortress. As Drake was not paid to defend the island, he departed with his ships. Norreys realised that it was not possible to defend the island without intercepting Scottish galleys and he returned to Carrickfergus in September 1575.




</doc>
<doc id="47135293" url="https://en.wikipedia.org/wiki?curid=47135293" title="List of monuments damaged by conflict in the Middle East during the 21st century">
List of monuments damaged by conflict in the Middle East during the 21st century

This is a list of monuments suffering damage from conflict in the Middle East during the 21st century. It is sorted by country.















</doc>
<doc id="6374235" url="https://en.wikipedia.org/wiki?curid=6374235" title="Crime of aggression">
Crime of aggression

A crime of aggression is a specific type of crime where a person plans, initiates, or executes an act of aggression using state military force that violates the Charter of the United Nations. The act is judged as a violation based on its character, gravity, and scale.

Acts of aggression include invasion, military occupation, annexation by the use of force, bombardment, and military blockade of ports.

The crime of aggression is a crime under the Rome Statute of the International Criminal Court. The definitions and the conditions for the exercise of jurisdiction over this crime were adopted by consensus on 11 June 2010 at the Kampala Review Conference by the States Parties to the Court.

In 1998, at the Rome Conference that adopted the Rome Statute of the International Criminal Court ("the Statute"), the crime was included as one of the crimes within the jurisdiction of the Court (Article 5.1) and over which any State that becomes party to the Statute accepts the Court's jurisdiction (Article 12.1). However, participants to the Rome Conference could not agree on the definition of the crime nor on further conditions for the Court's exercise of jurisdiction; the Statute did not allow the Court to exercise such jurisdiction until these outstanding issues were solved (Article 5.2). At the 2010 Review Conference ("the Conference"), States Parties agreed by consensus to adopt resolution RC/Res.6 accepting the amendments to the Statute adding the definition of the crime and the conditions for the exercise of jurisdiction over this crime.


Under the Statute, the definition of "crime of aggression" is stated as follows:

Under the Statute, the conditions for the exercise of jurisdiction for the "crime of aggression" by the Court are as defined below. With these provisions, the Court may exercise its jurisdiction over the "crime of aggression" in one or all of the following ways.

Exercise of jurisdiction over the crime of aggression (State referral, proprio motu)

Exercise of jurisdiction over the crime of aggression (Security Council referral)

Since the amendments on the crime of aggression were adopted under Article 121.5, they enter into force for each ratifying State individually. In addition, the amendments require two further conditions to be fulfilled for the Court to exercise jurisdiction, both for state referrals and "proprio motu" investigations (15 bis) and for Security Council referrals (15 ter):
At the 16th annual session of the Assembly of States Parties to the Rome Treaty, it was decided to activate jurisdiction of the Crime of Aggression starting July 17, 2018.

As of 29 November 2017, thirty-four (34) State Parties have ratified or acceded to the amendments to the Rome Statute. 
On 8 May 2012, Liechtenstein ratified the Amendments on the crime of aggression to the Rome Statute of the International Criminal Court thereby becoming the first state to ratify the amendments. On 13 January 2013, the amendments were ratified by Luxembourg, the first current non-permanent member of the Security Council to do so. Luxembourg also became the first state to implement the amendments into domestic legislation. because it has revised its criminal code and its code of criminal procedure in February 2012. During the High-Level Week of the United Nations General Assembly from 25–26 September 2013, the amendments were also ratified by Andorra, Cyprus, Slovenia, and Uruguay, respectively.
The states parties are shown in alphabetical order according to their official name within the Assembly of States Parties.

"See" Amendments to the Rome Statute of the International Criminal Court for the list of states parties to the amendments.

Liechtenstein, which served the Presidency of the Review Conference, issued a statement upon ratification to the amendments:
At the Review Conference, Japan issued two statements before and after the adoption of the amendments took place:

In 2006, the House of Lords ruled in the case or "R v Jones" that the crime of aggression did not exist in English law as it applied within the United Kingdom.

After its formal participation in the Review Conference as Observer, on 15 June 2010 the United States government held a special briefing on the Conference outlining what they deemed as accomplishments.

The following article(s) are related with the adopted amendments concerning the provision on "crime of aggression".

Exercise of jurisdiction




</doc>
<doc id="40157302" url="https://en.wikipedia.org/wiki?curid=40157302" title="1987 Lieyu massacre">
1987 Lieyu massacre

The 1987 Lieyu massacre, also known as the March 7 Incident, Donggang Incident or Donggang Massacre, occurred on 7 March 1987 at Donggang Bay, Lieyu Island ("Lesser Kinmen" or "Little Quemoy"), Kinmen, Fujian, Republic of China. According to the diary of General Hau Pei-tsun, nineteen unarmed Vietnamese boat people were killed by the ROC military. There may have been more than nineteen deaths.

The 1987 Lieyu massacre was preceded by an incident where a young couple from mainland China swam to Dadan Island (大膽島) seeking asylum. At that time, all the islets of the Kinmen Archipelago were considered as war zones under Martial Law, which was to allow the Republic of China (Taiwan) to prevent an attack by the People's Liberation Army of the Communist Party of China. The commanding major-general of the Dadan Defense Team, Premier deputy division commander of the 158 Lieyu Division (烈嶼師), received the couple and escorted them to the superior-level Kinmen Defense Command (金防部), but was immediately relieved of his post for violating the directive to "Accept no surrender in the war zone".

As a result, the commanding lieutenant-colonel of the neighboring Erdan Island, Deputy brigade commander of the 473 Brigade, summoned all the soldiers to reiterate the order that "Whoever lands on the island must be executed without exception". Soon after this, he was promoted to the position of 472 Nantang brigade commander (南塘旅), taking charge of all the units in the South Lieyu Defense Team.

At noon on 28 February 1987, one week before the Lieyu massacre, a local Chinese fishing boat crossed Xiamen Bay. It was intensely fired upon until it caught fire near Dadan Island. The fishermen on board waved a white cloth in an attempt to communicate their surrender. However, the boat was sunk by tank gun shots ordered by the new Dadan commander colonel, Secondary deputy division commander of the 158 Division, upon receiving the approval of the Kinmen Defense Command. There was only one survivor, who swam to cling onto a rock nearby, but who was eventually lost to the waves.

On 7 March 1987, a boat carrying Vietnamese refugees arrived in Kinmen requesting political asylum. The request was rejected by the Kinmen Defense Command. The boat was towed away from the shore by a patrol boat of the Amphibious Reconnaissance Battalion (ARB-101, 海龍蛙兵) with a warning not to return. However, for reasons unclear, the information about the boat's presence in the Southern sea was never forwarded to the front line of the coastal defense units, including those in the Lieyu island.

At 16:37 in the afternoon of 7 March 1987, under heavy fog, the Vietnamese boat was sighted off the shore of Lieyu. The 472 brigade commander and the local 1st Dashanding (大山頂) infantry battalion commander arrived at the scene with staff officers. Warning shots were fired by the ROC military. The Vietnamese boat stranded on the sand beach south-west of Donggang ("East Cape", Chinese: 東崗; Pinyin: Dōnggāng) Port (Fort 05), a sensitive strategic point between 2 forts with a blind angle on radar screen by the landform in front of the classified 240 mm howitzer M1 position of Kinmen Defense Command, and artillery battalions of 158 Division then. It was hit by crossfire and two bazooka shots by the WPN company in reinforcement. Armor-piercing shells penetrated the wooden hull without explosion. Three unarmed Vietnamese men jumped off the boat, raised their hands, and said in Chinese, "Don't shoot..." but were all shot dead instantly.

The local 3rd Dongang company commander captain on site received an order from the brigade commander to lead a search team boarding the boat. Two hand grenades were thrown into the boat. The commander then found that all the passengers were Vietnamese refugees with no weapon. The passengers said the vessel had experienced a mechanical failure. Because of the heavy fog, the strong seasonal currents and the rising tide since late afternoon, the boat drifted into the bay. The surviving passengers and the bodies of the dead were taken out of the boat and placed on the beach with neither first aid nor any life support supply rendered. Followed by intense telecommunication with the division HQ, the commanders at the scene received orders from their superiors - alleged directly by General Zhao, Commander of the Kinmen Defense Command - to execute the passengers to eliminate all the eyewitnesses. Some received multiple shots when one bullet did not kill. Among the bodies piled were elderly, women, one pregnant, children, and a baby in a sweater.

In the morning 09:00 of 8 March 1987, the Medical Platoon of the battalion HQ Company was called in to bury all the bodies at the beach. The platoon members were ordered to execute any surviving refugees. The wounded were buried alive, and those who were still moving or crying were dictated to be killed by military shovels. The entire boat was also instructed to be burned down aside from the only propeller non-flammable to gasoline, then all buried in sand to destroy all the evidence right away. The last victim, a little boy being hidden underneath a board cell was also executed by order without exception. The guarding sergeant of the battalion HQ company counted the bodies as more than nineteen.

Since some medics defied the direct order of victim execution, the brigade commander instructed the Nantang brigade HQ Company commander captain to dispatch soldiers taking over the battalion HQ and the battalion HQ company as emergency measure. Later that day, a real fishing boat from China approached the coast attempting to check out what happened. it was also shot to destroy, and sunk in the open sea with 4 confirmed kills to assure all lips sealed - which some veterans later called the "March 8 Incident".

Ten weeks later, the President of the Republic of China (Taiwan), Chiang Ching-kuo, reacted to concealment of the massacre by the 158 Division and the Kinmen Defense Command. Commander of the Kinmen Defense Command, General Zhao Wan-fu (Chinese: 趙萬富; Pinyin: Zhào Wànfù), said he was unaware of the event. While being questioned by the Chief of the General Staff, General Hau Pei-tsun, Zhao Wan-fu lied, "It was just a couple of 'Communist soldiers' (referring to the penetration of People's Liberation Army) being shot in the water", but Zhao's statement was obviously unbelievable. Then General Hau Pei-tsun ordered the exhumation of the corpses from the first scene, moved them toward a higher hidden slope to the right, filled with cement and built a concrete wall above it to prevent any future investigation. The corpses remain sealed in the final place with no tomb today.

The native store owner heard the crying of refugees overnight and made a phone call to inform the National Assembly member, Huang Chao-hui in Kaohsiung, but the contact was soon lost whereas all long-distance phone calls being routinely monitored by the communication supervision unit of Kinmen Defense Command. Nevertheless, the bodies were not buried deeply on the first scene. Influenced by tidal seawater and high temperatures, the bodies soon began to decompose and were dug out by wild dogs from the landfill on the back side of the western hill. Accounts of ghost sightings prompted villagers to hold religious ceremonies, making it all the more difficult to block the news.

In early May 1987, the British Hong Kong newspaper "South China Morning Post" first reported the massacre. Informed by the overseas office, higher officials questioned the Kinmen Defense Command but got no concrete response; instead, the Command swapped this coast defense battalion from the front line with another reserve battalion in the training base in urgency in order to strengthen the personnel control and communication restriction to prevent further leaking news, and their unit designation codes were also shifted for the 2 years to confuse outsiders. Twice of extra bonus cash summing up to half a month of a captain's salary, $6000 were also abnormally awarded to the company commanders against the government ethics on the eve of Dragon boat festival. Until the end of May, recently discharged conscript soldiers from Kinmen began to arrive in Taiwan Proper by the term schedule and finally able to appeal to the newly founded opposition party, Democratic Progressive Party. The information of the massacre started to spread in Taiwan.

As a key witness during the investigation in the late-May, the Chief Intelligence Officer of 158 Division, Lieutenant Colonel Lai Xu (徐萊), mysteriously disappeared after a supervision task over night patrol and checkpoints, same to another non-commissioned officer at post within a week. Their bodies were never found.

On 5 June 1987, "Independence Evening Post" was the first Taiwanese newspaper reporting the massacre with the formal questioning by the newly elected Parliament Member Wu Shu-chen from the Democratic Progressive Party to the Ministry of National Defense during the general assembly of Legislative Yuan. Her questions were repeatedly denied by the Military Spokesman, Major-general Zhang Hui Yuan, who accused Wu of "sabotaging the national reputation", and claimed it was actually "a Chinese fishing boat being sunk in the sea after ignoring the warnings". The case has been classified as military secret ever since for 20 years to prevent any further leaking information or the prosecution will apply. The following media reports were censored and the publication were banned by the Nationalist government. Eventually when the police broke into the "Freedom Era Weekly" magazine office for arrest with another count of Treason charge in April, 1989, Editor-in-chief, Cheng Nan-jung set himself on fire then died in blaze to protest for the freedom of speech. Journalist Zhang Youhua (張友驊) of "Independence Evening Post" on the other hand was sentenced to 1 year and 7 months with a probation period of 3 years in November 1991.

The official cover-up story of the Chinese fishing boat sunk by one shell of bombardment applied to the public for 13 years, until being uncovered by the publication of «"Diary of the Chief of the General Staff (1981-1989)"» by General Hau in 2000. The Government of the Republic of China has made no comment thereafter.

After the scandal was exposed, President Chiang Ching-kuo received a letter from Amnesty International expressing humanitarian concern, and assigned the Chief of General Staff, General Hau, to investigate this case. A special envoy of the Political Warfare Bureau was dispatched to Kinmen and found the case true. All the commanders and the corresponding political officers along the chain of command, including Kinmen Defense Command, 158 Lieyu Division, 472 Nantang Brigade, the 1st Dashanding Battalion, the HQ, WPN and 3rd Donggang companies were detained and brought back to Taipei, then only the division officers were charged in court martial in October, 1987.

Brigade Commander Zhong was sentenced to 1 year and 10 months for abetting murder; Battalion Commander Major Liu was sentenced to 1 year and 10 months for being an accomplice to murder; WPN and 3rd Company commanders, Captain Li and Captain Zhang, both were sentenced to 1 year and 8 months for murder - but all the sentences were commuted with a probation period of 3 years, therefore none of the convicted field commanders was required to spend a day in jail until later under heavy pressure from the international society and media. Their later regular retirement and pension plans were not affected.

The superior officers received no official punishment, and recovered their military career after President Chiang suddenly died in January, 1988. Brigade Commander Zhong took a senior lead colonel position in a military academy (陸軍通校); Division Commander Gong shifted to the Chief of Staff of the War College, National Defense University, before being promoted to the Deputy Commander of the Hua-Tung Defense Command (花東防衛司令部) in 1991; Kinmen Defense Commander Zhao was promoted to Deputy Chief Commander General of the Republic of China Army in 1989, and further to Deputy Chief of the General Staff of the Republic of China Armed Forces in 1991; then appointed with honours as a strategy advisor (戰略顧問) to the President of the Republic of China in 2 terms, and then the permanent title as the reviewer member (中評委) of the Central Committee of the ruling party, Kuomintang (Chinese Nationalist Party) till his death on Feb. 28, 2016. His official funeral was proceeded with his coffin covered by the National Flag, the military salute of the top-ranked generals, and Vice-president Wu Den-yih presenting the Commendation Decree of President Ma Ying-jeou, who praises Zhao's 50-year career in national security with so-called "loyalty, diligence, bravery, perseverance, intelligence, wisdom, insight and proficiency" (忠勤勇毅，才識閎通), whereas "his virtue and conducts have set a good example model for future generation to follow..." (武德景行，貽範永式... 逾五十載攄忠護民，越半世紀衛國干城，崇勛盛業，青史聿昭).

The Government of the Republic of China has never rendered an apology nor any compensation to the victim families or the victim country.




</doc>
<doc id="462662" url="https://en.wikipedia.org/wiki?curid=462662" title="Kfar Etzion massacre">
Kfar Etzion massacre

The Kfar Etzion massacre refers to a massacre of Jews that took place after a two-day battle in which Jewish Kibbutz residents and Haganah militia defended Kfar Etzion from a combined force of the Arab Legion and local Arab men on May 13, 1948, the day before the Israeli Declaration of Independence.
Of the 129 Haganah fighters and Jewish kibbutzniks who died during the defence of the settlement, Martin Gilbert states that fifteen were murdered on surrendering.

Controversy surrounds the responsibility and role of the Arab Legion in the killing of those who surrendered. The official Israeli version maintains that the kibbutz residents and Haganah soldiers were massacred by local Arabs and the Arab Legion of the Jordanian Army as they were surrendering. The Arab Legion version maintains that the Legion arrived too late to prevent the attack on the kibbutz by men from nearby Arab villages, which was allegedly motivated by a desire to revenge the massacre of Deir Yassin, and the destruction of one of their villages several months earlier. The surrendering Jewish residents and fighters are said to have been assembled in a courtyard, only to be suddenly fired upon; it is said that many died on the spot, while most of those who managed to flee were hunted down and killed.

Four prisoners survived the massacre and were transferred to Transjordan. Immediately following the surrender on May 13, the kibbutz was looted and razed to the ground. The members of the three other kibbutzim of the Gush Etzion surrendered the next day and were taken as POWs to Jordan.

The bodies of the victims were left unburied until, one and a half years later, the Jordanian government allowed Shlomo Goren to collect the remains, which were then interred at Mount Herzl. The survivors of the Etzion Bloc were housed in former Arab houses in Jaffa.

Kfar Etzion was a kibbutz founded in 1943, for military and agricultural ends, about 2 km west of the road between Jerusalem and Hebron. By the end of 1947, there were 163 adults and 50 children living there. Together with three nearby kibbutzim established 1945-1947, it formed "Gush Etzion" ("the Etzion Bloc"). According to one member of the settlement, relations were good between settlers and local Arabs, with attendance at each other's weddings, until November 1949.
The United Nations partition plan for Palestine of November 29, 1947 placed the bloc, an enclave in a purely Arab area, inside the boundaries of the intended Arab state, where, moreover, Jewish settlement was to be forbidden through a transitional period. For Hebronite Arabs, the bloc constituted an 'alien intrusion' on ground that had been wholly Arab for centuries,' though it had been built on land either purchased by Jews (1928) or acquired by them through a complex circumvention of Mandatory law in 1942. According to Henry Laurens, Kfar Etzion had started hostilities in the area in December by destroying a local Arab village. On 10 December, a convoy from Bethlehem en route to the Gush Etzion bloc was ambushed and 10 of its 26 passengers and escorts were killed. Though on January 5, the children and some women had been evacuated with British assistance, and though David Shaltiel recommended its evacuation, the Haganah, on Yigal Yadin's counsel, decided against withdrawing from the settlements for several reasons: they commanded a strategic position on Jerusalem's southern approach from Hebron, and were considered, in the words of Abdullah Tall, a 'sharp thorn stuck in the heart of a purely Arab area'. Several relief convoys from the Haganah in Jerusalem had been ambushed.

In the months prior to May 15, Haganah militiamen in the bloc's kibbutzim repeatedly fired on Arab civilians, and British traffic, including convoys, moving between Jerusalem and Hebron, under instruction to do so in order to draw and drain Arab forces from the fight for Jerusalem. On two occasions, April 12 and May 3, Arab Legion units were ambushed, and several legionnaires killed or wounded by the bloc militias, - Kfar Etzion soldiers being directly involved in the incident on April 12 - Arab irregular forces made small-scale attacks against the settlements. An emergency reinforcement convoy attempting to march to Gush Etzion under cover of darkness was discovered and its members killed by Palestinian Arab forces. Despite some emergency flights by an Auster from Jerusalem and Piper Cubs out of Tel Aviv onto an improvised airfield, adequate supplies were not getting in.

As the end of the British Mandate drew closer, the fighting in the region intensified. Although the Arab Legion was theoretically in Palestine under British command, they began to operate more and more independently. On March 27, land communication with the rest of the Yishuv was severed completely when the Nebi Daniel Convoy was ambushed on its return to Jerusalem, and 15 Haganah soldiers died before the remainder were extricated by the British. It was ambushes by the Etzion Bloc militias conducted against Arab Legion units on April 12 and May 4 that, according to a Hanagah analysis, tipped the Legion's policy towards the bloc from one of isolating it to destroying it. On May 4, following the last ambush of a Legion convoy, a joint force of British, Arab Legion and irregular troops launched a major punitive attack on Kfar Etzion. The Haganah abandoned a few outposts but generally resisted, and the attack failed, leaving 12 Haganah soldiers dead, 30 wounded, with a similar number of Arab legionnaires killed, and several dozen wounded. Units from the bloc may have attacked Arab traffic the following day, but the failure of the Legion's assault led Hebronites and Legion units to plan a final attack and destroy the Etzion Bloc militarily.
The final assault on Kfar Etzion began on May 12. Parts of two Arab Legion companies, assisted by hundreds of local irregulars, had a dozen armored cars and artillery, to which the Jewish defenders had no effective answer. The commander of Kfar Etzion requested from the Central Command in Jerusalem permission to evacuate the kibbutz, but was ordered to stay. Later in the day, the Arabs captured the Russian Orthodox monastery, which the Haganah used as a perimeter fortress for the Kfar Etzion area, killing twenty-four of its thirty-two defenders.

On May 13, an attack broke through Kfar Etzion's defences and reached the settlement's centre effectively cutting off the perimeter outposts from each other.

In the Israeli mainstream version, when the hopelessness of their position became undeniable on May 13, dozens of defenders, the "haverim", of Kfar Etzion laid down their arms and assembled in the courtyard, where they suddenly began to be shot at. Those not slain in the first volleys of fire pushed past the Arabs, and either escaped to hide, or gathered their weapons, and were hunted down. The number of people killed and the perpetrators, the Arab legion or local village irregulars or both, are in dispute. According to one account, the main group of about 50 defenders were surrounded by a large number of Arab irregulars, who shouted "Deir Yassin!" and ordered the Jews to sit down, stand up, and sit down again, when suddenly someone opened fire on the Jews with a machine gun and others joined in the killing. Those Jews not immediately cut down tried to run away but were pursued. According to Meron Benvenisti, hand grenades were thrown into a cellar, killing a group of 50 who were hiding there. The building was blown up.

According to other sources, 20 women hiding in a cellar were killed. David Ohana writes that 127 Israeli fighters were killed on the last day.
Arab losses during the two day battle, according to a Haganah estimate, numbered 69: 42 irregulars, and 27 legionnaires. A number of Israeli histories of the Kfar Etzion massacre (such as Levi, 1986, Isseroff, 2005) state that the defenders had put out the white flag and lined up to surrender in front of the school building of the German monastery. An Arab version recounts that a white flag was flown, and drew the Arabs into a trap where they were fired on. Benny Morris cites a Legion officer's statement that the defenders had not formally surrendered, that some resistance continued, with shooting at Arabs, after others had surrendered, that local villagers shot legionnaires trying to defend prisoners, and that legionnaires had to shoot some villagers engaged in the killings. The figure of 127 massacred appears to include both those who surrendered only to be slain, and the defenders who had been killed in battle over 12–13 May.

In another account, after the 133 defenders had assembled, they were photographed by a man in a kaffiyeh, and then an armored car apparently belonging to the Arab Legion opened fire with its machine gun, and then Arab irregulars joined in. A group of defenders managed to crawl into the cellar of the monastery, where they defended themselves until a large number of grenades were thrown into the cellar. The building was then blown up and collapsed on them. About 129 persons died in the battle and its aftermath. Only three of the remaining Kfar Etzion residents and one Palmach member survived. According to their own testimony, the circumstances of their survival were as follows.

Both Alisaa and Nahum said that the Legion soldiers actively participated in the Massacre.
A total of 157 defenders died in the battle of Gush Etzion (Levi, 1986), including those killed in the massacre at Kfar Etzion. About 2/3 of them were residents and the remainder were Hagana or Palmach soldiers.

On the following day, the Arab irregular forces continued their assault on the remaining three Etzion settlements. Fearing that the defenders might suffer the same fate as those of Kfar Etzion, Zionist leaders in Jerusalem negotiated a deal for the surrender of the settlements on condition that the Arab Legion protected the residents. The Red Cross took the wounded to Jerusalem, and the Arab Legion took the remainder as prisoners of war. In March 1949 320 prisoners from the Etzion settlements were released from the "Jordan POW camp at Mafrak", including 85 women.

On October 28, 1948, the Arab village al-Dawayima was conquered by the IDF 89th Commando Battalion. The Al-Dawayima massacre then took place, as the villagers were blamed for the Kfar Etzion massacre. Estimates of the number of murdered Arab villagers range from 80–100 to 100–200, depending on the source.

The bodies of the murdered of Kfar Etzion were left at the site for a year and a half, until in November 1949, the Chief Military Rabbi, Shlomo Goren was allowed to collect their bones. They were buried in a full military funeral on November 17 in Mount Herzl in Jerusalem. Their communal grave was the first grave in what is today the military cemetery of Mount Herzl.

The Etzion Bloc became a symbol of Zionist heroism and martyrdom among Israelis immediately after its fall, and this importance continues. The date of the massacre was enshrined as Israel's Day of Remembrance.

The site of the Etzion Bloc was recaptured by Israel during the 1967 war. The children who had been evacuated from the Bloc in 1948 led a public campaign for the Bloc to be resettled, and Prime Minister Levi Eshkol gave his approval. Kfar Etzion was re-established as a kibbutz in September 1967, as the first Israeli settlement in the West Bank after the war.




</doc>
<doc id="849750" url="https://en.wikipedia.org/wiki?curid=849750" title="List of war crimes">
List of war crimes

This article lists and summarizes the war crimes committed since the Hague Conventions of 1899 and 1907 and the crimes against humanity and crimes against peace that have been committed since these crimes were first defined in the Rome Statute.

Since many war crimes are not ultimately prosecuted (due to lack of political will, lack of effective procedures, or other practical and political reasons), historians and lawyers will often make a serious case that war crimes occurred, even if there was no formal investigations or prosecution of the alleged crimes or an investigation cleared the alleged perpetrators.

War crimes under international law were firmly established by international trials such as the Nuremberg Trials and the Tokyo Trials, in which Austrian, German and Japanese leaders were prosecuted for war crimes committed during World War II.
The term "concentration camp" was used to describe camps operated by the British Empire in South Africa during the Second Boer War in the years 1900–1902. As Boer farms were destroyed by the British under their "Scorched Earth" policy, many tens of thousands of women and children were forcibly moved into the concentration camps. Over 26,000 Boer women and children were to perish in these concentration camps.

In November 1901, the Manila correspondent of the "Philadelphia Ledger" wrote: "The present war is no bloodless, opera bouffe engagement; our men have been relentless, have killed to exterminate men, women, children, prisoners and captives, active insurgents and suspected people from lads of ten up, the idea prevailing that the Filipino as such was little better than a dog..."

In response to the Balangiga massacre, which wiped out a U.S. company garrisoning Samar town, U.S. Brigadier General Jacob H. Smith launched a retaliatory march across Samar with the instructions: "I want no prisoners. I wish you to kill and burn, the more you kill and burn the better it will please me. I want all persons killed who are capable of bearing arms in actual hostilities against the United States, ..."

The war resulted in the deaths of at least 200,000 Filipino civilians. Some estimates for total civilian dead reach up to 1,000,000.

World War I was the first major international conflict to take place following the codification of war crimes at the Hague Convention of 1907, including derived war crimes, such as the use of poisons as weapons, as well as crimes against humanity, and derivative crimes against humanity, such as torture, and genocide. Before, the Second Boer War took place after the Hague Convention of 1899. The Second Boer War (1899 until 1902) is known for the first concentration camps (1900 until 1902) for civilians in the 20th century.



At least 50,000 people were executed during the Spanish Civil War. In his updated history of the Spanish Civil War, Antony Beevor writes, "Franco's ensuing 'white terror' claimed 200,000 lives. The 'red terror' had already killed 38,000." Julius Ruiz concludes that "although the figures remain disputed, a minimum of 37,843 executions were carried out in the Republican zone with a maximum of 150,000 executions (including 50,000 after the war) in Nationalist Spain."

César Vidal puts the number of Republican victims at 110,965. In 2008 a Spanish judge, Socialist Baltasar Garzón, opened an investigation into the executions and disappearances of 114,266 people between 17 July 1936 and December 1951. Among the murders and executions investigated was that of poet and dramatist Federico García Lorca.

The Axis Powers (Germany, Italy, and Japan) were some of the most systematic perpetrators of war crimes in modern history. Contributing factors included Nazi race theory, a desire for "living space" that justified the eradication of native populations, and militaristic indoctrination that encouraged the terrorization of conquered peoples and prisoners of war. The Holocaust, the German attack on the Soviet Union and occupation of much of Europe, the Japanese occupation of Manchuria and the Philippines and attack on China all contributed to well over half of the civilian deaths in World War II and the conflicts that led up to the war. Even before post-war revelations of atrocities, Axis militaries were notorious for their brutal treatment of captured combatants.

According to the Nuremberg Trials, there were four major war crimes that were alleged against German military (and Waffen-SS and NSDAP) men and officers, each with individual events that made up the major charges.

1. Participation in a common plan of conspiracy for the accomplishment of crimes against peace

2. Planning, initiating and waging wars of aggression and other crimes against peace

3. War Crimes
Atrocities against enemy combatants or conventional crimes committed by military units (see War crimes of the Wehrmacht), and include:

4. Crimes against Humanity
Crimes committed well away from the lines of battle and unconnected in any way to military activity, distinct from war crimes
Other crimes against humanity included:

At least 10 million, and perhaps over 20 million perished directly and indirectly due to the commission of crimes against humanity and war crimes by the Nazi regime, of which the Holocaust lives on in particular infamy, for its particularly cruel nature and scope, and the industrialised nature of the genocide of Jewish citizens of states invaded or controlled by the Nazi regime. At least 5.9 million Jews were murdered by the Nazis, or 66 to 78% of Europe's Jewish population, although a complete count may never be known. Though much of Continental Europe suffered under the Nazi occupation, Poland, in particular, was the state most devastated by these crimes, with 90% of its Jews as well as many ethnic Poles slaughtered by the Nazis and their Ukrainian affiliates. After the war, from 1945–49, the Nazi regime was put on trial in two tribunals in Nuremberg, Germany by the victorious Allied powers.

The first tribunal indicted 24 major Nazi war criminals, and resulted in 19 convictions (of which 12 led to death sentences) and 3 acquittals, 2 of the accused died before a verdict was rendered, at least one of which by killing himself with cyanide. The second tribunal indicted 185 members of the military, economic, and political leadership of Nazi Germany, of which 142 were convicted and 35 were acquitted. In subsequent decades, approximately 20 additional war criminals who escaped capture in the immediate aftermath of World War II were tried in West Germany and Israel. In Germany and many other European nations, the Nazi Party and denial of the Holocaust is outlawed.



This section includes war crimes from 7 December 1941 when the United States was attacked by Japan and entered World War II. For war crimes before this date which took place during the Second Sino-Japanese War, please see the section above which is titled .
Chetnik ideology revolved around the notion of a Greater Serbia within the borders of Yugoslavia, to be created out of all territories in which Serbs were found, even if the numbers were small. A directive dated 20 December 1941, addressed to newly appointed commanders in Montenegro, Major Đorđije Lašić and Captain Pavle Đurišić, outlined, among other things, the cleansing of all non-Serb elements in order to create a Greater Serbia:

The Chetniks systemically massacred Muslims in villages that they captured. In late autumn of 1941 the Italians handed over the towns of Višegrad, Goražde, Foča and the surrounding areas, in south-east Bosnia to the Chetniks to run as a puppet administration and NDH forces were compelled by the Italians to withdraw from there. After the Chetniks gained control of Goražde on 29 November 1941, they began a massacre of Home Guard prisoners and NDH officials that became a systematic massacre of the local Muslim civilian population.

Several hundred Muslims were murdered and their bodies were left hanging in the town or thrown into the Drina river. On 5 December 1941, the Chetniks received the town of Foča from the Italians and proceeded to massacre around 500 Muslims. Additional massacres against the Muslims in the area of Foča took place in August 1942. In total, more than 2000 people were killed in Foča.

In early January, Chetniks entered Srebrenica and killed around 1000 Muslim civilians there and in nearby villages. Around the same time the Chetniks made their way to Višegrad where deaths were reportedly in the thousands. Massacres continued in the following months in the region. In the village of Žepa alone about three hundred were killed in late 1941. In early January, Chetniks massacred fifty-four Muslims in Čelebić and burned down the village. On 3 March, Chetniks burned forty-two Muslim villagers to death in Drakan.

In early January 1943 and again in early February, Montenegrin Chetnik units were ordered to carry out "cleansing actions" against Muslims, first in the Bijelo Polje county in Sandžak and then in February in the Čajniče county and part of Foča county in southeastern Bosnia, and in part of the Pljevlja county in Sandžak.

Pavle Đurišić, the officer in charge of these operations, reported to Mihailović, Chief of Staff of the Supreme Command, that on 10 January 1943: "thirty-three Muslim villages had been burned down, and 400 Muslim fighters (members of the Muslim self-protection militia supported by the Italians) and about 1,000 women and children had been killed, as against 14 Chetnik dead and 26 wounded".

In another report sent by Đurišić dated 13 February 1943, he reported that: "Chetniks killed about 1,200 Muslim fighters and about 8,000 old people, women, and children; Chetnik losses in the action were 22 killed and 32 wounded". He added that "during the operation the total destruction of the Muslim inhabitants was carried out regardless of sex and age". The total number of deaths caused by the anti-Muslim operations between January and February 1943 is estimated at 10,000. The casualty rate would have been higher had a great number of Muslims not already fled the area, most to Sarajevo, when the February action began. According to a statement from the Chetnik Supreme Command from 24 February 1943, these were countermeasures taken against Muslim aggressive activities; however, all circumstances show that these massacres were committed in accordance with implementing the directive of 20 December 1941.

Actions against the Croats were of a smaller scale but comparable in action. In early October 1942 in the village of Gata, where an estimated 100 people were killed and many homes burnt in reprisal taken for the destruction of roads in the area carried out on the Italians' account. That same month, formations under the command of Petar Baćović and Dobroslav Jevđević, who were participating in the Italian Operation Alfa in the area of Prozor, massacred over 500 Croats and Muslims and burnt numerous villages. Baćović noted that "Our Chetniks killed all men 15 years of age or older. ... Seventeen villages were burned to the ground." Mario Roatta, commander of the Italian Second Army, objected to these "massive slaughters" of noncombatant civilians and threatened to halt Italian aid to the Chetniks if they did not end.

The Ustaša intended to create an ethnically "pure" Greater Croatia, and they viewed those Serbs then living in Croatia, Bosnia and Herzegovina as the biggest obstacle to this goal. Ustasha ministers Mile Budak, Mirko Puk and Milovan Žanić declared in May 1941 that the goal of the new Ustasha policy was an ethnically pure Croatia. The strategy to achieve their goal was:

The Independent State of Croatia government cooperated with Nazi Germany in the Holocaust and exercised their own version of the genocide against Serbs, as well as Jews and Gypsies (Roma) (aka "gypsies") inside its borders. State policy towards Serbs had first been declared in the words of Milovan Žanić, a minister of the NDH Legislative council, on 2 May 1941:

This country can only be a Croatian country, and there is no method we would hesitate to use in order to make it truly Croatian and cleanse it of Serbs, who have for centuries endangered us and who will endanger us again if they are given the opportunity. According to the Simon Wiesenthal Center (citing the "Encyclopedia of the Holocaust"), "Ustasa terrorists killed 500,000 Serbs, expelled 250,000 and forced 250,000 to convert to Roman Catholicism. They murdered thousands of Jews and Gypsies." The execution methods used by the Ustasha were particularly brutal and sadistic and often included torture, dismemberment or decapitation. A Gestapo report to Heinrich Himmler from 1942 stated, "The Ustaše committed their deeds in a bestial manner not only against males of conscript age but especially against helpless old people, women and children."

Numerous concentration camps were built in the NDH, most notably Jasenovac, the largest, where around 100,000 Serbs, Jews, Roma, as well as a number of Croatian political dissidents, died, mostly from torture and starvation. It was established in August 1941 and not dismantled until April 1945, shortly before the end of the war. Jasenovac was a complex of five subcamps and three smaller camps spread out over , in relatively close proximity to each other, on the bank of the Sava river. Most of the camp was at Jasenovac, about southeast of Zagreb. The complex also included large grounds at Donja Gradina directly across the Sava River, the Jastrebarsko children's camp to the northwest, and the Stara Gradiška camp (Jasenovac V) for women and children to the southeast.

Unlike Nazi camps, most murders at Jasenovac were done manually using hammers, axes, knives and other implements. According to testimony, on the night of August 29, 1942, guards at the camp organized a competition to see who could slaughter the most inmates, with guard and former Franciscan priest Petar Brzica winning by cutting the throats of 1,360 inmates. A special knife called called a "Srbosjek" (Serb-cutter) was designed for the slaughtering of prisoners. Prisoners were sometimes tied with barbed wire, then taken to a ramp near to the Sava River where weights were placed on the wires, their throats and stomachs slashed before their bodies were dumped into the river. After unsuccessful experiments with gas vans, camp commander Vjekoslav Luburić had a gas chamber built at Jasenovac V, where a considerable number of inmates were killed during a three-month experiment with sulfur dioxide and Zyklon B, but this method was abandoned due to poor construction. The Ustashe cremated living inmates as well as corpses. Other methods of torture and killing done included: inserting hot nails under finger nails, mutilating parts of the body including plucking out eyeballs, tightening chains around ones head until the skull fractured and the eyes popped and also, placing salt in open wounds. Women were subjected to rape and torture, including breast mutilation. Pregnant women had their wombs cut out.

An escape attempt on 22 April 1945 by 600 male inmates failed and only 84 male prisoners escaped successfully. The remainder and about 400 other prisoners were then murdered by Ustasha guards, despite the fact that they knew the war was ending with Germany's capitulation. All the female inmates from the women's camp (more than 700) had been massacred by the guards the previous day. The guards then destroyed the camp and everything associated with it was burned to the ground. Other concentration camps were the Đakovo camp, Gospić camp, Jadovno camp, Kruščica camp and the Lepoglava camp.

Ustasha militias and death squads also burnt villages and killed thousands of civilian Serbs in the country-side in sadistic ways with various weapons and tools. Men, women, children were hacked to death, thrown alive into pits and down ravines, or set on fire in churches. Some Serb villages near Srebrenica and Ozren were wholly massacred, while children were found impaled by stakes in villages between Vlasenica and Kladanj. The Glina massacres, where thousands of Serbs were killed, are among the more notable instances of Ustasha cruelty.

Ante Pavelić, leader of the Ustasha, fled to Argentina and Spain which gave him protection, and was never extradited to stand trial for his war crimes. Pavelić died on 28 December 1959 at the Hospital Alemán in Madrid, where the Roman Catholic church had helped him to gain asylum, at the age of 70 from gunshot wounds sustained in an earlier assassination attempt by Montenegrin Blagoje Jovović. Some other prominent Ustashe figures and their respective fates:


Most Ustashe fled the country following the war, mainly with the help of Father Krunoslav Draganović, secretary of the College of Sian Girolamo who helped Ustasha fugitives immigrate illegally to South America.

The Ukrainian OUN-B group, along with their military force – Ukrainian Insurgent Army(UPA) – are responsible for a genocide on the Polish population in Volhynia and Eastern Galicia. Starting in March 1943, with its peak in the summer 1943, as many as 130,000 people (according to Ewa Siemaszko) were murdered, mostly women, children and elderly. Although the main target were Poles, many Jews, Czechs and those Ukrainians unwilling to participate in the crimes, were massacred as well. Lacking good armament and ammunition, UPA members commonly used tools such as axes and pitchforks for the slaughter. As a result of these massacres, almost the entire non-Ukrainian population of Volhynia was either killed or forced to flee.

UPA commanders responsible for the genocide:

The French Union's struggle against the independence movement backed by the Soviet Union and China claimed 500,000 to 1.5 million Vietnamese lives from 1945 to 1954. In the Haiphong massacre of 1946, about 6,000 Vietnamese were killed by naval artillery. The French employed electric shock treatment during interrogations of the Vietnamese, and nearly 10,000 Vietnamese perished in French concentration camps.

The French repressed the independence movement with killings and village burnings. Up to 90,000 local residents died in the fighting, along with about 800 French and other Europeans.

Several massacres were committed during this war which could be described as war crimes. Nearly 15,000 people, mostly combatants and militants, were killed during the war, including 6,000 Jews and about 8,000 Arabs.





The insurgency began in 1945 and was revived in 1954, winning independence in the early 1960s. The French army killed thousands of Algerians in the first round of fighting in 1945. After the Algerian independence movement formed a National Liberation Front (FLN) in 1954, the French Minister of the Interior joined the Minister of National Defense in 1955 in ordering that every rebel carrying a weapon, suspected of doing so, or suspected of fleeing, must be shot. French troops executed civilians from nearby villages when rebel attacks occurred, tortured both rebels and civilians, and interned Arabs in camps, where forced labor was required of some of them. 2,000,000 Algerians were displaced or forcibly resettled during the war. French sources estimated that 70,000 Muslim civilians were killed, or abducted and presumed killed, by the FLN during the war. The FLN also killed 30,000 to 150,000 in people in post-war reprisals.

During the war 95 U.S. Army personnel and 27 U.S. Marine Corps personnel were convicted by court-martial of the murder or manslaughter of Vietnamese.




It is estimated that Bangladesh guerilla army killed about 1,000 to 150,000 biharis or pro-Pakistani razakars.

The Extraordinary Chambers in the Courts of Cambodia for the Prosecution of Crimes Committed During the Period of Democratic Kampuchea, commonly known as the Cambodia Tribunal, is a joint court established by the Royal Government of Cambodia and the United Nations to try senior members of the Khmer Rouge for crimes against humanity committed during the Cambodian Civil War. The Khmer Rouge killed many people due to their political affiliation, education, class origin, occupation, or ethnicity.

During the 1975 invasion and the subsequent occupation, Indonesian forces murdered tens of thousands of civilians.

This war has ravaged the country for over 30 years, with several foreign actors playing important roles during different periods. Since 2001 US and NATO troops have been fighting in Afghanistan in the "War on Terror" that is also treated in the corresponding section below.

Over 100,000 civilians other than those killed in Saddam's genocide are estimated to have been killed by both sides of the war by R.J.Rummel.

"The Times" reports (November 26, 2005 p. 27):

Almost 20 years of fighting... has killed half a million people. Many of the dead are children... The LRA [a cannibalism cult] kidnaps children and forces them to join its ranks. And so, incredibly, children are not only the main victims of this war, but also its unwilling perpetrators... The girls told me they had been given to rebel commanders as "wives" and forced to bear them children. The boys said they had been forced to walk for days knowing they would be killed if they showed any weakness, and in some cases forced even to murder their family members... every night up to 10,000 children walk into the centre of Kitgum... because they are not safe in their own beds... more than 25,000 children have been kidnapped ...this year an average of 20 children have been abducted every week.

Also see List of ICTY indictees for a variety of war criminals and crimes during this era.

From "The Times" March 28, 2006 p. 43:

During the Algerian Civil War of the 1990s, a variety of massacres occurred through the country, many being identified as war crimes. The Armed Islamic Group (GIA) has avowed its responsibility for many of them, while for others no group has claimed responsibility. In addition to generating a widespread sense of fear, these massacres and the ensuing flight of population have resulted in serious depopulation of the worst-affected areas. The massacres peaked in 1997 (with a smaller peak in 1994), and were particularly concentrated in the areas between Algiers and Oran, with very few occurring in the east or in the Sahara.

During the First Chechen War (1994–1996) and Second Chechen War (1999–2000 battle phase, 2000–2009 insurgency phase) there were many allegations of war crimes and terrorism against both sides from various human rights organizations.



Allegations of war crimes in the 2006 Lebanon War refer to claims of various groups and individuals, including Amnesty International, Human Rights Watch, and United Nations officials, who accused both Hezbollah and Israel of violating international humanitarian law during the 2006 Lebanon War, and warned of possible war crimes. These allegations included intentional attacks on civilian populations or infrastructure, disproportionate or indiscriminate attacks in densely populated residential districts.

According to various media reports, between 1,000 and 1,200 Lebanese citizens were reported dead; there were between 1,500 and 2,500 people wounded and over 1,000,000 were temporarily displaced. Over 150 Israelis were killed (120 military); thousands wounded; and 300,000–500,000 were displaced because of Hezbollah firing tens of thousands of rockets at major cities in Israel.

During the Darfur conflict, Civil war in Chad (2005–2010)
The conflict in Darfur has been variously characterised as a genocide.

Sudanese authorities claim a death toll of roughly 19,500 civilians while many non-governmental organizations, such as the Coalition for International Justice, claim over 400,000 people have been killed.

In September 2004, the World Health Organization estimated there had been 50,000 deaths in Darfur since the beginning of the conflict, an 18-month period, mostly due to starvation. An updated estimate the following month put the number of deaths for the 6-month period from March to October 2004 due to starvation and disease at 70,000; These figures were criticised, because they only considered short periods and did not include deaths from violence. A more recent British Parliamentary Report has estimated that over 300,000 people have died, and others have estimated even more.

There were allegations of war crimes by both the Israeli military and Hamas. Criticism of Israel's conduct focused on the proportionality of its measures against Hamas, and on its alleged use of weaponised white phosphorus. Numerous reports from human right groups during the war claimed that white phosphorus shells were being used by Israel, often in or near populated areas. In its early statements the Israeli military denied using any form of white phosphorus, saying "We categorically deny the use of white phosphorus". It eventually admitted to its limited use and stopped using the shells, including as a smoke screen. The Goldstone report investigating possible war crimes in the 2009 war accepted that white phosphorus is not illegal under international law but did find that the Israelis were "systematically reckless in determining its use in build-up areas". It also called for serious consideration to be given to the banning of its use as an obscurant.

There are allegations that war crimes were committed by the Sri Lankan military and the rebel Liberation Tigers of Tamil Eelam during the Sri Lankan Civil War, particularly during the final months of the conflict in 2009. The alleged war crimes include attacks on civilians and civilian buildings by both sides; executions of combatants and prisoners by the government of Sri Lanka; enforced disappearances by the Sri Lankan military and paramilitary groups backed by them; acute shortages of food, medicine, and clean water for civilians trapped in the war zone; and child recruitment by the Tamil Tigers. It is widely accused that the Secretary of Defense Gotabaya Rajapakse (brother of President Mahinda Rajapaksa) ordered troops under his command to "Kill them All" when the troops on the grounds asked him for direction for handling the surrendering Tamil combatants.

A panel of experts appointed by UN Secretary-General (UNSG) Ban Ki-moon to advise him on the issue of accountability with regard to any alleged violations of international human rights and humanitarian law during the final stages of the civil war found "credible allegations" which, if proven, indicated that war crimes and crimes against humanity were committed by the Sri Lankan military and the Tamil Tigers. The panel has called on the UNSG to conduct an independent international inquiry into the alleged violations of international law. The Sri Lankan government has denied that its forces committed any war crimes and has strongly opposed any international investigation. It has condemned the UN report as "fundamentally flawed in many respects" and "based on patently biased material which is presented without any verification".

International organizations have accused the Syrian government, ISIL and other opposition forces of severe human rights violations, with many massacres occurring. Chemical weapons have been used many times during the conflict as well. The Syrian government is reportedly responsible for the majority of civilian casualties and war crimes, often through bombings. In addition, tens of thousands of protesters and activists have been imprisoned and there are reports of torture in state prisons. Over 470,000 people were killed in the war by 2017.




</doc>
<doc id="485683" url="https://en.wikipedia.org/wiki?curid=485683" title="1993 Sukhumi airliner attacks">
1993 Sukhumi airliner attacks

Five Tupolev civilian airliners belonging to Transair Georgia and Orbi Georgian Airways were hit during the war in Abkhazia by missiles allegedly fired by separatists in Sukhumi, Abkhazia. Over 150 people died in the attacks.

A Tu-134A was destroyed by Abkhaz small arms fire or missiles; there was no one on board.

A Tu-134 aircraft flying from Sochi was hit on approach to Sukhumi-Babusheri Airport by a Strela 2 surface-to-air missile. The plane crashed into the Black Sea, killing all five crew members and 22 passengers.

A Tu-154B aircraft flying from Tbilisi carrying civilians and internal security forces was on approach to Sukhumi-Babusheri Airport when it was struck by a Russian surface-to-air missiles. The plane crash-landed on the airstrip, and the ensuing fire killed 108 of the total 132 passengers and crew members.

Passengers were boarding a Tu-134 at Sukhumi when it was struck by rockets from an Abkhaz BM-21 Grad rocket launcher. It caught fire and burned out, leaving one crew member dead. The aircraft was due to operate a Sukhumi-Tbilisi service. On the same day a Tu-154 ("85359") was reportedly destroyed by mortar or artillery fire.


</doc>
<doc id="53779016" url="https://en.wikipedia.org/wiki?curid=53779016" title="Human shields (law)">
Human shields (law)

Human shields may be civilians used against their will to deter attacks on military targets during an international armed conflict or they may be civilians who voluntarily protect either military or civilian targets from attack. The use of human shields is forbidden by "Protocol I" of the Geneva Conventions. It is also a specific intent war crime as codified in the "Rome Statute", which was adopted in 1998. The language of the "Rome Statute" prohibits "utilizing the presence of a civilian or other protected person to render certain points, areas, or military forces immune from military operations."

Historically the law of armed conflict only applied to sovereign states. Non-international conflicts were governed by the domestic law of the State concerned. Under the current terms of the "Rome Statute" the use of human shields is defined as a war crime only in the context of an international armed conflict.

After the end of World War II, non-international armed conflicts have become more commonplace. The "Customary International Humanitarian Law" guide suggests that rules prohibiting use of civilians as human shields are "arguably" customary in non-international armed conflict. The development and application of humanitarian law to modern asymmetric warfare is currently being debated by legal scholars.

The laws of war first began to develop the distinction between military and civilian targets at The Second Hague Peace Conference of 1907.

During World War I, the concept of total war permitted most actions that supported the war effort. In "total war" targeting civilians was allowed, if it would support a military objective to demoralize the enemy. Indiscriminate bombing was considered an acceptable method to achieve the military advantage of defeating enemy morale and eroding popular support for the war effort. Early attempts to protect civilians as a class were largely unsuccessful. World War II was also fought within the framework of the total war concept.

The Geneva Conventions of 1949 were the first significant protections for civilians in war. These protections were expanded by the Additional Protocols in 1977. Protocol I requires that attacks be limited to military objectives, which are defined as targets that make an "effective contribution to military action" where the destruction of the target provides a "definite military advantage" to the attacker.

Military necessity can justify the use of force in certain circumstances, where there is a military advantage to be gained by an attack. When the use of force is excessive relative to its anticipated military advantage it is said to be disproportionate. Disproportionate force is prohibited under international law.

Risk to civilians does not bar military action, but the principle of proportionality requires that precautions be taken to minimize the harm to these protected persons. This analysis includes considerations like whether circumstances permit the attacker to time a military action to minimize the presence of civilians at the location.

Under the Rome Statute using protected persons as shields in an international armed conflict is a war crime. There is currently debate amongst legal scholars about whether traditional proportionality analysis should be modified to take into account the culpability of actors who use human shields to gain a strategic advantage. In modern asymmetric warfare it has become difficult to distinguish between military targets and civilians, but State actors still rely on traditional principles that present challenges when applied to asymmetric conflicts. Non-state forces, like guerillas and terrorists, conceal themselves among civilian populations and may take advantage of this position to launch attacks. When military action targeting these unconventional combatants results in civilian deaths, State actors may blame the deaths on enemy forces who use human shields.

Some scholars, including Ammon Rubinstein and Yaniv Roznai, argue that the use of human shields should be a factor in determining whether the use of force was justifiable under the guiding principles of distinction and proportionality. In their view, the use of human shields undermines an attackers right to self defense because the military necessity of self-defense must be a consideration in the excessive force analysis. Rubinstein and Roznai have described this analysis as a "proportionate proportionality."

Rubinstein and Roznai argue that an attack that would be disproportionate ought to be considered proportionate, if the presence of civilians is due to the wrongful actions of the enemy. They use the term "impeded party" to describe the burden placed on the attacking party under International Humanitarian Law norms. They point out that "attacking party" has traditionally been synonymous with the aggressor, but that it is often the attacker who is "defending democracy" and acting in self-defense when they use force in response to a prior attack.

Douglas Fischer believes that the increase of civilian casualties that began with the Vietnam War is partially due to an increased use of "illegal and perfidious" tactics in modern warfare, including the use of civilians as human shields. He has criticized Human Rights Watch for not including human shields doctrine as a factor in excessive force analysis.

Combatants in an armed conflict are prohibited from using protected civilians as involuntary human shields to support an unjust war effort. Civilians who are used as involuntary human shields by unlawful combatants do not lose their basic rights. The use of involuntary human shields does not release the other party from legal obligations to not target civilians or inflict excessive collateral damage.

Voluntary human shields may be considered "direct participants in hostilities," if they shield targeted personnel or properties. This could also be considered treason. However, if they are shielding protected personnel or properties, they may still retain their protected status. This debated area of customary international law has not yet been codified.

The United States and the European Union are considered the main sources for voluntary human shields. In 2003, human rights activists travelled to Baghdad to serve as human shields and protest the unpopular U.S. invasion.
Also in 2003, American peace activist Rachel Corrie was killed after she was crushed to death by an Israeli army bulldozer in Rafah while volunteering with the International Solidarity Movement as a human shield to prevent the demolition of homes in Palestine.

While IHL does prohibit attacks on civilians, the precautions a power must take before an attack remain ill-defined. Proportionality remains a nebulous standard that does not set a predictable standard of when a military action would be considered lawful. There is a lack of enforcement, and the increasing role of private actors and contractors on the battlefield presents new challenges.

During the Civil War, the United States adopted the Lieber Code, recognized by many scholars as the first detailed code governing conduct in war. Dr. Francis Lieber articulated an early version of the principle of proportionality: that civilians were not to be targeted, but were also not immune in all circumstances.

The use of human shields is prohibited and defined as a war crime by several U.S. military manuals. It is also defined as a crime triable by military commission under the US Military Commissions Act (2006).

If the belligerent attacks in areas where human shields are used, this can weaken international and domestic support by exploiting harmed civilians. For nations that are particularly sensitive to collateral damage, an enemy's use of shields may effectively deter or delay military actions.

There have been numerous documented incidents where this tactic has not been successful in deterring attacks, including the Amiriyah shelter bombing during the First Gulf War. After the death of two Western activists serving as voluntary human shields in Gaza, Véronique Dudouet wrote that human shields have become less effective, since bad media publicity no longer deters soldiers from using lethal force against them.


</doc>
<doc id="608199" url="https://en.wikipedia.org/wiki?curid=608199" title="War of aggression">
War of aggression

A war of aggression, sometimes also war of conquest, is a military conflict waged without the justification of self-defense, usually for territorial gain and subjugation. 

Wars without international legality (i.e. not out of self-defense nor sanctioned by the United Nations Security Council) can be considered wars of aggression; however, this alone usually does not constitute the definition of a war of aggression; certain wars may be unlawful but not aggressive (a war to settle a boundary dispute where the initiator has a reasonable claim, and limited aims, is one example).

In the judgment of the International Military Tribunal at Nuremberg, which followed World War II, "War is essentially an evil thing. Its consequences are not confined to the belligerent states alone, but affect the whole world. To initiate a war of aggression, therefore, is not only an international crime; it is the supreme international crime differing only from other war crimes in that it contains within itself the accumulated evil of the whole."
Article 39 of the United Nations Charter provides that the UN Security Council shall determine the existence of any act of aggression and "shall make recommendations, or decide what measures shall be taken in accordance with Articles 41 and 42, to maintain or restore international peace and security".

The Rome Statute of the International Criminal Court refers to the crime of aggression as one of the "most serious crimes of concern to the international community", and provides that the crime falls within the jurisdiction of the International Criminal Court (ICC). However, the Rome Statute stipulates that the ICC may not exercise its jurisdiction over the crime of aggression until such time as the states parties agree on a definition of the crime and set out the conditions under which it may be prosecuted. At the Kampala Review Conference on 11 June 2010, a total of 111 State Parties to the Court agreed by consensus to adopt a resolution accepting the definition of the crime and the conditions for the exercise of jurisdiction over this crime. The relevant amendments to the Statute, however has not been entered into force yet as of May 14, 2012.

Possibly the first trial for waging aggressive war is that of the Sicilian king Conradin in 1268.

The phrase is distinctly modern and diametrically opposed to the prior legal international standard of "might makes right", under the medieval and pre-historic beliefs of right of conquest. Since the Korean War of the early 1950s, waging such a war of aggression is a crime under the customary international law..

The origin of the concept, Maguire argues, emerged from the debate on Article 231 of the Treaty of Versailles of 1919: "Germany accepts the responsibility of Germany and her allies for causing all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by the aggression of Germany and her allies." Maguire argues: 

Two Conventions for the Definition of Aggression were signed in London on 3 and 4 July 1933. The first was signed by Czechoslovakia, Romania, the Soviet Union, Turkey and Yugoslavia, and came into effect on 17 February 1934, when it was ratified by all of them but Turkey. The second was signed by Afghanistan (ratified 20 October 1933), Estonia (4 December), Latvia (4 December), Persia (16 November), Poland (16 October), Romania (16 October), the Soviet Union (16 October) and Turkey, which ratified both treaties on 23 March 1934. Finland acceded to the second convention on 31 January 1934. The second convention was the first to be registered with the League of Nations Treaty Series on 29 March 1934, while the first was registered on 26 April. As Lithuania refused to sign any treaty including Poland, she signed the definition of aggression in a separate pact with the Soviet Union on 5 July 1933, also in London, and exchanged ratifications on 14 December. It was registered in the Treaty Series on 16 April 1934.

The signatories of both treaties were also signatories of the Kellogg–Briand Pact prohibiting aggression, and were seeking an agreed definition of the latter. Czechoslovakia, Romania and Yugoslavia were members of the Little Entente, and their signatures alarmed Bulgaria, since the definition of aggression clearly covered its support of the Internal Macedonian Revolutionary Organization. Both treaties base their definition on the "Politis Report" of the Committee of Security Questions made 24 March 1933 to the Conference for the Reduction and Limitation of Armaments, in answer to a proposal of the Soviet delegation. The Greek politician Nikolaos Politis was behind the inclusion of "support for armed bands" as a form of aggression. Ratifications for both treaties were deposited in Moscow, as the convention was primarily the work of Maxim Litvinov, the Soviet signatory. The convention defined an act of aggression as follows:

The League prerogative under that convention to expel a League member found guilty of aggression was used by the League Assembly only once, against the Soviet government itself, on December 14, 1939, following the Soviet invasion of Finland.

Primary documents:

In 1945, the London Charter of the International Military Tribunal defined three categories of crimes, including "crimes against peace". This definition was first used by Finland to prosecute the political leadership in the war-responsibility trials in Finland. The principles were later known as the Nuremberg Principles.

In 1950, the Nuremberg Tribunal defined Crimes against Peace, in Principle VI, specifically Principle VI(a), submitted to the United Nations General Assembly, as:
See: "Nuremberg Trials:" "The legal basis for the jurisdiction of the court was that defined by the Instrument of Surrender of Germany, political authority for Germany had been transferred to the Allied Control Council, which having sovereign power over Germany could choose to punish violations of international law and the laws of war. Because the court was limited to violations of the laws of war, it did not have jurisdiction over crimes that took place before the outbreak of war on September 1, 1939."

For committing this crime, the Nuremberg Tribunal sentenced a number of persons responsible for starting World War II. One consequence of this is that nations who are starting an armed conflict must now argue that they are either exercising the right of self-defense, the right of collective defense, or – it seems – the enforcement of the criminal law of "jus cogens". It has made formal declaration of war uncommon after 1945.

Reading the Tribunal's final judgment in court, British alternate judge Norman Birkett said:

Associate Supreme Court Justice William O. Douglas charged that the Allies were guilty of "substituting power for principle" at Nuremberg. "I thought at the time and still think that the Nuremberg trials were unprincipled," he wrote. "Law was created ex post facto to suit the passion and clamor of the time."

The relevant provisions of the Charter of the United Nations mentioned in the RSICC article 5.2 were framed to include the Nuremberg Principles. The specific principle is "Principle VI.a" "Crimes against peace", which was based on the provisions of the London Charter of the International Military Tribunal that was issued in 1945 and formed the basis for the post World War II war crime trials. The Charters provisions based on the Nuremberg Principle VI.a are:

The Inter-American Treaty of Reciprocal Assistance, signed in Rio de Janeiro on September 2, 1947, included a clear definition of aggression. Article 9 stated:

In addition to other acts which the Organ of Consultation may characterize as aggression, the following shall be considered as such:
The discussions on definition of aggression under the UN began in 1950, following the outbreak of the Korean War. As the western governments, headed by Washington, were in favor of defining the governments of North Korea and the People's Republic of China as aggressor states, the Soviet government proposed to formulate a new UN resolution defining aggression and based on the 1933 convention. As a result, on November 17, 1950, the General Assembly passed resolution 378, which referred the issue to be defined by the International Law Commission. The commission deliberated over this issue in its 1951 session and due to large disagreements among its members, decided "that the only practical course was to aim at a general and abstract definition (of aggression)". However, a tentative definition of aggression was adopted by the commission on June 4, 1951, which stated:

Aggression is the use of force by a State or Government against another State or Government, in any manner, whatever the weapons used and whether openly or otherwise, for any reason or for any purpose other than individual or collective self-defence or in pursuance of a decision or recommendation by a competent organ of the United Nations.
On December 14, 1974, the United Nations General Assembly adopted Resolution 3314, which defined the crime of aggression. This definition is not binding as such under international law, though it may reflect customary international law.

This definition makes a distinction between "aggression" (which "gives rise to international responsibility") and "war of aggression" (which is "a crime against international peace"). Acts of aggression are defined as armed invasions or attacks, bombardments, blockades, armed violations of territory, permitting other states to use one's own territory to perpetrate acts of aggression and the employment of armed irregulars or mercenaries to carry out acts of aggression. A war of aggression is a series of acts committed with a sustained intent. The definition's distinction between an "act" of aggression and a "war" of aggression make it clear that not every act of aggression would constitute a crime against peace; only war of aggression does. States would nonetheless be held responsible for acts of aggression.

The wording of the definition has been criticised by many commentators. Its clauses on the use of armed irregulars are notably vague, as it is unclear what level of "involvement" would entail state responsibility. It is also highly state-centric, in that it deems states to be the only actors liable for acts of aggression. Domestic or transnational insurgent groups, such as those that took part in the Sierra Leone Civil War and the Yugoslav Wars, were key players in their respective conflicts despite being non-state parties; they would not have come within the scope of the definition.

The Definition of Aggression also does not cover acts by international organisations. The two key military alliances at the time of the definition's adoption, NATO and the Warsaw Pact, were non-state parties and thus were outside the scope of the definition. Moreover, the definition does not deal with the responsibilities of individuals for acts of aggression. It is widely perceived as an insufficient basis on which to ground individual criminal prosecutions.

While this Definition of Aggression has often been cited by opponents of conflicts such as the 1999 Kosovo War and the 2003 Iraq War, it has no binding force in international law. The doctrine of "Nulla poena sine lege" means that, in the absence of binding international law on the subject of aggression, no penalty exists for committing acts in contravention of the definition. It is only recently that heads of state have been indicted over acts committed in wartime, in the cases of Slobodan Milošević of Serbia and Charles Taylor of Liberia. However, both were charged with war crimes, i.e. violations of the laws of war, rather than with the broader offence of "a crime against international peace" as envisaged by the Definition of Aggression.

The definition is not binding on the Security Council. The United Nations Charter empowers the General Assembly to make recommendations to the United Nations Security Council but the Assembly may not dictate to the Council. The resolution accompanying the definition states that it is intended to provide guidance to the Security Council to aid it "in determining, in accordance with the Charter, the existence of an act of aggression". The Security Council may apply or disregard this guidance as it sees fit. Legal commentators argue that the Definition of Aggression has had "no visible impact" on the deliberations of the Security Council.

The Rome Statute of the International Criminal Court lists the crime of aggression as one of the most serious crimes of concern to the international community, and provides that the crime falls within the jurisdiction of the International Criminal Court (ICC). However, Article 5.2 of the Rome Statute states that "The Court shall exercise jurisdiction over the crime of aggression once a provision is adopted in accordance with articles 121 and 123 defining the crime and setting out the conditions under which the Court shall exercise jurisdiction with respect to this crime. Such a provision shall be consistent with the relevant provisions of the Charter of the United Nations." The Assembly of States Parties of the ICC adopted such a definition in 2010 at the Review Conference in Kampala, Uganda.


List of reference documents (alphabetical by author):



</doc>
<doc id="48626" url="https://en.wikipedia.org/wiki?curid=48626" title="War crime">
War crime

A war crime is an act that constitutes a serious violation of the laws of war that gives rise to individual criminal responsibility. Examples of war crimes include intentionally killing civilians or prisoners, torturing, destroying civilian property, taking hostages, performing a perfidy, raping, using child soldiers, pillaging, declaring that no quarter will be given, and seriously violating the principles of distinction, proportionality, and military necessity.

The concept of war crimes emerged at the turn of the twentieth century when the body of customary international law applicable to warfare between sovereign states was codified. Such codification occurred at the national level, such as with the publication of the Lieber Code in the United States, and at the international level with the adoption of the treaties during the Hague Conventions of 1899 and 1907. Moreover, trials in national courts during this period further helped clarify the law. Following the end of World War II, major developments in the law occurred. Numerous trials of Axis war criminals established the Nuremberg principles, such as notion that war crimes constituted crimes defined by international law. Additionally, the Geneva Conventions in 1949 defined new war crimes and established that states could exercise universal jurisdiction over such crimes. In the late 20th century and early 21st century, following the creation of several international courts, additional categories of war crimes applicable to armed conflicts other than those between states, such as civil wars, were defined.

The trial of Peter von Hagenbach by an ad hoc tribunal of the Holy Roman Empire in 1474 was the first "international" war crimes trial, and also of command responsibility. He was convicted and beheaded for crimes that "he as a knight was deemed to have a duty to prevent", although he had argued that he was "just following orders".

In 1865, Henry Wirz, a Confederate States Army officer, was held accountable by a military tribunal and hanged for the appalling conditions at Andersonville Prison, where many Union prisoners of war died during the American Civil War.

The Hague Conventions were international treaties negotiated at the First and Second Peace Conferences at The Hague, Netherlands, in 1899 and 1907, respectively, and were, along with the Geneva Conventions, among the first formal statements of the laws of war and war crimes in the nascent body of secular international law.

The Geneva Conventions are four related treaties adopted and continuously expanded from 1864 to 1949 that represent a legal basis and framework for the conduct of war under international law. Every single member state of the United Nations has currently ratified the conventions, which are universally accepted as customary international law, applicable to every situation of armed conflict in the world. However, the Additional Protocols to the Geneva Conventions adopted in 1977 containing the most pertinent, detailed and comprehensive protections of international humanitarian law for persons and objects in modern warfare are still not ratified by a number of States continuously engaged in armed conflicts, namely the United States, Israel, India, Pakistan, Iraq, Iran, and others. Accordingly, states retain different codes and values with regard to wartime conduct. Some signatories have routinely violated the Geneva Conventions in a way which either uses the ambiguities of law or political maneuvering to sidestep the laws' formalities and principles.

Three conventions were revised and expanded with the fourth one added in 1949:

Two Additional Protocols were adopted in 1977 with the third one added in 2005, completing and updating the Geneva Conventions:

A small number of German military personnel of the First World War were tried in 1921 by the German Supreme Court for alleged war crimes.

The modern concept of war crime was further developed under the auspices of the Nuremberg Trials based on the definition in the London Charter that was published on August 8, 1945. (Also see Nuremberg Principles.) Along with war crimes the charter also defined crimes against peace and crimes against humanity, which are often committed during wars and in concert with war crimes.

Also known as the Tokyo Trial, the Tokyo War Crimes Tribunal or simply as the Tribunal, it was convened on May 3, 1946 to try the leaders of the Empire of Japan for three types of crimes: "Class A" (crimes against peace), "Class B" (war crimes), and "Class C" (crimes against humanity), committed during World War II.

On July 1, 2002, the International Criminal Court, a treaty-based court located in The Hague, came into being for the prosecution of war crimes committed on or after that date. Several nations, most notably the United States, China, Russia, and Israel, have criticized the court. The United States still participates as an observer. Article 12 of the Rome Statute provides jurisdiction over the citizens of non-contracting states in the event that they are accused of committing crimes in the territory of one of the state parties.

War crimes are defined in the statute that established the International Criminal Court, which includes:

However the court only has jurisdiction over these crimes where they are "part of a plan or policy or as part of a large-scale commission of such crimes".

The Saudi Arabian-led military intervention in Yemen has been dubbed as the world's worst humanitarian crisis by the United Nations.
The war of more than four years has led Yemen to the brink of extinction with millions of civilians facing famine and starvation at the hands of multiple warring parties; Houthi rebels, Saudi Arabia, and the United Arab Emirates.
A December 2019 report by the Amnesty International followed six months of research and documented the lack of medical attention and neglect faced by more than 4.5 million Yemenis, disabled in the war.

To date, the present and former heads of state and heads of government that have been charged with war crimes include:


War crimes are serious violations of the rules of customary and treaty law concerning international humanitarian law that have become accepted as criminal offenses for which there is individual responsibility.
Colloquial definitions of "war crime" include violations of established protections of the "laws of war", but also include failures to adhere to norms of procedure and rules of battle, such as attacking those displaying a peaceful flag of truce, or using that same flag as a ruse to mount an attack on enemy troops. The use of chemical and biological weapons in warfare are also prohibited by numerous chemical arms control agreements and the Biological Weapons Convention. Wearing enemy uniforms or civilian clothes to infiltrate enemy lines for espionage or sabotage missions is a legitimate ruse of war, though fighting in combat or assassinating individuals behind enemy lines while so disguised is not, as it constitutes unlawful perfidy. Attacking enemy troops while they are being deployed by way of a parachute is not a war crime. However, Protocol I, Article 42 of the Geneva Conventions explicitly forbids attacking parachutists who eject from disabled aircraft and surrendering parachutists once landed. Article 30 of the 1907 Hague Convention "IV – The Laws and Customs of War on Land" explicitly prohibits belligerents to punish enemy spies without previous trial.

The rule of war, also known as the Law of Armed Conflict, permit belligerents to engage in combat. A war crime occurs when superfluous injury or unnecessary suffering is inflicted upon an enemy.

War crimes also include such acts as mistreatment of prisoners of war or civilians. War crimes are sometimes part of instances of mass murder and genocide though these crimes are more broadly covered under international humanitarian law described as crimes against humanity. In 2008, the U.N. Security Council adopted Resolution 1820, which noted that "rape and other forms of sexual violence can constitute war crimes, crimes against humanity or a constitutive act with respect to genocide"; see also war rape. In 2016, the International Criminal Court convicted someone of sexual violence for the first time; specifically, they added rape to a war crimes conviction of Congo Vice President Jean-Pierre Bemba Gombo.

War crimes also included deliberate attacks on citizens and property of neutral states, such as the Japanese attack on Pearl Harbor. As the attack on Pearl Harbor happened while the U.S. and Japan were at peace and without a just cause for self-defense, the attack was declared by the Tokyo Trials to go beyond justification of military necessity and therefore constituted a war crime.

War crimes are significant in international humanitarian law because it is an area where international tribunals such as the Nuremberg Trials and Tokyo Trials have been convened. Recent examples are the International Criminal Tribunal for the Former Yugoslavia and the International Criminal Tribunal for Rwanda, which were established by the UN Security Council acting under Chapter VIII of the UN Charter.

Under the Nuremberg Principles, "war crimes" are different from crimes against peace. Crimes against peace include planning, preparing, initiating, or waging a war of aggression, or a war in violation of international treaties, agreements, or assurances. Because the definition of a state of "war" may be debated, the term "war crime" itself has seen different usage under different systems of international and military law. It has some degree of application outside of what some may consider to be a state of "war", but in areas where conflicts persist enough to constitute social instability.

The legalities of war have sometimes been accused of containing favoritism toward the winners ("Victor's justice"), as some controversies have not been ruled as war crimes. Some examples include the Allies' destruction of Axis cities during World War II, such as the firebombing of Dresden, the "Operation Meetinghouse" raid on Tokyo (the most destructive single bombing raid in history), and the atomic bombings of Hiroshima and Nagasaki. In regard to the strategic bombing during World War II, there was no international treaty or instrument protecting a civilian population specifically from attack by aircraft, therefore the aerial attacks on civilians were not officially war crimes. The Allies at the trials in Nuremberg and Tokyo never prosecuted the Germans, including Luftwaffe commander-in-chief Hermann Göring, for the bombing raids on Warsaw, Rotterdam, and British cities during the Blitz as well as the indiscriminate attacks on Allied cities with V-1 flying bombs and V-2 rockets, nor the Japanese for the aerial attacks on crowded Chinese cities. Although there are no treaties specific to aerial warfare, Protocol 1, Article 51 of the Geneva Conventions explicitly prohibits the bombardment of cities where civilian population might be concentrated regardless of any method. (see Aerial bombardment and international law).

Controversy arose when the Allies re-designated German POWs (under the protection of the 1929 Geneva Convention on Prisoners of War) as Disarmed Enemy Forces (allegedly unprotected by the 1929 Geneva Convention on Prisoners of War), many of which then were used for forced labor such as clearing minefields. By December 1945, six months after the war had ended, it was estimated by French authorities that 2,000 German prisoners were still being killed or maimed each month in mine-clearing accidents. The wording of the 1949 Third Geneva Convention was intentionally altered from that of the 1929 convention so that soldiers who "fall into the power" following surrender or mass capitulation of an enemy are now protected as well as those taken prisoner in the course of fighting.
Under the Law of Armed Conflict (LOAC), the death of non-combatants isn't necessarily a violation, there are many things to take into account. Civilians "cannot" be made the object of an attack, but the death/injury of civilians while conducting an attack on a military objective are governed under principles such as of proportionality and military necessity and can be permissible. Military necessity "permits the destruction of life of … persons whose destruction is incidentally unavoidable by the armed conflicts of the war; ... it does not permit the killing of innocent inhabitants for purposes of revenge or the satisfaction of a lust to kill. The destruction of property to be lawful must be imperatively demanded by the necessities of war"

For example, conducting an operation on ammunition depot or a terrorist training camp would not be prohibited because a farmer is plowing a field in the area, the farmer is not the object of attack and the operations would adhere to proportionality and military necessity. On the other hand, an extraordinary military advantage would be necessary to justify an operation posing risks of collateral death or injury to thousands of civilians. In "grayer cases" the legal question of whether the expected incidental harm is excessive may be very subjective. For this reason, States have chosen to apply a "clearly excessive" standard for determining whether a criminal violation has occurred.

When there is no justification for military action, such as civilians being made the object of attack, a proportionality analysis is unnecessary to conclude that the attack is unlawful.

For aerial strikes, pilots generally have to rely on information supplied external sources (Headquarters, ground troops) that a specific position is in fact a military target. In the case of former Yugoslavia, NATO pilots hit a civilian object (United States bombing of the Chinese embassy in Belgrade) that was of no military significance, but the pilots had no idea of determining it aside from their orders. The committee ruled that "the aircrew involved in the attack should not be assigned any responsibility for the fact they were given the wrong target and that it is inappropriate to attempt to assign criminal responsibility for the incident to senior leaders because they were provided with wrong information by officials of another agency.". The report also notes that "Much of the material submitted to the OTP consisted of reports that civilians had been killed, often inviting the conclusion to be drawn that crimes had therefore been committed. Collateral casualties to civilians and collateral damage to civilian objects can occur for a variety of reasons." 

The Rendulic Rule is a standard by which commanders are judged.

German General Lothar Rendulic was charged for ordering extensive destruction of civilian buildings and lands while retreating a suspected enemy attack in what is called scorched earth policy for the military purpose of denying the use of ground for the enemy. He overestimated the perceived risk but argued that Hague IV authorized the destruction because if it's necessary to war. He was acquitted of that charge.

Under the "Rendulic Rule" persons must assess the military necessity of an action based on the information available to them at that time; they cannot be judged based on information that subsequently comes to light.







</doc>
<doc id="56416531" url="https://en.wikipedia.org/wiki?curid=56416531" title="Razing of Friesoythe">
Razing of Friesoythe

The Razing of Friesoythe took place on 14 April 1945 during the Western Allies' invasion of Germany towards the end of World War II. The 4th Canadian (Armoured) Division attacked the German-held town of Friesoythe and one of its battalions, The Argyll and Sutherland Highlanders of Canada, captured it.

During the fighting the battalion's commander was killed by a German soldier, but it was incorrectly rumoured that he had been killed by a civilian. Under this mistaken belief, the division's commander, Major-General Christopher Vokes, ordered that the town be razed in retaliation and it was substantially destroyed. Twenty German civilians died in Friesoythe and the surrounding area during the two days of fighting and its aftermath. Similar, if usually less extreme, events occurred elsewhere in Germany as the Allies advanced in the closing weeks of the war.

The rubble of the town was used to fill craters in local roads to make them passable for the division's tanks and heavy vehicles. A few days earlier, the division had destroyed the centre of Sögel in another reprisal and also used the rubble to make the roads passable. Little official notice was taken of the incident and the Canadian Army official history glosses over it. It is covered in the regimental histories of the units involved and several accounts of the campaign. Forty years later, Vokes wrote in his autobiography that he had "no great remorse over the elimination of Friesoythe".

By September 1944, the Western Allies had reached Germany's western border, and by the end of October had captured Aachen, the first major German city to fall to them. Over the following six months they overran much of western Germany. In November the Supreme Headquarters Allied Expeditionary Force (SHAEF) publicly stated that the forces of the Western Allies would strictly adhere to international law in respect of their treatment of civilians. However, SHAEF's manual "Combating the Guerrilla" stated that there were circumstances where commanders could take "stern measures" against civilians as a rapid response to guerrilla attacks, although this was in breach of the Hague Conventions.

The frequency and nature of retaliatory actions differed between national contingents within the Western Allied forces. Following SHAEF's policy, United States Army forces destroyed German buildings on several occasions, sometimes entire villages, and took other measures against German civilians. French troops took a similar, if more rigorous, approach to that of the Americans. The British commanders disapproved of retaliations against civilians, and British troops carried out few reprisals.

The First Canadian Army served in the predominately British 21st Army Group, and more frequently retaliated against German civilians than the British. The commander of 4th Canadian (Armoured) Division, Major-General Christopher Vokes, believed that destroying property was the most appropriate way of responding to resistance by German civilians. The division carried out actions against German property more often than any other Canadian formation.

Soviet forces conducted retaliatory actions than more frequently than their Western Allies. The Soviet Union's leadership was concerned about the threat of a German resistance movement, and Soviet forces killed, raped and imprisoned large numbers of local civilians and destroyed property following guerrilla attacks.

The attitude towards the Germans throughout the Allied ranks was harsh. There was frustration at their continued resistance in a clearly hopeless cause, anger at the casualties they inflicted when the war was widely, and correctly, perceived to be almost over, and a general feeling that severe, even ruthless, treatment of German soldiers and civilians was justified. On 15 April the British reached Bergen-Belsen concentration camp, where the inmates had been reduced to cannibalism. The historian Rick Atkinson wrote that "the revelations of April ... sparked enduring outrage".

This was the case at all levels. An American officer wrote "The attitude of higher command seemed to be that these people ... should be made to feel the full significance of war and what their troops had done to other people." The US general George Patton wrote in his diary "In hundreds of villages ... most of the houses are heaps of stone ... I did most of it." When a sniper fired at one of his officers, he ordered several German houses to be burnt. When the commander of US 3rd Armored Division was killed in action inside Germany on 30 March, several villages were razed by his irate troops, captured wounded Germans were shot on the spot and at least 45 Germans were executed after surrendering. An artillery officer wrote home in April "we should fire about a thousand rounds into every [German] town. Do them good". At least one British battalion refused to take Waffen-SS prisoners, shooting those who surrendered; an officer of the battalion blamed this on SS "truculence". A British battalion commander summed up the risk-averse attitude within his unit: "At this stage of the war, no one was very keen to earn medals." A British pilot wrote: "It seemed a stupid time to die." A British corporal spoke for many when he wrote "Why don't the silly bastards give up?" Some divisions had suffered their last fatality by mid-April. The historian Max Hastings wrote "The final Anglo-American drive across Germany offered ... many foolish little battles which wasted men's lives".

In early April 1945 the 4th Canadian (Armoured) Division, as part of II Canadian Corps, moved out of the eastern Netherlands in the wake of the Allied crossing of the Rhine in Operation Plunder. The Canadian official history describes the circumstances as buoyant as it was recognized that the end of World War II in Europe was close. On 4 April, The Argyll and Sutherland Highlanders of Canada, part of the 10th Canadian Infantry Brigade, one of two brigades in the division, made an assault crossing of the Ems river and captured the town of Meppen, suffering only one casualty. German prisoners included several 17-year-old youths with less than eight weeks of military experience.

The division advanced a further to Sögel, which the 1st Battalion of The Lake Superior Regiment (Motor) captured on 9 April. The following day it repulsed several German counter-attacks before the town was declared cleared. Some German civilians joined the fighting and were believed to have killed several Canadian soldiers. Vokes, believing the civilians needed to be taught a lesson, ordered the destruction of the centre of the town. This was accomplished with several truckloads of dynamite. Vokes was aware that these actions violated the Hague Conventions and took care not to issue written instructions. Soldiers of the division started referring to Vokes as "The Sod of Sögel". The Canadian Army official history states: 

The Canadian advance continued across the Westphalian Lowland, reaching the outskirts of Friesoythe, a strategic crossroads, on 13 April. As it was early spring, the ground was sodden and heavy vehicles could not operate off the main roads. This made Friesoythe, west of Oldenburg, on the River Soeste, a potential bottleneck. If the Germans were to hold it, the bulk of the Canadians would be unable to continue their advance. Most of the population of 4,000 had evacuated to the countryside on 11–12 April. Several hundred paratroopers from Battalion Raabe of the German 7th Parachute Division and anti-tank guns defended the town. The paratroopers repelled an attack by the Lake Superior Regiment, which suffered several killed and wounded; German casualties are unknown.

Vokes ordered the resumption of the attack by the 1st Battalion, The Argyll and Sutherland Highlanders (Princess Louise's), commanded by Lieutenant-Colonel Frederick Wigle. The Argylls conducted a flanking night march and launched a dawn assault on 14 April. The attack met only scattered resistance from a disorganized garrison, and the Argylls secured the town by 10:30. During the confused fighting, approximately 50 German soldiers caught Wigle's tactical headquarters by surprise at around 08:30. A firefight broke out, resulting in the death of Wigle and several other soldiers. A rumour circulated that a local civilian had shot Wigle.

Vokes was furious when he heard of Wigle's death. He wrote in his autobiography that "a first-rate officer of mine, for whom I had a special regard and affection, and in whom I had a particular professional interest because of his talent for command, was killed. Not merely killed, it was reported to me, but sniped in the back". Vokes wrote, "I summoned my ... 'Mac,' I roared at him, 'I'm going to raze that goddam town. Tell 'em we're going to level the fucking place. Get the people the hell out of their houses first. Vokes's GSO1 (head of the operations staff), Lieutenant-Colonel Mackenzie Robinson, obeyed but convinced him to not put this order in writing or issue a proclamation to the local civilians. Hastings writes that the earlier incident at Sögel contributed to Vokes's fury.

The Argylls had spontaneously begun to burn Friesoythe in reprisal for the death of their commander. After Vokes had issued his order, the town was systematically set on fire with flamethrowers mounted on Wasp Carriers. In the side streets, soldiers threw petrol containers into buildings and ignited them with phosphorus grenades. The attack continued for over eight hours and Friesoythe was almost totally destroyed. As the commanding officer of the 1st Battalion, The Algonquin Regiment later wrote, "the raging Highlanders cleared the remainder of that town as no town has been cleared for centuries, we venture to say". The war diary of the 4th Canadian Armoured Brigade records, "when darkness fell Friesoythe was a reasonable facsimile of Dante's Inferno".

The Canadian official history states that Friesoythe "was set on fire in a mistaken reprisal". The rubble was used to reinforce the local roads for the division's tanks and heavy transport, which had been unable to move up due to the main roads near the town being badly cratered, and the smaller roads being inadequate to stand their weight.

During the fighting around Friesoythe and its aftermath, ten civilians from the town and another ten from the surrounding villages were killed. There were reports of civilians lying dead in the streets. According to one German assessment, 85–90 per cent of the town was destroyed during the reprisal. The estimates the destruction to have been as high as 90 per cent. The town's website records that of 381houses in the town proper, 231were destroyed and another 30  badly damaged. A few days later, a Canadian nurse wrote home that the convent on the edge of town was the only building left standing. In the suburb of Altenoythe, 120houses and 110other buildings were destroyed. In 2010, the author Mark Zuehlk suggested that, "Not all of Friesoythe was burnt, but its centre was destroyed".

The Argyll's war diary made no mention of their afternoon's activity, noting in passing that "many fires were raging". There is no record of the deliberate destruction at division, corps or army level. The war diary of the division's 8th Anti-Aircraft Regiment records "the Argylls were attacked in that town yesterday by German forces assisted by civilians and today the whole town is being systematically razed. A stern atonement ..." The 1st Battalion, The Argyll and Sutherland Highlanders (Princess Louise's) were awarded the battle honour "Friesoythe", as were the 1st Battalion, The Lake Superior Regiment (Motor) and the 1st Battalion, The Lincoln and Welland Regiment.

On 16 April The Lincoln and Welland Regiment attacked Garrel, south-west of Friesoythe. After a German act of perfidythe mayor surrendered the town but the first tank to enter was destroyed by a Panzerfaustthe battalion commander, Wigle's brother-in-law, ordered that "every building which did not show a white flag be fired". Before it could be carried out, the order was countermanded and the village was spared. A Canadian force was also authorized to burn down the village of Mittelsten following what the historian Perry Briddiscombe calls "an unnamed transgression". They were talked out of this by a Canadian engineer unit as Mittelsten's civilians were running an army sawmill.

In early 1946 Vokes heard an appeal against the death sentence of Kurt Meyer, a convicted German war criminal. Referring to his discussions about this Vokes said to the Canadian High Commissioner in London, "I told them of Sögel and Friesoythe and of the prisoners and civilians that my troops had killed in Italy and Northwest Europe". Vokes commuted the sentence to life imprisonment saying, "There isn't a general or colonel on the Allied side that I know of who hasn't said, 'Well, this time we don't want any prisoners".

The Canadian Army official historian, Colonel Charles Stacey, visited Friesoythe on 15 April. He wrote in the Canadian Army official history, which was published in 1960, that "there is no record of how this [destruction] came about". Responding to this, the historian Mark Zuehlke wrote that there were records of the events in the war diaries of several units, but that he did not believe Stacey's vagueness was an attempt at a cover-up. In his 1982 memoirs Stacey expanded upon the official history to comment that the only time he saw what could be considered a war crime committed by Canadian soldiers was when
Vokes commented in his autobiography, written forty years after the event, that he had "[a] feeling of no great remorse over the elimination of Friesoythe. Be that as it may." This position may have been motivated by a continued belief that the retaliatory actions were justified.





</doc>
<doc id="58006954" url="https://en.wikipedia.org/wiki?curid=58006954" title="Forcing a safeguard">
Forcing a safeguard

Forcing a safeguard is a war crime of violating a safeguard, which is an order to protect a property, locations or persons obtained from the enemy or neutral parties, or a guard or guard detachment to enforce this protection. In the United States, forcing a safeguard is punishable by death per Uniform Code of Military Justice (UCMJ).

A safeguard is often placed by a commanding officer in order to prevent looting, pillaging or wanton destruction of enemy property, or to prevent unauthorized requisitioning of goods. The commanding officer can often spare only an individual soldier or a small detachment to enforce the safeguard. Overpowering the guard(s) to loot the goods constitutes forcing a safeguard. Another type of safeguard is a written order left with the enemy or his property, with the intent to protect surrendered enemies from further violence. By placing the safeguard, the officer pledges the honor of his military to protect its target. Thus, violation of the safeguard by personnel from the same military jeopardizes the reputation of the entire country and its military.

In order to be convicted of forcing a safeguard, several conditions must be met:




</doc>
<doc id="45527641" url="https://en.wikipedia.org/wiki?curid=45527641" title="Destruction of cultural heritage by ISIL">
Destruction of cultural heritage by ISIL

Deliberate destruction and theft of cultural heritage has been conducted by the Islamic State of Iraq and the Levant since 2014 in Iraq, Syria, and to a lesser extent in Libya. The destruction targets various places of worship under ISIL control and ancient historical artifacts. In Iraq, between the fall of Mosul in June 2014 and February 2015, ISIL had plundered and destroyed at least 28 historical religious buildings. Valuable items from some buildings were looted in order to smuggle and sell them to foreigners to finance ISIS activities. By 23 March 2019, ISIL lost most of its territories in the Middle East, having been defeated in Iraq and Syria.

ISIL justifies the destruction of cultural heritage sites with its following of Salafism which, according to its followers, places "great importance on establishing tawhid (monotheism)", and "eliminating shirk (polytheism)." While it is often assumed that the group's actions are mindless acts of vandalism, there is an ideological underpinning to the destruction. ISIL views its actions in sites like Palmyra and Nimrud as being in accordance with Sunni Islamic tradition.

Beyond the ideological aspects of the destruction, there are other, more practical, reasons behind ISIL's destruction of historic sites. Grabbing the world's attention is easily done through the destruction of such sites, given the extensive media coverage and international condemnation that comes afterwards. Destroying historic ruins also allows ISIL to wipe the slate clean and to start afresh, leaving no traces of any previous culture or civilization, while also providing an ideal platform for the group to establish its own identity and leave its mark on history. Despite the images showing extreme destruction, ISIL has also been making use of the looted antiquities to finance their activities. Despite the UN's ban on the trade of artifacts looted from Syria since 2011, the group has been smuggling these artifacts out of the Middle East and on to the underground antique markets of Europe and North America.

In 2014, media reported destruction of multiple, Sunni and Shiite, mosques and shrines throughout areas captured by ISIL. Among them were the Al-Qubba Husseiniya Mosque in Mosul, Sheikh Jawad Al-Sadiq Mosque, Mosque of Arnā’ūt, Mosque of Qado, Mosque of Askar e- Mullah and Saad Bin Aqeel Shrine in Tal Afar, Sufi Ahmed al-Rifai Shrine and tomb and Sheikh Ibrahim shrine in Mahlabiya District and the so-called Tomb of the Girl (Qabr al-Bint) in Mosul. The Tomb of the Girl, reputed to honour a girl who died of a broken heart, was actually believed to be the tomb of medieval scholar Ali ibn al-Athir.

In June 2014, ISIL bulldozed the two buildings in the complex of the shrine of Fathi al-Ka'en.

On 24 September 2014, the Arba'een Wali Mosque and Shrine in Tikrit, containing forty tombs from the Umar era, was blown up. On 26 February 2015 ISIL blew up the 12th century Green Mosque in central Mosul.

In Mosul, ISIL also targeted several tombs with shrines built over them. In July 2014, ISIL destroyed one of the tombs of prophet Daniel (located in Mosul) by planted explosives. On 24 July 2014, the tomb and mosque of the prophet Jonah was destroyed with explosives. On 27 July, ISIL destroyed the tomb of Prophet Jirjis (George).

On 25 July 2014, the 13th-century shrine of Imam Awn al-Din in Mosul, one of the few structures to have survived the 13th-century Mongol invasion, was destroyed by ISIL. The destruction was mostly carried out with explosive devices, but in some cases bulldozers were used.

In March 2015, ISIL reportedly bulldozed to the ground the Hamu Al-Qadu Mosque in Mosul, dating back to 1880. The Hamu-Al-Qadu mosque contained an earlier tomb of Ala-al-din Ibn Abdul Qadir Gilani. In the same year ISIL ordered the removal of all decorative elements and frescoes from mosques in Mosul, even those containing Quranic verses that mention Allah. They were regarded by ISIL as "an erroneous form of creativity, contradicting the basics of sharia." At least one imam in Mosul opposing that order was shot to death.

ISIL also destroyed Sufi shrines near Tripoli, Libya, in March 2015. The shrines were destroyed by sledgehammers and bulldozers.

In June 2015, it was announced that ISIL had blown up the ancient tombs of Mohammed bin Ali and Nizar Abu Bahaaeddine, located close to the ruins of Palmyra.

In 2016, ISIL destroyed the Minaret of Anah located in Al Anbar Province, which dates back to the Abbasid era. The minaret was only rebuilt in 2013 after the destruction by an unknown perpetrator in 2006.

In 2017, ISIL destroyed the Great Mosque of al-Nuri and its leaning minaret. This was the mosque where ISIL leader Abu Bakr al-Baghdadi declared the establishment of the Islamic State caliphate three years prior.

In June 2014, it was reported that ISIL elements had been instructed to destroy all churches in Mosul. Since then, most churches within the city have been destroyed.

ISIL also blew up or demolished a number of other churches elsewhere in Iraq or in Syria. The Armenian Genocide Memorial Church in Deir ez-Zor, Syria was blown up by ISIL militants on 21 September 2014.

On 24 September 2014 ISIL militants destroyed with improvised explosive devices the 7th-century Green Church (also known as St Ahoadamah Church) belonging to the Assyrian Church of the East in Tikrit.

The Mar Behnam Monastery in Khidr Ilyas near Bakhdida, Iraq was destroyed by ISIL in March 2015.

, ISIL destroyed the Assyrian Christian Virgin Mary Church on Easter Sunday in the Syrian town of Tel Nasri. "As the 'joint forces' of Kurdish People's Protection Units and local Assyrian fighters attempted to enter the town", ISIL set off the explosives destroying what remained of the church. ISIL had controlled the church since 7 March 2015.

On 21 August 2015, the historic Monastery of St. Elian near Al-Qaryatayn in the Homs Governorate was destroyed by ISIL.

In May 2014, ISIL members smashed a 3,000-year-old neo-Assyrian statue from Tel Ajaja. Later reports indicated that over 40% of the artifacts at Tel Ajaja (Saddikanni) were looted by ISIS.

Parts of the Tal Afar Citadel were blown up by ISIL in December 2014, causing extensive damage.

In January 2015, ISIL reportedly destroyed large parts of the Nineveh Wall in al-Tahrir neighborhood of Mosul. Further parts of the walls, including the Mashka and Adad Gate, were blown up in April 2016.

In the Syrian city of Raqqa, ISIL publicly ordered the bulldozing of a colossal ancient Assyrian gateway lion sculpture from the 8th century BC. Another lion statue was also destroyed. Both statues originated from the Arslan Tash archaeological site. The destruction was published in the ISIL magazine, "Dabiq". Among the lost statues are those of Mulla Uthman al-Mawsili, of a woman carrying an urn, and of Abu Tammam.

On 26 February 2015, ISIL released a video showing the destruction of various ancient artifacts in the Mosul Museum. The affected artifacts originate from the Assyrian era and from the ancient city of Hatra. The video in particular shows the defacement of a granite lamassu statue from the right side of the Nergal Gate by a jackhammer. The statue remained buried until 1941 when heavy rains eroded the soil around the gate and exposed two statues on both sides. Several other defaced items in the museum were claimed to be copies, but this was later rebutted by Iraq's Minister of Culture, Adel Sharshab who said: "Mosul Museum had many ancient artifacts, big and small. None of them were transported to the National Museum of Iraq in Baghdad. Thus, all artifacts destroyed in Mosul are original except for four pieces that were made of gypsum".

On 5 March 2015, ISIL reportedly started the demolition of Nimrud, an Assyrian city from the 13th century BC. The local palace was bulldozed, while lamassu statues at the gates of the palace of Ashurnasirpal II were smashed. A video showing the destruction of Nimrud was released in April 2015.

On 7 March 2015, Kurdish sources reported that ISIL had begun the bulldozing of Hatra, which has been under threat of demolition after ISIL had occupied the adjacent area. The next day ISIL sacked Dur-Sharrukin, according to a Kurdish official from Mosul, Saeed Mamuzini.

The Iraqi Tourism and Antiquities Ministry launched the related investigation on the same day. On 8 April 2015, the Iraqi Ministry of Tourism reported that ISIL destroyed the remnants of the 12th-century Bash Tapia Castle in Mosul. As of early July 2015, 20% of Iraq's 10,000 archaeological sites has been under ISIL control.

In 2015 the face of the Winged Bull of Nineveh was damaged.

Following the capture of Palmyra in Syria, ISIL was reported as not intending to demolish the city's World Heritage Site (while still intending to destroy any statues deemed 'polytheistic'). On 27 May 2015, ISIL released an 87-second video showing parts of the apparently undamaged ancient colonnades, the Temple of Bel and the Roman theatre. On 27 June 2015, however, ISIL demolished the ancient Lion of Al-lāt statue in Palmyra. (It has since been restored, and is in storage in a Damascus museum until it can be determined that the statue can be safely returned to Palmyra.) Several other statues from Palmyra reportedly confiscated from a smuggler were also destroyed by ISIL. On 23 August 2015, it was reported that ISIL had blown up the 1st-century Temple of Baalshamin. On 30 August 2015, ISIL demolished the Temple of Bel with explosives. Satellite imagery of the site taken shortly after showed almost nothing remained.

According to the report issued on September 3, 2015 by ASOR Syrian Heritage initiative, ISIL also destroyed seven ancient tower tombs in Palmyra since the end of June over two phases. The last phase of destruction occurred between August 27 and September 2, 2015, including the destruction of the 2nd-century AD Tower of Elahbel, called "the most prominent example of Palmyra's distinct funerary monuments". Earlier, the ancient tombs of Iamliku and Atenaten were also destroyed. The Monumental Arch was also blown up in October.

When Palmyra was recaptured by Syrian government forces in March 2016, retreating ISIL fighters blew up parts of the 13th-century Palmyra Castle, causing extensive damage.

ISIL has also looted and demolished the Parthian/Roman city of Dura-Europos in east of Syria. Nicknamed "the Pompeii of the desert", the city was of particular archaeological significance.

It was reported on 1 January 2019 that Syrian authorities recovered two Roman-era funerary busts smuggled from Palmyra from an abandoned ISIL site in the Al-Sukhnah countryside.

Hatra () was an ancient city in the Ninawa Governorate and al-Jazira region of Iraq. A large fortified city and capital of the first Arab Kingdom, Hatra withstood invasions by the Romans in A.D. 116 and 198 thanks to its high, thick walls reinforced by towers. However about 240 ce, the city fell to Shāpūr I (reigned c. 240–272), the ruler of the Persian Sāsānian dynasty, and was destroyed. The remains of the city, especially the temples where Hellenistic and Roman architecture blend with Eastern decorative features, attest to the greatness of its civilization. The city lies 290 km (180 mi) northwest of Baghdad and 110 km (68 mi) southwest of Mosul.
On 7 March 2015, various sources including Iraqi officials reported that the militant group Islamic State of Iraq and the Levant (ISIL) had begun demolishing the ruins of Hatra. Video released by ISIL the next month showed destruction of the monuments. The ancient city was recaptured by the Popular Mobilization Forces on 26 April 2017.

ISIL has burned or stolen collections of books and papers from various locations, including the Central Library of Mosul (which they rigged with explosives and burned down), the library at the University of Mosul, a Sunni Muslim library, a 265-year-old Latin Church and Monastery of the Dominican Fathers, and the Mosul Museum Library. Some destroyed or stolen works date back to 5000 BC and include "Iraq newspapers dating to the early 20th century, maps and books from the Ottoman Empire, and book collections contributed by about 100 of Mosul’s establishment families." The stated goal is to destroy all non-Islamic books.

On 22 September 2014, the United States Secretary of State John Kerry announced that the Department of State had partnered with the American Schools of Orient Research Cultural Heritage Initiatives to "comprehensively document the condition of, and threats to, cultural heritage sites in Iraq and Syria to assess their future restoration, preservation, and protection needs". In 2014, the UNESCO's Committee for the Protection of Cultural Property in the Event of Armed Conflict condemned at the Ninth Meeting "repeated and deliberate attacks against cultural property... in particular in the Syrian Arab Republic and the Republic of Iraq". UNESCO Director-General Irina Bokova called the destructions in Mosul a violation of the United Nations Security Council Resolution 2199, and the destruction of Nimrud a war crime.

Former Prime Minister of Iraq Nouri al-Maliki reported that the local parliamentary tourism and antiquities committee had "filed complaints with the UN to condemn all ISIL crimes and abuses, including those that affect ancient places of worship". On 28 May 2015, the United Nations General Assembly unanimously passed a resolution, initiated by Germany and Iraq and sponsored by 91 UN member states, stating that ISIL's destruction of cultural heritage may amount to a war crime and urging international measures to halt such acts, which it described as a "tactic of war".

After the Palmyra temple's destruction in August 2015, the Institute for Digital Archaeology (IDA) announced plans to establish a digital record of historical sites and artifacts threatened by ISIL advance. To accomplish this goal, the IDA, in collaboration with UNESCO, will deploy 5,000 3D cameras to partners in the Middle East. The cameras will be used to capture 3D scans of local ruins and relics.

The general director of the Czech National Museum, Michal Lukeš, signed an agreement in June 2017 committing the institution to help Syria save, preserve and conserve much of its cultural and historical heritage damaged by war, including the ancient site of Palmyra; he met with Maamoun Abdulkarim and discussed plans for the works that are said to last until 2019.

In June 2017, The World Monuments Fund (WMF) announced launching a £500,000 scheme to train Syrian refugees near the Syrian-Jordanian border in traditional stone masonry. The aim is teaching them to develop skills necessary to be able to help in restoring cultural heritage sites that have been damaged or destroyed during the Syrian Civil War once peace is restored to Syria.

Minor restorations have already begun: Palmyrene funerary busts of a deceased man and a woman, damaged and defaced by ISIL, were taken from Palmyra, then to Beirut to be sent off to Rome. Italian experts restored the portraits using 3D technology to print resin prosthetics, which were coated with a thick layer of stone dust to blend in with the original stone; the prosthetics were attached to the damaged faces of the busts using strong magnets. The restored pieces are now back in Syria. Abdulkarim said the restoration of the busts "is the first real, visible positive step that the international community has taken to protect Syrian heritage".

However, the Rewards for Justice Program offers up to $5 million for information leading to disrupt the sale and/or trade of oil and antiquities by ISIL.




</doc>
<doc id="48401710" url="https://en.wikipedia.org/wiki?curid=48401710" title="Airstrikes on hospitals in Yemen">
Airstrikes on hospitals in Yemen

A Saudi Arabian-led military intervention in Yemen began in 2015, in an attempt to influence the outcome of the Yemeni Civil War. Saudi Arabia, spearheading a coalition of nine Arab states, began carrying out airstrikes in neighbouring Yemen and imposing an aerial and naval blockade on 26 March 2015, heralding a military intervention code-named Operation Decisive Storm (). More than 70 health facilities in Yemen have been destroyed by a series of airstrikes conducted by the Saudi Arabian-led coalition since March 2015. Many of these have been public health hospitals staffed or supported by Doctors Without Borders (MSF). Critics of the assaults say the airstrikes are war crimes in violation of the protections of health care facilities afforded by the internationally recognized rules of war and have called for independent investigations.

Many other civilians targets, including schools, and school buses in Yemen are also bombed by the Saudi-led coalition.

1,500 schools were damaged and destroyed during Yemeni Civil War.

The UN accused the Saudi-led coalition of "complete disregard for human life".

Doctors Without Borders reported that a Saudi Arabian-led coalition airstrike on 26 October 2015 had completely destroyed the Médecins Sans Frontières hospital in Saada, in northwestern Yemen, including the operating room. The first strike hit an unused part of the hospital, so the facility was completely evacuated at once. There were no direct casualties. The spokesman for the coalition forces, Brig-Gen Ahmed al-Asiri, disclaimed responsibility for the attack.

"With the hospital destroyed, at least 200,000 people now have no access to lifesaving medical care", MSF said. "This attack is another illustration of a complete disregard for civilians in Yemen, where bombings have become a daily routine," said Hassan Boucenine, MSF head of mission in Yemen. The GPS coordinates of the only hospital in the Haydan district were regularly shared with the Saudi-led coalition, and the roof of the facility was clearly identified with the MSF logo, he said. Abdallah al-Mouallimi, the Saudi ambassador to the United Nations, said the coordinates were inaccurate, although he admitted that the airstrike was "a mistake".

The UNICEF said the hospital in Saada was the 39th health center hit in Yemen since March, when the violence escalated. MSF reports that the Saudi-led coalition, supported by the British military, has been bombing hospitals across Yemen for the past 10 months. As many as 130 health facilities have been hit. "More children in Yemen may well die from a lack of medicines and healthcare than from bullets and bombs," its executive director Anthony Lake said in a statement. He added that critical shortages of fuel, medication, electricity and water could mean many more will close. Amnesty International said the strike may amount to a war crime and called for an independent investigation.

On December 3, 2015 an airstrike by the Saudi Arabian-led intervention in Yemen hit a health center in Taiz, wounding nine people. Two hospital staff were among the wounded. "The bombing of civilians and hospitals is a violation of international humanitarian law," said Jerome Alin, head of MSF head of mission in Yemen.

On January 10, 2016, Shiara Hospital, supported by MSF in Razeh district, Saada Governorate, Northern Yemen, was hit by a projectile and shrapnel from the Saudi-led coalition. Six people died and another 7 were injured, including three MSF staff, two of them in critical condition. Several buildings at the medical facility collapsed after the attack, although the critical areas of the hospital were not destroyed. The rocket hit a corridor leading from the main gate to the hospital buildings, with a metal fence alongside. The wounded were hit by shrapnel from the missile, and also by shards of metal from the fence. The injuries were brutal. Vickie Hawkins, Executive Director of MSF-UK, said, "... there is a risk that "errors" in war situations will become normalised—just as "collateral damage" has been normalised in people's minds since the first Gulf War. This would provide the perfect alibi for armies to shrug off accusations of war crimes and crimes against humanity. It perpetuates impunity.

In a separate attack by the Saudi-led coalition, an airstrike was reported to have hit a center for the blind in the capital Sana'a, resulting in multiple injuries.

On August 15, 2016, after the collapse of a UN-sponsored cease-fire, an airstrike by the Saudi Arabian-led intervention in Yemen destroyed a hospital operated by Yemen's Ministry of Health and supported by MSF and UNICEF in Abs District, Hajjah Governorate in northwestern Yemen. The bombardment struck the hospital's triage area near the emergency room and killed at least 19 and wounded 24 people. At the time of the attack, there were 23 patients in the surgery ward, 25 in the maternity ward, 13 newborns and 12 patients in the pediatric ward, MSF said. The hospital had a 14-bed emergency room, a maternity unit and a surgical unit. Hospital staff were among the dead and wounded. “There were no armed people there,” a witness said. The hospital was reportedly treating child victims of another airstrike on a school in the town of Haydan, in neighboring Saada province, in which 10 children died and another 30 were wounded, all between the ages of 8 and 15 years. MSF has now withdrawn its staff members from Haydan, Razeh, Al Gamouri and Yasnim hospitals in Saada governorate and Abs and Al Gamouri hospitals in Hajjah governorate. Ban Ki-moon, the United Nations secretary general, condemned the attack in a statement, emphasizing that antagonists in the Yemen conflict had damaged or destroyed more than 70 health facilities since the hostilities began 17 months ago.

Doctors Without Borders reported that a Saudi Arabian coalition airstrike struck a new Médecins Sans Frontières cholera treatment center in Abs, in northwestern Yemen. Doctors Without Borders reported that they had provided GPS coordinates to The Kingdom of Saudi Arabia on twelve separate occasions, and had received nine written responses confirming receipt of those coordinates 

On 2 August 2018, airstrikes on hospital, harbor and fish market in Al Hudaydah killed at least 55 people and wounded 124.

A missile blew up a gas station near a hospital in Saada Governorate, which damaged the hospital and killed seven civilians.

Air raids by a Saudi-UAE-led coalition killed at least 11 civilians, including school children and left more than 39 people wounded in Sanaa, according to an Al Jazeera report. Also The Associated Press said 13 were killed, including 7 children, and more than 100 were wounded. Daily Mail reported a total of 54 wounded and 11 children killed in Sanaa. Youssef al-Hadrii, a spokesman for the Houthi-controlled health ministry, said most of the children killed in the bombing of houses and a school since the war beginning. There was no comment from the coalition.

According to aid group and Yemeni officials, 8 people including 3 civilians were killed by a Houthi drone and missile strike attack in Mocha that targeted a refugee camp and a hospital ran by Doctors without Borders, causing the hospital to shut down.



</doc>
<doc id="18306447" url="https://en.wikipedia.org/wiki?curid=18306447" title="Peter von Hagenbach">
Peter von Hagenbach

Peter von Hagenbach (or Pierre de Hagenbach or Pietro di Hagenbach or Pierre d’Archambaud or Pierre d'Aquenbacq, circa 1420 – May 9, 1474) was a Bourguignon knight from Alsace and Germanic military and civil commander.

He was born into an Alsatian-Burgundian family, originally from Hagenbach and owned a castle there.

He was instated as bailiff of Upper Alsace by Charles the Bold, Duke of Burgundy, to administer the territories and rights on the Upper Rhine which had been mortgaged by Duke Sigmund of Further Austria for 50,000 florins in the in 1469. There he coined the term "Landsknecht"—from German, "Land" ("land, country") + "Knecht" ("servant").

It was originally intended to indicate soldiers of the lowlands of the Holy Roman Empire as opposed to the Swiss mercenaries. As early as 1500 the misleading spelling "Lanzknecht" became common because of the phonetic and visual similarity between "Land"("e")"s" ("of the land/territory") and "Lanze" ("lance").

Following a rebellion by towns of the Upper Rhine against his tyranny, Hagenbach was put on trial for the atrocities committed during the occupation of Breisach, found guilty of war crimes, and beheaded at Breisach. His trial by an ad hoc tribunal of the Holy Roman Empire in 1474 was the first “international” recognition of commanders’ obligations to act lawfully. He was convicted of crimes, specifically murder, rape and perjury, among other crimes, "he as a knight was deemed to have a duty to prevent." He defended himself by arguing that he was only following orders from the Duke of Burgundy, to whom the Holy Roman Empire had given Breisach. Although there was no explicit use of a doctrine of command responsibility, it is seen as the first trial based on that principle. As well, it includes the earliest documented prosecution of gender-based/targeted crimes when he was convicted for rapes committed by his troops.




</doc>
<doc id="4281562" url="https://en.wikipedia.org/wiki?curid=4281562" title="Decree of War to the Death">
Decree of War to the Death

The Decree of War to the Death, in Spanish Decreto de Guerra a Muerte, was a decree issued by the South American leader Simón Bolívar which permitted murder and any atrocities whatsoever to be committed against civilians born in Spain, other than those actively assisting South American independence, and furthermore exonerated Latin Americans who had already committed such murders and atrocities. The phrase "war to the death" was used as a euphemism for these atrocities.

The decree was an explicit "war of extermination" in Bolívar's attempt to maintain Venezuelan independence in the war with Spain, since he felt that the Spanish Army's use of atrocities against those who supported the First Republic of Venezuela had contributed decisively to its defeat.

Bolívar promulgated the decree on June 15, 1813, in the Venezuelan city of Trujillo.

The decree states that it was created as a response to severe crimes and massacres by Spanish soldiers after the fall of the First Republic, in which Spanish leaders allegedly stole property and executed thousands of Republicans: "we could not indifferently watch the afflictions inflicted to you by the barbaric Spaniards, who have annihilated you with robbery and destroyed you with death, infringed the most solemn treaties and capitulations [a reference to the San Mateo Capitulation, 1812]; in one word, committed every crime, reducing the Republic of Venezuela to the most horrific desolation." It proclaimed that all Peninsular people in Spanish America who didn't actively participate in favor of its independence would be killed, and all South Americans would be spared, even if they had cooperated with the Spanish authorities. (See below for full declaration). The document's ultimate goal was to assure the Venezuelan elites that they would not be unfavorably treated for having collaborated with Domingo de Monteverde and the royalist authorities. The Decree was the first step in transforming the common and legal view of the Venezuelan war of liberation from a mere rebellion (or at best a civil war) taking place in one of Spain's colonies, to a full-fledged international war between two distinct countries, Venezuela and Spain.

This so-called "Guerra a Muerte" was widely practised on both sides, resulting in some extreme brutalities on both sides, such as the execution of Spanish prisoners in Caracas and La Guaira in February 1814, on orders from Bolívar himself, just before the collapse of the Second Republic of Venezuela, and the killing of several renowned citizens in New Granada by the royalist army under Pablo Morillo in 1815, 1816 and 1817.

The declaration remained in effect until November 26, 1820, when General Pablo Morillo met with Bolívar at Santa Ana de Trujillo to declare the war of independence a conventional war.

Note: The term "Americans" here means natives of the American continent.

""Venezuelans: an army of brothers, sent by the sovereign Congress of New Granada, has come to free you, and it is already amongst you, after evicting the oppressors from the provinces of Mérida and Trujillo.

"We are the ones sent to destroy the Spaniards, to protect the Americans, and to reestablish the republican governments that formed the Confederation of Venezuela. The states covering our arms (weapons) are once again ruled by their old constitutions and magistrates, fully enjoying their liberty and independence; for our mission is only to break the chains of servitude, which still oppress some of our peoples, not claiming to create laws, or enforce acts of domination, which the right of war could authorize us to do.

"Touched by your misfortunes, we could not indifferently watch the afflictions inflicted to you by the barbaric Spaniards, who have annihilated you with robbery and destroyed you with death, infringed the most solemn treaties and capitulations; in one word, committed every crime, reducing the Republic of Venezuela to the most horrific desolation. It is so that justice demands vindication, and necessity forces us to take it. May the monsters that infest Colombian soil, and have covered it with blood disappear for good; may their punishment be equal to the magnitude of their treason, so that the stain of our ignominy is washed off, and to show the nations of the universe that the sons of America cannot be offended without punishment.

"In spite of our just resentments against the iniquitous Spaniards, our magnanimity still deigns itself to open, for the last time, a route to conciliation and friendship; we still invite them to live peacefully among us, if, hating their crimes and turning to good faith, they cooperate with us in the destruction of the intruding government of Spain, and the reestablishment of the Republic of Venezuela.

"All Spaniards who do not conspire against tyranny in favor of our just cause, using the most effective and active resources, will be considered enemies, and will be punished as traitors to the homeland, and therefore, will be promptly executed. On the other hand, a general and absolute pardon is issued to all Spaniards who pass into our army, with or without their weapons; to those who offer aid to the good citizens working hard to shake off the shackles of tyranny. War officers and magistrates that proclaim the government of Venezuela and join our cause will keep their destinies and work positions; in one word, all Spaniards who perform service for the State will be reputed and treated as Americans.

"And you, Americans, who have been separated from the road of justice by error and perfidy, know that your brothers forgive you and seriously regret your misdeeds, intimately persuaded that you cannot be guilty, and that only the ignorance and blindness imposed on you by the authors of your crimes could cause you to perpetrate them. Do not fear the sword that comes to avenge you and cut the ignominious bindings which tie you to your executioners' fate. Rely on absolute immunity for your honor, life and properties; the mere title of Americans will be you warranty and safeguard. Our weapons have come to protect you, and will never be used against a single one of our brothers.

"This amnesty extends to the very traitors who have most recently committed their acts of felony; and will be so religiously carried out that no reason, cause or pretext will be enough to make us break our offer, no matter how extraordinary the reasons you give us to excite our adversity.

"Spaniards and Canarians, count on death, even if indifferent, if you do not actively work in favor of the independence of America. Americans, count on life, even if guilty.""





</doc>
<doc id="23176765" url="https://en.wikipedia.org/wiki?curid=23176765" title="Alfried Krupp von Bohlen und Halbach Foundation">
Alfried Krupp von Bohlen und Halbach Foundation

The Alfried Krupp von Bohlen und Halbach Foundation () is a major German philanthropic foundation, created by and named in honour of Alfried Krupp von Bohlen und Halbach, a convicted criminal against humanity, and the former owner and head of the Krupp company. Once it was the largest company in Europe, and one of largest wartime users of slave labor in Nazi Germany, including the Krupp munitions factory ("Weichsel Union Metallwerke") in the Auschwitz death camp. In 1959, the company promised to pay individual compensations of DM5,000 ($1,190) to 2,000 slave workers (or 2% of all the estimated estimated 100,000 slave workers), or DM10,000,000 ($2,380,000) in total.

On the death of Alfried Krupp von Bohlen und Halbach in 1967, the entire holdings of the Krupp family were transferred to the foundation. Today, the foundation is the largest shareholder of the ThyssenKrupp industrial conglomerate (20.9% as of 2018) and largely controls the board of the company. The foundation is also tasked with preserving the “unity” of ThyssenKrupp and uses proceeds from ThyssenKrupp’s dividend payments to further good causes in science and education. In the 2018 money, the aforementioned 1959 compensation payouts equate to $20,537,000, that is, €18,401,000. The sum is 0.02 percent of ThysenKrupp's 2018 assets, net income and equity, or over €72billion.





</doc>
<doc id="58107763" url="https://en.wikipedia.org/wiki?curid=58107763" title="Dahyan air strike">
Dahyan air strike

On 9 August 2018, Saudi Arabian expeditionary aircraft bombed a civilian school bus passing through a crowded market with U.S.-made bombs in Dahyan, Saada Governorate, Yemen, near the border with Saudi Arabia. At least 40 children were killed, all under 15 years old and most under age 10. Sources disagree on the exact number of deaths, but they estimate that the air strike killed about 51 people.

According to Save the Children, at the time of the attack the children were on a bus heading back to school from a picnic when the driver stopped to get refreshment at the market in Dahyan. Most of the children were under age 10, according to the International Committee of the Red Cross. A Red Cross–supported hospital in Saada received the bodies of 29 children under 15 years of age and 48 wounded individuals, 30 of whom were children. A total of 40 children were killed in the strike.

According to a resident of Dahyan, the warplanes had been loitering over the area for more than an hour before they attacked. Another witness said, "Our shops were open and shoppers were walking around as usual. All those who died were residents, children and shop owners." According to Yahya Hussein, a teacher who was traveling separately from the bus, "The scene can't be described—there was body parts and blood everywhere."

The bomb that killed the children was a 227 kg (500 lb) laser-guided Mk 82 bomb made by Lockheed Martin. It had been supplied by the United States to Saudi Arabia.

The attack came to light after videos were posted on Twitter depicting the remains of the bus and the children. Images of the victims were aired on the Al Masirah TV network, highlighting dramatic images of blood and debris-covered children lying on hospital stretchers. The Saudi Arabian coalition later issued a statement saying that they conducted an airstrike in Saada but were targeting Houthi missile launchers. The mass funeral of the children was aired on the Al Mariah TV network, with thousands of Yemenis participating.

The official Saudi Arabian press agency called the strike a "legitimate military action" which targeted those who were responsible for a rebel missile attack on the Saudi Arabian city of Jizan on Wednesday. They also claimed that the airstrikes "conformed to international and humanitarian laws" and that Houthis were using children as human shields. Yemeni journalist Nasser Arrabyee reported that there were no Houthis in the vicinity of the strike. A Houthi spokesman said that the coalition showed "clear disregard for civilian life", as the attack had targeted a crowded public place in the city. During the mass funeral of the children, many signs were visible protesting against the United States, Saudi Arabia, and Israel.

On 1 September 2018, the Saudi Arabian-led coalition admitted mistakes, expressing regrets and pledged to hold those responsible for the strikes accountable.

United Nations Secretary-General António Guterres condemned the attack and called for an independent and prompt investigation, and UNICEF strongly condemned the attack.

The United States Department of State called for Saudi Arabia to conduct an investigation into the strike. The United Kingdom's Foreign and Commonwealth Office expressed "deep concern", called for a transparent investigation, and called upon all parties to prevent civilian casualties and to co-operate with the UN to reach a lasting political solution in Yemen. UK Foreign Secretary Jeremy Hunt defended the Saudi–British alliance as important in fighting Islamists.

The head of the Yemeni delegation of the International Committee of the Red Cross tweeted, "@ICRC_Yemen-supported hospital has received dozens of dead and wounded. Under international humanitarian law, civilians must be protected during conflict."



</doc>
<doc id="63490803" url="https://en.wikipedia.org/wiki?curid=63490803" title="Massacres of Keqëkolle and Prapashticë">
Massacres of Keqëkolle and Prapashticë

Massacres of Keqëkolle and Prapashticë () were a series of massacres committed by Serbian troops of the Third Army, in the region of Gallapi, in the village Keqekolla, in Pristina, Kosovo, on January 10, 1921.
The battalions was led by Radovan Radonić and Bozhidar Paunovic who, prior to the massacre, had committed several atrocities in the villages Popovë, Majac, Lupc, Bellopojë, Tërrnavë, Sharban, Koliq, Keqekollë, Ballaban and Prapashticë. On Januar 10, 1921, Serbian soldiers entered into Mulla Ademis house and forced him to watch as they beheaded his whole family. The massacred included: his wife Mihrije Emërllahu, (68), his son Mehmet Emërllahu (30), Hasime Emërllahu (the sons wife), Selime Adem Emërllahu, Tahire Adem Emërllahu, Mustafë Adem Emërllahu, and a baby in a cradle. Afterwards, Mulla Ademi was beheaded too and the bodies were cut to pieces and burned.

Among the massacred was Mulla Ademi Emërllahu (1850-1921), an Albanian author, scholar and imam, who was forced to watch along as 9 family members were beheaded, before being beheaded himself. A total number of 1020 Albanian civlians were massacred, carried out by Colonel Radovan Radonić and Bozhidar Paunovic. Of Mulla Ademis family, only one boy survived who was not present at the moment. It is believed that between 1020 and 1600 Albanians were killed in total that day. The month was called "Black January" afterwards by the locals.

After the massacre, the Serbian troops gathered all boys and men in the ages of 15-70 from the villages of Kurtaj, Qorraj, Çelaj, Myftaraj, Spahijaj, Balaj, to the town square where they were massacred, numbering 1020 Albanians. The family of Gjaka were forced to wait outside as the Serb soldiers filled the house with hay and set it ablaze. The mother in the family tried to rescue the baby by throwing out the window. The soldiers continued to throw it back inside the house. After the baby was thrown out, the Serb soldiers shot and bayonetted it on the street.


</doc>
<doc id="63501741" url="https://en.wikipedia.org/wiki?curid=63501741" title="Gradec massacre">
Gradec massacre

Gradec Massacre (Alb. "Masakra e Gradecit") was part of a series massacres of 150 Albanian civilians in the village of Gradec, near Tetova and Gostivar, committed by the Serbian army led by Nikola Jorgovanović, on November 15, 1915. Edith Durham stated that in the village of Gostivar the Serbian forces massacred entire populations and burned down whole villages. In the village of Gradec, 50 Albanians were bayoneted and similar events occurred in Reçan, Reç, Zdunje, Raven, Vërtok, Mirditë, Orqush, Simnicë, Banjicë, Kalisht, Çegran and Qafë. The massacres were committed by Chetnik commander Ilia Novoselo, Marko Angjeli, Miladin Boshuk and Risto Turcani. Atrocities had been begun in May 1915, where the Serbian forces harassed the Albanians living at the border. The Albanians of Reka were massacred by the Serbian Army in September 1915, and the houses were burned down and pillaged. The villages of Shtirovica, Tërnica, Strazimiri, and Reka were burned down and the inhabitants massacred. In the St. Jovan Bigori monastery in Shtirovica, Serb soldiers massacred 45 Albanians. The Serbian government then demanded that the Orthodox Albanians of Zhirovnica declare themselves as Serbs.


</doc>
<doc id="63500770" url="https://en.wikipedia.org/wiki?curid=63500770" title="Massacres of Albanians during the Great Retreat">
Massacres of Albanians during the Great Retreat

Massacres of Albanians during the Great Retreat (Alb. "Masakrat ndaj shqiptarevë gjat tërheqjes e ushtrisë sërbe") were 
massacres, crimes and pillaging which soldiers in the Serbian army committed against the Albanian civil population during the Great Retreat in 1915.


</doc>
<doc id="1111272" url="https://en.wikipedia.org/wiki?curid=1111272" title="Premedication">
Premedication

Premedication is using medication before some other therapy (usually surgery or chemotherapy) to prepare for that forthcoming therapy. Typical examples include premedicating with a sedative or analgesic before surgery; using prophylactic (preventive) antibiotics before surgery; and using antiemetics or antihistamines before chemotherapy.

Premedication before chemotherapy for cancer often consists of drug regimens (usually 2 or more drugs, e.g. dexamethasone, diphenhydramine and omeprazole) given to a patient minutes to hours before the chemotherapy to avert side effects or hypersensitivity reactions (i.e. allergic reactions).

Melatonin has been found to be effective as a premedication in both adults and children due to its pharmacological properties of hypnotic, antinociceptive and anticonvulsant which produce effective anxiolosis and sedation. Unlike midazolam melatonin does not impair psychomotor skills or adversely affect the quality of recovery. It has a faster recovery time compared to midazolam and has a reduced incidence of post-operative excitement and results in a reduction in dose required of propofol and thiopental.

Midazolam is effective in children in reducing anxiety associated with separation from parents and induction of anesthesia. Sufentanil is also sometimes used as a premedication. Clonidine is becoming increasingly popular as a premedication for children. One drawback of clonidine is that it can take up to 45 minutes to take full effect. In children, clonidine has been found to be equal to and possibly superior to benzodiazepines as a premedication. It has a more favourable side effect profile. It also reduces the need for an induction agent. It improves post-operative pain relief, is better at inducing sedation at induction, reduces agitated emergence, reduces shivering and post-operative nausea and vomiting and reduces post-operative delirium associated with sevoflurane anaesthesia. Benzodiazepines such as midazolam are more commonly used due largely to a lack of a marketing effort by the pharmaceutical companies. As a result, clonidine is becoming increasingly popular with anesthesiologists. Dexmedetomidine and atypical antipsychotic agents are other premedications which are used particularly in very uncooperative children.

Non-drug interventions for children include playing relaxing music, massages, reducing noise and controlling light to maintain the sleep wake cycle. Other non-pharmacological options for children who refuse or cannot tolerate premedication include clown doctors; low sensory stimulation and hand-held video games may also help reduce anxiety during induction of general anesthesia.


</doc>
<doc id="12924448" url="https://en.wikipedia.org/wiki?curid=12924448" title="Biosimilar">
Biosimilar

A biosimilar is a biologic medical product (also known as biologic) highly similar to another already approved biological medicine (the 'reference medicine'). Biosimilars are approved according to the same standards of pharmaceutical quality, safety and efficacy that apply to all biological medicines.. Biosimilars are officially approved versions of original "innovator" products and can be manufactured when the original product's patent expires. Reference to the innovator product is an integral component of the approval.

Unlike with generic drugs of the more common small-molecule type, biologics generally exhibit high molecular complexity and may be quite sensitive to changes in manufacturing processes. Despite that heterogeneity, all biopharmaceuticals, including biosimilars, must maintain consistent quality and clinical performance throughout their lifecycle.. A biosimilar is not regarded as a generic of a biological medicine. This is mostly because the natural variability and more complex manufacturing of biological medicines do not allow an exact replication of the molecular micro-heterogeneity.
Drug-related authorities such as the EU's European Medicines Agency (EMA), the US's Food and Drug Administration (FDA), and the Health Products and Food Branch of Health Canada hold their own guidance on requirements for demonstration of the similar nature of two biological products in terms of safety and efficacy. According to them, analytical studies demonstrate that the biological product is highly similar to the reference product, despite minor differences in clinically inactive components, animal studies (including the assessment of toxicity), and a clinical study or studies (including the assessment of immunogenicity and pharmacokinetics or pharmacodynamics). They are sufficient to demonstrate safety, purity, and potency in one or more appropriate conditions of use for which the reference product is licensed and is intended to be used and for which licensure is sought for the biological product.

The World Health Organization (WHO) published its "Guidelines for the evaluation of similar biotherapeutic products (SBPs)" in 2009. The purpose of this guideline is to provide an international norm for evaluating biosimilars with a high degree of similarity with an already licensed, reference biotherapeutic medicine.

Europe was the first region in the world to develop a legal, regulatory, and scientific framework for approving biosimilar medicines. The EMA has granted a marketing authorisation for more than 50 biosimilars since 2006 (first approved biosimilar Somatropin(Growth hormone)). The first monoclonal antibody that was approved in 2013, was infliximab, putting the EU at the forefront of biologics regulatory science. . Meanwhile, on March 6, 2015, the FDA approved the United States's first biosimilar product, the biosimilar of filgrastim called filgrastim-sndz (trade name Zarxio) by Sandoz.

Approval of medicines in the EU relies on a solid legal framework, which in 2004, introduced a dedicated route for the approval of biosimilars. The EU has pioneered the regulation of biosimilars since the approval of the first one (the growth hormone somatropin) in 2006. Since then, the EU has approved the highest number of biosimilars worldwide, and consequently has the most extensive experience of their use and safety. All medicines produced using biotechnology and those for specific indications (e.g. for cancer, neurodegeneration and auto-immune diseases) must be approved in the EU through the EMA (via the so-called 'centralised procedure'). Nearly all biosimilars approved for use in the EU have been approved centrally, as they use biotechnology for their production. Some biosimilars may be approved at national level, such as some low-molecular weight heparins derived from porcine intestinal mucosa. When a company applies for marketing authorisation at EMA, data are evaluated by EMA's scientific committees on human medicines and on safety (the CHMP and PRAC), as well as by EU experts on biological medicines (Biologics Working Party) and specialists in biosimilars (Biosimilar Working Party). The review by EMA results in a scientific opinion, which is then sent to the European Commission, which ultimately grants an EU-wide marketing authorisation. . 

In the United States, the Food and Drug Administration (FDA) held that new legislation was required to enable them to approve biosimilars to those biologics originally approved through the PHS Act pathway. Additional Congressional hearings have been held. On March 17, 2009, the Pathway for Biosimilars Act was introduced in the House. See the Library of Congress website and search H.R. 1548 in 111th Congress Session. Since 2004 the FDA has held a series of public meetings on biosimilars.

The FDA gained the authority to approve biosimilars (including interchangeables that are substitutable with their reference product) as part of the Patient Protection and Affordable Care Act signed by President Obama on March 23, 2010.

The FDA has previously approved biologic products using comparability, for example, Omnitrope in May 2006, but this like Enoxaparin was also to a reference product, Genotropin, originally approved as a biologic drug under the FD&C Act.

On March 6, 2015, Zarxio obtained the first approval of FDA. Sandoz's Zarxio is biosimilar to Amgen's Neupogen (filgrastim), which was originally licensed in 1991. This is the first product to be passed under the Biologics Price Competition and Innovation Act of 2009 (BPCI Act), which was passed as part of the Affordable Healthcare Act. But Zarxio was approved as a biosimilar, not as an interchangeable product, the FDA notes. And under the BPCI Act, only a biologic that has been approved as an "interchangeable" may be substituted for the reference product without the intervention of the health care provider who prescribed the reference product. The FDA said its approval of Zarxio is based on review of evidence that included structural and functional characterization, animal study data, human pharmacokinetic and pharmacodynamics data, clinical immunogenicity data and other clinical safety and effectiveness data that demonstrates Zarxio is biosimilar to Neupogen.

In March 2020, most protein products that were approved as drug products (including every insulin currently on the market ) are scheduled to open up to biosimilar and interchangeable competition in the United States. However, "chemically synthesized polypeptides" are excluded from this transition, which means that a product that falls within this category won't be able to come to market as a biosimilar or interchangeable product, but will have to come to the market under a different pathway.

Cloning of human genetic material and development of in vitro biological production systems has allowed the production of virtually any recombinant DNA based biological substance for eventual development of a drug. Monoclonal antibody technology combined with recombinant DNA technology has paved the way for tailor-made and targeted medicines. Gene- and cell-based therapies are emerging as new approaches.

Recombinant therapeutic proteins are of a complex nature (composed of a long chain of amino acids, modified amino acids, derivatized by sugar moieties, folded by complex mechanisms). These proteins are made in living cells (bacteria, yeast, animal or human cell lines). The ultimate characteristics of a drug containing a recombinant therapeutic protein are to a large part determined by the process through which they are produced: choice of the cell type, development of the genetically modified cell for production, production process, purification process, formulation of the therapeutic protein into a drug.

After the expiry of the patent of approved recombinant drugs (e.g., insulin, human growth hormone, interferons, erythropoietin, monoclonal antibodies and more) any other biotech company can develop and market these biologics (thus called biosimilars).
Every biological (or biopharmaceutical products) displays a certain degree of variability, even between different batches of the same product, which is due to the inherent variability of the biological expression system and the manufacturing process. Any kind of reference product has undergone numerous changes in its manufacturing processes, and such changes in the manufacturing process (ranging from a change in the supplier of cell culture media to new purification methods or new manufacturing sites) was substantiated with appropriate data and was approved by the EMA. In contrast, it is mandatory for biosimilars to take a both non-clinical and clinical test that the most sensitive clinical models are asked to show to enable detection of differences between the two products in terms of human pharmacokinetics (PK) and pharmacodynamics (PD), efficacy, safety, and immunogenicity.

The current concept of development of biosimilar mAbs follows the principle that an extensive state of the art physicochemical, analytical and functional comparison of the molecules is complemented by comparative non-clinical and clinical data that establish equivalent efficacy and safety in a clinical "model" indication that is most sensitive to detect any minor differences (if these exist) between biosimilar and its reference mAb also at the clinical level.

The European Medicines Agency (EMA) has recognized this fact, which has resulted in the establishment of the term "biosimilar" in recognition that, whilst biosimilar products are similar to the original product, they are not exactly the same.
Every biological displays a certain degree of variability. However, provided that structure and function(s), pharmacokinetic profiles and pharmacodynamic effect(s) and/or efficacy can be shown to be comparable for the biosimilar and the reference product, those adverse drug reactions which are related to exaggerated pharmacological effects can also be expected at similar frequencies.

Originally the complexity of biological molecules led to requests for substantial efficacy and safety data for a biosimilar approval. This has been progressively replaced with a greater dependence on assays, from quality through to clinical, that show assay sensitivity sufficient to detect any significant difference in dose. However, the safe application of biologics depends on an informed and appropriate use by healthcare professionals and patients. Introduction of biosimilars also requires a specifically designed pharmacovigilance plan. It is difficult and costly to recreate biologics because the complex proteins are derived from living organisms that are genetically modified. In contrast, small molecule drugs made up of a chemically based compound can be easily replicated and are considerably less expensive to reproduce. In order to be released to the public, biosimilars must be shown to be as close to identical to the parent innovator biologic product based on data compiled through clinical, animal, analytical studies and conformational status.

Generally, once a drug is released in the market by the FDA, it has to be re-evaluated for its safety and efficacy once every six months for the first and second years. Afterward, re-evaluations are conducted yearly, and the result of the assessment should be reported to authorities such as FDA. Biosimilars are required to undergo pharmacovigilance (PVG) regulations as its reference product. Thus biosimilars approved by the EMA (European Medicines Agency) are required to submit a risk management plan (RMP) along with the marketing application and have to provide regular safety update reports after the product is in the market. The RMP includes the safety profile of the drug and proposes the prospective pharmacovigilance studies.

Several PK studies, such as studies conducted by Committee for Medicinal Products for Human Use (CHMP), have been conducted under various ranges of conditions; Antibodies from an originator's product versus antibodies from an biosimilar; combination therapy and monotherapy; various diseases, etc. on the purpose to verify comparability in pharmacokinetics of the biosimilar with the reference medicinal product in a sufficiently sensitive and homogeneous population. Importantly, provided that structure and function(s), pharmacokinetic profiles and pharmacodynamic effect(s) and/or efficacy can be shown to be comparable for the biosimilar and the reference product, those adverse drug reactions which are related to exaggerated pharmacological effects can also be expected at similar frequencies.

EU has the largest number of approved biosimilar medicines up to date. The EMA's scientific committees evaluate the majority of marketing authorisation applications for biosimilar medicines before they can be approved and marketed in the EU. The EMA evaluates biosimilars according to the same standards of pharmaceutical quality, safety and efficacy that apply to all biological medicines approved in the EU.

Source: European Medicines Agency (April 2019) - *positive recommendation from EMA, EC decision might be pending

The Biologics Price Competition and Innovation Act of 2009 (BPCI Act) was originally sponsored and introduced on June 26, 2007, by Senator Edward Kennedy (D-MA). It was formally passed under the Patient Protection and Affordable Care Act (PPAC Act), signed into law by President Barack Obama on March 23, 2010. The BPCI Act was an amendment to the Public Health Service Act (PHS Act) to create an abbreviated approval pathway for biological products that are demonstrated to be highly similar (biosimilar) to a Food and Drug Administration (FDA) approved biological product. The BPCI Act is similar, conceptually, to the Drug Price Competition and Patent Term Restoration Act of 1984 (also referred to as the "Hatch-Waxman Act") which created biological drug approval through the Federal Food, Drug, and Cosmetic Act (FFD&C Act). The BPCI Act aligns with the FDA's longstanding policy of permitting appropriate reliance on what is already known about a drug, thereby saving time and resources and avoiding unnecessary duplication of human or animal testing. The FDA has released a total of four draft guidelines related to biosimilar or follow-on biologics development. Upon the release of the first three guidance documents the FDA held a public hearing on May 11, 2012.

In 2018, the FDA released a Biosimilars Action Plan to implement regulations from the BPCI, including limiting the abuse of the Risk Evaluation and Mitigation Strategy (REMS) system for evergreening and transitioning insulin and human growth hormone to regulation as biologics rather than drugs.

In Europe no unique identifier of a biosimilar medicine product is required, same rules are followed as for all biologics. For identifying and tracing biological medicines in the EU, medicines have to be distinguished by the tradename and batch number and this is particularly important in cases where more than one medicine with the same INN exists on the market. This ensures that, in line with EU requirements for ADR reporting, the medicine can be correctly identified if any product-specific safety (or immunogenicity) concern arises.
The report 1 of the May 2017 WHO Expert Consultation on Improving Access to and Use of Similar Biotherapeutic Products, published in October 2017, revealed on page 4, that following the outcome arising from the meeting: "No consensus was reached on whether WHO should continue with the BQ...WHO will not be proceeding with this at present."
On 14 February 2019, Health Canada announced the decision that both the brand name and non-proprietary name should be used throughout the medication use process. Biologics that share the same non-proprietary name can be distinguished by their unique brand names .
The US decided on a different approach, being only jurisdiction requiring the assignment of a four character alphabetic suffix to the nonproprietary name of the original product to distinguish between innovator drugs and their biosimilars.

In the United States, biosimilars have not had the expected impact on prices, leading a 2019 proposal to price regulate instead after an exclusivity period. Another proposal requires originators to share the underlying cell lines.

In 2019, the proposed Biologic Patent Transparency Act would help address evergreening "patent thickets" by requiring that all patents protecting a biosimilar be disclosed.

Biosimilars have found it difficult to get market share, which led biosimilar developer Pfizer to sue Johnson & Johnson over anticompetitive contracts with pharmacy benefit managers which bundle discounts; these are sometimes called the "rebate wall", and the rebates are generally unavailable to customers.

A proposed rule affecting Medicare / Medicaid enrollees announced later in 2019 A proposed law entitled Prescription Pricing for the People Act of 2019 was introduced requesting that the FTC investigate rebating. In 2019, pharmaceuticals CEOs testified before a Senate committee, with companies disagreeing on biosimilar competition reform. The House Oversight Committee and Senate Finance Committee both held hearings in early 2019.

The legal requirements of approval pathways, together with the costly manufacturing processes, escalates the developing costs for biosimilars that could be between $75–$250 million per molecule. This market entry barrier affects not only the companies willing to produce them but could also delay availability of inexpensive alternatives for public healthcare institutions that subsidize treatment for their patients. Even though the biosimilars market is rising, the price drop for biological drugs at risk of patent expiration will not be as great as for other generic drugs; in fact it has been estimated that the price for biosimilar products will be 65%-85% of their originators. Biosimilars are drawing market's attention since there is an upcoming patent cliff, which will put nearly 36% of the $140 billion market for biologic drugs at risk (as of 2011), this considering only the top 10 selling products.

The global biosimilars market was in 2013 and is expected to reach by 2020 driven by the patent expiration of additional ten blockbuster biologic drugs.

Certain companies (in some cases subsidiaries) tend to operate as , with major ones including Teva, Mylan, and Sandoz and may also extend that focus to biosimilars. Sandoz, for example, introduced the first biosimilar in the United States, and plans to introduce another in 2020. Newer companies such as India-based Sun Pharma, Aurobindo Pharma, and Dr. Reddy's Laboratories as well as Canada-based Apotex have taken share in traditional generics, which has led older companies to shift their focus to complex drugs such as biosimilars.



</doc>
<doc id="16383884" url="https://en.wikipedia.org/wiki?curid=16383884" title="Drug detoxification">
Drug detoxification

Drug detoxification is variously the intervention in a case of physical dependence to a drug; the process and experience of a withdrawal syndrome; and any of various treatments for acute drug overdose.

A detoxification program for physical dependence does not necessarily address the precedents of addiction, social factors, psychological addiction, or the often-complex behavioral issues that intermingle with addiction.

The United States Department of Health and Human Services acknowledges three steps in a drug detoxification process:


The principle of rapid detoxification is to use heavy sedation alongside dosing with opioid antagonists. This approach is expensive, ineffective and extremely dangerous.

The concept of "detoxification" comes from the discredited autotoxin theory of George E. Pettey and others. David F. Musto says that "according to Pettey, opiates stimulated the production of toxins in the intestines, which had the physiological effect associated with withdrawal phenomena... Therefore treatment would consist of purging the body of toxins and any lurking morphine that might remain to stimulate toxin production in the future."

Naltrexone therapy, which critics claim lacks long-term efficacy and can actually be detrimental to a patient's long-term recovery, has led to controversy. Additionally, there have been many questions raised about the ethics as well as safety of rapid detox following a number of deaths resulting from the procedure.

Some researchers say, that relapses to injection use of illicit opioids during or following repeated detoxification episodes carry the substantial potential for injury associated with uncontrolled drug use and include drug overdose, infections, and death.



</doc>
<doc id="2050894" url="https://en.wikipedia.org/wiki?curid=2050894" title="Astringent">
Astringent

An astringent (sometimes called adstringent) is a chemical that shrinks or constricts body tissues. The word derives from the Latin "adstringere", which means "to bind fast". Calamine lotion, witch hazel, and yerba mansa, a Californian plant, are astringents.

Astringency, the dry, puckering mouthfeel caused by the tannins in unripe fruits, lets the fruit mature by deterring eating. Ripe fruits and fruit parts including blackthorn (sloe berries), "Aronia" chokeberry, chokecherry, bird cherry, rhubarb, quince and persimmon fruits, and banana skins are very astringent; citrus fruits, like lemons, are somewhat astringent. Tannins, being a kind of polyphenol, bind salivary proteins and make them precipitate and aggregate, producing a rough, "sandpapery", or dry sensation in the mouth. The tannins in some teas and red grape wines like Cabernet Sauvignon and Merlot produce mild astringency.

In medicine, astringents cause constriction or contraction of mucous membranes and exposed tissues and are often used internally to reduce discharge of blood serum and mucous secretions. This can happen with a sore throat, hemorrhages, diarrhea, and peptic ulcers. Externally applied astringents, which cause mild coagulation of skin proteins, dry, harden, and protect the skin. People with acne are often advised to use astringents if they have oily skin. Mild astringents relieve such minor skin irritations as those resulting from superficial cuts; allergies; insect bites; anal hemorrhoids; and fungal infections such as athlete's foot.

Some common astringents are alum, acacia, sage, yarrow, witch hazel, bayberry, distilled vinegar, very cold water, and rubbing alcohol. Astringent preparations include silver nitrate, potassium permanganate, zinc oxide, zinc sulfate, Burow's solution, tincture of benzoin, and such vegetable substances as tannic and gallic acids. Balaustines are the red rose-like flowers of the pomegranate, which are very bitter to the taste. In medicine, their dried form has been used as an astringent. Some metal salts and acids have also been used as astringents. Redness-reducing eye drops contain an astringent. Use of Goulard's Extract has been discontinued, because of lead poisoning.


</doc>
<doc id="6133004" url="https://en.wikipedia.org/wiki?curid=6133004" title="Sialogogue">
Sialogogue

A sialogogue, sialagogue, ptysmagogue or ptyalagogue is a drug or substance that increases the flow rate of saliva.

Sialogogues can be used in the treatment of xerostomia (the subjective feeling of having a dry mouth), to stimulate any functioning salivary gland tissue to produce more saliva. Saliva has a bactericidal effect, so when low levels of it are secreted, the risk of caries increases. Not only this, but fungal infections such as oral candidosis also can be a consequence of low salivary flow rates. The buffer effect of saliva is also important, neutralising acids that cause tooth enamel demineralisation. The following are used in dentistry to treat xerostomia:


A tincture is prepared from the root of the pyrethrium (pyrethrum) or pellitory (a number of plants in the "Chrysanthemum" family). It is found growing in Levant and parts of Limerick and Clare in Ireland. The root powder was used as flavouring in tooth powders in the past.

Some of the pyrethrin extracts find use as relatively environmentally benign insecticides.



</doc>
<doc id="5508354" url="https://en.wikipedia.org/wiki?curid=5508354" title="Vasoactivity">
Vasoactivity

A vasoactive substance is an endogenous agent or pharmaceutical drug that has the effect of either increasing or decreasing blood pressure and/or heart rate through its vasoactivity, that is, vascular activity (effect on blood vessels). By adjusting vascular compliance and vascular resistance, typically through vasodilation and vasoconstriction, it helps the body's homeostatic mechanisms (such as the renin–angiotensin system) to keep hemodynamics under control. For example, angiotensin, bradykinin, histamine, nitric oxide, and vasoactive intestinal peptide are important endogenous vasoactive substances. Vasoactive drug therapy is typically used when a patient has the blood pressure and heart rate monitored constantly. The dosage is typically titrated (adjusted up or down) to achieve a desired effect or range of values as determined by competent clinicians.

Vasoactive drugs are typically administered using a volumetric infusion device (IV pump). This category of drugs require close observation of the patient with near immediate intervention required by the clinicians in charge of the patient's care. Important vasoactive substances are angiotensin-11, endothelin-1, and alpha-adrenergic agonists.

Various vasoactive agents, such as prostanoids, phosphodiesterase inhibitors, and endothelin antagonists, are approved for the treatment of pulmonary arterial hypertension. The use of vasoactive agents for patients with pulmonary hypertension may cause harm and unnecessary expense to persons with left heart disease or hypoxemic types of lung diseases.


</doc>
<doc id="22907964" url="https://en.wikipedia.org/wiki?curid=22907964" title="Drug coupon">
Drug coupon

A drug coupon is a coupon intended to help consumers save money on pharmaceutical drugs. They are offered by drug companies or distributed to consumers via doctors and pharmacists, and most can be obtained online. There are drug coupons for drugs from many categories such as cholesterol, acne, migraine, allergies, etc.

Direct-to-consumer or "DTC" marketing of prescription drugs is common in the United States. Patients frequently inquire about or request medications they have seen advertised in print or on television.

Pharmaceutical companies use drug coupons as a marketing tool to stimulate demand for their products.

Drug coupons are commonly offered for new products to stimulate demand or ameliorate high co-pays for non-formulary (non-preferred products) as a way to level the playing field and remove the disincentive for using a drug that is not covered by insurance.

In an effort to avoid unregulated resale of drugs, the Prescription Drug Marketing Act of 1988 banned the traffic or counterfeiting of redeemable drug coupons. For instance, it is against the law to buy or sell coupons for prescription drugs on E-Bay.

Most drug coupons are printed by consumers using their personal computer and printer. Drug coupons reduce out-of-pocket costs for consumers in a variety of ways such as instant savings offers, free trial offers (also known as try-before-you-buy offers), copay reduction or rebates.

Generic drug companies rarely offer coupons, though insurance companies occasionally offer discounts on generic drugs.

In addition PBMs (Pharmacy Benefit Managers) offer discount cards that act similarly to coupons. These cards work for both generic and brand medications and can save cash paying customers up to 75% on their prescription medication.

Medicare was designed by the U.S. government in 1965 to help senior citizens and the disadvantaged afford health care. The Medicare Prescription Drug, Improvement, and Modernization Act of 2003 (MMA) put Medicare Part D into effect on January 1, 2006, expanding Medicare benefits to include subsidized prescription products for Medicare beneficiaries in the United States.

Drug coupons may not be used for prescription products paid for in full or in part by any government sponsored insurance such as Medicare Part D, Medicaid, MediCal Tricare, etc. However, patients in the so-called Medicare Part D "doughnut hole" who pay cash for their medication may use coupons. Citation needed

Coupons are prevalent in product categories where there is a lot of competition, such as dermatology products, lipid modifying agents and the medical treatment of ophthalmological conditions such as glaucoma. So-called "lifestyle medications" (e.g., treatments for baldness and erectile dysfunction and cosmetic-oriented products such as wrinkle fillers) typically are not covered by insurance and are unlikely to have coupons.

So-called specialty pharmaceuticals (such as TNF inhibitors) also use coupons as a marketing tactic.

Coupons for oral or topical agents can provide significant savings, sometimes as much as several hundred dollars per year.

Coupons (usually in the form of co-pay assistance programs) for specialty pharmaceutical products, particularly injectable drugs such as TNF inhibitors can be worth several thousand dollars per year.

Unlike the distribution of drug samples to patients, without exception drug coupons require a valid prescription written by a licensed physician and dispensed by a registered pharmacist in order for the offer to be fulfilled. Having a physician and pharmacist involved in the transaction enhances patient safety, unlike the use of drug samples.

This dual layer of professional scrutiny ensures that the drug product has been properly stored, is not expired, is dosed correctly in an age-appropriate fashion, and that the patient's medication history has been checked to avoid drug interactions.

Drug samples, on the other hand, are often dispensed by doctor's office staff without the benefit of a prescription. The storage of drug samples is largely unregulated, leading to the possibility of spoilage or pilferage. Additionally, in case of a recall or other similar event there is a paper trail that simplifies patient notification.

Critics of prescription drug coupon programs have argued that these programs lead to higher healthcare costs for consumers. Typically, American patients with health insurance pay a percentage of the cost of a prescription drug out of pocket, with insurance companies responsible for the rest of the medication's cost. Insurance companies charge higher copayments for brand-name drugs than for generics in order to encourage patients to choose less expensive alternative medications when they are available. However, by reducing a patient's copayment, prescription drug coupons also reduce a patient's incentive to choose a less expensive generic medication. As an example, NPR reported in 2009 that the generic acne medication Minocin cost $109 a month while a newer alternative called Solodyn cost $514 a month. Solodyn can be taken only once daily, while Minocin must be taken twice daily. Since Solodyn's manufacturer offers a coupon to reduce copays, patients may believe that both drugs cost the same amount.

Drug coupon advocates argue that coupon programs enhance medication adherence by reducing or eliminating drug copays.

As of 2008, the U.S. Food and Drug Administration (FDA) was planning a study to see if coupons make patients overlook drug risks and side effects in their effort to save money. All medicines come with a certain level of risk. This is why all prescription drugs include information reviewed and approved by the U.S. Food and Drug Administration (FDA) about how the medication works in the body, its uses and when it should not be used, possible side effects, the recommended dosage, and other facts about the appropriate use of the drug. Most Americans have never heard of “Risk Evaluation and Mitigation Strategies” (REMS); yet, these drug safety protections give millions of Americans with serious diseases, such as cancer and multiple sclerosis, access to medicines that would otherwise be too dangerous to be allowed on the market. REMS programs exist because certain medicines can cause serious side effects, life-threatening infections, allergic reactions, liver damage or birth defects. Before patients take one of these drugs, they and their prescriber should be aware of the risks, and may need to take certain steps that lessen these risks, in order to ensure they benefit from the treatment.

Unions have filed several lawsuits seeking to ban drug coupons, characterizing them as illegal kickbacks. As of June, 2013 Three of these lawsuits have been dismissed. Six more are pending. This ruling is seen as vindication for the Pharmaceutical industry that drug coupons are an acceptable business practice.




</doc>
<doc id="21054515" url="https://en.wikipedia.org/wiki?curid=21054515" title="Drug reference standard">
Drug reference standard

1. Definition<br>
A pharmaceutical reference standard is a highly characterized material suitable to test the identity, strength, quality and purity of substances for pharmaceutical use and medicinal products.

2. Pharmacopoeial Reference standards<br>
Pharmacopoeial Reference standards are a subset of Pharmaceutical Reference Standards. They are established for the intended use described in Pharmacopeial texts (monographs and general chapters). Pharmacopeial Reference Standards are available from various pharmacopoeias such as United States Pharmacopeia and the European Pharmacopoeia.
Where pharmacopoeial tests or assays call for the use of a Pharmacopoeial Reference Standard, only those results obtained using the specified Pharmacopoeial Reference Standard are conclusive.




</doc>
<doc id="30366960" url="https://en.wikipedia.org/wiki?curid=30366960" title="Metallopharmaceutical">
Metallopharmaceutical

A metallopharmaceutical is a drug that contains a metal as an active ingredient. Most commonly metallopharmaceuticals are used as anticancer or antimicrobial agents. The efficiency of metallopharmaceuticals is crucially dependent on the respective trace metal binding forms.

Examples of metallopharmaceuticals include:




</doc>
<doc id="3084027" url="https://en.wikipedia.org/wiki?curid=3084027" title="Targeted therapy">
Targeted therapy

Targeted therapy or molecularly targeted therapy is one of the major modalities of medical treatment (pharmacotherapy) for cancer, others being hormonal therapy and cytotoxic chemotherapy. As a form of molecular medicine, targeted therapy blocks the growth of cancer cells by interfering with specific targeted molecules needed for carcinogenesis and tumor growth, rather than by simply interfering with all rapidly dividing cells (e.g. with traditional chemotherapy). Because most agents for targeted therapy are biopharmaceuticals, the term "biologic therapy" is sometimes synonymous with "targeted therapy" when used in the context of cancer therapy (and thus distinguished from chemotherapy, that is, cytotoxic therapy). However, the modalities can be combined; antibody-drug conjugates combine biologic and cytotoxic mechanisms into one targeted therapy.

Another form of targeted therapy involves use of nanoengineered enzymes to bind to a tumor cell such that the body's natural cell degradation process can digest the cell, effectively eliminating it from the body.

Targeted cancer therapies are expected to be more effective than older forms of treatments and less harmful to normal cells. Many targeted therapies are examples of immunotherapy (using immune mechanisms for therapeutic goals) developed by the field of cancer immunology. Thus, as immunomodulators, they are one type of biological response modifiers.

The most successful targeted therapies are chemical entities that target or preferentially target a protein or enzyme that carries a mutation or other genetic alteration that is specific to cancer cells and not found in normal host tissue. One of the most successful molecular targeted therapeutic is Gleevec, which is a kinase inhibitor with exceptional affinity for the oncofusion protein BCR-Abl which is a strong driver of tumorigenesis in chronic myelogenous leukemia. Although employed in other indications, Gleevec is most effective targeting BCR-Abl. Other examples of molecular targeted therapeutics targeting mutated oncogenes, include PLX27892 which targets mutant B-raf in melanoma.

There are targeted therapies for lung cancer, colorectal cancer, head and neck cancer, breast cancer, multiple myeloma, lymphoma, prostate cancer, melanoma and other cancers.

Biomarkers are usually required to aid the selection of patients who will likely respond to a given targeted therapy.

Co-targeted therapy involves the use of one or more therapeutics aimed at multiple targets, for example PI3K and MEK, in an attempt to generate a synergistic response and prevent the development of drug resistance.

The definitive experiments that showed that targeted therapy would reverse the malignant phenotype of tumor cells involved treating Her2/neu transformed cells with monoclonal antibodies in vitro and in vivo by Mark Greene's laboratory and reported from 1985.

Some have challenged the use of the term, stating that drugs usually associated with the term are insufficiently selective. The phrase occasionally appears in scare quotes: "targeted therapy". Targeted therapies may also be described as "chemotherapy" or "non-cytotoxic chemotherapy", as "chemotherapy" strictly means only "treatment by chemicals". But in typical medical and general usage "chemotherapy" is now mostly used specifically for "traditional" cytotoxic chemotherapy.

The main categories of targeted therapy are currently "small molecules" and "monoclonal antibodies."

Many are tyrosine-kinase inhibitors.




Several are in development and a few have been licensed by the FDA and the European Commission. Examples of licensed monoclonal antibodies include:

Many antibody-drug conjugates (ADCs) are being developed. See also ADEPT (antibody-directed enzyme prodrug therapy).

In the U.S., the National Cancer Institute's "Molecular Targets Development Program" (MTDP) aims to identify and evaluate molecular targets that may be candidates for drug development.



</doc>
<doc id="20769112" url="https://en.wikipedia.org/wiki?curid=20769112" title="Asymmetric membrane capsule">
Asymmetric membrane capsule

The asymmetric membrane capsule is an example of a single core osmotic delivery system, consisting of a drug-containing core surrounded by an asymmetric membrane made with a non disintegrating polymer (cellulose acetate, ethylcellulose etc.)


</doc>
<doc id="958884" url="https://en.wikipedia.org/wiki?curid=958884" title="Lifestyle drug">
Lifestyle drug

Lifestyle drug is an imprecise term commonly applied to medications which treat non–life-threatening and non-painful conditions such as baldness, wrinkles, erectile dysfunction, or acne, which the speaker perceives as either not medical problems at all or as minor medical conditions relative to others. It is sometimes intended as a pejorative, bearing the implication that the scarce medical research resources allocated to develop such drugs were spent frivolously when they could have been better spent researching cures for more serious medical conditions. Proponents, however, point out that improving the patient's subjective quality of life has always been a primary concern of medicine, and argue that these drugs are doing just that. It finds broad use in both media and scholarly journals.

There is direct impact of lifestyle drugs on society, particularly in the developing world. Implications associated with labeling of indications and products sales of these lifestyle drugs may be varied. Drugs can, over time, switch from 'lifestyle' to 'mainstream' use.

Though no precise widely accepted definition or criteria are associated with the term, there is much debate within the fields of pharmacology and bioethics around the propriety of developing such drugs, particularly after the commercial debut of Viagra.

The German government's health insurance scheme has denied insurance coverage for some "Lebensstildroge" ("lifestyle drugs") which they deem spurious.

Critics of pharmaceutical firms claim that pharmaceutical firms actively medicalize; that is, they invent novel disorders and diseases which were not recognized as such before their "cures" could be profitably marketed, in effect pathologizing what were widely regarded as normal conditions of human existence. The consequences are said to include generally greater worries about health, misallocation of limited medical research resources to comparatively minor conditions while many serious diseases remain uncured, and needless health care expenditure. This medicalization of some element of human condition has significance, in principle, as a matter for political discourse or dialogue in civil society concerning values or morals.

Social critics also question the propriety of devoting huge research budgets towards creating these drugs when far more dangerous diseases like cancer and AIDS remain uncured. It is sometimes claimed that lifestyle drugs amount to little more than medically sanctioned recreational drug use.



</doc>
<doc id="40969005" url="https://en.wikipedia.org/wiki?curid=40969005" title="Topological drugs">
Topological drugs

Topological inhibitors (so-called “topological drugs”) are rigid three-dimensional molecules of inorganic, organic and hybrid compounds (as guests) that form multicentered supramolecular interactions in vacant cavities of protein macromolecules and their complexes (as hosts) .
Extensive surface and very diverse geometry make cage compounds with an encapsulated metal ion (clathrochelates) suitable for targeting both the active and allosteric sites of enzymes as well as the interfaces of their macromolecular complexes. An efficient structure- and concentration-dependent transcription inhibition in a model in vitro systems based on RNA and DNA polymerases by the iron(II) mono- and bis-clathrochelates at their submicro- and nanomolar concentrations, respectively, is observed in. Molecular docking and preincubation experiments suggested that these cage compounds form supramolecular assemblies with protein residues as well as with DNA and RNA. Thus, they are prospective precursors for the design of antiviral and anticancer drug candidates.

The modern approach to the pharmaceutical screening is based on several paradigms and empirical rules. They limit the chemical compounds that have a potential as pharmaceuticals for diagnostics and therapy; many chemical systems and classes have been excluded from the routine screening studies and, therefore, are terra incognita for biochemists, pharmacologists, and clinicians. For example, one of the Lipinski's rules states that the molecular weight of the chemical compounds that can be used for pharmaceutical screening should be under 500 Da. The biological activity of these low-weight compounds, constituting up to 90% of the pharmaceutical market (by nomenclature), in most cases is governed by their interactions with biological targets via either covalent or supramolecular bonding. The selectivity and specificity of these interactions determine the potential of the low-weight chemical compounds in drug therapy (for example, in the cases of the virus infections and the neurodegenerative diseases). Among the low-weight pharmaceuticals, the xenobiotic and abiotic compounds, which have neither the natural analogs nor the structural similarity to the biological molecules, are of particular interest. An example of such abiotic (xenobiotic) compounds is the carbocyclic adamantane derivatives. These compounds are widely used in drug therapy of the human diseases .

Second type of the pharmaceuticals available on the market comprises the biological, biologically revealed, and biomimetic macromolecules with molecular weights more than 2000 Da, the therapeutic activity of which is defined mainly by the complex protein – protein interactions. This class of the high-weight biological macromolecules includes, for example, insulins, enzymes, interferons, proteins (albumin, etc.), and vaccines as well.
At the same time, only limited number of the middle-weight compounds with a molecular weight from 500 to 2000 Da is used as the therapeutics or is now under study as potent chemical compounds for drug therapy and diagnostics.

Besides the restrictions applied on the bioavailability of the middle-weight molecules, their screening seems to be prospective owing to the multicentered and geometrically directed interactions that these molecules can form with biological targets (i.e., the topological recognition of a “guest” middle-weight molecule by a “host” biological receptor).

The success of the drug therapy of the virus and Parkinson's diseases using abiotic polyhedral adamantane molecules suggests the pharmaceutical potential of other xenobiotics with rigid three-dimensional molecular structure. For example, the functionalized fullerenes have been used for drug therapy of the HIV and other virus diseases: their hydrophobic ball-like molecules “block” a virus active site. This site can also be a target for other xenobiotics, which are geometrically similar to those of the functionalized fullerenes and are complementary to the HIV protease active site. Cage complexes with a metal ion encapsulated in a three-dimensional macropolycyclic ligand cavity (clathrochelates) are the topological analogs of the fullerene derivatives with similar geometric parameters. This similarity between the clathrochelates and the functionalized fullerenes has been confirmed by the molecular docking of a set of the clathrochelate structures to the active site of the HIV protease. Most these calculated macrobicyclic inhibitors have the same mode of the inhibition as their fullerene-containing analogs, and the calculated (for clathrochelates and fullerenes) and experimental (for fullerenes) inhibition constants are close.

When the influence of the cage metal complexes on biochemical processes is based on the topological similarity of their molecules to those of the chemical compounds with improved biological and pharmaceutical activities, the applicability of these clathrochelates is determined by their availability. The clathrochelate molecules have four axes and eight sites of functionalization. In contrast to the functionalized fullerenes, such direct functionalization may be easily employed using convenient procedures and commercially available chemical reagents. Thus, the three-dimensional rigid cage molecules with given location of the pharmacophore substituents can be obtained, i.e. it is possible to use their clathrochelate frameworks as the three-dimensional scaffolds for the design of the topological drugs.

So, (i) the specific abiotic interactions of the middle-weight molecules (for example, the clathrochelates) with biological macromolecules are possible, (ii) these interactions may change both the structure and the functions of these macromolecules, (iii) the middle-weight conformationally rigid macropolycyclic complexes with biomimetic substituents (“guests”) that bind to the molecules of the biological target (“hosts”) are prospective for the drug design, (iv) the macropolycyclic framework may be considered as a scaffold for the direct “bottom-up” synthesis of the biologically active compounds based on the “geometrical recognition” and “topological invasion” principles, (v) the clathrochelate complexes of transition metals are suitable for the design of the “topological drugs” owing to the availability of these geometrically well-organized three-dimensional macrobicycles, (vi) there are effective pathways for the direct synthesis of the functionalized clathrochelates with different symmetry, structure, and chemical reactivity, as well as of the complexes with given physical and physico-chemical properties.

The concept of the “topological drugs” seems to be a promising one for the drug search from the library of the middle-weight compounds as well as for the decrease of the resistance to the pharmaceutically active chemical compounds. This concept needs both the theoretical studies and the experimental improvements of different biological and biomimetic systems as well as of various types of the cage complexes.


</doc>
<doc id="334816" url="https://en.wikipedia.org/wiki?curid=334816" title="Route of administration">
Route of administration

A route of administration in pharmacology and toxicology is the path by which a drug, fluid, poison, or other substance is taken into the body.

Routes of administration are generally classified by the location at which the substance is applied. Common examples include oral and intravenous administration. Routes can also be classified based on where the target of action is. Action may be topical (local), enteral (system-wide effect, but delivered through the gastrointestinal tract), or parenteral (systemic action, but delivered by routes other than the GI tract). Route of administration and dosage form are aspects of drug delivery.

Routes of administration are usually classified by application location (or exposition).

The route or course the active substance takes from application location to the location where it has its target effect is usually rather a matter of pharmacokinetics (concerning the processes of uptake, distribution, and elimination of drugs). Exceptions include the transdermal or transmucosal routes, which are still commonly referred to as "routes of administration".

The location of the target effect of active substances are usually rather a matter of pharmacodynamics (concerning e.g. the physiological effects of drugs). An exception is topical administration, which generally means that both the application location and the effect thereof is local.

Topical administration is sometimes defined as both a local application location and local pharmacodynamic effect, and sometimes merely as a local application location regardless of location of the effects.

Administration through the gastrointestinal tract is sometimes termed "enteral or enteric administration" (literally meaning 'through the intestines'). "Enteral/enteric administration" usually includes "oral" (through the mouth) and "rectal" (into the rectum) administration, in the sense that these are taken up by the intestines. However, uptake of drugs administered orally may also occur already in the stomach, and as such "gastrointestinal" (along the gastrointestinal tract) may be a more fitting term for this route of administration. Furthermore, some application locations often classified as "enteral", such as sublingual (under the tongue) and sublabial or buccal (between the cheek and gums/gingiva), are taken up in the proximal part of the gastrointestinal tract without reaching the intestines. Strictly enteral administration (directly into the intestines) can be used for systemic administration, as well as local (sometimes termed topical), such as in a contrast enema, whereby contrast media is infused into the intestines for imaging. However, for the purposes of classification based on location of effects, the term enteral is reserved for substances with systemic effects.

Many drugs as tablets, capsules, or drops are taken orally. Administration methods directly into the stomach include those by gastric feeding tube or gastrostomy. Substances may also be placed into the small intestines, as with a duodenal feeding tube and enteral nutrition. Enteric coated tablets are designed to dissolve in the intestine, not the stomach, because the drug present in the tablet causes irritation in the stomach.

The rectal route is an effective route of administration for many medications, especially those used at the end of life. The walls of the rectum absorb many medications quickly and effectively. Medications delivered to the distal one-third of the rectum at least partially avoid the "first pass effect" through the liver, which allows for greater bio-availability of many medications than that of the oral route. Rectal mucosa is highly vascularized tissue that allows for rapid and effective absorption of medications. A suppository is a solid dosage form that fits for rectal administration. In hospice care, a specialized rectal catheter, designed to provide comfortable and discreet administration of ongoing medications provides a practical way to deliver and retain liquid formulations in the distal rectum, giving health practitioners a way to leverage the established benefits of rectal administration.

The parenteral route is any route that is not enteral ("par-" + "enteral").

Parenteral administration can be performed by injection, that is, using a needle (usually a hypodermic needle) and a syringe, or by the insertion of an indwelling catheter.

Locations of application of parenteral administration include:



The definition of the topical route of administration sometimes states that both the application location and the pharmacodynamic effect thereof is local.

In other cases, "topical" is defined as applied to a localized area of the body or to the surface of a body part regardless of the location of the effect. By this definition, topical administration also includes transdermal application, where the substance is administered onto the skin but is absorbed into the body to attain systemic distribution.

If defined strictly as having local effect, the topical route of administration can also include enteral administration of medications that are poorly absorbable by the gastrointestinal tract. One poorly absorbable antibiotic is vancomycin, which is recommended by mouth as a treatment for severe "Clostridium difficile" colitis.

The reason for choice of routes of drug administration are governing by various factors:


In acute situations, in emergency medicine and intensive care medicine, drugs are most often given intravenously. This is the most reliable route, as in acutely ill patients the absorption of substances from the tissues and from the digestive tract can often be unpredictable due to altered blood flow or bowel motility.

Enteral routes are generally the most convenient for the patient, as no punctures or sterile procedures are necessary. Enteral medications are therefore often preferred in the treatment of chronic disease. However, some drugs can not be used enterally because their absorption in the digestive tract is low or unpredictable. Transdermal administration is a comfortable alternative; there are, however, only a few drug preparations that are suitable for transdermal administration.

Identical drugs can produce different results depending on the route of administration. For example, some drugs are not significantly absorbed into the bloodstream from the gastrointestinal tract and their action after enteral administration is therefore different from that after parenteral administration. This can be illustrated by the action of naloxone (Narcan), an antagonist of opiates such as morphine. Naloxone counteracts opiate action in the central nervous system when given intravenously and is therefore used in the treatment of opiate overdose. The same drug, when swallowed, acts exclusively on the bowels; it is here used to treat constipation under opiate pain therapy and does not affect the pain-reducing effect of the opiate.

The oral route is generally the most convenient and costs the least. However, some drugs can cause gastrointestinal tract irritation. For drugs that come in delayed release or time-release formulations, breaking the tablets or capsules can lead to more rapid delivery of the drug than intended. The oral route is limited to formulations containing small molecules only while biopharmaceuticals (usually proteins) would be digested in the stomach and thereby become ineffective. Biopharmaceuticals have to be given by injection or infusion. However, recent research (2018) found an organic ionic liquid suitable for oral insulin delivery (a biopharmaceutical) into the blood stream.

Oral administration is often denoted "PO" from "per os", the Latin for "by mouth".

The bioavailability of oral administration is affected by the amount of drug that is absorbed across the intestinal epithelium and first-pass metabolism.

By delivering drugs almost directly to the site of action, the risk of systemic side effects is reduced. 

Skin absorption (dermal absorption), for example, is to directly deliver drug to the skin and, hopefully, to the systemic circulation. However, skin irritation may result, and for some forms such as creams or lotions, the dosage is difficult to control. Upon contact with the skin, the drug penetrates into the dead stratum corneum and can afterwards reach the viable epidermis, the dermis, and the blood vessels.

Inhaled medications can be absorbed quickly and act both locally and systemically. Proper technique with inhaler devices is necessary to achieve the correct dose. Some medications can have an unpleasant taste or irritate the mouth.

In general, only 20–50% of the pulmonary-delivered dose rendered in powdery particles will be deposited in the lung upon mouth inhalation. The remainder of 50-70% undeposited aerosolized particles are cleared out of lung as soon as exhalation.

An inhaled powdery particle that is >8 μm is structurally predisposed to depositing in the central and conducting airways (conducting zone) by inertial impaction.

An inhaled powdery particle that is between 3 and 8 μm in diameter tend to largely deposit in the transitional zones of the lung by sedimentation.

An inhaled powdery particle that is <3 μm in diameter is structurally predisposed to depositing primarily in the respiratory regions of the peripheral lung via diffusion.

Particles that deposit in the upper and central airways are rarely absorbed systemically because they are going to be removed by mucociliary clearance in an efficient and rapid fashion.

Inhalation by smoking a substance is likely the most rapid way to deliver drugs to the brain, as the substance travels directly to the brain without being diluted in the systemic circulation. The severity of dependence on psychoactive drugs tends to increase with more rapid drug delivery.

The term injection encompasses intravenous (IV), intramuscular (IM), subcutaneous (SC) and intradermal (ID) administration.

Parenteral administration generally acts more rapidly than topical or enteral administration, with onset of action often occurring in 15–30 seconds for IV, 10–20 minutes for IM and 15–30 minutes for SC. They also have essentially 100% bioavailability and can be used for drugs that are poorly absorbed or ineffective when they are given orally. Some medications, such as certain antipsychotics, can be administered as long-acting intramuscular injections. Ongoing IV infusions can be used to deliver continuous medication or fluids.

Disadvantages of injections include potential pain or discomfort for the patient and the requirement of trained staff using aseptic techniques for administration. However, in some cases, patients are taught to self-inject, such as SC injection of insulin in patients with insulin-dependent diabetes mellitus. As the drug is delivered to the site of action extremely rapidly with IV injection, there is a risk of overdose if the dose has been calculated incorrectly, and there is an increased risk of side effects if the drug is administered too rapidly.

Drug administration via the nasal cavity yields rapid drug absorption and therapeutic effects. This is because drug absorption through the nasal passages doesn't go through the gut before entering capillaries situated at tissue cells and then systemic circulation and such absorption route allows transport of drugs into the central nervous system via the pathways of olfactory and trigeminal nerve.

Intranasal absorption features low lipophilicity, enzymatic degradation within the nasal cavity, large molecular size, and rapid mucociliary clearance from the nasal passages, which explains the low risk of systemic exposure of the administered drug absorbed via intranasal.

Sublingual administration is fulfilled by placing the drug between the tongue and the lower surface of the mouth. The sublingual mucosa is highly permeable and thereby provides access to the underlying expansive network composed of capillaries, leading to rapid drug absorption.

Buccally administered a medication is achieved by placing the drug between gums and the inner lining of the cheek. In comparison with sublingual tissue, buccal tissue is less permeable resulting in slower absorption. 

Sublabial administration

Neural drug delivery is the next step beyond the basic addition of growth factors to nerve guidance conduits. Drug delivery systems allow the rate of growth factor release to be regulated over time, which is critical for creating an environment more closely representative of in vivo development environments.




</doc>
<doc id="41589960" url="https://en.wikipedia.org/wiki?curid=41589960" title="USP Controlled Room Temperature">
USP Controlled Room Temperature

The USP Controlled Room Temperature is a series of United States Pharmacopeia guidelines for the storage of pharmaceuticals; the relevant omnibus standard is USP 797. Although 100% compliance remains challenging for any given facility, the larger protocol may be regarded as constituting a form of clean room which is included in a suite of best practices.


</doc>
<doc id="42663521" url="https://en.wikipedia.org/wiki?curid=42663521" title="Cholekinetic">
Cholekinetic

A cholekinetic drug is a pharmaceutical drug which increases the contractile power of the bile duct.



</doc>
<doc id="45516653" url="https://en.wikipedia.org/wiki?curid=45516653" title="Radiopharmaceutical">
Radiopharmaceutical

Radiopharmaceuticals, or medicinal radiocompounds, are a group of pharmaceutical drugs containing radioactive isotopes. Radiopharmaceuticals can be used as diagnostic and therapeutic agents. Radiopharmaceuticals emit radiation themselves, which is different from contrast media which absorb or alter external electromagnetism or ultrasound. Radiopharmacology is the branch of pharmacology that specializes in these agents. 

The main group of these compounds are the radiotracers used to diagnose dysfunction in body tissues. While not all medical isotopes are radioactive, radiopharmaceuticals are the oldest and still most common such drugs.
As with other pharmaceutical drugs, there is standardization of the drug nomenclature for radiopharmaceuticals, although various standards coexist. The International Nonproprietary Names (INNs), United States Pharmacopeia (USP) names, and IUPAC names for these agents are usually similar other than trivial style differences. The details are explained at "Radiopharmacology § Drug nomenclature for radiopharmaceuticals".

A list of nuclear medicine radiopharmaceuticals follows. Some radioisotopes are used in ionic or inert form without attachment to a pharmaceutical; these are also included. There is a section for each radioisotope with a table of radiopharmaceuticals using that radioisotope. The sections are ordered alphabetically by the English name of the radioisotope. Sections for the same element are then ordered by atomic mass number.

Ca is a beta and gamma emitter.

C is a positron emitter.

C is a beta emitter.

Cr is a gamma emitter.
Co is a gamma emitter.

Co is a gamma emitter.

Er is a beta emitter.

F is a positron emitter with a half-life of 109 minutes. It is produced in medical cyclotrons, usually from oxygen-18, and then chemically attached to a pharmaceutical. See PET scan.

Ga is a gamma emitter. See gallium scan.

Ga is a positron emitter, with a 68-minute half-life, produced by elution from germanium-68 in a gallium-68 generator. See also positron emission tomography.

H or tritium is a beta emitter.

In is a gamma emitter.

Iodine-123 (I-123) is a gamma emitter. It is used only diagnostically, as its radiation is penetrating and short-lived.

I is a gamma emitter with a long half-life of 59.4 days (the longest of all radioiodines used in medicine). Iodine-123 is preferred for imaging, so I-125 is used diagnostically only when the test requires a longer period to prepare the radiopharmaceutical and trace it, such as a fibrinogen scan to diagnose clotting. I-125's gamma radiation is of medium penetration, making it more useful as a therapeutic isotope for brachytherapy implant of radioisotope capsules for local treatment of cancers.

I is a beta and gamma emitter. It is used both to destroy thyroid and thyroid cancer tissues (via beta radiation, which is short-range), and also other neuroendocrine tissues when used in MIBG. It can also be seen by a gamma camera, and can serve as a diagnostic imaging tracer, when treatment is also being attempted at the same time. However iodine-123 is usually preferred when only imaging is desired.

Fe is a beta and gamma emitter.

Kr is a gamma emitter.

 is a beta emitter.

N is a positron emitter.

O is a positron emitter.

P is a beta emitter.

Ra is an alpha emitter.

Rb is a positron and gamma emitter.

Sm is a beta and gamma emitter.

Se is a gamma emitter.

Na is a positron and gamma emitter.

Na is a beta and gamma emitter.

Sr is a beta emitter.

Tc is a gamma emitter. It is obtained on-site at the imaging center as the soluble pertechnetate which is eluted from a technetium-99m generator, and then either used directly as this soluble salt, or else used to synthesize a number of technetium-99m-based radiopharmaceuticals.

Tl is a gamma emitter.

Xe is a gamma emitter.

Y is a beta emitter.



</doc>
<doc id="46363582" url="https://en.wikipedia.org/wiki?curid=46363582" title="Me-too compound">
Me-too compound

A me-too compound or follow-on drug is a drug product that contains an active pharmaceutical ingredient (API) that is chemically related, and usually very structurally similar, to a known active pharmaceutical ingredient. The me-too may differ in some respects from the prototype drug (e.g. side effect profile or activity), but uses the same mechanism and is used for the same purpose as the original. 

The term follows from the phrase "me too" and is usually used in a negative way, the idea being the me-too drug simply rode the coattails of the research and development done to develop the prototype API. Me-too drugs can be novel compounds themselves, and drug products containing them can serve as market competition, driving prices down; similarly active compounds are exhibited soon after a novel API is made available.



</doc>
<doc id="46592885" url="https://en.wikipedia.org/wiki?curid=46592885" title="Botanical drug">
Botanical drug

A botanical drug is defined in the United States Federal Food, Drug, and Cosmetic Act as a botanical product that is marketed as diagnosing, mitigating, treating, or curing a disease; a botanical product in turn, is a finished, labeled product that contains ingredients from plants. Chemicals that are purified from plants, like paclitaxel, and highly purified products of industrial fermentation, like biopharmaceuticals, are not considered to be botanical products.

In 2006 the Food and Drug Administration approved the first botanical drug in the United States: sinecatechins, a green tea extract for genital warts.

A botanical drug product is defined in the United States Federal Food, Drug, and Cosmetic Act (FD&C) as a botanical drug that is marketed as diagnosing, mitigating, treating, or curing a disease; a botanical product in turn, is a finished, labeled product that contains vegetable matter as ingredients. Chemicals that are purified from plants, like paclitaxel or artemisinin, and highly purified products of industrial fermentation, like biopharmaceuticals, are not considered to be botanical products.

The FD&C act separately regulates uses of botanical products as food (including dietary supplements), medical devices (e.g., gutta-percha), and cosmetics.

Like other drugs, botanical drugs may be sold over the counter (OTC) or by prescription only. For OTC drugs, a monograph must be created by the company that wants to market the drug and then approved by the FDA, after which it is published in the Federal Register. For prescription drugs, a New Drug Application (NDA) must be filed with and approved by the FDA; clinical data included in the NDA is gathered under an Investigational New Drug Application which the FDA also must approve before clinical testing begins.

Assessment of the safety and toxicity of botanical drugs in clinical trials, and in ensuring their quality once the drug is on the market, is complicated by the nature of the raw ingredients; problems arise in identifying the correct plants to harvest, in the quality of plants harvested, in their processing, and in the stability of the active components, which are often poorly understood.

The FDA relies on a combination of tests and controls to ensure the identity and quality of botanical drugs. The tests include "fingerprinting" using spectroscopy or chromatography, chemical or biological assays, and process controls on raw material collection and processing. The standards are higher for botanical drugs than for extracts or plant matter used in dietary supplements.

If the substance being developed as a botanical drug has been used in traditional medicine, it may be possible to begin initial, small clinical trials without conducting extensive toxicology testing. However, toxicity testing is required before beginning larger clinical trials and trials that will be used to get approval to sell a botanical drug.

The endogenous Chinese pharmaceutical industry is made up mostly of companies that make herbal traditional chinese medicines and sell them over the counter; the US regulatory pathway is similar to that established by the China Food and Drug Administration.

The European regulatory pathway is also similar.


As of 2012, the pharmaceutical industry had expressed strong interest in developing botanical drugs, with more than 500 applications pending at the FDA. Part of the interest stems from a desire to address the Chinese market, where herbal medicines remain widely used and had $13 billion in sales in 2011.



</doc>
