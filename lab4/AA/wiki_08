<doc id="16971924" url="https://en.wikipedia.org/wiki?curid=16971924" title="Economic restructuring">
Economic restructuring

Economic restructuring is used to indicate changes in the constituent parts of an economy in a very general sense. In the western world, it is usually used to refer to the phenomenon of urban areas shifting from a manufacturing to a service sector economic base. It has profound implications for productive capacities and competitiveness of cities and regions. This transformation has affected demographics including income distribution, employment, and social hierarchy; institutional arrangements including the growth of the corporate complex, specialized producer services, capital mobility, informal economy, nonstandard work, and public outlays; as well as geographic spacing including the rise of world cities, spatial mismatch, and metropolitan growth differentials.

As cities experience a loss of manufacturing jobs and growth of services, sociologist Saskia Sassen affirms that a widening of the social hierarchy occurs where high-level, high-income, salaried professional jobs expands in the service industries alongside a greater incidence of low-wage, low-skilled jobs, usually filled by immigrants and minorities. A "missing middle" eventually develops in the wage structure. Several effects of this social polarization include the increasing concentration of poverty in large U.S. cities, the increasing concentration of black and Hispanic populations in large U.S. cities, and distinct social forms such as the underclass, informal economy, and entrepreneurial immigrant communities. In addition, the declining manufacturing sector leaves behind strained blue-collared workers who endure chronic unemployment, economic insecurity, and stagnation due to the global economy's capital flight. Wages and unionization rates for manufacturing jobs also decline. One other qualitative dimension involves the feminization of the job supply as more and more women enter the labor force usually in the service sector.

Both costs and benefits are associated with economic restructuring. Greater efficiency, job creation, gentrification, and enhanced national competitiveness are associated with social exclusion and inclusion. The low-skilled, low-income population faces the loss of opportunities, full participation in society, lack of access in labor market and school, weak position in housing markets, limited political participation, and restricted social-cultural integration. Conversely, high-skilled, high-income professionals enjoy social inclusion with modern amenities, conveniences, social participation, and full access to public resources.

Furthermore, sociologist William Julius Wilson argues that the deindustrialization of manufacturing employment have exacerbated joblessness in impoverished African American communities correlating with a rise in single-mother households, high premature mortality rates, and increasing incarceration rates among African American males. With some African Americans gaining professional upward mobility through affirmative action and equal opportunity sanctions in education and employment, African Americans without such opportunities fall behind. This creates a growing economic class division among the African American demographic accentuated by global economic restructuring without government response to the disadvantaged. Furthermore, Wilson asserts that as the black middle class leave the predominantly black inner city neighborhoods, informal employment information networks are eroded. This isolates poor, inner city residents from the labor market compounding the concentration of poverty, welfare dependency, rise of unemployment, and physical isolation in these areas.

City youth are also affected such as in New York City. The declines in education, health care, and social services and the dearth of jobs for those with limited education and training along with the decay of public environments for outdoor play and recreation have all contributed to fewer autonomous outdoor play or "hanging out" places for young people. This in turn affects their gross motor development, cultural build-up, and identity construction. Children become prisoners of home relying on television and other outlets for companionship. Contemporary urban environments restricts the opportunities for children to forge and negotiate peer culture or acquire necessary social skills. Overall, their ecologies have eroded in recent years brought about by global restructuring.

When the 1973 oil crisis affected the world capitalist economy, economic restructuring was used to remedy the situation by geographically redistributing production, consumption, and residences.
City economies across the globe moved from goods-producing to service-producing outlets.
Breakthroughs in transportation and communications made industrial capital much more mobile. Soon, producer services emerged as a fourth basic economic sector where routine low-wage service employment moved to low-cost sites and advanced corporate services centralized in cities. These technological upheavals brought about changes in institutional arrangements with the prominence of large corporations, allied business and financial services, nonprofit and public sector enterprises. Global cities such as New York and London become centers for international finance and headquarters for multinational corporations offering cross currency exchange services as well as buildup of foreign banking and trading. Other cities become regional headquarters centers of low-wage manufacturing. In all these urban areas the corporate complex grows offering banking, insurance, advertising, legal council, and other service functions. Economic restructuring allows markets to expand in size and capacity from regional to national to international scopes.

Altogether, these institutional arrangements buttressed by improved technology reflect the interconnectedness and internationalization of firms and economic processes. Consequently, capital, goods, and people rapidly flow across borders. Where the mode of regulation began with Fordism and Taylorization in the industrial age then to mass consumption of Keynesian economics policies, it evolves to differentiated and specialized consumption through international competition. Additionally, in the labor market, nonstandard work arrangements develop in the form of part-time work, temporary agency and contract company employment, short-term employment, contingent work, and independent contracting. Global economic changes and technological improvements in communications and information systems encouraged competitive organizations to specialize in production easily and assemble temporary workers quickly for specific projects. Thus, the norm of standard, steady employment unravels beginning in the mid-1970s.

Another shift in institutional arrangement involves public resources. As economic restructuring encourages high-technology service and knowledge-based economies, massive public de-investment results. Across many parts of the U.S. and the industrialized Western nations, steep declines in public outlays occur in housing, schools, social welfare, education, job training, job creation, child care, recreation, and open space. To remedy these cutbacks, privatization is installed as a suitable measure. Though it leads to some improvements in service production, privatization leads to less public accountability and greater unevenness in the distribution of resources. With this reform in privatizing public services, neoliberalism has become the ideological platform of economic restructuring. Free market economic theory has dismantled Keynesian and collectivists’ strategies and promoted the Reagan and Thatcher politics of the 1980s. Soon free trade, flexible labor, and capital flight are used from Washington D.C. to London to Moscow. Moreover, economic restructuring requires decentralization as states hand down power to local governments. Where the federal government focuses on mainly warfare-welfare concerns, local governments focus on productivity. Urban policy reflects this market-oriented shift from once supporting government functions to now endorsing businesses.

Urban landscapes especially in the U.S. have significantly altered in response to economic restructuring. Cities such as Baltimore, Detroit, St. Louis and others face population losses which result in thousands of abandoned homes, unused buildings, and vacant lots, contributing to urban decay. Such transformations frustrate urban planning and revitalization, fostering deviance in the forms of drug-related activity and vagrancy. Older, compact, industrial U.S. cities have been rendered obsolete. Urban spaces become playgrounds for the urban gentry, wastelands for low-paid service workers, and denizens for the underground economy. In some areas, gentrification projects have caused displacement of poverty-stricken residents. Sunbelt cities such as Miami and Atlanta rise to become key business centers while Snowbelt cities such as Buffalo and Youngstown decline. Even housing markets respond to economic restructuring with decaying housing stocks, escalating housing prices, depleting tax base, changes in financing, and reduction in federal support for housing. Soon, spatial divisions among wealthy and poor households exacerbate. Moreover, with the movement of blue-collared employment from central cities, geographically entrenched housing discrimination, and suburban land use policy, African American youths in inner cities become victims of spatial mismatch, where their residences provide only weak and negative employment growth and they usually lack access to intrametropolitan mobility. High-order services, an expanding sector in the industrialized world, become spatially concentrated in a relative small number of large metropolitan areas, particularly in suburban office agglomerations.

In cultural terms, economic restructuring has been associated with postmodernity as its counterpart concerning flexible accumulation. Additionally, the term carries with it three core themes: historical, radical rupture into post-industrial economic order; priority of economic forces over social/political forces; and structure over agency where the process is independent of human will, as it takes place according to economic logic (Logan & Swanstrom 1990). In addition, economic restructuring demonstrates the increasing complex and human-capital intensive modern society in Western nations.



</doc>
<doc id="8081066" url="https://en.wikipedia.org/wiki?curid=8081066" title="Fundamental plane (spherical coordinates)">
Fundamental plane (spherical coordinates)

The fundamental plane in a spherical coordinate system is a plane of reference that divides the sphere into two hemispheres. The geocentric latitude of a point is then the angle between the fundamental plane and the line joining the point to the centre of the sphere.

For a geographic coordinate system of the Earth, the fundamental plane is the Equator. Celestial coordinate systems have varying fundamental planes:



</doc>
<doc id="27288010" url="https://en.wikipedia.org/wiki?curid=27288010" title="Boundary problem (spatial analysis)">
Boundary problem (spatial analysis)

A boundary problem in analysis is a phenomenon in which geographical patterns are differentiated by the shape and arrangement of boundaries that are drawn for administrative or measurement purposes. The boundary problem occurs because of the loss of neighbors in analyses that depend on the values of the neighbors. While geographic phenomena are measured and analyzed within a specific unit, identical spatial data can appear either dispersed or clustered depending on the boundary placed around the data. In analysis with point data, dispersion is evaluated as dependent of the boundary. In analysis with areal data, statistics should be interpreted based upon the boundary.

In spatial analysis, four major problems interfere with an accurate estimation of the statistical parameter: the boundary problem, scale problem, pattern problem (or spatial autocorrelation), and modifiable areal unit problem. The boundary problem occurs because of the loss of neighbours in analyses that depend on the values of the neighbours. While geographic phenomena are measured and analyzed within a specific unit, identical spatial data can appear either dispersed or clustered depending on the boundary placed around the data. In analysis with point data, dispersion is evaluated as dependent of the boundary. In analysis with area data, statistics should be interpreted based upon the boundary.

In geographical research, two types of areas are taken into consideration in relation to the boundary: an area surrounded by fixed natural boundaries (e.g., coastlines or streams), outside of which neighbours do not exist, or an area included in a larger region defined by arbitrary artificial boundaries (e.g., an air pollution boundary in modeling studies or an urban boundary in population migration). In an area isolated by the natural boundaries, the spatial process discontinues at the boundaries. In contrast, if a study area is delineated by the artificial boundaries, the process continues beyond the area.

If a spatial process in an area occurs beyond a study area or has an interaction with neighbours outside artificial boundaries, the most common approach is to neglect the influence of the boundaries and assume that the process occurs at the internal area. However, such an approach leads to a significant model misspecification problem.

That is, for measurement or administrative purposes, geographic boundaries are drawn, but the boundaries per se can bring about different spatial patterns in geographic phenomena. It has been reported that the difference in the way of drawing the boundary significantly affects identification of the spatial distribution and estimation of the statistical parameters of the spatial process. The difference is largely based on the fact that spatial processes are generally unbounded or fuzzy-bounded, but the processes are expressed in data imposed within boundaries for analysis purposes. Although the boundary problem was discussed in relation to artificial and arbitrary boundaries, the effect of the boundaries also occurs according to natural boundaries as long as it is ignored that properties at sites on the natural boundary such as streams are likely to differ from those at sites within the boundary.

The boundary problem occurs with regard not only to horizontal boundaries but also to vertically drawn boundaries according to delineations of heights or depths (Pineda 1993). For example, biodiversity such as the density of species of plants and animals is high near the surface, so if the identically divided height or depth is used as a spatial unit, it is more likely to find fewer number of the plant and animal species as the height or depth increases.

By drawing a boundary around a study area, two types of problems in measurement and analysis takes place. The first is an "edge effect". This effect originates from the ignorance of interdependences that occur outside the bounded region. Griffith and Griffith and Amrhein highlighted problems according to the edge effect. A typical example is a cross-boundary influence such as cross-border jobs, services and other resources located in a neighbouring municipality.

The second is a "shape effect" that results from the artificial shape delineated by the boundary. As an illustration of the effect of the artificial shape, point pattern analysis tends to provide higher levels of clustering for the identical point pattern within a unit that is more elongated. Similarly, the shape can influence interaction and flow among spatial entities. For example, the shape can affect the measurement of origin-destination flows since these are often recorded when they cross an artificial boundary. Because of the effect set by the boundary, the shape and area information is used to estimate travel distances from surveys, or to locate traffic counters, travel survey stations, or traffic monitoring systems. From the same perspective, Theobald (2001; retrieved from) argued that measures of urban sprawl should consider interdependences and interactions with nearby rural areas.

In spatial analysis, the boundary problem has been discussed along with the modifiable areal unit problem (MAUP) inasmuch as MAUP is associated with the arbitrary geographic unit and the unit is defined by the boundary. For administrative purposes, data for policy indicators are usually aggregated within larger units (or enumeration units) such as census tracts, school districts, municipalities and counties. The artificial units serve the purposes of taxation and service provision. For example, municipalities can effectively respond to the need of the public in their jurisdictions. However, in such spatially aggregated units, spatial variations of detailed social variables cannot be identified. The problem is noted when the average degree of a variable and its unequal distribution over space are measured.

Several strategies for resolving geographic boundary problems in measurement and analysis have been proposed. To identify the effectiveness of the strategies, Griffith reviewed traditional techniques that were developed to mitigate the edge effects: ignoring the effects, undertaking a torus mapping, construction of an empirical butter zone, construction of an artificial butter zone, extrapolation into a buffer zone, utilizing a correction factor, etc. The first method (i.e., the ignorance of the edge effects), assumes and infinite surface in which the edge effects do not occur. In fact, this approach has been used by traditional geographical theories (e.g., central place theory). Its main shortcoming is that empirical phenomena occur within a finite area, so an infinite and homogeneous surface is unrealistic. The remaining five approaches are similar in that they attempted to produce unbiased parameter estimation, that is, to provide a medium by which the edge effects are removed. (He called these "operational solutions" as opposed to "statistical solutions" to be discussed below.) Specifically, the techniques aim at a collection of data beyond the boundary of the study area and fit a larger model, that is, mapping over the area or over-bounding the study area. Through simulation analysis, however, Griffith and Amrhein identified the inadequacy of such an overbounding technique. Moreover, this technique can bring about issues related to large-area statistics, that is, ecological fallacy. By expanding the boundary of the study area, micro-scale variations within the boundary can be ignored.

As alternatives to operational solutions, Griffith examined three correction techniques (i.e., "statistical solutions") in removing boundary-induced bias from inference. They are (1) based on generalized least squares theory, (2) using dummy variables and a regression structure (as a way of creating a buffer zone), and (3) regarding the boundary problem as a missing values problem. However, these techniques require rather strict assumptions about the process of interest. For example, the solution according to the generalized least squares theory utilizes time-series modeling that needs an arbitrary transformation matrix to fit the multidirectional dependencies and multiple boundary units found in geographical data. Martin also argued that some of the underlying assumptions of the statistical techniques are unrealistic or unreasonably strict. Moreover, Griffith (1985) himself also identified the inferiority of the techniques through simulation analysis.

As particularly applicable using GIS technologies, a possible solution for addressing both edge and shape effects is to an re-estimation of the spatial or process under repeated random realizations of the boundary. This solution provides an experimental distribution that can be subjected to statistical tests. As such, this strategy examines the sensitivity in the estimation result according to changes in the boundary assumptions. With GIS tools, boundaries can be systematically manipulated. The tools then conduct the measurement and analysis of the spatial process in such differentiated boundaries. Accordingly, such a sensitivity analysis allows the evaluation of the reliability and robustness of place-based measures that defined within artificial boundaries. In the meantime, the changes in the boundary assumptions refer not only to altering or tilting the angles of the boundary, but also differentiating between the boundary and interior areas in examination and considering a possibility that isolated data collection points close to the boundary may show large variances.



</doc>
<doc id="25313082" url="https://en.wikipedia.org/wiki?curid=25313082" title="Pan-region">
Pan-region

A pan-region is a geographic region or state’s sphere of economic, political and cultural influence extending beyond that state's borders. For example, the pan-region of the United States of America (USA) regions both bordering the USA and its close neighbours including, Canada, Mexico, and many South America other states.

The idea of pan-regions or spheres of economic and cultural influence was first developed by Karl Ernst Haushofer (8/27/1869-3/10/1946), a German General, geographer and geo-politician. Pan-regions contributed to Geopolitic or the German theories of foreign policy during the interwar period (1918–1939) or the time from the end of World War I and the beginning of World War II. Haushofer’s pan-regions divided the world under three supreme leading states in economy, politics and culture. Those three states included the USA who controlled North America and much of South America, Germany who controlled Europe, much of Africa and western Asia and Japan who controlled central, eastern, and the islands of southern Asia. These leading states could expect their regions to develop economic and political alliance with their leading state as well as yield to sanctions and major cultural designations.

Historically, the world was divided into three spheres of control, however after the end of World War II, Germany and Japan’s control over their various regions have diminished with the success of other nations. For example, German control over Europe has suffered with the development of the European Union and emergence of other foreign powers. Japan also is beginning to lose economic dominance over its pan-region with the emergence of a thriving Chinese economy.


</doc>
<doc id="28016564" url="https://en.wikipedia.org/wiki?curid=28016564" title="Two-step floating catchment area method">
Two-step floating catchment area method

The two-step floating catchment area (2SFCA) method is a method for combining a number of related types of information into a single, immediately meaningful, index that allows comparisons to be made across different locations. Its importance lies in the improvement over considering the individual sources of information separately, where none on its own provides an adequate summary.

The two-step floating catchment area (2SFCA) method is a special case of a gravity model of spatial interaction that was developed to measure spatial accessibility to primary care physicians. 2SFCA can also be used to measure other accessibility such as accessibility to jobs, to cancer care facilities, etc. It was inspired by the spatial decomposition idea first proposed by Radke and Mu (2000).

The 2SFCA method not only has most of the advantages of a gravity model, but is also intuitive to interpret, as it uses essentially a special form of physician-to-population ratio. It is easy to implement in a GIS environment. In essence, the 2SFCA method measures spatial accessibility as a ratio of primary-care physicians to population, combining two steps:

It has been recently enhanced by considering distance decay within catchments and called the enhanced two-step floating catchment area (E2SFCA) method.

Furthermore, the use of capping certain services according to nearby population size, can improve the accuracy when analyzing across areas of different environments (i.e. rural and urban).

The method has been applied to other related public health issues, such as access to healthy food retailers.




</doc>
<doc id="27891886" url="https://en.wikipedia.org/wiki?curid=27891886" title="Solar equator">
Solar equator

The solar equator is the latitude on Earth at which the Sun is observed directly overhead at midday. Due to the obliquity of Earth's axis, the solar equator varies during the year, from the Tropic of Capricorn on the December solstice to the Tropic of Cancer on the June solstice. On the day of either equinox, the Sun's position is at the zenith when viewed from the geographic equator. The Sun can never be observed directly overhead from outside of the tropics. The solar equator is coplanar to the ecliptic.



</doc>
<doc id="10943769" url="https://en.wikipedia.org/wiki?curid=10943769" title="Place identity">
Place identity

Place identity or place-based identity refers to a cluster of ideas about place and identity in the fields of geography, urban planning, urban design, landscape architecture, environmental psychology, ecocriticism and urban sociology/ecological sociology. Place identity is sometimes called urban character, neighbourhood character or local character. Place identity has become a significant issue in the last 25 years in urban planning and design. Place identity concerns the meaning and significance of places for their inhabitants and users, and how these meanings contribute to individuals' conceptualizations of self. Place identity also relates to the context of modernity, history and the politics of representation. In other words, historical determinism, which intersects historical events, social spaces and groups by gender, class, ethnicity. In this way, it explores how spaces have evolved over time by exploring the social constructs through time and the development of space, place and power. To the same extent, the politics of representation is brought into context, as the making of place identity in a community also relates to the exclusion or inclusion in a community. Through this, some have argued that place identity has become an area for social change because it gives marginalized communities agency over their own spaces. In the same respect, it is argued that place identity has also been used to intervene social change and perpetuate oppression from a top-down approach by creating segregated spaces for marginalized communities.

In some ways it is related to the concepts of place attachment and sense of place. Place identity is largely related to the concepts of community formation because it recognizes that geographical spaces do not solely bond a community together but rather there are social bonds that account for community formation. Those social forces often are feelings of belonging and security, which involve theoretical formations of community. Theoretical formations of community, which were identified in "Community: Seeking Safety in an Insecure World" (Bauman, 2001) as bonds formed by similar locality, culture, language, kinship and/or experiences. In addition, identity also conceives feelings of security and freedom as one is able to self-identify and especially, when it comes to being able to foster agency over community formation. In addition, the similar and shared experiences of culture, language and locality foster the sense of community. This fostering of community is largely seen as an extension of agency because when a community is able to achieve a sense of place and place attachment, this allows for individuals to reinforce their own identities and strengthen their bonds within their community.

Methodologies for understanding place identity primarily involve qualitative techniques, such as interviewing, participant observation, discourse analysis and mapping a range of physical elements. Some urban planners, urban designers and landscape architects use forms of deliberative planning, design charettes and participatory design with local communities as a way of working with place identity to transform existing places as well as create new ones. This kind of planning and design process is sometimes referred to as placemaking.

The following case studies are examples of how place identity is researched on the field.

In a study by Lee Cuba and David M. Hummon (1993), they focus on Cape Cod, Massachusetts residents and how social and environmental factors are associated with place identity. Place identity in regards to "at-homeness" was defined by existence, affiliations, and locus. Community members were asked if they feel at home in Cape Cod to measure the positive responses for existence. The open-ended responses to why community members feel at home were used to measure place affiliation. A close-ended question, "Do you associate feeling at home with living in this particular house or apartment, with living in this community, or with living on the Cape, in general?" was used to measure locus. Most respondents reported they did feel "at home".

Michigan and the Great Lakes are analyzed to see the values and connections shared within the residents of Michigan. A questionnaire was given to Michigan residents to see how attached the residents are. The questionnaire consisted of statements and the statements were evaluated through the five-point Likert scale. As a result, the data revealed "Michigan's voters have developed a strong sense of place regarding the state".

These two case studies shows that place has a lot more to offer than just a physical location. Understanding how to measure a sense of place assist policy makers in decision making and creating potential policy implementation. They will take the community's issues into consideration during the planning process once they understand the values of a community.

Cuba, L. & Hummon, D.M. (1993). A place to call home: Identification with dwelling, community, and region. The Sociological Quarterly, 34 (1), 111-131.

Hague, C. and Jenkins, P. (Eds) (2005). Place identity, planning and participation, London ; New York : Routledge, 2005. (hard cover) 0415262429 (soft cover) 0203646754 (ebook)

Proshansky, H. M. (1978). 'The city and self-identity', Journal of Environment and Behaviour, Vol. 10, pp. 57–83

Nanzer, B. (2004). Measuring sense of place: A scale of Michigan. Administrative Theory & Praxis, 26 (3), 362-382.

Proshansky, H. M., Fabian, A. K. and Kaminoff, R. (1983). 'Place-identity: Physical world socialization of the self', Journal of Environmental Psychology, Vol. 3, pp. 57–83

Relph, E (1976) Place and placelessness. London: Pion, 1976 ()

Roudavski, Stanislav (2008). "Staging Places as Performances: Creative Strategies for Architecture" (PhD, University of Cambridge)


</doc>
<doc id="23797849" url="https://en.wikipedia.org/wiki?curid=23797849" title="Laminar sublayer">
Laminar sublayer

The laminar sublayer, also called the viscous sublayer, is the region of a mainly-turbulent flow that is near a no-slip boundary and in which the flow is laminar. As such, it is a type of boundary layer. The existence of the laminar sublayer can be understood in that the flow velocity decreases towards the no-slip boundary. Because of this, the Reynolds number decreases until at some point the flow crosses the threshold from turbulent to laminar.

The laminar sublayer is important for river-bed ecology: below the laminar-turbulent interface, the flow is stratified, but above it, it rapidly becomes well-mixed. This threshold can be important in providing homes and feeding grounds for benthic organisms.

Whether the roughness due to the bed sediment or other factors are smaller or larger than this sublayer has an important bearing in hydraulics and sediment transport. Flow is defined as hydraulically rough if the roughness elements are larger than the laminar sublayer (thereby perturbing the flow), and as hydraulically smooth if they are smaller than the laminar sublayer (and therefore ignorable by the main body of the flow).


</doc>
<doc id="5949047" url="https://en.wikipedia.org/wiki?curid=5949047" title="Hjulström curve">
Hjulström curve

The Hjulström curve, named after Filip Hjulström (1902–1982), is a graph used by hydrologists and geologists to determine whether a river will erode, transport, or deposit sediment. It was originally published in his doctoral thesis "Studies of the morphological activity of rivers as illustrated by the River Fyris." in 1935. The graph takes sediment particle size and water velocity into account. 

The upper curve shows the critical erosion velocity in cm/s as a function of particle size in mm, while the lower curve shows the deposition velocity as a function of particle size. Note that the axes are logarithmic. 

The plot shows several key concepts about the relationships between erosion, transportation, and deposition. For particle sizes where friction is the dominating force preventing erosion, the curves follow each other closely and the required velocity increases with particle size. However, for cohesive sediment, mostly clay but also silt, the "erosion" velocity increases with decreasing grain size, as the cohesive forces are relatively more important when the particles get smaller. The critical velocity for deposition, on the other hand, depends on the settling velocity, and that decreases with decreasing grainsize. The Hjulström curve shows that sand particles of a size around 0.1 mm require the lowest stream velocity to erode.

The curve was expanded by Åke Sundborg in 1956. He significantly improved the level of detail in the cohesive part of the diagram, and added lines for different modes of transportation. The result is called the "Sundborg diagram", or the "Hjulström-Sundborg Diagram", in the academic literature.

This curve dates back to early 20th century research on river geomorphology and has no more than a historical value nowadays, although its simplicity is still attractive. Among the drawbacks of this curve are that it does not take the water depth into account and more importantly, that it does not show that sedimentation is caused by flow velocity "deceleration" and erosion is caused by flow "acceleration". The dimensionless Shields Diagram is now unanimously accepted for initiation of sediment motion in rivers. Much work was done on river sediment transport formulae in the second half of the 20th century and that work should be used preferably to Hjulström's curve.



</doc>
<doc id="36248807" url="https://en.wikipedia.org/wiki?curid=36248807" title="Geo-literacy">
Geo-literacy

As defined by National Geographic, geo-literacy is "the ability to use geographic understanding and geographic reasoning to make decisions".

The term "geo-literacy" arose from the National Geographic Society's "Fight against Geographic Illiteracy." The organization released various media to help explain the concept to the general public. In an editorial, Daniel C. Edelson, vice president for education at National Geographic, said, "The National Geographic Society's concern for geo-literacy comes from our mission. We see geo-literacy as providing the tools that will enable communities to protect natural and cultural resources, reduce violent conflict, and improve the quality of life worldwide. However, having a geo-literate populace is also critical for maintaining economic competitiveness, quality of life, and national security in our modern, interconnected world.", and have released various media to help explain it to the general public. In addition, the National Geographic Society set up the Fund for Geo-literacy, in which donations help fund the printing of materials for education, professional development for the educators, and programs to help build awareness of the importance of geo-literacy.

According to Edelson, the 3 components of geo-literacy are:

"Kid World Citizen", a site which provides "multicultural, educational activities to teach...kids about the world", and who, listed the following "age-appropriate lessons to increase geo-literacy in primary school students":


In 2012, InTeGrate ("a community effort to improve geoscience literacy and build a workforce that can make use of geoscience to solve societal issues") held a Module Author Meeting from May 16–18 on the topic.

In 2002, Robert E. Nolan of the Education Resources Information Center published a research report/journal article entitled "Geo-Literacy: How Well Adults Understand the World in Which They Live", which included "a test of physical and geopolitical geography...completed by 321 adults". The years of formal education and age were correlated with geographic literacy, and informal learning, such as travel, reading, media, was used as the primary source of geographic knowledge for those with higher educational attainment. A notable finding was that women, regardless of education level, scored significantly lower than men."

In 2001, the Arizona Geographic Alliance launched a project called "The GeoLiteracy Project" that integrated geography education into reading and writing instruction. In this case, their term "GeoLiteracy" referred to the integration of geography and traditional language arts literacy. 

In 1997, Linda Ferguson and Eva LaMar established an educational project they called "The Geo-Literacy Project". LaMar describes geo-literacy as "the use of visual learning and communication tools to build an in-depth understanding -- or literacy -- of geography, geology, and local history." 



</doc>
<doc id="36908659" url="https://en.wikipedia.org/wiki?curid=36908659" title="Mountain research">
Mountain research

Mountain research or "montology", traditionally also known as "orology" (from Greek "oros" ὄρος for 'mountain' and "logos" λόγος), is a field of research that regionally concentrates on the Earth's surface's part covered by mountain landscapes.

Different approaches have been developed to define "mountainous" areas. While some use an altitudinal difference of 300 m inside an area to define that zone as mountainous, others consider differences from 1000 m or more, depending on the areas' latitude. Additionally, some include steepness to define mountain regions, hence excluding high plateaus (e.g. the Andean Altiplano or the Tibetan Plateau), zones often seen to be mountainous. A more pragmatic but useful definition has been proposed by the Italian Statistics Office ISTAT, which classifies municipalities as mountainous


The United Nations Environmental Programme has produced a map of mountain areas worldwide using a combination of criteria, including regions with


In a broader sense, mountain research is considered any research "in" mountain regions: for instance disciplinary studies on Himalayan plants, Andean rocks, Alpine cities, or Carpathian people. It is comparable to research that concentrates on the Arctic and Antarctic (polar research) or coasts (coastal research).

In a narrower sense, mountain research focuses "on" mountain regions, their description and the explanation of the human-environment interaction in (positive) and the sustainable development of (normative) these areas. So-defined mountain research is situated at the nexus of natural sciences, social sciences and humanities. Drawing on Alexander von Humboldt's work in the Andean realm, mountain geography and ecology are considered core areas of study; nevertheless important contributions are coming from anthropology, geology, economics, history or spatial planning. In sum, a narrowly defined mountain research applies an interdisciplinary and integrative regional approach. Slaymaker summarizes:

Mountain research or "orology"—not to be confused with orography—, is sometimes denominated "montology". This term stems from Carl Troll's "mountain geoecology"—geoecology being Troll's English translation of the German "Landschaftsökologie"—and appeared at a meeting in Cambridge, Massachusetts in 1977. Since then, scholars such as Jack D. Ives, Bruno Messerli and Robert E. Rhoades have claimed the development of montology as interdisciplinary mountain research. The term montology was included in the Oxford English Dictionary in 2002. It defines montology as:

On the one hand, the term "montology" received criticism due to the mix of Latin ("mōns", pl. "montēs") and Greek ("logos"). On the other hand, however, this is also the—well accepted—case in several, already established disciplines such as glaciology or sociology.

The following list includes peer-reviewed journals that have a focus on mountain research and are open to both the natural and the social sciences:





</doc>
<doc id="38262946" url="https://en.wikipedia.org/wiki?curid=38262946" title="Land systems">
Land systems

Land systems constitute the terrestrial component of the Earth system and encompass all processes and activities related to the human use of land, including socioeconomic, technological and organizational investments and arrangements, as well as the benefits gained from land and the unintended social and ecological outcomes of societal activities. Changes in land systems have large consequences for the local environment and human well-being and are at the same time pervasive factors of global environmental change. Land provides vital resources to society, such as food, fuel, fibres and many other ecosystem services that support production functions, regulate risks of natural hazards, or provide cultural and spiritual services. By using the land, society alters and modifies the quantity and quality of the provision of these services.

Land system changes are the direct result of human decision making at multiple scales ranging from local land owners decisions to national scale land use planning and global trade agreements. The aggregate impact of many local land system changes has far reaching consequences for the Earth System, that feedback on ecosystem services, human well-being and decision making. As a consequence, land system change is both a cause and consequence of socio-ecological processes.

The Global Land Programme (GLP) of Future Earth is an interdisciplinary community of science and practice fostering the study of land systems and the co-design of solutions for global sustainability.


</doc>
<doc id="9177450" url="https://en.wikipedia.org/wiki?curid=9177450" title="Easting and northing">
Easting and northing

Easting and northing are geographic Cartesian coordinates for a point. Easting is the eastward-measured distance (or the "x"-coordinate) and northing is the northward-measured distance (or the "y"-coordinate). When using common projections such as the transverse Mercator projection, these are distances projected on an imaginary surface similar to a bent sheet of paper, and are not the same as distances measured on the curved surface of the Earth.

Easting and northing coordinates are commonly measured in metres from the axes of some horizontal datum. However, other units (e.g., survey feet) are also used. The coordinates are most commonly associated with the Universal Transverse Mercator coordinate system (UTM), which has unique zones that cover the Earth to provide detailed referencing.

Locations can be found using easting/northing (or " x", "y") pairs. The pair is usually represented conventionally with easting first, northing second.

For example, the peak of Mount Assiniboine (at ) in UTM Zone 11 is represented by codice_1. Other conventions can also be used, such as a truncated grid reference, which would shorten the example coordinates to codice_2.

Negative northing and easting values indicate a position due south and west of the origin, respectively.

Usually associated with a map projection is a "natural origin", e.g., at which the ellipsoid and flat map surfaces coincide. To ensure that the northing and easting coordinates on a map are not negative, map projections may set up a "false origin", specified in terms of "false northing" and "false easting" values, that offset the true origin.



</doc>
<doc id="711728" url="https://en.wikipedia.org/wiki?curid=711728" title="Governmentality">
Governmentality

Governmentality is a concept first developed by the French philosopher Michel Foucault in the later years of his life, roughly between 1977 and his death in 1984, particularly in his lectures at the Collège de France during this time.

The concept has been elaborated further from an "Anglo-Neo Foucauldian" perspective in the social sciences, especially by authors such as Peter Miller, Nikolas Rose, and Mitchell Dean. Governmentality can be understood as:


Governmentality may also be understood as:

This term was thought by some commentators to be made by the "...linking of governing ("gouverner") and modes of thought ("mentalité")". In fact, it was not coined by uniting words "gouvernement" and "mentalité", but simply by making "gouvernement" into "gouvernementalité" just like "musical" into "musicalité" [i.e. government + -al- "adjective" + -ité "abstract noun"] (see Michel Senellart's "Course Context" in Foucault's "Security, territory, population" lectures). To fully understand this concept, it is important to realize that in this case, Foucault does not only use the standard, strictly political definition of "governing" or government used today, but he also uses the broader definition of governing or government that was employed until the eighteenth century. That is to say, that in this case, for Foucault, "...'government' also signified problems of self-control, guidance for the family and for children, management of the household, directing the soul, etc." In other words, for our purposes, government is "...the conduct of conduct..."

In his lectures at the Collège de France, Foucault often defines governmentality as the "art of government" in a wide sense, i.e. with an idea of "government" that is not limited to state politics alone, that includes a wide range of control techniques, and that applies to a wide variety of objects, from one's control of the self to the "biopolitical" control of populations. In the work of Foucault, this notion is indeed linked to other concepts such as biopolitics and power-knowledge. The genealogical exploration of the modern state as "problem of government" does not only deepen Foucault's analyses on sovereignty and biopolitics; it offers an analytics of government which refines both Foucault's theory of power and his understanding of freedom.

The concept of "governmentality" develops a new understanding of power. Foucault encourages us to think of power not only in terms of hierarchical, top-down power of the state. He widens our understanding of power to also include the forms of social control in disciplinary institutions (schools, hospitals, psychiatric institutions, etc.), as well as the forms of knowledge. Power can manifest itself positively by producing knowledge and certain discourses that get internalised by individuals and guide the behaviour of populations. This leads to more efficient forms of social control, as knowledge enables individuals to govern themselves.

"Governmentality" applies to a variety of historical periods and to different specific power regimes. However, it is often used (by other scholars and by Foucault himself) in reference to "neoliberal governmentality", i.e. to a type of governmentality that characterizes advanced liberal democracies. In this case, the notion of governmentality refers to societies where power is de-centered and its members play an active role in their own self-government, e.g. as posited in neoliberalism. Because of its active role, individuals need to be regulated from 'inside'. A particular form of governmentality is characterized by a certain form of knowledge ("savoir" in French). In the case of neoliberal governmentality (a kind of governmentality based on the predominance of market mechanisms and of the restriction of the action of the state) the knowledge produced allows the construction of auto-regulated or auto-correcting selves.

In his lecture titled Governmentality, Foucault gives us a definition of governmentality:
As Foucault's explicit definition is rather broad, perhaps further examination of this definition would be useful.

We shall begin with a closer inspection of the first part of Foucault's definition of governmentality:

This strand of the three-part definition states that governmentality is, in other words, all of the components that make up a government that has as its end the maintenance of a well-ordered and happy society (population). The government's means to this end is its "apparatuses of security," that is to say, the techniques it uses to provide this society a feeling of economic, political, and cultural well-being. The government achieves these ends by enacting "political economy," and in this case, the meaning of economy is the older definition of the term, that is to say, "economy at the level of the entire state, which means exercising towards its inhabitants, and the wealth and behavior of each and all, a form of surveillance and control as attentive as that of the head of a family over his household and his goods". Thus, we see that this first part of the definition states that governmentality is a government with specific ends, means to these ends, and particular practices that should lead to these ends.

The second part of Foucault's definition (the "resulting, on the one hand, in formation of a whole series of specific governmental apparatuses, and, on the other, in the development of a whole complex of savoirs") presents governmentality as the long, slow development of Western governments which eventually took over from forms of governance like sovereignty and discipline into what it is today: bureaucracies and the typical methods by which they operate.

The next and last part of Foucault's definition of governmentality can be restated as the evolution from the Medieval state, which maintained its territory and an ordered society within this territory through a blunt practice of simply imposing its laws upon its subjects, to the early Renaissance state, which became more concerned with the "disposing of things", and so began to employ strategies and tactics to maintain a content and thus stable society, or in other words to "render a society governable".

Thus, if one takes these three definitions together, governmentality may be defined as the process through which a form of government with specific ends (a happy and stable society), means to these ends ("apparatuses of security"), and with a particular type of knowledge ("political economy"), to achieve these ends, evolved from a medieval state of justice to a modern administrative state with complex bureaucracies.

The concept of governmentality segues from Foucault's ethical, political and historical thoughts from the late 1970s to the early 1980s. His most widely known formulation of this notion is his lecture entitled "Security, territory and population" (1978). A deeper and richer reflection on the notion of governmentality is provided in Foucault's course on "The Birth of Biopolitics" at the Collège de France in 1978–1979. The course was first published in French in 2004 as "Naissance de la biopolitique: Cours au Collège de France (1978-1979)" (Paris: Gallimard & Seuil). This notion is also part of a wider analysis on the topic of disciplinary institutions, on neoliberalism and the "Rule of Law", the "microphysics of power" and also on what Foucault called biopolitics. In the second and third volumes of "The History of Sexuality", namely, "The Use of Pleasure" (1984) and "The Care of the Self" (1984), and in his lecture on "Technologies of the Self" (1982), Foucault elaborated a distinction between subjectivation and forms of subjectification by exploring how selves were fashioned and then lived in ways which were both heteronomously and autonomously determined. Also, in a series of lectures and articles, including "The Birth of Biopolitics" (1979), ""Omnes et Singulatim": Towards a Criticism of Political Reason" (1979), "The Subject and Power" (1982) and "What is Enlightenment?" (1984), he posed questions about the nature of contemporary social orders, the conceptualization of power, human freedom and the limits, possibilities and sources of human actions, etc. that were linked to his understanding of the notion of "governmentality".

The notion of governmentality (not to confuse with governance) gained attention in the English-speaking academic world mainly through the edited book "The Foucault Effect" (1991), which contained a series of essays on the notion of governmentality, together with a translation of Foucault's 1978 short text on "gouvernementalité".

Hunt and Wickham, in their work "Foucault and Law" [1994] begin the section on governmentality with a very basic definition derived from Foucault's work. They state, "governmentality is the dramatic expansion in the scope of government, featuring an increase in the number and size of the governmental calculation mechanisms" [1994:76]. In other words, governmentality describes the new form of governing that arose in the mid-eighteenth century that was closely allied with the creation and growth of the modern bureaucracies. In giving this definition, Hunt and Wickham conceive of the term as consisting of two parts 'governmental' and '–ity' - governmental meaning pertaining to the government of a country; and the suffix –ity meaning the study of. They acknowledge that this definition lacks some of Foucault's finer nuances and try to redress this by explaining some more of Foucault's ideas, including reason of state, the problem of population, modern political economy, liberal securitisation, and the emergence of the human sciences" [1994:77].

Kerr's approach to the term is more complex. He conceives of the term as an abbreviation of "governmental rationality" [1999:174]. In other words, it is a way of thinking about the government and the practices of the government. To him it is not "a zone of critical-revolutionary study, but one that conceptually reproduces capitalist rule" [1999:197] by asserting that some form of government (and power) will always be necessary to control and constitute society. By defining governmentality only in terms of the state, Kerr fails to take account of other forms of governance and the idea of mentalities of government in this broader sense.

Dean's understanding of the term incorporates both other forms of governance and the idea of mentalities of government, as well as Hunt and Wickham's, and Kerr's approaches to the term. In line with Hunt and Wickham's approach, Dean acknowledges that in a very narrow sense, governmentality can be used to describe the emergence of a government that saw that the object of governing power was to optimise, use and foster living individuals as members of a population [1999:19]. He also includes the idea of government rationalities, seeing governmentality as one way of looking at the practices of government. In addition to the above, he sees government as anything to do with conducting oneself or others. This is evident in his description of the word in his glossary: "Governmentality: How we think about governing others and ourselves in a wide variety of contexts..." [1999:212]. This reflects that the term government to Foucault meant not so much the political or administrative structures of the modern state as the way in which the conduct of individuals or of groups may be directed. To analyse government is to analyse those mechanisms that try to shape, sculpt, mobilise and work through the choices, desires, aspirations, needs, wants and lifestyles of individuals and groups [Dean, 1999:12].

Dean's main contribution to the definition of the term, however, comes from the way he breaks the term up into 'govern' 'mentality', or mentalities of governing—mentality being a mental disposition or outlook. This means that the concept of governmentality is not just a tool for thinking about government and governing but also incorporates how and what people who are governed think about the way they are governed. He defines thinking as a "collective activity" [1999:16], that is, the sum of the knowledge, beliefs and opinions held by those who are governed. He also raises the point that a mentality is not usually "examined by those who inhabit it" [1999:16]. This raises the interesting point that those who are governed may not understand the unnaturalness of both the way they live and the fact that they take this way of life for granted—that the same activity in which they engage in "can be regarded as a different form of practice depending on the mentalities that invest it" [1999:17]. Dean highlights another important feature of the concept of governmentality—its reflexivity. He explains:
"On the one hand, we govern others and ourselves according to what we take to be true about who we are, what aspects of our existence should be worked upon, how, with what means, and to what ends. On the other hand, the ways in which we govern and conduct ourselves give rise to different ways of producing truth. [1999:18]

By drawing attention to the 'how and why', Dean connects "technologies of power" [Lemke, 2001:191] to the concept of governmentality. According to Dean any definition of governmentality should incorporate all of Foucault's intended ideas. A complete definition of the term governmentality must include not only government in terms of the state, but government in terms of any "conduct of conduct" [Dean, 1999:10]. It must incorporate the idea of mentalities and the associations that go with that concept: that it is an attitude towards something, and that it is not usually understood "from within its own perspective" [1999:16], and that these mentalities are collective and part of a society's culture. It must also include an understanding of the ways in which conduct is governed, not just by governments, but also by ourselves and others.

The semantic linking of governing and mentalities in governmentality indicates that it is not possible to study technologies of power without an analysis of the mentality of rule underpinning them. The practice of going to the gym, expounded below, is a useful example because it shows how our choices, desires, aspirations, needs, wants and lifestyles have been mobilised and shaped by various technologies of power.

A mentality of rule is any relatively systematic way of thinking about government. It delineates a discursive field in which the exercise of power is 'rationalised' [Lemke, 2001:191]. Thus Neo-liberalism is a mentality of rule because it represents a method of rationalising the exercise of government, a rationalisation that obeys the internal rule of maximum economy [Foucault, 1997:74]. Fukuyama [in Rose, 1999: 63] writes "a liberal State is ultimately a limited State, with governmental activity strictly bounded by the sphere of individual liberty". However, only a certain type of liberty, a certain way of understanding and exercising freedom is compatible with Neo-liberalism. If Neo-liberalist government is to fully realize its goals, individuals must come to recognize and act upon themselves as both free and responsible [Rose, 1999:68]. Thus Neo-liberalism must work to create the social reality that it proposes already exists. For as Lemke states, a mentality of government "is not pure, neutral knowledge that simply re-presents the governing reality" [Lemke, 2001:191] instead, Neo-liberalism constitutes an attempt to link a reduction in state welfare services and security systems to the increasing call for subjects to become free, enterprising, autonomous individuals. It can then begin to govern its subjects, not through intrusive state bureaucracies backed with legal powers, the imposition of moral standards under a religious mandate, but through structuring the possible field of action in which they govern themselves, to govern them through their freedom. Through the transformation of subjects with duties and obligations, into individuals, with rights and freedoms, modern individuals are not merely 'free to choose' but obliged to be free, "to understand and enact their lives in terms of choice" [Rose, 1999:87]. This freedom is a different freedom to that offered in the past. It is a freedom to realize our potential and our dreams through reshaping the way in which we conduct our lives.

Cartographic mapping has historically been a key strategy of governmentality. Harley, drawing on Foucault, affirms that State-produced maps "extend and reinforce the legal statutes, territorial imperatives, and values stemming from the exercise of political power". Typically, State-led mapping conforms to Bentham's concept of a panopticon, in which 'the one views the many'. From a Foucauldian vantage point, this was the blueprint for disciplinary power.

Through processes of neoliberalism, the State has "hollowed out" some of its cartographic responsibilities and delegated power to individuals who are at a lower geographical scale. 'People's cartography' is believed to deliver a more democratic spatial governance than traditional top-down State-distribution of cartographic knowledge. Thus subverting Harley's theory that mapping is uniquely a source of power for the powerful. Joyce challenges Foucauldian notions of Panopticism, contending that neoliberal governmentality is more adequately conceptualised by an omniopticon - 'the many surveilling the many'. Collaborative mapping initiatives utilising GPS technology are arguably omniopticons, with the ability to reverse the panoptic gaze.

Through our freedom, particular self-governing capabilities can be installed in order to bring our own ways of conducting and evaluating ourselves into alignment with political objectives [Rose, 1996:155]. These capabilities are enterprise and autonomy. Enterprise here designates an array of rules for the conduct of one's everyday existence: energy, initiative, ambition, calculation, and personal responsibility. The enterprising self will make an enterprise of its life, seek to maximize its own human capital, project itself a future, and seek to shape life in order to become what it wishes to be. The enterprising self is thus both an active self and a calculating self, a self that calculates about itself and that acts upon itself in order to better itself [Rose, 1996:154]. Autonomy is about taking control of our undertakings, defining our goals, and planning to achieve our needs through our own powers [Rose, 1996:159]. The autonomy of the self is thus not the eternal antithesis of political power, but one of the objectives and instruments of modern mentalities for the conduct of conduct [Rose, 1996:155].

These three qualities: freedom, enterprise and autonomy are embodied in the practice of going to the gym. It is our choice to go the gym, our choice which gym to go to. By going to the gym we are working on ourselves, on our body shape and our physical fitness. We are giving ourselves qualities to help us perform better than others in life, whether to attract a better mate than others, or to be able to work more efficiently, more effectively and for longer without running out of steam to give us an advantage over our competitors. When we go to the gym, we go through our own discipline, on our own timetable, to reach our own goals. We design and act out our routine by ourselves. We do not need the ideas or support of a team, it is our self that makes it possible. The practice of going to the gym, of being free, enterprising, autonomous, is imbued with particular technologies of power.

Technologies of power are those "technologies imbued with aspirations for the shaping of conduct in the hope of producing certain desired effects and averting certain undesired ones" [Rose, 1999:52]. The two main groups of technologies of power are technologies of the self, and technologies of the market. Foucault defined technologies of the self as techniques that allow individuals to effect by their own means a certain number of operations on their own bodies, minds, souls, and lifestyle, so as to transform themselves in order to attain a certain state of happiness, and quality of life. Technologies of the market are those technologies based around the buying and selling of goods that enable us to define who we are, or want to be. These two technologies are not always completely distinct, as both borrow bits of each other from time to time.

Technologies of the self refer to the practices and strategies by which individuals represent to themselves their own ethical self-understanding. One of the main features of technologies of self is that of expertise. Expertise has three important aspects. First, its grounding of authority in a claim to scientificity and objectivity creates distance between self-regulation and the state that is necessary with liberal democracies. Second, expertise can "mobilise and be mobilised within political argument in distinctive ways, producing a new relationship between knowledge and government. Expertise comes to be accorded a particular role in the formulation of programs of government and in the technologies that seek to give them effect" [Rose, 1996:156]. Third, expertise operates through a relationship with the self-regulating abilities of individuals. The plausibility inherent in a claim to scientificity binds "subjectivity to truth and subjects to experts" [Rose, 1996:156]. Expertise works through a logic of choice, through a transformation of the ways in which individuals constitute themselves, through "inculcating desires for self-development that expertise itself can guide and through claims to be able to allay the anxieties generated when the actuality of life fails to live up to its image [Rose, 1999:88].

The technologies of the self involved in the practice of, for example, going to the gym are the: technology of responsibilisation, technology of healthism, technology of normalisation and technology of self-esteem.

In line with its desire to reduce the scope of government (e.g. welfare) Neo-liberalism characteristically develops indirect techniques for leading and controlling individuals without being responsible for them. The main mechanism is through the technology of responsibilisation. This entails subjects becoming responsibilised by making them see social risks such as illness, unemployment, poverty, public safety etc. not as the responsibility of the state, but actually lying in the domain for which the individual is responsible and transforming it into a problem of 'self-care' [Lemke, 2001:201] and of 'consumption'. The practice of going to the gym can be seen as a result of responsibilisation, our responsibility to remain free of illness so as to be able to work and to care for our dependants (children, elderly parents etc.) This technology somewhat overlaps with the technology of healthism.

Healthism links the "public objectives for the good health and good order of the social body with the desire of individuals for health and well-being" [Rose, 1999:74]. Healthy bodies and hygienic homes may still be objectives of the state, but it no longer seeks to discipline, instruct, moralise or threaten us into compliance. Rather "individuals are addressed on the assumption that they want to be healthy and enjoined to freely seek out the ways of living most likely to promote their own health" [Rose, 1999:86-87] such as going to the gym. However while the technology of responsibilisation may be argued to be a calculated technique of the state, the wave of Healthism is less likely to be a consequence of state planning, but arising out of the newer social sciences such as nutrition and human movement. Healthism assigns, as do most technologies of the self, a key role to experts. For it is experts who can tell us how to conduct ourselves in terms of safe, precise techniques to improve cardiovascular fitness, muscle strength, and overall health. The borrowing from technologies of the market by technologies of the self can be clearly seen in the area of healthism. The idea of health, the goal of being healthy, the joys brought by good health and the ways of achieving it are advertised to us in the same manner as goods and services are marketed by sales people. By adhering to the principles of healthism, our personal goals are aligned with political goals and we are thus rendered governable.

Another technology of power arising from the social sciences is that of normalisation. The technology of norms was given a push by the new methods of measuring population. A norm is that "which is socially worthy, statistically average, scientifically healthy and personally desirable". The important aspect of normality, is that while the norm is natural, those who wish to achieve normality will do so by working on themselves, controlling their impulses in everyday conduct and habits, and inculcating norms of conduct into their children, under the guidance of others. Norms are enforced through the calculated administration of shame. Shame entails an anxiety over the exterior behaviour and appearance of the self, linked to an injunction to care for oneself in the name of achieving quality of life [Rose, 1999:73]. Norms are usually aligned with political goals, thus the norm would be fit, virile, energetic individuals, able to work, earn money, and spend it and thus sustain the economy. For instance, the practice of going to the gym allows one to achieve this 'normality'. Through shame we are governed into conforming with the goals of Neo-liberalism.

Self-esteem is a practical and productive technology linked to the technology of norms, which produces of certain kinds of selves. Self-esteem is a technology in the sense that it is a specialised knowledge of how to esteem ourselves to estimate, calculate, measure, evaluate, discipline, and to judge our selves. The 'self-esteem' approach considers a wide variety of social problems to have their source in a lack of self-esteem on the part of the persons concerned. 'Self-esteem' thus has much more to do with self-assessment than with self-respect, as the self continuously has to be measured, judged and disciplined in order to gear personal 'empowerment' to collective yardsticks. These collective yardsticks are determined by the norms previously discussed. Self-esteem is a technology of self for "evaluating and acting upon ourselves so that the police, the guards and the doctors do not have to do so". By taking up the goal of self-esteem, we allow ourselves to be governable from a distance. The technology of self-esteem and other similar psychological technologies also borrow from technologies of the market, namely consumption. A huge variety of self-help books, tapes, videos and other paraphernalia are available for purchase by the individual.

The technologies of the market that underlie the practice of going to the gym can be described as the technology of desire, and the technology of identity through consumption. The technology of desire is a mechanism that induces in us desires that we work to satisfy. Marketers create wants and artificial needs in us through advertising goods, experiences and lifestyles that are tempting to us. These advertisements seek to convey the sense of individual satisfaction brought about by the purchase or use of this product. We come to desire these things and thus act in a manner that allows us to achieve these things, whether by working harder and earning more money or by employing technologies of the self to shape our lifestyle to the manner we desire . The borrowing of technologies of the self by technologies of the market extends even further in this case. Marketers use the knowledge created by psyche- discourses, especially psychological characteristics as the basis of their market segmentation. This allows them to appeal more effectively to each individual. Thus we are governed into purchasing commodities through our desire.

The technology of identity through consumption utilises the power of goods to shape identities. Each commodity is imbued with a particular meaning, which is reflected upon those who purchase it, illuminating the kind of person they are, or want to be. Consumption is portrayed as placing an individual within a certain form of life. The technology of identity through consumption can be seen in the choices that face the gym attendee. To go to an expensive gym because it demonstrates wealth/success or to go to a moderately priced gym so as to appear economical. The range of gym wear is extensive. Brand name to portray the abilities portrayed in its advertising, expensive to portray commitment, or cheap to portray your unconcern for other people's opinions. All of these choices of consumption are used to communicate our identity to others, and thus we are governed by marketers into choosing those products that identify with our identity.

These technologies of the market and of the self are the particular mechanisms whereby individuals are induced into becoming free, enterprising individuals who govern themselves and thus need only limited direct governance by the state. The implementation of these technologies is greatly assisted by experts from the social sciences. These experts operate a regime of the self, where success in life depends on our continual exercise of freedom, and where our life is understood, not in terms of fate or social status, but in terms of our success or failure in acquiring the skills and making the choices to actualise ourself. If we engage in the practice of going to the gym, we are undertaking an exercise in self-government. We do so by drawing upon certain forms of knowledge and expertise provided by gym instructors, health professionals, of the purveyors of the latest fitness fad. Depending on why we go to the gym, we may calculate number of calories burned, heart-rate, or muscle size. In all cases, we attend the gym for a specific set of reasons underpinned by the various technologies of the self and the market. The part of ourselves we seek to work upon, the means by which we do so, and who we hope to become, all vary according to the nature of the technology of power by which we are motivated [Dean, 1999:17]. All of these various reasons and technologies are underpinned by the mentality of government that seeks to transform us into a free, enterprising, autonomous individual: Neo-liberalism. Furthermore, Neo-liberalism seeks to create and disseminate definitions of freedom, autonomy and what it means to be enterprising that re-create forms of behavior amenable to neo-liberal goals.

Ecogovernmentality (or eco-governmentality) is the application of Foucault's concepts of biopower and governmentality to the analysis of the regulation of social interactions with the natural world. Timothy W. Luke theorized this as environmentality and green governmentality. Ecogovernmentality began in the mid-1990s with a small body of theorists (Luke, Darier, and Rutherford) the literature on ecogovernmentality grew as a response to the perceived lack of Foucauldian analysis of environmentalism and in environmental studies.

Following Michel Foucault, writing on ecogovernmentality focuses on how government agencies, in combination with producers of expert knowledge, construct "The Environment." This construction is viewed both in terms of the creation of an object of knowledge and a sphere within which certain types of intervention and management are created and deployed to further the government's larger aim of managing the lives of its constituents. This governmental management is dependent on the dissemination and internalization of knowledge/power among individual actors. This creates a decentered network of self-regulating elements whose interests become integrated with those of the State.

According to Foucault, there are several instances where the Western, "liberal art of government" enters into a period of crisis, where the logic of ensuring freedom (which was defined against the background of risk or danger) necessitates actions "which potentially risk producing exactly the opposite."

The inherently contradictory logics that lead to such contradictions are identified by Foucault as:


Examples of this contradictory logic which Foucault cites are the policies of the Keynesian welfare state under F.D. Roosevelt, the thought of the German liberals in the Freiburg school, and the thought of American libertarian economists such as the Chicago School which attempt to free individuals from the lack of freedom perceived to exist under socialism and fascism, but did so by using state interventionist models.

These governmental crises may be triggered by phenomena such as a discursive concern with increasing economic capital costs for the exercise of freedom, e.g., prices for purchasing resources, the need for excessive state coercion and interventionism to protect market freedoms, e.g., anti-trust and anti-monopoly legislation that leads to a "legal strait-jacket" for the state, local protests rejecting the disciplinary mechanisms of the market society and state. and finally, the destructive and wasteful effects of ineffective mechanisms for producing freedom.

Scholars have recently suggested that the concept of governmentality may be useful in explaining the operation of evidence-based health care and the internalization of clinical guidelines relating to best practice for patient populations, such as those developed by the American Agency for Health Care Research and Quality and the British National Institute for Health and Clinical Excellence (NICE). Recent research by Fischer and colleagues at the University of Oxford has renewed interest in Foucault's exploration of potential resistance to governmentality, and its application to health care, drawing on Foucault's recently published final lectures at the College de France.

Jeffreys and Sigley (2009) highlight that governmentality studies have focused on advanced liberal democracies, and preclude considerations of non-liberal forms of governmentality in both western and non-western contexts. Recent studies have broken new ground by applying Foucault's concept of governmentality to non-western and non-liberal settings, such as China. Jeffreys (2009) for example provides a collection of essay on China's approach to governance, development, education, the environment, community, religion, and sexual health where the notion of 'Chinese governmentally' is based not on the notion of 'freedom and liberty' as in the western tradition but rather, on a distinct rational approach to planning and administration. Such new studies thus use Foucault's Governmentalities to outline the nature of shifts in governance and contribute to emerging studies of governmentality in non-western contexts.


http://www.inderscience.com/info/inarticle.php?artid=67421


</doc>
<doc id="501118" url="https://en.wikipedia.org/wiki?curid=501118" title="Hermit kingdom">
Hermit kingdom

The term hermit kingdom is used to refer to any country, organization or society which willfully walls itself off, either metaphorically or physically, from the rest of the world. In the current geopolitical order, the East Asian country of North Korea is regarded as a prime example of a hermit kingdom, and the term is contemporarily used to describe that nation state.

Korea in the age of Joseon dynasty was the subject of the first use of the term, in William Elliot Griffis's 1882 book "Corea: The Hermit Nation", and Korea was frequently described as a hermit kingdom until 1905 when it became a protectorate of Japan. The term is still commonplace throughout Korea and it is often used by Koreans themselves to describe pre-modern Korea. 

Today, the term is often applied to North Korea in news and social media, and in 2009 it was used by United States Secretary of State Hillary Clinton. 


</doc>
<doc id="16953152" url="https://en.wikipedia.org/wiki?curid=16953152" title="Extreme environment">
Extreme environment

An extreme environment is a habitat that is considered very hard to survive in due to its considerably extreme conditions such as temperature, accessibility to different energy sources or under high pressure. For an area to be considered an extreme environment, it must contain certain conditions and aspects that are considered very hard for other life forms to survive. Pressure conditions may be extremely high or low; high or low content of oxygen or carbon dioxide in the atmosphere; high levels of radiation, acidity, or alkalinity; absence of water; water containing a high concentration of salt or sugar; the presence of sulphur, petroleum, and other toxic substances.

Examples of extreme environments include the geographical poles, very arid deserts, volcanoes, deep ocean trenches, upper atmosphere, Mt Everest, outer space, and the environments of every planet in the Solar System except the Earth. Any organisms living in these conditions are often very well adapted to their living circumstances, which is usually a result of long-term evolution. Physiologists have long known that organisms living in extreme environments are especially likely to exhibit clear examples of evolutionary adaptation because of the presumably intense past natural selection they have experienced.

The distribution of extreme environments on Earth has varied through geological time. Humans generally do not inhabit extreme environments. There are organisms referred to as extremophiles that do live in such conditions and are so well-adapted that they readily grow and multiply. Extreme environments are usually hard to survive in.

Most of the moons and planets in the Solar System are also extreme environments. Astrobiologists have not yet found life in any environments beyond Earth, though experiments have shown that tardigrades can survive the harsh vacuum and intense radiation of outer space. The conceptual modification of conditions in locations beyond Earth, to make them more habitable by humans and other terrestrial organisms, is known as terraforming.

Among extreme environments are places that are alkaline, acidic, or unusually hot or cold or salty, or without water or oxygen. There are also places altered by humans, such as mine tailings or oil impacted habitats.
Many different habitats can be considered extreme environments, such as the polar ice caps, the driest spots in deserts, and abysmal depths in the ocean. Many different places on the Earth demand that species become highly specialized if they are to survive. In particular, microscopic organisms that can't be seen with the naked eye often thrive in surprising places. 

Due to the dangerously low temperatures, the amount of species that can survive in the these remote areas is very slim. Over years of evolution and adaptation to this extremely cold environment, both microscopic and larger species have survived and thrived no matter what conditions they are faced. By changing their eating patterns and due to their dense pelt or their body fat, only a few species have been capable of adapting to such harsh conditions and have learned how the thrive in these cold environments.  

A desert is known for its extreme temperatures and extremely dry climate. The type of species that reside in this area have adapted to these harsh conditions over years and years. Species that are able to store water and have learned how to protect themselves from the suns harsh rays are the only ones who are capable of surviving in these extreme environments.  

The oceans depths and temperatures contains some of the most extreme conditions for any species to survive. The deeper one travels, the higher the pressure and the lower the visibility gets, causing completely blacked out conditions. Many of these conditions are too intense for humans to travel to, so instead of sending humans down to these depths to collect research, scientists are using smaller submarines or deep sea drones to study these creatures and extreme environments.

There are many different species that are either commonly known or not known amongst many people. These species have either adapted over time into these extreme environments or they have resided their entire life no matter how many generations. The different species that are able to live in these environments because of their flexibility with adaptation. Many can adapt to different climate conditions and hibernate if need be to survive.

The following list contains only a few species that live in extreme environments . 

Different Types of Species 







</doc>
<doc id="44426150" url="https://en.wikipedia.org/wiki?curid=44426150" title="Spatial association">
Spatial association

Spatial association is the degree to which things are similarly arranged in space. Analysis of the distribution patterns of two phenomena is done by map overlay. If the distributions are similar, then the spatial association is strong, and vice versa. In a Geographic Information System, the analysis can be done quantitatively. For example, a set of observations (as points or extracted from raster cells) at matching locations can be intersected and examined by regression analysis.

Like spatial autocorrelation, this can be a useful tool for spatial prediction. In spatial modeling, the concept of spatial association allows the use of covariates in a regression equation to predict the geographic field and thus produce a map.


</doc>
<doc id="14389994" url="https://en.wikipedia.org/wiki?curid=14389994" title="Natural landscape">
Natural landscape

A natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.

In "Silent Spring" (1962) Rachel Carson describes a roadside verge as it used to look: "Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year" and then how it looks now following the use of herbicides: "The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.

The phrase "natural landscape" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape "separate" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape ("Urlandschaft") and its opposite cultural landscape ("Kulturlandschaft") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase "natural landscape" by a geographer can be found in Carl O. Sauer's paper "The Morphology of Landscape" (1925).

The concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, "that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape." With regard to landscape gardening John Aikin, commented in 1794: "Whatever, therefore, there be of "novelty" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: "straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape".

In his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.

Subsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a "Landschaftskunde" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the "Urlandschaft" (original landscape) or landscape that existed before major human induced changes and the "Kulturlandschaft" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.

The term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.

Matters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: "The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations." On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.

The dualism of the first definition has its roots is an "ancient concept", because early people viewed "nature, or the nonhuman world […] as a divine "Other", godlike in its separation from humans." In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. 
With this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.

What is meant by natural, within the American conservation movement, has been changing over the last century and a half.

In the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and "avoid disturbances such as logging, grazing, fire and insect outbreaks." This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.

A century later, in the mid-twentieth century, it began to be believed that the earlier policy of "protection from disturbance was inadequate to preserve park values", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that "A national park should represent a vignette of primitive America". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.

However, recent research in various disciplines indicates that a pristine natural or "primitive" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.

Also important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: "Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it "involves a dualistic vision in which the human is entirely outside the natural" and affirms that "wildness (as opposed to wilderness) can be found anywhere" even "in the cracks of a Manhattan sidewalk." According to Cronon we have to "abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild." Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.

The landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only " the high summits of the Cairngorm Mountains, consist entirely of natural elements. These "high summits" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.

The Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.
Visitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.

No place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.
Even the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.

Cultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.



</doc>
<doc id="47470974" url="https://en.wikipedia.org/wiki?curid=47470974" title="Glacial refugium">
Glacial refugium

A glacial refugium ("plural refugia") is a geographic region which made possible the survival of flora and fauna in times of ice ages and allowed for post-glacial re-colonization. Different types of glacial refugia can be distinguished, namely nunatak, peripheral and lowland refugia. Glacial refugia have been suggested as a major cause of the patterns of distributions of flora and fauna in both temperate and tropical latitudes. However, in spite of the continuing use of historical refugia to explain modern-day species distributions, especially in birds, doubt has been cast on the validity of such inferences, as much of the differentiation between populations observed today may have occurred before or after their restriction to refugia.

Traditionally, the identification of glacial refugia have occurred through the assessment of palaeoecological evidence, to determine the origins of modern taxa.  For example, paleoecological approaches, which focus on the study of fossil organisms and their remains, have been used to reconstruct the distributions of pollen in Europe, for the 13,000 years since the last glaciation. Researchers in this case ultimately established the spread of forest trees from the mountainous southern fringe of Europe, which suggests that this area served as a glacial refugia during this time.

In studies exploring the extent of glacial refugia in mountain species, three distinct types of glacial refugia have been identified.

A nunatak is a type of glacial refugia that is located on the snow-free, exposed peaks of mountains, which lie above the ice sheet during glaciations.  The identification of ‘diversity hotspots’ in areas, which should have been migration regions during major glacial episodes, is evidence for nunatak glacial refugia.  For example, the Monte Rosa mountain ranges, the Avers, and the Engadine and the Bernina are all floristically rich proposed nunatak regions, which are indicative nunatak glacial survival.

Peripheral glacial refugia still exists within the mountain system but contrary to nunataks, which exist on the peaks, this type of refugia is located along the borders of mountain systems.  Evidence for this type of mountain refugia can be found along the borders of the Carpathian Mountains, Pyrenees or European Alps, all of which were formally glaciated mountain systems. For example, using the amplified fragment length polymorphism (AFLP) technique, researchers have been able to infer the survival of "Phyteuma globulariifolium" in peripheral refugia in the European Alps.

Lowland glacial refugia, unlike nunatak and peripheral glacial refugia, is a type of refugia that exists outside of the mountain system in the lowlands. Situated beyond the limits of ice shields, lowland refugia has been identified for a number of plant and animal species. For example, through allozyme analysis, researchers have been able to confirm the continuous distribution of "Zygaena exulans" in the between the foothills of the Pyrenees and the Alps during the last ice age.



</doc>
<doc id="898161" url="https://en.wikipedia.org/wiki?curid=898161" title="Geopark">
Geopark

A geopark is a unified area that advances the protection and use of geological heritage in a sustainable way, and promotes the economic well-being of the people who live there. There are global geoparks and national geoparks.

A UNESCO definition of "global geopark" is a unified area with a geological heritage of international significance. Geoparks use that heritage to promote awareness of key issues facing society in the context of our dynamic planet. Many geoparks promote awareness of geological hazards, including volcanoes, earthquakes and tsunamis and many help prepare disaster mitigation strategies with local communities. Geoparks embody records of past climate changes and are indicators of current climate changes as well as demonstrating a "best practise" approach to using renewable energy and employing the best standards of "green tourism". Tourism industry promotion in geoparks, as a geographically sustainable and applicable tourism model, aims to sustain, and even enhance, the geographical character of a place.

Geoparks also inform about the sustainable use and need for natural resources, whether they are mined, quarried or harnessed from the surrounding environment while at the same time promoting respect for the environment and the integrity of the landscape. Geoparks are not a legislative designation though the key heritage sites within a geopark are often protected under local, regional or national legislation. The multidisciplinary nature of the concept of geopark and tourism promotion in geoparks differentiates itself from other models of sustainable tourism. In fact, sustainable tourism promotion within geoparks encompasses many of the features of sustainable tourism including geo-tourism (geo-site tourism: as a basic factor), community-based tourism and integrated rural tourism (as a vital need), ecotourism, and cultural heritage tourism.

The Global Geoparks Network (GGN) is supported by United Nations Educational, Scientific and Cultural Organization (UNESCO). Many national geoparks and other local geoparks projects also exist which are not included in the Global Geoparks Network.

The geoparks initiative was launched by UNESCO in response to the perceived need for an international initiative that recognizes sites representing an earth science interest. Global Geoparks Network aims at enhancing the value of such sites while at the same time creating employment and promoting regional economic development. 
The 195 Member States of UNESCO ratified the creation of a new label, the UNESCO Global Geoparks, on 17 November 2015. This expressed governmental recognition of the importance of managing outstanding geological sites and landscapes in a holistic manner. This new designation formalized UNESCO's relationship with the Global Geoparks Network.
The Global Geoparks Network works in synergy with UNESCO's World Heritage Centre and Man and the Biosphere (MAB) World Network of Biosphere Reserves.

The Global Geoparks Network (GGN) was established in 1998 and received "ad hoc" support from UNESCO from 2001 until 2015, when the relationship and designation was formalized. Since 2015, members are officially designated by UNESCO, as UNESCO Global Geoparks. According to the Statutes and Operational Guidelines of the UNESCO Global Geoparks, for a geopark to apply to be included in the GGN, it needs to:


See UNESCO Global Geoparks.




</doc>
<doc id="47802272" url="https://en.wikipedia.org/wiki?curid=47802272" title="Edgelands">
Edgelands

Edgelands are the transitional, liminal areas of space to be found on the boundaries of country and town—with the spread of urbanisation, an increasingly important facet of the twenty-first century world.

The concept of Edgelands was introduced by Marion Shoard in 2002, to cover the disorganised but often fertile hinterland between planned town and over-managed country. However a century and a half earlier, Victor Hugo had already highlighted the existence of what he called "bastard countryside...ugly but bizarre, made up of two different natures, which surrounds certain great cities"; while Richard Jeffries similarly explored the London edgeland in "Nature near London" (1883).
Alice Coleman (Kings College London, dept geography) in 2nd Land Use Survey of Great Britain, refers to "rurban fringe". Indicating a similar landscape but with negative overtones.

Nevertheless it was only in the last decades of the twentieth century - as a distinct realm of Nature increasingly disappeared beneath the commodifying impact of globalising late capitalism - that the significance of the unstructured borderlands between organised town and organised country, part man-made, part natural, both for wildlife and for human exploration, came into fuller focus. Psychogeography charted the London orbital, while bombsites, canal banks and brownfield sites were documented in poetry and prose, film and photography; and the borderlands as an untapped, transgressive resource became almost the object of a new cult.




</doc>
<doc id="47827949" url="https://en.wikipedia.org/wiki?curid=47827949" title="Internet geography">
Internet geography

Internet geography, also called cybergeography, is a subdiscipline of geography that studies the spatial organization of the Internet, from social, economic, cultural, and technological perspectives. The core assumption of Internet geography is that the location of servers, websites, data, services, and infrastructure is key to understand the development and the dynamics of the Internet. Among the topics covered by this discipline, of particular importance are information geography and digital divides.



</doc>
<doc id="50516307" url="https://en.wikipedia.org/wiki?curid=50516307" title="Primary care service area">
Primary care service area

Primary Care Service Areas are geographic areas that are self-sufficient markets of primary care. These areas are designed in a manner such that the majority of patients living in these areas use primary care services form within the area. This ensures that any geographic targeting of policies and resources reach the patients they are meant for. These geographies have been created in Australia, United States and Switzerland using big data and Geographic information systems. In Australia, while they have been developed for the state of New South Wales, they have not found application among policymakers, where, as of 2016 much larger geographies called Primary Health Networks are used for primary care management. However, they have found an especially wide audience amongst policymakers and researchers in the United States, where they were first developed. Thus for example the Health Resources and Services Administration uses them to designate areas of workforce shortage. Primary Care Service Areas are thus for example an appropriate geography for measuring primary care physician supply or geographic access to General practitioners.



</doc>
<doc id="50427403" url="https://en.wikipedia.org/wiki?curid=50427403" title="International date line in Judaism">
International date line in Judaism

The international date line in Judaism is used to demarcate the change of one calendar day to the next in the Jewish calendar. The Jewish calendar defines days as running from sunset to sunset rather than midnight to midnight. So in the context of Judaism, an international date line demarcates when the line of sundown moving across the Earth's surface stops being the sunset ending and starting one day and starts being the sunset ending and starting the following day.

However, the conventional International Date Line is a relatively recent geographic and political construct whose exact location has moved from time to time depending on the needs of different interested parties. While it is well-understood why the conventional date line is located in the Pacific Ocean, there are not really objective criteria for its exact placement within the Pacific. In that light, it cannot be taken for granted that the conventional International Date Line can (or should) be used as a date line under Jewish law. In practice, within Judaism the "halakhic" date line is similar to, but not necessarily identical with, the conventional Date Line, and the differences can have consequences under religious law.

Many of the opinions about the "halakhic" date line are structured as a response to the question of what days someone should observe as Shabbat and Jewish holidays. Shabbat occurs every seven days at any location on earth. It is normally thought to occur on Saturday—or more precisely, from Friday at sundown to Saturday at nightfall. But if the "halakhic" date line is not identical to the conventional Date Line, it is possible that what is Saturday with respect to the conventional Date Line is not Saturday with respect to the "halakhic" date line, at least in some places.

There are several opinions regarding where exactly the "halakhic" date line should be according to Jewish law, and at least one opinion that says that no "halakhic" date line really exists.

1. "90 degrees east of Jerusalem." The concept of a "halakhic" date line is mentioned in the "Baal HaMeor," a 12th-century Talmudic commentary, which seems to indicate that the day changes in an area where the time is six hours ahead of Jerusalem (90 degrees east of Jerusalem, about 125.2°E, a line now known to run through Australia, the Philippines, China and Russia). This line, which he refers to as the "K'tzai Hamizrach" (the easternmost line), is used to calculate the day of Rosh Hashanah, the Jewish New Year. According to some sources it is alluded to in both the Babylonian Talmud (Rosh Hashanah and Eruvin) and in the Jerusalem Talmud. The "Kuzari" of Yehuda Halevi, also a 12th-century work, seems to agree with this ruling.

Later decisors like the "Chazon Ish" (twentieth century) fundamentally agree with this ruling. However, they recognize practical issues associated with the pure use of a line of longitude for this purpose. As an example, 125.2°E passes directly through Dongfeng Street in Changchun, China. If this line of longitude were used strictly, people could simply avoid Shabbat altogether by crossing the street. To prevent that, the Chazon Ish rules that the contiguous land masses to the east of that line of longitude are considered secondary ("tafel") to the land masses west of that line. As a result, he rules that the date line runs along 125.2°E when over water, but curves around the eastern coast of mainland Asia and Australia. By this view, Russia, China and Australia are west of the date line and observe Shabbat on local Saturday. Japan, New Zealand and Tasmania are east of the date line and should observe Shabbat on local Sunday, as defined by the conventional International Date Line. By this view, the Philippines and Indonesia would have portions west of the line and portions east of the line.

2. "180 degrees east of Jerusalem." Rabbi Yechiel Michel Tucazinsky ruled that the International Date Line is 180 degrees east of Jerusalem. That would mean that the date line, rather than being near 180°, would be at 144.8°W. By this view, places east of the conventional International Date Line but west of 144.8°W—Alaska, Hawaii and a variety of archipelagos in the Pacific—would observe Shabbat on the local Friday instead of the local Saturday.

It is possible (but not certain) under this view to apply the principal of "tafel" described above as well. In that event, mainland Alaska would be east of the date line, but the Aleutian Islands would be west of the date line.

3. "Mid-Pacific." A variety of decisors rule that the date line runs in the middle of the Pacific Ocean, close to (but not necessarily the same as) the conventional International Date Line. According to this point of view, all of the major populated areas of the Pacific (such as New Zealand, Japan, Alaska and Hawaii) observe Shabbat on local Saturday (that is, consistent with the conventional International Date Line). Only certain Pacific islands, generally having few or no permanent Jewish residents, might not observe Shabbat on local Saturday.

4. "Following local custom/There is no fixed date line." According to Rabbi Menachem Mendel Kasher, there is no clear tradition or Talmudic source dictating any of the preceding opinions as binding. For that reason, and consistent with a responsum of the Radbaz, Rabbi Kasher starts with the default law that a Jew not knowing the proper day for Shabbat should count days from the last time s/he observed Shabbat, and that every seven days is Shabbat. In his view, established Jewish communities are presumed to have fixed their calendars according to this principle. Therefore, Shabbat in an established community is whatever day the community has established. Rabbi Isser Zalman Meltzer and Rabbi Zvi Pesach Frank apparently agree with this position. This position does not in and of itself require a formal date line to be established, and Rabbi Kasher does not seem to think that it is necessary to do so. But the "de facto" result of this position is consistent with the conventional International Date Line, at least anywhere there is an established Jewish community.

In practice, the conventional International Date Line (or another mid-Pacific line near it) is the "de facto" date line under Jewish law, at least for established Jewish communities. The communities of Japan, New Zealand, Hawaii, and French Polynesia all observe Shabbat on local Saturday "(i.e.," Friday night until Saturday night). No known Jewish community observes Shabbat on a day other than local Saturday. However, that practical conclusion is reached in two different ways, resulting in somewhat different practice patterns in each case.

Following local custom/There is no fixed date line. As noted above, according to this point of view, Shabbat is simply observed on the date previously established as Shabbat by the local community—uniformly, local Saturday—without any need for any further observance. This appears to be the default practice for residents of such places as Japan, New Zealand and Hawaii. At minimum, it is difficult to find evidence of other practices by residents of those areas.

Establishment of a date line by a majority among three halakhic positions. The travelers' guide of the Star-K kosher supervision service, compiled according to the rulings of its rabbinic administrator, Rabbi Moshe Heinemann, uses the following approach, which is also cited by others. According to this approach, the first three numbered sections above constitute three valid, parallel, "halakhic" rulings. Shabbat is consequently fully observed on whichever day is consistent with the majority view among those opinions (two out of three). However, out of respect to the minority view of the third ruling, and with an eye toward not desecrating Shabbat, Torah-level prohibitions are to be avoided on the day consistent with the minority view, although that day is otherwise considered a weekday. According to this rule, practice is as follows: 
The Star-K's international kosher supervision staff follows this approach, and there is evidence that some other travelers also do. Authorities suggesting this approach often advise travelers to avoid the zone of doubt entirely near weekends, or to consult with a competent rabbinical authority directly.

The issues discussed in the previous section apply "per se" to individuals or communities in fixed locations. However, the act of crossing the date line (wherever it may be drawn) introduces a number of additional issues under Jewish law. Questions potentially affected include:


In some cases, crossing the date line (wherever it may be drawn) has a specific impact on practice or prohibitions under Jewish law. In others, an individual's count of days (by the experience of sunset and sunrise) is the determining factor, regardless of the crossing of the date line. Details around specific questions, cases and rulings of Jewish law are beyond the scope of this article.

Before Israeli astronaut Ilan Ramon flew on the Space Shuttle "Columbia" in 2003, he decided (after consultation with rabbis) to observe Shabbat according to time in his last residence, Cape Canaveral, since he would be crossing the date line and observing sunset many times per day. Judith Resnik, the first American Jewish astronaut in space, lit (electronic) Shabbat candles according to the time in Houston, TX, her home and the location of Mission Control.



</doc>
<doc id="2269821" url="https://en.wikipedia.org/wiki?curid=2269821" title="Tobler's first law of geography">
Tobler's first law of geography

The First Law of Geography, according to Waldo Tobler, is "everything is related to everything else, but near things are more related than distant things." This first law is the foundation of the fundamental concepts of spatial dependence and spatial autocorrelation and is utilized specifically for the inverse distance weighting method for spatial interpolation and to support the regionalized variable theory for kriging. It is a modern formulation of David Hume's principle of contiguity.

Tobler first presented his seminal idea during a meeting of the International Geographical Union's Commission on Qualitative Methods held in 1969 and later published by him in 1970. Though simple in its presentation, this idea is profound. Without it, "the full range of conditions anywhere on the Earth's surface could in principle be found packed within any small area. There would be no regions of approximately homogeneous conditions to be described by giving attributes to area objects. Topographic surfaces would vary chaotically, with slopes that were everywhere infinite, and the contours of such surfaces would be infinitely dense and contorted. Spatial analysis, and indeed life itself, would be impossible."

Less well known is his second law, which complements the first: "The phenomenon external to an area of interest affects what goes on inside".

The theory is based upon the concept of the friction of distance "where distance itself hinders interaction between places. The farther two places are apart, the greater the hindrance", or cost. For example, one is less likely to travel across town to purchase a sandwich than walk to the corner store for the same sandwich. In this example hindrance, or cost, can readily be counted in time (amount of time as well as value of time), transportation costs and personal muscle energy loss which are added to the price of the purchase and thus result in high levels of friction. The friction of distance and the increase in cost combine causing the distance decay effect.


</doc>
<doc id="2371378" url="https://en.wikipedia.org/wiki?curid=2371378" title="Surroundings">
Surroundings

Surroundings are the area around a given physical or geographical point or place. The exact definition depends on the field. Surroundings can also be used in geography (when it is more precisely known as vicinity, or vicinage) and mathematics, as well as philosophy, with the literal or metaphorically extended definition.

In thermodynamics, the term (and its synonym, environment) is used in a more restricted sense, meaning everything outside the thermodynamic system. Often, the simplifying assumptions are that energy and matter may move freely within the surroundings, and that the surroundings have a uniform composition.



</doc>
<doc id="18963910" url="https://en.wikipedia.org/wiki?curid=18963910" title="Geography">
Geography

Geography (from Greek: , "geographia", literally "earth description") is a field of science devoted to the study of the lands, features, inhabitants, and phenomena of the Earth and planets. The first person to use the word γεωγραφία was Eratosthenes (276–194 BCE). Geography is an all-encompassing discipline that seeks an understanding of Earth and its human and natural complexities—not merely where objects are, but also how they have changed and come to be.

Geography is often defined in terms of two branches: human geography and physical geography. Human geography deals with the study of people and their communities, cultures, economies, and interactions with the environment by studying their relations with and across space and place. Physical geography deals with the study of processes and patterns in the natural environment like the atmosphere, hydrosphere, biosphere, and geosphere.

The four historical traditions in geographical research are spatial analyses of natural and the human phenomena, area studies of places and regions, studies of human-land relationships, and the Earth sciences. Geography has been called "the world discipline" and "the bridge between the human and the physical sciences".

Geography is a systematic study of the Universe and its features. Traditionally, geography has been associated with cartography and place names. Although many geographers are trained in toponymy and cartology, this is not their main preoccupation. Geographers study the space and the temporal database distribution of phenomena, processes, and features as well as the interaction of humans and their environment. Because space and place affect a variety of topics, such as economics, health, climate, plants and animals, geography is highly interdisciplinary. The interdisciplinary nature of the geographical approach depends on an attentiveness to the relationship between physical and human phenomena and its spatial patterns.

Geography as a discipline can be split broadly into two main subsidiary fields: human geography and physical geography. The former largely focuses on the built environment and how humans create, view, manage, and influence space. The latter examines the natural environment, and how organisms, climate, soil, water, and landforms produce and interact. The difference between these approaches led to a third field, environmental geography, which combines physical and human geography and concerns the interactions between the environment and humans.

Physical geography (or physiography) focuses on geography as an Earth science. It aims to understand the physical problems and the issues of lithosphere, hydrosphere, atmosphere, pedosphere, and global flora and fauna patterns (biosphere). Physical Geography is the study of earth's seasons, climate, atmosphere, soil, streams, landforms, and oceans. 

Human geography is a branch of geography that focuses on the study of patterns and processes that shape the human society. It encompasses the human, political, cultural, social, and economic aspects.

Various approaches to the study of human geography have also arisen through time and include:

Integrated geography is concerned with the description of the spatial interactions between humans and the natural world. It requires an understanding of the traditional aspects of physical and human geography, as well as the ways that human societies conceptualize the environment. Integrated geography has emerged as a bridge between human and physical geography, as a result of the increasing specialisation of the two sub-fields. Furthermore, as the human relationship with the environment has changed as a result of globalization and technological change, a new approach was needed to understand the changing and dynamic relationship. Examples of areas of research in environmental geography include: emergency management, environmental management, sustainability, and political ecology.

Geomatics is concerned with the application of computers to the traditional spatial techniques used in cartography and topography. Geomatics emerged from the quantitative revolution in geography in the mid-1950s. Today, geomatics methods include spatial analysis, geographic information systems (GIS), remote sensing, and global positioning systems (GPS). Geomatics has led to a revitalization of some geography departments, especially in Northern America where the subject had a declining status during the 1950s.

Regional geography is concerned with the description of the unique characteristics of a particular region such as its natural or human elements. The main aim is to understand, or define the uniqueness, or character of a particular region that consists of natural as well as human elements. Attention is paid also to regionalization, which covers the proper techniques of space delimitation into regions.


As spatial interrelationships are key to this synoptic science, maps are a key tool. Classical cartography has been joined by a more modern approach to geographical analysis, computer-based geographic information systems (GIS).

In their study, geographers use four interrelated approaches:

Cartography studies the representation of the Earth's surface with abstract symbols (map making). Although other subdisciplines of geography rely on maps for presenting their analyses, the actual making of maps is abstract enough to be regarded separately. Cartography has grown from a collection of drafting techniques into an actual science.

Cartographers must learn cognitive psychology and ergonomics to understand which symbols convey information about the Earth most effectively, and behavioural psychology to induce the readers of their maps to act on the information. They must learn geodesy and fairly advanced mathematics to understand how the shape of the Earth affects the distortion of map symbols projected onto a flat surface for viewing. It can be said, without much controversy, that cartography is the seed from which the larger field of geography grew. Most geographers will cite a childhood fascination with maps as an early sign they would end up in the field.

Geographic information systems (GIS) deal with the storage of information about the Earth for automatic retrieval by a computer, in an accurate manner appropriate to the information's purpose. In addition to all of the other subdisciplines of geography, GIS specialists must understand computer science and database systems. GIS has revolutionized the field of cartography: nearly all mapmaking is now done with the assistance of some form of GIS software. GIS also refers to the science of using GIS software and GIS techniques to represent, analyse, and predict the spatial relationships. In this context, GIS stands for "geographic information science".

Remote sensing is the science of obtaining information about Earth features from measurements made at a distance. Remotely sensed data comes in many forms, such as satellite imagery, aerial photography, and data obtained from hand-held sensors. Geographers increasingly use remotely sensed data to obtain information about the Earth's land surface, ocean, and atmosphere, because it: (a) supplies objective information at a variety of spatial scales (local to global), (b) provides a synoptic view of the area of interest, (c) allows access to distant and inaccessible sites, (d) provides spectral information outside the visible portion of the electromagnetic spectrum, and (e) facilitates studies of how features/areas change over time. Remotely sensed data may be analysed either independently of, or in conjunction with other digital data layers (e.g., in a geographic information system).

Geostatistics deal with quantitative data analysis, specifically the application of statistical methodology to the exploration of geographic phenomena. Geostatistics is used extensively in a variety of fields, including hydrology, geology, petroleum exploration, weather analysis, urban planning, logistics, and epidemiology. The mathematical basis for geostatistics derives from cluster analysis, linear discriminant analysis and non-parametric statistical tests, and a variety of other subjects. Applications of geostatistics rely heavily on geographic information systems, particularly for the interpolation (estimate) of unmeasured points. Geographers are making notable contributions to the method of quantitative techniques.

Geographic qualitative methods, or ethnographical research techniques, are used by human geographers. In cultural geography there is a tradition of employing qualitative research techniques, also used in anthropology and sociology. Participant observation and in-depth interviews provide human geographers with qualitative data.

The oldest known world maps date back to ancient Babylon from the 9th century BC. The best known Babylonian world map, however, is the "Imago Mundi" of 600 BC. The map as reconstructed by Eckhard Unger shows Babylon on the Euphrates, surrounded by a circular landmass showing Assyria, Urartu and several cities, in turn surrounded by a "bitter river" (Oceanus), with seven islands arranged around it so as to form a seven-pointed star. The accompanying text mentions seven outer regions beyond the encircling ocean. The descriptions of five of them have survived. In contrast to the "Imago Mundi", an earlier Babylonian world map dating back to the 9th century BC depicted Babylon as being further north from the center of the world, though it is not certain what that center was supposed to represent.

The ideas of Anaximander (c. 610–545 BC): considered by later Greek writers to be the true founder of geography, come to us through fragments quoted by his successors. Anaximander is credited with the invention of the gnomon, the simple, yet efficient Greek instrument that allowed the early measurement of latitude. Thales is also credited with the prediction of eclipses. The foundations of geography can be traced to the ancient cultures, such as the ancient, medieval, and early modern Chinese. The Greeks, who were the first to explore geography as both art and science, achieved this through Cartography, Philosophy, and Literature, or through Mathematics. There is some debate about who was the first person to assert that the Earth is spherical in shape, with the credit going either to Parmenides or Pythagoras. Anaxagoras was able to demonstrate that the profile of the Earth was circular by explaining eclipses. However, he still believed that the Earth was a flat disk, as did many of his contemporaries. One of the first estimates of the radius of the Earth was made by Eratosthenes.

The first rigorous system of latitude and longitude lines is credited to Hipparchus. He employed a sexagesimal system that was derived from Babylonian mathematics. The meridians were sub-divided into 360°, with each degree further subdivided into 60 (minutes). To measure the longitude at different locations on Earth, he suggested using eclipses to determine the relative difference in time. The extensive mapping by the Romans as they explored new lands would later provide a high level of information for Ptolemy to construct detailed atlases. He extended the work of Hipparchus, using a grid system on his maps and adopting a length of 56.5 miles for a degree.

From the 3rd century onwards, Chinese methods of geographical study and writing of geographical literature became much more comprehensive than what was found in Europe at the time (until the 13th century). Chinese geographers such as Liu An, Pei Xiu, Jia Dan, Shen Kuo, Fan Chengda, Zhou Daguan, and Xu Xiake wrote important treatises, yet by the 17th century advanced ideas and methods of Western-style geography were adopted in China.
During the Middle Ages, the fall of the Roman empire led to a shift in the evolution of geography from Europe to the Islamic world. Muslim geographers such as Muhammad al-Idrisi produced detailed world maps (such as Tabula Rogeriana), while other geographers such as Yaqut al-Hamawi, Abu Rayhan Biruni, Ibn Battuta, and Ibn Khaldun provided detailed accounts of their journeys and the geography of the regions they visited. Turkish geographer, Mahmud al-Kashgari drew a world map on a linguistic basis, and later so did Piri Reis (Piri Reis map). Further, Islamic scholars translated and interpreted the earlier works of the Romans and the Greeks and established the House of Wisdom in Baghdad for this purpose. Abū Zayd al-Balkhī, originally from Balkh, founded the "Balkhī school" of terrestrial mapping in Baghdad. Suhrāb, a late tenth century Muslim geographer accompanied a book of geographical coordinates, with instructions for making a rectangular world map with equirectangular projection or cylindrical equidistant projection.

Abu Rayhan Biruni (976–1048) first described a polar equi-azimuthal equidistant projection of the celestial sphere. He was regarded as the most skilled when it came to mapping cities and measuring the distances between them, which he did for many cities in the Middle East and the Indian subcontinent. He often combined astronomical readings and mathematical equations, in order to develop methods of pin-pointing locations by recording degrees of latitude and longitude. He also developed similar techniques when it came to measuring the heights of mountains, depths of the valleys, and expanse of the horizon. He also discussed human geography and the planetary habitability of the Earth. He also calculated the latitude of Kath, Khwarezm, using the maximum altitude of the Sun, and solved a complex geodesic equation in order to accurately compute the Earth's circumference, which was close to modern values of the Earth's circumference. His estimate of 6,339.9 km for the Earth radius was only 16.8 km less than the modern value of 6,356.7 km. In contrast to his predecessors, who measured the Earth's circumference by sighting the Sun simultaneously from two different locations, al-Biruni developed a new method of using trigonometric calculations, based on the angle between a plain and mountain top, which yielded more accurate measurements of the Earth's circumference, and made it possible for it to be measured by a single person from a single location.
The European Age of Discovery during the 16th and the 17th centuries, where many new lands were discovered and accounts by European explorers such as Christopher Columbus, Marco Polo, and James Cook revived a desire for both accurate geographic detail, and more solid theoretical foundations in Europe. The problem facing both explorers and geographers was finding the latitude and longitude of a geographic location. The problem of latitude was solved long ago but that of longitude remained; agreeing on what zero meridian should be was only part of the problem. It was left to John Harrison to solve it by inventing the chronometer H-4 in 1760, and later in 1884 for the International Meridian Conference to adopt by convention the Greenwich meridian as zero meridian.

The 18th and the 19th centuries were the times when geography became recognized as a discrete academic discipline, and became part of a typical university curriculum in Europe (especially Paris and Berlin). The development of many geographic societies also occurred during the 19th century, with the foundations of the Société de Géographie in 1821, the Royal Geographical Society in 1830, Russian Geographical Society in 1845, American Geographical Society in 1851, and the National Geographic Society in 1888. The influence of Immanuel Kant, Alexander von Humboldt, Carl Ritter, and Paul Vidal de la Blache can be seen as a major turning point in geography from a philosophy to an academic subject.

Over the past two centuries, the advancements in technology with computers have led to the development of geomatics and new practices such as participant observation and geostatistics being incorporated into geography's portfolio of tools. In the West during the 20th century, the discipline of geography went through four major phases: environmental determinism, regional geography, the quantitative revolution, and critical geography. The strong interdisciplinary links between geography and the sciences of geology and botany, as well as economics, sociology and demographics have also grown greatly, especially as a result of earth system science that seeks to understand the world in a holistic view.




 www. nationalgeographic.org

</doc>
<doc id="3280844" url="https://en.wikipedia.org/wiki?curid=3280844" title="Fictional geography">
Fictional geography

Fictional geography is the use of maps, text and imagery to create lands and territories to accompany works of fiction. Depending on the completeness and complexity of the work, varying media, levels of collaboration and a number of other factors, the depiction of geographical components to works of fiction can range from simple drawings of a small area as in "The Twenty-One Balloons" by William Pène du Bois to an entire fictional world as in "The Lord of the Rings" by Tolkien or even an entire galaxy as in "Star Trek" and its variants.

One of the most notable examples of fictional geography is that created by J. R. R. Tolkien to produce the Shire and its expansion to include all of Middle-earth.



</doc>
<doc id="31655501" url="https://en.wikipedia.org/wiki?curid=31655501" title="Hemispheres of Earth">
Hemispheres of Earth

In geography and cartography, the hemispheres of Earth refer to any division of the globe into two hemispheres (from Ancient Greek , meaning "half of a sphere").

The most common such divisions are by latitudinal or longitudinal markers:

The East–West division can also be seen in a cultural sense, as a division into two cultural hemispheres.

However, other schemes have sought to divide the planet in a way that maximizes the preponderance of one geographic feature or another in each division:

Earth may also be split into hemispheres of day and night by the terrestrial terminator.



</doc>
<doc id="55633" url="https://en.wikipedia.org/wiki?curid=55633" title="Region">
Region

In geography, regions are areas that are broadly divided by physical characteristics (physical geography), human impact characteristics (human geography), and the interaction of humanity and the environment (environmental geography). Geographic regions and sub-regions are mostly described by their imprecisely defined, and sometimes transitory boundaries, except in human geography, where jurisdiction areas such as national borders are defined in law.

Apart from the global continental regions, there are also hydrospheric and atmospheric regions that cover the oceans, and discrete climates above the land and water masses of the planet. The land and water global regions are divided into subregions geographically bounded by large geological features that influence large-scale ecologies, such as plains and features.

As a way of describing spatial areas, the concept of regions is important and widely used among the many branches of geography, each of which can describe areas in regional terms. For example, ecoregion is a term used in environmental geography, cultural region in cultural geography, bioregion in biogeography, and so on. The field of geography that studies regions themselves is called regional geography.

In the fields of physical geography, ecology, biogeography, zoogeography, and environmental geography, regions tend to be based on natural features such as ecosystems or biotopes, biomes, drainage basins, natural regions, mountain ranges, soil types. Where human geography is concerned, the regions and subregions are described by the discipline of ethnography.

A region has its own nature that could not be moved. The first nature is its natural environment (landform, climate, etc.). The second nature is its physical elements complex that were built by people in the past. The third nature is its socio-cultural context that could not be replaced by new immigrants.

Global regions distinguishable from space, and are therefore clearly distinguished by the two basic terrestrial environments, land and water. However, they have been generally recognised as such much earlier by terrestrial cartography because of their impact on human geography. They are divided into largest of land regions, known as continents, and the largest of water regions known as oceans. There are also significant regions that do not belong to either classification, such as archipelago regions that are littoral regions, or earthquake regions that are defined in geology.

Continental regions are usually based on broad experiences in human history and attempts to reduce very large areas to more manageable regionalization for the purpose of study. As such they are conceptual constructs, usually lacking distinct boundaries. Oceanic division into maritime regions are used in conjunction with the relationship to the central area of the continent, using directions of the compass.

Some continental regions are defined by the major continental feature of their identity, such as the Amazon basin, or the Sahara, which both occupy a significant percentage of their respective continental land area.

To a large extent, major continental regions are mental constructs created by considering an efficient way to define large areas of the continents. For the most part, the images of the world are derived as much from academic studies, from all types of media, or from personal experience of global exploration. They are a matter of collective human knowledge of its own planet and are attempts to better understand their environments.

Regional geography is a branch of geography that studies regions of all sizes across the Earth. It has a prevailing descriptive character. The main aim is to understand or define the uniqueness or character of a particular region, which consists of natural as well as human elements. Attention is paid also to regionalization, which covers the proper techniques of space delimitation into regions.

Regional geography is also considered as a certain approach to study in geographical sciences (similar to quantitative or critical geographies; for more information, see history of geography).

Human geography is a branch of geography that focuses on the study of patterns and processes that shape human interaction with various discrete environments. It encompasses human, political, cultural, social, and economic aspects among others that are often clearly delineated. While the major focus of human geography is not the physical landscape of the Earth (see physical geography), it is hardly possible to discuss human geography without referring to the physical landscape on which human activities are being played out, and environmental geography is emerging as a link between the two. Regions of human geography can be divided into many broad categories:

The field of historical geography involves the study of human history as it relates to places and regions or the study of how places and regions have changed over time.

D. W. Meinig, a historical geographer of America, describes many historical regions in his book "The Shaping of America: A Geographical Perspective on 500 Years of History". For example, in identifying European "source regions" in early American colonization efforts, he defines and describes the "Northwest European Atlantic Protestant Region", which includes sub-regions such as the "Western Channel Community", which itself is made of sub-regions such as the "English West Country" of Cornwall, Devon, Somerset, and Dorset.

In describing historic regions of America, Meinig writes of "The Great Fishery" off the coast of Newfoundland and New England, an oceanic region that includes the Grand Banks. He rejects regions traditionally used in describing American history, like New France, "West Indies", the Middle Colonies, and the individual colonies themselves (Province of Maryland, for example). Instead he writes of "discrete colonization areas," which may be named after colonies but rarely adhere strictly to political boundaries. Historic regions of this type Meinig writes about include "Greater New England" and its major sub-regions of "Plymouth," "New Haven shores" (including parts of Long Island), "Rhode Island" (or "Narragansett Bay"), "the Piscataqua," "Massachusetts Bay," "Connecticut Valley," and to a lesser degree, regions in the sphere of influence of Greater New England, "Acadia" (Nova Scotia), "Newfoundland and The Fishery/The Banks."

Other examples of historical regions include Iroquoia, Ohio Country, Illinois Country, and Rupert's Land.

A tourism region is a geographical region that has been designated by a governmental organization or tourism bureau as having common cultural or environmental characteristics. These regions are often named after a geographical, former, or current administrative region or may have a name created for tourism purposes. The names often evoke certain positive qualities of the area and suggest a coherent tourism experience to visitors. Countries, states, provinces, and other administrative regions are often carved up into tourism regions to facilitate attracting visitors.

Some of the more famous tourism regions based on historical or current administrative regions include Tuscany in Italy and Yucatán in Mexico. Famous examples of regions created by a government or tourism bureau include the United Kingdom's Lake District and California's Wine Country.
great plains region

Natural resources often occur in distinct regions. Natural resource regions can be a topic of physical geography or environmental geography, but also have a strong element of human geography and economic geography. A coal region, for example, is a physical or geomorphological region, but its development and exploitation can make it into an economic and a cultural region. Some examples of natural resource regions include the Rumaila Field, the oil field that lies along the border or Iraq and Kuwait and played a role in the Gulf War; the Coal Region of Pennsylvania, which is a historical region as well as a cultural, physical, and natural resource region; the South Wales Coalfield, which like Pennsylvania's coal region is a historical, cultural, and natural region; the Kuznetsk Basin, a similarly important coal mining region in Russia; Kryvbas, the economic and iron ore mining region of Ukraine; and the James Bay Project, a large region of Quebec where one of the largest hydroelectric systems in the world has been developed.

Sometimes a region associated with a religion is given a name, like Christendom, a term with medieval and renaissance connotations of Christianity as a sort of social and political polity. The term Muslim world is sometimes used to refer to the region of the world where Islam is dominant. These broad terms are very vague when used to describe regions.

Within some religions there are clearly defined regions. The Roman Catholic Church, the Church of England, the Eastern Orthodox Church, and others, define ecclesiastical regions with names such as diocese, eparchy, ecclesiastical provinces, and parish.

For example, the United States is divided into 32 Roman Catholic ecclesiastical provinces. The Lutheran Church–Missouri Synod is organized into 33 geographic "districts", which are subdivided into "circuits" (the Atlantic District (LCMS), for example). The Church of Jesus Christ of Latter-day Saints uses regions similar to dioceses and parishes, but uses terms like ward and stake.

In the field of political geography, regions tend to be based on political units such as sovereign states; subnational units such as administrative regions, provinces, states (in the United States), counties, townships, territories, etc.; and multinational groupings, including formally defined units such as the European Union, the Association of Southeast Asian Nations, and NATO, as well as informally defined regions such as the Third World, Western Europe, and the Middle East.

The word "region" is taken from the Latin "regio" (derived from "regere", to rule), and a number of countries have borrowed the term as the formal name for a type of subnational entity (e.g., the "región", used in Chile). In English, the word is also used as the conventional translation for equivalent terms in other languages (e.g., the "область" ("oblast"), used in Russia alongside a broader term "регион").

The following countries use the term "region" (or its cognate) as the name of a type of subnational administrative unit:
The Canadian province of Québec also uses the "administrative region" ("région administrative").

Scotland had local government regions from 1975 to 1996.

In Spain the official name of the autonomous community of Murcia is "Región de Murcia". Also, some single-province autonomous communities such as Madrid use the term "región" interchangeably with "comunidad autónoma".

Two län (counties) in Sweden are officially called 'regions': Skåne and Västra Götaland, and there is currently a controversial proposal to divide the rest of Sweden into large regions, replacing the current counties.

The government of the Philippines uses the term "region" (in Filipino, "rehiyon") when it's necessary to group provinces, the primary administrative subdivision of the country. This is also the case in Brazil, which groups its primary administrative divisions ("estados"; "states") into "grandes regiões" (greater regions) for statistical purposes, while Russia uses "экономические районы" (economic regions) in a similar way, as does Romania and Venezuela.

The government of Singapore makes use of the term "region" for its own administrative purposes.

The following countries use an administrative subdivision conventionally referred to as a region in English:

China has five 自治区 ("zìzhìqū") and two 特別行政區 (or 特别行政区; "tèbiéxíngzhèngqū"), which are translated as "autonomous region" and "special administrative region", respectively.

There are many relatively small regions based on local government agencies such as districts, agencies, or regions. In general, they are all regions in the general sense of being bounded spatial units. Examples include electoral districts such as Washington's 6th congressional district and Tennessee's 1st congressional district; school districts such as Granite School District and Los Angeles Unified School District; economic districts such as the Reedy Creek Improvement District; metropolitan areas such as the Seattle metropolitan area, and metropolitan districts such as the Metropolitan Water Reclamation District of Greater Chicago, the Las Vegas-Clark County Library District, the Metropolitan Police Service of Greater London, as well as other local districts like the York Rural Sanitary District, the Delaware River Port Authority, the Nassau County Soil and Water Conservation District, and C-TRAN.

The traditional territorial divisions of some countries are also commonly rendered in English as "regions". These informal divisions do not form the basis of the modern administrative divisions of these countries, but still define and delimit local regional identity and sense of belonging. Examples include:


Functional regions are usually understood to be the areas organised by the horizontal functional relations (flows, interactions) that are maximised within a region and minimised across its borders so that the principles of internal cohesiveness and external separation regarding spatial interactions are met (see, for instance, Farmer and Fotheringham, 2011; Klapka, Halas, 2016; Smart, 1974). A functional region is not an abstract spatial concept, but to a certain extent it can be regarded as a reflection of the spatial behaviour of individuals in a geographic space. 
The functional region is conceived as a general concept while its inner structure, inner spatial flows, and interactions need not necessarily show any regular pattern, only selfcontainment. The concept of self-containment remains the only crucial defining characteristic of a functional region. Nodal regions, functional urban regions, daily urban systems, local labour-market areas (LLMAs), or travel-to-work areas (TTWAs) are considered to be special instances of a general functional region that need to fulfil some specific conditions regarding, for instance, the character of the region-organising interaction or the presence of urban cores, (Halas et al., 2015).

In military usage, a region is shorthand for the name of a military formation larger than an Army Group and smaller than an Army Theater or simply Theater. The full name of the military formation is Army Region. The size of an Army Region can vary widely but is generally somewhere between about 1 million and 3 million soldiers. Two or more Army Regions could make up an Army Theater. An Army Region is typically commanded by a full General (US four stars), a Field Marshal, or General of the Army (US five stars), or Generalissimo (Soviet Union), in the US Armed Forces, an Admiral may also command a region. Due to the large size of this formation, its use is rarely employed. Some of the very few examples of an Army Region are each of the Eastern, Western, and southern (mostly in Italy) fronts in Europe during World War II. The military map unit symbol for this echelon of formation (see Military organization and APP-6A) consists of six Xs.

Media geography is a spatio-temporal understanding, brought through different gadgets of media, nowadays, media became inevitable at different proportions and everyone supposed to consumed at different gravity. The spatial attributes are studied with the help of media outputs in shape of images which are contested in nature and pattern as well where politics is inseparable. Media geography is giving spatial understanding of mediated image.





</doc>
<doc id="3190431" url="https://en.wikipedia.org/wiki?curid=3190431" title="Spatial analysis">
Spatial analysis

Complex issues arise in spatial analysis, many of which are neither clearly defined nor completely resolved, but form the basis for current research. The most fundamental of these is the problem of defining the spatial location of the entities being studied.

Classification of the techniques of spatial analysis is difficult because of the large number of different fields of research involved, the different fundamental approaches which can be chosen, and the many forms the data can take.

Spatial analysis can perhaps be considered to have arisen with early attempts at cartography and surveying but many fields have contributed to its rise in modern form. Biology contributed through botanical studies of global plant distributions and local plant locations, ethological studies of animal movement, landscape ecological studies of vegetation blocks, ecological studies of spatial population dynamics, and the study of biogeography. Epidemiology contributed with early work on disease mapping, notably John Snow's work of mapping an outbreak of cholera, with research on mapping the spread of disease and with location studies for health care delivery. Statistics has contributed greatly through work in spatial statistics. Economics has contributed notably through spatial econometrics. Geographic information system is currently a major contributor due to the importance of geographic software in the modern analytic toolbox. Remote sensing has contributed extensively in morphometric and clustering analysis. Computer science has contributed extensively through the study of algorithms, notably in computational geometry. Mathematics continues to provide the fundamental tools for analysis and to reveal the complexity of the spatial realm, for example, with recent work on fractals and scale invariance. Scientific modelling provides a useful framework for new approaches.

Spatial analysis confronts many fundamental issues in the definition of its objects of study, in the construction of the analytic operations to be used, in the use of computers for analysis, in the limitations and particularities of the analyses which are known, and in the presentation of analytic results. Many of these issues are active subjects of modern research.

Common errors often arise in spatial analysis, some due to the mathematics of space, some due to the particular ways data are presented spatially, some due to the tools which are available. Census data, because it protects individual privacy by aggregating data into local units, raises a number of statistical issues. The fractal nature of coastline makes precise measurements of its length difficult if not impossible. A computer software fitting straight lines to the curve of a coastline, can easily calculate the lengths of the lines which it defines. However these straight lines may have no inherent meaning in the real world, as was shown for the coastline of Britain.

These problems represent a challenge in spatial analysis because of the power of maps as media of presentation. When results are presented as maps, the presentation combines spatial data which are generally accurate with analytic results which may be inaccurate, leading to an impression that analytic results are more accurate than the data would indicate.

The definition of the spatial presence of an entity constrains the possible analysis which can be applied to that entity and influences the final conclusions that can be reached. While this property is fundamentally true of all analysis, it is particularly important in spatial analysis because the tools to define and study entities favor specific characterizations of the entities being studied. Statistical techniques favor the spatial definition of objects as points because there are very few statistical techniques which operate directly on line, area, or volume elements. Computer tools favor the spatial definition of objects as homogeneous and separate elements because of the limited number of database elements and computational structures available, and the ease with which these primitive structures can be created.

Spatial dependency is the co-variation of properties within geographic space: characteristics at proximal locations appear to be correlated, either positively or negatively. Spatial dependency leads to the spatial autocorrelation problem in statistics since, like temporal autocorrelation, this violates standard statistical techniques that assume independence among observations. For example, regression analyses that do not compensate for spatial dependency can have unstable parameter estimates and yield unreliable significance tests. Spatial regression models (see below) capture these relationships and do not suffer from these weaknesses. It is also appropriate to view spatial dependency as a source of information rather than something to be corrected.

Locational effects also manifest as spatial heterogeneity, or the apparent variation in a process with respect to location in geographic space. Unless a space is uniform and boundless, every location will have some degree of uniqueness relative to the other locations. This affects the spatial dependency relations and therefore the spatial process. Spatial heterogeneity means that overall parameters estimated for the entire system may not adequately describe the process at any given location.

Spatial measurement scale is a persistent issue in spatial analysis; more detail is available at the modifiable areal unit problem (MAUP) topic entry. Landscape ecologists developed a series of scale invariant metrics for aspects of ecology that are fractal in nature. In more general terms, no scale independent method of analysis is widely agreed upon for spatial statistics.

Spatial sampling involves determining a limited number of locations in geographic space for faithfully measuring phenomena that are subject to dependency and heterogeneity. Dependency suggests that since one location can predict the value of another location, we do not need observations in both places. But heterogeneity suggests that this relation can change across space, and therefore we cannot trust an observed degree of dependency beyond a region that may be small. Basic spatial sampling schemes include random, clustered and systematic. These basic schemes can be applied at multiple levels in a designated spatial hierarchy (e.g., urban area, city, neighborhood). It is also possible to exploit ancillary data, for example, using property values as a guide in a spatial sampling scheme to measure educational attainment and income. Spatial models such as autocorrelation statistics, regression and interpolation (see below) can also dictate sample design.

The fundamental issues in spatial analysis lead to numerous problems in analysis including bias, distortion and outright errors in the conclusions reached. These issues are often interlinked but various attempts have been made to separate out particular issues from each other.

In discussing the coastline of Britain, Benoit Mandelbrot showed that certain spatial concepts are inherently nonsensical despite presumption of their validity. Lengths in ecology depend directly on the scale at which they are measured and experienced. So while surveyors commonly measure the length of a river, this length only has meaning in the context of the relevance of the measuring technique to the question under study.
The locational fallacy refers to error due to the particular spatial characterization chosen for the elements of study, in particular choice of placement for the spatial presence of the element.

Spatial characterizations may be simplistic or even wrong. Studies of humans often reduce the spatial existence of humans to a single point, for instance their home address. This can easily lead to poor analysis, for example, when considering disease transmission which can happen at work or at school and therefore far from the home.

The spatial characterization may implicitly limit the subject of study. For example, the spatial analysis of crime data has recently become popular but these studies can only describe the particular kinds of crime which can be described spatially. This leads to many maps of assault but not to any maps of embezzlement with political consequences in the conceptualization of crime and the design of policies to address the issue.

This describes errors due to treating elements as separate 'atoms' outside of their spatial context. The fallacy is about transferring individual conclusions to spatial units.

The ecological fallacy describes errors due to performing analyses on aggregate data when trying to reach conclusions on the individual units. Errors occur in part from spatial aggregation. For example, a pixel represents the average surface temperatures within an area. Ecological fallacy would be to assume that all points within the area have the same temperature. This topic is closely related to the modifiable areal unit problem.

A mathematical space exists whenever we have a set of observations and quantitative measures of their attributes. For example, we can represent individuals' incomes or years of education within a coordinate system where the location of each individual can be specified with respect to both dimensions. The distance between individuals within this space is a quantitative measure of their differences with respect to income and education. However, in spatial analysis, we are concerned with specific types of mathematical spaces, namely, geographic space. In geographic space, the observations correspond to locations in a spatial measurement framework that capture their proximity in the real world. The locations in a spatial measurement framework often represent locations on the surface of the Earth, but this is not strictly necessary. A spatial measurement framework can also capture proximity with respect to, say, interstellar space or within a biological entity such as a liver. The fundamental tenet is Tobler's First Law of Geography: if the interrelation between entities increases with proximity in the real world, then representation in geographic space and assessment using spatial analysis techniques are appropriate.

The Euclidean distance between locations often represents their proximity, although this is only one possibility. There are an infinite number of distances in addition to Euclidean that can support quantitative analysis. For example, "Manhattan" (or "Taxicab") distances where movement is restricted to paths parallel to the axes can be more meaningful than Euclidean distances in urban settings. In addition to distances, other geographic relationships such as connectivity (e.g., the existence or degree of shared borders) and direction can also influence the relationships among entities. It is also possible to compute minimal cost paths across a cost surface; for example, this can represent proximity among locations when travel must occur across rugged terrain.

 Spatial data comes in many varieties and it is not easy to
Urban and Regional Studies deal with large tables of spatial data obtained from censuses and surveys. It is necessary to simplify the huge amount of detailed information in order to extract the main trends. Multivariable analysis (or Factor analysis, FA) allows a change of variables, transforming the many variables of the census, usually correlated between themselves, into fewer independent "Factors" or "Principal Components" which are, actually, the eigenvectors of the data correlation matrix weighted by the inverse of their eigenvalues. This change of variables has two main advantages:

Factor analysis depends on measuring distances between observations : the choice of a significant metric is crucial. The Euclidean metric (Principal Component Analysis), the Chi-Square distance (Correspondence Analysis) or the Generalized Mahalanobis distance (Discriminant Analysis ) are among the more widely used. More complicated models, using communalities or rotations have been proposed.

Using multivariate methods in spatial analysis began really in the 1950s (although some examples go back to the beginning of the century) and culminated in the 1970s, with the increasing power and accessibility of computers. Already in 1948, in a seminal publication, two sociologists, Wendell Bell and Eshref Shevky, had shown that most city populations in the US and in the world could be represented with three independent factors : 1- the « socio-economic status » opposing rich and poor districts and distributed in sectors running along highways from the city center, 2- the « life cycle », i.e. the age structure of households, distributed in concentric circles, and 3- « race and ethnicity », identifying patches of migrants located within the city. In 1961, in a groundbreaking study, British geographers used FA to classify British towns. Brian J Berry, at the University of Chicago, and his students made a wide use of the method, applying it to most important cities in the world and exhibiting common social structures. 
The use of Factor Analysis in Geography, made so easy by modern computers, has been very wide but not always very wise.

Since the vectors extracted are determined by the data matrix, it is not possible to compare factors obtained from different censuses. A solution consists in fusing together several census matrices in a unique table which, then, may be analyzed. This, however, assumes that the definition of the variables has not changed over time and produces very large tables, difficult to manage. A better solution, proposed by psychometricians, groups the data in a « cubic matrix », with three entries (for instance, locations, variables, time periods). A Three-Way Factor Analysis produces then three groups of factors related by a small cubic « core matrix ». This method, which exhibits data evolution over time, has not been widely used in geography. In Los Angeles, however, it has exhibited the role, traditionally ignored, of Downtown as an organizing center for the whole city during several decades.

Spatial autocorrelation statistics measure and analyze the degree of dependency among observations in a geographic space. Classic spatial autocorrelation statistics include Moran's formula_1, Geary's formula_2, Getis's formula_3 and the standard deviational ellipse. These statistics require measuring a spatial weights matrix that reflects the intensity of the geographic relationship between observations in a neighborhood, e.g., the distances between neighbors, the lengths of shared border, or whether they fall into a specified directional class such as "west". Classic spatial autocorrelation statistics compare the spatial weights to the covariance relationship at pairs of locations. Spatial autocorrelation that is more positive than expected from random indicate the clustering of similar values across geographic space, while significant negative spatial autocorrelation indicates that neighboring values are more dissimilar than expected by chance, suggesting a spatial pattern similar to a chess board.

Spatial autocorrelation statistics such as Moran's formula_1 and Geary's formula_2 are global in the sense that they estimate the overall degree of spatial autocorrelation for a dataset. The possibility of spatial heterogeneity suggests that the estimated degree of autocorrelation may vary significantly across geographic space. Local spatial autocorrelation statistics provide estimates disaggregated to the level of the spatial analysis units, allowing assessment of the dependency relationships across space. formula_3 statistics compare neighborhoods to a global average and identify local regions of strong autocorrelation. Local versions of the formula_1 and formula_2 statistics are also available.

Spatial stratified heterogeneity, referring to the within-strata variance less than the between strata-variance, is ubiquitous in ecological phenomena, such as ecological zones and many ecological variables. Spatial stratified heterogeneity of an attribute can be measured by geographical detector "q"-statistic:
where a population is partitioned into "h" = 1, ..., "L" strata; "N" stands for the size of the population, σ stands for variance of the attribute. The value of "q" is within [0, 1], 0 indicates no spatial stratified heterogeneity, 1 indicates perfect spatial stratified heterogeneity.The value of "q" indicates the percent of the variance of an attribute explained by the stratification.The "q" follows a noncentral "F" probability density function.

Spatial interpolation methods estimate the variables at unobserved locations in geographic space based on the values at observed locations. Basic methods include inverse distance weighting: this attenuates the variable with decreasing proximity from the observed location. Kriging is a more sophisticated method that interpolates across space according to a spatial lag relationship that has both systematic and random components. This can accommodate a wide range of spatial relationships for the hidden values between observed locations. Kriging provides optimal estimates given the hypothesized lag relationship, and error estimates can be mapped to determine if spatial patterns exist.

Spatial regression methods capture spatial dependency in regression analysis, avoiding statistical problems such as unstable parameters and unreliable significance tests, as well as providing information on spatial relationships among the variables involved. Depending on the specific technique, spatial dependency can enter the regression model as relationships between the independent variables and the dependent, between the dependent variables and a spatial lag of itself, or in the error terms. Geographically weighted regression (GWR) is a local version of spatial regression that generates parameters disaggregated by the spatial units of analysis. This allows assessment of the spatial heterogeneity in the estimated relationships between the independent and dependent variables. The use of Bayesian hierarchical modeling in conjunction with Markov chain Monte Carlo (MCMC) methods have recently shown to be effective in modeling complex relationships using Poisson-Gamma-CAR, Poisson-lognormal-SAR, or Overdispersed logit models. Statistical packages for implementing such Bayesian models using MCMC include WinBugs and CrimeStat.

Spatial stochastic processes, such as Gaussian processes are also increasingly being deployed in spatial regression analysis. Model-based versions of GWR, known as spatially varying coefficient models have been applied to conduct Bayesian inference. Spatial stochastic process can become computationally effective and scalable Gaussian process models, such as Gaussian Predictive Processes and Nearest Neighbor Gaussian Processes (NNGP).

Spatial interaction or "gravity models" estimate the flow of people, material or information between locations in geographic space. Factors can include origin propulsive variables such as the number of commuters in residential areas, destination attractiveness variables such as the amount of office space in employment areas, and proximity relationships between the locations measured in terms such as driving distance or travel time. In addition, the topological, or connective, relationships between areas must be identified, particularly considering the often conflicting relationship between distance and topology; for example, two spatially close neighborhoods may not display any significant interaction if they are separated by a highway. After specifying the functional forms of these relationships, the analyst can estimate model parameters using observed flow data and standard estimation techniques such as ordinary least squares or maximum likelihood. Competing destinations versions of spatial interaction models include the proximity among the destinations (or origins) in addition to the origin-destination proximity; this captures the effects of destination (origin) clustering on flows. Computational methods such as artificial neural networks can also estimate spatial interaction relationships among locations and can handle noisy and qualitative data.

Spatial interaction models are aggregate and top-down: they specify an overall governing relationship for flow between locations. This characteristic is also shared by urban models such as those based on mathematical programming, flows among economic sectors, or bid-rent theory. An alternative modeling perspective is to represent the system at the highest possible level of disaggregation and study the bottom-up emergence of complex patterns and relationships from behavior and interactions at the individual level. 

Complex adaptive systems theory as applied to spatial analysis suggests that simple interactions among proximal entities can lead to intricate, persistent and functional spatial entities at aggregate levels. Two fundamentally spatial simulation methods are cellular automata and agent-based modeling. Cellular automata modeling imposes a fixed spatial framework such as grid cells and specifies rules that dictate the state of a cell based on the states of its neighboring cells. As time progresses, spatial patterns emerge as cells change states based on their neighbors; this alters the conditions for future time periods. For example, cells can represent locations in an urban area and their states can be different types of land use. Patterns that can emerge from the simple interactions of local land uses include office districts and urban sprawl. Agent-based modeling uses software entities (agents) that have purposeful behavior (goals) and can react, interact and modify their environment while seeking their objectives. Unlike the cells in cellular automata, simulysts can allow agents to be mobile with respect to space. For example, one could model traffic flow and dynamics using agents representing individual vehicles that try to minimize travel time between specified origins and destinations. While pursuing minimal travel times, the agents must avoid collisions with other vehicles also seeking to minimize their travel times. Cellular automata and agent-based modeling are complementary modeling strategies. They can be integrated into a common geographic automata system where some agents are fixed while others are mobile.

Calibration plays a pivotal role in both CA and ABM simulation and modelling approaches. Initial approaches to CA proposed robust calibration approaches based on stochastic, Monte Carlo methods. ABM approaches rely on agents' decision rules (in many cases extracted from qualitative research base methods such as questionnaires). Recent Machine Learning Algorithms calibrate using training sets, for instance in order to understand the qualities of the built environment.

Spatial analysis of a conceptual geological model is the main purpose of any MPS algorithm. The method analyzes the spatial statistics of the geological model, called the training image, and generates realizations of the phenomena that honor those input multiple-point statistics.

A recent MPS algorithm used to accomplish this task is the pattern-based method by Honarkhah. In this method, a distance-based approach is employed to analyze the patterns in the training image. This allows the reproduction of the multiple-point statistics, and the complex geometrical features of the training image. Each output of the MPS algorithm is a realization that represents a random field. Together, several realizations may be used to quantify spatial uncertainty.

One of the recent methods is presented by Tahmasebi et al. uses a cross-correlation function to improve the spatial pattern reproduction. They call their MPS simulation method as the CCSIM algorithm. This method is able to quantify the spatial connectivity, variability and uncertainty. Furthermore, the method is not sensitive to any type of data and is able to simulate both categorical and continuous scenarios. CCSIM algorithm is able to be used for any stationary, non-stationary and multivariate systems and it can provide high quality visual appeal model.,

Geospatial analysis, or just spatial analysis, is an approach to applying statistical analysis and other analytic techniques to data which has a geographical or spatial aspect. Such analysis would typically employ software capable of rendering maps processing spatial data, and applying analytical methods to terrestrial or geographic datasets, including the use of geographic information systems and geomatics.

Geographic information systems (GIS) — a large domain that provides a variety of capabilities designed to capture, store, manipulate, analyze, manage, and present all types of geographical data — utilizes geospatial analysis in a variety of contexts, operations and applications.

Geospatial analysis, using GIS, was developed for problems in the environmental and life sciences, in particular ecology, geology and epidemiology. It has extended to almost all industries including defense, intelligence, utilities, Natural Resources (i.e. Oil and Gas, Forestry ... etc.), social sciences, medicine and Public Safety (i.e. emergency management and criminology), disaster risk reduction and management (DRRM), and climate change adaptation (CCA). Spatial statistics typically result primarily from observation rather than experimentation.

Vector-based GIS is typically related to operations such as map overlay (combining two or more maps or map layers according to predefined rules), simple buffering (identifying regions of a map within a specified distance of one or more features, such as towns, roads or rivers) and similar basic operations. This reflects (and is reflected in) the use of the term spatial analysis within the Open Geospatial Consortium (OGC) “simple feature specifications”. For raster-based GIS, widely used in the environmental sciences and remote sensing, this typically means a range of actions applied to the grid cells of one or more maps (or images) often involving filtering and/or algebraic operations (map algebra). These techniques involve processing one or more raster layers according to simple rules resulting in a new map layer, for example replacing each cell value with some combination of its neighbours’ values, or computing the sum or difference of specific attribute values for each grid cell in two matching raster datasets. Descriptive statistics, such as cell counts, means, variances, maxima, minima, cumulative values, frequencies and a number of other measures and distance computations are also often included in this generic term spatial analysis. Spatial analysis includes a large variety of statistical techniques (descriptive, exploratory, and explanatory statistics) that apply to data that vary spatially and which can vary over time. Some more advanced statistical techniques include Getis-ord Gi* or Anselin Local Moran's I which are used to determine clustering patterns of spatially referenced data.

Geospatial analysis goes beyond 2D and 3D mapping operations and spatial statistics. It includes: 

Traditionally geospatial computing has been performed primarily on personal computers (PCs) or servers. Due to the increasing capabilities of mobile devices, however, geospatial computing in mobile devices is a fast-growing trend. The portable nature of these devices, as well as the presence of useful sensors, such as Global Navigation Satellite System (GNSS) receivers and barometric pressure sensors, make them useful for capturing and processing geospatial information in the field. In addition to the local processing of geospatial information on mobile devices, another growing trend is cloud-based geospatial computing. In this architecture, data can be collected in the field using mobile devices and then transmitted to cloud-based servers for further processing and ultimate storage. In a similar manner, geospatial information can be made available to connected mobile devices via the cloud, allowing access to vast databases of geospatial information anywhere where a wireless data connection is available.

Geographic information systems (GIS) and the underlying geographic information science that advances these technologies have a strong influence on spatial analysis. The increasing ability to capture and handle geographic data means that spatial analysis is occurring within increasingly data-rich environments. Geographic data capture systems include remotely sensed imagery, environmental monitoring systems such as intelligent transportation systems, and location-aware technologies such as mobile devices that can report location in near-real time. GIS provide platforms for managing these data, computing spatial relationships such as distance, connectivity and directional relationships between spatial units, and visualizing both the raw data and spatial analytic results within a cartographic context. Subtypes include:









</doc>
<doc id="17351446" url="https://en.wikipedia.org/wiki?curid=17351446" title="Geographical feature">
Geographical feature

Geographical features are naturally-created features of the earth. Natural geographical features consist of landforms and ecosystems. For example, terrain types, (physical factors of the environment) are natural geographical features. Conversely, human settlements or other engineered forms are considered cheeseburger of artificial geographical features.

There are two different terms to describe habitats: ecosystem and biome. An ecosystem is a community of organisms. In contrast, biomes occupy large areas of the globe and often encompass many different kinds of geographical features, including mountain ranges.

Biotic diversity within an ecosystem is the variability among living organisms from all sources, including "inter alia", terrestrial, marine and other aquatic ecosystems. Living organisms are continually engaged in a set of relationships with every other element constituting the environment in which they exist, and ecosystem describes any situation where there is relationship between organisms and their environment.

Biomes represent large areas of ecologically similar communities of plants, animals, and soil organisms. Biomes are defined based on factors such as plant structures (such as trees, shrubs, and grasses), leaf types (such as broadleaf and needleleaf), plant spacing (forest, woodland, savanna), and climate. Unlike ecozones, biomes are not defined by genetic, taxonomic, or historical similarities. Biomes are often identified with particular patterns of ecological succession and climax vegetation.

A landform comprises a geomorphological unit and is largely defined by its surface form and location in the landscape, as part of the terrain, and as such is typically an element of topography. Landforms are categorized by features such as elevation, slope, orientation, stratification, rock exposure, and soil type. They include berms, mounds, hills, cliffs, valleys, rivers, and numerous other elements. Oceans and continents are the highest-order landforms.

A body of water is any significant accumulation of water, usually covering the Earth. The term "body of water" most often refers to oceans, seas, and lakes, but it may also include smaller pools of water such as ponds, creeks or wetlands. Rivers, streams, canals, and other geographical features where water moves from one place to another are not always considered bodies of water, but they are included as geographical formations featuring water.

A settlement is a permanent or temporary community in which people live. Settlements range in components from a small number of dwellings grouped together to the largest of cities with surrounding urbanized areas. Other landscape features such as roads, enclosures, field systems, boundary banks and ditches, ponds, parks and woods, mills, manor houses, moats, and churches may be considered part of a settlement.

Engineered geographic features include highways, bridges, airports, railroads, buildings, dams, and reservoirs, and are part of the anthroposphere because they are man-made geographic features.

Cartographic features are types of abstract geographical features, which appear on maps but not on the planet itself, even though they are located on the planet. For example, latitudes, longitudes, the Equator, and the Prime Meridian are shown on maps of the Earth, but it do not physically exist. It is a theoretical line used for reference, navigation, and measurement.



</doc>
<doc id="31346116" url="https://en.wikipedia.org/wiki?curid=31346116" title="Geographical cluster">
Geographical cluster

A geographical cluster is a localised anomaly, usually an excess of something given the distribution or variation of something else. Often it is considered as an incidence rate that is unusual in that there is more of some variable than might be expected. Examples would include: a local excess disease rate, a crime hot spot, areas of high unemployment, accident blackspots, unusually high positive residuals from a model, high concentrations of flora or fauna, physical features or events like earthquake epicenters etc... 

Identifying these extreme regions may be useful in that there could be implicit geographical associations with other variables that can be identified and would be of interest. Pattern detection via the identification of such geographical clusters is a very simple and generic form of geographical analysis that has many applications in many different contexts. The emphasis is on localized clustering or patterning because this may well contain the most useful information. 

A geographical cluster is different from a high concentration as it is generally second order, involving the factoring in of the distribution of something else. 

Identifying geographical clusters can be an important stage in a geographical analysis. Mapping the locations of unusual concentrations may help identify causes of these. Some techniques include the Geographical Analysis Machine and Besag and Newell's cluster detection method.


</doc>
<doc id="53452291" url="https://en.wikipedia.org/wiki?curid=53452291" title="Geography of aging">
Geography of aging

Geography of aging or gerontological geography is an emerging field of knowledge of Human Geography that analyzes the socio-spatial implications of aging of the population from the understanding of the relationships between the physical-social environment and the elderly, at different scales, micro (City, region, country), etc.

Since the 1970s in a number of developed countries such as the United States, Canada, the United Kingdom, Germany, Sweden, France, Spain, Australia, New Zealand and Japan, there have been increasing studies focusing on the understanding of spatial patterns of aging population, as well as aspects related to residential changes and provision of health and social services. Among the geographers of aging is S. Harper, who identified the phenomenon of aging associated with the social construction of old age and the processes of residential mobility of this group to the urban periphery, mainly nursing homes and sheltered housing.

The contribution of geographers of aging, such as Graham D. Rowles, SM. Golant, S. Harper, G. Laws, are contributing to environmental gerontology by understanding the environmental aspects of gerontology in developed and developing countries. Also in Spain, some geographers, such as Gloria Fernández-Mayoralas, Fermina Rojo-Pérez and Vicente Rodríguez-Rodríguez, have made outstanding contributions to the study of residential strategies, access to health services, and, in general, quality of Life of the elderly, as well as the impacts of Northern European retirees on the Costa del Sol (Spain).

In Latin America and Spain, Diego Sánchez-González has shed light on the deepening of issues such as the physical-built and social environment and the quality of life of the elderly; the importance of the natural environment (therapeutic natural landscape) on active and healthy aging in the place; residential strategies for the maintenance of the elderly in the communities; the socio-environmental vulnerability of the elderly in the face of climate change; as well as issues related to the attachment to the place (identity and public space); elderly people with disabilities and social exclusion; leisure and tourism of elderly; and the planning of gerontological and geriatric services.



</doc>
<doc id="57078824" url="https://en.wikipedia.org/wiki?curid=57078824" title="Earth section paths">
Earth section paths

Earth section paths are paths on the earth defined by the intersection of a reference ellipsoid and a plane. Common examples of earth sections include the great ellipse and normal sections. This page provides a unifying approach to all earth sections and their associated geodetic problems.

The indirect problem for earth sections is: given two points, formula_1 and formula_2 on the surface of the reference ellipsoid, find the length, formula_3, of the short arc of a spheroid section from formula_1 to formula_2 and also find the departure and arrival (true north referenced) azimuths of that curve, formula_6 and formula_7. Let formula_8 have geodetic latitude formula_9 and longitude formula_10 (k=1,2). This problem is best solved using analytic geometry in ECEF coordinates.
Let formula_11 and formula_12 be the ECEF coordinates of the two points, computed using the geodetic to ECEF transformations discussed here.

To define the section plane select any third point formula_13 not on the line from formula_14 to formula_15. Choosing formula_13 to be on the surface normal at formula_1 will define the normal section at formula_1. If formula_13 is the origin then the earth section is the great ellipse. (The origin would be co-linear with 2 antipodal points so a different point must be used in that case). Since there are infinitely many choices for formula_13, the above problem is really a class of problems (one for each plane). Let formula_13 be given. To put the equation of the plane into the standard form, formula_22, where formula_23, requires the components of a unit vector, formula_24, normal to the section plane. These components may be computed as follows: The vector from formula_13 to formula_14 has components formula_27, and the vector from formula_14 to formula_15 has components formula_30. Therefore, formula_31 = formula_32×formula_33), where formula_34 is the unit vector in the direction of formula_35. The orientation convention used here is that formula_31 points to the left of the path. If this is not the case then redefine formula_37 = -formula_37. Finally, the parameter d for the plane may be computed using the dot product of formula_31 with a vector from the origin to any point on the plane, such as formula_14, i.e. d = formula_41. The equation of the plane (in vector form) is thus formula_31 ⋅ formula_43 = d, where formula_43 is the position vector of (x, y, z).

Examination of the ENU to ECEF transformation reveals that the ECEF coordinates of a unit vector pointing east at any point on the ellipsoid is: formula_45=formula_46, a unit vector pointing north is formula_47=formula_48, and a unit vector pointing up is formula_49=formula_50. A vector tangent to the path is:
formula_51 so the east component of formula_52 is formula_53, and the north component is formula_54. Therefore, the azimuth may be obtained from a two-argument arctangent function, formula_55=formula_56. Use this method at both formula_1 and formula_2 to get formula_6 and formula_7.

The (non-trivial) intersection of a plane and ellipsoid is an ellipse. Therefore, the arc length, formula_3, on the section path from formula_1 to formula_2 is an elliptic integral that may be computed to any desired accuracy using a truncated series. Before this can be done the ellipse must be defined and the limits of integration computed.
Let the ellipsoid given by formula_64, and let formula_65.
If p=0 then the section is a horizontal circle of radius formula_66, which has no solution if formula_67.

If p>0 then Gilbertson showed that the ECEF coordinates of the center of the ellipse is formula_68, where formula_69,

the semi-major axis is formula_70, in the direction formula_71, and
the semi-minor axis is formula_72, in the direction formula_73, which has no solution if formula_74.

The polar form relative to center for the equation of an ellipse is formula_75, where formula_76, relates to the ellipse eccentricity, not the spheroid eccentricity (see ellipse). Let P be a point on the ellipse and formula_77, then the vector from formula_78 to formula_43 has components formula_80. Using an argument similar to the one for azimuth above, let formula_81, then formula_82, and formula_83, and formula_84. In this way we obtain the central angles formula_85 and formula_86 corresponding to formula_1 and formula_2 respectively. Care must be taken to ensure that formula_89 ≤ formula_90 ≤ formula_91. Then the arc length along the ellipse is given by formula_3 =formula_93 Substituting formula_94 above into this formula, performing the indicated operations, using one more term than Gilbertson's expression and regrouping, results in 
formula_95, where
Alternatively, expansions for the Meridian arc may be used here by replacing the spheroid eccentricity with the section ellipse eccentricity.

The direct problem is given formula_97, the distance, formula_98, and departure azimuth, formula_6, find formula_100 and the arrival azimuth, formula_7.

Construct the tangent vector at formula_97, formula_103, where formula_104 and formula_105 are unit vectors pointing north and east (respectively) at formula_97. Pick a vector, formula_37, to define the section plane, paying attention to orientation. Observe that formula_37 must not be in span{formula_109} (otherwise the plane would be tangent to the earth at formula_97, so no path would result). The normal vector formula_31 = formula_32×formula_113), together with formula_97 defines the plane.

This is a 2-d problem in span{formula_116}, which will be solved with the help of the arc length formula above. The basic approach is to use Newton-Raphson iteration to arrive at formula_100. The basis of the estimate is that the position vector of any point on the section ellipse may be expressed in terms of the position vector of the center and the central angle as 
formula_118.
To get an initial estimate of formula_119, let formula_120, formula_121=Central_Angleformula_122 (see the arc length section above),
formula_123, formula_124.

Now initialize formula_125 = formula_126, and iterate the following steps:
exit when formula_128

No more than three iterations are usually necessary, although nearly antipodal cases can be problematic.
Finally, let formula_129, and formula_100 = ECEF_to_Geoformula_131 using Bowring's 1985 algorithm, or the algorithm here.

Alternatively, inversion of the arc length series may be used to avoid iterations.

Azimuth may be obtained by the same method as the indirect problem: formula_132=formula_133, where the subscript 2 indicates evaluation of the associated quantity at formula_2.

Let formula_13 be the origin, so that formula_136 = the position vector of formula_14. The above approach provides an alternative to that of others, such as Bowring.

The normal section at formula_1 is determined by letting formula_136 = formula_140 (the surface normal at formula_1). The above approach provides an alternative to that of others, such as Bowring.

The mean normal section from formula_1 to formula_2 is determined by letting formula_136 = formula_145. This is a good approximation to the geodesic from formula_1 to formula_2 for aviation or sailing.

A class of sections may be imagined by rotating formula_140 about the chord connecting formula_1 and formula_150 All of these may be solved with the single approach above.

Let two section planes be given: formula_151⋅formula_43 = formula_153, and formula_154⋅formula_43 = formula_156. Assuming that the two planes are not parallel, the line of intersection is on both planes. Hence orthogonal to both normals, i.e. in the direction of formula_157.

Since formula_151 and formula_154 are not colinear formula_151, formula_154, formula_162 is a basis for formula_163. Therefore, there exist constants formula_164 and formula_165 such that the line of intersection of the 2 planes is given by formula_43 = formula_167 + formula_168 + tformula_162, where t is an independent parameter.

Since this line is on both section planes, it satisfies both: 
formula_164 + formula_165(formula_151·formula_154) = formula_153, and 
formula_164(formula_151·formula_154) + formula_165 = formula_156.

Solving these equations for formula_180 and formula_181 gives 
formula_164 [1 - (formula_183 ] = formula_153 - formula_156(formula_151·formula_154), and 
formula_165 [1 - (formula_183 ] = formula_156 - formula_153(formula_151·formula_154).

Define the "dihedral angle", formula_55, by formula_195 = formula_196·formula_197.
Then formula_164 = formula_199 , and formula_165 = formula_201.

On the intersection line we have formula_43 = formula_13 + tformula_162, where formula_13 = formula_167 + formula_168.
Hence: formula_208 = formula_209 + tformula_210, formula_211 = formula_212 + tformula_213, and formula_214 = formula_215 + tformula_216, where
formula_209= formula_218 + formula_219, formula_212 = formula_221 + formula_222, and formula_215 = formula_224 +formula_225.
and formula_226=(formula_227,formula_228,formula_229), for i=1,2,3.

To find the intersection of this line with the earth, plug the line equations into formula_64, to get
formula_231, where formula_232 = formula_233, 
formula_234 = formula_235,
formula_236 = formula_237.

Therefore, the line intersects the earth at formula_238. If formula_239, then there is no intersection. If formula_240, then the line is tangent to the earth at formula_241 (i.e. the sections intersect at that single point).

Observe that formula_242 since formula_151 and formula_154are not colinear. Plugging t into
formula_43 = formula_13 + tformula_162, gives the points of intersection of the earth sections.

on an earth section path may be found by dropping the subscripts on the given section; formula_248, formula_249, and setting formula_250, so that formula_251. Then solve for formula_252 such that formula_240.

Since formula_254, and formula_255, we must have formula_256. Plugging t into formula_43 = formula_258, gives the points of intersection of the earth sections. Alternatively, just set formula_259.

on an earth section path may be found by dropping the subscripts on the given section; formula_248, formula_249, and setting formula_262, where formula_263 is the longitude to be solved for such that formula_240.

Alternatively, just set formula_265.


</doc>
<doc id="57425862" url="https://en.wikipedia.org/wiki?curid=57425862" title="Geographic contiguity">
Geographic contiguity

Geographic contiguity is the characteristic in geography of political or geographical land divisions, as a group, not being interrupted by other land or water. Such divisions are referred to as being "contiguous." In the United States, for example, the "48 contiguous states" excludes Hawaii and Alaska, which do not share borders with other U.S. states.

Other examples of geographical contiguity might include the "contiguous European Union" excluding member states such as Ireland, Sweden, Finland, Malta and Cyprus (these being non-contiguous), or the "contiguous United Kingdom" referring to all parts of the country excepting Northern Ireland (it being geographically non-contiguous).

Two or more contiguous municipalities can be consolidated into one, or one municipality can consist of many noncontiguous elements. For example, the Financially Distressed Municipalities Act allows the commonwealth of Pennsylvania to merge contiguous municipalities to reduce financial distress.

Geographic contiguity is important in biology, especially animal ranges. For a particular species, its habitat may be a 'contiguous range', or it might be broken, requiring periodic, typically seasonal migrations (see: Disjunct distribution). The same concept of contiguous range is true for human transportation studies in an attempt to understand census geography. It also comes into play with electoral geography and politics.


</doc>
<doc id="19877" url="https://en.wikipedia.org/wiki?curid=19877" title="Map">
Map

A map is a symbolic depiction emphasizing relationships between elements of some space, such as objects, regions, or themes.

Many maps are static, fixed to paper or some other durable medium, while others are dynamic or interactive. Although most commonly used to depict geography, maps may represent any space, real or fictional, without regard to context or scale, such as in brain mapping, DNA mapping, or computer network topology mapping. The space being mapped may be two dimensional, such as the surface of the earth, three dimensional, such as the interior of the earth, or even more abstract spaces of any dimension, such as arise in modeling phenomena having many independent variables.

Although the earliest maps known are of the heavens, geographic maps of territory have a very long tradition and exist from ancient times. The word "map" comes from the medieval Latin "Mappa mundi", wherein "mappa" meant napkin or cloth and "mundi" the world. Thus, "map" became the shortened term referring to a two-dimensional representation of the surface of the world.

Cartography or "map-making" is the study and practice of crafting representations of the Earth upon a flat surface (see History of cartography), and one who makes maps is called a cartographer.

Road maps are perhaps the most widely used maps today, and form a subset of navigational maps, which also include aeronautical and nautical charts, railroad network maps, and hiking and bicycling maps. In terms of quantity, the largest number of drawn map sheets is probably made up by local surveys, carried out by municipalities, utilities, tax assessors, emergency services providers, and other local agencies. Many national surveying projects have been carried out by the military, such as the British Ordnance Survey: a civilian government agency, internationally renowned for its comprehensively detailed work.

In addition to location information, maps may also be used to portray contour lines indicating constant values of elevation, temperature, rainfall, etc.

The orientation of a map is the relationship between the directions on the map and the corresponding compass directions in reality. The word "orient" is derived from Latin , meaning east. In the Middle Ages many maps, including the T and O maps, were drawn with east at the top (meaning that the direction "up" on the map corresponds to East on the compass). The most common cartographic convention, is that north is at the top of a map.

Maps not oriented with north at the top:

Many maps are drawn to a scale expressed as a ratio, such as 1:10,000, which means that 1 unit of measurement on the map corresponds to 10,000 of that same unit on the ground. The scale statement can be accurate when the region mapped is small enough for the curvature of the Earth to be neglected, such as a city map. Mapping larger regions, where curvature cannot be ignored, requires projections to map from the curved surface of the Earth to the plane. The impossibility of flattening the sphere to the plane without distortion means that the map cannot have constant scale. Rather, on most projections the best that can be attained is accurate scale along one or two paths on the projection. Because scale differs everywhere, it can only be measured meaningfully as point scale per location. Most maps strive to keep point scale variation within narrow bounds. Although the scale statement is nominal it is usually accurate enough for most purposes unless the map covers a large fraction of the earth. At the scope of a world map, scale as a single number is practically meaningless throughout most of the map. Instead, it usually refers to the scale along the equator.
Some maps, called cartograms, have the scale deliberately distorted to reflect information other than land area or distance. For example, this map (at the right) of Europe has been distorted to show population distribution, while the rough shape of the continent is still discernible.

Another example of distorted scale is the famous London Underground map. The basic geographical structure is respected but the tube lines (and the River Thames) are smoothed to clarify the relationships between stations. Near the center of the map stations are spaced out more than near the edges of map.

Further inaccuracies may be deliberate. For example, cartographers may simply omit military installations or remove features solely in order to enhance the clarity of the map. For example, a road map may not show railroads, smaller waterways or other prominent non-road objects, and even if it does, it may show them less clearly (e.g. dashed or dotted lines/outlines) than the main roads. Known as decluttering, the practice makes the subject matter that the user is interested in easier to read, usually without sacrificing overall accuracy. Software-based maps often allow the user to toggle decluttering between ON, OFF and AUTO as needed. In AUTO the degree of decluttering is adjusted as the user changes the scale being displayed.

Geographic maps use a projection to translating the three-dimensional real surface of the geoid to a two-dimensional picture. Projection always distorts the surface. There are many ways to apportion the distortion, and so there are many map projections. Which projection to use depends on the purpose of the map.

The various features shown on a map are represented by conventional signs or symbols. For example, colors can be used to indicate a classification of roads. Those signs are usually explained in the margin of the map, or on a separately published characteristic sheet.

Some cartographers prefer to make the map cover practically the entire screen or sheet of paper, leaving no room "outside" the map for information about the map as a whole.
These cartographers typically place such information in an otherwise "blank" region "inside" the mapcartouche, map legend, title, compass rose, bar scale, etc.
In particular, some maps contain smaller "sub-maps" in otherwise blank regions—often one at a much smaller scale showing the whole globe and where the whole map fits on that globe, and a few showing "regions of interest" at a larger scale in order to show details that wouldn't otherwise fit.
Occasionally sub-maps use the same scale as the large map—a few maps of the contiguous United States include a sub-map to the same scale for each of the two non-contiguous states.

To communicate spatial information effectively, features such as rivers, lakes, and cities need to be labeled. Over centuries cartographers have developed the art of placing names on even the densest of maps. Text placement or name placement can get mathematically very complex as the number of labels and map density increases. Therefore, text placement is time-consuming and labor-intensive, so cartographers and GIS users have developed automatic label placement to ease this process.

Maps of the world or large areas are often either 'political' or 'physical'. The most important purpose of the political map is to show territorial borders; the purpose of the physical is to show features of geography such as mountains, soil type or land use including infrastructure such as roads, railroads and buildings. Topographic maps show elevations and relief with contour lines or shading. Geological maps show not only the physical surface, but characteristics of the underlying rock, fault lines, and subsurface structures.

From the last quarter of the 20th century, the indispensable tool of the cartographer has been the computer. Much of cartography, especially at the data-gathering survey level, has been subsumed by Geographic Information Systems (GIS). The functionality of maps has been greatly advanced by technology simplifying the superimposition of spatially located variables onto existing geographical maps. Having local information such as rainfall level, distribution of wildlife, or demographic data integrated within the map allows more efficient analysis and better decision making. In the pre-electronic age such superimposition of data led Dr. John Snow to identify the location of an outbreak of cholera. Today, it is used by agencies of the human kind, as diverse as wildlife conservationists and militaries around the world.

Even when GIS is not involved, most cartographers now use a variety of computer graphics programs to generate new maps.

Interactive, computerised maps are commercially available, allowing users to "zoom in" or "zoom out" (respectively meaning to increase or decrease the scale), sometimes by replacing one map with another of different scale, centered where possible on the same point. In-car global navigation satellite systems are computerised maps with route-planning and advice facilities which monitor the user's position with the help of satellites. From the computer scientist's point of view, zooming in entails one or a combination of:


For example:

"See also: Webpage (Graphics), PDF (Layers), MapQuest, Google Maps, Google Earth, OpenStreetMap or Yahoo! Maps."

The maps that reflect the territorial distribution of climatic conditions based on the results of long-term observations are called climatic maps. These maps can be compiled both for individual climatic features (temperature, precipitation, humidity) and for combinations of them at the earth's surface and in the upper layers of the atmosphere. Climatic maps show climatic features across a large region and permit values of climatic features to be compared in different parts of the region. When generating the map, interpolation can be used to synthesize values where there are no measurements, under the assumption that conditions change smoothly.

Climatic maps generally apply to individual months and to the year as a whole, sometimes to the four seasons, to the growing period, and so forth. On maps compiled from the observations of ground meteorological stations, atmospheric pressure is converted to sea level. Air temperature maps are compiled both from the actual values observed on the surface of the earth and from values converted to sea level. The pressure field in free atmosphere is represented either by maps of the distribution of pressure at different standard altitudes—for example, at every kilometer above sea level—or by maps of baric topography on which altitudes (more precisely geopotentials) of the main isobaric surfaces (for example, 900, 800, and 700 millibars) counted off from sea level are plotted. The temperature, humidity, and wind on aeroclimatic maps may apply either to standard altitudes or to the main isobaric surfaces.

Isolines are drawn on maps of such climatic features as the long-term mean values (of atmospheric pressure, temperature, humidity, total precipitation, and so forth) to connect points with equal values of the feature in question—for example, isobars for pressure, isotherms for temperature, and isohyets for precipitation. Isoamplitudes are drawn on maps of amplitudes (for example, annual amplitudes of air temperature—that is, the differences between the mean temperatures of the warmest and coldest month). Isanomals are drawn on maps of anomalies (for example, deviations of the mean temperature of each place from the mean temperature of the entire latitudinal zone). Isolines of frequency are drawn on maps showing the frequency of a particular phenomenon (for example, annual number of days with a thunderstorm or snow cover). Isochrones are drawn on maps showing the dates of onset of a given phenomenon (for example, the first frost and appearance or disappearance of the snow cover) or the date of a particular value of a meteorological element in the course of a year (for example, passing of the mean daily air temperature through zero). Isolines of the mean numerical value of wind velocity or isotachs are drawn on wind maps (charts); the wind resultants and directions of prevailing winds are indicated by arrows of different length or arrows with different plumes; lines of flow are often drawn. Maps of the zonal and meridional components of wind are frequently compiled for the free atmosphere. Atmospheric pressure and wind are usually combined on climatic maps. Wind roses, curves showing the distribution of other meteorological elements, diagrams of the annual course of elements at individual stations, and the like are also plotted on climatic maps.

Maps of climatic regionalization, that is, division of the earth's surface into climatic zones and regions according to some classification of climates, are a special kind of climatic map.

Climatic maps are often incorporated into climatic atlases of varying geographic range (globe, hemispheres, continents, countries, oceans) or included in comprehensive atlases. Besides general climatic maps, applied climatic maps and atlases have great practical value. Aeroclimatic maps, aeroclimatic atlases, and agroclimatic maps are the most numerous.

Maps exist of the Solar System, and other cosmological features such as star maps. In addition maps of other bodies such as the Moon and other planets are technically not "geo"graphical maps.

Diagrams such as schematic diagrams and Gantt charts and treemaps display logical relationships between items, rather than geographical relationships. Topological in nature, only the connectivity is significant. The London Underground map and similar subway maps around the world are a common example of these maps.

General-purpose maps provide many types of information on one map. Most atlas maps, wall maps, and road maps fall into this category. The following are some features that might be shown on general-purpose maps: bodies of water, roads, railway lines, parks, elevations, towns and cities, political boundaries, latitude and longitude, national and provincial parks. These maps give a broad understanding of location and features of an area. The reader may gain an understanding of the type of landscape, the location of urban places, and the location of major transportation routes all at once.



Some countries required that all published maps represent their national claims regarding border disputes. For example:

In 2010, the People's Republic of China began requiring that all online maps served from within China be hosted there, making them subject to Chinese laws.









</doc>
<doc id="26044771" url="https://en.wikipedia.org/wiki?curid=26044771" title="Outline of geography">
Outline of geography

The following outline is provided as an overview of and topical guide to geography:

Geography – study of earth and its people.


As "the bridge between the human and physical sciences," geography is divided into two main branches:

Other branches include:

All the branches are further described below...










Regional geography – study of world regions. Attention is paid to unique characteristics of a particular region such as its natural elements, human elements, and regionalization which covers the techniques of delineating space into regions. Regional geography breaks down into the study of specific regions.

Region – an area, defined by physical characteristics, human characteristics, or functional characteristics. The term is used in various ways among the different branches of geography. A region can be seen as a collection of smaller units, such as a country and its political divisions, or as one part of a larger whole, as in a country on a continent.

List of supercontinents
A "supercontinent" is a landmass comprising more than one continental core, or craton.


Continent – one of several large landmasses on Earth. They are generally identified by convention rather than any specific criteria, but seven areas are commonly regarded as continents. They are:

Subregion (list)

Ecozone
The World Wildlife Fund (WWF) developed a system of eight biogeographic realms (ecozones):

Ecoregion
Ecozones are further divided into ecoregions. The World has over 800 terrestrial ecoregions. "See Lists of ecoregions by country."


Topics pertaining to the geographical study of the World throughout history:













Topics common to the various branches of geography include:


Geographic feature – component of a planet that can be referred to as a location, place, site, area, or region, and therefore may show up on a map. A geographic feature may be natural or artificial.


Natural geographic feature – an ecosystem or natural landform.

Ecosystem – community of living organisms in conjunction with the nonliving components of their environment (things like air, water and mineral soil), interacting as a system. These biotic and abiotic components are regarded as linked together through nutrient cycles and energy flows. 

Natural landform – terrain or body of water. Landforms are topographical elements, and are defined by their surface form and location in the landscape. Landforms are categorized by traits such as elevation, slope, orientation, stratification, rock exposure, and soil type. Some landforms are artificial, such as certain islands, but most landforms are natural.



Artificial geographic feature – a thing that was made by humans that may be indicated on a map. It may be physical and exist in the real world (like a bridge or city), or it may be abstract and exist only on maps (such as the Equator, which has a defined location, but cannot be seen where it lies).



Some awards and competitions in the field of geography:

A geographer is a scientist who studies Earth's physical environment and human habitat. Geographers are historically known for making maps, the subdiscipline of geography known as cartography. They study the physical details of the environment and also its effect on human and wildlife ecologies, weather and climate patterns, economics, and culture. Geographers focus on the spatial relationships between these elements.



Educational frameworks upon which primary and secondary school curricula for geography are based upon include:



</doc>
<doc id="128608" url="https://en.wikipedia.org/wiki?curid=128608" title="Population density">
Population density

Population density (in agriculture: standing stock and standing crop) is a measurement of population per unit area, or exceptionally unit volume; it is a quantity of type number density. It is frequently applied to living organisms, most of the time to humans. It is a key geographical term. In simple terms, population density refers to the number of people living in an area per square kilometre.

Population density is population divided by total land area or water volume, as appropriate.

Low densities may cause an extinction vortex and lead to further reduced fertility. This is called the Allee effect after the scientist who identified it. Examples of the causes of reduced fertility in low population densities are:

For humans, population density is the number of people per unit of area, usually quoted per square kilometre or square mile, and which may include or exclude for example areas of water or glaciers. Commonly this may be calculated for a county, city, country, another territory or the entire world.

The world's population is around 7,500,000,000 and Earth's total area (including land and water) is 510,000,000 km (197,000,000 sq. mi.). Therefore, the worldwide human population density is around 7,500,000,000 ÷ 510,000,000 = 14.7 per km (38 per sq. mi.). If only the Earth's land area of 150,000,000 km (58,000,000 sq. mi.) is taken into account, then human population density is 50 per km (129 per sq. mi.). This includes all continental and island land area, including Antarctica. If Antarctica is also excluded, then population density rises to over 55 people per km (over 142 per sq. mi.). However, over half of the Earth's land mass consists of areas inhospitable to human habitation, such as deserts and high mountains, and population tends to cluster around seaports and fresh-water sources. Thus, additional criteria are needed to make simple population density values useful.

Several of the most densely populated territories in the world are city-states, microstates and urban dependencies. These territories have a relatively small area and a high urbanization level, with an economically specialized city population drawing also on rural resources outside the area, illustrating the difference between high population density and overpopulation.

Deserts have very limited potential for growing crops as there is not enough rain to support them. Thus their population density is generally low. However some cities in the Middle East, such as Dubai, have been increasing in population and infrastructure growth at a fast pace.

Cities with high population densities are, by some, considered to be overpopulated, though this will depend on factors like quality of housing and infrastructure and access to resources. Very densely populated cities are mostly in Asia (particularly Southeast Asia); Africa's Lagos, Kinshasa and Cairo; South America's Bogotá, Lima and São Paulo; and Mexico City and Saint Petersburg also fall into this category.

City population and especially area are, however, heavily dependent on the definition of "urban area" used: densities are almost invariably higher for the centre only than when suburban settlements and intervening rural areas are included, as in the agglomeration or metropolitan area (the latter sometimes including neighboring cities).

In comparison, based on a world population of 7 billion, the world's inhabitants, as a loose crowd taking up almost 1 m (10 sq. ft) per person (cf. Jacobs Method), would occupy a space a little larger than Delaware's land area.

Although arithmetic density is the most common way of measuring population density, several other methods have been developed to provide a more accurate measure of population density over a specific area.





</doc>
<doc id="62562757" url="https://en.wikipedia.org/wiki?curid=62562757" title="Page layout (cartography)">
Page layout (cartography)

Page layout, also map layout, is the part of cartography that involves assembling various map elements on a page. It follows principles similar to page layout in graphic design, like balance, gestalt, and visual hierarchy. Some of the non-map elements involved in map layout are legends, titles, scale bars, inset maps, and north arrows. 

Some of the major principles involved in map layout are balance, gestalt, and visual hierarchy. Map elements should be well-balanced, meaning that their visual weight is evenly distributed on the page. Gestalt is the overall effect of a composition, stemming from the idea that the whole is more than the sum of its parts. A map’s visual hierarchy (the elements that stand out the most) should match its intellectual hierarchy (the most important parts of the map).

In addition to the map itself, there are various elements that are included in the map layout. The following are common elements of a map layout.

Legends explain how to interpret the map's symbols. Explanations of point, line, and area symbols can all be included in the legend. However, generally, the legend only includes explanations of symbols that are not already obvious. The legend may also give details about the variable being displayed, publication, or authorship. Often, the legend is critical to understanding a map, so it is important that legends are designed effectively. Using principles of gestalt, various sets of rules have been created for legend spacing, alignment, and grouping.

Titles should be short and to the point. They typically include information about the location of the map and the subject of the map. The title should be an important part of the layout's visual hierarchy because the title tells people what the map is about.

Scale is important to include on a map because it explains the size relationship between map features and the real world. Scale is commonly represented with a scale bar, a representative fraction, or a verbal scale.

Inset maps are smaller maps that are included on the same page as the main map. They can show additional information related to the main map. For example, a locator map can be used to show where the map’s area of interest is in a larger context or to zoom in on an area of the main map. Inset maps may also show non-contiguous areas of the main map, like Alaska and Hawaii in a map of the United States.

North arrows orient readers and tell them which way the map is facing. They are especially important in areas that map viewers are not familiar with. In other cases, they may be unnecessary. North arrows can be very complex or very simple.

Charts and graphs can provide a non-spatial view of data to support the map. This allows for data to be included in the layout that is not appropriate for a map or difficult to achieve in a map such as change over time.

Supplementary text is used to provide context or explain the map or other map elements. Thematic mapping often uses text to add content that supports the purpose of the map.

Non-map images can be added to a layout for a variety of reasons. It can offer a photo view of the area modeled in the map so that a reader can see the location looks like. Images can also be used to show examples of data points or methods used for data collection data collection. Logos of companies are also a common use of images on layout.



</doc>
<doc id="25724950" url="https://en.wikipedia.org/wiki?curid=25724950" title="Geodesign">
Geodesign

Geodesign is a set of concepts and methods used to involve all stakeholders and various professions in collaboratively designing and realizing the optimal solution for spatial challenges in the built and natural environments, utilizing all available techniques and data in an integrated process. Originally, geodesign was mainly applied during the design and planning phase. "Geodesign is a design and planning method which tightly couples the creation of design proposals with impact simulations informed by geographic contexts." Now, it is also used during realization and maintenance phases and to facilitate re-use of for example buildings or industrial areas. Geodesign includes project conceptualization, analysis, design specification, stakeholder participation and collaboration, design creation, simulation, and evaluation (among other stages). 

Geodesign builds greatly on a long history of work in geographic information science, computer-aided design, landscape architecture, and other environmental design fields. See for instance, the work of Ian McHarg and Carl Steinitz. 

Members of the various disciplines and practices relevant to geodesign have held defining discussions at a workshop on Spatial Concepts in GIS and Design in December 2008 and the GeoDesign Summit in January 2010. GeoDesign Summit 2010 Conference Videos from Day 1 and Day 2 are an important resource to learn about the many different aspects of GeoDesign. ESRI co-founder Jack Dangermond has introduced each of the GeoDesign Summit meetings. Designer and technologist Bran Ferren, was the keynote speaker for the first and fourth Summit meetings in Redlands, California. During the fourth conference he presented a provocative view of how what is needed is a 250-year plan, and how GeoDesign was a key concept in making this a reality. Carl Steinitz was a presenter at both the 2010 and 2015 Summits.

The 2013 Geodesign Summit drew a record 260 attendees from the United States and abroad (watch video coverage of the summit). That same year, a master's degree in Geodesign — the first of its kind in the nation — began at Philadelphia University. Claudia Goetz Phillips, director of Landscape Architecture and GeoDesign at Philadelphia University says "it is very exciting to be at the forefront of this exciting and relevant paradigm shift in how we address twenty-first-century global to local design and planning issues."

The theory underpinning Geodesign derives from the work of Patrick Geddes in the first half of the twentieth century and Ian McHarg in its second half. They advocated a layered approach to regional planning, landscape planning and urban planning. McHarg drew the layers on translucent overlays. Through the work of Jack Dangermond, Carl Steinitz, Henk Scholten and others the layers were modeled with Geographical Information Systems (GIS). The three components of this term each say something about its character. 'Geographical' implies that the layers are geographical (geology, soils, hydrology, roads, land use etc.). 'Information' implies a positivist and scientific methodology. 'System' implies the use of computer technology for the information processing.
The scientific aspects of Geodesign contrast with the cultural emphasis of Landscape Urbanism but the two approaches to landscape planning share a concern for layered analysis which sits comfortably with postmodern and post-postmodern theory.

Nascent geodesign technology extends geographic information systems so that in addition to analyzing existing environments and geodata, users can synthesize new environments and modify geodata. See, for example, CommunityViz or marinemap.

"GeoDesign brings geographic analysis into the design process, where initial design sketches are instantly vetted for suitability against myriad database layers describing a variety of physical and social factors for the spatial extent of the project. This on-the-fly suitability analysis provides a framework for design, giving land-use planners, engineers, transportation planners, and others involved with design, the tools to leverage geographic information within their design workflows."





</doc>
<doc id="33964621" url="https://en.wikipedia.org/wiki?curid=33964621" title="Counter-mapping">
Counter-mapping

Counter-mapping refers to efforts to map "against dominant power structures, to further seemingly progressive goals". The term was coined by Nancy Peluso in 1995 to describe the commissioning of maps by forest users in Kalimantan, Indonesia, as a means of contesting state maps of forest areas that typically undermined indigenous interests. The resultant counter-hegemonic maps had the ability to strengthen forest users' resource claims. There are numerous expressions closely related to counter-mapping: ethnocartography, alternative cartography, mapping-back, counter-hegemonic mapping, and public participatory mapping. Moreover, the terms: critical cartography, subversive cartography, bio-regional mapping, and remapping are sometimes used interchangeably with counter-mapping, but in practice encompass much more.

Whilst counter-mapping still primarily refers to indigenous cartographic efforts, it is increasingly being applied to non-indigenous mapping initiatives in economically developed countries. Such counter-mapping efforts have been facilitated by processes of neoliberalism, and technological democratisation. Examples of counter-mapping include attempts to demarcate and protect traditional territories, community mapping, Public Participatory Geographical Information Systems, and mapping by a relatively weak state to counter the resource claims of a stronger state. The power of counter-maps to advocate policy change in a bottom-up manner led commentators to affirm that counter-mapping should be viewed as a tool of governance.

Despite its emancipatory potential, counter-mapping has not gone without criticism. There is a tendency for counter-mapping efforts to overlook the knowledge of women, minorities, and other vulnerable, disenfranchised groups. From this perspective, counter-mapping is only empowering for a small subset of society, whilst others become further marginalised.

Nancy Peluso, professor of forest policy, coined the term 'counter-mapping' in 1995, having examined the implementation of two forest mapping strategies in Kalimantan. One set of maps belonged to state forest managers, and the international financial institutions that supported them, such as the World Bank. This strategy recognised mapping as a means of protecting local claims to territory and resources to a government that had previously ignored them. The other set of maps had been created by Indonesian NGOs, who often contract international experts to assist with mapping village territories. The goal of the second set of maps was to co-opt the cartographic conventions of the Indonesian state, to legitimise the claims by the Dayak people, indigenous to Kalimantan, to the rights to forest use. Counter-mappers in Kalimantan have acquired GIS technologies, satellite technology, and computerised resource management tools, consequently making the Indonesian state vulnerable to counter-maps. As such, counter-mapping strategies in Kalimantan have led to successful community action to block, and protest against, oil palm plantations and logging concessions imposed by the central government.

It must, however, be recognised that counter-mapping projects existed long before coinage of the term. Counter-maps are rooted in map art practices that date to the early 20th century; in the mental maps movement of the 1960s; in indigenous and bioregional mapping; and parish mapping.

In 1985, the charity Common Ground launched the Parish Maps Project, a bottom-up initiative encouraging local people to map elements of the environment valued by their parish. Since then, more than 2,500 English parishes have made such maps. Parish mapping projects aim to put every local person in an 'expert' role. Clifford exemplifies this notion, affirming: "making a parish map is about creating a community expression of values, and about beginning to assert ideas for involvement. It is about taking the place in your own hands". The final map product is typically an artistic artefact, usually painted, and often displayed in village halls or schools. By questioning the biases of cartographic conventions and challenging predominant power effects of mapping, The Parish Maps Project is an early example of what Peluso went on to term 'counter-mapping'

The development of counter-mapping can be situated within the neoliberal political-economic restructuring of the state. Prior to the 1960s, equipping a map-making enterprise was chiefly the duty of a single agency, funded by the national government. In this sense, maps have conventionally been the products of privileged knowledges. However, processes of neoliberalism, predominantly since the late 1970s, have reconfigured the state's role in the cartographic project. Neoliberalism denotes an emphasis on markets and minimal states, whereby individual choice is perceived to have replaced the mass-production of commodities. The fact that citizens are now performing cartographic functions that were once exclusively state-controlled can be partially explained through a shift from "roll-back neoliberalism", in which the state dismantled some of its functions, to "roll-out neoliberalism", in which new modes of operating have been constructed. In brief, the state can be seen to have "hollowed out" and delegated some of its mapping power to citizens.

Governmentality refers to a particular form of state power that is exercised when citizens self-discipline by acquiescing to state knowledge. Historically, cartography has been a fundamental governmentality strategy, a technology of power, used for surveillance and control. Competing claimants and boundaries made no appearance on state-led maps. This links to Foucault's notion of "subjugated knowledges" - ones that did not rise to the top, or were disqualified. However, through neoliberalising processes, the state has retracted from performing some of its cartographic functions. Consequently, rather than being passive recipients of top-down map distribution, people now have the opportunity to claim sovereignty over the mapping process. In this new regime of neoliberal cartographic governmentality the "insurrection of subjugated knowledges" occurs, as counter-mapping initiatives incorporate previously marginalised voices.

In response to technological change, predominantly since the 1980s, cartography has increasingly been democratised. The wide availability of high-quality location information has enabled mass-market cartography based on Global Positioning System receivers, home computers, and the internet. The fact that civilians are using technologies which were once elitist led Brosius "et al". to assert that counter-mapping involves "stealing the master's tools". Nevertheless, numerous early counter-mapping projects successfully utilised manual techniques, and many still use them. For instance, in recent years, the use of simple sketch mapping approaches has been revitalised, whereby maps are made on the ground, using natural materials. Similarly, the use of scale model constructions and felt boards, as means of representing cartographic claims of different groups, have become increasingly popular. Consequently, Wood "et al." assert that counter-mappers can "make gateau out of technological crumbs".

In recent years, Public Participation Geographical Information Systems (PPGIS) have attempted to take the power of the map out of the hands of the cartographic elite, putting it into the hands of the people. For instance, Kyem designed a PPGIS method termed Exploratory Strategy for Collaboration, Management, Allocation, and Planning (ESCMAP). The method sought to integrate the concerns and experiences of three rural communities in the Ashanti Region of Ghana into official forest management practices. Kyem concluded that, notwithstanding the potential of PPGIS, it is possible that the majority of the rich and powerful people in the area would object to some of the participatory uses of GIS. For example, loggers in Ghana affirmed that the PPGIS procedures were too open and democratic. Thus, despite its democratising potential, there are barriers to its implementation. More recently, Wood "et al". disputed the notion of PPGIS entirely, affirming that it is "scarcely GIS, intensely hegemonic, hardly public, and anything but participatory".

Governance makes problematic state-centric notions of regulation, recognising that there has been a shift to power operating across several spatial scales. Similarly, counter-mapping complicates state distribution of cartography, advocating bottom-up participatory mapping projects (see GIS and environmental governance). Counter-mapping initiatives, often without state assistance, attempt to exert power. As such, counter-mapping conforms to Jessop's notion of "governance without government". Another characteristic of governance is its "purposeful effort to steer, control or manage sectors or facets of society" towards a common goal. Likewise, as maps exude power and authority, they are a trusted medium with the ability to 'steer' society in a particular direction. In brief, cartography, once the tool of kings and governments, is now being used as a tool of governance - to advocate policy change from the grassroots. The environmental sphere is one context in which counter-mapping has been utilised as a governance tool.

In contrast to expert knowledges, lay knowledges are increasingly valuable to decision-makers, in part due to the scientific uncertainty surrounding environmental issues. Participatory counter-mapping projects are an effective means of incorporating lay knowledges into issues surrounding environmental governance. For instance, counter-maps depicting traditional use of areas now protected for biodiversity have been used to allow resource use, or to promote public debate about the issue, rather than forcing relocation. For example, the World Wide Fund for Nature used the results of counter-mapping to advocate for the reclassification of several strictly protected areas into Indonesian national parks, including Kayan Mentarang and Gunung Lorentz. The success of such counter-mapping efforts led Alcorn to affirm that governance (grassroots mapping projects), rather than government (top-down map distribution), offers the best hope for good natural resource management. In short, it can be seen that "maps are powerful political tools in ecological and governance discussions".

Numerous counter-mapping types exist, for instance: protest maps, map art, counter-mapping for conservation, and PPGIS. In order to emphasise the wide scope of what has come to be known as counter-mapping, three contrasting counter-mapping examples are elucidated in this section: indigenous counter-mapping, community mapping, and state counter-mapping, respectively.

Counter-mapping has been undertaken most in the Third World. Indigenous peoples are increasingly turning to participatory mapping, appropriating both the state's techniques and manner of representation. Counter-mapping is a tool for indigenous identity-building, and for bolstering the legitimacy of customary resource claims. The success of counter-mapping in realising indigenous claims can be seen through Nietschmann's assertion:

The power of indigenous counter-mapping can be exemplified through the creation of Nunavut. In 1967, Frank Arthur Calder and the Nisaga'a Nation Tribal Council brought an action against the Province of British Columbia for a declaration that aboriginal title to specified land had not been lawfully extinguished. In 1973, the Canadian Supreme Court found that there was, in fact, an aboriginal title. The Canadian government attempted to extinguish such titles by negotiating treaties with the people who had not signed them. As a first step, the Inuit Tapirisat of Canada studied Inuit land occupancy in the Arctic, resulting in the publication of the "Inuit Land Use and Occupancy Project". Diverse interests, such as those of hunters, trappers, fishermen and berry-pickers mapped out the land they had used during their lives. As Usher noted:

These maps played a fundamental role in the negotiations that enabled the Inuit to assert an aboriginal title to the 2 million km² in Canada, today known as Nunavut. Evidently, counter-mapping is a tool by which indigenous groups can re-present the world in ways which destabilise dominant representations. 

Indigenous peoples have begun remapping areas of the world that were once occupied by their ancestors as an act of reclamation of land stolen from them by country governments. Indigenous peoples have begun this process all over the world from the Indigenous peoples from the United States, Aboriginal peoples from Australia, and Amazonian people from Brazil. The people of the lands have begun creating their own maps of the land in terms of the borders of the territory and pathways around the territory. When Native peoples first began this process it was done by hand, but presently GPS systems and other technological mapping devices are used. Indigenous maps are reconceptualizing the "average" map and creatively representing space as well as the culture of those who live in the space. Indigenous people are creating maps that are for their power and social benefit instead of the ones forced on them through different titling, and description. Indigenous peoples are also creating maps to adjust to the contamination and pollution that is present In their land. Specifically in Peru, Indigenous peoples are using mapping to identify problem areas and innovating and creating strategies to combat these risks for the future. 

White colonists saw land as property and a commodity to be possessed. As a result, as settlers grew in numbers and journeyed west, land was claimed and sold for profit. White colonists would “develop” the land and take ownership of it, believing the land was theirs to own. Indigenous peoples, on the other hand, saw themselves as connected with the land spiritually and that the land, instead owned them. Land to Aboriginal people is a major part of their identity and spirituality. They saw the land as being sacred and needing to be protected. Indigenous peoples believe it is their responsibility to take care of the land. As Marion Kickett states in her research, “Land is very important to Aboriginal people with the common belief of 'we don’t own the land, the land owns us'. Aboriginal people have always had a spiritual connection to their land..." These differing perspectives on land caused many disputes during the era of Manifest Destiny and as white settler populations began to increase and move into Indigenous peoples’ territory. The Indigenous people believed they were to serve the land while white colonists believed the land should serve them. As a result, when the two sides came in contact, they disputed over how to "claim" land. The height of this conflict began to occur during Manifest Destinyas the white colonist population began to grow and move westward into more parts of Indigenous lands and communities.

Maps represent and reflect how an individual or society names and projects themselves onto nature, literally and symbolically. Mapping, while seemingly objective is political and a method of control on territory.Mapmaking has thus both socio-cultural (myth-making) and technical (utilitarian and economic) functions and traditions. The difference between boundaries and territories made by the White colonists and Indigenous people were vastly different, and expressed their views on the land and nature. Indigenous peoples' territory often ended at rivers, mountains, and hills or were defined by relationships between different tribes, resources, and trade networks. The relationships between tribes would determine the access to the land and its resources. Instead of the borders being hard edges like the United States’, border on Indigenous peoples’ lands were more fluid and would change based on marriages between chiefs and their family members, hunting clans, and heredity. In Indigenous maps the landmarks would be drawn on paper and in some cases described. Detailed knowledge of the thickness of ice, places of shelter and predators were placed in maps to inform the user for what to look for when in the territory. 

Maps made by White colonists in America were first based on populations, created territories based on the edges of civilization. After the creation of the United States government, state land was designated by Congress and intended to be given equally by latitude and longitudinal coordinates. The  ending of railroad tracks and crossings also designated the ending of one state to another, creating a fence-like boundary. In a special case, after the acquisition of the Louisiana Purchase, the United States had to decide between the territory where slavery was legal and where it was not. The Missouri Compromisewas birthed as a result and a boundary line was created at the longitude and latitude lines of 36’30”. The states were documented by their coordinates and borders were made at the numbered locations. These numbered locations would stretch for miles and encompass all in that territory even if it belonged to Indigenous peoples’. That is often how land would be stolen from Indigenous peoples. The land that would be "claimed" by the United States Government would stretch across Indigenous lands without consideration of their borders. Indigenous peoples' lands were absorbed by the borders of America's newly mapped states and were forced out as a result. Their livelihoods and mythology tied to the land was also destroyed. White colonists claimed the land for their own and Indigenous peoples were no longer allowed to occupy the space. Another way was the differences in the way each group mapped the land. The United States Government would not recognize a Tribes territory without a map and most tribes did not have maps that were in the style of European maps, therefore they were ignored.

Community mapping can be defined as: "local mapping, produced collaboratively, by local people and often incorporating alternative
local knowledge". OpenStreetMap is an example of a community mapping initiative, with the potential to counter the hegemony of state-dominated map distribution.

OpenStreetMap (OSM), a citizen-led spatial data collection website, was founded by Steve Coast in 2004. Data are collected from diverse public domain sources; of which GPS tracks are the most important, collected by volunteers with GPS receivers. there were 340,522 registered OSM users, who had uploaded 2.121 billion GPS points onto the website. The process of map creation explicitly relies upon sharing and participation; consequently, every registered OSM user can edit any part of the map. Moreover, 'map parties' - social events which aim to fill gaps in coverage, help foster a community ethos. In short, the grassroots OSM project can be seen to represent a paradigm shift in who creates and shares geographic information - from the state, to society. 
However, rather than countering the state-dominated cartographic project, some commentators have affirmed that OSM merely replicates the 'old' socio-economic order. For instance, Haklay affirmed that OSM users in the United Kingdom tend not to map council estates; consequently, middle-class areas are disproportionately mapped. Thus, in opposition to notions that OSM is a radical cartographic counter-culture, are contentions that OSM "simply recreates a mirror copy of existing topographic mapping".

What has come to be known as counter-mapping is not limited to the activities of non-state actors within a particular nation-state; relatively weak states also engage in counter-mapping in an attempt to challenge other states.

East Timor's on-going effort to gain control of gas and oil resources from Australia, which it perceives at its own, is a form of counter-mapping. This dispute involves a cartographic contestation of Australia's mapping of the seabed resources between the two countries. As Nevins contends: whilst Australia's map is based on the status quo - a legacy of a 1989 agreement between Australia and the Indonesian occupier of East Timor at that time, East Timor's map represents an enlarged notion of what its sea boundaries should be, thereby entailing a redrawing of the map. This form of counter-mapping thus represents a claim by a relatively weak state, East Timor, to territory and resources that are controlled by a stronger state, Australia. However, Nevins notes that there is limited potential of realising a claim through East Timor's counter-map: counter-mapping is an effective strategy only when combined with broader legal and political strategies.

Counter-mapping's claim to incorporate counter-knowledges, and thereby empower traditionally disempowered people, has not gone uncontested. A sample of criticisms leveled at counter-mapping:


To summarise, whilst counter-mapping has the potential to transform map-making from "a science of princes", the investment required to create a map with the ability to challenge state-produced cartography means that counter-mapping is unlikely to become a "science of the masses".



<nowiki>https://www.creativespirits.info/aboriginalculture/land/meaning-of-land-to-aboriginal-people</nowiki>

<nowiki>http://sharingculture.info/the-importance-of-land.html</nowiki>

<nowiki>https://www.theguardian.com/science/blog/2018/mar/06/counter-mapping-cartography-that-lets-the-powerless-speak</nowiki>

<nowiki>https://www.youtube.com/watch?v=U-P4U5npRCg</nowiki>

<nowiki>https://www.wonderopolis.org/wonder/how-did-the-states-get-their-shapes</nowiki>


http://www.aughty.org/pdf/community_mapping.pdf


</doc>
<doc id="37628586" url="https://en.wikipedia.org/wiki?curid=37628586" title="Spatial citizenship">
Spatial citizenship

Spatial citizenship describes the ability of individuals and groups to interact and participate in societal spatial decision making through the reflexive production and use of geo-media (geographic media such as maps, virtual globes, GIS, and the Geoweb). Spatial citizens are lay users who are able to use geo-media to question existing perspectives on action in space (e.g. social rules, spatial planning) and to produce, communicate, and negotiate alternative spatial visions.

Spatial citizenship is an educational approach at the intersection of citizenship education and geography education. Its main theoretical reference points are emancipatory forms of citizenship and the "reflexive appropriation of space".

Spatial citizenship can be distinguished from traditional citizenship education approaches in many respects: 

Spatial citizenship has become a conceptual reference point in theories of action-oriented social geography and new cultural geography. These approaches contend that human beings constantly appropriate spaces, as they attach meanings to geographically located physical matter in order to prepare it for their own actions.

In these theories, spaces are regarded as being socially constructed. To a large extent, the attachment of meaning works unconsciously, following socially accepted, mainstream categories and discourses. Meanings given to physical objects determine the actions deemed possible. For instance, a field of asphalt in a city centre might have multiple meanings: it may be interpreted as a parking area as well as a place for ball games, with both meanings competing for dominance. As soon as one meaning becomes superior, which is a result of social power relations, the other meaning may decline, become invisible, and eventually is not used anymore. The superiority of a specific meaning over another one might be supported by artifacts representing meanings attached, such as signs on buildings, structural modifications of the physical environment, or symbols and explanations of the socio-cultural significance of places and objects in spatial representations visualized via geo-media.

A mature appropriation of space therefore includes the conscious attachment of meaning as well as awareness of meanings being attached to places by others. It includes a sensibility to the multitude of meanings transported and hidden by a mainstream discourse. Keys to the mature appropriation of space are therefore the deconstruction of socially produced meanings, as well as the ability to communicate one's own, potentially contradictory meanings and negotiate them with others. This process is often mediated through the formation of a collective spatialized identity. Given that space – its uses and symbolic significance – is often the site of social and political struggle, it becomes the container for action while at the same time shaping the group development of a "us". In particular, when groups re-imagine public space for political usages, this expression of spatial citizenship is the outcome of a spatially informed collective identity.

Geographic media (geo-media) are especially important for attaching meaning to places as they clearly connect location, information and visualization. In addition to this, geo-media represents mainly single meanings out of the many that are possible. Nowadays, geo-media have become more and more present in everyday life due to mobile computing in combination with Geoweb applications. For instance, maps on smart phones guide people in their everyday actions, but at the same time limit their opportunities for action by limiting the variety of potential meanings.

Scholars of spatial citizenship understand geo-media as instruments of reflection and communication.

The goal of education for spatial citizenship is to enable learners to achieve a reflexive appropriation of space as the basis for mature action in space by reflective geo-media use and active, reflective geo-media production. Using a broad variety of learning environments orientated toward the learners' needs, the educational approach of spatial citizenship is applicable at different levels from primary to tertiary education. Apart from technological proficiency, spatial citizenship education aims at two additional main competencies:

The European Commission-funded project SPACIT furthers education for spatial citizenship by developing teacher training standards, curricula, and learning modules for teacher education. Another EU-funded project, digital-earth.eu, linked with the SPACIT project by connecting stakeholders using or interested in using geo-media in education. It supported spatial citizenship through the creation of educational standards, the collection of best-practice examples, and the provision of learning environments applicable to teachers in everyday classroom situations. Digital-earth.eu also promoted these concepts related to spatial citizenship in political circles concerned with the development of the Europe2020 goals.



</doc>
<doc id="1019827" url="https://en.wikipedia.org/wiki?curid=1019827" title="Polity">
Polity

A polity is an identifiable political entity—any group of people who have a collective identity, who are organized by some form of institutionalized social relations, and have a capacity to mobilize resources. A polity can be any other group of people organized for governance (such as a corporate board), the government of a country, or country subdivision.

In geopolitics, a polity can be manifested in different forms such as a state, an empire, an international organization, a political organization and other identifiable, resource-manipulating organizational structures. A polity like a state does not need to be a sovereign unit. The most preeminent polities today are Westphalian states and nation-states, commonly referred to as nations.

A polity encapsulates a vast multitude of organizations, many of which form the fundamental apparatus of contemporary states such as their subordinate civil and local government authorities. Polities do not need to be in control of any geographic areas, as not all political entities and governments have controlled the resources of one fixed geographic area. The historical Steppe Empires originating from the Eurasian Steppe are the most prominent example of non-sedentary polities. These polities differ from states because of their lack of a fixed, defined territory. Empires also differ from states in that their territories are not statically defined or permanently fixed and consequently that their body politic was also dynamic and fluid. It is useful then to think of a polity as a political community.

A polity can also be defined either as a faction within a larger (usually state) entity or at different times as the entity itself. For example, Kurds in Iraqi Kurdistan are parts of their own separate and distinct polity. However, they are also members of the sovereign state of Iraq which is itself a polity, albeit one which is much less specific and as a result much less cohesive. Therefore, it is possible for an individual to belong to more than one polity at a time.

Thomas Hobbes was a highly significant figure in the conceptualisation of polities, in particular of states. Hobbes considered notions of the state and the body politic in "Leviathan", his most notable work.

In previous centuries, a body politic was also understood to mean "the physical person of the sovereign", i.e. emperor, monarch or dictator in monarchies and despotisms and the electorate in republics. As many polities have become more democratic in the last few centuries the body politic, where sovereignty is bestowed, has grown to a much greater size than simply the ruling elite such as the monarchy. In present times, it may also refer to the representation of a group such as ones drawn along ethnic or gender lines. Cabinets in liberal democracies are chosen to represent the body politic.




</doc>
<doc id="3605920" url="https://en.wikipedia.org/wiki?curid=3605920" title="Political agenda">
Political agenda

A political agenda is a list of subjects or problems to which government officials as well as individuals outside the government are paying serious attention at any given time.

It is most often shaped by political and policy elites, but can also be influenced by non-governmental activist groups, private sector lobbyists, think tanks, courts, and world events. Media coverage has also been linked to the success of the rise of political parties and their ability to get their ideas on the agenda. Although the media does often have an effect on the political agenda, these results are not always immediate. When there is a great time difference between decisions and results it is called a political agenda lag.

Political agenda is strongly tied to state centralization. The more centralized a state, the more citizens will likely try to affect the political agenda. For this reason, many political elites tend to prefer a non-centralized state where they can maintain more control over the political agenda.

The “Political Agenda Effect” asserts that when citizens from different backgrounds get together, their agenda will change in a way that takes their demands away from elites to focus more on public goods. The “Escalation Effect” contends that if citizens get together, this will induce elites to form national resources to fight against them and maintain the political agenda the way they desire.

The impact agenda is the increasing requirements for researchers to prove that there are real world impacts from their research. It is related to the political agenda because often governments measure a positive real world impact only in terms of the political agenda they have. When it comes to building the political agenda, there are three main models which are commonly cited: the outside initiative model, mobilization model, and inside initiative model.

The political agenda is essentially defined as what governmental officials find important to discuss. Those closest to the policy process have the biggest control on what issues reach the political agenda. They are the ones with the most power to decide which ideas or issues have the most importance, and which ideas or issues are unimportant. For example, the President of the United States, has the power to make treaties, appoint ambassadors, appoint judges of the supreme court etc. These types of powers ultimately shape what voices are present in parliament and subsequently what issues reach the political agenda.

Some non-governmental activist groups, such as neighborhood associations, advocate for civic beautification or improvement of communities. Many other important activist groups, like those oriented towards human rights and social justice, campaign for broad ideals. These groups work to put continuous pressure on government leaders that shape the agenda. If enough pressure is exerted onto political leaders through activist groups, it can change which issues and ideas ultimately reach the political agenda. For example the American Bar Association (ABA) and the American Medical Association (AMA), usually try to influence politicians on professional jobs.

Think tanks are in need of financial backing. Most times wealthy and established investors who wish to advance a certain idea or cause onto the political agenda establish them. These issues or causes may include: economics, taxes, foreign policy, global development, education, children and families, or healthcare. Examples of think tanks that promote a certain political perspective onto the political agenda are the Heritage Foundation and American Enterprise Institute which are highly conservative. On the other side, the Center for American Progress, are more liberal with their motives.

When the courts make a decision that changes a previous line of thinking, that idea immediately is on the political agenda because laws and public administration must change accordingly. The Mabo decision by the High Court in 1992 which overturned previous laws about establishing native titles is an example of this.

When something unexpected happens it can force the political agenda to change immediately. For example, when Hurricane Katrina or the World Trade Centre attacks occurred they were unexpected but priority changing events. When big world events (i.e. disasters/tragedies) occur they are often followed by a policy response as well, and so what issues and ideas reach the political agenda are sometimes changed simply due to what happened in the world.

There are three main theories on how political agendas are set and which groups have the greatest say in the decisions regarding them. They are; the pluralist theory, the elitist theory, and the institutional theory.

The pluralist theory suggests that policy-making is divided into several categories or “arenas”. Groups that do not have any power in one particular arena, most often have power in another arena. There is a marketplace for competing policies, and interests, and any group may win the arena. Elections often determine who gets to decide on each public policy.

In the elitist theory a main power elite dominates the entire agenda setting process to serve their own interests. These interests hold the power in all the arenas and they always win every election. There are very few people that actually organize into separate interest groups. In order to retain power and control, the main elite works at keeping key issues off the agenda. This suppression of issues threatens democracy.

This theory believes that legislative committees and bureaucratic institutions are the main controllers of the agenda. Because social interests and issues have much impact on what is considered by the legislative committees and bureaucratic institutions, individuals do not benefit from agenda decisions. This type of system leads to more conservative policy decisions than those under the pluralist scenario, but far more conservative than under the elite scenario.

The media is tightly linked to what issues gain importance on the political agenda. It affects what ideas become widespread and therefore what is demanded from politicians. Numerous studies have done research to prove this:

Hajo B Boomgaarden and Rens Vliegenthart write on the media’s relation to political agenda in their article "Explaining the rise of anti-immigrant parties: The role of news media content". In this article they study the media coverage on anti-immigration in the Netherlands for the period of 1990 to 2002 and found that it directly relates to the success of anti-immigration populist parties such as Centrumdemocraten (CD), the Centrum Partij (CP), and the Lijst Pim Fortuyn (LPF) during the same time period. Their analysis used the importance of news media as the explanatory factor of why anti-immigration gained prevalence on the political agenda, while controlling for other real world factors and developments at the time such as the influence of the economy, immigration, or the leadership of then President Pim Fortuyn. This was done by conducting a content analysis of five of the most popular Dutch national newspapers. The empirical results showed support of anti-immigration was around 4% in 1994, and rose to 16% in 2001 during the same time that media coverage of anti-immigration was at its peak. This means, the test showed that media content can be held at least partly responsible for the rise of anti-immigrant parties in the Netherlands and the changing of the political agenda in this way.

A similar study done by Julie Sevenans, Stefaan Walgrave & Gwendolyn Joanna Epping compares the behavior of politicians in comparison to the media on a global scale. The study was completed during one week, in Flemish Belgium. Every day, eight news outlets were studied and fully coded for a total of 2448 cases. The study looked at individual politicians cognitive attention for these specific news stories, via a face-to face survey of MPs to see if they recalled, hadtalked about, or considered the content covered in these news outlets. The results showed that the prominence and usefulness of a news story affect whether a news story is noticed, talked about or considered by MPs. This work showed that political agenda-setting effects most likely begin from the selective adoption on the cognitive, and individual level of MPs. Politicians both consume the news much how regular citizens by paying more attention the most prominent stories. However, they are also selective in that they pay the most attention to news that is political in nature or match their interests. More specifically, politicians pay more attention to: news that is more prominent, about the region their parliament is responsible for, issues they are personally specialized in, news about issues that are salient for their party, and news about politics. All of these claims were confirmed by statistical analysis. Relating to the political agenda, the implications of the fact that MPs care so much about media reports are twofold: some MPs may think media coverage is reflective of public opinion, while others may feel the media affect what the public sees as important. In either case, politicians are interpreting that the public cares about major news stories and taking this into account when setting the political agenda.

George Edwards and Dan Wood conducted a time series analysis of presidential, mass media and congressional attention to five political issues: crime, education, health care, U.S.-Soviet relations, and the Arab-Israeli conflict. The end conclusion was that most of the time presidents react corresponding to fluctuations in media attention on an issue. It too showed a relationship between the media and political agenda.

Although the media does often have an effect on the political agenda, these results are not always immediate. Dearing and Rogers (1996) conducted a study on this and concluded that time lags from what is in the media transferring in the political agenda can take up to a few weeks to several months.

The political agenda is tied to state centralization because the more centralized a state is, the more political elites have control over the political agenda. However if a state is too centralized, the more the public may feel they need they need to advocate to change the political agenda as well. The Political Agenda can be further broken down into two concepts: the political agenda effect, and the escalation effect.

The political agenda effect states that state centralization alters the dynamics of political action and conflict in society. State centralization, which involves elites coordinating nationally, induces citizens to organize nationally as well, rather than at the local or the “parochial” level. When this happens and citizens from different regions, sectors, interests, backgrounds, or ethnicity all join together to organize and discuss certain policies their agenda will change in a direction that switches their demands from power-holders to focus more on public goods. In this case then a state that has a higher level of centralization it may incite citizens to try to change the agenda themselves. Therefore, political elites might instead prefer a non-centralized state where they can still maintain more control over the political agenda. Elites may strategically opt for a non-centralized state in order to induce the citizens to not organize nationally and thus avert the political agenda effect.

The “Escalation Effect” contends that if citizens get together, it will force elites to form national resources to fight against them and maintain the political agenda the way they desire. In the case that citizens band together in a national organization, this will entice political elites to also form a national organization and pool their resources together in attempt to fight against the citizens. National organizations created by citizens might have a lower probability of success in comparison to organizations formed by elites, but in either case they will still indirectly benefit the weaker citizen groups, who would have otherwise remained unorganized. An escalation of the conflict can be seen as ensuing in this scenario.

The beginnings of the concept of the “impact agenda” can be traced to William Waldegrave’s 1993 white paper “Realizing Our Potential”. The impact agenda describes how there are increasing requirements set out by the state for researchers to relate their studies to real world issues in order to validate their research and access government funding. This is shown by the fact that the Biological and Sciences Research Council announced in 2012 that it expects its institutes to detail impact. This idea has been heavily criticized by scientists for allowing non-scientists to pick winners and losers and for constraining researchers to only create an impact that is aligned with the government's political agenda.

Roger Cobb, Jennie Keith Ross and Marc Howard Ross developed the “models of agenda building” theory to specify three different models: the outside initiative model, mobilization model, and inside initiative model. These models are designed to show the different ways the political agenda changes. The study related success of an idea being translated from the "public agenda" (being discussed regularly) to the “formal agenda” (government taking serious considerations into making changes in that specific area). Success in this study meant an issue was placed on the formal agenda and given attention by decision makers. Results showed that achieving agenda status is more difficult in modern nations than in smaller nations rooted in face-to-face interaction. More specifically:
The study also found that there are components of political agendas that hold true across nations and across different models:

The outside initiative model discusses the process where issues arise in non-governmental organizations and then are expanded to reach the formal agenda. The order of events starts with a grievance, an expansion of interest supported by nongovernmental groups, and then an exertion of pressure onto decision makers. It is about the process through which issues arise in non-governmental groups and are then expanded sufficiently to reach, first the public agenda and then the formal agenda. The outside initiative model is most prevalent in egalitarian societies.

The mobilization model is focused around political agenda issues that are initiated within government and subsequently reach the public agenda and formal agenda status. Its focus is on the internal mechanism and how politicians work to get ideas formalized onto the agenda. However, success in implementation does require support from the public under this model as well. The mobilization model is most commonly linked with hierarchical societies, or those societies which emphasize a wide gap between the leader and his or her followers.

The inside initiative model describes when issues are initiated within government, but supporters make no effort to expand it to the public. It is a model that is opposed to public participation. Instead, supporters of the issues rely solely on their own ability to apply the right amount of pressure to ensure formal agenda status. The inside access model is most often seen in societies with high concentrations of wealth and status.


</doc>
<doc id="3680801" url="https://en.wikipedia.org/wiki?curid=3680801" title="Head of state succession">
Head of state succession

Head of state succession is the process by which nations transfer leadership of their highest office from one person to another. The succession of a head of state can be brought about through various means, the most common of which include:


The changing of national leadership has been the topic of several films, novels, and television series, including the following:



</doc>
<doc id="4900279" url="https://en.wikipedia.org/wiki?curid=4900279" title="Interest articulation">
Interest articulation

Interest articulation is a way for members of a society to express their needs to a system of government. It can range from personal contact with government officials to the development of interest groups (e.g. trade unions, professional associations, religious groups) who act in the interest of larger groups of people. Interest articulation can have different effects in different types of government and can include both legal (i.e.: lobbying, peaceful protest, phone calls and letters to policymakers) and illegal activities (e.g. assassination, riots). Interest articulation leads to interest aggregation.

The types of interest groups, as identified by Gabriel Almond, are:



</doc>
<doc id="24493" url="https://en.wikipedia.org/wiki?curid=24493" title="Outline of public affairs">
Outline of public affairs

The following outline is provided as an overview of and topical guide to public affairs:

Public affairs – catch-all term that includes public policy as well as public administration, both of which are closely related to and draw upon the fields of political science and economics.








</doc>
<doc id="5162951" url="https://en.wikipedia.org/wiki?curid=5162951" title="Exclusive mandate">
Exclusive mandate

An exclusive mandate is a government's assertion of its legitimate authority over a certain territory, part of which another government controls with stable, "de facto" sovereignty. It is also known as a claim to sole representation or an exclusive authority claim. The concept was particularly important during the Cold War period when a number of states were divided on ideological grounds.

For nearly all of the 41 years that Germany was split into two countries, the Federal Republic of Germany (West Germany) claimed to be the sole legitimate successor to the German Reich that existed from 1871 to 1945. This claim was initially based solely on the government's mandate by virtue of free elections. To that end, it claimed Berlin, capital of united Germany from 1871 to 1945, as its capital, with the provisional capital in Bonn.

In a statement made before the Bundestag, German Chancellor Konrad Adenauer asserted this mandate as early as October 21, 1949, in response to the constitution of the German Democratic Republic (GDR) coming into effect. The Secretary of State Summit of the three western powers on September 18, 1950 in New York City, supported Chancellor Adenauer's claim.

When the Soviet Union proclaimed the sovereignty of the GDR, the West German Bundestag once again unanimously insisted that the Federal Republic was the sole legitimate representative of the German people. At the Treaties of Paris (""), at which the Federal Republic of Germany was admitted into the North Atlantic Treaty Organization, the allied nations adopted the position which the three western allies had already confirmed at the Nine-Power Conference in London: that the Federal Republic had the exclusive right to act on behalf of the entire German people in matters of foreign policy. The western nations thereby recognized the Federal Republic as the only lawful government for Germany as a whole.

Aside from such considerations pertaining to international law, the reunification clause of the Basic Law suggested that international recognition of the German Democratic Republic was to be avoided, so as not to sever the constitutional mandate to a unified German state.

Until 1973, the Federal Republic took a strict line in claiming an exclusive mandate for all of Germany. Under the Hallstein Doctrine, the Federal Republic broke diplomatic relations with states that maintained diplomatic relations with the GDR, except for the Soviet Union. On different levels, such as in international sports, there were, however, a wide range of international cooperations which even led to unified German teams in six Olympic Games. Over time, especially after the election of a social-liberal coalition led by Willy Brandt in 1969, the exclusive mandate was softened, as it severely limited the Federal Republic's domestic and international autonomy. Starting in 1973, under the "Ostpolitik" policy, the Federal Republic took the line that the Democratic Republic was a "de facto" government within a single German nation, in respect of which the Federal Republic was the sole representative "de jure", but limited to its own territorial extent; hence relinquishing any claim to be "de jure" the government of Germany as a whole.

Judicially, an exclusive mandate had been claimed to have arisen from the proposition that the German state as a whole had been preserved, that only one German state could legitimately exist, and that that one state was identical with the Federal Republic. The German Democratic Republic was therefore held to be an illegally constituted Soviet puppet state occupying territory that rightfully belonged to the Federal Republic, thus lacking autonomy. An alternate view held that the GDR was in a state of civil war with the FRG government, and therefore could not be recognized as a state under international law. A third, the so-called "umbrella state" theory, entails the existence of two fragment states under the umbrella of a single German nation that had been formed in 1871 and which had never actually been annihilated; this theory arose in the late 1960s and was maintained in a ruling of the Federal Constitutional Court of Germany of 31 July 1973 upholding the "Basic Treaty" by which relations between East and West Germany were normalised. Crucially, although the Constitutional Court reaffirmed the proposition that the pre-1945 German state had been preserved and organised, albeit partially, in the institutions of the Federal Republic, the Justices explicitly rejected the proposition that this would imply an exclusive mandate; "...identity does not require exclusivity".

With the admission of both German states to the United Nations in 1973, matters regarding the exclusive mandate were no longer relevant. Nevertheless, the Constitutional Court maintained that the Federal Republic continued to bear a responsibility for the whole German people; albeit that this responsibility could only be discharged in respect of Germans physically present in its territory or within its jurisdiction. Accordingly, the Federal Republic of Germany did not recognize a distinct citizenship for the German Democratic Republic; if East Germans presented themselves in West Germany, or at a West German embassy in a third country, they could obtain a West German passport. Generally, the Federal Republic considered East Germans to be German citizens under the old 1871–1945 all-German citizenship (i.e. "Bundesbürger", citizens of West Germany). Refugees who fled from the GDR were therefore not deported, and automatically qualified for West German citizenship.

In addition, visitors from the GDR would receive a West German passport upon request, for example, in order to ease travel to the United States. After the fall of the Berlin wall in November 1989, East Germans were greeted with Begrüßungsgeld (100 West German Deutsche Mark) and could travel freely within West Germany, while West German access to the East was still hindered for some weeks by visa and the Mindestumtausch mandatory minimum exchange of 25 DM.

The 1949 constitution of the German Democratic Republic also acknowledged that Germany was an indivisible republic, and thus there was only one German citizenship. The GDR, therefore, was also founded on the premise of being the "de jure" sovereign representative of all Germany. Initially, it regarded the West German regime as an illegally constituted NATO puppet state, a line accepted by most of the Eastern bloc. The GDR erected the Berlin Wall in 1961 partly to prevent Germans moving freely within Germany. In 1974, however, the reunification clause was stricken from the GDR's constitution. Thereafter, it regarded itself as a separate state from West Germany. The Communist regime collapsed in the fall of 1989. East Germany lingered on for another year until it declared its accession to the Federal Republic in the German reunification of 1990.

The 1919-1921 Irish Republic and the Republic of Ireland from 1938-1999 both claimed the entire island of Ireland, including Northern Ireland which is part of the UK. Article 2 of the Irish Constitution stated, "The national territory consists of the whole island of Ireland, its islands and the territorial seas." In 1999, following the Good Friday Agreement, Ireland passed the Nineteenth Amendment which deleted Article 2.

The Republic of China (ROC) was established in mainland China in 1912 following the conclusion of the Xinhai Revolution which led to the collapse of the Qing dynasty. The Chinese Civil War that broke out in 1927 was fought between the Kuomintang-led ROC government and the Communist Party of China (CPC).

Since the end of the Chinese Civil War in 1949, the "de facto" territories of the ROC are limited to the Taiwan Area which includes the island of Taiwan (ceded to the Empire of Japan in 1895 by the Qing dynasty of China; handover to the Republic of China in 1945) and several other islands. Meanwhile, the People's Republic of China (PRC), established in 1949 by the CPC, controls mainland China, Hong Kong and Macau. Officially, both the ROC and the PRC claim "de jure" sovereignty over all of China (including Taiwan), and regard the other government as being in rebellion.

Until 1971, the ROC was the representative of "China" at the United Nations (UN) and was a permanent member of the UN Security Council with veto power. In 1971, the PRC replaced the ROC as the representative of "China" at the UN. Since 1972, the ROC was excluded from all UN subcommittees. After the UN switched recognition from the ROC to the PRC, many states followed suit. As of 2020, the ROC maintains official diplomatic relations with 14 UN member states and the Holy See; many other states maintain unofficial relations with the ROC. The UN formally designates ROC-held territories as "Taiwan, Province of China" as of 2020.

The ROC currently participates in numerous international events and organizations under the name "Chinese Taipei" while the World Trade Organization officially refers to ROC-controlled territories as the "Separate Customs Territory of Taiwan, Penghu, Kinmen and Matsu".

The exclusive mandate claim of the ROC has softened with the rise of Taiwanese nationalism and Taiwan independence movement. The ROC's sovereignty claim over areas under the control of the PRC is also not actively pursued under the pro-independence Democratic Progressive Party-led government.

When North Korea and South Korea were created within months of each other in 1948, both claimed sovereignty over the entire Korean peninsula. Both states claimed that the other was an unlawfully constituted puppet state of the United States and the Soviet Union, respectively. In 1991, however, both nations joined the UN, as part of their reconciliation policy.

The Democratic Republic of Vietnam was proclaimed in 1945; the Republic of Vietnam gained its independence from France in 1954. While elections were intended to be held in 1955 to reunite the country, they never took place. For the next 20 years, both staked claims to all of Vietnam, claiming that the other was an illegally constituted puppet state. This continued until South Vietnam unconditionally surrendered to North Vietnam in 1975.

When some European countries (such as Switzerland) started recognizing North Vietnam towards the end of the Vietnam war, South Vietnam did not interrupt its diplomatic relations with them. Switzerland thus recognized North Vietnam in 1971 but also turned its consulate in Saigon (South Vietnam) into an embassy until the end of the war in 1975.

In 1979, Vietnam invaded and occupied Cambodia (at that time was ruled by the Khmer Rouge as Democratic Kampuchea) establishing the People's Republic of Kampuchea, but it was dismissed by the People's Republic of China as a "puppet state". At the time, both of the countries had disputed the claims of being the sole legitimate representative of all the Khmer people of Cambodia in the United Nations. This resulted in its seat being retained by the Coalition Government of Democratic Kampuchea, a coalition government formed in 1982 as a government in exile and composed of the royalist FUNCINPEC party, the republican Khmer People's National Liberation Front and the Khmer Rouge-backed Party of Democratic Kampuchea.

A similar situation occurred at the start of the Syrian Civil War in March 2011 when two governments claimed sovereignty over the whole Syria: The Syrian government headed by Bashar al-Assad and the various opposition groups seeking to remove Assad consisting of the National Coalition for Syrian Revolutionary and Opposition Forces, Syrian National Council and the Syrian Interim Government. Both entities are considered puppet entities backed by the Russian Federation/Iran and the United States/Saudi Arabia.

In addition, the Islamic State of Iraq and the Levant (ISIS/ISIL), a Sunni Islamist fundamentalist militant group, controlled part of the Syrian territory along with portions of neighbouring Iraq.

In a more ambiguous situation, the Kurdish territory of northeast Syria became controlled by Syrian Kurdish federal state Rojava when Syrian government forces left the area, or areas were liberated from ISIL occupation.



</doc>
<doc id="3628628" url="https://en.wikipedia.org/wiki?curid=3628628" title="Dual mandate">
Dual mandate

A dual mandate is the practice in which elected officials serve in more than one elected or other public position simultaneously. This practice is sometimes known as double jobbing in Britain and cumul des mandats in France; not to be confused with "double dipping" in the United States, i.e. being employed by and receiving a retirement pension from the same public authority at the same time. Thus, if someone who is already mayor of a town or city councillor becomes elected as MP or senator at the national or state legislature and retains both positions, this is a dual mandate.

Political and legal approaches toward dual mandate-holding vary widely. In some countries, dual mandates are a well-established part of the political culture; in others they may be prohibited by law. For example, in federal states, federal office holders are often not permitted to hold state office. In states with a presidential system of government, membership of the executive, the legislature, or the judiciary generally disqualifies a person from simultaneously holding office in either of the other two bodies. In states with bicameral legislatures, one usually cannot simultaneously be a member of both houses. The holder of one office who wins election or appointment to another where a dual mandate is prohibited must either resign the former office or refuse the new one.

A member of the European Parliament (MEP) may not be a member of the national legislature of a member state. This dates from a 2002 European Union decision, which came into effect at the 2004 European elections in most member states, at the 2007 national election in the Republic of Ireland, and at the 2009 European elections in the United Kingdom.

Originally, MEPs were nominated by national parliamentarians from among their own membership. Prior to the first direct elections in 1979, the dual mandate was discussed. Some advocated banning it, arguing that MEPs who were national MPs were often absent from one assembly in order to attend the other (indeed, the early death of Peter Michael Kirk was blamed by his election agent on overwork resulting from his dual mandate). Others claimed that members with a dual mandate enhanced communication between national and European assemblies. There was a particular interest in the dual mandate question in Denmark: Eurosceptic Danish Social Democrats supported a compulsory dual mandate, to ensure that the state's MEPs expressed the same views as the national legislature, and the government of Denmark supported a compulsory dual mandate when the other eight member states supported an optional dual mandate. However, a 1976 European Parliament law preparing for the 1979 elections expressly permitted a dual mandate. In 1978 the German politician Willy Brandt suggested that one third of MEPs should be national MPs.

Dual mandates are rare in Australia. It is illegal to be a member of any state parliament and the Australian parliament simultaneously. A member of a state parliament seeking federal office must resign before seeking election to the Federal Parliament. It is possible but unusual to be a member of a local government and another parliament. A recent example is Dr Kerryn Phelps who maintained her position as a Councillor on the City of Sydney Council while sitting in Federal Parliament as the Member for Wentworth between 2018 and 2019.

In 2004 Clover Moore became the independent member for Sydney in the NSW Parliament without resigning as Lord Mayor of Sydney. The issue of Moore holding both positions had brought the issue to the forefront in Australia and led the premier of New South Wales in 2012 to propose a new law, dubbed in the media as the "Get Clover bill", which banned this dual mandate. The proposed law was adopted and in September 2012 Moore resigned her NSW seat soon after she was reelected as mayor.

As in neighboring France, the culture of dual mandates is very strong in Belgium and that country currently has one of the highest percentage of dual mandate holders (MPs, aldermen, municipal councilors) in the world. During the 2003–2009 period, 87.3% of members of the Walloon (French-speaking) Parliament held dual mandates, followed by 86.5% in the Flemish (Dutch-speaking) Parliament, 82 0% in the Chamber of Representatives (the Federal lower house) and 68.9% in the Senate. During that same period, 76.5% of all European Parliament MPs from Belgium held dual mandates.

More than one-fifth of all Belgian MPs were mayor at the same time with, by far, by the highest proportion (40%) to be found in the Walloon Parliament.

In Canada dual mandates are rare and are frequently barred by legislation at the federal, provincial, or territorial level. At the federal level, section 39 of the Constitution Act, 1867 prevents a Senator from being elected as a Member of Parliament; similarly, s. 65(c) of the Canada Elections Act makes members of provincial or territorial legislatures ineligible to be candidates to the House of Commons. At the provincial level, the situation varies from one province to another.

In most circumstances, an elected official almost always resigns their first post when elected to another. Dual representation has occurred occasionally when the member was elected to a second office shortly before their other term of office was due to expire anyway and whereby the short time frame would not merit the cost of a special by-election. In 1996, for example, Jenny Kwan continued to be a Vancouver city councillor after being elected to the provincial legislature. The British Columbia legislature had debated a "Dual Office Prohibition Act" which failed to pass second reading.

In the first few years after Confederation in 1867, however, double mandates were common. In the first House of Commons, there were fifteen Members of Parliament from Quebec who simultaneously held seats in the National Assembly of Quebec, including the Premier Pierre-Joseph-Olivier Chauveau. There were also four members of Parliament from Ontario who also held seats in the Legislative Assembly of Ontario, including the first two Premiers, John Sandfield Macdonald and Edward Blake. Other prominent federal politicians with double mandates included George-Étienne Cartier, Christopher Dunkin, Hector Langevin, the second Premier of British Columbia Amor de Cosmos, and two members from Manitoba, Donald Smith and Pierre Delorme. Another famous example is that of the "de facto" leader of the Liberals, George Brown, who ran for both federal and provincial seats in 1867. Brown lost both elections, and soon thereafter began campaigning for the prohibition of double mandates.

The double mandate was prohibited from the start in Nova Scotia and New Brunswick; it was abolished in Ontario in 1872, in Manitoba in 1873, and in 1873 the federal parliament passed a law against it; Quebec passed its own law abolishing it in 1874.

However, dual mandates within a province remained legal. From 1867 to 1985, 305 mayors were also members of the Quebec legislative assembly (MLA). The two best-known cases were those of S.N. Parent who was simultaneously mayor of Quebec City (1894-1906), MLA and Premier of Quebec (1900-1905). Longtime Montreal Mayor Camilien Houde (1928–32, 1938–40) was also simultaneously MLA for a total of 2 /1/2 years during his mandates as mayor. However that type of dual mandate had virtually ceased when laws adopted in 1978 and 1980 prohibited MNAs from holding any local mandate.

It is common for the MPs of the Finnish Parliament to hold a mandate as a member of their local municipal council as well. 79 percent of MPs elected to parliament in 2011 were also municipal council members.

The "cumul des mandats" (, "accumulation of mandates") is a common practice in the French Fifth Republic (1958–present). It consists of simultaneously holding two or more elective offices at different levels of government — local, regional, national and European — as mayors, MPs, senators, Members of the European Parliament, and President of the General Council in their home regions. Sometimes, officials hold as many as four positions. While officials may not be elected to more than one office at the same level (such as being both an MP and a senator), they may hold offices in any combination at the municipal, departmental, regional, national and European levels. The "cumul des mandats" is controversial in France, being accused of fostering absenteeism and cronyism.

Several laws to limit the practice have been introduced in recent decades. By far the most coveted local mandate is that of mayor, traditionally a highly prestigious function in France.

A hotly debated law to prohibit all dual mandates, to take effect in 2017, was adopted in July 2013. Following the adoption of the law, former President Sarkozy and other members of the opposition UMP party have declared that if elected in 2017, their party would revise or even revoke that law. Many Socialist Party MPs and senators have also expressed their unease with the law imposed by President Hollande and might welcome a review of the law. In the meantime, the ubiquitous 'député-maire' (MP and mayor) and 'sénateur-maire' are still familiar figures of the French political scene.


Multiple mandates at the legislative level

Parliamentary mandates are incompatible with each other:


A member from one of the above assemblies can not combine its mandate with more than one of the following mandates :


Exceptions: They can hold a third office in a town of less than 3,500 inhabitants.

They may also hold a third office as a councillor, vice-president or president of an Urban community, an Agglomeration community or a "Communauté de communes", as these terms are elected by indirect universal suffrage, by municipal councils from among the councillors.

For example, a member of the National Assembly has the right to be general/regional councillor or President of a regional/general council. They cannot hold a third office unless they are the mayor, deputy mayor or municipal councillor of a city of less than 3,500 inhabitants.

In 2008, 85% of members of parliament held multiple posts Following the June 2012 legislative elections, it was still the case that 85% of all National Assembly members (438 deputies out of 577) held a double mandate (often as mayor of a mid- to large-size city) and 33 have four mandates. Currently, out of 348 senators, 152 are also mayors.

The accumulation of local mandates

They cannot have more than two local mandates.

The following mandates are incompatible each other:


For example, an elected official cannot be mayor and President of the Regional Council. However, all other local mandates are cumulative. A mayor can also be a general councillor and a president of a Regional Council can also be deputy-mayor of a city.

Exceptions are the same as those for parliamentarians (Cities of less than 3,500 inhabitants and the intercommunalities)

The accumulation of mandates and governmental functions 

A member of the French government cannot be a member of any assembly. However, he may retain any local mandate he or she holds. A cabinet minister can exercise a maximum of two local mandates in addition to his or her government function.

For example, the Prime Minister, a Minister or Secretary of State can be mayor, or President of a general, regional or intercommunal council or sit in one of these assemblies.

Currently, over two-thirds of the members of the French government are engaged in one or two more local mandates.

The rationales for holding multiple offices are varied. Holding a seat in the Senate, National Assembly, or European Parliament gives local mayors a valuable method of tapping funds to develop their home cities and regions. It also can give opportunities to curry favor with other important officials, with opportunities at each level. Salaries for positions can be combined (to a point) as well. For politicians with national ambitions, retaining a position in a local town can give them a down-to-earth aura that can appeal to voters. These advantages have made politicians very wary of reducing the practice of the "cumul" with legislation despite other moves to end perceptions of favoritism and corruption among politicians.

It has been common practice in France since the French Third Republic (1870). But there are also many cases of "cumul" before this period, for example, the writer Alexis de Tocqueville was a member from 1839 to 1851. In 1849 he was appointed Foreign Affairs minister, and at the same time he was elected President of the General Council of Manche from 1849 to 1851 (councillor from 1842 to 1852).

There are several reasons for this phenomenon, and one of them is that France has a long tradition of centralization, compared to countries such as Germany, Italy, and Spain. Local governments have less power and skills than that the "Länder" of Germany, or "Autonomous Communities" of Spain. The local mandates in France are less important than in other countries, and therefore politicians have more time to devote to a parliamentary mandate.

The "cumul" is a widespread practice and has grown much more prevalent in modern France. In 1946, 36 percent of deputies in the National Assembly held an additional office. By 1956, this number had already increased to 42 percent and by 1970, 70 percent of deputies held an additional elected office; in 1988, 96 percent did.

Many of the most prominent politicians in France make use or have made use of the "cumul". Jacques Chirac served as Mayor of Paris between 1977 and 1995. During this same time, Chirac also served as a deputy in the National Assembly from Corrèze, briefly as Member of the European Parliament, and even as Prime Minister between 1986 and 1988. Former Prime Minister Pierre Bérégovoy served concurrently as mayor of Nevers and deputy of Nièvre in the mid-1980s.

According to French law against accumulation of electoral mandates, Yves Jégo should have resigned from one of the following mandates before the 21st of April 2010 (one month after the Regional elections) :
But giving as a pretext a legal complaint from the Front National's candidates, he held the three of them for more than a year, plus his local mandate of president of the communauté de communes des deux fleuves (CC2F).

Lionel Jospin (Prime Minister from 1997 to 2002) imposed on his government ministers an unwritten rule of having no local office. For example, Catherine Trautmann stepped down as Mayor of Strasbourg (while remaining a member of the city council) to become Minister of Culture; conversely, Martine Aubry stepped down from the Ministry of Labour when elected Mayor of Lille in 2001. This rule was more or less upheld by Jacques Chirac during the governments of Jean-Pierre Raffarin and Dominique de Villepin for the 2002-2007 term, with a few notable exceptions (Jean-François Copé was mayor of Meaux, Nicolas Sarkozy was President of the Hauts-de-Seine General Council); for instance, Philippe Douste-Blazy had to step down from the Toulouse mayorship upon joining the government.

, no such rule was stated for the François Fillon government: Alain Juppé, former Minister for Development was mayor of Bordeaux, and was defeated in his National Assembly constituency (a third cumulative mandate) by 50.9% to 49.1% of the votes by the Socialist candidate. Additionally, Hervé Morin, the Minister of Defense, is mayor of Épaignes, and Éric Besson, Minister of Immigration and National Identity, is the mayor of Donzère.

In Hong Kong, dual mandate is common for members of the territory's Legislative Council, who serve concurrently as members of one of the territory's eighteen district councils. Before the abolition of the two municipal councils in the territory in 1999, it was common for politicians to serve concurrently at all three levels.

The instability caused by the close result of the 1981 general election was exacerbated by the number of government TDs who also served as MEPs and for whom the opposition refused pairing when they were abroad. This led to further elections in February 1982 and again in November.

In 1991, cabinet ministers and junior ministers were prohibited from serving as local councillors. The prohibition was extended to other Oireachtas members by the Local Government (No. 2) Act 2003, an amendment to the Local Government Act 2001. Attempts to include it in the 2001 Act failed after a rebellion by Fianna Fáil backbenchers; the 2003 Act passed after a compensation package was agreed for those losing out.

The 2001 Act prohibited being a member of multiple county or city councils, or multiple town councils, or both a town and city council. Brian O'Shea was a member of both Waterford City Council and Waterford County Council until 1993. County councillors were allowed to sit on a town council, and many did so. The 2003 Act provided that a candidate elected simultaneously to a forbidden combination of local councils has three days to choose which seat to take up, with the other or others then being considered vacant. The Local Government Reform Act 2014 abolished town councils and instead subdivided most counties into municipal districts; the county council's members are the district councillors for all districts within the county.

Dual mandate is common in Malaysia. According to the Federal Constitution, a Member of Parliament (MP), whether elected to the Dewan Rakyat or appointed to the Dewan Negara, cannot hold membership in both houses of the Parliament. However, an MP may be elected as member of a State Legislative Assembly (MLA) at the same time. Consequently, an MP may be appointed to the State Cabinet through appointment as a nominated MLA while an MLA may be appointed as Minister or Deputy Minister in the Federal Government due to having membership in the Parliament.

At present:

Per the Spanish Constitution, legislators in the regional assemblies of the Autonomous Communities are barred from being elected to a seat in the Congress of Deputies, the lower house of the Cortes Generales. More precisely, regional legislators can run for the seat, but if elected they must choose between the regional and national parliaments. Nevertheless, members of lower tiers of the Spanish decentralized structure, such as provincial councillors or members of local councils, including mayors, can and have held seats in the Congress of Deputies. The rule barring regional legislators does not apply to the upper house of the Cortes, the Senate: in fact, regional legislatures are entitled to appoint a varying number of members from their ranks to the Senate, according to the population of the region. Currently, the Autonomous Communities appoint 56 Senators, the other 208 being directly elected in general elections.

At the EU level, prior to the 2009 European Parliament elections, there were a small number of members of the European Parliament who were also members of the House of Lords. However, it is now European law that a member of the European Parliament (MEP) may not be a member of the legislature of a member state. This, with regard to the United Kingdom, therefore applies to the House of Commons and the House of Lords, as the constituent bodies forming that member state's legislature. As it is impossible to disclaim a life peerage, it has been ruled that peers (who sit as members of the House of Lords) must take a "leave of absence" from the Lords in order to be an MEP; this is also the procedure for when a peer is the UK's European Commissioner, which has in recent times usually been the case.

There have been members of the House of Commons also holding seats in the devolved bodies in Scotland, Wales and Northern Ireland. The November 2009 report by the Committee on Standards in Public Life into the controversy surrounding MPs' expenses noted that "double jobbing" was "unusually ingrained in the political culture" of Northern Ireland, where 16 of 18 MPs were MLAs, compared to one Scottish MP being an MSP (First Minister Alex Salmond), and no Welsh MPs being AMs. The Committee recommended that Westminster ban multiple mandates from the 2011 assembly elections. Parties in Northern Ireland agreed to a ban from the 2015 elections. The ability to dual mandate between the Assembly of Northern Ireland and the House of Commons (or the Irish Dáil Éireann) ended as from the 2016 Northern Ireland Assembly election following the Northern Ireland (Miscellaneous Provisions) Act 2014. The Wales Act 2014 also applied a similar restriction on the National Assembly for Wales (in that its members cannot also be members of the House of Commons) as from the assembly election in 2016. As of 2019, it remains possible for members of the Scottish Parliament to be members of the UK Parliament, though at present none are.

In circumstances other than the Greater Manchester mayoralty, UK law does not prohibit a member of the House of Commons or the House of Lords from being simultaneously a mayor or council leader. They are also not allowed to serve as a Councillor for a constituent council if elected as a directly elected Mayor. Thus Ken Livingstone remained MP for Brent East until the dissolution of Parliament despite his election as Mayor of London a year before. Boris Johnson resigned his seat as MP for Henley on being elected mayor in 2008, but became an MP again in 2015, a year prior to the end of his second term as mayor (he did not seek a third term). Sadiq Khan, elected as the Labour mayor in the 2016 election, resigned his seat as MP for Tooting soon after his election to the mayoralty. Numerous members of the House of Lords however hold positions in local government.

At a lower level, it is common for people to hold seats on both a district council and a county council. Several MPs have also retained their council seats, most often until the expiration of their terms; Mike Hancock simultaneously held a council seat and a seat in Parliament between his election to Parliament in 1997 and his defeat in the local elections in 2014.

Some members of the Irish republican party Sinn Féin held the office as a Member of The Northern Ireland Assembly and Member of Parliament within the respective houses. Martin McGuinness, former Deputy First Minister of Northern Ireland held the office as Member of the Northern Ireland Assembly and simultaneously was a MP in the House of Commons. However, in 2012 Sinn Féin committed to end dual jobbing; this resulted in McGuinness' resignation from the House of Commons in 2013.

"The term dual mandate is also applied to the twin objectives of the Federal Reserve System: to control inflation and promote employment."

The United States Constitution prohibits members of the Senate or House from holding positions within the Executive Branch (Art. I, Sec. 6, cl. 2), and limits the president to his salary as chief executive, saying he may not "receive... any other Emolument from the United States, or any of them" (Art. II, Sec. 1, cl. 7). However, the Constitution places no restrictions that would prevent state or local office holders from simultaneously holding office in any branch of the federal government.

Historically, the U.S. inherited many basic political traditions from Great Britain, which in the eighteenth century tolerated several different forms of dual mandate. Following the establishment of the original Continental Congress and later Confederation Congress, the states possessed absolute discretion in regards to how delegates were chosen to serve, and it became common for state legislatures to appoint members from within their own ranks to Congress. At the time, this was a largely uncontroversial practise since it was widely assumed that the Congress would have relatively little to do (especially in peacetime) and that most of the consequential decision-making would take place at the state and local levels. A ban on dual mandates would therefore have been widely seen as unnecessary and unwelcome as it would have effectively barred Congressional delegates from what were perceived to be more important political posts, thus making election to the national Congress (already seen as a considerable burden due to the difficulties of eighteenth century travel) quite undesirable.

During the convention that established the present U.S. constitution, attention was primarily given to designing a federal government with branches that would be able operate independently of each other and free of undesirable foreign influence, which resulted in the aforementioned prohibitions. Barring state and local officials from federal office was not seriously debated. If it had been, it would likely have been fiercely opposed especially by the nascent anti-Federalist movement, many of whose members were keen to ensure that state officials with a vested interest in defending states' rights would be allowed to also serve simultaneously at the federal level, especially in Congress.

For the first few decades after the First United States Congress convened in 1789, Congress met infrequently and some states endeavored to accommodate dual mandates by holding their legislative sessions at times that would not conflict with Congressional sessions. Eventually, as the federal government grew in importance, Congress came to be seen as a source of great power. This created the potential for conflicts of interest and made it increasingly difficult to justify the holding of mandates at different levels of government to voters. In a closely related development, Congress began meeting more frequently than originally intended, which eventually made it impractical in most states for one person to serve simultaneously in the state and federal governments.

In time, the vast majority of states banned dual state and federal mandates. Today, the practice is forbidden by many state constitutions of many U.S. states, but as of January, 2018 it was still legal in Connecticut, only for municipal offices. Unlike many other attempts at the state level intended to place additional restrictions besides those in the U.S. Constitution regarding who can represent them in Congress, most of which have been ruled unconstitutional by the United States Supreme Court, state-originated bans on dual mandates are constitutional because their prohibitions technically restrict who is allowed to serve at the state and/or local level (i.e. they typically place some sort of "de jure" prohibition barring federal officials from simultaneously serving at the state and/or local levels, resulting in a "de facto" prohibition on the reverse arrangement occurring).

Unlike many federations, U.S. states do not generally restrict state or federal officials from seeking office at another level of government without resigning their existing offices first. For example, in the four U.S. presidential elections contested from 1988 to 2000 inclusive, three sitting state governors were nominated for the presidency, these being Michael Dukakis in 1988, Bill Clinton in 1992 and George W. Bush in 2000 (Clinton and Bush were elected president), while in 2016 sitting governor Mike Pence was elected vice president. Elsewhere, serving state officials often seek federal office, one prominent example being Illinois State Senator Barack Obama's election to the United States Senate in 2004 - Obama quickly resigned from the Illinois Senate after being elected to the U.S. Senate despite not being legally required to do so, and served as a U.S. senator until 2008 when he was elected president. Also, it is not uncommon for sitting federal officials to contest election to state offices, although in these cases the office sought is usually one of the state's highest political posts, typically governor - one such recent example being the aforementioned Mike Pence who was a sitting U.S. Representative when he was first elected governor.

Also typically permitted is for one person to seek multiple offices at the same level of government in the same election, although attempting to simultaneously seek multiple offices in the same branch of government (e.g. a sitting U.S. Representative seeking re-election to the House "and" election to the U.S. Senate) is severely frowned on and prohibited in many states (the constitutionality of these prohibitions is uncertain). Recent examples include three 2000 and 2012 presdential elections where Senator Joe Lieberman and Representative Paul Ryan respectively sought re-election and election to the vice presidency - neither was elected vice president, but both were re-elected to the offices in which they were the incumbents.

In April 1984, Governor of Florida Bob Graham received legislation that passed unanimously in both houses of the Florida Legislature that would forbid public officials from receiving retirement pay and regular pay simultaneously for the same position.

In August 2008, Governor of Illinois Rod Blagojevich proposed legislation that would prohibit dual-office holding as part of changes to the state's ethics bill, stating that "dual government employment creates the potential for a conflict of interest because a legislator's duties to his or her constituents and his or her public employer are not always consistent." Critics, such as Representative Susana Mendoza, called the actions "spite" on the part of the governor.

Fulfilling a campaign pledge that he had made when first running for the New Jersey Legislature, Jack Sinagra sponsored a bill passed by the New Jersey Senate in 1992 that would ban the practice. At the time that the legislation first passed, there were some twenty elected officials who served in the New Jersey Legislature and another elected office, including Assemblyman Bill Pascrell, who was also mayor of Paterson, New Jersey; State Senator Ronald Rice, who also served on the Newark City Council; and Assemblyman John E. Rooney, who was also mayor of Northvale. These officials protested the proposed ban as interfering with the will of voters to elect officials as they see fit. A newspaper called former State senator Wayne R. Bryant the "king of double dipping" because he was collecting salaries from as many as four public jobs he held simultaneously.

Governor of New Jersey Jon Corzine signed legislation in September 2007 that banned the practice statewide, but the 19 legislators holding multiple offices as of February 1, 2008, were grandfathered into the system and allowed to retain their positions. As of November 2019, only four of the nineteen (listed in bold) continue to hold a dual mandate.

Senators:
Assembly members:

In February 2001, Jean Schmidt introduced legislation in the Ohio House of Representatives that would forbid public officials from receiving a government pension while still serving in office.




</doc>
<doc id="16553762" url="https://en.wikipedia.org/wiki?curid=16553762" title="Legislative calendar">
Legislative calendar

A legislative calendar is used by legislatures in the United States to plan their business during the legislative session. 

Typically, one of the first items mentioned on the calendar is passing the bill enacting procedures and deadlines for the session. Time may also be allotted for considering the budget bill, which is usually the major item of business in a session. The calendar may provide scheduled committee hearings and generally includes many important deadlines. 

For instance, California has a fiscal deadline, which is the date on the legislative calendar by which all bills with fiscal effect must have been taken up in a policy Committee and referred to a fiscal Committee; any fiscal bill missing the deadline is considered "dead" unless it receives a rule waiver allowing further consideration. 

Some legislatures have a "crossover day," which is the point in the session after which each house only considers legislation sent to it by the other house. In the U.S. Congress, the phrase "placed on calendar" accompanies a bill that is pending before committees of either house; the bill is assigned a calendar number, which determines when it will be considered by that house.

On a legislative calendar, a "legislative day" is a day on which the Legislature actually meets. The Virginia General Assembly has six legislative days per week (Monday through Saturday), probably reflecting the desire to have a citizen legislature that accomplishes its business in a relatively short, intense annual session, after which the members return to their full-time employment. The Oklahoma legislature, by contrast, has four legislative days per week.

The daily version of the legislative calendar is sometimes called the daily file, agenda or calendar, which lists all the bills that will be considered on a given day. 

The term "legislative calendar" can also refer to the final published compilation of the action on each instrument during a legislative session. It can also refer to a list of legislation available to be heard by the Legislature.



</doc>
<doc id="4927404" url="https://en.wikipedia.org/wiki?curid=4927404" title="Legislative session">
Legislative session

A legislative session is the period of time in which a legislature, in both parliamentary and presidential systems, is convened for purpose of lawmaking, usually being one of two or more smaller divisions of the entire time between two elections. In each country the procedures for opening, ending, and in between sessions differs slightly. A session may last for the full term of the legislature or the term may consist of a number of sessions. These may be of fixed duration, such as a year, or may be used as a parliamentary procedural device. A session of the legislature is brought to an end by an official act of prorogation. In either event, the effect of prorogation is generally the clearing of all outstanding matters before the legislature.

Historically, each session of a parliament would last less than one year, ceasing with a prorogation during which legislators could return to their constituencies. In more recent times, development in transportation technology has permitted these individuals to journey with greater ease and frequency from the legislative capital to their respective electoral districts (sometimes called "ridings", electorate, division) for short periods, meaning that parliamentary sessions typically last for more than one year, though the length of sessions varies. Legislatures plan their business within a legislative calendar, which lays out how bills will proceed before a session ceases, although related but unofficial affairs may be conducted by legislators outside a session or during a session on days in which parliament is not meeting.

While a parliament is prorogued, between two legislative sessions, the legislature is still constitutedi.e. no general election takes place and all Members of Parliament thus retain their seats. In many legislatures, prorogation causes all orders of the bodybills, motions, etc.to be expunged. Prorogations should thus not be confused with recesses, adjournments, or holiday breaks from legislation, after which bills can resume exactly where they left off. In the United Kingdom, however, the practice of terminating all bills upon prorogation has slightly altered; public bills may be re-introduced in the next legislative session, and fast-tracked directly to the stage they reached in the prorogued legislative session.

A new session will often begin on the same day that the previous session ended. In most cases, when parliament reconvenes for a new legislative session, the head of state, or a representative thereof, will address the legislature in an opening ceremony.

In both parliamentary and presidential systems, sessions are referred to by the name of the body and an ordinal numberfor example, the "2nd Session of the 39th Canadian Parliament" or the "1st Session of the 109th United States Congress".

Governments today end sessions whenever it is most convenient, but a “good faith exercise of the power” to prorogue parliament does not include preventing it from frustrating the prime minister's agenda. When the Parliament of the Kingdom of Italy conquered the power to decide on its recalling, the MP Modigliani spoke of a "coup d'état", in the event that the right to prorogue or close the session was exercised immediately after Parliament had recalled itself.

During the electoral campaign, this break takes place so as to prevent the upper house from sitting and to purge all upper chamber business before the start of the next legislative session. It is not uncommon for a session of parliament to be put into recess during holidays and then resumed a few weeks later exactly where it left off.

In Commonwealth realms, legislative sessions can last from a few weeks to over a year; between general elections; there are usually anywhere from one to six sessions of parliament before a dissolution by either the Crown-in-Council or the expiry of a legally mandated term limit. Each session begins with a speech from the throne, read to the members of both legislative chambers either by the reigning sovereign or a viceroy or other representative. Houses of parliament in some realms will, following this address, introduce a "pro forma" bill as a symbol of the right of parliament to give priority to matters other than the monarch's speech (always written by the cabinet of the day).

In the parliament of the United Kingdom, prorogation is immediately preceded by a speech to both legislative chambers, with procedures similar to the Throne Speech. The monarch usually approves the oration—which recalls the prior legislative session, noting major bills passed and other functions of the government—but rarely delivers it in person, Queen Victoria being the last to do so. Instead, the speech is presented by the Lords Commissioners and read by the Leader of the House of Lords. When King Charles I dissolved the Parliament of England in 1628, after the Petition of Right, he gave a prorogation speech that effectively cancelled all future meetings of the legislature, at least until he again required finances.

Prior to 1977, it was common for the federal Parliament to have up to three sessions, with Parliament being prorogued at the end of each session and recalled at the beginning of the next. This was not always the case, for instance the 10th Parliament (1926–1928) went full term without prorogation. The practice of having multiple sessions in the same parliament gradually fell into disuse, and all parliaments from 1978 to 2013 had a single session. (There were only four prorogations since 1961, twice to allow the visiting Queen to "open" Parliament, once after the death of Prime Minister Harold Holt and for political reasons in 2016.) Since 1990, it has been the practice for the parliament to be prorogued on the same day that the House is dissolved so that the Senate will not be able to sit during the election period.

However, on 21 March 2016, Prime Minister Malcolm Turnbull announced that the 44th Parliament, elected in 2013, would be prorogued on 15 April and that a second session would begin on 18 April. Prorogation is now a procedural device, the effect of which is to call the Parliament back on a particular date (especially the Senate, which the government did not control), and to wipe clean all matters before each House, without triggering an election.

In the Parliament of Canada and its provinces, the legislature is typically prorogued upon the completion of the agenda set forth in the Speech from the Throne (called the "legislative programme" in the UK). It remains in recess until the monarch, governor general, or lieutenant governor summons parliamentarians again. Historically, long prorogations allowed legislators to spend part of their year in the capital city and part in their home ridings. However, this reason has become less important with the advent of rapid transcontinental travel.

More recently, prorogations have triggered speculation that they were advised by the sitting prime minister for political purposes: for example, in the 40th Parliament, the first prorogation occurred in the midst of a parliamentary dispute, in which the opposition parties expressed intent to defeat the minority government, and the second was suspected by opposition Members of Parliament to be a way to avoid investigations into the Afghan detainees affair and triggered citizen protests. In October 2012, the provincial legislature of Ontario was prorogued under similar circumstances, allegedly to avoid scrutiny of the provincial Government on a number of issues.

Bills are numbered within each session. For example, in the federal House of Commons each session's government bills are numbered from C-2 to C-200, and the numbering returns again to C-2 following a prorogation (Bill C-1 is a "pro-forma" bill).

In the United States, some state legislatures meet only part of the year. Depending upon limitations of the state's constitution, if business arises that must be addressed before the next regular session, the governor may call a special session.

The US Congress is renewed every two years as required by the US Constitution, with all members of the House of Representatives up for reelection and one-third of the members of the US Senate up for reelection. (Senators serve a six-year term; House members serve a two-year term). Each Congress sits in two sessions lasting approximately one year. Thus, the 1st session of the 114th Congress commenced on January 3, 2015 and the 2nd session commenced on January 3, 2016, with the same members and no intervening election. All legislative business, however, is cleared at the end of each session. It is common for bills to be reintroduced in the second session that were not passed in the first session, and the restrictions on reconsideration only apply to a single session.

When the leaders of the majority party in each house have determined that no more business will be conducted by that house during that term of Congress, a motion is introduced to adjourn "sine die", effectively dissolving that house. Typically, this is done at some point after the general congressional election in November of even-numbered years. If the party in power is retained, it may happen as early as mid-November and members return to their districts for the holiday season. However, when the party in power is ousted or if important business, such as approval of appropriation bills, has not been completed, Congress will often meet in a lame-duck session, adjourning as late as December 31, before the newly elected Congress takes office on January 3.



</doc>
<doc id="5670063" url="https://en.wikipedia.org/wiki?curid=5670063" title="Favourite">
Favourite

A favourite (British) or favorite (American English) was the intimate companion of a ruler or other important person. In post-classical and early-modern Europe, among other times and places, the term was used of individuals delegated significant political power by a ruler. It was especially a phenomenon of the 16th and 17th centuries, when government had become too complex for many hereditary rulers with no great interest in or talent for it, and political institutions were still evolving. From 1600 to 1660 there were particular successions of all-powerful minister-favourites in much of Europe, particularly in Spain, England, France and Sweden.

The term is also sometimes employed by writers who want to avoid terms such as "royal mistress", "friend", "companion", or "lover" (of either sex). Several favourites had sexual relations with the monarch (or the monarch's spouse), but the feelings of the monarch for the favourite ran the gamut from a simple faith in the favourite's abilities to various degrees of emotional affection and dependence, and sometimes even encompassed sexual infatuation.

The term has an inbuilt element of disapproval and is defined by the "Oxford English Dictionary" as "One who stands unduly high in the favour of a prince", citing Shakespeare: "Like favourites/ Made proud by Princes" ("Much Ado about Nothing", 3.1.9).

Favourites inevitably tended to incur the envy and loathing of the rest of the nobility, and monarchs were sometimes obliged by political pressure to dismiss or execute them; in the Middle Ages nobles often rebelled in order to seize and kill a favourite. Too close a relationship between monarch and favourite was seen as a breach of the natural order and hierarchy of society. Since many favourites had flamboyant "over-reaching" personalities, they often led the way to their own downfall with their rash behaviour. As the opinions of the gentry and bourgeoisie grew in importance, they too often strongly disliked favourites. Dislike from all classes could be especially intense in the case of favourites who were elevated from humble, or at least minor, backgrounds by royal favour. Titles and estates were usually given lavishly to favourites, who were compared to mushrooms because they sprang up suddenly overnight, from a bed of excrement. The King's favourite Piers Gaveston is a "night-grown mushrump" (mushroom) to his enemies in Christopher Marlowe's "Edward II".
Their falls could be even more sudden, but after about 1650, executions tended to give way to quiet retirement. Favourites who came from the higher nobility, such as Leicester, Lerma, Olivares, and Oxenstierna, were often less resented and lasted longer. Successful minister-favourites also usually needed networks of their own favourites and relatives to help them carry out the work of government – Richelieu had his "créatures" and Olivares his "hechuras". Oxenstierna and William Cecil, who both died in office, successfully trained their sons to succeed them.

The favourite can often not be easily distinguished from the successful royal administrator, who at the top of the tree certainly needed the favour of the monarch, but the term is generally used of those who first came into contact with the monarch through the social life of the court, rather than the business of politics or administration. Figures like William Cecil and Jean-Baptiste Colbert, whose accelerated rise through the administrative ranks owed much to their personal relations with the monarch, but who did not attempt to behave like grandees of the nobility, were also often successful. Elizabeth I had Cecil as Secretary of State and later Lord High Treasurer from the time she ascended the throne in 1558 until his death 40 years later. She had more colourful relationships with several courtiers; the most lasting and intimate one was with Robert Dudley, Earl of Leicester, who was also a leading politician. Only in her last decade was the position of the Cecils, father and son, challenged by Robert Devereux, 2nd Earl of Essex, when he fatally attempted a coup against the younger Cecil.

Cardinal Wolsey was one figure who rose through the administrative hierarchy, but then lived extremely ostentatiously, before falling suddenly from power. In the Middle Ages in particular, many royal favourites were promoted in the church, English examples including Saints Dunstan and Thomas Becket; Bishops William Waynflete, Robert Burnell and Walter Reynolds. Cardinal Granvelle, like his father, was a trusted Habsburg minister who lived grandly, but he was not really a favourite, partly because most of his career was spent away from the monarch.

Some favourites came from very humble backgrounds: Archibald Armstrong, jester to James I of England infuriated everyone else at court but managed to retire a wealthy man; unlike Robert Cochrane, a stonemason (probably a senior one, more like an architect than an artisan) who became Earl of Mar before the Scottish nobles revolted against him, and hanged him and other low-born favourites of James III of Scotland. Olivier le Daim, the barber of Louis XI, acquired a title and important military commands before he was executed on vague charges brought by nobles shortly after his master died, without the knowledge of the new king. It has been claimed that le Daim's career was the origin of the term, as "favori" (the French word) first appeared around the time of his death in 1484. "Privado" in Spanish was older, but was later partly replaced by the term "valido"; in Spanish, both terms were less derogatory than in French and English.

Such rises from menial positions became progressively harder as the centuries progressed; one of the last families able to jump the widening chasm between servants and nobility was that of Louis XIV's valet, Alexandre Bontemps, whose descendants, holding the office for a further three generations, married into many great families, even eventually including the extended royal family itself. Queen Victoria's John Brown came much too late; the devotion of the monarch and ability to terrorize her household led to hardly any rise in social or economic position.

In England, the scope for giving political power to a favourite was reduced by the growing importance of Parliament. After the "mushroom" Buckingham was assassinated by John Felton in 1628, Charles I turned to Thomas Wentworth, 1st Earl of Strafford, who had been a leader of Parliamentary opposition to Buckingham and the King, but had become his supporter after Charles made concessions. Strafford can therefore hardly be called a favourite in the usual sense even though his relationship with Charles became very close. He was also from a well-established family, with powerful relations. After several years in power, Strafford was impeached by a Parliament now very hostile to him. When that process failed, it passed a bill of attainder for his execution without trial, and it put enough pressure on Charles that to his subsequent regret, Charles signed it, and Strafford was executed in 1641. There were later minister-favourites in England, but they knew that the favour of the monarch alone was not sufficient to rule, and most also had careers in Parliament.

In France, the movement was in the opposite direction. On the death of Cardinal Mazarin in 1661, the 23-year-old Louis XIV determined that he would rule himself, and he did not allow the delegation of power to ministers that had marked the previous 40 years. The absolute monarchy pioneered by Cardinal Richelieu, Mazarin's predecessor, was to be led by the monarch himself. Louis had many powerful ministers, notably Jean-Baptiste Colbert, in finances, and François-Michel le Tellier, Marquis de Louvois, the army, but overall direction was never delegated, and no subsequent French minister ever equaled the power of the two cardinals.

The Spanish Habsburgs were not capable of so much energy, but when Olivares was succeeded by his nephew, Luis Méndez de Haro, the last real "valido", the control of government into a single pair of hands had already been weakened.

Favourites were the subject of much contemporary debate, some of it involving a certain amount of danger for the participants. There were a large number of English plays on the subject, amongst the best known to be Marlowe's "Edward II" in which Piers Gaveston is a leading character, and "Sejanus His Fall" (1603), for which Ben Jonson was called before the Privy Council, accused of "Popery and treason", as the play was claimed by his enemies to contain allusions to the contemporary court of James I of England. Sejanus, whose career under Tiberius was vividly described by Tacitus, was the subject of numerous works all around Europe. Shakespeare was more cautious, and with the exceptions of Falstaff, badly disappointed in his hopes of becoming a favourite, and Cardinal Wolsey in "Henry VIII", he gives no major parts to favourites.

Francis Bacon, almost a favourite himself, devoted much of his essay "On Friendship" to the subject, writing as a rising politician under Elizabeth I:
It is a strange thing to observe, how high a rate great kings and monarchs do set upon this fruit of friendship, whereof we speak: So great, as they purchase it, many times, at the hazard of their own safety and greatness. For princes, in regard of the distance of their fortune from that of their subjects and servants, cannot gather this fruit, except (to make themselves capable thereof) they raise some persons to be, as it were, companions and almost equals to themselves, which many times sorteth to inconvenience. The modern languages give unto such persons the name of "favorites", or privadoes ... . And we see plainly that this hath been done, not by weak and passionate princes only, but by the wisest and most politic that ever reigned; who have oftentimes joined to themselves some of their servants; whom both themselves have called friends, and allowed other likewise to call them in the same manner; using the word which is received between private men.

Writing of George III's old tutor, John Stuart, who became Prime Minister. Lord Macaulay wrote in 1844: "He was a favourite, and favourites have always been odious in this country. No mere favourite had been at the head of the government since the dagger of Felton had reached the heart of the Duke of Buckingham".






</doc>
<doc id="25202419" url="https://en.wikipedia.org/wiki?curid=25202419" title="Government spin-off">
Government spin-off

Government spin-off is civilian goods which are the collateral result of military or governmental research. One prominent example of a type of government spin-off is technology that has been commercialized through NASA funding, research, licensing, facilities, or assistance. NASA spin-off technologies have been publicized by the agency in its "Spinoff" publication since 1976.

The Internet is a specific example of a government spin-off resulting from DARPA funding.

In some fields, such as computer hardware, private sector development has outpaced government and military research, and the government procures commercial off-the-shelf products for many applications.


</doc>
<doc id="1163448" url="https://en.wikipedia.org/wiki?curid=1163448" title="Ruling clique">
Ruling clique

A ruling clique is a group of people who jointly rule an oligarchic form of government.

Ruling cliques generally differ from another type of oligarchy: a military junta. Military juntas are always ruled by military personnel (often high-ranking like general). A ruling clique can include people from various professions. The ruling elites who comprise the leadership tend to form a council, political party, or another form of an organized group. The high-ranking members share a rough balance of power although sometimes one or more members seek to increase their power at the expense of others or some of them may attempt to transform the system into an autocracy or make it more democratic.

Some ruling cliques could be considered a form of aristocracy while others are based on a very small circle of rulers rather than a broader based organization such as a political party. In some cases, the entire ruling clique is composed of a council of leaders who are the only members of the clique.


</doc>
<doc id="859412" url="https://en.wikipedia.org/wiki?curid=859412" title="Ministry (collective executive)">
Ministry (collective executive)

In constitutional usage in Commonwealth realms and in some other systems, a ministry (usually preceded by the definite article, i.e., the ministry) is a collective body of government ministers headed by a prime minister or premier, and also referred to as the head of government. It is described by the Oxford Dictionary as "a period of government under one prime minister". Although the term "cabinet" can in some circumstances be a synonym, a ministry can be a broader concept which might include office-holders who do not participate in cabinet meetings. Other titles can include "administration" (in the United States) or "government" (in common usage among most parliamentary systems) to describe similar collectives.

The term is primarily used to describe the successive governments of the United Kingdom, Canada, Australia and New Zealand, which share a common parliamentary political heritage. In the United Kingdom, Australia and New Zealand, a new ministry begins after each election, regardless of whether the prime minister is re-elected, and whether there may have been a minor rearrangement of the ministry. For example, after winning the 1979 general election, Margaret Thatcher (as Prime Minister of the United Kingdom) formed the first Thatcher ministry. After being re-elected at the 1983 general election, she formed the second Thatcher ministry, and so on. In Canada, a new ministry is only formed if the government loses an election or the succeeding prime minister being from the same party as his or her predecessor.

Despite the use of the term "ministry" in this sense being rare in Portugal nowadays, until the first half of the 20th century, the term was frequently used in this country to designate the collective body of government ministers. From 1911 to 1933, the official title of the Prime Minister of Portugal was even that of "President of the Ministry" (), reflecting his role as the head of the collective ministry.


</doc>
<doc id="2973918" url="https://en.wikipedia.org/wiki?curid=2973918" title="Speaker of the senate">
Speaker of the senate

Speaker of the Senate is a title given to the presiding officer of the Senate in a small number of jurisdictions and mainly amongst English-speaking countries. 




</doc>
<doc id="174363" url="https://en.wikipedia.org/wiki?curid=174363" title="Public trust">
Public trust

The concept of the public trust relates back to the origins of democratic government and its seminal idea that within the public lies the true power and future of a society; therefore, whatever "trust" the public places in its officials must be respected.
One of the reasons that bribery is regarded as a notorious evil is that it contributes to a culture of political corruption in which the public trust is eroded. Other issues related to political corruption or betrayal of public trust are lobbying, special interest groups and the public cartel.

In the Philippines, "betrayal of public trust" is one of the impeachable offenses. In "Francisco, Jr. vs. Nagmamalasakit na mga Manananggol ng mga Manggagawang Pilipino, Inc.," the Supreme Court of the Philippines ruled that the definition of "betrayal of public trust" is "a non-justiciable political question which is beyond the scope of its judicial power" under the Constitution. It did not prescribe which branch of government has the power to define it, but implies that Congress, which handles impeachment cases, has the power to do so.



</doc>
<doc id="2211969" url="https://en.wikipedia.org/wiki?curid=2211969" title="Administrative centre">
Administrative centre

An administrative centre is a seat of regional administration or local government, or a county town, or the place where the central administration of a commune is located.

In countries which have French as one of their administrative languages (such as Belgium, Luxembourg, Switzerland or many African countries) and in some other countries (such as Italy, cf. cognate "capoluogo"), a chef-lieu (, plural form "chefs-lieux" (literally "chief place" or "head place"), is a town or city that is pre-eminent from an administrative perspective. The ‘f’ in chef-lieu is pronounced, in contrast to chef-d'oeuvre where it is mute. 

The capital of an Algerian Province is called a chef-lieu. The capital of a district, the next largest division, is also called a chef-lieu. While the capital of the lowest division, the municipalities, is called agglomeration de chef-lieu (chef-lieu agglomeration) and is abbreviated as A.C.L.

The chef-lieu in Belgium is the administrative centre of each of the ten Provinces of Belgium. Three of these cities also give their name to their province (Antwerp, Liège and Namur).

Luxembourg is divided into two judicial arrondissements (Luxembourg City, Diekirch), three administrative districts (Luxembourg City, Diekirch, Grevenmacher), four electoral circonscriptions (constituencies), twelve cantons and one hundred and five communes (municipalities; Luxembourgish: "Gemengen").

Arrondissements, districts and cantons have each a chef-lieu and are named after it. The same is true for each commune which is composed of more than one town or village. Usually (with a few exceptions), the commune is named after the communal chef-lieu.

The chef-lieu of a département is known as the "préfecture". This is the town or city where the prefect of the départment (and all services under his/her control) is situated, in a building known as the prefecture. In every French region, one of the départments has pre-eminence over the others, and the prefect carries the title of "Prefect of region X…, Prefect of Department Z…" and the city where the regional prefect is found is known as "chef-lieu of the region" or, more commonly, "Regional prefecture". The services are, however, controlled by the prefecture of the départment. 

The chef-lieu of an arrondissement, commonly known as the "sous-préfecture" is the city or town where the sub-prefect of the arrondissement (and the services directly under his/her control) is situated, in a building called the sub-prefecture. The arrondissement where the département prefecture is located does not normally have a sub-prefect or sub-prefecture, the administration being devolved usually to the "Secretary-general of the departmental prefecture", who functions as sub-prefect for the arrondissement. 

The chef-lieu of a canton is usually the biggest city or town within the canton, but has only a nominal role. No specific services are controlled by it. In past decades, there was always a Gendarmerie, a treasurer and a justice of the peace. 

The chef-lieu of a commune is the principal area of the town or city that gives the commune its name, the other areas of the town being called hamlets. French typographers will use a capital for the ‘Le’ or ‘La’ preceding the name of places having ‘chef-lieu of town’ status, and lowercase ‘le’ or ‘la’ for hamlets.

In the Hashemite Kingdom of Jordan, the administrative centres are known as "chief towns" or "nahias". Nahias may be in charge of a sub-district ("qda"), a district ("liwa"), or a governorate ("muhafazah").

The chef-lieu indicates the principal city of the provinces of New Caledonia. So Nouméa is the chef-lieu of South Province. But the chef-lieu can also mean the principal area within a town. So Wé is part of the town of Lifou, but is the chef-lieu of Lifou. In the Loyalty Islands and the other islands, the name of the chef-lieu differs from that of the name of the town. For the towns of the mainland, the chef-lieu has the same name as the town. Nouméa is a town composed only of Nouméa.

Many of the West African states which gained independence from France in the mid-20th century also inherited the French administrative structure of Departments and Communes, headed by a "Chief-Lieu". States still using Chief-Lieu to identify the administrative headquarters of a government subdivision include Senegal, Burkina Faso, Benin, Mali, and Niger. 

Taking Niger and Mali as examples, the administrative subdivisions down to the Commune level each have a formal place of administrative headquarters, titled the chef-lieu. The larger portion of the terminology of administrative division is inherited from colonial rule as part of French West Africa, and has survived and been somewhat modified over time. In both nations there have been remarkably parallel histories. With the decentralization process begun in both nations in the 1990s, the chef-lieu has transitioned from the location of the Governor, Commandant, or Prefect and their staff, to the location of Commune, Cercles of Mali/Departments of Niger, and Regional Councils and a variety of decentralized bodies. The "chefs-lieux" of a Region, Cercle or Département, is usually also a Communal chef-lieu. Both nations collect these councils in a "High Council of Collectivites" seated at the nation's capital. Smaller sub-divisions in Mali's Communes (Villages, Tribal councils, Quarters) are administered from or identified as a "Place"/"Site" ("Site" in French), so the chef-lieu is literally the "Chief-Place" even at the lowest level.

In Russia, the term is applied to the inhabited localities, which serve as a seat of government of entities of various levels. The only exception to this rule is the republics, for which the term "capital" is used to refer to the seat of government. The capital of Russia is also an entity to which the term "administrative centre" does not apply. A similar arrangement exists in Ukraine.

In Sweden there are two levels of administrative centre; the local municipal and the regional county.

"Central locality" "(Swedish: "centralort")" is a term commonly ascribed to the settlement that serves as a municipal administrative centre. This level handles the local administrative and political tasks of the surrounding settlements. Since central place theory was the guiding principle during the municipal reform 1962-1977, most municipalities were dominated by a larger urban area where the political seat was located. Most municipalities are named for their central locality, but there are several exceptions.

There are many deviations from the central locality principle. Some municipalities are dominated by two or more towns of similar size, and sometimes they share the municipal administration, with the municipality having its official address in one of the towns. For example, both Skillingaryd and Vaggeryd are central localities of Vaggeryd Municipality. Conversely, there are municipalities within metropolitan areas. For example, there are twenty-six municipalities within the Stockholm metropolitan area.

The term "central locality" has no legal standing and it is unclear how it should be applied to these municipalities. Some municipalities appointing one or several localities to be the central locality.

A "residence city" (Swedish: "residensstad") is the town or city which is the political and administrative seat of the county. This level handles the more regional political and administrative tasks of the county, such as healthcare and public transport. The name comes from that this is the town or city where the governor "(Swedish: "landshövding")" have their residence. There are some exceptions to this, however. In the newer amalgamated "greater counties", often referred to as "regions", the administrative centre is placed in one of the older "residence cities". Examples of this is Malmö in Region Scania and Gothenburg in the Västra Götaland Region.

The term chef-lieu is applied to the capital of each Swiss canton. In 16 of the 26 cantons, the territory is subdivided into districts. Every district also has a city nominated as chef-lieu and each has a prefect.

The term chef-lieu is used to designate the capital of each gouvernorat (department). Each of the 24 gouvernorats is subdivided into delegations (districts) which each have a central city as chef-lieu of delegation.

In the United Kingdom it is the centre of a local authority, which is distinct from a historic county with a county town.




</doc>
<doc id="25062892" url="https://en.wikipedia.org/wiki?curid=25062892" title="Soil and grain">
Soil and grain

Soil and grain () was a common political term in East Asia for the state. Altars of soil and grain (millet) were constructed alongside ancestral altars. Local kings performed ceremonies of soil and grain to affirm their sovereignty at Beijing Shejitan and Seoul Sajiktan. It has also been rendered "gods of soil and grain" in English, owing to its associations of prayer and supernatural possibilities.

During the Warring States period, ministers defied their rulers by claiming a greater loyalty to the "soil and grain".

A similar concept to "sheji" is that of Tu Di, the Earth Deity.



 


</doc>
<doc id="21418402" url="https://en.wikipedia.org/wiki?curid=21418402" title="Criticism of government">
Criticism of government

Criticism of government may refer to criticism of particular government, or of the concept of government itself. In certain cases, such as in certain monarchies and authoritarian (or sometimes totalitarian or dictatorial) governments, criticizing the government is considered criminal speech and is punished in accord which the perceived severity of the claimed crime.


</doc>
<doc id="2589535" url="https://en.wikipedia.org/wiki?curid=2589535" title="Civilian control of the military">
Civilian control of the military

Civilian control of the military is a doctrine in military and political science that places ultimate responsibility for a country's strategic decision-making in the hands of the civilian political leadership, rather than professional military officers. The reverse situation, where professional military officers control national politics, is called a military dictatorship. A lack of control over the military may result in a state within a state. One author, paraphrasing Samuel P. Huntington's writings in "The Soldier and the State", has summarized the civilian control ideal as "the proper subordination of a competent, professional military to the ends of policy as determined by civilian authority".

Civilian control is often seen as a prerequisite feature of a stable liberal democracy. Use of the term in scholarly analyses tends to take place in the context of a democracy governed by elected officials, though the subordination of the military to political control is not unique to these societies. One example is the People's Republic of China. Mao Zedong stated that "Our principle is that the Party commands the gun, and the gun must never be allowed to command the Party," reflecting the primacy of the Communist Party of China (and communist parties in general) as decision-makers in Marxist–Leninist and Maoist theories of democratic centralism.

As noted by University of North Carolina at Chapel Hill professor Richard H. Kohn, "civilian control is not a fact but a process". Affirmations of respect for the values of civilian control notwithstanding, the actual level of control sought or achieved by the civilian leadership may vary greatly in practice, from a statement of broad policy goals that military commanders are expected to translate into operational plans, to the direct selection of specific targets for attack on the part of governing politicians. National Leaders with limited experience in military matters often have little choice but to rely on the advice of professional military commanders trained in the art and science of warfare to inform the limits of policy; in such cases, the military establishment may enter the bureaucratic arena to advocate for or against a particular course of action, shaping the policy-making process and blurring any clear cut lines of civilian control.

Advocates of civilian control generally take a Clausewitzian view of war, emphasizing its political character. The of Georges Clemenceau, "War is too serious a matter to entrust to military men" (also frequently rendered as "War is too important to be left to the generals"), wryly reflect this view. Given that broad strategic decisions, such as the decision to declare a war, start an invasion, or end a conflict, have a major impact on the citizens of the country, they are seen by civilian control advocates as best guided by the will of the people (as expressed by their political representatives), rather than left solely to an elite group of tactical experts. The military serves as a special government agency, which is supposed to "implement", rather than "formulate", policies that require the use of certain types of physical force. Kohn succinctly summarizes this view when he writes that:
The point of civilian control is to make security subordinate to the larger purposes of a nation, rather than the other way around. The purpose of the military is to defend society, not to define it.

A state's effective use of force is an issue of great concern for all national leaders, who must rely on the military to supply this aspect of their authority. The danger of granting military leaders full or sovereignty is that they may ignore or supplant the democratic decision-making process, and use physical force, or the threat of physical force, to achieve their preferred outcomes; in the worst cases, this may lead to a coup or military dictatorship. A related danger is the use of the military to crush domestic political opposition through intimidation or sheer physical force, interfering with the ability to have free and fair elections, a key part of the democratic process. This poses the paradox that "because we fear others we create an institution of violence to protect us, but then we fear the very institution we created for protection". Also, military personnel, because of the nature of their job, are much more willing to use force to settle disputes than civilians because they are trained military personnel that specialize strictly in warfare. The military is authoritative and hierarchical, rarely allowing discussion and prohibiting dissention. For instance, in the Empire of Japan, prime ministers and almost everyone in high positions were military people like Hideki Tojo, and advocated and basically pressured the leaders to start military conflicts against China and others because they believed that they would ultimately be victorious.

Many of the Founding Fathers of the United States were suspicious of standing militaries. As Samuel Adams wrote in 1768, "Even when there is a necessity of the military power, within a land, a wise and prudent people will always have a watchful and jealous eye over it". Even more forceful are the words of Elbridge Gerry, a delegate to the American Constitutional Convention, who wrote that "[s]tanding armies in time of peace are inconsistent with the principles of republican Governments, dangerous to the liberties of a free people, and generally converted into destructive engines for establishing despotism."

In Federalist No. 8, one of "The Federalist" papers documenting the ideas of some of the Founding Fathers, Alexander Hamilton expressed concern that maintaining a large standing army would be a dangerous and expensive undertaking. In his principal argument for the ratification of the proposed constitution, he argued that only by maintaining a strong union could the new country avoid such a pitfall. Using the European experience as a negative example and the British experience as a positive one, he presented the idea of a strong nation protected by a navy with no need of a standing army. The implication was that control of a large military force is, at best, difficult and expensive, and at worst invites war and division. He foresaw the necessity of creating a civilian government that kept the military at a distance.

James Madison, another writer of many of "The Federalist" papers, expressed his concern about a standing military in comments before the Constitutional Convention in June 1787:

In time of actual war, great discretionary powers are constantly given to the Executive Magistrate. Constant apprehension of War, has the same tendency to render the head too large for the body. A standing military force, with an overgrown Executive, will not long be safe companions to liberty. The means of defense against foreign danger, have been always the instruments of tyranny at home. Among the Romans it was a standing maxim to excite a war, whenever a revolt was apprehended. Throughout all Europe, the armies kept up under the pretext of defending, have enslaved the people.
The United States Constitution placed considerable limitations on the legislature. Coming from a tradition of legislative superiority in government, many were concerned that the proposed Constitution would place so many limitations on the legislature that it would become impossible for such a body to prevent an executive from starting a war. Hamilton argued in Federalist No. 26 that it would be equally as bad for a legislature to be unfettered by any other agency and that restraints would actually be more likely to preserve liberty. James Madison, in Federalist No. 47, continued Hamilton’s argument that distributing powers among the various branches of government would prevent any one group from gaining so much power as to become unassailable. In Federalist No. 48, however, Madison warned that while the separation of powers is important, the departments must not be so far separated as to have no ability to control the others.

Finally, in Federalist No. 51, Madison argued that to create a government that relied primarily on the good nature of the incumbent to ensure proper government was folly. Institutions must be in place to check incompetent or malevolent leaders. Most importantly, no single branch of government ought to have control over any single aspect of governing. Thus, all three branches of government must have some control over the military, and the system of checks and balances maintained among the other branches would serve to help control the military.

Hamilton and Madison thus had two major concerns: (1) the detrimental effect on liberty and democracy of a large standing army and (2) the ability of an unchecked legislature or executive to take the country to war precipitously. These concerns drove American military policy for the first century and a half of the country’s existence. While armed forces were built up during wartime, the pattern after every war up to and including World War II was to demobilize quickly and return to something approaching pre-war force levels. However, with the advent of the Cold War in the 1950s, the need to create and maintain a sizable peacetime military force "engendered new concerns" of militarism and about how such a large force would affect civil–military relations in the United States.

The United States' Posse Comitatus Act, passed in 1878, prohibits any part of the Army or the Air Force (since the U.S. Air Force evolved from the U.S. Army) from engaging in domestic law enforcement activities unless they do so pursuant to lawful authority. Similar prohibitions apply to the Navy and Marine Corps by service regulation, since the actual Posse Comitatus Act does not apply to them. The Coast Guard is exempt from Posse Comitatus since it normally operates under the Department of Homeland Security versus the Department of Defense and enforces U.S. laws, even when operating as a service with the U.S. Navy.

The act is often misunderstood to prohibit any use of federal military forces in law enforcement, but this is not the case. For example, the President has explicit authority under the Constitution and federal law to use federal forces or federalized militias to enforce the laws of the United States. The act's primary purpose is to prevent local law enforcement officials from utilizing federal forces in this way by forming a "posse" consisting of federal Soldiers or Airmen.

There are, however, practical political concerns in the United States that make the use of federal military forces less desirable for use in domestic law enforcement. Under the U.S. Constitution, law and order is primarily a matter of state concern. As a practical matter, when military forces are necessary to maintain domestic order and enforce the laws, state militia forces under state control i.e., that state's Army National Guard and/or Air National Guard are usually the force of first resort, followed by federalized state militia forces i.e., the Army National Guard and/or Air National Guard "federalized" as part of the U.S. Army and/or U.S. Air Force, with active federal forces (to include "federal" reserve component forces other than the National Guard) being the least politically palatable option.

Strong democratic control of the military is a prerequisite for membership in NATO. Strong democracy and rule of law, implying democratic control of the military, are prerequisites for membership in the European Union.

Maoist military-political theories of people's war and democratic centralism also support the subordination of military forces to the directives of the communist party (although the guerrilla experience of many early leading Communist Party of China figures may make their status as civilians somewhat ambiguous). In a 1929 essay "On Correcting Mistaken Ideas in the Party", Mao explicitly refuted "comrades [who] regard military affairs and politics as opposed to each other and [who] refuse to recognize that military affairs are only one means of accomplishing political tasks", prescribing increased scrutiny of the People's Liberation Army by the Party and greater political training of officers and enlistees as a means of reducing military autonomy . In Mao's theory, the military—which serves both as a symbol of the revolution and an instrument of the dictatorship of the proletariat—is not merely expected to defer to the direction of the ruling non-uniformed Party members (who today exercise control in the People's Republic of China through the Central Military Commission), but also to actively participate in the revolutionary political campaigns of the Maoist era.
Civilian leaders cannot usually hope to challenge their militaries by means of force, and thus must guard against any potential usurpation of powers through a combination of policies, laws, and the inculcation of the values of civilian control in their armed services. The presence of a distinct civilian police force, militia, or other paramilitary group may mitigate to an extent the disproportionate strength that a country's military possesses; civilian gun ownership has also been justified on the grounds that it prevents potential abuses of power by authorities (military or otherwise). Opponents of gun control have cited the need for a balance of power in order to enforce the civilian control of the military.

The establishment of a civilian head of state, head of government or other government figure as the military's commander-in-chief within the chain of command is one legal construct for the propagation of civilian control.

In the United States, Article I of the Constitution gives the Congress the power to declare war (in the War Powers Clause), while Article II of the Constitution establishes the President as the commander-in-chief. Ambiguity over when the President could take military action without declaring war resulted in the War Powers Resolution of 1973.

American presidents have used the power to dismiss high-ranking officers as a means to assert policy and strategic control. Three examples include Abraham Lincoln's dismissal of George McClellan in the American Civil War when McClellan failed to pursue the Confederate Army of Northern Virginia following the Battle of Antietam, Harry S. Truman relieving Douglas MacArthur of command in the Korean War after MacArthur repeatedly contradicted the Truman administration's stated policies on the war's conduct, and Barack Obama's acceptance of Stanley McChrystal's resignation in the War in Afghanistan after a "Rolling Stone" article was published where he mocked several members of the Obama administration, including Vice President Joe Biden.

Differing opinions exist as to the desirability of distinguishing the military as a body separate from the larger society. In "The Soldier and the State", Huntington argued for what he termed "objective civilian control", "focus[ing] on a politically neutral, autonomous, and professional officer corps". This autonomous professionalism, it is argued, best inculcates an esprit de corps and sense of distinct military corporateness that prevents political interference by sworn servicemen and -women. Conversely, the tradition of the citizen-soldier holds that "civilianizing" the military is the best means of preserving the loyalty of the armed forces towards civilian authorities, by preventing the development of an independent "caste" of warriors that might see itself as existing fundamentally apart from the rest of society. In the early history of the United States, according to Michael Cairo,
[the] principle of civilian control... embodied the idea that every qualified citizen was responsible for the defense of the nation and the defense of liberty, and would go to war, if necessary. Combined with the idea that the military was to embody democratic principles and encourage citizen participation, the only military force suitable to the Founders was a citizen militia, which minimized divisions between officers and the enlisted.
In a less egalitarian practice, societies may also blur the line between "civilian" and "military" leadership by making direct appointments of non-professionals (frequently social elites benefitting from patronage or nepotism) to an officer rank. A more invasive method, most famously practiced in the Soviet Union and People's Republic of China, involves active monitoring of the officer corps through the appointment of political commissars, posted parallel to the uniformed chain of command and tasked with ensuring that national policies are carried out by the armed forces. The regular rotation of soldiers through a variety of different postings is another effective tool for reducing military autonomy, by limiting the potential for soldiers' attachment to any one particular military unit. Some governments place responsibility for approving promotions or officer candidacies with the civilian government, requiring some degree of deference on the part of officers seeking advancement through the ranks.

Historically, direct control over military forces deployed for war was hampered by the technological limits of command, control, and communications; national leaders, whether democratically elected or not, had to rely on local commanders to execute the details of a military campaign, or risk centrally-directed orders' obsolescence by the time they reached the front lines. The remoteness of government from the action allowed professional soldiers to claim military affairs as their own particular sphere of expertise and influence; upon entering a state of war, it was often expected that the generals and field marshals would dictate strategy and tactics, and the civilian leadership would defer to their informed judgments.

Improvements in information technology and its application to wartime command and control (a process sometimes labeled the "Revolution in Military Affairs") has allowed civilian leaders removed from the theater of conflict to assert greater control over the actions of distant military forces. Precision-guided munitions and real-time videoconferencing with field commanders now allow the civilian leadership to intervene even at the tactical decision-making level, designating particular targets for destruction or preservation based on political calculations or the counsel of non-uniformed advisors.

In the United States the Hatch Act of 1939 does not directly apply to the military, however, Department of Defense Directive 1344.10 (DoDD 1344.10) essentially applies the same rules to the military. This helps to ensure a non-partisan military and ensure smooth and peaceful transitions of power.

Political officers screened for appropriate ideology have been integrated into supervisory roles within militaries as a way to maintain the control by political rulers. Historically they are associated most strongly with the Soviet Union and China rather than liberal democracies.

While civilian control forms the normative standard in almost every society outside of military dictatorships, its practice has often been the subject of pointed criticism from both uniformed and non-uniformed observers, who object to what they view as the undue "politicization" of military affairs, especially when elected officials or political appointees micromanage the military, rather than giving the military general goals and objectives (like "Defeat Country X"), and letting the military decide how best to carry those orders out. By placing responsibility for military decision-making in the hands of non-professional civilians, critics argue, the dictates of military strategy are subsumed to the political, with the effect of unduly restricting the fighting capabilities of the nation's armed forces for what should be immaterial or otherwise lower priority concerns.

The "Revolt of the Admirals" that occurred in 1949 was an attempt by senior US Navy personnel, to force a change in budgets directly opposed to the directives given by the Civilian leadership.

U.S. President Bill Clinton faced frequent allegations throughout his time in office (particularly after the Battle of Mogadishu) that he was ignoring military goals out of political and media pressure—a phenomenon termed the "CNN effect". Politicians who personally lack military training and experience but who seek to engage the nation in military action may risk resistance and being labeled "chickenhawks" by those who disagree with their political goals.

In contesting these priorities, members of the professional military leadership and their non-uniformed supporters may participate in the bureaucratic bargaining process of the state's policy-making apparatus, engaging in what might be termed a form of regulatory capture as they attempt to restrict the policy options of elected officials when it comes to military matters. An example of one such set of conditions is the "Weinberger Doctrine", which sought to forestall another American intervention like that which occurred in the Vietnam War (which had proved disastrous for the morale and fighting integrity of the U.S. military) by proposing that the nation should only go to war in matters of "vital national interest", "as a last resort", and, as updated by Weinberger's disciple Colin Powell, with "overwhelming force". The process of setting military budgets forms another contentious intersection of military and non-military policy, and regularly draws active lobbying by rival military services for a share of the national budget.

Nuclear weapons in the U.S. are controlled by the civilian United States Department of Energy, not by the Department of Defense.

During the 1990s and 2000s, public controversy over LGBT policy in the U.S. military led to many military leaders and personnel being asked for their opinions on the matter and being given deference although the decision was ultimately not theirs to make.

During his tenure, Secretary of Defense Donald Rumsfeld raised the ire of the military by attempting to reform its structure away from traditional infantry and toward a lighter, faster, more technologically driven force. In April 2006, Rumsfeld was severely criticized by some retired military officers for his handling of the Iraq War, while other retired military officers came out in support of Rumsfeld. Although no active military officers have spoken out against Rumsfeld, the actions of these officers is still highly unusual. Some news accounts have attributed the actions of these generals to the Vietnam war experience, in which officers did not speak out against the administration's handling of military action. Later in the year, immediately after the November elections in which the Democrats gained control of the Congress, Rumsfeld resigned.




</doc>
<doc id="2609064" url="https://en.wikipedia.org/wiki?curid=2609064" title="Municipal services">
Municipal services

Municipal services or city services refer to basic services that residents of a city expect the city government to provide in exchange for the taxes which citizens pay. Basic city services may include sanitation (both sewer and refuse), water, streets, the public library, schools, food inspection, fire department, police, ambulance, and other health department issues and transportation. City governments often operate or contract for additional utilities like electricity, gas and cable television. Mumbai even provides a lighthouse service.

The available municipal services for any individual municipality will depend on location, history, geography, statutes and tradition. Provided services may vary from country to country or even within a country. Services may be run directly by a department of the municipality or be sub-contracted to a third party.

Funding for the services provided varies with the municipality in question. Funding can include tax revenue (property tax, income tax, municipal sales tax), fees (such as building permits), Grants from other Governments, fines such as speeding or parking violations, usage fees for optional services, or other sources such as profits from municipally owned or operated utilities.
Probably the greatest influence is the country in which the municipality is located.

In the UK, a combination of local taxation based on property value and central government grants is the main means of funding core services. This is supplemented by nominal fees for services provided (e.g. leisure facilities). For some services, a competitive fee is charged compared to commercial concerns that which allows a profit to be made. For other services, full commercial rates may be charged with municipally owned utilities or commercial property, for example. For the most part, services will be part subsidized by the municipality or fully subsidized by the municipality.
In recent years, UK councils have been given some leeway in finding alternative funding which can be the simple sponsorship of flower baskets to the trading of surplus buildings and land for services from private firms. In certain notorious cases, local councils have used council funds to speculate on the money markets.

Municipalities in other countries may have other methods of funding (e.g. local income taxes or even from the profits of utilities or industrial concerns fully owned by the municipality).

In the United Kingdom, until their abolishment, Municipal Corporations were powerful organizations. In their Victorian heyday, with the growth of urbanization and industrialization, they could be responsible for the promotion, organization, funding, building and management of everything from housing to water supplies, power in the form of both gas and electricity, to the introduction of electric trams; almost any activity that the city fathers thought
was necessary to promote the economic, social and environmental well-being of their municipality.

However, special powers needed to be granted through Local Acts in Parliament (for example, the Manchester Corporation Waterworks Act needed for the construction of Manchester's reservoirs). As times changed, the Municipal Corporations continued to try to advance the cause of their municipalities. For example, in the 1930s aviation was the new technological frontier and municipalities worked to promote themselves with the development of municipal airports.

For the most part, UK municipalities lost their in-house utilities to the nationalisation and centralisation of public utilities. One notable exception is Kingston upon Hull, which still has a municipally-owned telephone company known as Kingston Communications.

One trend in the UK, (in the name of efficiency) has been the privatisation of departments, the transfer of staff and assets to the new organisation and the contracting with the new organization for services to the council. This model has been used for services from road cleaning to social housing, to leisure facilities, though no council yet seems to dare to do so for more high-profile services such as schools and social services.

In the UK, fire and police services are not under direct municipal control, even when a force can be closely identified with a specific municipal area such as Greater London. However, fire and police services are in part paid for by a surcharge to local taxation, and although they have no say in operational matters, local government appoints members to a committee to oversee the running of each force.

Where there is a substantial industrial urban population isolated from other conurbations, or when and where the growth in demand is so great that it becomes uneconomic or impractical for commercial organisations to provide, the municipalities concerned may assume functions necessary for the growth and functioning of the city.

This was the case in Victorian England also today in Mumbai, for example, the Municipal Corporation of Greater Mumbai and Brihanmumbai Electric Supply and Transport are the major municipal organizations which are needed to allow the city to function. The Los Angeles Department of Water and Power is a legacy of Los Angeles, which found itself in a similar position of rapid growth at the beginning of the last century. The similarly isolated Johannesburg has chosen to run its services as standalone and self-funding, corporate entities.




</doc>
<doc id="298608" url="https://en.wikipedia.org/wiki?curid=298608" title="State of emergency">
State of emergency

A state of emergency is a situation in which a government is empowered to perform actions or impose policies that it would normally not be permitted to undertake. A government can declare such a state during a natural disaster, civil unrest, armed conflict, medical pandemic or epidemic or other biosecurity risk. Such declarations alert citizens to change their normal behavior and orders government agencies to implement emergency plans. "Justitium" is its equivalent in Roman law—a concept in which the senate could put forward a final decree (senatus consultum ultimum) that was not subject to dispute.

States of emergency can also be used as a rationale or pretext for suspending rights and freedoms guaranteed under a country's constitution or basic law, sometimes through martial law or revoking habeas corpus. The procedure for and legality of doing so vary by country.

Under international law, rights and freedoms may be suspended during a state of emergency; for example, a government can detain persons and hold them without trial. All rights that can be derogated from are listed in the International Covenant for Civil and Political Rights. Non-derogable rights cannot be suspended. Non-derogable rights are listed in Article 4 of the ICCPR; they include right to life, the rights to freedom from arbitrary deprivation of liberty, slavery, torture, and ill-treatment.

Some countries have made it illegal to modify emergency law or the constitution during the emergency; other countries have the freedom to change any legislation or rights based constitutional frameworks at any time that the legislative chooses to do so. Constitutions are contracts between the government and the private individuals of that country. The International Covenant for Civil and Political Rights (ICCPR) is an international law document signed and ratified by states. Therefore, the Covenant applies to only those persons acting in an official capacity, not private individuals. However, States Parties to the Covenant are expected to integrate it into national legislation. The state of emergency (within the ICCPR framework) must be publicly declared and the Secretary-General of the United Nations and all other States Parties to the Covenant must be notified immediately, to declare the reason for the emergency, the date on which the emergency is to start, the derogations that may take place, with the timeframe of the emergency and the date in which the emergency is expected to finish. Although this is common protocol stipulated by the ICCPR, its monitoring Committee of experts has no sanction power and its recommendations are therefore not always strictly followed; enforcement is therefore better regulated by the American and European Conventions and Courts on human rights.

Though fairly uncommon in democracies, dictatorial regimes often declare a state of emergency that is prolonged indefinitely for the life of the regime, or for extended periods of time so that derogations can be used to override human rights of their citizens usually protected by the International Covenant on Civil and political rights. In some situations, martial law is also declared, allowing the military greater authority to act. In other situations, emergency is not declared and de facto measures taken or decree-law adopted by the government. Ms. Nicole Questiaux (France) and Mr. Leandro Despouy (Argentina), two consecutive United Nations Special Rapporteurs, have recommended to the international community to adopt the following "principles" to be observed during a state or de facto situation of emergency : Principles of Legality, Proclamation, Notification, Time Limitation, Exceptional Threat, Proportionality, Non-Discrimination, Compatibility, Concordance and Complementarity of the Various Norms of International Law (cf. "Question of Human Rights and State of Emergency", E/CN.4/Sub.2/1997/19, at Chapter II; see also "").

Article 4 to the International Covenant on Civil and Political Rights (ICCPR), permits states to derogate from certain rights guaranteed by the ICCPR in "time of public emergency". Any measures derogating from obligations under the Covenant, however, must be to only the extent required by the exigencies of the situation, and must be announced by the State Party to the Secretary-General of the United Nations. The European Convention on Human Rights and American Convention on Human Rights have similar derogatory provisions. No derogation is permitted to the International Labour Conventions.

Some political theorists, such as Carl Schmitt, have argued that the power to decide the initiation of the state of emergency defines sovereignty itself. In "State of Exception" (2005), Giorgio Agamben criticized this idea, arguing that the mechanism of the state of emergency deprives certain people of their civil and political rights, producing his interpretation of "homo sacer".

In many democratic states there are a selection of legal definitions for specific states of emergency, when the constitution of the State is partially in abeyance depending on the nature of the perceived threat to the general public. In order of severity these may include:

Sometimes, the state of emergency can be abused by being invoked. An example would be to allow a state to suppress internal opposition without having to respect human rights. An example was the August 1991 attempted coup in the Soviet Union (USSR) where the coup leaders invoked a state of emergency; the failure of the coup led to the dissolution of the Soviet Union.

Derogations by states having ratified or acceded to binding international agreements such as the ICCPR, the American and European Conventions on Human Rights and the International Labour Conventions are monitored by independent expert committees, regional Courts and other State Parties.

The Constitution, which has been amended several times, has always allowed for a state of emergency (literally "estado de sitio", "state of siege"), to be declared if the constitution or the authorities it creates are endangered by internal unrest or foreign attack. This provision was much abused during dictatorships, with long-lasting states of siege giving the government a free hand to suppress opposition ( a state of emergency had been declared 52 times by democratic and dictatorial governments, starting in 1854 shortly after the constitution came into force). The American Convention on Human Rights (Pacto de San José de Costa Rica), adopted in 1969 but ratified by Argentina only in 1984 immediately after the end of the National Reorganization Process, restricts abuse of the state of emergency by requiring any signatory nation declaring such a state to inform the other signatories of its circumstances and duration, and what rights are affected.

State-of-emergency legislation differs in each state of Australia. With regard to emergency management, regions (usually on a local government area basis) that have been affected by a natural disaster are the responsibility of the state, until that state declares a State of Emergency where access to the Federal Emergency Fund becomes available to help respond to and recover from natural disasters. A State of Emergency does not apply to the whole state, but rather districts or shires, where essential services may have been disrupted.

In Victoria, the premier can declare a state of emergency if there is a threat to employment, safety or public order. The declaration expires after 30 days, and a resolution of either the upper or lower House of Parliament may revoke it earlier. Under the "Public Safety Preservation Act 1958", a declared state of emergency allows the premier to immediately make any desired regulations to secure public order and safety. However, these regulations expire if Parliament does not agree to continue them within seven days. Also, under the "Essential Services Act 1958", the premier (or delegate) may operate or prohibit operation of any essential service, such as transport, fuel, power, water or gas.

A state of emergency was declared in New South Wales in December 2019 in response to the 2019 bushfires. At that time of NSW bushland had been lost, resulting in over 600 homes lost and eight fatalities.

On 18 March 2020, a nationwide human biosecurity emergency was declared in Australia owing to the risks to human health posed by the coronavirus (COVID-19) pandemic, after the National Security Committee met the previous day. The "Biosecurity Act 2015" specifies that the Governor-General of Australia may declare such an emergency if the Health Minister (currently Greg Hunt) is satisfied that "a listed human disease is posing a severe and immediate threat, or is causing harm, to human health on a nationally significant scale". This gives the Minister sweeping powers, including imposing restrictions or preventing the movement of people and goods between specified places, and evacuations. The "Biosecurity (Human Biosecurity Emergency) (Human Coronavirus with Pandemic Potential) Declaration 2020" was declared by the Governor-General, David Hurley, under Section 475 of the Act.

Extreme act that, in Brazil ("Estado de Sítio" or "Estado de Exceção", in Portuguese), can be declared on the following circumstances:

The state of emergency could last for 30 days, being possible to extend it for more days in case of persistence of the reasons of exceptionality.

Only the President is able to declare or prorogate this State; after receiving formal authorization from National Congress and after consultation with the National Security Council or the Council of the Republic.

The federal government of Canada can use the Emergencies Act to invoke a state of emergency. A national state of emergency automatically expires after 90 days, unless extended by the Governor-in-Council. There are different levels of emergencies: Public Welfare Emergency, Public Order Emergency, International Emergency, and War Emergency.

The Emergencies Act replaced the War Measures Act in 1988. The War Measures Act was invoked three times in Canadian history, most controversially during the 1970 October Crisis, and also during World War I (from 1914 to 1920, against threat of Communism) and World War II (from 1942 to 1945, against perceived threat from Japanese Canadians following Imperial Japan's attack on Pearl Harbor).

Under the current Emergency Act a state of emergency can also be declared by provincial, territorial, and municipal governments. In addition Canada's federal government and any of its provincial governments can suspend, for five years at a time, Charter rights to fundamental freedoms in section 2, to legal rights in sections 7 through 14, and to equality rights in section 15 by legislation which invokes the notwithstanding clause, section 33, and therefore emergency powers can effectively be created even without using the Emergency Act.

The police chief in a district can impose a zone in which people can be body searched without a specific suspicion. Such an order must be issued in writing, published, and imposed for a limited period. The police law (article 6) regulates this area. The normal procedure calls for assisting the suspect to a private area and stripping them. The police can also impose a zone in which specific crimes such as violence, threats, blackmailing and vandalism can be punished with a double penalty length. The zone can only be imposed if there is an extraordinary crime development and the zone can only last up to three months unless the extraordinary crime development still applies.

If the police feel that a situation involving a crowd of people can get out of hand, they can order the assembly to be dissolved and "pass the street" in the name of the king. People that after three such warnings are still part of the crowd can then without further warning be subjugated to mass arrest. All people arrested can then be detained for 24 hours without charging them or taking them for a judge. This is called a precluding arrest.

Egyptians lived under an Emergency Law (Law No. 162 of 1958) from 1967 to 2012, except for an 18-month break in 1980 and 1981. The emergency was imposed during the 1967 Arab-Israeli War, and reimposed following the assassination of President Anwar Sadat. The law continuously extended every three years since 1981. Under the law, police powers were extended, constitutional rights suspended and censorship was legalized. The law sharply circumscribed any non-governmental political activity: street demonstrations, non-approved political organizations, and unregistered financial donations were formally banned. Some 17,000 people were detained under the law, and estimates of political prisoners run as high as 30,000. The emergency rule expired on 31 May 2012, and was put back in place in January 2013. Egypt declared a month-long national emergency on 14 August 2013.

The Egyptian president announced a one-month state of emergency across the country on 14 August 2013 and ordered the armed forces to help the Interior Ministry enforce security. The announcement made on state TV followed deadly countrywide clashes between supporters of deposed President Mohammed Morsi and the security forces.

Three main provisions concern various kind of "state of emergency" in France: Article 16 of the Constitution of 1958 allows, in time of crisis, "extraordinary powers" to the president. Article 36 of the same constitution regulates "state of siege" (""). Finally, the Act of 3 April 1955 allows the proclamation, by the Council of Ministers, of the "state of emergency" (""). The distinction between article 16 and the 1955 Act concerns mainly the distribution of powers: whereas in article 16, the executive power basically suspend the regular procedures of the Republic, the 1955 Act permits a twelve-day state of emergency, after which a new law extending the emergency must be voted by the Parliament. These dispositions have been used at various times, in 1955, 1958, 1961, 1988, 2005, and 2015.

The Weimar Republic constitution (1919–1933) allowed states of emergency under Article 48 to deal with rebellions. Article 48 was often invoked during the 14-year life of the Republic, sometimes for no reason other than to allow the government to act when it was unable to obtain a parliamentary majority.

After the 27 February 1933, Reichstag fire, an attack blamed on the communists, Adolf Hitler declared a state of emergency using Article 48, and then had President von Hindenburg sign the Reichstag Fire Decree, which suspended some of the basic civil liberties provided by the Weimar Constitution (such as habeas corpus, freedom of expression, freedom of the speech, the freedom to assemble or the privacy of communications) for the whole duration of the Third Reich. On 23 March, the Reichstag enacted the Enabling Act of 1933 with the required two-thirds majority, which enabled Chancellor Adolf Hitler and his cabinet to enact laws without the participation of the legislative. (The Weimar Constitution was never actually repealed by Nazi Germany, but it effectively became ineffective after the passage of the Enabling Act.) These two laws implemented the "Gleichschaltung", the Nazis' institution of totalitarianism.

In the postwar Federal Republic of Germany the "Emergency Acts" state that some of the basic constitutional rights of the Basic Law may be limited in case of a state of defence, a state of tension, or an internal state of emergency or disaster (catastrophe). These amendments to the constitution were passed on 30 May 1968, despite fierce opposition by the so-called "extra-parliamentary opposition" (see German student movement for details).

During the state of war, or turmoil which threatens national security or unity, and the Standing Committee of the National People's Congress believes is beyond the control of the local government, it can invoke Article 18 of the Hong Kong Basic Law and declare a "State of Emergency" in Hong Kong, thus the Central People's Government can selectively implement national laws not normally allowed in Hong Kong. Deployment of troops from the People's Liberation Army Hong Kong Garrison under the "Law of the People's Republic of China on Garrisoning the Hong Kong Special Administrative Region" can happen.

The Chief Executive of Hong Kong along with the Executive Council can prohibit public gatherings, issue curfew orders, prohibit the movement of vessels or aircraft, delegate authority, and other listed powers, under "Cap. 245 Public Order Ordinance".

Although the People's Liberation Army Hong Kong Garrison may not interfere in internal Hong Kong affairs, however, the Hong Kong Special Administrative Region Government may invoke Article 14 of the Hong Kong Basic Law and request the Central People's Government permission to have the garrison assist in "maintenance of public order or disaster relief".

Since 1997, a State of Emergency has never been declared. However, emergency measures have been used in varying degrees over the years during British rule and after the establishment of the Special Administrative Region. A few notable mentions are as follow:


On 4 October 2019, Carrie Lam, the Chief Executive of Hong Kong S.A.R., invoked Section 2(1) within "Cap. 241 Emergency Regulations Ordinance" implemented since 1922 and last amended by the Legislative Council in 1999, which allow the government to implement the new, "Cap. 241K Prohibition on Face Covering Regulation". The new regulation forbid public assembly participants from wearing masks or obscure faces during such events without reasonable excuses. The permitted excuses are: pre-existing medical or health reasons, religious reasons, and if the person uses the face covering for physical safety while performing an activity connected with their profession or employment. Any person defying the new regulation face possible criminal prosecution. The government's motive in doing so is to end months of social unrest and riots, however, did not declare a "State of Emergency". The new regulation took effect at 00:00 HKT on 5 October 2019. Offenders risked a maximum of one-year imprisonment or a fine of HK$25,000 (US$3,200).

The High Court of Hong Kong denied an application for a judicial injunction of the anti-mask law, on the same night shortly before the new regulation took effect. A subsequent attempt by pro-democrats to halt the new regulation also failed, however, the court recommended a judicial review at a later date.

On 18 November 2019, the High Court ruled the "Cap. 241 Emergency Regulations Ordinance" is "incompatible with the Basic Law", however, the court "leaves open the question of the constitutionality of the ERO insofar as it relates to any occasion of emergency." The court also held the ordinance meets the "prescribed by law" requirement. However, the court deemed s3(1)(b), (c), (d) and s5 of the regulation do not meet the proportionality test as they impose restrictions on fundamental rights that goes beyond what is necessary in furthering its intended goals.

On 22 November 2019, the High Court made the following remark:

"Nevertheless, we recognise that our Judgment is only a judgment at first instance, and will soon be subject to an appeal to the Court of Appeal. In view of the great public importance of the issues raised in this case, and the highly exceptional circumstances that Hong Kong is currently facing, we consider it right that we should grant a short interim suspension order so that the respondents may have an opportunity to apply to the Court of Appeal, if so advised, for such interim relief as may be appropriate. Accordingly, we shall grant an interim temporary suspension order to postpone the coming into operation of the declarations of invalidity for a period of 7 days up to the end of 29 November 2019, with liberty to apply."

On 26 November 2019, the High Court announced hearing for the government appeal against the judgement is on 9 January 2020.

On 27 November 2019, the Court of Appeal extended the interim suspension of the judgment until 10 December 2019.

On 10 December 2019, the Court of Appeal refused to suspend the "unconstitutional" ruling by the Court of First Instance on the anti-mask regulation. As scheduled, a full hearing will commence on 9 January 2020.

According to the Hungarian Constitution, the National Assembly of Hungary can declare state of emergency in case of armed rebellion or natural or industrial disaster. It expires after 30 days, but can be extended. Most civil rights can be suspended, but basic human rights (such as the right to life, the ban of torture, and freedom of religion) cannot.

During state of emergency, the Parliament cannot be disbanded.

The Icelandic constitution provides no mechanism for the declaration of war, martial law nor state of emergency.

The State of Emergency can be proclaimed by the President of India, when he/she perceives grave threats to the nation, albeit through the advice of the cabinet of ministers. Part XVIII of the Constitution of India gives the President the power to overrule many provisions, including the ones guaranteeing fundamental rights to the citizens of India

In India, a state of emergency was declared twice:


The first internal Emergency was declared by the president, Fakhruddin Ali Ahmed on advice of the then Prime Minister, Indira Gandhi The provisions of the Constitution allows the Prime Minister to rule by decree.

In Ireland declaring a state of "national emergency" involves Article 28.3.3° of the 1937 Constitution of Ireland, which states that:
In addition, during a "war or armed rebellion", military tribunals may try civilians, and the Defence Forces are not bound by habeas corpus.

The First Amendment of the Constitution of 1939 allows an emergency to be declared during wars in which the state is a non-belligerent, subject to resolutions by the houses of the Oireachtas. By the 2nd Amendment of 1941, an emergency ends, not automatically when the war does, but only by Oireachtas resolutions. The 21st Amendment of 2002 prevents the reintroduction of capital punishment during an emergency.

The first amendment was rushed through the Oireachtas after the outbreak of the Second World War, in which the state remained neutral. Immediately after, the required resolution was passed, in turn enabling the passage of the Emergency Powers Act 1939 (EPA), which granted the government and its ministers sweeping powers to issue statutory orders termed "Emergency Powers Orders" (EPOs). (The period in Ireland was and is referred to as "The Emergency".) The EPA expired in 1946, although some EPOs were continued under the Supplies and Services (Temporary Provisions) Act 1946 until as late as 1957. Rationing continued until 1951.

The 1939 state of emergency was not formally ended until a 1976 resolution, which also declared a new state of emergency in relation to the Troubles in Northern Ireland and in particular the recent assassination of the British ambassador to Ireland, Christopher Ewart Biggs. The Emergency Powers Act 1976 was then passed to increase the Garda Síochána powers to arrest, detain, and question those suspected of offences against the state. President Cearbhall Ó Dálaigh referred the bill under Article 26 of the Constitution to the Supreme Court, which upheld its constitutionality. The referral was condemned by minister Paddy Donegan as a "thundering disgrace", causing Ó Dálaigh to resign in protest. The 1976 EPA expired after one year, but the state of emergency persisted until 1995, when as part of the Northern Ireland peace process it was rescinded as a "confidence building measure" to satisfy physical force republicans after the Provisional IRA's 1994 ceasefire.

The Offences against the State Act does not require a state of emergency under Article 28.3.3°. Part V of the Act, which provides for a non-jury Special Criminal Court (SCC), is permitted under Article 38.3.1°. Part V is activated by a declaration from the government that it is "necessary to secure the preservation of public peace and order", and it can be rescinded by vote of Dáil Éireann. Provision for internment is similarly activated and rescinded (originally by Part VI of the 1939 act, later by Part II of a 1940 amending act). Parts V and VI were both activated during the Second World War and the IRA's late 1950s Border Campaign; Part V has been continually active since 1972.

Several official reviews of the Constitution and the Offences Against the State Acts have recommended a time limit within which the operation of Article 28.3.3° or Article 38.3.1° must either be explicitly renewed by resolution or else lapse.

Israel's Emergency Defence Regulations are older than the state itself, having been passed under the British Mandate for Palestine in 1945. A repeal was briefly considered in 1967 but cancelled following the Six-Day War. The regulations allow Israel, through its military, to control movements and prosecute suspected terrorists in occupied territories, and to censor publications that are deemed prejudicial to national defense.

The Standing Committee of the National People's Congress can declare a state of emergency and deploy troops from the People's Liberation Army Macau Garrison under the Article 14 of Macau's Basic Law on the defence of the Macau Special Administrative Region.

The Chief Executive of Macau can use the Macau national security law to prohibit public gatherings, issue curfew orders, prohibit other activities perceived to be a threat against the Region or People's Republic of China.

Since 1999 no emergency measure have been enacted. Prior to 1999 emergency measures have been used for 1 major incident:

In Malaysia, if the Yang di-Pertuan Agong (Monarch) is satisfied that a grave emergency exists whereby the security, or the economic life, or public order in the Federation or any part thereof is threatened, he may issue a Proclamation of Emergency making therein a declaration to that effect.

In the history of Malaysia, a state of emergency was declared by the then-colonial government of Britain. The state of emergency lasted from 1948 until 1960 to deal with the communists led by Chin Peng.

States of emergency were also declared during the "Konfrontasi" in 1962, the 1966 Sarawak constitutional crisis and 1977 Kelantan Emergency.

When a race riot broke out on 13 May 1969, a state of emergency was declared.

On 11 August 2005 a state of emergency was announced for the world's 13th largest port, Port Klang and the district of Kuala Selangor after air pollution there reached dangerous levels (defined as a value greater than 500 on the Air Pollution Index or API).

Thiery Rommel, the European Commission's envoy to Malaysia, told Reuters by telephone on 13 November 2007 (the last day of his mission) that, "Today, this country still lives under (a state of) emergency." Although not officially proclaimed as a state of emergency, the Emergency Ordinance and the Internal Security Act had allowed detention for years without trial.

On 23 June 2013 a state of emergency was declared by Prime Minister Najib Abdul Razak for Muar and Ledang, Johor as smoke from land-clearing fires in Indonesia pushed air pollution index to above 750. This was the first time in years that air quality had dipped to a hazardous level with conditions worsening as dry weather persisted and fires raged in Sumatra.

A state of emergency was declared on 26 December 2004, following the 2004 Indian Ocean Earthquake and Tsunami. The resulting tsunamis caused extensive damage to the country's infrastructure, cutting off communications from large swathes of the nation, decimating islands and forcing the closure of a number of resorts due to the damage.

On 5 February 2018, a state of emergency was declared by Maldives's President Abdulla Yameen for 15 days and ordered security forces into the supreme court and arrested a former president Maumoon Abdul Gayoom and the Chief Justice of Honorable Supreme court of Maldives.

Namibia declared last a State of Emergency due to an ongoing drought in 2016.

The Civil Defence Emergency Management Act 2002 gives the Government of New Zealand and local-body councils the power to issue a state of emergency, either over the entire country or within a specific region. This may suspend ordinary work and essential services if need be. States of emergency in New Zealand expire on the commencement of the seventh day after the date of a declaration, unless extended. However, the Minister of Civil Defence or a local mayor may lift a state of emergency after an initial review of a region's status.




In Nigeria, a state of emergency is usually declared in times of great civil unrest. In recent years, it has specifically been implemented in reaction to terrorist attacks on Nigerians by the Islamic jihadist group Boko Haram.

On 14 May 2013, Goodluck Jonathan declared a state of emergency for the entire northeastern states of Borno, Yobe and Adamawa. A more limited state of emergency had been declared on 31 December 2011 in parts of Yobe, Borno, Plateau and Niger states. This earlier declaration included the temporary shutdown of the international borders in those regions.

In Pakistan, a state of emergency was declared five times in its history:

The first three were regarded as the imposition of direct martial law.

The current Constitution of Portugal empowers the President of the Republic to declare a state of siege () or a state of emergency () in part or the entirety of the Portuguese territory, only in cases of actual or imminent aggression by foreign forces, serious threats to or disturbances of the democratic constitutional order, or public disasters.

Such declarations allow the entities that exercise sovereignty from suspending the exercise of some of the constitutionally defined rights, freedoms and guarantees, so that the public authorities can take the appropriate and strictly necessary measures for the prompt restoration of constitutional normality; the Constitution, however, sets a temporal limit for these states of emergency (no more than fifteen days, even though renewal is possible) and forbids any suspension of the right to life, to personal integrity, to personal identity, to civil capacity and citizenship, the non-retroactivity of criminal law, the right to a fair trial, or the freedom of conscience and religion. They also may not affect the constitutionally-defined competences and mode of operation of the entities that exercise sovereignty. The Assembly of the Republic may not be dissolved while a state of siege or a state of emergency is in force, nor can the Constitution itself be subject to amendment.

Before declaring a state of siege or a state of emergency, the President is required to consult with the Government and request authorisation to do so from the Assembly of the Republic.

During the Third Portuguese Republic, the only two times such states of exceptional suppression of constitutional provisions were declared were during the failed left-wing coup d'état of 25 November 1975 (state of siege, within the confines of the Lisbon Military Region), and during the 2020 coronavirus pandemic (state of emergency, in the entirety of the Portuguese territory).

In Romania, there are two types of states of emergency, each designed for a different type of situation.

The most well-known event in which the state of emergency has been enforced was because of 1977 Vrancea earthquake.

The last instance in which the "special zone of public safety" was enforced was in 8 December 2013-ongoing, in Pungești, Vaslui following civil unrest in Pungești from Chevron's plans to begin exploring shale-gas in the village. According to police officials, the special security zone will be maintained as long as there is conflict in the area that poses a threat to Chevron's operations. This special security zone has faced domestic and international criticism for alleged human-rights abuses.

Sierra Leone declared, on 7 February 2019, a State of Emergency due to ongoing rape and sexual violence in the country. On 24 March 2020, a state of emergency was declared by His Excellency (Rtd) Brigadier Julius Madaa Bio due to global pandemic of the coronavirus.

States of emergency in South Africa are governed by section 37 of the Constitution and by the State of Emergency Act, 1997. The President may declare a state of emergency only when "the life of the nation is threatened by war, invasion, general insurrection, disorder, natural disaster or other public emergency" and if the ordinary laws and government powers are not sufficient to restore peace and order. The declaration is made by proclamation in the "Government Gazette" and may only apply from the time of publication, not retroactively. It can only continue for 21 days unless the National Assembly grants an extension, which may be for at most three months at a time. The High Courts have the power, subject to confirmation by the Constitutional Court, to determine the validity of the declaration of a state of emergency.

During a state of emergency the President has the power to make emergency regulations "necessary or expedient" to restore peace and order and end the emergency. This power can be delegated to other authorities. Emergency measures can violate the Bill of Rights, but only to a limited extent. Some rights are inviolable, including amongst others the rights to life and to human dignity; the prohibition of discrimination on the grounds of race, sex or religion; the prohibition of torture or inhuman punishment; and the right of accused people to a fair trial. Any violation of a constitutional right must be strictly required by the emergency. Emergency measures may not indemnify the government or individuals for illegal actions. They may impose criminal penalties, but not exceeding three years' imprisonment. They may not require military service beyond that required by the ordinary laws governing the defence force. An emergency measure may be disapproved by the National Assembly, in which case it lapses, and no emergency measure may interfere with the elections, powers or sittings of Parliament or the provincial legislatures. The courts have the power to determine the validity of any emergency measure.

The constitution places strict limits on any detention without trial during a state of emergency. A friend or family member of the detainee must be informed, and the name and place of detention must be published in the "Government Gazette". The detainee must have access to a doctor and a legal representative. He or she must be brought before a court within at most ten days, for the court to determine whether the detention is necessary, and if not released may demand repeated review every ten days. At the court review the detainee must be allowed legal representation and must be allowed to appear in person. The provisions on detention without trial do not apply to prisoners of war in an international conflict; instead they must be treated in accordance with the Geneva Conventions and other international law.

In Spain, there are three degrees of state of emergency ("estado de emergencia" in Spanish): "alarma" (alarm or alert), "excepción" (exception[al circumstance]) and "sitio" (siege). They are named by the constitution, which limits which rights may be suspended, but regulated by the "Ley Orgánica 4/1981" (Organic Law).

On 4 December 2010, the first state of alert was declared following the air traffic controllers strike. It was the first time since the Francisco Franco's regime that a state of emergency was declared. The second state of alert was declared on 14 March 2020 due to the coronavirus pandemic.

In Sri Lanka, the President is able to proclaim emergency regulations under the "Public Security Ordinance" in the constitution in order to preserve public security and public order; suppression of mutiny, riot or civil commotion; or maintenance of supplies and services essential to the life of the community. These regulations last for one month unless confirmed otherwise by Parliament.

According to Art. 185 of the Swiss Federal Constitution The Federal Council (Bundesrat) can call up in their own competence military personnel of maximum 4000 militia for three weeks to safeguard inner or outer security (called Federal Intervention or Federal Execution, respectively). A larger number of soldiers or of a longer duration is subject to parliamentary decision. For deployments within Switzerland the principle of subsidiarity rules: as a first step, unrest has to be overcome with the aid of cantonal police units.

An emergency prevailed in Syria from 1962 to 2011. Originally predicated on the conflict with Israel, the emergency acted to centralize authority in the presidency and the national security apparatus while silencing public dissent. The emergency was terminated in response to protests that preceded the Syrian Civil War. Under the 2012 constitution, the president may pass an emergency decree with a 2/3 concurrence of his ministers, provided that he presents it to the legislature for constitutional review.

A state of emergency was declared in 1970 during the Black Power Revolution by then Prime Minister Eric Williams. During the attempted state coup by the Jamaat al Muslimeen against the NAR government of the then Prime Minister A. N. R. Robinson, a state of emergency was declared during the coup attempt and for a period after the coup.

On 4 August 1995, a state of emergency was declared to remove the Speaker of the House Occah Seepaul by Prime Minister Patrick Manning during a constitutional crisis. The government had attempted to remove the speaker via a no-confidence motion, which failed. The state of emergency was used to remove the speaker using the emergency powers granted.

The Prime Minister Kamla Persad-Bissessar announced a state of emergency on 22 August 2011 at 8:00 pm in an attempt to crack down on the trafficking of illegal drugs and firearms, in addition to gangs. The decision of the President, George Maxwell Richards, to issue the proclamation for the state of emergency was debated in the country's Parliament as required by the Constitution on 2 September 2011 and passed by the required simple majority of the House of Representatives. On 4 September the Parliament extended the state of emergency for a further 3 months. It ended in December 2011.

Since the foundation of the Republic of Turkey in 1923 the military conducted three "coups d'état" and announced martial law. Martial law between 1978 and 1983 was replaced by a state of emergency that lasted until November 2002.
The latest state of emergency was declared by President Erdoğan on 20 July 2016 following a failed coup attempt on 15 July 2016 by a faction of the country's armed forces. It was lifted in 2018.

In the United Kingdom, only the British Sovereign, on the advice of the Privy Council, or a Minister of the Crown in exceptional circumstances, has the power to introduce emergency regulations under the Civil Contingencies Act 2004, in case of an emergency, broadly defined as war or attack by a foreign power, terrorism which poses a threat of serious damage to the security of the UK, or events which threaten serious damage to human welfare or the environment of a place in the UK. The duration of these regulations is limited to thirty days, but may be extended by Parliament.
A state of emergency was last invoked in 1974 by Prime Minister Edward Heath in response to increasing industrial action.

The act grants wide-ranging powers to central and local government in the event of an emergency. It allows the modification of primary legislation by emergency regulation, with the exception of the Human Rights Act 1998 and Part 2 of the Civil Contingencies Act 2004.

The United States Constitution implicitly provides some emergency powers in the article about the executive power :

Aside from these, many provisions of law exist in various jurisdictions, which take effect only upon an executive declaration of emergency; some 500 federal laws take effect upon a presidential declaration of emergency. The National Emergencies Act regulates this process at the federal level. It requires the President to specifically identify the provisions activated and to renew the declaration annually so as to prevent an arbitrarily broad or open-ended emergency.
Presidents have occasionally taken action justified as necessary or prudent because of a state of emergency, only to have the action struck down in court as unconstitutional.

A state governor or local mayor may declare a state of emergency within his or her jurisdiction. This is common at the state level in response to natural disasters. The Federal Emergency Management Agency maintains a system of assets, personnel and training to respond to such incidents. For example, on 10 December 2015, Washington state Governor Jay Inslee declared a state of emergency due to flooding and landslides caused by heavy rains.

The 1977 International Emergency Economic Powers Act allows the government to freeze assets, limit trade and confiscate property in response to an "unusual and extraordinary threat" to the United States that originates substantially outside of it. As of 2015 more than twenty emergencies under the IEEPA remain active regarding various subjects, the oldest of which was declared in 1979 with regard to the government of Iran. Another ongoing national emergency, declared after the September 11 attacks, authorizes the president to retain or reactivate military personnel beyond their normal term of service.









</doc>
<doc id="30507231" url="https://en.wikipedia.org/wiki?curid=30507231" title="Governmental learning spiral">
Governmental learning spiral

The governmental learning spiral is a technique used to solve specific governance challenges. The governmental learning spiral—a heuristic and multidisciplinary tool—has been developed and implemented at international governmental learning events throughout the past decade. It is used during prearranged educational events such as conferences, e-learning, and trainings to improve performance in democratic governance. 

The technique consists of a nine-stage learning process divided into three phases, which include the planning and aftermath as well as the learning event itself.

A major characteristic of this type of governmental learning event is facilitation by a "learning broker" who oversees all aspects of event organization. These include logistics, content preparation, drafting and implementation of the agenda, moderation of the learning sessions, and follow-up activities. The learning broker designs the learning process according to the specific governance challenge at hand.

The event must be structured based on several factors:

The participants invited to the event must represent different substantive and organizational perspectives and play a precisely defined role as both knowledge holders and knowledge seekers. When this is achieved, participants have unlimited access to the collective wealth of the shared tacit and explicit knowledge.

The effects of applying the governmental learning spiral technique are threefold: The primary effect is that governments gain access to the latest knowledge in democratic governance, which they can then apply to specific governance challenges with concrete, practical action. A second effect is that—because of the iterative character of the learning process—the knowledge being learned is always validated and updated in real time to include the latest experiences on the subject. A third effect is that participation in the learning process evokes a sense of social belonging among the learning actors, which often leads to the creation of networks and communities of practice where governments continue to share their latest experiences and by doing so launch the next spin of the Governmental Learning Spiral.

The governmental learning spiral technique consists of a nine-stage template. The stages of the learning process are organized chronologically in the template and split into three distinct sequences for a particular learning event.

1. Before (framing phase): The "conceptualization", "triangulation", and "accommodation" stages are the preparatory stages, where the specific governance challenge is defined, existing knowledge on the topic is framed, participants are selected and inivited, and trust is established between the learning actors and the event facilitator and between participants and the learning process itself.

2. During (reflection phase): The "internalization", "externalization", "reconceptualization", and "transformation" stages represent the core of the educational process, where learning actors review and adapt new knowledge according to their personal needs. Thereafter the actors change their individual and organizational thinking and behavior in an elaborate inter- and intrapersonal procedure.

3. After (projection phase): The follow-up to the learning activity occurs in the "configuration" stage, where all the knowledge acquired during the event is made available and accessible to everyone involved in the event as well as to a wider audience. This new knowledge further serves in the final "iteration" stage as a frame for the next spin of the governmental learning spiral, as well as a feedback loop in the context of a new learning system.

Because knowledge in governance has a short half-life and has to be updated constantly, the learning process itself must also be ongoing. This iterative procedure, where knowledge is constantly reviewed, renewed, and transformed into political action in a real-time, multi-turn process, can be illustrated as a spiral. Each of the nine stages of the learning process is bound together by a "spin," which ends with the last iteration stage and restarts the next spin with its first configuration stage.







</doc>
<doc id="416518" url="https://en.wikipedia.org/wiki?curid=416518" title="Puppet ruler">
Puppet ruler

A puppet ruler is a person who has a title indicating possession of political power, but who, in reality, is controlled by outside individuals or forces. Such outside power can be exercised by a foreign government, in which case the puppet ruler's domain is called a puppet state. But the puppet ruler may also be controlled by internal forces, such as non-elected officials.

Governing through puppet presidents has long been a political tactic in Latin America. Many dictators and strongmen have formally handed over power to other officials for several reasons, often in order to follow constitutional provisions for elections and term limits, to provide a civilian façade for military rule, or to be able to go into semi-retirement away from the capital city. Strongmen who sometimes governed through figureheads included Diego Portales of Chile, Rafael Núñez of Colombia, Tomás Guardia Gutiérrez of Costa Rica, Fulgencio Batista of Cuba, Ulises Heureaux and Rafael Trujillo of the Dominican Republic, Gabriel García Moreno of Ecuador, Raoul Cédras of Haiti, Porfirio Díaz and Plutarco Elías Calles of Mexico, the Somoza family of Nicaragua, José Antonio Remón Cantera, Omar Torrijos and Manuel Noriega of Panama, Dési Bouterse of Suriname, and Antonio Guzmán Blanco and Juan Vicente Gómez of Venezuela. While figureheads who decided to act autonomously were often dismissed, on rare occasions the "puppets" later became significant political figures in their own right. For example, Lázaro Cárdenas turned against and exiled Calles to the United States and Joaquín Balaguer was elected to the Dominican presidency six times after the assassination of Trujillo.

A puppet does not have to be a national ruler. For example, Oscar K. Allen was widely recognized to be Huey Long's puppet while serving as governor of Louisiana.

In the years before and during World War II, Puyi, the deposed Emperor of China, is usually considered to have been the puppet ruler of Manchukuo, a client state of the Empire of Japan on the Chinese mainland. 

Win Myint, the current president of Myanmar, is widely viewed by political commentators as a puppet president for Aung San Suu Kyi, who is constitutionally barred from holding the office of president.


</doc>
<doc id="31569815" url="https://en.wikipedia.org/wiki?curid=31569815" title="Fractionalism">
Fractionalism

Fractionalism is the government system that is the closest to a confederation but differs when the market system is a central market owned mostly by the government and very little by the people, yet the people have more control over it than the control of the government. This is possible by the fact that in fractionalism, the central government, on the federal level, is made up of several levels to ensure the utmost security for the people's government.

The whole nation is broken down into several small city-states. The national government cannot intervene in any acts that the city-state chooses to enforce unless it could weaken the unity of the country as a whole. Other than that, the national government cannot intervene unless there is an almost guaranteed economic collapse in the city state, a widespread famine or biological plague, or other state of emergencies declared by that city state. However, in the case of war, the national military can inhabit any city state without the need to ask permission of any form of state or local government.


</doc>
<doc id="37072168" url="https://en.wikipedia.org/wiki?curid=37072168" title="Government district">
Government district

A government district or government area is any part of a city or town in which the primary land use is by state institutions (national legislature, official residencess, foreign ministry headquarters, and so on), as opposed to a residential neighbourhood, commercial district and industrial zone, or other types of neighborhoods. In some cities, authorities use planning or zoning laws to define the boundaries of government districts.

In most national and many sub-national (state, provincial, etc.) capital cities there are distinct government districts centred on the national legislature. However, non-capitals may also have an administrative district centered on city hall or a government office building.

In the national capital, Ottawa, the main government district, centered on Parliament Hill is within Downtown Ottawa (part of Centretown), but could be extended to include small parts of Lower Town and Sandy Hill if the prime minister's and government general's residences are included, as well as the district of Hull, Quebec in the neighboring city of Gatineau where several ministries are headquartered. These areas are all linked by the ceremonial route known as "Confederation Boulevard".

In Quebec City, the provincial capital of Quebec, the "Complexe du Parlement" (including the Parliament Building) and several other government buildings, are located within the neighborhood of Parliament Hill. This is within the districts of Vieux-Québec—Cap-Blanc—colline Parlementaire and Saint-Jean-Baptiste, which are themselves in the borough of La Cité-Limoilou.

Toronto, Ontario's government district is centered on Queen's Park home of the provincial parliament.

Edmonton Alberta's main government district, is centered on Capital Boulevard (107 Street south of Jasper Avenue) which is home to the Alberta Legislature Building and the Federal Public Building. However Edmonton City Hall and Canada Place anchor another government cluster to the east.




Neighborhoods by type

</doc>
<doc id="38566163" url="https://en.wikipedia.org/wiki?curid=38566163" title="Bureaucratic inertia">
Bureaucratic inertia

Bureaucratic inertia is the supposed inevitable tendency of bureaucratic organizations to perpetuate the established procedures and modes, even if they are counterproductive and/or diametrically opposed to established organizational goals. This unchecked growth may continue independently of the organization's success or failure. Through bureaucratic inertia, organizations tend to take on a life of their own beyond their formal objectives.

The United States Department of Agriculture has offices in almost all U.S. counties, even though only 14% of counties have valid farms or existing agricultural relevancy.

The Boston Consulting Group has advised firms to cut down on bureaucratic inertia and advised firms to eliminate cruft, bloat, and redundancy in the aspects of the business which are not front-line for the consumer (i.e. the "face" of the company who the customer deals with and who the customer thinks is the value-provider of the company).

NDIA/NDIS is an insurance scheme designed to fail on a bureaucratic level by the Australian Government that purports to "assist disabled people further access the community". It is, in fact, a ten page insurance policy managed by a religious organization in Victoria, who manages individual "plans" (policies) supposedly in concert with the recipient of the funds this scheme delivers, or does not, to disabled people qualifying for the entitlement. It has at least five different bureaucracies at any one time for any one recipient, operating independently with little communication between them. As stated, it is a political insurance policy (football) designed to fail on a bureaucratic level.


</doc>
<doc id="38686771" url="https://en.wikipedia.org/wiki?curid=38686771" title="Constitutional body">
Constitutional body

A constitutional body in India is a body or institute established by the Constitution of India. They can only be created or changed by passing a constitutional amendment bill, rather than by a regular, government or private bill.


</doc>
<doc id="7198484" url="https://en.wikipedia.org/wiki?curid=7198484" title="Internal security">
Internal security

Internal security, is the act of keeping peace within the borders of a sovereign state or other self-governing territories, generally by upholding the national law and defending against internal security threats. Responsibility for internal security may range from police to paramilitary forces, and in exceptional circumstances, the military itself.

Threats to the general peace may range from low-level civil disorder,large scale violence, or even an armed insurgency. Threats to internal security may be directed at either the state's citizens, or the organs and infrastructure of the state itself, and may range from petty crime, serious organized crime, political or industrial unrest, or even domestic terrorism. Foreign powers may also act as a threat to internal security, by either committing or sponsoring terrorism or rebellion, without actually declaring war.
Governmental responsibility for internal security will generally rest with an interior ministry, as opposed to a defence ministry. Depending on the state, a state's internal security will be maintained by either the ordinary police or law enforcement agencies or more militarized police forces (known as Gendarmerie or, literally, the Internal Troops.). Other specialized internal security agencies may exist to augment these main forces, such as border guards, special police units, or aspects of the state's intelligence agencies. In some states, internal security may be the primary responsibility of a secret police force.

The level of authorized force used by agencies and forces responsible for maintaining internal security might range from unarmed police to fully armed paramilitary organizations, or employ some level of less-lethal weaponry in between. For violent situations, internal security forces may contain some element of military type equipment such as non-military armored vehicles.

Depending on the organization of the state, internal security forces may have jurisdiction on national or federal levels. As the concept of internal security refers to the entity of the state and its citizens, persons who are threats to internal security may be designated as an enemy of the state or enemy of the people.

Persons detained by internal security forces may either be dealt with by the normal criminal justice system, or for more serious crimes against internal security such as treason, they may face special measures such as secret trials. In times of extreme unrest, internal security actions may include measures such as internment (detention without trial).

Depending on the nature of the specific state's form of government, enforcing internal security will generally not be carried out by a country's military forces, whose primary role is external defense, except in times of extreme unrest or other state of emergency, short of civil war. Often, military involvement in internal security is explicitly prohibited, or is restricted to authorized military aid to the civil power as part of the principle of civilian control of the military. Military special forces units may in some cases be put under the temporary command of civilian powers, for special internal security situations such as counter terrorism operations.



</doc>
<doc id="42299690" url="https://en.wikipedia.org/wiki?curid=42299690" title="Center of government">
Center of government

The center of government (CoG) is the institution or group of institutions that provide direct support to the chief executive (president or prime minister) in leading the management of government. Unlike line ministries and other government agencies, the CoG does not deliver services directly to the citizens, and it does not focus on a specific policy area. On the contrary, the CoG performs cross-government functions. A similar concept is "Core Executive".

Two types of CoG definition exist: by structure or by function.

In the first type, the defining criterion is the position in the structure of the Executive branch. It only includes institutions and units that directly and exclusively support the head of the government. For example, it refers to Ministries or General-Secretariats of the Presidency, Offices of the President or Prime Minister, and Cabinet Offices.

In the functional definitions, the defining criterion is that the institution performs whole-of-government functions, especially in terms of planning, coordination, monitoring, political management, and communications. Therefore, in addition to the previously mentioned institutions (such as Ministries of the Presidency), other institutions performing these tasks are also included, even if they are not part of the structure of the Presidency. For example, Ministries or Offices of Planning, , Inter-Ministerial Committees, and even Budget Offices are considered part of the CoG.

A number of authors favor functional definitions, for considering them more applicable to countries with different forms of government and institutional frameworks.

Chief Executives (Presidents and Prime Ministers) have had supporting institutions for a long time. In the United Kingdom, the dates to 1916, when the war demanded a stronger central coordination for the government. In the United States, the Executive Office of the President was established in 1939, after the stated that "the President needs help." Offices, Ministries or General Secretariats of the Presidency have also existed in Latin American for several decades now.

However, a number of factors lead to a greater importance of the CoG. The following factors have been highlighted: the cross-cutting nature of several current public problems, the need to lead with a unified orientation governments that have decentralized authority to autonomous agencies, and the growing interesting for achieving results for the citizens, beyond a purely fiscal coordination. These factors explain the interest that several governments and international organizations have on the topic, including projects by the Organisation for Cooperation and Economic Development and the Inter-American Development Bank.

Moreover, new institutional developments have generated increased interest in the CoG. The in the United Kingdom was established to ensure that the government's priorities were being implemented, through an intensive monitoring by the CoG. Several countries have replicated this model, including Malaysia and Chile.
Paraguay has established a "Center of Government" within the General-Secretariat of the Presidency, in charge of coordinating and monitoring the government action.

In OECD countries, the CoG institutions lead the functions of coordinating the preparation of Cabinet meetings, coordinating the formulation and implementation of policy, monitoring delivery, strategic planning for the whole-of-government, and communicating the government's messages. Regarding the characteristics of the CoG, there are important variations in terms of organization and staff. For example, in countries like the United Kingdom and Sweden most of the CoG staff belongs to the civil service, while in Canada, Australia and New Zealand political appointees have a larger presence. Important variations have also been identified in Latin America, not only in terms of organization but also in the capacity of CoGs to perform their functions: while some countries have more developed CoGs, others present more serious weaknesses. This heterogeneity across countries has also been detected in the Middle East and North Africa region.



</doc>
<doc id="46638919" url="https://en.wikipedia.org/wiki?curid=46638919" title="Diplomatic capital">
Diplomatic capital

Diplomatic capital refers to the trust, goodwill, and influence which a diplomat, or a state represented by its diplomats, has within international diplomacy. According to political scientist Rebecca Adler-Nissen, diplomatic capital is a kind of currency that can be traded in diplomatic negotiations and that is increased when positive ″social competences, reputation and personal authority" are portrayed.

Diplomatic capital can be accumulated for example by economic cooperation and by contributions to the solution of international crises, It is strengthened when in other countries the sentiment prevails that the interests of a state or the diplomats representing it are aligned with their own interests. Conversely, it can be squandered when a country engages in a confrontation, an armed conflict or a war, if that is perceived as unjust or at odds with the interests of others.

Diplomatic capital is also linked to the extent of enforcement of human rights.



</doc>
<doc id="3512364" url="https://en.wikipedia.org/wiki?curid=3512364" title="Tribal chief">
Tribal chief

A tribal chief or chieftain is the leader of a tribal society or chiefdom.

Tribal societies with social stratification under a single (or dual) leader emerged in the Neolithic period out of earlier tribal structures with little stratification, and they remained prevalent throughout the Iron Age.

In the case of indigenous tribal societies existing within larger colonial and post-colonial states, tribal chiefs may represent their tribe or ethnicity in a form of self-government.

The most common types are the chairman of a council (usually of "elders") and/or a broader popular assembly in "parliamentary" cultures, the war chief (may be an alternative or additional post in war time), the hereditary chief, and the politically dominant medicineman (in "theocratic" cultures).

The term is usually distinct from chiefs at lower levels, such as village chief (geographically defined) or clan chief (an essentially genealogical notion). The descriptive "tribal" requires an ethno-cultural identity (racial, linguistic, religious etc.) as well as some political (representative, legislative, executive and/or judicial) expression. In certain situations, and especially in a colonial context, the most powerful member of either a confederation or a federation of such tribal, clan or village chiefs would be referred to as a paramount chief. This term has largely fallen out of use, however, and such personages are now often called kings.

A woman who holds a chieftaincy in her own right or who derives one from her marriage to a male chief has been referred to alternatively as a chieftainess, a chieftess or, especially in the case of the former, a chief.

Anthropologist Elman Service distinguishes two stages of tribal societies: simple societies organized by limited instances of social rank and prestige, and more stratified societies led by chieftains or tribal kings (chiefdoms). Historically, tribal societies represent an intermediate stage between the band society of the Paleolithic stage and civilization with centralized, super-regional government based in cities. Stratified tribal societies led by tribal kings thus flourished from the Neolithic stage into the Iron Age, albeit in competition with urban civilisations and empires beginning in the Bronze Age. 
An important source of information for tribal societies of the Iron Age is Greco-Roman ethnography, which describes tribal societies surrounding the urban, imperialist civilisation of the Hellenistic and Roman periods.

After the collapse of the Western Roman Empire, tribal kingdoms were again established over much of Europe in the wake of the Migration period but they quickly turned into nobility-based feudalism afterwards. By the High Middle Ages, these had again coalesced into super-regional monarchies.

Tribal societies remained prevalent in much of the New World. Exceptions to tribal societies outside of Europe and Asia were Paleolithic or Mesolithic band societies in Oceania and in parts of Sub-Saharan Africa. Europeans forced centralized governments onto these societies during colonialism, but in some instances tribes have retained or regained partial self-government and their lifestyles.




Arabs, in particular peninsular Arabs, nomadic Bedouins and many Iraqis and Syrians, are largely organized in tribes, many of whom have official representatives in governments. Tribal chiefs are known as sheikhs, though this term is also sometimes applied as an honorific title to spiritual leaders of Sufism.

The Afro-Bolivian people, a recognized ethnic constituency of Bolivia, are led by a 
king whose title is also recognized by the Bolivian government.

In Botswana, the reigning kgosis of the various tribes are legally empowered to serve as advisers to the government as members of the Ntlo ya Dikgosi, the national House of Chiefs. In addition to this, they also serve as the ex officio chairs of the tribal "kgotlas", meetings of all of the members of the tribes, where political and social matters are discussed.

The band is the fundamental unit of governance among the First Nations in Canada (formerly called "Indians"). Most bands have elected chiefs, either directly elected by all members of the band, or indirectly by the band council, these chiefs are recognized by the Canadian state under the terms of the Indian Act. As well, there may be traditional hereditary or charismatic chiefs, who are usually not part of the Indian Act-sanctioned formal government. There were 614 bands in Canada in 2012. There is also a national organization, the Assembly of First Nations, which elects a "national chief" to act as spokesperson of all First Nations bands in Canada.

The offices and traditional realms of the nanas of Ghana are constitutionally protected by the republican constitution of the country. The chiefs serve as custodians of all traditional lands and the cultures of the traditional areas. They also serve as members of the Ghanaian National House of Chiefs.

Although both the Nigerian traditional rulers and the wider chieftaincy aren't mentioned in Nigeria's current constitution, they derive their powers from various so-called "Chiefs laws" and are therefore legally recognized. The traditional rulers and select chiefs usually serve as members of each federating state's State Council of Traditional Rulers and Chiefs.

The Solomon Islands have a Local Court Act which empowers chiefs to deal with crimes in their communities, thus assuring them of considerable effective authority.

Apo Rodolfo Aguilar (Kudol I) serves as the chieftain of the Tagbanwa tribes people living in Banuang Daan and Cabugao settlements in Coron Island, Palawan, Philippines. His position is recognized by the Filipino government.

Such figures as the king of the Zulu Nation and the rain queen are politically recognized in South Africa because they derive their status, not only from tribal custom, but also from the Traditional Leadership Clause of the country's current constitution.

The pre-colonial states that existed in what is today Uganda were summarily abolished following independence from Great Britain. However, following constitutional reforms in 1993, a number of them were restored as politically neutral constituencies of the state by the government of Yoweri Museveni. Such figures as the kabaka of Buganda and the omukama of Toro typify the Ugandan chieftaincy class.

Generally, a tribe or nation is considered to be part of an ethnic group, usually sharing cultural values. 
For example, the forest-dwelling Chippewa historically built dwellings from the bark of trees, as opposed to the Great Plains-dwelling tribes, who would not have access to trees, except by trade, for example for lodgepoles. Thus, the tribes of the Great Plains might have typically dwelt in skin-covered tipis rather than bark lodges. But some Plains tribes built their lodges of earth, as for example the Pawnee. The Pueblo people, meanwhile, built their dwellings of stone and earth.

A chief might be considered to hold all political power, say by oratory or by example. But on the North American continent, it was historically possible to evade the political power of another by migration. The Mingos, for example, were Iroquois who migrated further west to the sparsely populated Ohio Country during the 18th century. Two Haudenosaunee, or Iroquois, Hiawatha and the Great Peacemaker, formulated a constitution for the Iroquois Confederation.

The tribes were pacified by units of the United States Army in the nineteenth century, and were also subject to forced schooling in the decades afterward. Thus, it is uncommon for today's tribes to have a purely Native American cultural background, and today Native Americans are in many ways simply another ethnicity of the secular American people. Because formal education is now respected, some like Peter MacDonald, a Navajo, left their jobs in the mainstream U.S. economy to become chairpeople of their tribal councils or similar self-government institutions.

Not all tribal leaders need be men; Wilma Mankiller was a well-known Chief of the Cherokee Nation. Also, though the fount of power might be the chief, he or she is typically not free to wield power without the consent of a council of elders of some kind. For example: Cherokee men were not permitted to go to war without the consent of the council of women.

Tribal government is an official form of government in the United States, as it is in a number of countries around the world.

Historically, the U.S. government treated tribes as seats of political power, and made treaties with the tribes as legal entities. Be that as it may, the territory of these tribes fell under the authority of the Bureau of Indian Affairs as reservations held in trust for the tribes. Citizenship was formerly considered a tribal matter. For example, it was not until 1924 that the Pueblo people were granted U.S. citizenship, and it was not until 1948 that the Puebloans were granted the right to vote in state elections in New Mexico. In Wisconsin, the Menominee Nation has its own county Menominee County, Wisconsin with special car license plates; 87% of the county's population is Native American.

Mainstream Americans often find pride and comfort in realizing that at least part of their ethnic ancestry is Native American, although the connection is usually only sentimental and not economic or cultural. Thus, there is some political power in one's ability to claim a Native American connection (as in the Black Seminole).

Because the Nations were sovereign, with treaty rights and obligations, the Wisconsin tribes innovated Indian gaming in 1988, that is, on-reservation gambling casinos, which have since become a US$14 billion industry nationwide. This has been imitated in many of the respective states that still have Native American tribes. The money that this generates has engendered some political scandal. For example, the Tigua tribe, which fled their ancestral lands in New Mexico during the Pueblo revolt of 1680, and who then settled on land in El Paso County, Texas, has paid for a low probable return to the tribe because of the Jack Abramoff publicity.

Many of the tribes use professional management for their money. Thus, the Mescalero Apache renovated their Inn of the Mountain Gods to include gambling as well as the previous tourism, lodging, and skiing in the older Inn.

The Navajo nation defeated bids to open casinos in 1994, but by 2004 the Shiprock casino was a "fait accompli".






</doc>
<doc id="25690925" url="https://en.wikipedia.org/wiki?curid=25690925" title="Classification of the Functions of Government">
Classification of the Functions of Government

Classification of the Functions of Government (COFOG) is a classification defined by the United Nations Statistics Division. These functions are designed to be general enough to apply to the government of different countries. The accounts of each country in the United Nations are presented under these categories. The value of this is that the accounts of different countries can be compared.



</doc>
<doc id="3870861" url="https://en.wikipedia.org/wiki?curid=3870861" title="Governmental accounting">
Governmental accounting

Government accounting refers to the process of recording and the management of all financial transactions incurred by the government which includes its income and expenditures.

Various governmental accounting systems are used by various public sector entities. In the United States, for instance, there are two levels of government which follow different accounting standards set forth by independent, private sector boards. At the federal level, the Federal Accounting Standards Advisory Board (FASAB) sets forth the accounting standards to follow. Similarly, there is the Governmental Accounting Standards Board (GASB) for state and local level government. 

There is an important difference between private sector accounting and governmental accounting. The main reasons for this difference is the environment of the accounting system. In the government environment, public sector entities have different goals, as opposed to the private sector entities' one main goal of gaining profit. Also, in government accounting, the entity has the responsibility of fiscal accountability which is demonstration of compliance in the use of resources in a budgetary context. In the private sector, the budget is a tool in financial planning and it isn't mandatory to comply with it.

Government accounting refers to the field of accounting that specifically finds application in the public sector or government. A special field of accounting exists because:

An exception exists on the above-mentioned differences in the case of public utility businesses (for example Electricity Services) that may be intended to produce a net income or profit, but a significant debate exists over whether there should be such an exception. Nationalisation includes, amongst others, the argument that entities should be either private or public, and that the objectives of public entities should differ significantly from that of private entities. In other words, is the generation and reticulation of electricity with the objective to generate a profit in the public interest or not? And if it is the best way, shouldn’t it then be completely private instead of having access to public funds and monopolies?

Governmental accounting standards are currently being dominated by the accounting standards (internationally sometimes referred to as IFRS) originally designed for the private sector. The so-called Generally Recognised Accounting Practices (GRAP) that are being enforced in the public sector of countries such as South Africa, one of the front-runners in this regard is based on the Generally Accepted Accounting Practices originally developed for the private sector. The above and common sense raises the question of whether this is the best solution. It is, of course, cheaper and it is alleged that the history of separate development of accounting practices for government has not been successful. Even at the onset of the current fiscal crisis in Europe and other parts of the world, it was argued authoritatively that the sometimes inapplicable accounting practices of the private sector being used, have contributed to the origination of, and belated reaction to, the fiscal crisis.(1)

Sources (not directly quoted but used in synthesis):

1. Sanderson, I. Worldwide Credit Crisis and Stimulus Packages in Accountancy SA, June 2009, p. 14.

2. Conradie, J.M. The applicability of Generally Accepted Accounting Practice in the Central Government of South Africa (English summary of a thesis written in another language) University of Pretoria, 1994.

3. Conradie, J.M. Die toepaslikheid van Algemeen Aanvaarde Rekeningkundige Praktyk in die Sentrale Owerheid van Suid-Afrika. Universiteit van Pretoria, 1993. (Original full text of the summary.)
4. Donald amcool

The governmental accounting system sometimes uses the historic system of fund accounting. A set of separate, self-balancing accounts are responsible for managing resources that are assigned to specific purposes based on regulations and limitations.

The governmental accounting system has a different focus for measuring accounting than private sector accounting. Rather than measuring the flow of economic resources, governmental accounting measures the flow of financial resources. Instead of recognizing revenue when they are earned and expenses when they are incurred, revenue is recognized when there is money available to liquidate liabilities within the current accounting period, and expenses are recognized when there is a drain on current resources.

Governmental financial statements must be accompanied by required supplementary information (RSI). The RSI is a comparison of the actual expenses compared to the original budget created at the beginning of the fiscal year for the Government's General Fund and all major Special Revenue Funds.

The unique objectives of government accounting do not preclude the use of the double entry accounting system. There can, however, be other significant differences with private sector accounting practices, especially those that are intended to arrive at a net income result. The objectives for which government entities apply accountancy that can be organized in two main categories:
- The accounting of activities for accountability purposes. In other words, the representatives of the public, and officials appointed by them, must be accountable to the public for powers and tasks delegated. The public, who have no other choice but to delegate, are in a position that differs significantly from that of shareholders and therefore need financial information, to be supplied by accounting systems, that is applicable and relevant to them and their purposes.
- Decision-making purposes. The relevant role-players, especially officials and representatives, need financial information that is accounted, organized and presented for the objectives of their decision-making. These objectives bear, in many instances, no relation to net income results but are rather about service delivery and efficiency. The taxpayer, a very significant group, simply wants to pay as little as possible taxes for the essential services for which money is being coerced by law.
Sources (not directly quoted but used in synthesis):

1. Conradie, J.M. The applicability of Generally Accepted Accounting Practice in the Central Government of South Africa (English summary of a thesis written in another language) University of Pretoria, 1993.

2. Conradie, J.M. Die toepaslikheid van Algemeen Aanvaarde Rekeningkundige Praktyk in die Sentrale Owerheid van Suid-Afrika. Universiteit van Pretoria, 1993. (Original full text of the summary.)



</doc>
<doc id="325419" url="https://en.wikipedia.org/wiki?curid=325419" title="Political authorities">
Political authorities

Political authorities hold positions of power or influence within a system of government. Although some are exclusive to one or another form of government, many exist within several types.



</doc>
<doc id="5594801" url="https://en.wikipedia.org/wiki?curid=5594801" title="Joint session">
Joint session

A joint session or joint convention is, most broadly, when two normally separate decision-making groups meet together, often in a special session or other extraordinary meeting, for a specific purpose.

Most often it refers to when both houses of a bicameral legislature sit together. A joint session typically occurs to receive foreign or domestic diplomats or leaders, or to allow both houses to consider bills together.

Some constitutions give special power to a joint session, voting by majority of all members of the legislature regardless of which house or chamber they belong to. For example, in Switzerland a joint session of the two houses elects the members of the Federal Council (cabinet). In India, disputes between houses are resolved by a joint sitting but without an intervening election.

In the Australian federal parliament, a joint sitting can be held, under certain conditions, to overcome a deadlock between the two houses. For a deadlock to be declared, a bill has to be rejected twice by the Senate at an interval of at least three months, after which a double dissolution election can be held. If, following the election, the new parliament is still unable to pass the bill, it may be considered by a joint sitting of the House of Representatives and the Senate, and must achieve an absolute majority of the total number of members and senators in order to pass. The only example of this occurring was the Joint Sitting of the Australian Parliament of 1974 under the Whitlam Labor government, at which six deadlocked bills were passed.

Because the House has twice as many members as the Senate, the former has an advantage in a joint sitting. However, the voting system used for the Senate before 1949, which might be called "multiple at-large voting", often led to landslide if not wipe-out results in each state, resulting in a winning margin over the whole of Australia of up to 36-0. That would have given the party or grouping enjoying such a large Senate majority an advantage in any joint sitting, had there been one.

The voting system now used for the Senate, quota-preferential proportional representation, almost inevitably leads to very evenly divided results. Six senators are elected from each state and two from each territory. A party or grouping has to get at least 57% of the vote in any State to obtain a four-two majority of seats in that state, whereas from 51% to 56% of the vote yields only an equality of three seats to each major party or group.

The Federal Assembly is a formal joint session of the two houses of the bicameral Austrian Parliament, to swear the elected President of Austria into office.

The Chamber of Representatives and the Senate convene as United Chambers (; ; ) to swear the King into office, as stipulated by article 91 of the Constitution.

The Canadian government procedure is called a joint address, with the members of the House of Commons attending the Senate as guests. There is no procedure in Canada for both chambers of the Parliament to sit in a true joint session.

Various government agencies and non-governmental organizations may also meet jointly to handle problems which each of the involved parties has a stake in.

The Congress of France is an assembly of both houses of the French Parliament, convened at the Palace of Versailles, which can approve certain amendments to the constitution by a three-fifth majority of all members. Since 2008, the Congress may also be convened to hear an address from the President of the Republic.

The Federal Convention elects the President of Germany. It includes members from the Bundestag and representatives of the States of Germany.

In India, if an ordinary bill has been rejected by any house of the parliament and if more than six months have elapsed, the President may summon a joint session for purpose of passing the bill. The bill is passed by a simple majority of a joint sitting. Since the lower house (Lok Sabha) has more than twice the members of the upper house (Rajya Sabha), a group commanding a majority in the lower house of the Government of India can pass such a bill even if it was previously rejected by the upper house.

So far, Joint Session of the Parliament of India has been called for only three bills that have been passed at joint sessions: the Dowry Prohibition Act, 1961, the Banking Service Commission Repeal Bill, 1978, and the Prevention of Terrorism Act, 2002.

In the Irish Free State, the predecessor of the Republic of Ireland, the Governor-General's Address to both houses was made to the lower house, with Senators invited to attend.

In the Philippines, Congress can convene in a joint session for the following:

While the State of the Nation address occurs annually, and presidential elections occur every six years, the only time that the other two conditions were met after the approval of the 1987 constitution was after the declaration of martial law in Maguindanao after the Maguindanao massacre.

Joint sessions are typically held at the seat of the House of Representatives, which is at the Batasang Pambansa Complex, Quezon City.

English and later British monarchs have jointly addressed the House of Commons and the House of Lords since the 16th century. Since 1939, foreign heads of state and dignitaries have been invited to address both houses of Parliament, the first to do so was French President Albert Lebrun in March 1939.

The speech from the throne upon the State Opening of Parliament is made before a joint sitting of both Houses. This occurs in the House of Lords, the upper chamber, due to the constitutional convention that the monarch never enters the House of Commons. The closing of each of parliamentary session is also marked by a speech to both Houses.

The State of the Union Address of the president of the United States is traditionally made before a "joint session" of the United States Congress. Many states refer to an analogous event as a "joint convention". Such assemblies are typically held in the chamber of the lower house as the larger body. State constitutions of U.S. states may require joint conventions for other purposes; for example Tennessee's requires such to elect the secretary of state, the state treasurer, and the comptroller of the treasury.

The first foreign dignitary to address a joint session of Congress was Ambassador André de La Boulaye of France who addressed a joint session on May 20, 1934.



</doc>
<doc id="52674452" url="https://en.wikipedia.org/wiki?curid=52674452" title="Cabinet crisis">
Cabinet crisis

A cabinet crisis or government crisis is a situation when the government is challenged before the mandate period expires, because it threatens to resign over a proposal, or it is at risk at being dismissed after a motion of no confidence, a conflict between the parties in a coalition government or a coup d'état. It may also be the result of there being no clear majority willing to work together to form a government. During this period a caretaker government with a limited mandate may take care of the day-to-day affairs of the state, while waiting for a snap election.




</doc>
<doc id="862851" url="https://en.wikipedia.org/wiki?curid=862851" title="State government">
State government

A state government is the government of a country subdivision in a federal form of government, which shares political power with the federal or national government. A state government may have some level of political autonomy, or be subject to the direct control of the federal government. This relationship may be defined by a constitution.

The reference to "state" denotes country subdivisions which are officially or widely known as "states", and should not be confused with a "sovereign state". Provinces are usually divisions of unitary states. Their governments, which are also "provincial governments", are not the subject of this article.

The United States and Australia are the main examples of federal systems in which the term "state" is used for the subnational components of the federation. In addition, the Canadian provinces fulfil a similar role. The term for subnational units in non-English-speaking federal countries may also often be translated as "state", e.g. States of Germany (German "Länder").

The Commonwealth of Australia is a federal nation with six states (and two mainland territories). Section 51 of the Australian Constitution sets out the division of legislative power between the states and the Commonwealth (federal) government. The Commonwealth government is given a variety of legislative powers, including control of foreign policy, taxation (although this cannot discriminate between states or parts of states), and regulation of interstate commerce and corporations. Since the original ratification of the constitution, the High Court of Australia has settled a number of disputes concerning the extent of the Commonwealth's legislative powers, some of which have been controversial and extensively criticised; these included a dispute in 1982 over whether the Commonwealth was entitled to designate land for national heritage purposes under United Nations agreements, as well as numerous disputes over the extent of the Commonwealth's power over trade union and industrial relations legislation.

One difference between the Australian and United States models of federalism is that, in Australia, the Commonwealth Parliament has explicit constitutional power over marriage legislation; this has been a focal point for recent controversies over same-sex marriage.

Each state of Australia has a governor, who represents the Queen of Australia (currently Elizabeth II of the United Kingdom) and performs the ceremonial duties of a head of state. Every state also has a parliament; most states have a bicameral parliament, except for Queensland, where the upper chamber (the Legislative Council) was abolished in 1922. Like their Indian counterparts, Australian states have a Westminster system of parliamentary government; the head of government, known in each state as a Premier, is drawn from the state parliament.

In India, the state governments are the level of governments below the Union government. India is a Sovereign Socialist Secular Democratic Republic with a Parliamentary system of government. The Republic is governed in terms of the Constitution of India. Sovereignty is shared between the union and the state government, but the union government is given greater powers. The President is the constitutional head Executive of the State. Real executive power vests in a Union Council of Ministers with the Prime Minister as head of government. The States resembles the federal system. In the states, the Governor is the head of Executive, but real executive power vests with the Chief Minister who heads the Council of Ministers. The judicial setup of the country is headed by the Chief Justice of India at federal level, who presides over one of the largest judicial apparatus dispensing criminal, civil and all other forms of litigation, and Chief Justices of the High Courts at state level. The government head of its legal wing is the Attorney General of India at federal level and Advocate General at state level.

In Nigeria, States are constituent political entities, which shares sovereignty with the Federal Government of Nigeria. In the Nigerian Constitution all powers not granted to the States are reserved to the Federal Government.

In Pakistan, the provincial governments are the level of government below the federal government. Pakistan is a Sovereign Islamic Republic and a unitary Parliamentary Multi-party system form of a National Government.

The Provincial Government Head by the Elected Chief Minister .

South Africa is divided into nine provinces which have their own elected governments. Chapter Six of the Constitution of South Africa describes the division of power between the national government and the provincial governments, listing those "functional areas" of government that are exclusively reserved to the provincial governments and those where both levels of government have concurrent powers; the remaining areas not listed are reserved to the national government. In areas where both levels have concurrent powers there is a complex set of rules in the event of a conflict between national and provincial legislation. Generally in such a case the provincial legislation prevails, but national legislation may prescribe standards and frameworks for provinces to follow, and may prevent provinces from adversely affecting national interests or the interests of other provinces. The functional areas in which the provincial governments have powers include agriculture, arts and culture, primary and secondary education, the environment and tourism, health, housing, roads and transport, and social welfare.

The provincial governments are structured according to a parliamentary system in which the executive is dependent on and accountable to the legislature. The unicameral provincial legislature is elected by party-list proportional representation, and the legislature in turn elects one of its members as Premier to head the executive. The Premier appoints an Executive Council (a cabinet), consisting of members of the legislature, to administer the various departments of the provincial administration.

Under the 10th Amendment to the U.S. Constitution, all governmental powers not granted to the Federal government of the United States nor prohibited by it to the States, are reserved to the States respectively.



</doc>
<doc id="233475" url="https://en.wikipedia.org/wiki?curid=233475" title="Multi-party system">
Multi-party system

A multi-party system is a political system in which multiple political parties across the political spectrum run for national election, and all have the capacity to gain control of government offices, separately or in coalition. Apart from one-party-dominant and two-party systems, multi-party systems tend to be more common in parliamentary systems than presidential systems and far more common in countries that use proportional representation compared to countries that use first-past-the-post elections. Several parties compete for power and all of them have reasonable chance of forming government.

First-past-the-post requires concentrated areas of support for large representation in the legislature whereas proportional representation better reflects the range of a population's views. Proportional systems may have multi-member districts with more than one representative elected from a given district to the same legislative body, and thus a greater number of viable parties. Duverger's law states that the number of viable political parties is one, plus the number of seats in a district.

Argentina, Armenia, Austria, Australia, Belgium, Brazil, Canada, Croatia, the Czech Republic, Denmark, Finland, France, Germany, Iceland, India, Indonesia, Iran, Ireland, Israel, Italy, Kosovo, Lebanon, Maldives, Mexico, Moldova, Nepal, the Netherlands, New Zealand, Norway, Pakistan, the Philippines, Poland, Portugal, Romania, Russia, Serbia, Spain, Sri Lanka, Sweden, Switzerland, Taiwan, Tunisia, Ukraine, and the United Kingdom are examples of nations that have used a multi-party system effectively in their democracies. In these countries, usually no single party has a parliamentary majority by itself. Instead, multiple political parties are compelled to form compromised coalitions for the purpose of developing power blocks and attaining legitimate mandate.

A system where only two parties have the possibility of winning an election is called a two-party system. A system where only three parties have a "realistic possibility" of winning an election or forming a coalition is sometimes called a "Third-party system". But, in some cases the system is called a "Stalled Third-Party System," when there are three parties and all three parties win a large number of votes, but only two have a chance of winning an election. Usually this is because the electoral system penalises the third party, e.g. as in Canadian or UK politics. In the 2010 elections, the Liberal Democrats gained 23% of the total vote but won less than 10% of the seats due to the first-past-the-post electoral system. Despite this, they still had enough seats (and enough public support) to form coalitions with one of the two major parties, or to make deals in order to gain their support. An example is the Conservative-Liberal Democrat coalition formed after the 2010 general election. Another is the Lib-Lab pact during Prime Minister James Callaghan's Minority Labour Government; when Labour lost its three-seat majority in 1977, the pact fell short of a full coalition. In Canada, there are three major federal political parties: the Conservative Party of Canada, the Liberal Party of Canada, and the New Democratic Party. However, in recent Canadian history, the Liberals and Conservatives (and their states) have been the only two parties to elect a Prime Minister in Canada, with the New Democratic Party, Bloc Quebecois and Green Party often winning seats in the House of Commons. The main exception was the 2011 Canadian election when the New Democrats were the Official Opposition and the Liberal Party was reduced to third party status.

Unlike a one-party system (or a two-party system), a multi-party system encourages the general constituency to form multiple distinct, officially recognized groups, generally called political parties. Each party competes for votes from the enfranchised constituents (those allowed to vote). A multi-party system prevents the leadership of a single party from controlling a single legislative chamber without challenge.

If the government includes an elected Congress or Parliament, the parties may share power according to proportional representation or the first-past-the-post system. In proportional representation, each party wins a number of seats proportional to the number of votes it receives. In first-past-the-post, the electorate is divided into a number of districts, each of which selects one person to fill one seat by a plurality of the vote. First-past-the-post is not conducive to a proliferation of parties, and naturally gravitates toward a two-party system, in which only two parties have a real chance of electing their candidates to office. This gravitation is known as Duverger's law. Proportional representation, on the other hand, does not have this tendency, and allows multiple major parties to arise. But, recent coalition governments, such as that in the U.K., represent two-party systems rather than multi-party systems. This is regardless of the number of parties in government.

A two-party system requires voters to align themselves in large blocs, sometimes so large that they cannot agree on any overarching principles. Some theories argue that this allows centrists to gain control. On the other hand, if there are multiple major parties, each with less than a majority of the vote, the parties are strongly motivated to work together to form working governments. This also promotes centrism, as well as promoting coalition-building skills while discouraging polarization.


</doc>
<doc id="240683" url="https://en.wikipedia.org/wiki?curid=240683" title="Caretaker government">
Caretaker government

A caretaker government is a temporary "ad hoc" government that performs some governmental duties and functions in a country until a regular government is elected or formed. Depending on specific practice, it usually consists of either randomly selected or approved members of parliament or outgoing members until their dismissal.

Caretaker governments in representative democracies are usually limited in their function, serving only to maintain the "status quo", rather than truly govern and propose new legislation. Unlike the government it is meant to temporarily replace, a caretaker government does not have a legitimate mandate (electoral approval) to exercise aforementioned functions.

Caretaker governments may be put in place when a government in a parliamentary system is defeated in a motion of no confidence, or in the case when the house to which the government is responsible is dissolved, to be in place for an interim period until an election is held and a new government is formed. In this sense, in some countries which use a Westminster system of government, the caretaker government is simply the incumbent government, which continues to operate in the interim period between the normal dissolution of parliament for the purpose of holding an election and the formation of a new government after the election results are known. Unlike in ordinary times, the caretaker government's activities are limited by custom and convention.

In systems where coalition governments are frequent a caretaker government may be installed temporarily while negotiations to form a new coalition take place. This usually occurs either immediately after an election in which there is no clear victor or if one coalition government collapses and a new one must be negotiated. Caretaker governments are expected to handle daily issues and prepare budgets for discussion, but are not expected to produce a government platform or introduce controversial bills.

A caretaker government is often set up following a war until stable democratic rule can be restored, or installed, in which case it is often referred to as a provisional government.

Many countries are administered by a caretaker government during election periods, such as:
Other countries that use similar mechanisms include Pakistan and New Zealand.

 Iraq is governed by a caretaker government.

Heads of caretaker governments are often referred to as a "caretaker" head, for example "caretaker prime minister".

Similarly, but chiefly in the United States, caretakers are individuals who fill seats in government temporarily without ambitions to continue to hold office on their own. This is particularly true with regard to United States senators who are appointed to office by the governor of their state following a vacancy created by the death or resignation of a sitting senator. Sometimes governors wish to run for the seat themselves in the next election but do not want to be accused of unfairness by arranging their own appointments in the interim. Also, sometimes they do not wish to be seen as taking sides within a group of party factions or prejudicing of a primary election by picking someone who is apt to become an active candidate for the position. At one time, widows of politicians were often selected as caretakers to succeed their late husbands; this custom is rarely exercised today, as it could be viewed by some as nepotism.

In Canada and most other English-speaking countries, the more widely accepted term in this context is "interim", as in interim leader. In Italy, this kind of premier is the President of Government of Experts.

The following is a list of individuals who have been considered caretaker (or provisional or interim) heads of state or heads of government:





</doc>
<doc id="6784" url="https://en.wikipedia.org/wiki?curid=6784" title="Citizenship">
Citizenship

Citizenship is the status of a person recognized under the custom or law as being a legal member of a sovereign state or belonging to a nation. The idea of citizenship has been defined as the capacity of individuals to defend their rights in front of the governmental authority.

A person may have multiple citizenships. A person who does not have citizenship of any state is said to be stateless, while one who lives on state borders whose territorial status is uncertain is a border-lander.

Nationality is often used as a synonym for citizenship in English – notably in international law – although the term is sometimes understood as denoting a person's membership of a nation (a large ethnic group). In some countries, e.g. the United States, the United Kingdom, nationality and citizenship can have different meanings (for more information, see Nationality versus citizenship).

Each country has its own policies, regulations and criteria as to who is entitled to its citizenship. A person can be recognized or granted citizenship on a number of bases. Usually citizenship based on circumstances of birth is automatic, but in other cases an application may be required.

Whilst legitimate and usually limited in quota, the schemes are controversial. Costs for citizenship by investment range from as little as $100,000 (£74,900) to as much as €2.5m (£2.19m)

Many thinkers point to the concept of citizenship beginning in the early city-states of ancient Greece, although others see it as primarily a modern phenomenon dating back only a few hundred years and, for humanity, that the concept of citizenship arose with the first laws. "Polis" meant both the political assembly of the city-state as well as the entire society. Citizenship concept has generally been identified as a western phenomenon. There is a general view that citizenship in ancient times was a simpler relation than modern forms of citizenship, although this view has come under scrutiny. The relation of citizenship has not been a fixed or static relation, but constantly changed within each society, and that according to one view, citizenship might "really have worked" only at select periods during certain times, such as when the Athenian politician Solon made reforms in the early Athenian state.

Historian Geoffrey Hosking in his 2005 "Modern Scholar" lecture course suggested that citizenship in ancient Greece arose from an appreciation for the importance of freedom. Hosking explained:

Slavery permitted slave-owners to have substantial free time, and enabled participation in public life. Polis citizenship was marked by exclusivity. Inequality of status was widespread; citizens (πολίτης "politēs" < πόλις 'city') had a higher status than non-citizens, such as women, slaves, and resident foreigners (metics). The first form of citizenship was based on the way people lived in the ancient Greek times, in small-scale organic communities of the polis. Citizenship was not seen as a separate activity from the private life of the individual person, in the sense that there was not a distinction between public and private life. The obligations of citizenship were deeply connected into one's everyday life in the polis. These small-scale organic communities were generally seen as a new development in world history, in contrast to the established ancient civilizations of Egypt or Persia, or the hunter-gatherer bands 
elsewhere. From the viewpoint of the ancient Greeks, a person's public life was not separated from their private life, and Greeks did not distinguish between the two worlds according to the modern western conception. The obligations of citizenship were deeply connected with everyday life. To be truly human, one had to be an active citizen to the community, which Aristotle famously expressed: "To take no part in the running of the community's affairs is to be either a beast or a god!" This form of citizenship was based on obligations of citizens towards the community, rather than rights given to the citizens of the community. This was not a problem because they all had a strong affinity with the polis; their own destiny and the destiny of the community were strongly linked. Also, citizens of the polis saw obligations to the community as an opportunity to be virtuous, it was a source of honour and respect. In Athens, citizens were both ruler and ruled, important political and judicial offices were rotated and all citizens had the right to speak and vote in the political assembly.

In the Roman Empire, citizenship expanded from small-scale communities to the entirety of the empire. Romans realized that granting citizenship to people from all over the empire legitimized Roman rule over conquered areas. Roman citizenship was no longer a status of political agency, as it had been reduced to a judicial safeguard and the expression of rule and law. Rome carried forth Greek ideas of citizenship such as the principles of equality under the law, civic participation in government, and notions that "no one citizen should have too much power for too long", but Rome offered relatively generous terms to its captives, including chances for lesser forms of citizenship. If Greek citizenship was an "emancipation from the world of things", the Roman sense increasingly reflected the fact that citizens could act upon material things as well as other citizens, in the sense of buying or selling property, possessions, titles, goods. One historian explained:

Roman citizenship reflected a struggle between the upper-class patrician interests against the lower-order working groups known as the plebeian class. A citizen came to be understood as a person "free to act by law, free to ask and expect the law's protection, a citizen of such and such a legal community, of such and such a legal standing in that community". Citizenship meant having rights to have possessions, immunities, expectations, which were "available in many kinds and degrees, available or unavailable to many kinds of person for many kinds of reason". The law itself was a kind of bond uniting people. Roman citizenship was more impersonal, universal, multiform, having different degrees and applications.

During the European Middle Ages, citizenship was usually associated with cities and towns, and applied mainly to middle class folk. Titles such as burgher, grand burgher (German "Großbürger") and bourgeoisie denoted political affiliation and identity in relation to a particular locality, as well as membership in a mercantile or trading class; thus, individuals of respectable means and socioeconomic status were interchangeable with citizens.

During this era, members of the nobility had a range of privileges above commoners (see aristocracy), though political upheavals and reforms, beginning most prominently with the French Revolution, abolished privileges and created an egalitarian concept of citizenship.

During the Renaissance, people transitioned from being subjects of a king or queen to being citizens of a city and later to a nation. Each city had its own law, courts, and independent administration. And being a citizen often meant being subject to the city's law in addition to having power in some instances to help choose officials. City dwellers who had fought alongside nobles in battles to defend their cities were no longer content with having a subordinate social status, but demanded a greater role in the form of citizenship. Membership in guilds was an indirect form of citizenship in that it helped their members succeed financially. The rise of citizenship was linked to the rise of republicanism, according to one account, since independent citizens meant that kings had less power. Citizenship became an idealized, almost abstract, concept, and did not signify a submissive relation with a lord or count, but rather indicated the bond between a person and the state in the rather abstract sense of having rights and duties.

The modern idea of citizenship still respects the idea of political participation, but it is usually done through "elaborate systems of political representation at a distance" such as representative democracy. Modern citizenship is much more passive; action is delegated to others; citizenship is often a constraint on acting, not an impetus to act. Nevertheless, citizens are usually aware of their obligations to authorities, and are aware that these bonds often limit what they can do.

From 1790 until the mid-twentieth century, United States law used racial criteria to establish citizenship rights and regulate who was eligible to become a naturalized citizen. The Naturalization Act of 1790, the first law in U.S. history to establish rules for citizenship and naturalization, barred citizenship to all people who were not of European descent, stating that "any alien being a free white person, who shall have resided within the limits and under the jurisdiction of the United States for the term of two years, may be admitted to become a citizen thereof."

Under early U.S. laws, African Americans were not eligible for citizenship. In 1857, these laws were upheld in the US Supreme Court case "Dred Scott v. Sandford", which ruled that "a free negro of the African race, whose ancestors were brought to this country and sold as slaves, is not a 'citizen' within the meaning of the Constitution of the United States," and that "the special rights and immunities guarantied to citizens do not apply to them."

It was not until the abolition of slavery following the American Civil War that African Americans were granted citizenship rights. The 14th Amendment to the U.S. Constitution, ratified on July 9, 1868, stated that "all persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside." Two years later, the Naturalization Act of 1870 would extend the right to become a naturalized citizen to include "aliens of African nativity and to persons of African descent".

Despite the gains made by African Americans after the Civil War, Native Americans, Asians, and others not considered "free white persons" were still denied the ability to become citizens. The 1882 Chinese Exclusion Act explicitly denied naturalization rights to all people of Chinese origin, while subsequent acts passed by the US Congress, such as laws in 1906, 1917, and 1924, would include clauses that denied immigration and naturalization rights to people based on broadly defined racial categories. Supreme Court cases such as "Ozawa v. United States" (1922) and "U.S. v. Bhagat Singh Thind" (1923), would later clarify the meaning of the phrase "free white persons," ruling that ethnically Japanese, Indian, and other non-European people were not "white persons", and were therefore ineligible for naturalization under U.S. law.

Native Americans were not granted full US citizenship until the passage of the Indian Citizenship Act in 1924. However, even well into the 1960s some state laws prevented Native Americans from exercising their full rights as citizens, such as the right to vote. In 1962, New Mexico became the last state to enfranchise Native Americans.

It was not until the passage of the Immigration and Nationality Act of 1952 that the racial and gender restrictions for naturalization were explicitly abolished. However, the act still contained restrictions regarding who was eligible for US citizenship, and retained a national quota system which limited the number of visas given to immigrants based on their national origin, to be fixed "at a rate of one-sixth of one percent of each nationality's population in the United States in 1920". It was not until the passage of the Immigration and Nationality Act of 1965 that these immigration quota systems were drastically altered in favor of a less discriminatory system.

The 1918 constitution of revolutionary Russia granted citizenship to any foreigners who were living within Russia, so long as they were "engaged in work and [belonged] to the working class." It recognized "the equal rights of all citizens, irrespective of their racial or national connections" and declared oppression of any minority group or race "to be contrary to the fundamental laws of the Republic." The 1918 constitution also established the right to vote and be elected to soviets for both men and women "irrespective of religion, nationality, domicile, etc. [...] who shall have completed their eighteenth year by the day of election." The later constitutions of the USSR would grant universal Soviet citizenship to the citizens of all member republics in concord with the principles of non-discrimination laid out in the original 1918 constitution of Russia.

Israeli law distinguishes citizenship from nationality. The nationality of an Israeli Arab citizen is “Arab”, not Israeli, while the nationality of a Jewish citizen is “Jewish” not Israeli.

National Socialism or "Nazism", the German variant of twentieth century fascism whose precepts were laid out in Adolf Hitler's Mein Kampf, classified inhabitants of the nation into three main hierarchical categories, each of which would have different rights and duties in relation to the state: citizens, subjects, and aliens. The first category, citizens, were to possess full civic rights and responsibilities. Citizenship would be conferred only on males of German (or so-called "Aryan") heritage who had completed military service, and could be revoked at any time by the state. The Reich Citizenship Law of 1935 established racial criteria for citizenship in the German Reich, and because of this law Jews and others who could not prove "German" racial heritage were stripped of their citizenship.

The second category, subjects, referred to all others who were born within the nation's boundaries who did not fit the racial criteria for citizenship. Subjects would have no voting rights, could not hold any position within the state, and possessed none of the other rights and civic responsibilities conferred on citizens. All women were to be conferred "subject" status upon birth, and could only obtain "citizen" status if they worked independently or if they married a German citizen (see women in Nazi Germany).

The final category, aliens, referred to those who were citizens of another state, who also had no rights.

Citizenship status, under social contract theory, carries with it both rights and duties. In this sense, citizenship was described as "a bundle of rights -- primarily, political participation in the life of the community, the right to vote, and the right to receive certain protection from the community, as well as obligations." Citizenship is seen by most scholars as culture-specific, in the sense that the meaning of the term varies considerably from culture to culture, and over time. In China, for example, there is a cultural politics of citizenship which could be called "peopleship".

How citizenship is understood depends on the person making the determination. The relation of citizenship has never been fixed or static, but constantly changes within each society. While citizenship has varied considerably throughout history, and within societies over time, there are some common elements but they vary considerably as well. As a bond, citizenship extends beyond basic kinship ties to unite people of different genetic backgrounds. It usually signifies membership in a political body. It is often based on, or was a result of, some form of military service or expectation of future service. It usually involves some form of political participation, but this can vary from token acts to active service in government.

Citizenship is a status in society. It is an ideal state as well. It generally describes a person with legal rights within a given political order. It almost always has an element of exclusion, meaning that some people are not citizens, and that this distinction can sometimes be very important, or not important, depending on a particular society. Citizenship as a concept is generally hard to isolate intellectually and compare with related political notions, since it relates to many other aspects of society such as the family, military service, the individual, freedom, religion, ideas of right and wrong, ethnicity, and patterns for how a person should behave in society. When there are many different groups within a nation, citizenship may be the only real bond which unites everybody as equals without discrimination—it is a "broad bond" linking "a person with the state" and gives people a universal identity as a legal member of a specific nation.

Modern citizenship has often been looked at as two competing underlying ideas:


Scholars suggest that the concept of citizenship contains many unresolved issues, sometimes called tensions, existing within the relation, that continue to reflect uncertainty about what citizenship is supposed to mean. Some unresolved issues regarding citizenship include questions about what is the proper balance between duties and rights. Another is a question about what is the proper balance between political citizenship versus social citizenship. Some thinkers see benefits with people being absent from public affairs, since too much participation such as revolution can be destructive, yet too little participation such as total apathy can be problematic as well. Citizenship can be seen as a special elite status, and it can also be seen as a democratizing force and something that everybody has; the concept can include both senses. According to sociologist Arthur Stinchcombe, citizenship is based on the extent that a person can control one's own destiny within the group in the sense of being able to influence the government of the group. One last distinction within citizenship is the so-called consent descent distinction, and this issue addresses whether citizenship is a fundamental matter determined by a person choosing to belong to a particular nation––by their consent––or is citizenship a matter of where a person was born––that is, by their descent.

Some intergovernmental organizations have extended the concept and terminology associated with citizenship to the international level, where it is applied to the totality of the citizens of their constituent countries combined. Citizenship at this level is a secondary concept, with rights deriving from national citizenship.

The Maastricht Treaty introduced the concept of citizenship of the European Union. Article 17 (1) of the Treaty on European Union stated that: Citizenship of the Union is hereby established. Every person holding the nationality of a Member State shall be a citizen of the Union. Citizenship of the Union shall be additional to and not replace national citizenship.

An agreement known as the amended EC Treaty established certain minimal rights for European Union citizens. Article 12 of the amended EC Treaty guaranteed a general right of non-discrimination within the scope of the Treaty. Article 18 provided a limited right to free movement and residence in Member States other than that of which the European Union citizen is a national. Articles 18-21 and 225 provide certain political rights.

Union citizens have also extensive rights to move in order to exercise economic activity in any of the Member States which predate the introduction of Union citizenship.

Citizenship of the Mercosur is granted to eligible citizens of the Southern Common Market member states. It was approved in 2010 through the Citizenship Statute and should be fully implemented by the member countries in 2021, when the program will be transformed in an international treaty incorporated into the national legal system of the countries, under the concept of "Mercosur Citizen".

The concept of "Commonwealth Citizenship" has been in place ever since the establishment of the Commonwealth of Nations. As with the EU, one holds Commonwealth citizenship only by being a citizen of a Commonwealth member state. This form of citizenship offers certain privileges within some Commonwealth countries:

Although Ireland was excluded from the Commonwealth in 1949 because it declared itself a republic, Ireland is generally treated as if it were still a member. Legislation often specifically provides for equal treatment between Commonwealth countries and Ireland and refers to "Commonwealth countries and Ireland". Ireland's citizens are not classified as foreign nationals in the United Kingdom.

Canada departed from the principle of nationality being defined in terms of allegiance in 1921. In 1935 the Irish Free State was the first to introduce its own citizenship. However, Irish citizens were still treated as subjects of the Crown, and they are still not regarded as foreign, even though Ireland is not a member of the Commonwealth. The Canadian Citizenship Act of 1947 provided for a distinct Canadian Citizenship, automatically conferred upon most individuals born in Canada, with some exceptions, and defined the conditions under which one could become a naturalized citizen. The concept of Commonwealth citizenship was introduced in 1948 in the British Nationality Act 1948. Other dominions adopted this principle such as New Zealand, by way of the British Nationality and New Zealand Citizenship Act of 1948.

Citizenship most usually relates to membership of the nation state, but the term can also apply at the subnational level. Subnational entities may impose requirements, of residency or otherwise, which permit citizens to participate in the political life of that entity, or to enjoy benefits provided by the government of that entity. But in such cases, those eligible are also sometimes seen as "citizens" of the relevant state, province, or region. An example of this is how the fundamental basis of Swiss citizenship is citizenship of an individual commune, from which follows citizenship of a canton and of the Confederation. Another example is Åland where the residents enjoy a special provincial citizenship within Finland, "hembygdsrätt".

The United States has a federal system in which a person is a citizen of their specific state of residence, such as New Jersey or California, as well as a citizen of the United States. State constitutions may grant certain rights above and beyond what are granted under the United States Constitution and may impose their own obligations including the sovereign right of taxation and military service; each state maintains at least one military force subject to national militia transfer service, the state's national guard, and some states maintain a second military force not subject to nationalization.

"Active citizenship" is the philosophy that citizens should work towards the betterment of their community through economic participation, public, volunteer work, and other such efforts to improve life for all citizens. In this vein, citizenship education is taught in schools, as an academic subject in some countries. By the time children reach secondary education there is an emphasis on such unconventional subjects to be included in academic curriculum. While the diagram on citizenship to the right is rather facile and depth-less, it is simplified to explain the general model of citizenship that is taught to many secondary school pupils. The idea behind this model within education is to instill in young pupils that their actions (i.e. their vote) affect collective citizenship and thus in turn them.

It is taught in the Republic of Ireland as an exam subject for the Junior Certificate. It is known as Civic, Social and Political Education (CSPE). A new Leaving Certificate exam subject with the working title 'Politics & Society' is being developed by the National Council for Curriculum and Assessment (NCCA) and is expected to be introduced to the curriculum sometime after 2012.

Citizenship is offered as a General Certificate of Secondary Education (GCSE) course in many schools in the United Kingdom. As well as teaching knowledge about democracy, parliament, government, the justice system, human rights and the UK's relations with the wider world, students participate in active citizenship, often involving a social action or social enterprise in their local community.

There are two kinds of criticism of citizenship education in schools. Firstly, some philosophers of education argue that most governments and mainstream policies stimulate and advocate questionable approaches of citizenship education. These approaches aim to develop specific dispositions in students, dispositions conducive to political participation and solidarity. But there are radically different views on the nature of good citizenship and education should involve and develop autonomy and open-mindedness. Therefore, it requires a more critical approach than is possible when political participation and solidarity are conceived of as goals of education. Secondly, some educationalists argue that merely teaching children about the theory of citizenship is ineffective, unless schools themselves reflect democratic practices by giving children the opportunity to have a say in decision making. They suggest that schools are fundamentally undemocratic institutions, and that such a setting cannot instill in children the commitment and belief in democratic values that is necessary for citizenship education to have a proper impact. Some educationalists relate this criticism to John Dewey (see critical comments on this interpretation of Dewey: Van der Ploeg, 2016).






</doc>
<doc id="598010" url="https://en.wikipedia.org/wiki?curid=598010" title="Provisional government">
Provisional government

A provisional government, also called an interim government or transitional government, is an emergency governmental authority set up to manage a political transition generally in the cases of new nations or following the collapse of the previous governing administration. Provisional governments are generally appointed, and frequently arise, either during or after civil or foreign wars.

Provisional governments maintain power until a new government can be appointed by a regular political process, which is generally an election. They may be involved with defining the legal structure of subsequent regimes, guidelines related to human rights and political freedoms, the structure of the economy, government institutions, and international alignment. Provisional governments differ from caretaker governments, which are responsible for governing within an established parliamentary system and serve as placeholders following a motion of no confidence, or following the dissolution of the ruling coalition.

In opinion of Yossi Shain and Juan J. Linz, provisional governments can be classified to four groups:


The establishment of provisional governments is frequently tied to the implementation of transitional justice. Decisions related to transitional justice can determine who is allowed to participate in a provisional government.

The early provisional governments were created to prepare for the return of royal rule. Irregularly convened assemblies during the English Revolution, such as Confederate Ireland (1641–49), were described as "provisional". The Continental Congress, a convention of delegates from 13 British colonies on the east coast of North America became the provisional government of the United States in 1776, during the American Revolutionary War. The government shed its provisional status in 1781, following ratification of the Articles of Confederation, and continued until it was supplanted by the United States Congress in 1789.

The practice of using "provisional government" as part of a formal name can be traced to Talleyrand's government in France in 1814. In 1843, American pioneers in the Oregon Country, in the Pacific Northwest region of North America established the Provisional Government of Oregon—as the U.S. federal government had not yet extended its jurisdiction over the region—which existed until March 1849. The numerous provisional governments during the Revolutions of 1848 gave the word its modern meaning: A liberal government established to prepare for elections.

Numerous provisional governments have been established since the 1850s, including:
Provisional governments were also established throughout Europe as occupied nations were liberated from Nazi occupation by the Allies.



</doc>
<doc id="20949522" url="https://en.wikipedia.org/wiki?curid=20949522" title="Bureaucracy">
Bureaucracy

Bureaucracy () refers to both a body of non-elected government officials and an administrative policy-making group. Historically, a bureaucracy was a government administration managed by departments staffed with non-elected officials. Today, bureaucracy is the administrative system governing any large institution, whether publicly owned or privately owned. The public administration in many countries is an example of a bureaucracy, but so is the centralized hierarchical structure of a business firm.

Various commentators have noted the necessity of bureaucracies in modern society. The German sociologist Max Weber argued that bureaucracy constitutes the most efficient and rational way in which human activity can be organized and that systematic processes and organized hierarchies are necessary to maintain order, maximize efficiency, and eliminate favoritism. On the other hand, Weber also saw unfettered bureaucracy as a threat to individual freedom, with the potential of trapping individuals in an impersonal "iron cage" of rule-based, rational control.

The term "bureaucracy" originated in the French language: it combines the French word "bureau" – desk or office – with the Greek word κράτος ("kratos") – rule or political power. The French economist Jacques Claude Marie Vincent de Gournay (1712-1759) coined the word in the mid-18th century. Gournay never wrote the term down but a letter from a contemporary later quoted him:
The first known English-language use dates to 1818 with Irish novelist Lady Morgan referring to the apparatus used by the British to subjugate their Irish colony as "the Bureaucratie, or office tyranny, by which Ireland has so long been governed." By the mid-19th century the word appeared in a more neutral sense, referring to a system of public administration in which offices were held by unelected career officials. In this context "bureaucracy" was seen as a distinct form of management, often subservient to a monarchy. In the 1920s the German sociologist Max Weber expanded the definition to include any system of administration conducted by trained professionals according to fixed rules. Weber saw bureaucracy as a relatively positive development; however, by 1944 the Austrian economist Ludwig von Mises opined in the context of his experience in the Nazi regime that the term bureaucracy was "always applied with an opprobrious connotation," and by 1957 the American sociologist Robert Merton suggested that the term "bureaucrat" had become an "epithet, a "Schimpfwort"" in some circumstances. The word "bureaucracy" is also used in politics and government with a disapproving tone to disparage official rules that make it difficult to do things. In workplaces, the word is used very often to blame complicated rules, processes, and written work that make it hard to get something done. Socio-bureaucracy would then refer to certain social influences that may affect the function of a society.

Although the term "bureaucracy" first originated in the mid-18th century, organized and consistent administrative systems existed much earlier. The development of writing ( 3500 BC) and the use of documents was critical to the administration of this system, and the first definitive emergence of bureaucracy occurred in ancient Sumer, where an emergent class of scribes used clay tablets to administer the harvest and to allocate its spoils. Ancient Egypt also had a hereditary class of scribes that administered the civil-service bureaucracy.

A hierarchy of regional proconsuls and their deputies administered the Roman Empire. The reforms of Diocletian (Emperor from 284 to 305) doubled the number of administrative districts and led to a large-scale expansion of Roman bureaucracy. The early Christian author Lactantius ( 250 – 325) claimed that Diocletian's reforms led to widespread economic stagnation, since "the provinces were divided into minute portions, and many presidents and a multitude of inferior officers lay heavy on each territory." After the Empire split, the Byzantine Empire developed a notoriously complicated administrative hierarchy, and in the 20th century the term "Byzantine" came to refer to any complex bureaucratic structure.

In China, when the Qin dynasty (221–206 BC) unified China under the Legalist system, the emperor assigned administration to dedicated officials rather than nobility, ending feudalism in China, replacing it with a centralized, bureaucratic government. The form of government created by the first emperor and his advisors was used by later dynasties to structure their own government. Under this system, the government thrived, as talented individuals could be more easily identified in the transformed society. The Han dynasty (202 BC - 220 AD) established a complicated bureaucracy based on the teachings of Confucius, who emphasized the importance of ritual in a family, in relationships, and in politics. With each subsequent dynasty, the bureaucracy evolved. In 165 BC, Emperor Wen introduced the first method of recruitment to civil service through examinations, while
Emperor Wu (r. 141–87 BC), cemented the ideology of Confucius into mainstream governance installed a system of recommendation and nomination in government service known as "xiaolian", and a national academy whereby officials would select candidates to take part in an examination of the Confucian classics, from which Emperor Wu would select officials. In the Sui dynasty (581–618) and the subsequent Tang dynasty (618–907) the "shi" class would begin to present itself by means of the fully standardized civil service examination system, of partial recruitment of those who passed standard exams and earned an official degree. Yet recruitment by recommendations to office was still prominent in both dynasties. It was not until the Song dynasty (960–1279) that the recruitment of those who passed the exams and earned degrees was given greater emphasis and significantly expanded. During the Song dynasty (960–1279) the bureaucracy became meritocratic. Following the Song reforms, competitive examinations took place to determine which candidates qualified to hold given positions.
The imperial examination system lasted until 1905, six years before the Qing dynasty collapsed, marking the end of China's traditional bureaucratic system.

Instead of the inefficient and often corrupt system of tax farming that prevailed in absolutist states such as France, the Exchequer was able to exert control over the entire system of tax revenue and government expenditure. By the late 18th century, the ratio of fiscal bureaucracy to population in Britain was approximately 1 in 1300, almost four times larger than the second most heavily bureaucratized nation, France. Thomas Taylor Meadows, Britain's consul in Guangzhou, argued in his "Desultory Notes on the Government and People of China" (1847) that "the long duration of the Chinese empire is solely and altogether owing to the good government which consists in the advancement of men of talent and merit only," and that the British must reform their civil service by making the institution meritocratic. Influenced by the ancient Chinese imperial examination, the Northcote–Trevelyan Report of 1854 recommended that recruitment should be on the basis of merit determined through competitive examination, candidates should have a solid general education to enable inter-departmental transfers, and promotion should be through achievement rather than "preferment, patronage, or purchase". This led to implementation of Her Majesty's Civil Service as a systematic, meritocratic civil service bureaucracy.

In the British civil service, just as it was in China, entrance to the civil service was usually based on a general education in ancient classics, which similarly gave bureaucrats greater prestige. The Cambridge-Oxford ideal of the civil service was identical to the Confucian ideal of a general education in world affairs through humanism. (Well into the 20th century, Classics, Literature, History and Language remained heavily favoured in British civil service examinations. In the period of 1925–1935, 67 percent of British civil service entrants consisted of such graduates.) Like the Chinese model's consideration of personal values, the British model also took personal physique and character into account.

Like the British, the development of French bureaucracy was influenced by the Chinese system. Under Louis XIV of France, the old nobility had neither power nor political influence, their only privilege being exemption from taxes. The dissatisfied noblemen complained about this "unnatural" state of affairs, and discovered similarities between absolute monarchy and bureaucratic despotism. With the translation of Confucian texts during the Enlightenment, the concept of a meritocracy reached intellectuals in the West, who saw it as an alternative to the traditional "ancien regime" of Europe. Western perception of China even in the 18th century admired the Chinese bureaucratic system as favourable over European governments for its seeming meritocracy; Voltaire claimed that the Chinese had "perfected moral science" and François Quesnay advocated an economic and political system modeled after that of the Chinese.
The governments of China, Egypt, Peru and Empress Catherine II were regarded as models of Enlightened Despotism, admired by such figures as Diderot, D'Alembert and Voltaire.

Napoleonic France adopted this meritocracy system and soon saw a rapid and dramatic expansion of government, accompanied by the rise of the French civil service and its complex systems of bureaucracy. This phenomenon became known as "bureaumania". In the early 19th century, Napoleon attempted to reform the bureaucracies of France and other territories under his control by the imposition of the standardized Napoleonic Code. But paradoxically, that led to even further growth of the bureaucracy.

French civil service examinations adopted in the late 19th century were also heavily based on general cultural studies. These features have been likened to the earlier Chinese model.

By the mid-19th century, bureaucratic forms of administration were firmly in place across the industrialized world. Thinkers like John Stuart Mill and Karl Marx began to theorize about the economic functions and power-structures of bureaucracy in contemporary life. Max Weber was the first to endorse bureaucracy as a necessary feature of modernity, and by the late 19th century bureaucratic forms had begun their spread from government to other large-scale institutions.

The trend toward increased bureaucratization continued in the 20th century, with the public sector employing over 5% of the workforce in many Western countries. Within capitalist systems, informal bureaucratic structures began to appear in the form of corporate power hierarchies, as detailed in mid-century works like "The Organization Man" and "The Man in the Gray Flannel Suit". Meanwhile, in the Soviet Union and Eastern Bloc nations, a powerful class of bureaucratic administrators termed "nomenklatura" governed nearly all aspects of public life.

The 1980s brought a backlash against perceptions of "big government" and the associated bureaucracy. Politicians like Margaret Thatcher and Ronald Reagan gained power by promising to eliminate government regulatory bureaucracies, which they saw as overbearing, and return economic production to a more purely capitalistic mode, which they saw as more efficient. In the business world, managers like Jack Welch gained fortune and renown by eliminating bureaucratic structures inside corporations. Still, in the modern world, most organized institutions rely on bureaucratic systems to manage information, process records, and administer complex systems, although the decline of paperwork and the widespread use of electronic databases is transforming the way bureaucracies function.

Karl Marx theorized about the role and function of bureaucracy in his "Critique of Hegel's Philosophy of Right", published in 1843. In "Philosophy of Right", Hegel had supported the role of specialized officials in public administration, although he never used the term "bureaucracy" himself. By contrast, Marx was opposed to bureaucracy. Marx posited that while corporate and government bureaucracy seem to operate in opposition, in actuality they mutually rely on one another to exist. He wrote that "The Corporation is civil society's attempt to become state; but the bureaucracy is the state which has really made itself into civil society."

Writing in the early 1860s, political scientist John Stuart Mill theorized that successful monarchies were essentially bureaucracies, and found evidence of their existence in Imperial China, the Russian Empire, and the regimes of Europe. Mill referred to bureaucracy as a distinct form of government, separate from representative democracy. He believed bureaucracies had certain advantages, most importantly the accumulation of experience in those who actually conduct the affairs. Nevertheless, he believed this form of governance compared poorly to representative government, as it relied on appointment rather than direct election. Mill wrote that ultimately the bureaucracy stifles the mind, and that "a bureaucracy always tends to become a pedantocracy."

The German sociologist Max Weber was the first to formally study bureaucracy and his works led to the popularization of this term. In his essay "Bureaucracy", published in his magnum opus "Economy and Society", Weber described many ideal-typical forms of public administration, government, and business. His ideal-typical bureaucracy, whether public or private, is characterized by:

Weber listed several preconditions for the emergence of bureaucracy, including an increase in the amount of space and population being administered, an increase in the complexity of the administrative tasks being carried out, and the existence of a monetary economy requiring a more efficient administrative system. Development of communication and transportation technologies make more efficient administration possible, and democratization and rationalization of culture results in demands for equal treatment.

Although he was not necessarily an admirer of bureaucracy, Weber saw bureaucratization as the most efficient and rational way of organizing human activity and therefore as the key to rational-legal authority, indispensable to the modern world. Furthermore, he saw it as the key process in the ongoing rationalization of Western society. Weber also saw bureaucracy, however, as a threat to individual freedoms, and the ongoing bureaucratization as leading to a "polar night of icy darkness", in which increasing rationalization of human life traps individuals in a soulless "iron cage" of bureaucratic, rule-based, rational control. Weber's critical study of the bureaucratization of society became one of the most enduring parts of his work. Many aspects of modern public administration are based on his work, and a classic, hierarchically organized civil service of the Continental type is called "Weberian civil service".

Writing as an academic while a professor at Bryn Mawr College, Woodrow Wilson's essay "The Study of Administration" argued for bureaucracy as a professional cadre, devoid of allegiance to fleeting politics. Wilson advocated a bureaucracy that "is a part of political life only as the methods of the counting house are a part of the life of society; only as machinery is part of the manufactured product. But it is, at the same time, raised very far above the dull level of mere technical detail by the fact that through its greater principles it is directly connected with the lasting maxims of political wisdom, the permanent truths of political progress."

Wilson did not advocate a replacement of rule by the governed, he simply advised that, "Administrative questions are not political questions. Although politics sets the tasks for administration, it should not be suffered to manipulate its offices". This essay became a foundation for the study of public administration in America.

In his 1944 work "Bureaucracy", the Austrian economist Ludwig von Mises compared bureaucratic management to profit management. Profit management, he argued, is the most effective method of organization when the services rendered may be checked by economic calculation of profit and loss. When, however, the service in question can not be subjected to economic calculation, bureaucratic management is necessary. He did not oppose universally bureaucratic management; on the contrary, he argued that bureaucracy is an indispensable method for social organization, for it is the only method by which the law can be made supreme, and is the protector of the individual against despotic arbitrariness. Using the example of the Catholic Church, he pointed out that bureaucracy is only appropriate for an organization whose code of conduct is not subject to change. He then went on to argue that complaints about bureaucratization usually refer not to the criticism of the bureaucratic methods themselves, but to "the intrusion of bureaucracy into all spheres of human life." Mises saw bureaucratic processes at work in both the private and public spheres; however, he believed that bureaucratization in the private sphere could only occur as a consequence of government interference. According to him, "What must be realized is only that the strait jacket of bureaucratic organization paralyzes the individual's initiative, while within the capitalist market society an innovator still has a chance to succeed. The former makes for stagnation and preservation of inveterate methods, the latter makes for progress and improvement."

American sociologist Robert K. Merton expanded on Weber's theories of bureaucracy in his work "Social Theory and Social Structure", published in 1957. While Merton agreed with certain aspects of Weber's analysis, he also noted the dysfunctional aspects of bureaucracy, which he attributed to a "trained incapacity" resulting from "over conformity". He believed that bureaucrats are more likely to defend their own entrenched interests than to act to benefit the organization as a whole but that pride in their craft makes them resistant to changes in established routines. Merton stated that bureaucrats emphasize formality over interpersonal relationships, and have been trained to ignore the special circumstances of particular cases, causing them to come across as "arrogant" and "haughty".

In his book "A General Theory of Bureaucracy", first published in 1976, Dr. Elliott Jaques describes the discovery of a universal and uniform underlying structure of managerial or work levels in the bureaucratic hierarchy for any type of employment systems.

Elliott Jaques argues and presents evidence that for the bureaucracy to provide a valuable contribution to the open society some of the following conditions must be met:
The definition of effective bureaucratic hierarchy by Elliott Jaques is of importance not only to sociology but to social psychology, social anthropology, economics, politics, and social philosophy. They also have a practical application in business and administrative studies.




</doc>
<doc id="59539440" url="https://en.wikipedia.org/wiki?curid=59539440" title="Artificial intelligence in government">
Artificial intelligence in government

Artificial intelligence (AI) has a range of uses in government. It can be used to further public policy objectives (in areas such as emergency services, health and welfare), as well as assist the public to interact with the government (through the use of virtual assistants, for example). According to the Harvard Business Review, "Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world." Hila Mehr from the Ash Center for Democratic Governance and Innovation at Harvard University notes that AI in government is not new, with postal services using machine methods in the late 1990s to recognise handwriting on envelopes to automatically route letters. The use of AI in government comes with significant benefits, including efficiencies resulting in cost savings, for instance by reducing the number of front office staff, and reducing the opportunities for corruption, but it also carries risks.

The potential uses of AI in government are wide and varied, with Deloitte considering that "Cognitive technologies could eventually revolutionize every facet of government operations". Mehr suggests that six types of government problems are appropriate for AI applications:


Meher states that "While applications of AI in government work have not kept pace with the rapid expansion of AI in the private sector, the potential use cases in the public sector mirror common applications in the private sector."

Potential and actual uses of AI in government can be divided into three broad categories: those that contribute to public policy objectives; those that assist public interactions with the government; and other uses.

There are a range of examples of where AI can contribute to public policy objectives. These include:


AI can be used to assist members of the public to interact with government and access government services, for example by:


Examples of virtual assistants or chatbots being used by government include the following:


Other uses of AI in government include:


AI offers potential efficiencies and costs savings for the government. For example, Deloitte has estimated that automation could save US Government employees between 96.7 million to 1.2 billion hours a year, resulting in potential savings of between $3.3 billion to $41.1 billion a year. The Harvard Business Review has stated that while this may lead a government to reduce employee numbers, "Governments could instead choose to invest in the quality of its services. They can re-employ workers’ time towards more rewarding work that requires lateral thinking, empathy, and creativity — all things at which humans continue to outperform even the most sophisticated AI program."

Potential risks associated with the use of AI in government include AI becoming susceptible to bias, a lack of transparency in how an AI application may make decisions, and the accountability for any such decisions.



</doc>
<doc id="12229" url="https://en.wikipedia.org/wiki?curid=12229" title="Government">
Government

A government is the system or group of people governing an organized community, often a state.

In the case of its broad associative definition, government normally consists of legislature, executive, and judiciary. Government is a means by which organizational policies are enforced, as well as a mechanism for determining policy. Each government has a kind of constitution, a statement of its governing principles and philosophy. Typically the philosophy chosen is some balance between the principle of individual freedom and the idea of absolute state authority (tyranny).

While all types of organizations have governance, the word "government" is often used more specifically to refer to the approximately 200 independent national governments on Earth, as well as subsidiary organizations.

Historically prevalent forms of government include monarchy, aristocracy, timocracy, oligarchy, democracy, theocracy and tyranny. The main aspect of any philosophy of government is how political power is obtained, with the two main forms being electoral contest and hereditary succession.

Libertarianism and anarchism are political ideologies that seek to limit or abolish government, finding government disruptive to self organization and freedom.

A government is the system to govern a state or community.

The word "government" derives, ultimately, from the Greek verb κυβερνάω ["kubernáo"] (meaning "to steer" with gubernaculum (rudder), the metaphorical sense being attested in Plato's Ship of State).

The Columbia Encyclopedia defines government as "a system of social control under which the right to make laws, and the right to enforce them, is vested in a particular group in society".

While all types of organizations have governance, the word "government" is often used more specifically to refer to the approximately 200 independent national governments on Earth, as well as their subsidiary organizations.

Finally, "government" is also sometimes used in English as a synonym for governance.

The moment and place that the phenomenon of human government developed is lost in time; however, history does record the formations of early governments. About 5,000 years ago, the first small city-states appeared. By the third to second millenniums BC, some of these had developed into larger governed areas: Sumer, Ancient Egypt, the Indus Valley Civilization, and the Yellow River Civilization.

The development of agriculture and water control projects were a catalyst for the development of governments. On occasion a chief of a tribe was elected by various rituals or tests of strength to govern his tribe, sometimes with a group of elder tribesmen as a council. The human ability to precisely communicate abstract, learned information allowed humans to become ever more effective at agriculture, and that allowed for ever increasing population densities. David Christian explains how this resulted in states with laws and governments:

Starting at the end of the 17th century, the prevalence of republican forms of government grew. The Glorious Revolution in England, the American Revolution, and the French Revolution contributed to the growth of representative forms of government. The Soviet Union was the first large country to have a Communist government. Since the fall of the Berlin Wall, liberal democracy has become an even more prevalent form of government.

In the nineteenth and twentieth century, there was a significant increase in the size and scale of government at the national level. This included the regulation of corporations and the development of the welfare state.

In political science, it has long been a goal to create a typology or taxonomy of polities, as typologies of political systems are not obvious. It is especially important in the political science fields of comparative politics and international relations. Like all categories discerned within forms of government, the boundaries of government classifications are either fluid or ill-defined.

Superficially, all governments have an official or ideal form. The United States is a constitutional republic, while the former Soviet Union was a socialist republic. However self-identification is not objective, and as Kopstein and Lichbach argue, defining regimes can be tricky. For example, Voltaire argued that "the Holy Roman Empire is neither Holy, nor Roman, nor an Empire".

Identifying a form of government is also difficult because many political systems originate as socio-economic movements and are then carried into governments by parties naming themselves after those movements; all with competing political-ideologies. Experience with those movements in power, and the strong ties they may have to particular forms of government, can cause them to be considered as forms of government in themselves.

Other complications include general non-consensus or deliberate "distortion or bias" of reasonable technical definitions to political ideologies and associated forms of governing, due to the nature of politics in the modern era. For example: The meaning of "conservatism" in the United States has little in common with the way the word's definition is used elsewhere. As Ribuffo notes, "what Americans now call conservatism much of the world calls liberalism or neoliberalism"; a "conservative" in Finland would be labeled a "socialist" in the United States. Since the 1950s conservatism in the United States has been chiefly associated with the Republican Party. However, during the era of segregation many Southern Democrats were conservatives, and they played a key role in the Conservative Coalition that controlled Congress from 1937 to 1963.

Opinions vary by individuals concerning the types and properties of governments that exist. "Shades of gray" are commonplace in any government and its corresponding classification. Even the most liberal democracies limit rival political activity to one extent or another while the most tyrannical dictatorships must organize a broad base of support thereby creating difficulties for "pigeonholing" governments into narrow categories. Examples include the claims of the United States as being a plutocracy rather than a democracy since some American voters believe elections are being manipulated by wealthy Super PACs.

The Classical Greek philosopher Plato discusses five types of regimes: aristocracy, timocracy, oligarchy, democracy and tyranny. Plato also assigns a man to each of these regimes to illustrate what they stand for. The tyrannical man would represent tyranny for example. These five regimes progressively degenerate starting with aristocracy at the top and tyranny at the bottom.

One method of classifying governments is through which people have the authority to rule. This can either be one person (an autocracy, such as monarchy), a select group of people (an aristocracy), or the people as a whole (a democracy, such as a republic).

Thomas Hobbes stated on their classification:

An autocracy is a system of government in which supreme power is concentrated in the hands of one person, whose decisions are subject to neither external legal restraints nor regularized mechanisms of popular control (except perhaps for the implicit threat of a coup d'état or mass insurrection).

Aristocracy (Greek ἀριστοκρατία "aristokratía", from ἄριστος "" "excellent", and κράτος "" "power") is a form of government that places power in the hands of a small, privileged ruling class.

Many monarchies were aristocracies, although in modern constitutional monarchies the monarch himself or herself has little real power. The term "Aristocracy" could also refer to the non-peasant, non-servant, and non-city classes in the Feudal system.

Democracy is a system of government where the citizens exercise power by voting. In a direct democracy, the citizens as a whole form a governing body and vote directly on each issue. In a representative democracy the citizens elect representatives from among themselves. These representatives meet to form a governing body, such as a legislature. In a constitutional democracy the powers of the majority are exercised within the framework of a representative democracy, but the constitution limits the majority and protects the minority, usually through the enjoyment by all of certain individual rights, e.g. freedom of speech, or freedom of association.

A republic is a form of government in which the country is considered a "public matter" (Latin: "res publica"), not the private concern or property of the rulers, and where offices of states are subsequently directly or indirectly elected or appointed rather than inherited. The people, or some significant portion of them, have supreme control over the government and where offices of state are elected or chosen by elected people. A common simplified definition of a republic is a government where the head of state is not a monarch. Montesquieu included both democracies, where all the people have a share in rule, and aristocracies or oligarchies, where only some of the people rule, as republican forms of government.

Other terms used to describe different republics include Democratic republic, Parliamentary republic, Federal republic, and Islamic Republic.

Federalism is a political concept in which a "group" of members are bound together by covenant with a governing representative head. The term "federalism" is also used to describe a system of government in which sovereignty is constitutionally divided between a central governing authority and constituent political units, variously called states, provinces or otherwise. Federalism is a system based upon democratic principles and institutions in which the power to govern is shared between national and provincial/state governments, creating what is often called a federation. Proponents are often called federalists.

Historically, most political systems originated as socioeconomic ideologies. Experience with those movements in power and the strong ties they may have to particular forms of government can cause them to be considered as forms of government in themselves.

Certain major characteristics are defining of certain types; others are historically associated with certain types of government.

This list focuses on differing approaches that political systems take to the distribution of sovereignty, and the autonomy of regions within the state.





</doc>
<doc id="49214523" url="https://en.wikipedia.org/wiki?curid=49214523" title="Outline of government">
Outline of government

The following outline is provided as an overview of and topical guide to government:

Government – 

Government - is a general term which can be used to refer to public bodies organizing the political life of the society. Government can also refer to the collective head of the executive branch of power in a polity.

Public policies -
Legislative power -

Executive power -

Judicial power -

Constitution -

Five characteristics of a state

Evolutionary Theory -

Social Contract Theory -

Divine Theory -

Meritocracy -

Form a More Perfect Union -

Establish Justice -

Insure Domestic Tranquility -

Provide for the Common Defense -

Promote the General Welfare -

Secure the Blessings of Liberty -

History of government

Ordered government

Limited government

Representative government

Magna Carta

Petition of Right

English Bill of Rights

Charter

Royal Colonies - New Hampshire, Massachusetts, New York, New Jersey, Virginia, North Carolina, South Carolina, and Georgia
Proprietary colonies - Maryland, Pennsylvania, Delaware
Charter colonies - Connecticut and Rhode Island

New England Confederation

Albany Plan of Union

Delegate

Boycott

Repeal

Popular sovereignty

Declaration of Independence

Articles of Confederation

Ratification

Presiding Officer

Framers of the Constitution -

Virginia Plan -

New Jersey Plan -

Connecticut Compromise -

Three-Fifths Compromise -

Slave Trade Compromise -

Federalists -

Anti-Federalists -

Quorum -

Democracy -

Dictatorship -

Unitary government -

Federal government -

Confederate government (Confederation) -

Presidential government -

Parliamentary government -

Popular sovereignty 
Limited government
Human equality

Free enterprise system -

Law of supply and demand -

Mixed economy -

Preamble

Articles

Popular Sovereignty

Limited Government
Separation of powers

Checks and balances
Judicial review
Federalism

Chambers

Parliament
Parliamentary procedure
Types

Legislator -

Committee member -

Trustee -

Delegate -

Partisan -

Politico -

Senator -

Money

Money


</doc>
<doc id="250522" url="https://en.wikipedia.org/wiki?curid=250522" title="Regime">
Regime

In politics, a regime (also known as "régime", from the original French spelling) is the form of government or the set of rules, cultural or social norms, etc. that regulate the operation of a government or institution and its interactions with society.

While the word "régime" originates as a synonym for any type of government, modern usage has given it a negative connotation, implying an authoritarian government or dictatorship. Webster's definition states that the word "régime" refers simply to a form of government, while Oxford English Dictionary defines "regime" as "a government, especially an authoritarian one". 

Contemporary academic usage of the term "regime" is broader than popular and journalistic usage, meaning "an intermediate stratum between the government (which makes day-to-day decisions and is easy to alter) and the state (which is a complex bureaucracy tasked with a range of coercive functions)." In global studies and international relations the concept of "regime" is also used to name international regulatory agencies (see International regime), which lie outside of the control of national governments. Some authors thus distinguish analytically between institutions and regimes while recognizing that they are bound up with each other:

In other words, regimes can be defined as sets of protocols and norms embedded either in institutions or institutionalized practices – formal such as states or informal such as the "liberal trade regime" – that are publicly enacted and relatively enduring.



</doc>
<doc id="16554664" url="https://en.wikipedia.org/wiki?curid=16554664" title="Living systems">
Living systems

Living systems are open self-organizing life forms that interact with their environment. These systems are maintained by flows of information, energy and matter.

Some scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.

Living systems theory is a general theory about the existence of all living systems, their structure, interaction, behavior and development. This work is created by James Grier Miller, which was intended to formalize the concept of life. According to Miller's original conception as spelled out in his magnum opus "Living Systems", a "living system" must contain each of twenty "critical subsystems", which are defined by their functions and visible in numerous systems, from simple cells to organisms, countries, and societies. In "Living Systems" Miller provides a detailed look at a number of systems in order of increasing size, and identifies his subsystems in each. 
Miller considers living systems as a subset of all systems. Below the level of living systems, he defines space and time, matter and energy, information and entropy, levels of organization, and physical and conceptual factors, and above living systems ecological, planetary and solar systems, galaxies, etc.

Living systems according to Parent (1996) are by definition "open self-organizing systems that have the special characteristics of life and interact with their environment. This takes place by means of information and material-energy exchanges. Living systems can be as simple as a single cell or as complex as a supranational organization such as the European Union. Regardless of their complexity, they each depend upon the same essential twenty subsystems (or processes) in order to survive and to continue the propagation of their species or types beyond a single generation".

Miller said that systems exist at eight "nested" hierarchical levels: cell, organ, organism, group, organization, community, society, and supranational system. At each level, a system invariably comprises twenty critical subsystems, which process matter–energy or information except for the first two, which process both matter–energy and information: reproducer and boundary.

The processors of matter–energy are: 

The processors of information are:

James Grier Miller in 1978 wrote a 1,102-page volume to present his living systems theory. He constructed a general theory of living systems by focusing on concrete systems—nonrandom accumulations of matter–energy in physical space–time organized into interacting, interrelated subsystems or components. Slightly revising the original model a dozen years later, he distinguished eight "nested" hierarchical levels in such complex structures. Each level is "nested" in the sense that each higher level contains the next lower level in a nested fashion.

His central thesis is that the systems in existence at all eight levels are open systems composed of twenty critical subsystems that process inputs, throughputs, and outputs of various forms of matter–energy and information. Two of these subsystems—reproducer and boundary—process both matter–energy and information. Eight of them process only matter–energy. The other ten process information only. 
All nature is a continuum. The endless complexity of life is organized into patterns which repeat themselves—theme and variations—at each level of system. These similarities and differences are proper concerns for science. From the ceaseless streaming of protoplasm to the many-vectored activities of supranational systems, there are continuous flows through living systems as they maintain their highly organized steady states.
Seppänen (1998) says that Miller applied general systems theory on a broad scale to describe all aspects of living systems.

Miller's theory posits that the mutual interrelationship of the components of a system extends across the hierarchical levels. Examples: Cells and organs of a living system thrive on the food the organism obtains from its suprasystem; the member countries of a supranational system reap the benefits accrued from the communal activities to which each one contributes. Miller says that his eclectic theory "ties together past discoveries from many disciplines and provides an outline into which new findings can be fitted".

Miller says the concepts of space, time, matter, energy, and information are essential to his theory because the living systems exist in space and are made of matter and energy organized by information. Miller's theory of living systems employs two sorts of spaces: physical or geographical space, and conceptual or abstracted spaces. Time is the fundamental "fourth dimension" of the physical space–time continuum/spiral. Matter is anything that has mass and occupies physical space. Mass and energy are equivalent as one can be converted into the other. Information refers to the degrees of freedom that exist in a given situation to choose among signals, symbols, messages, or patterns to be transmitted.

Other relevant concepts are system, structure, process, type, level, echelon, suprasystem, subsystem, transmissions, and steady state. A system can be conceptual, concrete or abstracted. The structure of a system is the arrangement of the subsystems and their components in three-dimensional space at any point of time. Process, which can be reversible or irreversible, refers to change over time of matter–energy or information in a system. Type defines living systems with similar characteristics. Level is the position in a hierarchy of systems. Many complex living systems, at various levels, are organized into two or more echelons. The suprasystem of any living system is the next higher system in which it is a subsystem or component. The totality of all the structures in a system which carry out a particular process is a subsystem. Transmissions are inputs and outputs in concrete systems. Because living systems are open systems, with continually altering fluxes of matter–energy and information, many of their equilibria are dynamic—situations identified as steady states or flux equilibria.

Miller identifies the comparable matter–energy and information processing critical subsystems. Elaborating on the eight hierarchical levels, he defines society, which constitutes the seventh hierarchy, as "a large, living, concrete system with [community] and lower am levels of living systems as subsystems and components". Society may include small, primitive, totipotential communities; ancient city–states, and kingdoms; as well as modern nation–states and empires that are not supranational systems. Miller provides general descriptions of each of the subsystems that fit all eight levels.

A supranational system, in Miller's view, "is composed of two or more societies, some or all of whose processes are under the control of a decider that is superordinate to their highest echelons". However, he contends that no supranational system with all its twenty subsystems under control of its decider exists today. The absence of a supranational decider precludes the existence of a concrete supranational system. Miller says that studying a supranational system is problematical because its subsystems
...tend to consist of few components besides the decoder. These systems do little matter-energy processing. The power of component societies [nations] today is almost always greater than the power of supranational deciders. Traditionally, theory at this level has been based upon intuition and study of history rather than data collection. Some quantitative research is now being done, and construction of global-system models and simulations is currently burgeoning.

At the supranational system level, Miller's emphasis is on international organizations, associations, and groups comprising representatives of societies (nation–states). Miller identifies the subsystems at this level to suit this emphasis. Thus, for example, the reproducer is "any multipurpose supranational system which creates a single purpose supranational organization" (p. 914); and the boundary is the "supranational forces, usually located on or near supranational borders, which defend, guard, or police them" (p. 914).

Not just those specialized in international communication, but all communication science scholars could pay particular attention to the major contributions of living systems theory (LST) to social systems approaches that Bailey has pointed out:

Bailey says that LST, perhaps the "most integrative" social systems theory, has made many more contributions that may be easily overlooked, such as: providing a detailed analysis of types of systems; making a distinction between concrete and abstracted systems; discussion of physical space and time; placing emphasis on information processing; providing an analysis of entropy; recognition of totipotential systems, and partipotential systems; providing an innovative approach to the structure–process issue; and introducing the concept of joint subsystem—a subsystem that belongs to two systems simultaneously; of dispersal—lateral, outward, upward, and downward; of inclusion—inclusion of something from the environment that is not part of the system; of artifact—an animal-made or human-made inclusion; of adjustment process, which combats stress in a system; and of critical subsystems, which carry out processes that all living systems need to survive.

LST's analysis of the twenty interacting subsystems, Bailey adds, clearly distinguishing between matter–energy-processing and information-processing, as well as LST's analysis of the eight interrelated system levels, enables us to understand how social systems are linked to biological systems. LST also analyzes the irregularities or "organizational pathologies" of systems functioning (e.g., system stress and strain, feedback irregularities, information–input overload). It explicates the role of entropy in social research while it equates negentropy with information and order. It emphasizes both structure and process, as well as their interrelations.

It omits the analysis of subjective phenomena, and it overemphasizes concrete Q-analysis (correlation of objects) to the virtual exclusion of R-analysis (correlation of variables). By asserting that societies (ranging from totipotential communities to nation-states and non-supranational systems) have greater control over their subsystem components than supranational systems have, it dodges the issue of transnational power over the contemporary social systems. Miller's supranational system bears no resemblance to the modern world-system that Immanuel Wallerstein (1974) described, although both of them were looking at the same living (dissipative) structure.





</doc>
<doc id="18789195" url="https://en.wikipedia.org/wiki?curid=18789195" title="The Seven Pillars of Life">
The Seven Pillars of Life

The Seven Pillars of Life are the essential principles of life described by Daniel E. Koshland in 2002 in order to create a universal definition of life. One stated goal of this universal definition is to aid in understanding and identifying artificial and extraterrestrial life. The seven pillars are Program, Improvisation, Compartmentalization, Energy, Regeneration, Adaptability, and Seclusion. These can be abbreviated as PICERAS.

Koshland defines "Program" as an "organized plan that describes both the ingredients themselves and the kinetics of the interactions among ingredients as the living system persists through time." In natural life as it is known on Earth, the program operates through the mechanisms of nucleic acids and amino acids, but the concept of program can apply to other imagined or undiscovered mechanisms.

"Improvisation" refers to the living system's ability to change its program in response to the larger environment in which it exists. An example of improvisation on earth is natural selection.

"Compartmentalization" refers to the separation of spaces in the living system that allow for separate environments for necessary chemical processes. Compartmentalization is necessary to protect the concentration of the ingredients for a reaction from outside environments.

Because living systems involve net movement in terms of chemical movement or body movement, and lose energy in those movements through entropy, energy is required for a living system to exist. The main source of energy on Earth is the sun, but other sources of energy exist for life on Earth, such as hydrogen gas or methane, used in chemosynthesis.

"Regeneration" in a living system refers to the general compensation for losses and degradation in the various components and processes in the system. This covers the thermodynamic loss in chemical reactions, the wear and tear of larger parts, and the larger decline of components of the system in ageing. Living systems replace these losses by importing molecules from the outside environment, synthesizing new molecules and components, or creating new generations to start the system over again.

"Adaptability" is the ability of a living system to respond to needs, dangers, or changes. It is distinguished from improvisation because the response is timely and does not involve a change of the program. Adaptability occurs from a molecular level to a behavioral level through feedback and feedforward systems. For example, an animal seeing a predator will respond to the danger with hormonal changes and escape behavior.

"Seclusion" is the separation of chemical pathways and the specificity of the effect of molecules, so that processes can function separately within the living system. In organisms on Earth, proteins aid in seclusion because of their individualized structure that are specific for their function, so that they can efficiently act without affecting separate functions.

Y. N. Zhuravlev and V. A. Avetisov have analyzed Koshland's seven pillars from the context of primordial life and, though calling the concept "elegant," point out that the pillars of compartmentalization, program, and seclusion don't apply well to the non-differentiated earliest life.



</doc>
<doc id="44262036" url="https://en.wikipedia.org/wiki?curid=44262036" title="Coleridge's theory of life">
Coleridge's theory of life

Romanticism grew largely out of an attempt to understand not just inert nature, but also vital nature. Romantic works in the realm of art and Romantic medicine were a response to the general failure of the application of method of inertial science to reveal the foundational laws and operant principles of vital nature. German romantic science and medicine sought to understand the nature of the life principle identified by John Hunter as distinct from matter itself via Johan Friedrich Blumenbach's "Bildungstrieb" and Romantic medicine's "Lebenskraft", as well as Röschlaub's development of the Brunonian system of medicine system of John Brown, in his "excitation theory" of life (German:"Erregbarkeit theorie"), working also with Schelling's "Naturphilosophie", the work of Goethe regarding morphology, and the first dynamic conception of physiology of Richard Saumarez. However, it is in Samuel Taylor Coleridge that we find the question of life and vital nature most intensely and comprehensively examined, particularly in his Hints towards the Formation of a more Comprehensive Theory of Life (1818), providing the foundation for Romantic philosophy, science and medicine. The work is key to understanding the relationship of Romantic literature and science.

The Enlightenment had developed a philosophy and science supported by formidable twin pillars: the first the Cartesian split of mind and matter, the second Newtonian physics, with its conquest of inert nature, both of which focused the mind's gaze on things or objects. For Cartesian philosophy, life existed on the side of matter, not mind; and for the physical sciences, the method that had been so productive for revealing the secrets of inert nature, should be equally productive in examining vital nature. The initial attempt to seek the cause and principle of life in matter was challenged by John Hunter, who held that the principle of life was not to be found nor confined within matter, but existed independently of matter itself, and informed or animated it, that is, he implied, it was the unifying or antecedent cause of the things or what Aristotelean philosophy termed "natura naturata" (the outer appearances of nature).
This reduction of the question of life to matter, and the corollary, that the method of the inertial sciences was the way to understand the very phenomenon of life, that is, its very nature and essence as a power ("natura naturans"), not as manifestations through sense-perceptible appearances ("natura naturata"), also reduced the individual to a material-mechanical 'thing' and seemed to render human freedom an untenable concept. It was this that Romanticism challenged, seeking instead to find an approach to the essence of nature as being also vital not simply inert, through a systematic method involving not just physics, but physiology (living functions). For Coleridge, quantitative analysis was anti-realist and needed to be grounded in qualitative analysis ('-ologies') (as was the case with Goethe's approach).
At the same time, the Romantics had to deal with the idealistic view that life was a 'somewhat' outside of things, such that the things themselves lost any real existence, a stream coming through Hume and Kant, and also infusing the German natural philosophical stream, German idealism, and in particular "naturphilosophie", eventuating scientifically in the doctrine of 'vitalism'. For the Romantics, life is independent of and antecedent to nature, but also infused and suspended in nature, not apart from it, As David Bohm expresses it in more modern terms "In nature nothing remains constant…everything comes from other things and gives rise to other things. This principle is…at the foundation of the possibility of our understanding nature in a rational way."

And as Coleridge explained, 'this antecedent unity, or cause and principle of each union, it has since the time of Bacon and Kepler been customary to call a law." And as law, "we derive from it a progressive insight into the necessity and generation of the phenomena of which it is the law."

Coleridge's was the dominant mind on many issues involving the philosophy and science in his time, as John Stuart Mill acknowledged, along with others since who have studied the history of Romanticism.

For Coleridge, as for many of his romantic contemporaries, the idea that matter itself can beget life only dealt with the various changes in the arrangement of particles and did not explain life itself as a principle or power that lay behind the material manifestations, "natura naturans" or "the productive power suspended and, as it were, quenched in the product" Until this were addressed, according to Coleridge, "we have not yet attained to a science of nature."

This productive power is above sense experience, but not above nature herself, that is, supersensible, but not supernatural, and, thus, not 'occult' as was the case with vitalism. Vitalism failed to distinguish between spirit and nature, and then within nature, between the visible appearances and the invisible, yet very real and not simply hypothesized notion, essence or motivating principle ("natura naturans"). Even Newton spoke of things invisible in themselves (though not in their manifestations), such as force, though Comte, the thorough materialist, complained of the use of such terms as the 'force of gravity' as being relics of animism.

Matter was not a 'datum' or thing in and of itself, but rather a product or effect, and for Coleridge, looking at life in its broadest sense, it was the product of a polarity of forces and energies, but derived from a unity which is itself a power, not an abstract or nominal concept, that is Life, and this polar nature of forces within the power of Life is the very law or 'Idea' (in the Platonic sense) of Creation.

For Coleridge the essence of the universe is motion and motion is driven by a dynamic polarity of forces that is both inherent in the world as potential and acting inherently in all manifestations. This polarity is the very dynamic that acts throughout all of nature, including into the more particular form of 'life biological', as well as of mind and consciousness.

And this polarity is dynamic, that is real, though not visible, and not simply logical or abstract. Thus, the polarity results in manifestations that are real, as the opposite powers are not contradictory, but counteracting and inter-penetrating.

Thus, then, Life itself is not a thing—a self-subsistent hypostasis—but an act and process...

And in that sense Coleridge re-phrases the question "What is Life?" to "What is not Life that really is?"

This dynamic polar essence of nature in all its functions and manifestations is a universal law in the order of the law of gravity and other physical laws of inert nature. And, critically, this dynamic polarity of constituent powers of life at all levels is not outside or above nature, but is within nature ("natura naturans"), not as a part of the visible product, but as the ulterior natural functions that produce such products or things.

It is these functions that provided the bridge being sought by Romantic science and medicine, in particular by Andreas Röschlaub and the Brunonian system of medicine, between the inertial science of inert nature (physics) and the vital science of vital nature (physiology) and its therapeutic application or physic (the domain of the physician).

Coleridge was influenced by German philosophy, in particular Kant, Fichte and Schelling (Naturphilosophie), as well as the physiology of Blumenbach and the dynamic excitation theory of life of the Brunonian system. He sought a path that was neither the mystical tendency of the earlier vitalists nor the materialistic reductionist approach to natural science, but a dynamic one.

Coleridge's challenge was to describe something that was dynamic neither in mystical terms not materialistic ones, but via analogy, drawing from the examples of inertial science. As one writer explains, he uses the examples of electricity, magnetism and gravity not because they are like life, but because they offer a way of understanding powers, forces and energies, which lie at the heart of life. And using these analogies, Coleridge seeks to demonstrate that life is not a material force, but a product of relations amongst forces. Life is not linear and static, but dynamic process of self-regulation and Emergent evolution that results in increasing complexity and individuation. This spiral, upward movement (cf. Goethe's ideas) creates a force for organization that unifies, and is most intense and powerful in that which is most complex and most individual - the self-regulating, enlightened, developed individual mind. But at the same time, this process of life increases interdependence (like the law of comparative advantage in economics) and associational powers of the mind. Thus, he is not talking about an isolated, individual subjective mind, but about the evolution of a higher level of consciousness and thought at the core of the process of life.

And the direction of this motion is towards increasing individuation, that is the creation of specific, individual units of things. At the same time, given the dynamic polarity of the world, there must always be an equal and opposite tendency, in this case, that of connection. So, a given of our experience is that man is both an individual, tending in each life and in history generally to greater and greater individualization, and a social creature seeking interaction and connection. It is the dynamic interplay between the individuation and connecting forces that leads to higher and higher individuation.

Coleridge makes a further distinction between mathematics and life, the latter being productive or creative, that is, living, and the former ideal. Thus, the mathematical approach that works so well with inert nature, is not suitable for vital nature.

The counteraction then of the two assumed forces does not depend on their meeting from opposite directions; the power which acts in them is indestructible; it is therefore inexhaustibly re-ebullient; and as something must be the result of these two forces, both alike infinite, and both alike indestructible; and as rest or neutralization cannot be this result; no other conception is possible, but that the product must be a tertium aliquid, [a third thing] or finite generation. Consequently this conception is necessary. Now this tertium aliquid can be no other than an inter-penetration of the counteracting powers, partaking of both… Consequently the 'constituent powers', that have given rise to a body, may then reappear in it as its function: "a Power, acting in and by its Product or Representative to a predetermined purpose is a Function...the first product of its energy is the thing itself: "ipsa se posuit et iam facta est ens positum". Still, however, its productive energy is not exhausted in this product, but overflows, or is effluent, as the specific forces, properties, faculties, of the product. It reappears, in short, as the function of the body...The vital functions are consequents of the "Vis Vitae Principium Vitale", and presuppose the Organs, as the Functionaries.

Life, that is, the essential polarity in unity (multeity in unity) in Coleridge’s sense also has a four beat cycle, different from the arid dialectics of abstraction - namely the tension of the polar forces themselves, the charge of their synthesis, the discharge of their product (indifference) and the resting state of this new form (predominance). The product is not a neutralization, but a new form of the essential forces, these forces remaining within, though now as the functions of the form.

To make it adequate, we must substitute the idea of positive production for that of rest, or mere neutralization. To the fancy alone it is the null-point, or zero, but to the reason it is the punctum saliens, and the power itself in its eminence.

This dynamic polarity that is Life is expressed at different levels. At its most basic it is Space-Time, with its product - motion. The interplay of both gives us either a line or a circle, and then there are different degrees possible within a given form or “predominance” of forces. Geometry is not conceivable except as the dynamic interplay of space (periphery) and time (point). Space, time and motion are also geometrically represented by width, length (breadth) and depth. And this correspondence is repeated throughout the scale of Life.

Matter, then, is the product of the dynamic forces - repulsion (centrifugal), and attraction (centripetal); it is not itself a productive power. It is also the mass of a given body.

Coleridge’s understanding of life is contrasted with the materialist view which is essentially reduced to defining life as that which is the opposite of not-life, or that which resists death, that is, that which is life.

The problem for Coleridge and the Romantics was that the intellect, 'left to itself' as Bacon stated, was capable of apprehending only the outer forms of nature (natura naturata) and not the inmost, living functions (natura naturans) giving rise to these forms. Thus, effects can only be 'explained' in terms of other effects, not causes. It takes a different capacity to 'see' these living functions, which is an imaginative activity. For Coleridge, there is an innate, primitive or 'primary' imagination that configures invisibly sense-experience into perception, but a rational perception, that is, one raised into consciousness and awareness and then rationally presentable, requires a higher level, what he termed 'secondary imagination', which is able to connect with the thing being experienced, penetrate to its essence in terms of the living dynamics upholding its outer form, and then present the phenomena as and within its natural law, and further, using reason, develop the various principles of its operation.

This cognitive capacity involved what Coleridge termed the 'inmost sense' or what Goethe termed the Gemüt. It also involved the reactivation of the old Greek noetic capacity, and the ability to 'see' or produce the theory (Greek "theoria" from the verb 'to see') dynamic polarities, or natural Laws, the dynamic transcendent (foundational) entities that Plato termed 'Ideas' ("eidos").

Since "natura naturata" is sustained by "natura naturans", and the creative power of "natura naturans" is one of a kind with the human mind, itself creative, then there must be a correspondence or connection between the mind and the things we perceive, such that we can overcome the apparent separation between the object and the representation in the mind of the object that came to bedevil Enlightenment thought (Hume, Kant). As one commentator noted "to speak at all of the unity of intelligence and nature is of course flatly to contradict Descartes."

For Coleridge the power of life lies in every seed as a potential to be unfolded as a result of interaction with the environment (heat, light, air, moisture, etc.), an insight which allowed him to see in the Brunonian system a dynamic polarity in excitation theory.
Coleridge also saw that there was a progressive movement through time and space of life or the law of polarity, from the level of physics (space and time) and the mineral or inert nature (law of gravity, operating through forces of attraction and repulsion), up to man, with his law of resonance in terms of his innate desire to be himself (force of individuation) and to also connect with like-minded (force of connection), as Goethe expressed in his novel "Elective Affinities" ("Wahlverwandschaften") as well as in his own life's experience.
Evolution occurred because the original polarity of creation, the very 'Law of Creation', itself gives birth to subsequent polarities, as each pole is itself a unity that can be further polarized (what Wilhelm Reich later termed 'orgonomic functionalism' and what at the biological level constitutes physiology), an insight that would later be taken up by the concept of emergent evolution, including the emergence of mind and consciousness.

And that this is so, is also an intimate and shared experience of all humans, as is set out in Reid's Common Sense philosophy. As Coleridge states

That nature evolves towards a purpose, and that is the unfolding of the human mind and consciousness in all its levels and degrees, is not teleological but a function of the very nature of the law of polarity or creation itself, namely that of increasing individuation of an original unity, what Coleridge termed 'multeity in unity'. As he states, "without assigning to nature as nature, a conscious purpose" we must still "distinguish her agency from a blind and lifeless mechanism."

While man contains and is subject to the various laws of nature, man as a self-conscious being is also the summa of a process of creation leading to greater mind and consciousness, that is, a creative capacity of imagination. Instead of being a creature of circumstance, man is the creator of them, or at least has that potential.



</doc>
<doc id="3447756" url="https://en.wikipedia.org/wiki?curid=3447756" title="Carbon-based life">
Carbon-based life

Carbon is a primary component of all known life on Earth, representing approximately 45–50% of all dry biomass. Carbon compounds occur naturally in great abundance on Earth. Complex biological molecules almost always consist of carbon atoms bonded with other elements, especially oxygen and hydrogen and frequently also nitrogen, phosphorus, and sulfur.

Because it is lightweight and relatively small in size, carbon molecules are easy for enzymes to manipulate. It is frequently assumed in astrobiology that if life exists elsewhere in the Universe, it will also be carbon-based. Critics refer to this assumption as carbon chauvinism.

Carbon is capable of forming a vast number of compounds, more than any other element, with almost ten million compounds described to date, and yet that number is but a fraction of the number of theoretically possible compounds under standard conditions. For this reason, carbon has often been referred to as the "king of the elements". The enormous diversity of carbon-containing compounds, known as organic compounds, has led to a distinction between them and compounds that do not contain carbon, known as inorganic compounds. The branch of chemistry that studies organic compounds is known as organic chemistry.

Carbon is the 15th most abundant element in the Earth's crust, and the fourth most abundant element in the universe by mass, after hydrogen, helium, and oxygen. Carbon's widespread abundance, its ability to form stable bonds with numerous other elements, and its unusual ability to form polymers at the temperatures commonly encountered on Earth enables it to serve as a common element of all known living organisms. In a 2018 study, carbon was found to compose approximately 550 billion tons of all life on Earth. It is the second most abundant element in the human body by mass (about 18.5%) after oxygen.

The most important characteristics of carbon as a basis for the chemistry of life are that each carbon atom is capable of forming up to four valence bonds with other atoms simultaneously, and that the energy required to make or break a bond with a carbon atom is at an appropriate level for building large and complex molecules which may be both stable and reactive. Carbon atoms bond readily to other carbon atoms; this allows the building of arbitrarily long macromolecules and polymers in a process known as catenation. "What we normally think of as 'life' is based on chains of carbon atoms, with a few other atoms, such as nitrogen or phosphorus", per Stephen Hawking in a 2008 lecture, "carbon [...] has the richest chemistry."

The most notable classes of biological macromolecules used in the fundamental processes of living organisms include:


There are not many other elements which appear to be promising candidates for supporting biological systems and processes as fundamentally as carbon does, for example, processes such as metabolism. The most frequently suggested alternative is silicon. Silicon shares a group in the periodic table with carbon, can also form four valence bonds, and also bonds to itself readily, though generally in the form of crystal lattices rather than long chains. Despite these similarities, silicon is considerably more electropositive than carbon, and silicon compounds do not readily recombine into different permutations in a manner that would plausibly support lifelike processes.

Speculations about the chemical structure and properties of hypothetical non-carbon-based life have been a recurring theme in science fiction. Silicon is often used as a substitute for carbon in fictional lifeforms because of its chemical similarities. In cinematic and literary science fiction, when man-made machines cross from non-living to living, this new form is often presented as an example of non-carbon-based life. Since the advent of the microprocessor in the late 1960s, such machines are often classed as "silicon-based life". Other examples of fictional "silicon-based life" can be seen in the episode "The Devil in the Dark" from "", in which a living rock creature's biochemistry is based on silicon, and in "The X-Files" episode "Firewalker", in which a silicon-based organism is discovered in a volcano.

In the film adaptation of Arthur C. Clarke's "2010" (1984), a character argues, "Whether we are based on carbon or on silicon makes no fundamental difference; we should each be treated with appropriate respect." This quote may be the basis of Steve Jobs' quip when he introduced Carbon within Mac OS X, "Carbon. All life forms will be based on it."




</doc>
<doc id="2169038" url="https://en.wikipedia.org/wiki?curid=2169038" title="Homochirality">
Homochirality

Homochirality is a uniformity of chirality, or handedness. Objects are chiral when they cannot be superposed on their mirror images. For example, the left and right hands of a human are approximately mirror images of each other but are not their own mirror images, so they are chiral. In biology, 19 of the 20 natural amino acids are homochiral, being -chiral (left-handed), while sugars are -chiral (right-handed). Homochirality can also refer to enantiopure substances in which all the constituents are the same enantiomer (a right-handed or left-handed version of an atom or molecule), but some sources discourage this use of the term.

It is unclear whether homochirality has a purpose, however, it appears to be a form of information storage. One suggestion is that it reduces entropy barriers in the formation of large organized molecules. It has been experimentally verified that amino acids form large aggregates in larger abundance from an enantiopure samples of the amino acid than from racemic (enantiomerically mixed) ones.

It is not clear whether homochirality emerged before or after life, and many mechanisms for its origin have been proposed. Some of these models propose three distinct steps: mirror-symmetry breaking creates a minute enantiomeric imbalance, chiral amplification builds on this imbalance, and chiral transmission is the transfer of chirality from one set of molecules to another.

Amino acids are the building blocks of peptides and enzymes while sugar-peptide chains are the backbone of RNA and DNA. In biological organisms, amino acids appear almost exclusively in the left-handed form (-amino acids) and sugars in the right-handed form (R-sugars). Since the enzymes catalyze reactions, they enforce homochirality on a great variety of other chemicals, including hormones, toxins, fragrances and food flavors. Glycine is achiral, as are some other non-proteinogenic amino acids are either achiral (such as dimethylglycine) or of the enantiomeric form.

Biological organisms easily discriminate between molecules with different chiralities. This can affect physiological reactions such as smell and taste. Carvone, a terpenoid found in essential oils, smells like mint in its L-form and caraway in its R-form. Limonene tastes like lemons when right-handed and oranges when left-handed.

Homochirality also affects the response to drugs. Thalidomide, in its left-handed form, cures morning sickness; in its right-handed form, it causes birth defects. Unfortunately, even if a pure left-handed version is administered, some of it can convert to the right-handed form in the patient. Many drugs are available as both a racemic mixture (equal amounts of both chiralities) and an enantiopure drug (only one chirality). Depending on the manufacturing process, enantiopure forms can be more expensive to produce than stereochemical mixtures.

Chiral preferences can also be found at a macroscopic level. Snail shells can be right-turning or left-turning helices, but one form or the other is strongly preferred in a given species. In the edible snail "Helix pomatia", only one out of 20,000 is left-helical. The coiling of plants can have a preferred chirality and even the chewing motion of cows has a 10% excess in one direction.

Known mechanisms for the production of non-racemic mixtures from racemic starting materials include: asymmetric physical laws, such as the electroweak interaction; asymmetric environments, such as those caused by circularly polarized light, quartz crystals, or the Earth's rotation; and statistical fluctuations during racemic synthesis. Once established, chirality would be selected for. A small enantiomeric excess can be amplified into a large one by asymmetric autocatalysis, such as in the Soai reaction. In asymmetric autocatalysis, the catalyst is a chiral molecule, which means that a chiral molecule is catalysing its own production. An initial enantiomeric excess, such as can be produced by polarized light, then allows the more abundant enantiomer to outcompete the other.

One supposition is that the discovery of an enantiomeric imbalance in molecules in the Murchison meteorite supports an extraterrestrial origin of homochirality: there is evidence for the existence of circularly polarized light originating from Mie scattering on aligned interstellar dust particles which may trigger the formation of an enantiomeric excess within chiral material in space. Interstellar and near-stellar magnetic fields can align dust particles in this fashion. Another speculation (the Vester-Ulbricht hypothesis) suggests that fundamental chirality of physical processes such as that of the beta decay (see Parity violation) leads to slightly different half-lives of biologically relevant molecules. Homochirality may also result from spontaneous absolute asymmetric synthesis.

It is also possible that homochirality is simply a result of the natural autoamplification process of life—that either the formation of life as preferring one chirality or the other was a chance rare event which happened to occur with the chiralities we observe, or that all chiralities of life emerged rapidly but due to catastrophic events and strong competition, the other unobserved chiral preferences were wiped out by the preponderance and metabolic, enantiomeric enrichment from the 'winning' chirality choices. The emergence of chirality consensus as a natural autoamplification process has been associated with the 2nd law of thermodynamics.

In 1953, Charles Frank proposed a model to demonstrate that homochirality is a consequence of autocatalysis. In his model the and enantiomers of a chiral molecule are autocatalytically produced from an achiral molecule A

while suppressing each other through a reaction that he called "mutual antagonism"
In this model the racemic state is unstable in the sense that the slightest enantiomeric excess will be amplified to a completely homochiral state. This can be shown by computing the reaction rates from the law of mass action:
where formula_2 is the rate constant for the autocatalytic reactions, formula_3 is the rate constant for mutual antagonism reaction, and the concentration of A is kept constant for simplicity. By defining the enantiomeric excess formula_4 as
we can compute the rate of change of enatiomeric excess using chain rule from the rate of change of the concentrations of enantiomeres and .
Linear stability analysis of this equation shows that the racemic state formula_7 is unstable. Starting from almost everywhere in the concentration space, the system evolves to a homochiral state.

It is generally understood that autocatalysis alone does not yield to homochirality, and the presence of the mutually antagonistic relationship between the two enantiomers is necessary for the instability of the racemic mixture. However, recent studies show that homochirality could be achieved from autocatalysis in the absence of the mutually antagonistic relationship, but the underlying mechanism for symmetry-breaking is different.

There are several laboratory experiments that demonstrate how a small amount of one enantiomer at the start of a reaction can lead to a large excess of a single enantiomer as the product. For example, the Soai reaction is autocatalytic. If the reaction is started with some of one of the product enantiomers already present, the product acts as an enantioselective catalyst for production of more of that same enantiomer. The initial presence of just 0.2 equivalent one enantiomer can lead to up to 93% enantiomeric excess of the product.

Another study concerns the proline catalyzed aminoxylation of propionaldehyde by nitrosobenzene. In this system, a small enantiomeric excess of catalyst leads to a large enantiomeric excess of product.

Serine octamer clusters are also contenders. These clusters of 8 serine molecules appear in mass spectrometry with an unusual homochiral preference, however there is no evidence that such clusters exist under non-ionizing conditions and amino acid phase behavior is far more prebiotically relevant. The recent observation that partial sublimation of a 10% enantioenriched sample of leucine results in up to 82% enrichment in the sublimate shows that enantioenrichment of amino acids could occur in space. Partial sublimation processes can take place on the surface of meteors where large variations in temperature exist. This finding may have consequences for the development of the Mars Organic Detector scheduled for launch in 2013 which aims to recover trace amounts of amino acids from the Mars surface exactly by a sublimation technique.

A high asymmetric amplification of the enantiomeric excess of sugars are also present in the amino acid catalyzed asymmetric formation of carbohydrates

One classic study involves an experiment that takes place in the laboratory. When sodium chlorate is allowed to crystallize from water and the collected crystals examined in a polarimeter, each crystal turns out to be chiral and either the form or the form. In an ordinary experiment the amount of crystals collected equals the amount of crystals (corrected for statistical effects). However, when the sodium chlorate solution is stirred during the crystallization process the crystals are either exclusively or exclusively . In 32 consecutive crystallization experiments 14 experiments deliver -crystals and 18 others -crystals. The explanation for this symmetry breaking is unclear but is related to autocatalysis taking place in the nucleation process.

In a related experiment, a crystal suspension of a racemic amino acid derivative continuously stirred, results in a 100% crystal phase of one of the enantiomers because the enantiomeric pair is able to equilibrate in solution (compare with dynamic kinetic resolution).

Many strategies in asymmetric synthesis are built on chiral transmission. Especially important is the so-called organocatalysis of organic reactions by proline for example in Mannich reactions.

There exists no theory elucidating correlations among -amino acids. If one takes, for example, alanine, which has a small methyl group, and phenylalanine, which has a larger benzyl group, a simple question is in what aspect, -alanine resembles -phenylalanine more than -phenylalanine, and what kind of mechanism causes the selection of all -amino acids. Because it might be possible that alanine was and phenylalanine was .

It was reported in 2004 that excess racemic ,-asparagine (Asn), which spontaneously forms crystals of either isomer during recrystallization, induces asymmetric resolution of a co-existing racemic amino acid such as arginine (Arg), aspartic acid (Asp), glutamine (Gln), histidine (His), leucine (Leu), methionine (Met), phenylalanine (Phe), serine (Ser), valine (Val), tyrosine (Tyr), and tryptophan (Trp). The enantiomeric excess of these amino acids was correlated almost linearly with that of the inducer, i.e., Asn. When recrystallizations from a mixture of 12 ,-amino acids (Ala, Asp, Arg, Glu, Gln, His, Leu, Met, Ser, Val, Phe, and Tyr) and excess ,-Asn were made, all amino acids with the same configuration with Asn were preferentially co-crystallized. It was incidental whether the enrichment took place in - or -Asn, however, once the selection was made, the co-existing amino acid with the same configuration at the α-carbon was preferentially involved because of thermodynamic stability in the crystal formation. The maximal ee was reported to be 100%. Based on these results, it is proposed that a mixture of racemic amino acids causes spontaneous and effective optical resolution, even if asymmetric synthesis of a single amino acid does not occur without an aid of an optically active molecule.

This is the first study elucidating reasonably the formation of chirality from racemic amino acids with experimental evidences.

This term was introduced by Kelvin in 1904, the year that he published his Baltimore Lecture of 1884. Kelvin used the term homochirality as a relationship between two molecules, i.e. two molecules are homochiral if they have the same chirality. Recently, however, homochiral has been used in the same sense as enantiomerically pure. This is permitted in some journals (but not encouraged), its meaning changing into the preference of a process or system for a single optical isomer in a pair of isomers in these journals.




</doc>
<doc id="32703814" url="https://en.wikipedia.org/wiki?curid=32703814" title="Chirality">
Chirality

Chirality is a property of asymmetry important in several branches of science. The word "chirality" is derived from the Greek ("kheir"), "hand," a familiar chiral object.

An object or a system is "chiral" if it is distinguishable from its mirror image; that is, it cannot be superposed onto it. Conversely, a mirror image of an "achiral" object, such as a sphere, cannot be distinguished from the object. A chiral object and its mirror image are called "enantiomorphs" (Greek, "opposite forms") or, when referring to molecules, "enantiomers". A non-chiral object is called "achiral" (sometimes also "amphichiral") and can be superposed on its mirror image.

The term was first used by Lord Kelvin in 1893 in the second Robert Boyle Lecture at the Oxford University Junior Scientific Club which was published in 1894:

Human hands are perhaps the most universally recognized example of chirality. The left hand is a non-superimposable mirror image of the right hand; no matter how the two hands are oriented, it is impossible for all the major features of both hands to coincide across all axes. This difference in symmetry becomes obvious if someone attempts to shake the right hand of a person using their left hand, or if a left-handed glove is placed on a right hand. In mathematics, "chirality" is the property of a figure that is not identical to its mirror image.A molecule is said to chiral if its all valence is occupied by different atom or group of atom.

In mathematics, a figure is chiral (and said to have chirality) if it cannot be mapped to its mirror image by rotations and translations alone. For example, a right shoe is different from a left shoe, and clockwise is different from anticlockwise. See for a full mathematical definition.

A chiral object and its mirror image are said to be enantiomorphs. The word "enantiomorph" stems from the Greek (enantios) 'opposite' + (morphe) 'form'. A non-chiral figure is called achiral or amphichiral.

The helix (and by extension a spun string, a screw, a propeller, etc.) and Möbius strip are chiral two-dimensional objects in three-dimensional ambient space. The J, L, S and Z-shaped "tetrominoes" of the popular video game Tetris also exhibit chirality, but only in a two-dimensional space.

Many other familiar objects exhibit the same chiral symmetry of the human body, such as gloves, glasses (where two lenses differ in prescription), and shoes. A similar notion of chirality is considered in knot theory, as explained below.

Some chiral three-dimensional objects, such as the helix, can be assigned a right or left handedness, according to the right-hand rule.

In geometry a figure is achiral if and only if its symmetry group contains at least one "orientation-reversing" isometry.
In two dimensions, every figure that possesses an axis of symmetry is achiral, and it can be shown that every "bounded" achiral figure must have an axis of symmetry.
In three dimensions, every figure that possesses a plane of symmetry or a center of symmetry is achiral. There are, however, achiral figures lacking both plane and center of symmetry.
In terms of point groups, all chiral figures lack an improper axis of rotation (S). This means that they cannot contain a center of inversion (i) or a mirror plane (σ). Only figures with a point group designation of C, C, D, T, O, or I can be chiral.

A knot is called achiral if it can be continuously deformed into its mirror image, otherwise it is called chiral. For example, the unknot and the figure-eight knot are achiral, whereas the trefoil knot is chiral.

In physics, chirality may be found in the spin of a particle, where the handedness of the object is determined by the direction in which the particle spins. Not to be confused with helicity, which is the projection of the spin along the linear momentum of a subatomic particle, chirality is a purely quantum mechanical phenomenon like spin. Although both can have left-handed or right-handed properties, only in the massless case do they have a simple relation. In particular for a massless particle the helicity is the same as the chirality while for an antiparticle they have opposite sign.

The "handedness" in both chirality and helicity relate to the rotation of a particle while it proceeds in linear motion with reference to the human hands. The thumb of the hand points towards the direction of linear motion whilst the fingers curl into the palm, representing the direction of rotation of the particle (i.e. clockwise and counterclockwise). Depending on the linear and rotational motion, the particle can either be defined by left-handedness or right-handedness. A symmetry transformation between the two is called parity. Invariance under parity by a Dirac fermion is called "chiral symmetry".

Electromagnetic wave propagation as handedness is wave polarization and described in terms of helicity (occurs as a helix). Polarization of an electromagnetic wave is the property that describes the orientation, i.e., the time-varying, direction (vector), and amplitude of the electric field vector. 

Chiral mirrors are a class of metamaterials that reflect circularly polarized light of a certain helicity in a handedness-preserving manner, while absorbing circular polarization of the opposite handedness. However, most absorbing chiral mirrors operate only in a narrow frequency band, as limited by the causality principle. Employing a different design methodology that allows undesired waves to pass through instead of absorbing the undesired waveform, chiral mirrors are able to show good performance in broadband.

A "chiral molecule" is a type of molecule that has a non-superposable mirror image. The feature that is most often the cause of chirality in molecules is the presence of an asymmetric carbon atom.

The term "chiral" in general is used to describe the object that is non-superposable on its mirror image.

In chemistry, chirality usually refers to molecules. Two mirror images of a chiral molecule are called enantiomers or optical isomers. Pairs of enantiomers are often designated as "right-", "left-handed" or, if they have no bias, "achiral". As polarized light passes through a chiral molecule, the plane of polarization, when viewed along the axis toward the source, will be rotated clockwise (to the right) or anticlockwise (to the left). A right handed rotation is dextrorotary (d); that to the left is levorotary (l). The d- and l-isomers are the same compound but are called enantiomers. An equimolar mixture of the two optical isomers will produce no net rotation of polarized light as it passes through. Left handed molecules have l- prefixed to their names; d- is prefixed to right handed molecules.

Molecular chirality is of interest because of its application to stereochemistry in inorganic chemistry, organic chemistry, physical chemistry, biochemistry, and supramolecular chemistry.

More recent developments in chiral chemistry include the development of chiral inorganic nanoparticles that may have the similar tetrahedral geometry as chiral centers associated with sp3 carbon atoms traditionally associated with chiral compounds, but at larger scale. Helical and other symmetries of chiral nanomaterials were also obtained.

All of the known life-forms show specific chiral properties in chemical structures as well as macroscopic anatomy, development and behavior. In any specific organism or evolutionarily related set thereof, individual compounds, organs, or behavior are found in the same single enantiomorphic form. Deviation (having the opposite form) could be found in a small number of chemical compounds, or certain organ or behavior but that variation strictly depends upon the genetic make up of the organism. From chemical level (molecular scale), biological systems show extreme stereospecificity in synthesis, uptake, sensing, metabolic processing. A living system usually deals with two enantiomers of the same compound in drastically different ways.

In biology, homochirality is a common property of amino acids and carbohydrates. The chiral protein-making amino acids, which are translated through the ribosome from genetic coding, occur in the form. However, -amino acids are also found in nature. The monosaccharides (carbohydrate-units) are commonly found in -configuration. DNA double helix is chiral (as any kind of helix is chiral), and B-form of DNA shows a right-handed turn.

Sometimes, when two enantiomers of a compound found in organisms, they significantly differ in their taste, smell and other biological actions. For example, (+)-limonene found in orange (causing its smell), and (–)-limonene found in lemons (causing its smell), show different smells due to different biochemical interactions at human nose. (+)-Carvone is responsible for the smell of caraway seed oil, whereas (–)-carvone is responsible for smell of spearmint oil.

Also, for artificial compounds, including medicines, in case of chiral drugs, the two enantiomers sometimes show remarkable difference in effect of their biological actions. Darvon (dextropropoxyphene) is a painkiller, whereas its enantiomer, Novrad (levopropoxyphene) is an anti-cough agent. In case of penicillamine, the ("S"-isomer used in treatment of primary chronic arthritis, whereas the ("R")-isomer has no therapeutic effect as well as being highly toxic. In some cases the less therapeutically active enantiomer can cause side effects. For example, ("S"-naproxen is an analgesic but the ("R"-isomer cause renal problems. The naturally occurring plant form of alpha-tocopherol (vitamin E) is RRR-α-tocopherol whereas the synthetic form (all-racemic vitamin E, or dl-tocopherol) is equal parts of the stereoisomers RRR, RRS, RSS, SSS, RSR, SRS, SRR and SSR with progressively decreasing biological equivalency, so that 1.36 mg of dl-tocopherol is considered equivalent to 1.0 mg of d-tocopherol.
Macroscopic examples of chirality are found in the plant kingdom, the animal kingdom and all other groups of organism. A simple example is the coiling direction of any climber plant, which can grow to form either a left- or right-handed helix.
In anatomy, chirality is found in the imperfect mirror image symmetry of many kinds of animal bodies. Organisms such as gastropods exhibit chirality in their coiled shells, resulting in an asymmetrical appearance. Over 90% of gastropod species have "dextral" (right-handed) shells in their coiling, but a small minority of species and genera are virtually always "sinistral" (left-handed). A very few species (for example "Amphidromus perversus") show an equal mixture of dextral and sinistral individuals.

In humans, chirality (also referred to as "handedness" or "laterality") is an attribute of humans defined by their unequal distribution of fine motor skill between the left and right hands. An individual who is more dexterous with the right hand is called "right-handed", and one who is more skilled with the left is said to be "left-handed". Chirality is also seen in the study of facial asymmetry.

In the case of the health condition "situs inversus totalis", in which all the internal organs are flipped horizontally (i.e. the heart placed slightly to the right instead of the left), chirality poses some problems should the patient require a liver or heart transplant, as these organs are chiral, thus meaning that the blood vessels which supply these organs would need to be rearranged should a normal, non "situs inversus" organ be requrired.

In flatfish, the Summer flounder or fluke are left-eyed, while halibut are right-eyed.

Chirality, chiral theory and even "knots" of a sort feature very heavily in the 2019 video game "Death Stranding", by Hideo Kojima and Kojima Productions.


 

</doc>
<doc id="183290" url="https://en.wikipedia.org/wiki?curid=183290" title="Life extension">
Life extension

Life extension is the idea of extending the human lifespan, either modestly – through improvements in medicine – or dramatically by increasing the maximum lifespan beyond its generally settled limit of 125 years. The ability to achieve such dramatic changes, however, does not currently exist.

Some researchers in this area, and "life extensionists", "immortalists" or "longevists" (those who wish to achieve longer lives themselves), believe that future breakthroughs in tissue rejuvenation, stem cells, regenerative medicine, molecular repair, gene therapy, pharmaceuticals, and organ replacement (such as with artificial organs or xenotransplantations) will eventually enable humans to have indefinite lifespans (agerasia) through complete rejuvenation to a healthy youthful condition. The ethical ramifications, if life extension becomes a possibility, are debated by bioethicists.

The sale of purported anti-aging products such as supplements and hormone replacement is a lucrative global industry. For example, the industry that promotes the use of hormones as a treatment for consumers to slow or reverse the aging process in the US market generated about $50 billion of revenue a year in 2009. The use of such products has not been proven to be effective or safe.

During the process of aging, an organism accumulates damage to its macromolecules, cells, tissues, and organs. Specifically, aging is characterized as and thought to be caused by "genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication." Oxidation damage to cellular contents caused by free radicals is believed to contribute to aging as well.

The longest documented human lifespan is 122 years, the case of Jeanne Calment who according to records was born in 1875 and died in 1997, whereas the maximum lifespan of a wildtype mouse, commonly used as a model in research on aging, is about three years. Genetic differences between humans and mice that may account for these different aging rates include differences in efficiency of DNA repair, antioxidant defenses, energy metabolism, proteostasis maintenance, and recycling mechanisms such as autophagy.

The average lifespan in a population is lowered by infant and child mortality, which are frequently linked to infectious diseases or nutrition problems. Later in life, vulnerability to accidents and age-related chronic disease such as cancer or cardiovascular disease play an increasing role in mortality. Extension of expected lifespan can often be achieved by access to improved medical care, vaccinations, good diet, exercise and avoidance of hazards such as smoking.

Maximum lifespan is determined by the rate of aging for a species inherent in its genes and by environmental factors. Widely recognized methods of extending maximum lifespan in model organisms such as nematodes, fruit flies, and mice include caloric restriction, gene manipulation, and administration of pharmaceuticals. Another technique uses evolutionary pressures such as breeding from only older members or altering levels of extrinsic mortality.
Some animals such as hydra, planarian flatworms, and certain sponges, corals, and jellyfish do not die of old age and exhibit potential immortality.

Much life extension research focuses on nutrition—diets or supplements— although there is little evidence that they have an effect. The many diets promoted by anti-aging advocates are often contradictory. 

In some studies calorie restriction has been shown to extend the life of mice, yeast, and rhesus monkeys. However, a more recent study did not find calorie restriction to improve survival in rhesus monkeys. In humans the long-term health effects of moderate caloric restriction with sufficient nutrients are unknown.

The free-radical theory of aging suggests that antioxidant supplements might extend human life. Reviews, however, have found that vitamin A (as β-carotene) and vitamin E supplements may increase mortality. Other reviews have found no relationship between vitamin E and other vitamins with mortality.

The anti-aging industry offers several hormone therapies. Some of these have been criticized for possible dangers and a lack of proven effect. For example, the American Medical Association has been critical of some anti-aging hormone therapies.

While growth hormone (GH) decreases with age, the evidence for use of growth hormone as an anti-aging therapy is mixed and based mostly on animal studies. There are mixed reports that GH or IGF-1 modulates the aging process in humans and about whether the direction of its effect is positive or negative.

The extension of life has been a desire of humanity and a mainstay motif in the history of scientific pursuits and ideas throughout history, from the Sumerian Epic of Gilgamesh and the Egyptian Smith medical papyrus, all the way through the Taoists, Ayurveda practitioners, alchemists, hygienists such as Luigi Cornaro, Johann Cohausen and Christoph Wilhelm Hufeland, and philosophers such as Francis Bacon, René Descartes, Benjamin Franklin and Nicolas Condorcet. However, the beginning of the modern period in this endeavor can be traced to the end of the 19th – beginning of the 20th century, to the so-called "fin-de-siècle" (end of the century) period, denoted as an "end of an epoch" and characterized by the rise of scientific optimism and therapeutic activism, entailing the pursuit of life extension (or life-extensionism). Among the foremost researchers of life extension at this period were the Nobel Prize winning biologist Elie Metchnikoff (1845-1916) -- the author of the cell theory of immunity and vice director of Institut Pasteur in Paris, and Charles-Édouard Brown-Séquard (1817-1894) -- the president of the French Biological Society and one of the founders of modern endocrinology.

Sociologist James Hughes claims that science has been tied to a cultural narrative of conquering death since the Age of Enlightenment. He cites Francis Bacon (1561–1626) as an advocate of using science and reason to extend human life, noting Bacon's novel "New Atlantis", wherein scientists worked toward delaying aging and prolonging life. Robert Boyle (1627–1691), founding member of the Royal Society, also hoped that science would make substantial progress with life extension, according to Hughes, and proposed such experiments as "to replace the blood of the old with the blood of the young". Biologist Alexis Carrel (1873–1944) was inspired by a belief in indefinite human lifespan that he developed after experimenting with cells, says Hughes.

Regulatory and legal struggles between the Food and Drug Administration (FDA) and the Life Extension organization included seizure of merchandise and court action. In 1991, Saul Kent and Bill Faloon, the principals of the organization, were jailed for four hours and were released on $850,000 bond each. After 11 years of legal battles, Kent and Faloon convinced the US Attorney’s Office to dismiss all criminal indictments brought against them by the FDA.

In 2003, Doubleday published "The Immortal Cell: One Scientist's Quest to Solve the Mystery of Human Aging," by Michael D. West. West emphasised the potential role of embryonic stem cells in life extension.

Other modern life extensionists include writer Gennady Stolyarov, who insists that death is "the enemy of us all, to be fought with medicine, science, and technology"; transhumanist philosopher Zoltan Istvan, who proposes that the "transhumanist must safeguard one's own existence above all else"; futurist George Dvorsky, who considers aging to be a problem that desperately needs to be solved; and recording artist Steve Aoki, who has been called "one of the most prolific campaigners for life extension".

In 1991, the American Academy of Anti-Aging Medicine (A4M) was formed. The American Board of Medical Specialties recognizes neither anti-aging medicine nor the A4M's professional standing.

In 2003, Aubrey de Grey and David Gobel formed the Methuselah Foundation, which gives financial grants to anti-aging research projects. In 2009, de Grey and several others founded the SENS Research Foundation, a California-based scientific research organization which conducts research into aging and funds other anti-aging research projects at various universities. In 2013, Google announced Calico, a new company based in San Francisco that will harness new technologies to increase scientific understanding of the biology of aging. It is led by Arthur D. Levinson, and its research team includes scientists such as Hal V. Barron, David Botstein, and Cynthia Kenyon. In 2014, biologist Craig Venter founded Human Longevity Inc., a company dedicated to scientific research to end aging through genomics and cell therapy. They received funding with the goal of compiling a comprehensive human genotype, microbiome, and phenotype database.

Aside from private initiatives, aging research is being conducted in university laboratories, and includes universities such as Harvard and UCLA. University researchers have made a number of breakthroughs in extending the lives of mice and insects by reversing certain aspects of aging.

Some critics dispute the portrayal of aging as a disease. For example, Leonard Hayflick, who determined that fibroblasts are limited to around 50 cell divisions, reasons that aging is an unavoidable consequence of entropy. Hayflick and fellow biogerontologists Jay Olshansky and Bruce Carnes have strongly criticized the anti-aging industry in response to what they see as unscrupulous profiteering from the sale of unproven anti-aging supplements.

Research by Sobh and Martin (2011) suggests that people buy anti-aging products to obtain a hoped-for self (e.g., keeping a youthful skin) or to avoid a feared-self (e.g., looking old). The research shows that when consumers pursue a hoped-for self, it is expectations of success that most strongly drive their motivation to use the product. The research also shows why doing badly when trying to avoid a feared self is more motivating than doing well. When product use is seen to fail it is more motivating than success when consumers seek to avoid a feared-self.

Though many scientists state that life extension and radical life extension are possible, there are still no international or national programs focused on radical life extension. There are political forces staying for and against life extension. By 2012, in Russia, the United States, Israel, and the Netherlands, the Longevity political parties started. They aimed to provide political support to radical life extension research and technologies, and ensure the fastest possible and at the same time soft transition of society to the next step – life without aging and with radical life extension, and to provide access to such technologies to most currently living people.

Some tech innovators and Silicon Valley entrepreneurs have invested heavily into anti-aging research. This includes Larry Ellison (founder of Oracle), Peter Thiel (former PayPal CEO), Larry Page (co-founder of Google), and Peter Diamandis.

Leon Kass (chairman of the US President's Council on Bioethics from 2001 to 2005) has questioned whether potential exacerbation of overpopulation problems would make life extension unethical. He states his opposition to life extension with the words:
John Harris, former editor-in-chief of the Journal of Medical Ethics, argues that as long as life is worth living, according to the person himself, we have a powerful moral imperative to save the life and thus to develop and offer life extension therapies to those who want them.

Transhumanist philosopher Nick Bostrom has argued that any technological advances in life extension must be equitably distributed and not restricted to a privileged few. In an extended metaphor entitled "The Fable of the Dragon-Tyrant", Bostrom envisions death as a monstrous dragon who demands human sacrifices. In the fable, after a lengthy debate between those who believe the dragon is a fact of life and those who believe the dragon can and should be destroyed, the dragon is finally killed. Bostrom argues that political inaction allowed many preventable human deaths to occur.

Controversy about life extension is due to fear of overpopulation and possible effects on society. Biogerontologist Aubrey De Grey counters the overpopulation critique by pointing out that the therapy could postpone or eliminate menopause, allowing women to space out their pregnancies over more years and thus "decreasing" the yearly population growth rate. Moreover, the philosopher and futurist Max More argues that, given the fact the worldwide population growth rate is slowing down and is projected to eventually stabilize and begin falling, superlongevity would be unlikely to contribute to overpopulation.

A Spring 2013 Pew Research poll in the United States found that 38% of Americans would want life extension treatments, and 56% would reject it. However, it also found that 68% believed most people would want it and that only 4% consider an "ideal lifespan" to be more than 120 years. The median "ideal lifespan" was 91 years of age and the majority of the public (63%) viewed medical advances aimed at prolonging life as generally good. 41% of Americans believed that radical life extension (RLE) would be good for society, while 51% said they believed it would be bad for society. One possibility for why 56% of Americans claim they would reject life extension treatments may be due to the cultural perception that living longer would result in a longer period of decrepitude, and that the elderly in our current society are unhealthy.

Religious people are no more likely to oppose life extension than the unaffiliated, though some variation exists between religious denominations.

Mainstream medical organizations and practitioners do not consider aging to be a disease. David Sinclair says: "I don't see aging as a disease, but as a collection of quite predictable diseases caused by the deterioration of the body". The two main arguments used are that aging is both inevitable and universal while diseases are not. However, not everyone agrees. Harry R. Moody, director of academic affairs for AARP, notes that what is normal and what is disease strongly depend on a historical context. David Gems, assistant director of the Institute of Healthy Ageing, argues that aging should be viewed as a disease. In response to the universality of aging, David Gems notes that it is as misleading as arguing that Basenji are not dogs because they do not bark. Because of the universality of aging he calls it a "special sort of disease". Robert M. Perlman, coined the terms "aging syndrome" and "disease complex" in 1954 to describe aging.

The discussion whether aging should be viewed as a disease or not has important implications. One view is, this would stimulate pharmaceutical companies to develop life extension therapies and in the United States of America, it would also increase the regulation of the anti-aging market by the FDA. Anti-aging now falls under the regulations for cosmetic medicine which are less tight than those for drugs.

Theoretically, extension of maximum lifespan in humans could be achieved by reducing the rate of aging damage by periodic replacement of damaged tissues, molecular repair or rejuvenation of deteriorated cells and tissues, reversal of harmful epigenetic changes, or the enhancement of enzyme telomerase activity.

Research geared towards life extension strategies in various organisms is currently under way at a number of academic and private institutions. Since 2009, investigators have found ways to increase the lifespan of nematode worms and yeast by 10-fold; the record in nematodes was achieved through genetic engineering and the extension in yeast by a combination of genetic engineering and caloric restriction. A 2009 review of longevity research noted: "Extrapolation from worms to mammals is risky at best, and it cannot be assumed that interventions will result in comparable life extension factors. Longevity gains from dietary restriction, or from mutations studied previously, yield smaller benefits to Drosophila than to nematodes, and smaller still to mammals. This is not unexpected, since mammals have evolved to live many times the worm's lifespan, and humans live nearly twice as long as the next longest-lived primate. From an evolutionary perspective, mammals and their ancestors have already undergone several hundred million years of natural selection favoring traits that could directly or indirectly favor increased longevity, and may thus have already settled on gene sequences that promote lifespan. Moreover, the very notion of a "life-extension factor" that could apply across taxa presumes a linear response rarely seen in biology."

There are a number of chemicals intended to slow the aging process currently being studied in animal models. One type of research is related to the observed effects of a calorie restriction (CR) diet, which has been shown to extend lifespan in some animals. Based on that research, there have been attempts to develop drugs that will have the same effect on the aging process as a caloric restriction diet, which are known as Caloric restriction mimetic drugs. Some drugs that are already approved for other uses have been studied for possible longevity effects on laboratory animals because of a possible CR-mimic effect; they include rapamycin, metformin and other geroprotectors. MitoQ, resveratrol and pterostilbene are dietary supplements that have also been studied in this context.

Other attempts to create anti-aging drugs have taken different research paths. One notable direction of research has been research into the possibility of using the enzyme telomerase in order to counter the process of telomere shortening. However, there are potential dangers in this, since some research has also linked telomerase to cancer and to tumor growth and formation.

Future advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair machines, including ones operating within cells and utilizing as yet hypothetical molecular computers, in his 1986 book Engines of Creation. Raymond Kurzweil, a futurist and transhumanist, stated in his book "The Singularity Is Near" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030. According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a "medical" use for Feynman's theoretical nanomachines (see biological machine). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) "swallow the doctor". The idea was incorporated into Feynman's 1959 essay "There's Plenty of Room at the Bottom."

Some life extensionists suggest that therapeutic cloning and stem cell research could one day provide a way to generate cells, body parts, or even entire bodies (generally referred to as reproductive cloning) that would be genetically identical to a prospective patient. Recently, the US Department of Defense initiated a program to research the possibility of growing human body parts on mice. Complex biological structures, such as mammalian joints and limbs, have not yet been replicated. Dog and primate brain transplantation experiments were conducted in the mid-20th century but failed due to rejection and the inability to restore nerve connections. As of 2006, the implantation of bio-engineered bladders grown from patients' own cells has proven to be a viable treatment for bladder disease. Proponents of body part replacement and cloning contend that the required biotechnologies are likely to appear earlier than other life-extension technologies.

The use of human stem cells, particularly embryonic stem cells, is controversial. Opponents' objections generally are based on interpretations of religious teachings or ethical considerations. Proponents of stem cell research point out that cells are routinely formed and destroyed in a variety of contexts. Use of stem cells taken from the umbilical cord or parts of the adult body may not provoke controversy.

The controversies over cloning are similar, except general public opinion in most countries stands in opposition to reproductive cloning. Some proponents of therapeutic cloning predict the production of whole bodies, lacking consciousness, for eventual brain transplantation.

Replacement of biological (susceptible to diseases) organs with mechanical ones could extend life. This is the goal of the 2045 Initiative.

Cryonics is the low-temperature freezing (usually at ) of a human corpse, with the hope that resuscitation may be possible in the future. It is regarded with skepticism within the mainstream scientific community and has been characterized as quackery.

Another proposed life extension technology would combine existing and predicted future biochemical and genetic techniques. SENS proposes that rejuvenation may be obtained by removing aging damage via the use of stem cells and tissue engineering, telomere-lengthening machinery, allotopic expression of mitochondrial proteins, targeted ablation of cells, immunotherapeutic clearance, and novel lysosomal hydrolases.

While many biogerontologists find these ideas "worthy of discussion" and SENS conferences feature important research in the field, some contend that the alleged benefits are too speculative given the current state of technology, referring to it as "fantasy rather than science".

Genome editing, in which nucleic acid polymers are delivered as a drug and are either expressed as proteins, interfere with the expression of proteins, or correct genetic mutations, has been proposed as a future strategy to prevent aging.

A large array of genetic modifications have been found to increase lifespan in model organisms such as yeast, nematode worms, fruit flies, and mice. As of 2013, the longest extension of life caused by a single gene manipulation was roughly 50% in mice and 10-fold in nematode worms.

In "The Selfish Gene", Richard Dawkins describes an approach to life-extension that involves "fooling genes" into thinking the body is young. Dawkins attributes inspiration for this idea to Peter Medawar. The basic idea is that our bodies are composed of genes that activate throughout our lifetimes, some when we are young and others when we are older. Presumably, these genes are activated by environmental factors, and the changes caused by these genes activating can be lethal. It is a statistical certainty that we possess more lethal genes that activate in later life than in early life. Therefore, to extend life, we should be able to prevent these genes from switching on, and we should be able to do so by "identifying changes in the internal chemical environment of a body that take place during aging... and by simulating the superficial chemical properties of a young body".

One hypothetical future strategy that, as some suggest, "eliminates" the complications related to a physical body, involves the copying or transferring (e.g. by progressively replacing neurons with transistors) of a conscious mind from a biological brain to a non-biological computer system or computational device. The basic idea is to scan the structure of a particular brain in detail, and then construct a software model of it that is so faithful to the original that, when run on appropriate hardware, it will behave in essentially the same way as the original brain. Whether or not an exact copy of one's mind constitutes actual life extension is matter of debate.

Some scientists believe that the dead may one day be "resurrected" through simulation technology.

Some clinics currently offer injection of blood products from young donors. The alleged benefits of the treatment, none of which have been demonstrated in a proper study, include a longer life, darker hair, better memory, better sleep, curing heart diseases, diabetes and Alzheimer. The approach is based on parabiosis studies such as Irina Conboy do on mice, but Conboy says young blood does not reverse aging (even in mice) and that those who offer those treatments have misunderstood her research. Neuroscientist Tony Wyss-Coray, who also studied blood exchanges on mice as recently as 2014, said people offering those treatments are "basically abusing people's trust" and that young blood treatments are "the scientific equivalent of fake news". The treatment appeared in HBO's Silicon Valley fiction series.

Two clinics in California, run by Jesse Karmazin and David C. Wright, offer $8,000 injections of plasma extracted from the blood of young people. Karmazin has not published in any peer-reviewed journal and his current study does not use a control group.




</doc>
<doc id="4585070" url="https://en.wikipedia.org/wiki?curid=4585070" title="Non-cellular life">
Non-cellular life

Non-cellular life, or acellular life is life that exists without a cellular structure for at least part of its life cycle. Historically, most (descriptive) definitions of life postulated that a living organism must be composed of one or more cells, but this is no longer considered necessary, and modern criteria allow for forms of life based on other structural arrangements.

The primary candidates for non-cellular life are viruses. A minority of biologists consider viruses to be living organisms, but most do not. Their primary objection is that no known viruses are capable of autonomous reproduction: they must rely on cells to copy them. However, the recent discovery of giant viruses that possess genes for part of the required translation machinery has raised the prospect that they may have had extinct ancestors that could evolve and replicate independently. Most biologists agree that such an ancestor would be a true non-cellular lifeform, but its existence and characteristics are still uncertain.

Engineers sometimes use the term "artificial life" to refer to software and robots inspired by biological processes, but these do not satisfy any biological definition of life.

The nature of viruses was unclear for many years following their discovery as pathogens. They were described as poisons or toxins at first, then as "infectious proteins", but with advances in microbiology it became clear that they also possessed genetic material, a defined structure, and the ability to spontaneously assemble from their constituent parts. This spurred extensive debate as to whether they should be regarded as fundamentally organic or inorganic — as very small biological organisms or very large biochemical molecules — and since the 1950s many scientists have thought of viruses as existing at the border between chemistry and life; a gray area between living and nonliving.

The recent discovery of giant viruses (aka "giruses," nucleocytoplasmic large DNA viruses, NCLDVs) like pandoravirus has reignited this debate, since they are not only physically larger than previously known viruses, but also possess much more extensive genomes, including genes coding for aminoacyl tRNA synthetases, key proteins involved in translation, which were only ever before seen in cellular organisms. Some biologists have hypothesized that the ancestors of giant viruses evolved from cells and engaged in life processes (like self-replication) independent of cells, abilities they eventually lost secondarily. Some have further hypothesized an ancient viral lineage that originated alongside the earliest archaea or before the LUCA. Such a virus would constitute a "bona fide" lifeform, and its descendants (at least the giant viruses, but possibly all known viruses) could be phylogenetically classified in a fourth domain of life.

Ongoing research is being conducted in this area, using techniques such as phylogenetic bracketing on the giant viruses to infer characteristics of their proposed progenitor. Meanwhile, other phylogenetic analyses have argued that giant viruses are merely highly derived dsDNA viruses.

Viral replication and self-assembly has implications for the study of the origin of life, as it lends further credence to the hypothesis that life could have started as self-assembling organic molecules.

Viroids are the smallest infectious pathogens known to biologists, consisting solely of short strands of circular, single-stranded RNA without protein coats. They are mostly plant pathogens and some are animal pathogens, from which some are of commercial importance. Viroid genomes are extremely small in size, ranging from 246 to 467 nucleobases. In comparison, the genome of the smallest known viruses capable of causing an infection by themselves are around 2,000 nucleobases in size. Viroids are the first known representatives of a new biological realm of sub-viral pathogens.

Viroid RNA does not code for any protein. Its replication mechanism hijacks RNA polymerase II, a host cell enzyme normally associated with synthesis of messenger RNA from DNA, which instead catalyzes "rolling circle" synthesis of new RNA using the viroid's RNA as a template. Some viroids are ribozymes, having catalytic properties which allow self-cleavage and ligation of unit-size genomes from larger replication intermediates.

Viroids attained significance beyond plant virology since one possible explanation of their origin is that they represent “living relics” from a hypothetical, ancient, and non-cellular RNA world before the evolution of DNA or protein. This view was first proposed in the 1980s, and regained popularity in the 2010s to explain crucial intermediate steps in the evolution of life from inanimate matter (Abiogenesis).

In discussing the taxonomic domains of life, the terms "Acytota" or "Aphanobionta" are occasionally used as the name of a viral kingdom, domain, or empire. The corresponding cellular life name would be Cytota. Non-cellular organisms and cellular life would be the two top-level subdivisions of life, whereby life as a whole would be known as organisms, Naturae, or Vitae. The taxon Cytota would include three top-level subdivisions of its own, the domains Bacteria, Archaea, and Eukarya.



</doc>
<doc id="18393" url="https://en.wikipedia.org/wiki?curid=18393" title="Life">
Life

Life is a characteristic that distinguishes physical entities that have biological processes, such as signaling and self-sustaining processes, from those that do not, either because such functions have ceased (they have died), or because they never had such functions and are classified as inanimate. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. Biology is the science concerned with the study of life.

There is currently no consensus regarding the definition of life. One popular definition is that organisms are open systems that maintain homeostasis, are composed of cells, have a life cycle, undergo metabolism, can grow, adapt to their environment, respond to stimuli, reproduce and evolve. Other definitions sometimes include non-cellular life forms such as viruses and viroids.

Abiogenesis is the natural process of life arising from non-living matter, such as simple organic compounds. The prevailing scientific hypothesis is that the transition from non-living to living entities was not a single event, but a gradual process of increasing complexity. Life on Earth first appeared as early as 4.28 billion years ago, soon after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. The earliest known life forms are microfossils of bacteria. Researchers generally think that current life on Earth descends from an RNA world, although RNA-based life may not have been the first life to have existed. The classic 1952 Miller–Urey experiment and similar research demonstrated that most amino acids, the chemical constituents of the proteins used in all living organisms, can be synthesized from inorganic compounds under conditions intended to replicate those of the early Earth. Complex organic molecules occur in the Solar System and in interstellar space, and these molecules may have provided starting material for the development of life on Earth.

Since its primordial beginnings, life on Earth has changed its environment on a geologic time scale, but it has also adapted to survive in most ecosystems and conditions. Some microorganisms, called extremophiles, thrive in physically or geochemically extreme environments that are detrimental to most other life on Earth. The cell is considered the structural and functional unit of life. There are two kinds of cells, prokaryotic and eukaryotic, both of which consist of cytoplasm enclosed within a membrane and contain many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division, in which the parent cell divides into two or more daughter cells.

In the past, there have been many attempts to define what is meant by "life" through obsolete concepts such as odic force, hylomorphism, spontaneous generation and vitalism, that have now been disproved by biological discoveries. Aristotle is considered to be the first person to classify organisms. Later, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Eventually new groups and categories of life were discovered, such as cells and microorganisms, forcing dramatic revisions of the structure of relationships between living organisms. Though currently only known on Earth, life need not be restricted to it, and many scientists speculate in the existence of extraterrestrial life. Artificial life is a computer simulation or human-made reconstruction of any aspect of life, which is often used to examine systems related to natural life.

Death is the permanent termination of all biological functions which sustain an organism, and as such, is the end of its life. Extinction is the term describing the dying out of a group or taxon, usually a species. Fossils are the preserved remains or traces of organisms.

The definition of life has long been a challenge for scientists and philosophers, with many varied definitions put forward. This is partially because life is a process, not a substance. This is complicated by a lack of knowledge of the characteristics of living entities, if any, that may have developed outside of Earth. Philosophical definitions of life have also been put forward, with similar difficulties on how to distinguish living things from the non-living. Legal definitions of life have also been described and debated, though these generally focus on the decision to declare a human dead, and the legal ramifications of this decision.

Since there is no unequivocal definition of life, most current definitions in biology are descriptive. Life is considered a characteristic of something that preserves, furthers or reinforces its existence in the given environment. This characteristic exhibits all or most of the following traits:

These complex processes, called physiological functions, have underlying physical and chemical bases, as well as signaling and control mechanisms that are essential to maintaining life.

From a physics perspective, living beings are thermodynamic systems with an organized molecular structure that can reproduce itself and evolve as survival dictates. Thermodynamically, life has been described as an open system which makes use of gradients in its surroundings to create imperfect copies of itself. Hence, life is a self-sustained chemical system capable of undergoing Darwinian evolution. A major strength of this definition is that it distinguishes life by the evolutionary process rather than its chemical composition.

Others take a systemic viewpoint that does not necessarily depend on molecular chemistry. One systemic definition of life is that living things are self-organizing and autopoietic (self-producing). Variations of this definition include Stuart Kauffman's definition as an autonomous agent or a multi-agent system capable of reproducing itself or themselves, and of completing at least one thermodynamic work cycle. This definition is extended by the apparition of novel functions over time.

Whether or not viruses should be considered as alive is controversial. They are most often considered as just replicators rather than forms of life. They have been described as "organisms at the edge of life" because they possess genes, evolve by natural selection, and replicate by creating multiple copies of themselves through self-assembly. However, viruses do not metabolize and they require a host cell to make new products. Virus self-assembly within host cells has implications for the study of the origin of life, as it may support the hypothesis that life could have started as self-assembling organic molecules.

To reflect the minimum phenomena required, other biological definitions of life have been proposed, with many of these being based upon chemical systems. Biophysicists have commented that living things function on negative entropy. In other words, living processes can be viewed as a delay of the spontaneous diffusion or dispersion of the internal energy of biological molecules towards more potential microstates. In more detail, according to physicists such as John Bernal, Erwin Schrödinger, Eugene Wigner, and John Avery, life is a member of the class of phenomena that are open or continuous systems able to decrease their internal entropy at the expense of substances or free energy taken in from the environment and subsequently rejected in a degraded form.

Living systems are open self-organizing living things that interact with their environment. These systems are maintained by flows of information, energy, and matter.

Some scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory would arise out of the ecological and biological sciences and attempt to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.

The idea that the Earth is alive is found in philosophy and religion, but the first scientific discussion of it was by the Scottish scientist James Hutton. In 1785, he stated that the Earth was a superorganism and that its proper study should be physiology. Hutton is considered the father of geology, but his idea of a living Earth was forgotten in the intense reductionism of the 19th century. The Gaia hypothesis, proposed in the 1960s by scientist James Lovelock, suggests that life on Earth functions as a single organism that defines and maintains environmental conditions necessary for its survival. This hypothesis served as one of the foundations of the modern Earth system science.

The first attempt at a general living systems theory for explaining the nature of life was in 1978, by American biologist James Grier Miller. Robert Rosen (1991) built on this by defining a system component as "a unit of organization; a part with a function, i.e., a definite relation between part and whole." From this and other starting concepts, he developed a "relational theory of systems" that attempts to explain the special properties of life. Specifically, he identified the "nonfractionability of components in an organism" as the fundamental difference between living systems and "biological machines."

A systems view of life treats environmental fluxes and biological fluxes together as a "reciprocity of influence," and a reciprocal relation with environment is arguably as important for understanding life as it is for understanding ecosystems. As Harold J. Morowitz (1992) explains it, life is a property of an ecological system rather than a single organism or species. He argues that an ecosystemic definition of life is preferable to a strictly biochemical or physical one. Robert Ulanowicz (2009) highlights mutualism as the key to understand the systemic, order-generating behavior of life and ecosystems.

Complex systems biology (CSB) is a field of science that studies the emergence of complexity in functional organisms from the viewpoint of dynamic systems theory. The latter is also often called systems biology and aims to understand the most fundamental aspects of life. A closely related approach to CSB and systems biology called relational biology is concerned mainly with understanding life processes in terms of the most important relations, and categories of such relations among the essential functional components of organisms; for multicellular organisms, this has been defined as "categorical biology", or a model representation of organisms as a category theory of biological relations, as well as an algebraic topology of the functional organization of living organisms in terms of their dynamic, complex networks of metabolic, genetic, and epigenetic processes and signaling pathways. Alternative but closely related approaches focus on the interdependance of constraints, where constraints can be either molecular, such as enzymes, or macroscopic, such as the geometry of a bone or of the vascular system.

It has also been argued that the evolution of order in living systems and certain physical systems obeys a common fundamental principle termed the Darwinian dynamic. The Darwinian dynamic was formulated by first considering how macroscopic order is generated in a simple non-biological system far from thermodynamic equilibrium, and then extending consideration to short, replicating RNA molecules. The underlying order-generating process was concluded to be basically similar for both types of systems.

Another systemic definition called the operator theory proposes that "life is a general term for the presence of the typical closures found in organisms; the typical closures are a membrane and an autocatalytic set in the cell" and that an organism is any system with an organisation that complies with an operator type that is at least as complex as the cell. Life can also be modeled as a network of inferior negative feedbacks of regulatory mechanisms subordinated to a superior positive feedback formed by the potential of expansion and reproduction.

Some of the earliest theories of life were materialist, holding that all that exists is matter, and that life is merely a complex form or arrangement of matter. Empedocles (430 BC) argued that everything in the universe is made up of a combination of four eternal "elements" or "roots of all": earth, water, air, and fire. All change is explained by the arrangement and rearrangement of these four elements. The various forms of life are caused by an appropriate mixture of elements.

Democritus (460 BC) thought that the essential characteristic of life is having a soul ("psyche"). Like other ancient writers, he was attempting to explain what makes something a "living" thing. His explanation was that fiery atoms make a soul in exactly the same way atoms and void account for any other thing. He elaborates on fire because of the apparent connection between life and heat, and because fire moves.

The mechanistic materialism that originated in ancient Greece was revived and revised by the French philosopher René Descartes, who held that animals and humans were assemblages of parts that together functioned as a machine. In the 19th century, the advances in cell theory in biological science encouraged this view. The evolutionary theory of Charles Darwin (1859) is a mechanistic explanation for the origin of species by means of natural selection.

Hylomorphism is a theory first expressed by the Greek philosopher Aristotle (322 BC). The application of hylomorphism to biology was important to Aristotle, and biology is extensively covered in his extant writings. In this view, everything in the material universe has both matter and form, and the form of a living thing is its soul (Greek "psyche", Latin "anima"). There are three kinds of souls: the "vegetative soul" of plants, which causes them to grow and decay and nourish themselves, but does not cause motion and sensation; the "animal soul", which causes animals to move and feel; and the "rational soul", which is the source of consciousness and reasoning, which (Aristotle believed) is found only in man. Each higher soul has all of the attributes of the lower ones. Aristotle believed that while matter can exist without form, form cannot exist without matter, and that therefore the soul cannot exist without the body.

This account is consistent with teleological explanations of life, which account for phenomena in terms of purpose or goal-directedness. Thus, the whiteness of the polar bear's coat is explained by its purpose of camouflage. The direction of causality (from the future to the past) is in contradiction with the scientific evidence for natural selection, which explains the consequence in terms of a prior cause. Biological features are explained not by looking at future optimal results, but by looking at the past evolutionary history of a species, which led to the natural selection of the features in question.

Spontaneous generation was the belief that living organisms can form without descent from similar organisms. Typically, the idea was that certain forms such as fleas could arise from inanimate matter such as dust or the supposed seasonal generation of mice and insects from mud or garbage.

The theory of spontaneous generation was proposed by Aristotle, who compiled and expanded the work of prior natural philosophers and the various ancient explanations of the appearance of organisms; it held sway for two millennia. It was decisively dispelled by the experiments of Louis Pasteur in 1859, who expanded upon the investigations of predecessors such as Francesco Redi. Disproof of the traditional ideas of spontaneous generation is no longer controversial among biologists.

Vitalism is the belief that the life-principle is non-material. This originated with Georg Ernst Stahl (17th century), and remained popular until the middle of the 19th century. It appealed to philosophers such as Henri Bergson, Friedrich Nietzsche, and Wilhelm Dilthey, anatomists like Xavier Bichat, and chemists like Justus von Liebig. Vitalism included the idea that there was a fundamental difference between organic and inorganic material, and the belief that organic material can only be derived from living things. This was disproved in 1828, when Friedrich Wöhler prepared urea from inorganic materials. This Wöhler synthesis is considered the starting point of modern organic chemistry. It is of historical significance because for the first time an organic compound was produced in inorganic reactions.

During the 1850s, Hermann von Helmholtz, anticipated by Julius Robert von Mayer, demonstrated that no energy is lost in muscle movement, suggesting that there were no "vital forces" necessary to move a muscle. These results led to the abandonment of scientific interest in vitalistic theories, although the belief lingered on in pseudoscientific theories such as homeopathy, which interprets diseases and sickness as caused by disturbances in a hypothetical vital force or life force.

The age of the Earth is about 4.54 billion years. Evidence suggests that life on Earth has existed for at least 3.5 billion years, with the oldest physical traces of life dating back 3.7 billion years; however, some theories, such as the Late Heavy Bombardment theory, suggest that life on Earth may have started even earlier, as early as 4.1–4.4 billion years ago, and the chemistry leading to life may have begun shortly after the Big Bang, 13.8 billion years ago, during an epoch when the universe was only 10–17 million years old.

More than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct.

Although the number of Earth's catalogued species of lifeforms is between 1.2 million and 2 million, the total number of species in the planet is uncertain. Estimates range from 8 million to 100 million, with a more narrow range between 10 and 14 million, but it may be as high as 1 trillion (with only one-thousandth of one percent of the species described) according to studies realized in May 2016. The total number of related DNA base pairs on Earth is estimated at 5.0 x 10 and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon). In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.

All known life forms share fundamental molecular mechanisms, reflecting their common descent; based on these observations, hypotheses on the origin of life attempt to find a mechanism explaining the formation of a universal common ancestor, from simple organic molecules via pre-cellular life to protocells and metabolism. Models have been divided into "genes-first" and "metabolism-first" categories, but a recent trend is the emergence of hybrid models that combine both categories.

There is no current scientific consensus as to how life originated. However, most accepted scientific models build on the Miller–Urey experiment and the work of Sidney Fox, which show that conditions on the primitive Earth favored chemical reactions that synthesize amino acids and other organic compounds from inorganic precursors, and phospholipids spontaneously form lipid bilayers, the basic structure of a cell membrane.

Living organisms synthesize proteins, which are polymers of amino acids using instructions encoded by deoxyribonucleic acid (DNA). Protein synthesis entails intermediary ribonucleic acid (RNA) polymers. One possibility for how life began is that genes originated first, followed by proteins; the alternative being that proteins came first and then genes.

However, because genes and proteins are both required to produce the other, the problem of considering which came first is like that of the chicken or the egg. Most scientists have adopted the hypothesis that because of this, it is unlikely that genes and proteins arose independently.

Therefore, a possibility, first suggested by Francis Crick, is that the first life was based on RNA, which has the DNA-like properties of information storage and the catalytic properties of some proteins. This is called the RNA world hypothesis, and it is supported by the observation that many of the most critical components of cells (those that evolve the slowest) are composed mostly or entirely of RNA. Also, many critical cofactors (ATP, Acetyl-CoA, NADH, etc.) are either nucleotides or substances clearly related to them. The catalytic properties of RNA had not yet been demonstrated when the hypothesis was first proposed, but they were confirmed by Thomas Cech in 1986.

One issue with the RNA world hypothesis is that synthesis of RNA from simple inorganic precursors is more difficult than for other organic molecules. One reason for this is that RNA precursors are very stable and react with each other very slowly under ambient conditions, and it has also been proposed that living organisms consisted of other molecules before RNA. However, the successful synthesis of certain RNA molecules under the conditions that existed prior to life on Earth has been achieved by adding alternative precursors in a specified order with the precursor phosphate present throughout the reaction. This study makes the RNA world hypothesis more plausible.

Geological findings in 2013 showed that reactive phosphorus species (like phosphite) were in abundance in the ocean before 3.5 Ga, and that Schreibersite easily reacts with aqueous glycerol to generate phosphite and glycerol 3-phosphate. It is hypothesized that Schreibersite-containing meteorites from the Late Heavy Bombardment could have provided early reduced phosphorus, which could react with prebiotic organic molecules to form phosphorylated biomolecules, like RNA.

In 2009, experiments demonstrated Darwinian evolution of a two-component system of RNA enzymes (ribozymes) "in vitro". The work was performed in the laboratory of Gerald Joyce, who stated "This is the first example, outside of biology, of evolutionary adaptation in a molecular genetic system."

Prebiotic compounds may have originated extraterrestrially. NASA findings in 2011, based on studies with meteorites found on Earth, suggest DNA and RNA components (adenine, guanine and related organic molecules) may be formed in outer space.

In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.

According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe.

The diversity of life on Earth is a result of the dynamic interplay between genetic opportunity, metabolic capability, environmental challenges, and symbiosis. For most of its existence, Earth's habitable environment has been dominated by microorganisms and subjected to their metabolism and evolution. As a consequence of these microbial activities, the physical-chemical environment on Earth has been changing on a geologic time scale, thereby affecting the path of evolution of subsequent life. For example, the release of molecular oxygen by cyanobacteria as a by-product of photosynthesis induced global changes in the Earth's environment. Because oxygen was toxic to most life on Earth at the time, this posed novel evolutionary challenges, and ultimately resulted in the formation of Earth's major animal and plant species. This interplay between organisms and their environment is an inherent feature of living systems.

The biosphere is the global sum of all ecosystems. It can also be termed as the zone of life on Earth, a closed system (apart from solar and cosmic radiation and heat from the interior of the Earth), and largely self-regulating. By the most general biophysiological definition, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere, geosphere, hydrosphere, and atmosphere.

Life forms live in every part of the Earth's biosphere, including soil, hot springs, inside rocks at least deep underground, the deepest parts of the ocean, and at least high in the atmosphere. Under certain test conditions, life forms have been observed to thrive in the near-weightlessness of space and to survive in the vacuum of outer space. Life forms appear to thrive in the Mariana Trench, the deepest spot in the Earth's oceans. Other researchers reported related studies that life forms thrive inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States, as well as beneath the seabed off Japan. In August 2014, scientists confirmed the existence of life forms living below the ice of Antarctica. According to one researcher, "You can find microbes everywhere—they're extremely adaptable to conditions, and survive wherever they are."

The biosphere is postulated to have evolved, beginning with a process of biopoesis (life created naturally from non-living matter, such as simple organic compounds) or biogenesis (life created from living matter), at least some 3.5 billion years ago. The earliest evidence for life on Earth includes biogenic graphite found in 3.7 billion-year-old metasedimentary rocks from Western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone from Western Australia. More recently, in 2015, "remains of biotic life" were found in 4.1 billion-year-old rocks in Western Australia. In 2017, putative fossilized microorganisms (or microfossils) were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that were as old as 4.28 billion years, the oldest record of life on earth, suggesting "an almost instantaneous emergence of life" after ocean formation 4.4 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. According to biologist Stephen Blair Hedges, "If life arose relatively quickly on Earth ... then it could be common in the universe."

In a general sense, biospheres are any closed, self-regulating systems containing ecosystems. This includes artificial biospheres such as Biosphere 2 and BIOS-3, and potentially ones on other planets or moons.

The inert components of an ecosystem are the physical and chemical factors necessary for life—energy (sunlight or chemical energy), water, heat, atmosphere, gravity, nutrients, and ultraviolet solar radiation protection. In most ecosystems, the conditions vary during the day and from one season to the next. To live in most ecosystems, then, organisms must be able to survive a range of conditions, called the "range of tolerance." Outside that are the "zones of physiological stress," where the survival and reproduction are possible but not optimal. Beyond these zones are the "zones of intolerance," where survival and reproduction of that organism is unlikely or impossible. Organisms that have a wide range of tolerance are more widely distributed than organisms with a narrow range of tolerance.

To survive, selected microorganisms can assume forms that enable them to withstand freezing, complete desiccation, starvation, high levels of radiation exposure, and other physical or chemical challenges. These microorganisms may survive exposure to such conditions for weeks, months, years, or even centuries. Extremophiles are microbial life forms that thrive outside the ranges where life is commonly found. They excel at exploiting uncommon sources of energy. While all organisms are composed of nearly identical molecules, evolution has enabled such microbes to cope with this wide range of physical and chemical conditions. Characterization of the structure and metabolic diversity of microbial communities in such extreme environments is ongoing.

Microbial life forms thrive even in the Mariana Trench, the deepest spot in the Earth's oceans. Microbes also thrive inside rocks up to below the sea floor under of ocean.

Investigation of the tenacity and versatility of life on Earth, as well as an understanding of the molecular systems that some organisms utilize to survive such extremes, is important for the search for life beyond Earth. For example, lichen could survive for a month in a simulated Martian environment.

All life forms require certain core chemical elements needed for biochemical functioning. These include carbon, hydrogen, nitrogen, oxygen, phosphorus, and sulfur—the elemental macronutrients for all organisms—often represented by the acronym CHNOPS. Together these make up nucleic acids, proteins and lipids, the bulk of living matter. Five of these six elements comprise the chemical components of DNA, the exception being sulfur. The latter is a component of the amino acids cysteine and methionine. The most biologically abundant of these elements is carbon, which has the desirable attribute of forming multiple, stable covalent bonds. This allows carbon-based (organic) molecules to form an immense variety of chemical arrangements. Alternative hypothetical types of biochemistry have been proposed that eliminate one or more of these elements, swap out an element for one not on the list, or change required chiralities or other chemical properties.

Deoxyribonucleic acid is a molecule that carries most of the genetic instructions used in the growth, development, functioning and reproduction of all known living organisms and many viruses. DNA and RNA are nucleic acids; alongside proteins and complex carbohydrates, they are one of the three major types of macromolecule that are essential for all known forms of life. Most DNA molecules consist of two biopolymer strands coiled around each other to form a double helix. The two DNA strands are known as polynucleotides since they are composed of simpler units called nucleotides. Each nucleotide is composed of a nitrogen-containing nucleobase—either cytosine (C), guanine (G), adenine (A), or thymine (T)—as well as a sugar called deoxyribose and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. According to base pairing rules (A with T, and C with G), hydrogen bonds bind the nitrogenous bases of the two separate polynucleotide strands to make double-stranded DNA. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10, and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).

DNA stores biological information. The DNA backbone is resistant to cleavage, and both strands of the double-stranded structure store the same biological information. Biological information is replicated as the two strands are separated. A significant portion of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences.

The two strands of DNA run in opposite directions to each other and are therefore anti-parallel. Attached to each sugar is one of four types of nucleobases (informally, "bases"). It is the sequence of these four nucleobases along the backbone that encodes biological information. Under the genetic code, RNA strands are translated to specify the sequence of amino acids within proteins. These RNA strands are initially created using DNA strands as a template in a process called transcription.

Within cells, DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.

DNA was first isolated by Friedrich Miescher in 1869. Its molecular structure was identified by James Watson and Francis Crick in 1953, whose model-building efforts were guided by X-ray diffraction data acquired by Rosalind Franklin.

The first known attempt to classify organisms was conducted by the Greek philosopher Aristotle (384–322 BC), who classified all living organisms known at that time as either a plant or an animal, based mainly on their ability to move. He also distinguished animals with blood from animals without blood (or at least without red blood), which can be compared with the concepts of vertebrates and invertebrates respectively, and divided the blooded animals into five groups: viviparous quadrupeds (mammals), oviparous quadrupeds (reptiles and amphibians), birds, fishes and whales. The bloodless animals were also divided into five groups: cephalopods, crustaceans, insects (which included the spiders, scorpions, and centipedes, in addition to what we define as insects today), shelled animals (such as most molluscs and echinoderms), and "zoophytes" (animals that resemble plants). Though Aristotle's work in zoology was not without errors, it was the grandest biological synthesis of the time and remained the ultimate authority for many centuries after his death.

The exploration of the Americas revealed large numbers of new plants and animals that needed descriptions and classification. In the latter part of the 16th century and the beginning of the 17th, careful study of animals commenced and was gradually extended until it formed a sufficient body of knowledge to serve as an anatomical basis for classification.

In the late 1740s, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Linnaeus attempted to improve the composition and reduce the length of the previously used many-worded names by abolishing unnecessary rhetoric, introducing new descriptive terms and precisely defining their meaning. The Linnaean classification has eight levels: domains, kingdoms, phyla, class, order, family, genus, and species.

The fungi were originally treated as plants. For a short period Linnaeus had classified them in the taxon Vermes in Animalia, but later placed them back in Plantae. Copeland classified the Fungi in his Protoctista, thus partially avoiding the problem but acknowledging their special status. The problem was eventually solved by Whittaker, when he gave them their own kingdom in his five-kingdom system. Evolutionary history shows that the fungi are more closely related to animals than to plants.

As new discoveries enabled detailed study of cells and microorganisms, new groups of life were revealed, and the fields of cell biology and microbiology were created. These new organisms were originally described separately in protozoa as animals and protophyta/thallophyta as plants, but were united by Haeckel in the kingdom Protista; later, the prokaryotes were split off in the kingdom Monera, which would eventually be divided into two separate groups, the Bacteria and the Archaea. This led to the six-kingdom system and eventually to the current three-domain system, which is based on evolutionary relationships. However, the classification of eukaryotes, especially of protists, is still controversial.

As microbiology, molecular biology and virology developed, non-cellular reproducing agents were discovered, such as viruses and viroids. Whether these are considered alive has been a matter of debate; viruses lack characteristics of life such as cell membranes, metabolism and the ability to grow or respond to their environments. Viruses can still be classed into "species" based on their biology and genetics, but many aspects of such a classification remain controversial.

In May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.

The original Linnaean system has been modified over time as follows:
In the 1960s cladistics emerged: a system arranging taxa based on clades in an evolutionary or phylogenetic tree.

Cells are the basic unit of structure in every living thing, and all cells arise from pre-existing cells by division. Cell theory was formulated by Henri Dutrochet, Theodor Schwann, Rudolf Virchow and others during the early nineteenth century, and subsequently became widely accepted. The activity of an organism depends on the total activity of its cells, with energy flow occurring within and between them. Cells contain hereditary information that is carried forward as a genetic code during cell division.

There are two primary types of cells. Prokaryotes lack a nucleus and other membrane-bound organelles, although they have circular DNA and ribosomes. Bacteria and Archaea are two domains of prokaryotes. The other primary type of cells are the eukaryotes, which have distinct nuclei bound by a nuclear membrane and membrane-bound organelles, including mitochondria, chloroplasts, lysosomes, rough and smooth endoplasmic reticulum, and vacuoles. In addition, they possess organized chromosomes that store genetic material. All species of large complex organisms are eukaryotes, including animals, plants and fungi, though most species of eukaryote are protist microorganisms. The conventional model is that eukaryotes evolved from prokaryotes, with the main organelles of the eukaryotes forming through endosymbiosis between bacteria and the progenitor eukaryotic cell.

The molecular mechanisms of cell biology are based on proteins. Most of these are synthesized by the ribosomes through an enzyme-catalyzed process called protein biosynthesis. A sequence of amino acids is assembled and joined together based upon gene expression of the cell's nucleic acid. In eukaryotic cells, these proteins may then be transported and processed through the Golgi apparatus in preparation for dispatch to their destination.

Cells reproduce through a process of cell division in which the parent cell divides into two or more daughter cells. For prokaryotes, cell division occurs through a process of fission in which the DNA is replicated, then the two copies are attached to parts of the cell membrane. In eukaryotes, a more complex process of mitosis is followed. However, the end result is the same; the resulting cell copies are identical to each other and to the original cell (except for mutations), and both are capable of further division following an interphase period.

Multicellular organisms may have first evolved through the formation of colonies of identical cells. These cells can form group organisms through cell adhesion. The individual members of a colony are capable of surviving on their own, whereas the members of a true multi-cellular organism have developed specializations, making them dependent on the remainder of the organism for survival. Such organisms are formed clonally or from a single germ cell that is capable of forming the various specialized cells that form the adult organism. This specialization allows multicellular organisms to exploit resources more efficiently than single cells. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule, called GK-PID, may have allowed organisms to go from a single cell organism to one of many cells.

Cells have evolved methods to perceive and respond to their microenvironment, thereby enhancing their adaptability. Cell signaling coordinates cellular activities, and hence governs the basic functions of multicellular organisms. Signaling between cells can occur through direct cell contact using juxtacrine signalling, or indirectly through the exchange of agents as in the endocrine system. In more complex organisms, coordination of activities can occur through a dedicated nervous system.

Though life is confirmed only on Earth, many think that extraterrestrial life is not only plausible, but probable or inevitable. Other planets and moons in the Solar System and other planetary systems are being examined for evidence of having once supported simple life, and projects such as SETI are trying to detect radio transmissions from possible alien civilizations. Other locations within the Solar System that may host microbial life include the subsurface of Mars, the upper atmosphere of Venus, and subsurface oceans on some of the moons of the giant planets.
Beyond the Solar System, the region around another main-sequence star that could support Earth-like life on an Earth-like planet is known as the habitable zone. The inner and outer radii of this zone vary with the luminosity of the star, as does the time interval during which the zone survives. Stars more massive than the Sun have a larger habitable zone, but remain on the Sun-like "main sequence" of stellar evolution for a shorter time interval. Small red dwarfs have the opposite problem, with a smaller habitable zone that is subject to higher levels of magnetic activity and the effects of tidal locking from close orbits. Hence, stars in the intermediate mass range such as the Sun may have a greater likelihood for Earth-like life to develop. The location of the star within a galaxy may also affect the likelihood of life forming. Stars in regions with a greater abundance of heavier elements that can form planets, in combination with a low rate of potentially habitat-damaging supernova events, are predicted to have a higher probability of hosting planets with complex life. The variables of the Drake equation are used to discuss the conditions in planetary systems where civilization is most likely to exist. Use of the equation to predict the amount of extraterrestrial life, however, is difficult; because many of the variables are unknown, the equation functions as more of a mirror to what its user already thinks. As a result, the number of civilizations in the galaxy can be estimated as low as 9.1 x 10 or as high as 156 million; for the calculations, see Drake equation.

Artificial life is the simulation of any aspect of life, as through computers, robotics, or biochemistry. The study of artificial life imitates traditional biology by recreating some aspects of biological phenomena. Scientists study the logic of living systems by creating artificial environments—seeking to understand the complex information processing that defines such systems. While life is, by definition, alive, artificial life is generally referred to as data confined to a digital environment and existence.

Synthetic biology is a new area of biotechnology that combines science and biological engineering. The common goal is the design and construction of new biological functions and systems not found in nature. Synthetic biology includes the broad redefinition and expansion of biotechnology, with the ultimate goals of being able to design and build engineered biological systems that process information, manipulate chemicals, fabricate materials and structures, produce energy, provide food, and maintain and enhance human health and the environment.

Death is the permanent termination of all vital functions or life processes in an organism or cell. It can occur as a result of an accident, medical conditions, biological interaction, malnutrition, poisoning, senescence, or suicide. After death, the remains of an organism re-enter the biogeochemical cycle. Organisms may be consumed by a predator or a scavenger and leftover organic material may then be further decomposed by detritivores, organisms that recycle detritus, returning it to the environment for reuse in the food chain.

One of the challenges in defining death is in distinguishing it from life. Death would seem to refer to either the moment life ends, or when the state that follows life begins. However, determining when death has occurred is difficult, as cessation of life functions is often not simultaneous across organ systems. Such determination therefore requires drawing conceptual lines between life and death. This is problematic, however, because there is little consensus over how to define life. The nature of death has for millennia been a central concern of the world's religious traditions and of philosophical inquiry. Many religions maintain faith in either a kind of afterlife or reincarnation for the soul, or resurrection of the body at a later date.

Extinction is the process by which a group of taxa or species dies out, reducing biodiversity. The moment of extinction is generally considered the death of the last individual of that species. Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively after a period of apparent absence. Species become extinct when they are no longer able to survive in changing habitat or against superior competition. In Earth's history, over 99% of all the species that have ever lived are extinct; however, mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.

Fossils are the preserved remains or traces of animals, plants, and other organisms from the remote past. The totality of fossils, both discovered and undiscovered, and their placement in fossil-containing rock formations and sedimentary layers (strata) is known as the "fossil record". A preserved specimen is called a fossil if it is older than the arbitrary date of 10,000 years ago. Hence, fossils range in age from the youngest at the start of the Holocene Epoch to the oldest from the Archaean Eon, up to 3.4 billion years old.




</doc>
<doc id="20025013" url="https://en.wikipedia.org/wiki?curid=20025013" title="Reverence for Life">
Reverence for Life

The phrase Reverence for Life is a translation of the German phrase: "." These words came to Albert Schweitzer on a boat trip on the Ogooué River in French Equatorial Africa (now Gabon), while searching for a universal concept of ethics for our time.

Schweitzer made the phrase the basic tenet of an ethical philosophy, which he developed and put into practice. He gave expression to its development in numerous books and publications during his life and also in manuscripts which have recently been published; the main work being his unfinished four-part "Philosophy of Culture" () subtitled: "The World-view of Reverence for Life". He also used his hospital in Lambaréné in Gabon (Central Africa) to demonstrate this philosophy in practice.

He believed that Reverence for Life is a concept that develops from observation of the world around us. In 'Civilization and Ethics' he expressed this in these words:

James Brabazon (Author of the Biography of Albert Schweitzer) defined Reverence for Life with the following statement:

Albert Schweitzer hoped that the ethic of Reverence for Life would make its way in the world on the basis of his explanation of it in his books and talks, the example of his life and the force of its own argument based on the depth of fundamental thought. To some extent this is taking place as is evidenced by the growth of the environmental movement. (The book "Silent Spring", by Rachael Carson, which is widely credited with helping launch the environmental movement, was dedicated to Albert Schweitzer.)

Albert Schweitzer believed that ethical values which could underpin the ideal of true civilization had to have their foundation in deep thought and be world- and life-affirming. He therefore embarked on a search for ethical values in the various major religions and world-views accessible to him, but could not find any that were able, unequivocally, to combine ethics with life-affirmation. It was not until two years after moving out to Gabon to establish the Albert Schweitzer Hospital that he finally found the simple statement which answered his quest.

In his autobiography "Out of My Life and Thought," Schweitzer explains this process.

"Having described how at the beginning of the summer of 1915 he awoke from some kind of mental daze, asking himself why he was only criticizing civilization and not working on something constructive.". He relates how he asked himself the question:

But what is civilization?

The essential element in civilization is the ethical perfecting of the individual as well as society. At the same time, every spiritual and every material step forward has significance for civilization. The will to civilization is, then, the universal will to progress that is conscious of the ethical as the highest value. In spite of the great importance we attach to the achievements of science and human prowess, it is obvious that only a humanity that is striving for ethical ends can benefit in full measure from material progress and can overcome the dangers that accompany it...

The only possible way out of chaos is for us to adopt a concept of the world based on the ideal of true civilization.

But what is the nature of that concept of the world in which the will to the general progress and the will to the ethical progress join and are linked?

It consists in an ethical affirmation of the world and of life.

What is affirmation of the world and of life?...

For months on end, I lived in a continual state of mental agitation. Without the least success I concentrated - even during my daily work at the hospital - on the real nature of the affirmation of life and of ethics, and on the question of what they have in common. I was wandering about in a thicket where no path was to be found. I was pushing against an iron door that would not yield...

In that mental state, I had to take a long journey up the river . . . Lost in thought, I sat on deck of the barge, struggling to find the elementary and universal concept of the ethical that I had not discovered in any philosophy. I covered sheet after sheet with disconnected sentences merely to concentrate on the problem. Two days passed. Late on the third day, at the very moment when, at sunset, we were making our way through a herd of hippopotamuses, there flashed upon my mind, unforeseen and unsought, the phrase : “Reverence for Life”. [lang|de| Ehrfurcht vor dem Leben] The iron door had yielded. The path in the thicket had become visible. Now I had found my way to the principle in which affirmation of the world and ethics are joined together!” 

-- Albert Schweitzer

According to some authors, Schweitzer's thought, and specifically his development for reverence for life, was influenced by Indian religious thought and in particular Jain principle of ahimsa (non-violence). Albert Schweitzer has noted the contribution of Indian influence in his book "Indian Thought and Its Development":
The laying down of the commandment to not kill and to not damage is one of the greatest events in the spiritual history of mankind. Starting from its principle, founded on world and life denial, of abstention from action, ancient Indian thought - and this is a period when in other respects ethics have not progressed very far - reaches the tremendous discovery that ethics know no bounds. So far as we know, this is for the first time clearly expressed by Jainism.

It should not be overlooked, however, that as a child he felt deeply for the suffering of all the creatures around him. It beat upon him. He wrote, "As far back as I can remember I was saddened by the amount of misery I saw in the world around me. Youth's unqualified joie de vivre I never really knew...One thing especially saddened me was that the unfortunate animals had to suffer so much pain and misery...It was quite incomprehensible to me -- this was before I began going to school -- why in my evening prayers I should pray for human beings only. So when my mother had prayed with me and had kissed me good-night, I used to add silently a prayer that I composed myself for all living creatures. It ran thus: "O heavenly Father, protect and bless all things that have breath guard them from all evil, and let them sleep in peace..." 

Schweitzer twice went fishing with some boys "because they asked [him] to" and "this sports was soon made impossible for me by the treatment of the worms that were put on the hook...and the wrenching of the mouths of the fishes that were caught. I gave it up...From experiences like these, which moved my heart...there slowly grew up in me an unshakeable conviction that we have no right to inflict suffering and death on another living creature, and that we ought all of us to feel what a horrible thing it is to cause suffering and death..." The concept of Reverence for Life was incipient in Schweizer almost from birth.

This awareness affected him throughout his life, as when he would carefully, gently scoop a spider out of a hole it had fallen into before planting a crop there, to feed his patients and their families who also worked on the hospital farm. He wrote that, just as our own existence is significance to each of us, "[a creature's] existence is significant to it." He wrote that "...my relation to my own being and to the objective world is determined by reverence for life. This reverence for life is given as an element of my will-to-live..." and this will-to-live existed in all creatures and was to be respected.

As a child he was taught, and later as an adult taught his congregations, the "fundamental truths of the religion of Jesus as something not hostile to reason, but, on the contrary, as strengthening them." Many later thanked him, saying that this teaching had "helped them to keep their religion in later life." A great mind, he went through "heaps of books" which were piled so high he had to make aisles to get through them. He studied the life of Jesus in a depth few have ever achieved. His own philosophy, which came to be encapsulated in the phrase Reverence for Life, had this bedrock in the Four Gospels of the New Testament.

In his book "The Philosophy of Civilization", Schweitzer wrote, "Ethics are responsibility without limit toward all that lives...Love means more, since it includes fellowship in suffering, in joy, and in effort...

The word ‘will’ in the sense of determination or firmness of purpose is rarely used today and therefore Schweitzer’s use of the word as translated from the German word ‘Wille’ may appear unfamiliar. However, it is a significant part of Schweitzer’s message. He held the view in the 1920s that people had largely lost touch with their own will, having subjugated it to outside authority and sacrificed it to external circumstances.

He therefore pointed back to that elemental part of ourselves that can be in touch with our ‘will’ and can exercise it for the good of all.

In "Out of My Life and Thought" Schweitzer wrote:

In his search for an answer to the problems posed by what was to him the obvious decline of western civilization, Albert Schweitzer was not prepared to give up the belief in progress which is so much taken for granted by people of European descent. Rather, he sought to identify why this ‘will to progress’ was seemingly going off the rails and causing the disintegration of European civilization.

He came to the following conclusion: ("Out of my Life and Thought")




</doc>
<doc id="4685715" url="https://en.wikipedia.org/wiki?curid=4685715" title="Hiding Place (band)">
Hiding Place (band)

Hiding Place were a rock band from East Kilbride, Scotland.

The band comprised singer Paul McCallion, guitarists JayJay and Del Robertson Somerville, and Doug Smith, with Pitchshifter drummer Jason Bowld joining in 2004.

The band's debut release was the "At One Time or Another" EP in 2004. They toured in support of InMe and released "(What if) the Truth Looks Clearer Empty" later that year. They toured with The Rasmus and opened for Metallica, and "Cruel Kindness" followed in November 2004.

McCallion and Bowles were also part of the 'supergroup' This Is Menace, along with members of Pitchshifter, Therapy?, Hundred Reasons, Funeral for a Friend and Carcass.

CD (82876607592):

CD (82876619682):

7" (82876657807):
CD (82876657802):




</doc>
<doc id="7664848" url="https://en.wikipedia.org/wiki?curid=7664848" title="World riddle">
World riddle

The term "world riddle" or "world-riddle" has been associated, for over 100 years, with Friedrich Nietzsche (who mentioned "Welträthsel" in several of his writings)
and with the biologist-philosopher Ernst Haeckel, who, as a professor of zoology at the University of Jena,
wrote the book "Die Welträthsel" in 1895–1899, in modern spelling "Die Welträtsel" (German "The World-riddles"), with the English version published under the title "The Riddle of the Universe", 1901.

The term "world riddle" concerns the nature of the universe and the meaning of life.

The question and answer of the World Riddle has also been examined as an inspiration or allegorical meaning within some musical compositions, such as the unresolved harmonic progression at the end of "Also sprach Zarathustra" (1896) by composer Richard Strauss, made famous in the film "".
Friedrich Nietzsche referred to the "World Riddle" ("Welträthsel") in several of his writings; however, his direct influence was limited to a few years, by his failing health.

Ernst Haeckel viewed the World Riddle as a dual-question of the form, "What is the nature of the physical universe and what is the nature of human thinking?" which he explained would have a single answer since humans and the universe were contained within one system, a mono-system, as Haeckel wrote in 1895:

Haeckel had written that human behavior and feeling could be explained, within the laws of the physical universe, as "mechanical work of the ganglion-cells" as stated.

The philosopher and psychologist William James has questioned the attitude of thinking that a single answer applies to everything or everyone. In his book "Pragmatism" (1907) he satirized the world-riddle as follows:

Emil du Bois-Reymond used the term "World Riddle" in 1880 for seven great questions of science, such as the ultimate nature of matter and the origin of simple sensations. In the lecture to the Berlin Academy of Sciences he declared that neither science nor philosophy could ever explain these riddles.




</doc>
<doc id="8553751" url="https://en.wikipedia.org/wiki?curid=8553751" title="Biological organisation">
Biological organisation

Biological organization is the hierarchy of complex biological structures and systems that define life using a reductionistic approach. The traditional hierarchy, as detailed below, extends from atoms to biospheres. The higher levels of this scheme are often referred to as an ecological organization concept, or as the field, hierarchical ecology.

Each level in the hierarchy represents an increase in organizational complexity, with each "object" being primarily composed of the previous level's basic unit. The basic principle behind the organization is the concept of "emergence"—the properties and functions found at a hierarchical level are not present and irrelevant at the lower levels.

The biological organization of life is a fundamental premise for numerous areas of scientific research, particularly in the medical sciences. Without this necessary degree of organization, it would be much more difficult—and likely impossible—to apply the study of the effects of various physical and chemical phenomena to diseases and physiology (body function). For example, fields such as cognitive and behavioral neuroscience could not exist if the brain was not composed of specific types of cells, and the basic concepts of pharmacology could not exist if it was not known that a change at the cellular level can affect an entire organism. These applications extend into the ecological levels as well. For example, DDT's direct insecticidal effect occurs at the subcellular level, but affects higher levels up to and including multiple ecosystems. Theoretically, a change in one atom could change the entire biosphere.

The simple standard biological organization scheme, from the lowest level to the highest level, is as follows:

More complex schemes incorporate many more levels. For example, a molecule can be viewed as a grouping of elements, and an atom can be further divided into subatomic particles (these levels are outside the scope of biological organization). Each level can also be broken down into its own hierarchy, and specific types of these biological objects can have their own hierarchical scheme. For example, genomes can be further subdivided into a hierarchy of genes.

Each level in the hierarchy can be described by its lower levels. For example, the organism may be described at any of its component levels, including the atomic, molecular, cellular, histological (tissue), organ and organ system levels. Furthermore, at every level of the hierarchy, new functions necessary for the control of life appear. These new roles are not functions that the lower level components are capable of and are thus referred to as "emergent properties".

Every organism is organised, though not necessarily to the same degree. An organism can not be organised at the histological (tissue) level if it is not composed of tissues in the first place.

Empirically, a large proportion of the (complex) biological systems we observe in nature exhibit hierarchic structure. On theoretical grounds we could expect complex systems to be hierarchies in a world in which complexity had to evolve from simplicity. System hierarchies analysis performed in the 1950s, laid the empirical foundations for a field that would be, from the 1980s, hierarchical ecology.

The theoretical foundations are summarized by thermodynamics.
When biological systems are modeled as physical systems, in its most general abstraction, they are thermodynamic open systems that exhibit self-organised behavior, and the set/subset relations between dissipative structures can be characterized in a hierarchy.

A simpler and more direct way to explain the fundamentals of the "hierarchical organization of life", was introduced in Ecology by Odum and others as the "Simon's hierarchical principle"; Simon emphasized that hierarchy "emerges almost inevitably through a wide variety of evolutionary processes, for the simple reason that hierarchical structures are stable".

To motivate this deep idea, he offered his "parable" about imaginary watchmakers.




</doc>
<doc id="4254743" url="https://en.wikipedia.org/wiki?curid=4254743" title="Chorography">
Chorography

Chorography (from χῶρος "khōros", "place" and γράφειν "graphein", "to write") is the art of describing or mapping a region or district, and by extension such a description or map. This term derives from the writings of the ancient geographer Pomponius Mela and Ptolemy, where it meant the geographical description of regions. However, its resonances of meaning have varied at different times. Richard Helgerson states that "chorography defines itself by opposition to chronicle. It is the genre devoted to place, and chronicle is the genre devoted to time". Darrell Rohl prefers a broad definition of "the representation of space or place".

In his text of the "Geographia" (2nd century CE), Ptolemy defined geography as the study of the entire world, but chorography as the study of its smaller parts—provinces, regions, cities, or ports. Its goal was "an impression of a part, as when one makes an image of just an ear or an eye"; and it dealt with "the qualities rather than the quantities of the things that it sets down". Ptolemy implied that it was a "graphic" technique, comprising the making of views (not simply maps), since he claimed that it required the skills of a draftsman or landscape artist, rather than the more technical skills of recording "proportional placements". Ptolemy's most recent English translators, however, render the term as "regional cartography".

Ptolemy's text was rediscovered in the west at the beginning of the fifteenth century, and the term "chorography" was revived by humanist scholars. An early instance is a small-scale map of Britain in an early fifteenth-century manuscript, which is labelled a "tabula chorographica". John Dee in 1570 regarded the practice as "an underling, and a twig of "Geographie"", by which the "plat" [plan or drawing] of a particular place would be exhibited to the eye.
The term also came to be used, however, for "written" descriptions of regions. These regions were extensively visited by the writer, who then combined local topographical description, summaries of the historical sources, and local knowledge and stories, into a text. The most influential example (at least in Britain) was probably William Camden's "Britannia" (first edition 1586), which described itself on its title page as a "Chorographica descriptio". William Harrison in 1587 similarly described his own "Description of Britaine" as an exercise in chorography, distinguishing it from the historical/chronological text of Holinshed's "Chronicles" (to which the "Description" formed an introductory section). Peter Heylin in 1652 defined chorography as "the exact description of some Kingdom, Countrey, or particular Province of the same", and gave as examples Pausanias's "Description of Greece" (2nd century AD); Camden's "Britannia" (1586); Lodovico Guicciardini's "Descrittione di tutti i Paesi Bassi" (1567) (on the Low Countries); and Leandro Alberti's "Descrizione d'Italia" (1550).

Camden's "Britannia" was predominantly concerned with the history and antiquities of Britain, and, probably as a result, the term chorography in English came to be particularly associated with antiquarian texts. William Lambarde, John Stow, John Hooker, Michael Drayton, Tristram Risdon, John Aubrey and many others used it in this way, arising from a gentlemanly topophilia and a sense of service to one's county or city, until it was eventually often applied to the genre of county history. A late example was William Grey's "Chorographia" (1649), a survey of the antiquities of the city of Newcastle upon Tyne. Even before Camden's work appeared, Andrew Melville in 1574 had referred to chorography and chronology as the "twa lights" [two lights] of history.
However, the term also continued to be used for maps and map-making, particularly of sub-national or county areas. William Camden praised the county mapmakers Christopher Saxton and John Norden as "most skilfull (sic) Chorographers"; and Robert Plot in 1677 and Christopher Packe in 1743 both referred to their county maps as chorographies.

By the beginning of the eighteenth century the term had largely fallen out of use in all these contexts, being superseded for most purposes by either "topography" or "cartography". Samuel Johnson in his "Dictionary" (1755) made a distinction between geography, chorography and topography, arguing that geography dealt with large areas, topography with small areas, but chorography with intermediary areas, being "less in its object than geography, and greater than topography". In practice, however, the term is only rarely found in English by this date.

In more technical geographical literature, the term had been abandoned as city views and city maps became more and more sophisticated and demanded a set of skills that required not only skilled draftsmanship but also some knowledge of scientific surveying. However, its use was revived for a second time in the late nineteenth century by the geographer Ferdinand von Richthofen. He regarded chorography as a specialization within geography, comprising the description through field observation of the particular traits of a given area.

The term is also now widely used by historians and literary scholars to refer to the early modern genre of topographical and antiquarian literature.




</doc>
<doc id="11023939" url="https://en.wikipedia.org/wiki?curid=11023939" title="Public humanities">
Public humanities

Public humanities is the work of engaging diverse publics in reflecting on heritage, traditions, and history, and the relevance of the humanities to the current conditions of civic and cultural life. Public humanities is often practiced within federal, state, nonprofit and community-based cultural organizations that engage people in conversations, facilitate and present lectures, exhibitions, performances and other programs for the general public on topics such as history, philosophy, popular culture and the arts. 

Workers within the public humanities endeavor to create physical and virtual spaces where the public can engage in conversation, learning and reflection about issues and ideas. Public humanities projects include exhibitions and programming related to historic preservation, oral history, archives, material culture, public art, cultural heritage, and cultural policy. One example of this type of project is the Humanities Truck, an experimental mobile platform for collecting, exhibiting, preserving, and expanding dialogue around the humanities in and around the Washington, D.C. area, and is sponsored by the Henry Luce Foundation and American University. The National Endowment for the Humanities notes that public humanities projects it has supported in the past include "interpretation at historic sites, television and radio productions, museum exhibitions, podcasts, short videos, digital games, websites, mobile apps, and other digital media." Many practitioners of public humanities are invested in ensuring the accessibility and relevance of the humanities to the general public or community groups.

The American Council of Learned Societies' National Task Force on Scholarship and the Public Humanities suggests that the nature of public humanities work is to teach the public the findings of academic scholarship: it sees "scholarship and the public humanities not as two distinct spheres but as parts of a single process, the process of taking private insight, testing it, and turning it into public knowledge." Others suggest a more balanced understanding of the ways in which history, heritage and culture are shared between the academy and the public, drawing on the notion of shared historical authority.

Subfields of the public humanities include public history, public sociology, public folklore, public anthropology, public philosophy, historic preservation, museum studies, museum education, cultural heritage management, community archaeology, public art, and public science.

Several universities have established programs in the public humanities (or have otherwise expressed commitments to public humanities via the creation of centers, degrees, or certificate programs with investments in various forms of "public" work), including:




</doc>
<doc id="12769389" url="https://en.wikipedia.org/wiki?curid=12769389" title="Humanities in the United States">
Humanities in the United States

Humanities in the United States refers to the study of humanities disciplines, such as literature, history, language, performing and visual arts or philosophy, in the United States of America.

Many American colleges and universities seek to provide a broad "liberal arts education", in which all college students to study the humanities in addition to their specific area of study. Prominent proponents of liberal arts in the United States have included Mortimer J. Adler and E.D. Hirsch. A liberal arts focus is often coupled with curricular requirements; colleges including Saint Anselm College and Providence College have mandatory two-year core curricula in the humanities for their students.

The 1980 United States Rockefeller Commission on the Humanities described the humanities in its report, "The Humanities in American Life":
Through the humanities we reflect on the fundamental question: What does it mean to be human? The humanities offer clues but never a complete answer. They reveal how people have tried to make moral, spiritual, and intellectual sense of a world in which irrationality, despair, loneliness, and death are as conspicuous as birth, friendship, hope, and reason.

The very concept of the ‘humanities’ as a class or kind, distinct from the ’sciences’, has come under repeated attack in the twentieth century. T.S. Kuhn’s "The Structure of Scientific Revolutions" argued that the forces driving scientific progress often have less to do with objective inference from unbiased observation than with much more value-laden sociological and cultural factors. More recently, Richard Rorty has argued that the distinction between the sciences and the humanities is harmful to both pursuits, placing the former on an undeserved pedestal and condemning the latter to irrationality. Rorty’s position requires a wholesale rejection of such traditional philosophical distinctions as those between appearance and reality, subjective and objective, replacing them with what he endorses as a new ‘fuzziness’. This leads to a kind of pragmatism where "the oppositions between the humanities, the arts, and the sciences, might gradually fade away... In this situation, ‘the humanities’ would no longer think of themselves as such..."

In the United States, the late 20th century saw a challenge to the "elitism" of the humanities, which Edward Said has characterized as a "conservative philosophy of gentlemanly refinement, or sensibility." Such postmodernists argued that the humanities should go beyond the study of "dead white males" to include work by women and people of color, and without religious bias. The French philosopher Michel Foucault has been a very influential part of this movement, stating in "The Order of Things" that "we can study only individuals, not human nature." However some in the humanities believed that such changes could be detrimental; the result is said to be what E. D. Hirsch Jr. refers to as declining cultural literacy.

President Lyndon Johnson signed the National Foundation on the Arts and Humanities Act in 1965, creating the National Council on the Humanities, and funded the National Endowment for the Humanities (NEH) in 1969. NEH is an independent grant-making agency of the United States government dedicated to supporting research, education, preservation, and public programs in the humanities (see Public humanities).

NEH facilitated the creation of State Humanities Councils in the 56 U.S. states and territories. Each council operates independently, defining the "humanities" in relationship to the disciplines, subjects, and values valued in the regions they serve. Councils give grant funds to individuals, scholars, and nonprofit organizations dedicated to the humanities in their region. Councils also offer diverse programs and services that respond to the needs of their communities and according to their own definitions of the humanities.

Criticism of the traditional humanities/liberal arts degree program has been leveled by critics who see them as both expensive and relatively "useless" in the modern American job market, where several years of specialized study is required in most job fields. According to a 2018 report by the Humanities Indicators, unemployment rates for humanities majors were slightly higher and their earnings were slightly lower than the averages for college degree recipients with similar degree levels (though both were still substantially better than for those without a college degree). Their overall levels of satisfaction with their jobs and their lives, however, were essentially the same as graduates from other fields, with more than 85% of humanities graduates reporting they were satisfied with their jobs. As of 2015, approximately five million people employed in management and professional jobs had bachelor’s degrees in the humanities.




</doc>
<doc id="16799902" url="https://en.wikipedia.org/wiki?curid=16799902" title="Hprints">
Hprints

hprints (pronounced in English as aitch prints) is an archive for electronic preprints of academic papers in the fields of arts and humanities. It can be accessed freely via the Internet since it is an open access repository aiming at making scholarly documents publicly available to the widest possible audience.

The aim of hprints is to make Nordic research available through an open access online electronic full text archive, but the limitation to Nordic countries is claimed to be mainly an initial restriction for funding reasons. The archive will primarily contain electronic research documents in the form of preprints, reprints, working papers, book chapters, conference reports, invited lecture manuscripts etc. The archive is set up, maintained and promoted by Copenhagen University Library and consortium members. The consortium original members were:
Submissions of electronic text material to the archive is decentral and take place at the local individual researcher, or research group level.

Hence hprints is a tool for scientific communication between academic scholars, who can upload full-text research material such as articles, papers, conference papers, book chapters etc. The content of the deposited material should be comparable to that of a scientific paper that a scholar would consider suitable for publication in for example a peer reviewed scientific journal.

It is possible to search and find the paper by defined topics through an Internet search. Secondly, all submitted papers are stored permanently and receive a stable web address, as for example the paper in this example:

May 2007 the Nordic funding agency for libraries, Nordbib, granted the hprints project 287,000 DKK as part of its financing programme "Work Package 2: Focus area on Content and Accessibility". The plan was to launch an archive one year from this date i.e. approximately June 2008: The hprints project wishes to provide a policy and a technical infrastructure that permits open access to research within the arts and humanities. The assumption was that this will result in a number of advantages with respect to the electronic accessibility and visibility of the arts and humanities research area.

October 2007, the Advisory Board of the Nordbib "hprints project" chose the system to be used for the Nordic arts and humanities e-print archive. Three possible alternatives existed: EPrints from the University of Southampton, LUR from Lund University Libraries, and HAL from the French national research council (CNRS). Both EPrints and LUR are free open source software that can be set up locally or hosted commercially, while HAL is a functioning archive, to which portals can be set up.

At the hprints Advisory Board meeting in October, it was decided to collaborate with the French research council, Centre National de la Recherche Scientifique (CNRS). Hence, the hprints e-print archive for Nordic arts and humanities is set up as a Nordic HAL portal with its own layout and adapted to the requirements of hprints.

March 2008 hprints opened for public access. Since the archive is a part of HAL and papers will be shared with the French national archive.




</doc>
<doc id="17956828" url="https://en.wikipedia.org/wiki?curid=17956828" title="List of people considered a founder in a Humanities field">
List of people considered a founder in a Humanities field

Those known as the father, mother, or considered a founder in a Humanities field are those who have made important contributions to that field. In some fields several people are considered the founders, while in others the title of being the "father" is debatable. Some of the people who have humanity are given in REFERENCES.


</doc>
<doc id="5178" url="https://en.wikipedia.org/wiki?curid=5178" title="Classics">
Classics

Classics or classical studies is the study of classical antiquity, and in Western culture traditionally refers to the study of Classical Greek and Roman literature in their original languages of Ancient Greek and Latin, respectively. It may also include Greco-Roman philosophy, history, and archaeology as secondary subjects. Traditionally in the West, the study of the Greek and Roman classics was considered one of the cornerstones of the humanities and a fundamental element of a rounded education. The study of classics has therefore traditionally been a cornerstone of a typical elite European education.

Study encompasses specifically a time-period of history from the mid-2nd millennium BC to the 6th century AD.
This term is used for novels, TV shows and paperback documents also in a formal way. Hardbound books are also parts of modern and Athenian classics.

Classics also means for best fictional novels, crime documents, best Tv shows and excellent paperback books

The word "classics" is derived from the Latin adjective "classicus", meaning "belonging to the highest class of citizens". The word was originally used to describe the members of the highest class in ancient Rome. By the 2nd century AD the word was used in literary criticism to describe writers of the highest quality. For example, Aulus Gellius, in his "Attic Nights", contrasts "classicus" and "proletarius" writers. By the 6th century AD, the word had acquired a second meaning, referring to pupils at a school. Thus the two modern meanings of the word, referring both to literature considered to be of the highest quality, and to the standard texts used as part of a curriculum, both derive from Roman use.

In the Middle Ages, classics and education were tightly intertwined; according to Jan Ziolkowski, there is no era in history in which the link was tighter. Medieval education taught students to imitate earlier classical models, and Latin continued to be the language of scholarship and culture, despite the increasing difference between literary Latin and the vernacular languages of Europe during the period.

While Latin was hugely influential, however, Greek was barely studied, and Greek literature survived almost solely in Latin translation. The works of even major Greek authors such as Hesiod, whose names continued to be known by educated Europeans, were unavailable in the Middle Ages. In the thirteenth century, the English philosopher Roger Bacon wrote that "there are not four men in Latin Christendom who are acquainted with the Greek, Hebrew, and Arabic grammars."

Along with the unavailability of Greek authors, there were other differences between the classical canon known today and the works valued in the Middle Ages. Catullus, for instance, was almost entirely unknown in the medieval period. The popularity of different authors also waxed and waned throughout the period: Lucretius, popular during the Carolingian period, was barely read in the twelfth century, while for Quintilian the reverse is true.

The Renaissance led to the increasing study of both ancient literature and ancient history, as well as a revival of classical styles of Latin. From the 14th century, first in Italy and then increasingly across Europe, Renaissance Humanism, an intellectual movement that "advocated the study and imitation of classical antiquity", developed. Humanism saw a reform in education in Europe, introducing a wider range of Latin authors as well as bringing back the study of Greek language and literature to Western Europe. This reintroduction was initiated by Petrarch (1304–1374) and Boccaccio (1313–1375) who commissioned a Calabrian scholar to translate the Homeric poems. This humanist educational reform spread from Italy, in Catholic countries as it was adopted by the Jesuits, and in countries that became Protestant such as England, Germany, and the Low Countries, in order to ensure that future clerics were able to study the New Testament in the original language.

The late 17th and 18th centuries are the period in Western European literary history which is most associated with the classical tradition, as writers consciously adapted classical models. Classical models were so highly prized that the plays of William Shakespeare were rewritten along neoclassical lines, and these "improved" versions were performed throughout the 18th century.

From the beginning of the 18th century, the study of Greek became increasingly important relative to that of Latin.
In this period Johann Winckelmann's claims for the superiority of the Greek visual arts influenced a shift in aesthetic judgements, while in the literary sphere, G.E. Lessing "returned Homer to the centre of artistic achievement".
In the United Kingdom, the study of Greek in schools began in the late 18th century. The poet Walter Savage Landor claimed to have been one of the first English schoolboys to write in Greek during his time at Rugby School.

The 19th century saw the influence of the classical world, and the value of a classical education, decline, especially in the US, where the subject was often criticised for its elitism. By the 19th century, little new literature was still being written in Latin – a practice which had continued as late as the 18th century – and a command of Latin declined in importance. Correspondingly, classical education from the 19th century onwards began to increasingly de-emphasise the importance of the ability to write and speak Latin. In the United Kingdom this process took longer than elsewhere. Composition continued to be the dominant classical skill in England until the 1870s, when new areas within the discipline began to increase in popularity.
In the same decade came the first challenges to the requirement of Greek at the universities of Oxford and Cambridge, though it would not be finally abolished for another 50 years.

Though the influence of classics as the dominant mode of education in Europe and North America was in decline in the 19th century, the discipline was rapidly evolving in the same period. Classical scholarship was becoming more systematic and scientific, especially with the "new philology" created at the end of the 18th and beginning of the 19th century. Its scope was also broadening: it was during the 19th century that ancient history and classical archaeology began to be seen as part of classics rather than separate disciplines.

During the 20th century, the study of classics became less common. In England, for instance, Oxford and Cambridge universities stopped requiring students to have qualifications in Greek in 1920, and in Latin at the end of the 1950s. When the National Curriculum was introduced in England, Wales, and Northern Ireland in 1988, it did not mention the classics. By 2003, only about 10% of state schools in Britain offered any classical subjects to their students at all. In 2016, AQA, the largest exam board for A-Levels and GCSE's in England, Wales and Northern Ireland, announced that it would be scrapping A-Level subjects in Classical Civilization, Archaeology, and Art History. This left just one out of five exam boards in England which still offered Classical Civilization as a subject. The decision was immediately denounced by archaeologists and historians, with Natalie Haynes of the "Guardian" stating that the loss of the A-Level would deprive state school students, 93% of all students, the opportunity to study classics while making it once again the exclusive purview of wealthy private-school students.

However, the study of classics has not declined as fast elsewhere in Europe. In 2009, a review of "Meeting the Challenge", a collection of conference papers about the teaching of Latin in Europe, noted that though there is opposition to the teaching of Latin in Italy, it is nonetheless still compulsory in most secondary schools. The same can be said in the case of France or Greece, too. Indeed, Ancient Greek is one of the compulsory subjects in Greek secondary education, whereas in France, Latin is one of the optional subjects that can be chosen in a majority of middle schools and high schools. Ancient Greek is also still being taught, but not as much as Latin.

One of the most notable characteristics of the modern study of classics is the diversity of the field. Although traditionally focused on ancient Greece and Rome, the study now encompasses the entire ancient Mediterranean world, thus expanding the studies to Northern Africa as well as parts of the Middle East.

Philology is the study of language preserved in written sources; classical philology is thus concerned with understanding any texts from the classical period written in the classical languages of Latin and Greek.
The roots of classical philology lie in the Renaissance, as humanist intellectuals attempted to return to the Latin of the classical period, especially of Cicero, and as scholars attempted to produce more accurate editions of ancient texts. Some of the principles of philology still used today were developed during this period, for instance, the observation that if a manuscript could be shown to be a copy of an earlier extant manuscript, then it provides no further evidence of the original text, was made as early as 1489 by Angelo Poliziano. Other philological tools took longer to be developed: the first statement, for instance, of the principle that a more difficult reading should be preferred over a simpler one, was in 1697 by Jean Le Clerc.

The modern discipline of classical philology began in Germany at the turn of the nineteenth century. It was during this period that scientific principles of philology began to be put together into a coherent whole, in order to provide a set of rules by which scholars could determine which manuscripts were most accurate. This "new philology", as it was known, centred around the construction of a genealogy of manuscripts, with which a hypothetical common ancestor, closer to the original text than any existing manuscript, could be reconstructed.

Classical archaeology is the oldest branch of archaeology, with its roots going back to J.J. Winckelmann's work on Herculaneum in the 1760s. It was not until the last decades of the 19th century, however, that classical archaeology became part of the tradition of Western classical scholarship. It was included as part of Cambridge University's Classical Tripos for the first time after the reforms of the 1880s, though it did not become part of Oxford's Greats until much later.

The second half of the 19th century saw Schliemann's excavations of Troy and Mycenae; the first excavations at Olympia and Delos; and Arthur Evans' work in Crete, particularly on Knossos. This period also saw the foundation of important archaeological associations (e.g. the Archaeological Institute of America in 1879), including many foreign archaeological institutes in Athens and Rome (the American School of Classical Studies at Athens in 1881, British School at Athens in 1886, American Academy in Rome in 1895, and British School at Rome in 1900).

More recently, classical archaeology has taken little part in the theoretical changes in the rest of the discipline, largely ignoring the popularity of "New Archaeology", which emphasised the development of general laws derived from studying material culture, in the 1960s. New Archaeology is still criticized by traditional minded scholars of classical archaeology despite a wide acceptance of its basic techniques.

Some art historians focus their study on the development of art in the classical world. Indeed, the art and architecture of Ancient Rome and Greece is very well regarded and remains at the heart of much of our art today. For example, Ancient Greek architecture gave us the Classical Orders: Doric, Ionic, and Corinthian. The Parthenon is still the architectural symbol of the classical world.

Greek sculpture is well known and we know the names of several Ancient Greek artists: for example, Phidias.

With philology, archaeology, and art history, scholars seek understanding of the history and culture of a civilisation, through critical study of the extant literary and physical artefacts, in order to compose and establish a continual historic narrative of the Ancient World and its peoples. The task is difficult due to a dearth of physical evidence: for example, Sparta was a leading Greek city-state, yet little evidence of it survives to study, and what is available comes from Athens, Sparta's principal rival; likewise, the Roman Empire destroyed most evidence (cultural artefacts) of earlier, conquered civilizations, such as that of the Etruscans.

The English word "philosophy" comes from the Greek word φιλοσοφία, meaning "love of wisdom", probably coined by Pythagoras. Along with the word itself, the discipline of philosophy as we know it today has its roots in ancient Greek thought, and according to Martin West "philosophy as we understand it is a Greek creation". Ancient philosophy was traditionally divided into three branches: logic, physics, and ethics. However, not all of the works of ancient philosophers fit neatly into one of these three branches. For instance, Aristotle's "Rhetoric" and "Poetics" have been traditionally classified in the West as "ethics", but in the Arabic world were grouped with logic; in reality, they do not fit neatly into either category.

From the last decade of the eighteenth century, scholars of ancient philosophy began to study the discipline historically. Previously, works on ancient philosophy had been unconcerned with chronological sequence and with reconstructing the reasoning of ancient thinkers; with what Wolfgang-Ranier Mann calls "New Philosophy", this changed.

A relatively recent new discipline within the classics is "reception studies", which developed in the 1960s at the University of Konstanz.
Reception studies is concerned with how students of classical texts have understood and interpreted them.
As such, reception studies is interested in a two-way interaction between reader and text, taking place within a historical context.

Though the idea of an "aesthetics of reception" was first put forward by Hans Robert Jauss in 1967, the principles of reception theory go back much earlier than this.
As early as 1920, T. S. Eliot wrote that "the past [is] altered by the present as much as the present is directed by the past"; Charles Martindale describes this as a "cardinal principle" for many versions of modern reception theory.

Ancient Greece was the civilization belonging to the period of Greek history lasting from the Archaic period, beginning in the eighth century BC, to the Roman conquest of Greece after the Battle of Corinth in 146 BC. The Classical period, during the fifth and fourth centuries BC, has traditionally been considered the height of Greek civilisation. The Classical period of Greek history is generally considered to have begun with the first and second Persian invasions of Greece at the start of the Greco-Persian wars, and to have ended with the death of Alexander the Great.

Classical Greek culture had a powerful influence on the Roman Empire, which carried a version of it to many parts of the Mediterranean region and Europe; thus Classical Greece is generally considered to be the seminal culture which provided the foundation of Western civilization.

Ancient Greek is the historical stage in the development of the Greek language spanning the Archaic (c. 8th to 6th centuries BC), Classical (c. 5th to 4th centuries BC), and Hellenistic (c. 3rd century BC to 6th century AD) periods of ancient Greece and the ancient world. It is predated in the 2nd millennium BC by Mycenaean Greek. Its Hellenistic phase is known as Koine ("common") or Biblical Greek, and its late period mutates imperceptibly into Medieval Greek. Koine is regarded as a separate historical stage of its own, although in its earlier form it closely resembles Classical Greek. Prior to the Koine period, Greek of the classical and earlier periods included several regional dialects.

Ancient Greek was the language of Homer and of classical Athenian historians, playwrights, and philosophers. It has contributed many words to the vocabulary of English and many other European languages, and has been a standard subject of study in Western educational institutions since the Renaissance. Latinized forms of Ancient Greek roots are used in many of the scientific names of species and in other scientific terminology.

The earliest surviving works of Greek literature are epic poetry. Homer's "Iliad" and "Odyssey" are the earliest to survive to us today, probably composed in the eighth century BC. These early epics were oral compositions, created without the use of writing.
Around the same time that the Homeric epics were composed, the Greek alphabet was introduced; the earliest surviving inscriptions date from around 750 BC.
European drama was invented in ancient Greece. Traditionally this was attributed to Thespis, around the middle of the sixth century BC, though the earliest surviving work of Greek drama is Aeschylus' tragedy "The Persians", which dates to 472 BC. Early Greek tragedy was performed by a chorus and two actors, but by the end of Aeschylus' life, a third actor had been introduced, either by him or by Sophocles. The last surviving Greek tragedies are the "Bacchae" of Euripides and Sophocles' Oedipus at Colonus, both from the end of the fifth century BC.

Surviving Greek comedy begins later than tragedy; the earliest surviving work, Aristophanes' "Acharnians", comes from 425 BC. However, comedy dates back as early as 486 BC, when the Dionysia added a competition for comedy to the much earlier competition for tragedy. The comedy of the fifth century is known as Old Comedy, and it comes down to us solely in the eleven surviving plays of Aristophanes, along with a few fragments. Sixty years after the end of Aristophanes' career, the next author of comedies to have any substantial body of work survive is Menander, whose style is known as New Comedy.

Two historians flourished during Greece's classical age: Herodotus and Thucydides. Herodotus is commonly called the father of history, and his "History" contains the first truly literary use of prose in Western literature. Of the two, Thucydides was the more careful historian. His critical use of sources, inclusion of documents, and laborious research made his History of the Peloponnesian War a significant influence on later generations of historians. The greatest achievement of the 4th century was in philosophy. There were many Greek philosophers, but three names tower above the rest: Socrates, Plato, and Aristotle. These have had a profound influence on Western society.

Greek mythology is the body of myths and legends belonging to the ancient Greeks concerning their gods and heroes, the nature of the world, and the origins and significance of their own cult and ritual practices. They were a part of religion in ancient Greece. Modern scholars refer to the myths and study them in an attempt to throw light on the religious and political institutions of Ancient Greece and its civilization, and to gain understanding of the nature of myth-making itself.

Greek religion encompassed the collection of beliefs and rituals practiced in ancient Greece in the form of both popular public religion and cult practices. These different groups varied enough for it to be possible to speak of Greek religions or "cults" in the plural, though most of them shared similarities. Also, the Greek religion extended out of Greece and out to neighbouring islands.

Many Greek people recognized the major gods and goddesses: Zeus, Poseidon, Hades, Apollo, Artemis, Aphrodite, Ares, Dionysus, Hephaestus, Athena, Hermes, Demeter, Hestia and Hera; though philosophies such as Stoicism and some forms of Platonism used language that seems to posit a transcendent single deity. Different cities often worshipped the same deities, sometimes with epithets that distinguished them and specified their local nature.

The earliest surviving philosophy from ancient Greece dates back to the 6th century BC, when according to Aristotle Thales of Miletus was considered to have been the first Greek philosopher. Other influential pre-Socratic philosophers include Pythagoras and Heraclitus. The most famous and significant figures in classical Athenian philosophy, from the 5th to the 3rd centuries BC, are Socrates, his student Plato, and Aristotle, who studied at Plato's Academy before founding his own school, known as the Lyceum. Later Greek schools of philosophy, including the Cynics, Stoics, and Epicureans, continued to be influential after the Roman annexation of Greece, and into the post-Classical world.

Greek philosophy dealt with a wide variety of subjects, including political philosophy, ethics, metaphysics, ontology, and logic, as well as disciplines which are not today thought of as part of philosophy, such as biology and rhetoric.

The language of ancient Rome was Latin, a member of the Italic family of languages. The earliest surviving inscription in Latin comes from the 7th century BC, on a brooch from Palestrina. Latin from between this point and the early 1st century BC is known as Old Latin. Most surviving Latin literature is Classical Latin, from the 1st century BC to the 2nd century AD. Latin then evolved into Late Latin, in use during the late antique period. Late Latin survived long after the end of classical antiquity, and was finally replaced by written Romance languages around the 9th century AD. Along with literary forms of Latin, there existed various vernacular dialects, generally known as Vulgar Latin, in use throughout antiquity. These are mainly preserved in sources such as graffiti and the Vindolanda tablets.

The earliest surviving Latin authors, writing in Old Latin, include the playwrights Plautus and Terence. Much of the best known and most highly thought of Latin literature comes from the classical period, with poets such as Virgil, Horace, and Ovid; historians such as Julius Caesar and Tacitus; orators such as Cicero; and philosophers such as Seneca the Younger and Lucretius. Late Latin authors include many Christian writers such as Lactantius, Tertullian and Ambrose; non-Christian authors, such as the historian Ammianus Marcellinus, are also preserved.

According to legend, the city of Rome was founded in 753 BC; in reality, there had been a settlement on the site since around 1000 BC, when the Palatine Hill was settled. The city was originally ruled by kings, first Roman, and then Etruscan – according to Roman tradition, the first Etruscan king of Rome, Tarquinius Priscus, ruled from 616 BC. Over the course of the 6th century BC, the city expanded its influence over the entirety of Latium. Around the end of the 6th century – traditionally in 510 BC – the kings of Rome were driven out, and the city became a republic.

Around 387 BC, Rome was sacked by the Gauls following the Battle of the Allia. It soon recovered from this humiliating defeat, however, and in 381 the inhabitants of Tusculum in Latium were made Roman citizens. This was the first time Roman citizenship was extended in this way. Rome went on to expand its area of influence, until by 269 the entirety of the Italian peninsula was under Roman rule. Soon afterwards, in 264, the First Punic War began; it lasted until 241. The Second Punic War began in 218, and by the end of that year, the Carthaginian general Hannibal had invaded Italy. The war saw Rome's worst defeat to that point at Cannae; the largest army Rome had yet put into the field was wiped out, and one of the two consuls leading it was killed. However, Rome continued to fight, annexing much of Spain and eventually defeating Carthage, ending her position as a major power and securing Roman preeminence in the Western Mediterranean.
The classical languages of the Ancient Mediterranean world influenced every European language, imparting to each a learned vocabulary of international application. Thus, Latin grew from a highly developed cultural product of the Golden and Silver eras of Latin literature to become the "international lingua franca" in matters diplomatic, scientific, philosophic and religious, until the 17th century. Long before this, Latin had evolved into the Romance languages and Ancient Greek into Modern Greek and its dialects. In the specialised science and technology vocabularies, the influence of Latin and Greek is notable. Ecclesiastical Latin, the Roman Catholic Church's official language, remains a living legacy of the classical world in the contemporary world.

Latin had an impact far beyond the classical world. It continued to be the pre-eminent language for serious writings in Europe long after the fall of the Roman empire. The modern Romance languages – such as French, Spanish, and Italian – all derive from Latin. Latin is still seen as a foundational aspect of European culture.

The legacy of the classical world is not confined to the influence of classical languages. The Roman empire was taken as a model by later European empires, such as the Spanish and British empires. Classical art has been taken as a model in later periods – medieval Romanesque architecture and Enlightenment-era neoclassical literature were both influenced by classical models, to take but two examples, while Joyce's "Ulysses" is one of the most influential works of twentieth century literature.





</doc>
<doc id="22101040" url="https://en.wikipedia.org/wiki?curid=22101040" title="Caucasology">
Caucasology

Caucasology, or Caucasiology refers to the historical and geopolitical studies of Caucasus region. The branch has more than 150 years history. In 1972, the Caucasiological Center (renamed to International Caucasiological Center in 2000) was founded under the auspices of the Israel President Zalman Shazar.




</doc>
<doc id="1828083" url="https://en.wikipedia.org/wiki?curid=1828083" title="Linguistic turn">
Linguistic turn

The linguistic turn was a major development in Western philosophy during the early 20th century, the most important characteristic of which is the focusing of philosophy and the other humanities primarily on the relations between language, language users, and the world.

Very different intellectual movements were associated with the "linguistic turn", although the term itself is commonly thought to have been popularised by Richard Rorty's 1967 anthology "The Linguistic Turn", in which he discusses the turn towards linguistic philosophy. According to Rorty, who later dissociated himself from linguistic philosophy and analytic philosophy generally, the phrase "the linguistic turn" originated with philosopher Gustav Bergmann.

Traditionally, the linguistic turn is taken to also mean the birth of analytic philosophy. One of the results of the linguistic turn was an increasing focus on logic and philosophy of language, and the cleavage between ideal language philosophy and ordinary language philosophy.

According to Michael Dummett, the linguistic turn can be dated to Gottlob Frege's 1884 work "The Foundations of Arithmetic", specifically paragraph 62 where Frege explores the identity of a numerical proposition. 

In order to answer a Kantian question about numbers, "How are numbers given to us, granted that we have no idea or intuition of them?" Frege invokes his "context principle", stated at the beginning of the book, that only in the context of a proposition do words have meaning, and thus finds the solution to be in defining "the sense of a proposition in which a number word occurs." Thus an ontological and epistemological problem, traditionally solved along idealist lines, is instead solved along linguistic ones.

This concern for the logic of propositions and their relationship to "facts" was later taken up by the notable analytic philosopher Bertrand Russell in "On Denoting", and played a weighty role in his early work in logical atomism.

Ludwig Wittgenstein, an associate of Russell, was one of the progenitors of the linguistic turn. This follows from his ideas in his "Tractatus Logico-Philosophicus" that philosophical problems arise from a misunderstanding of the logic of language, and from his remarks on language games in his later work. His later work (specifically "Philosophical Investigations") significantly departs from the common tenets of analytic philosophy and might be viewed as having some resonance in the post-structuralist tradition.

W. V. O. Quine describes the historical continuity of the linguistic turn with earlier philosophy in "Two Dogmas of Empiricism": "Meaning is what essence becomes when it is divorced from the object of reference and wedded to the word."

Later in the twentieth century, philosophers like Saul Kripke in "Naming and Necessity" drew metaphysical conclusions from closely analyzing language.

Decisive for the linguistic turn in the humanities were the works of yet another tradition, namely the continental structuralism of Ferdinand de Saussure, an approach introduced in his "Cours de linguistique générale", published posthumously in 1916. He said language is a system of signs, comparable to writing systems, sign systems used by the deaf, and systems of symbolic rites and can therefore by studied systematically. He proposed the new science semiology—from the Greek "semeion" meaning the sign. It was later called semiotics, the science of signs. Prior to the work of Saussure in the early twentieth century, linguistics focused mainly on etymology, an historical analysis (also called a diachronic analysis) tracing the history of the meanings of individual words. Saussure was critical of the comparative philologists of the 19th century, who—basing their investigations only on Indo-European languages—whose conclusions, he said, had "no basis in reality." At that time "language was to be a "fourth natural kingdom." Saussure approached language by examining the present functioning of language (a synchronic analysis)—a relational approach in which he looked at the "system of relations between words as the source of meanings." Sassure described the synchronic, as the static side of the science of linguistics, in contrast to the diachronic, which has to do with evolution. By comparing different languages, Saussure demonstrated that there is "no fixed bond" between the signified—for example the real chair—and the signifier—the 'chair', 'chaise', etc. Spontaneous expressions of reality are not dictated by "natural forces". Saussure demonstrated the grammatical consequences of phonetic evolution by illustrating how diachronic facts take on different forms, for example "chaise" 'chair' and "chair" 'desk', and "chaire" 'pulpit'. 

Saussure held that definitions of concepts cannot exist independently from a linguistic system defined by difference, or, to put it differently, that a concept of something cannot exist without being named. Thus differences between meanings structure our perception; there is no "real" chair except insofar as we are manipulating symbolic systems. We would not even be able to recognize a chair "as" a chair without simultaneously recognising that a chair is "not" everything else - in other words a chair is defined as being a specific collection of characteristics which are themselves defined in certain ways, and so on, and all of this within the symbolic system of language. Thus, a large part of what we think of as "reality" is really a convention of naming and characterising, a convention which is itself called "language".

Structuralism was the initial outcome of Saussure's linguistic turn, which later led to poststructuralism with the input of Friedrich Nietzsche's ideas. Influential poststructuralist theorists include Judith Butler, Luce Irigaray, Julia Kristeva, Gilles Deleuze, Michel Foucault and Jacques Derrida. The power of language, more specifically of certain metahistorical tropes, in historical discourse was explored by Hayden White.

These various movements often lead to the notion that language 'constitutes' reality, a position contrary to intuition and to most of the Western tradition of philosophy. The traditional view (what Derrida called the 'metaphysical' core of Western thought) saw words as functioning labels attached to concepts. According to this view, there is something like 'the "real" chair', which exists in some external reality and corresponds roughly with a concept in human thought, "chair", to which the linguistic word "chair" refers.





</doc>
<doc id="1072404" url="https://en.wikipedia.org/wiki?curid=1072404" title="World community">
World community

The term world community is used primarily in political and humanitarian contexts to describe an international aggregate of nation states of widely varying types. In most connotations, the term is used to convey meanings attached to consensus or inclusion of all people in all lands and their governments.

World community often is a semi-personal rhetorical connotation that represents Humanity in a singular context as in "…for the sake of the World Community" or "…with the approval of the World Community".

The term sometimes is used to reference the United Nations or its affiliated agencies as bodies of governance. Other times it is a generic term with no explicit ties to states or governments but retaining a political connotation.

In terms of human needs, humanitarian aid, human rights, and other discourse in the humanities, the world community is akin to the conceptual Global village aimed at the inclusion of non-aligned countries, aboriginal peoples, the Third World into the "connected" world via the communications infrastructure or at least representative ties to it.

In terms of the World economy, the "world community" is seen by some economists as an inter-dependent system of goods and services with semi-permeable boundaries and flexible sets of import/export rules. Proponents of Globalization may tend to establish or impose more rigidity to this framework. Controversy has arisen as to whether this paradigm will strengthen or weaken the world as a community. See World Trade Organization

When considering Sustainable development and Ecology, the inter-dependence angle generally expands quickly to a Global context. In this paradigm, the planet as a whole represents a single Biome and the World's population represents the Ecological succession in a singular eco-system. This also can be recognized as the World Community.

Many religions have taken on the challenge of establishing a world community through the propagation of their doctrine, faith and practice.

In the Bahá'í Faith, `Abdu'l-Bahá, successor and son of Bahá'u'lláh, produced a series of documents called the Tablets of the Divine Plan. Now in a book form, after their publication in 1917 and their 'unveiling' in New York in 1919, these tablets contained an outline and a plan for the expansion of the Bahá'í community into Asia, Asia minor, Europe and the Americas, indeed, throughout the planet.

The formal implementation of this plan, known as 'Abdu'l-Baha's Divine Plan, began in 1937 in the first of a series of sub-plans, the Seven Year Plan of 1937 to 1944. Shoghi Effendi, the leader of the Baha'i community until 1957 and then the Universal House of Justice from 1963, were instrumental in the organization and design of future sub-plans. This led to the creation and establishment of a world community, with members of the faith estimated to have reached 5 to 6 million in the early 21st century, while also being the second most geographically widespread religion in the world.

In Buddhism "the conventional Sangha of monks has been entrusted by the Buddha with the task of leading all people in creating the ideal world community of noble disciples or truly civilized people."

A Benedictine monk, Friar John Main, inspired the World Community for Christian meditation through the practice of meditation centered around the Maranatha mantra, meaning "Come Lord."

The Lutheran Church in America had issued a social statement - "World Community: Ethical Imperatives in an Age of Interdependence" Adopted by the Fifth Biennial Convention, Minneapolis, Minnesota, June 25-July 2, 1970. Since then The Evangelical Lutheran Church in America has formed and retained the statement as a 'historical document'.

The term world community is often used in the context of establishing and maintaining world peace through a peace process or through a resolvable end to local-regional wars and global-world wars. Many social movements and much political theory deals with issues revolving around the institutionalization of the process of propagating the ideal of a world community. A world community is one which has a global vision, is established throughout the world, that is it has a membership that exists in most of the countries on the planet and that involves the participation of its members in a variety of ways.




</doc>
<doc id="51319" url="https://en.wikipedia.org/wiki?curid=51319" title="Intellectual history">
Intellectual history

Intellectual history refers to the history of ideas and thinkers. This history cannot be considered without the knowledge of the humans who created, discussed, wrote about, and in other ways were concerned with ideas. Intellectual history as practiced by historians is parallel to the history of philosophy as done by philosophers, and is more akin to the history of ideas. Its central premise is that ideas do not develop in isolation from the people who developed and use them, and that one must study ideas not only as abstract propositions but also in terms of the culture, lives, and historical contexts.

Intellectual history aims to understand ideas from the past by putting them in context. The term "context" in the preceding sentence is ambiguous: it can be political, cultural, intellectual, and social. One can read a text both in terms of a chronological context (for example, as a contribution to a discipline or tradition as it extended over time) or in terms of a contemporary intellectual moment (for example, as participating in a debate particular to a certain time and place). Both of these acts of contextualization are typical of what intellectual historians do, nor are they exclusive. Generally speaking, intellectual historians seek to place concepts and texts from the past in multiple contexts.

It is important to realize that intellectual history is not just the history of intellectuals. It studies ideas as they are expressed in texts, and as such is different from other forms of cultural history which deal also with visual and other non-verbal forms of evidence. Any written trace from the past can be the object of intellectual history. The concept of the "intellectual" is relatively recent, and suggests someone professionally concerned with thought. Instead, anyone who has put pen to paper to explore his or her thoughts can be the object of intellectual history. A famous example of an intellectual history of a non-canonical thinker is Carlo Ginzburg's study of a 16th-century Italian miller, Menocchio, in his seminal work "The Cheese and the Worms".

Although the field emerged from European disciplines of "Kulturgeschichte" and "Geistesgeschichte", the historical study of ideas has engaged not only western intellectual traditions but others as well, including those in other parts of the world. Increasingly, historians are calling for a global intellectual history that will show the parallels and interrelations in the history of thought of all human societies. Another important trend has been the history of the book and of reading, which has drawn attention to the material aspects of how books were designed, produced, distributed, and read.

Intellectual history as a self-conscious discipline is a relatively recent phenomenon. It has precedents, however, in the history of philosophy, the history of ideas, and in cultural history as practiced since Burckhardt or Voltaire. The scholarly efforts of the eighteenth century can be traced to Francis Bacon’s call for what he termed a literary history in his "The Advancement of Learning". In economics, John Maynard Keynes (1883-1946) was both a historian of economic thought himself, and the subject of study by historians of economic thought because of the significance of the Keynesian revolution.

However, the discipline of intellectual history as it is now understood emerged in the immediate postwar period, in its earlier incarnation as "the history of ideas" under the leadership of Arthur Lovejoy, the founder of the "Journal of the History of Ideas". Since that time, Lovejoy's formulation of "unit-ideas" has been developed in different and often diverging directions, some of which more historically sensitive accounts of intellectual activity as historically situated (contextualism), and this shift is reflected in the replacement of the phrase history of ideas by "intellectual history".

Intellectual history includes the history of thought in many disciplines, such as the history of philosophy, and the history of economic thought. Analytical concepts, such as the nature of paradigms and the causes of paradigm shifts, have been borrowed from the study of other disciplines, exemplified by the use of the ideas of Thomas Kuhn as presented in his book "The Structure of Scientific Revolutions" to explain revolutions in thought in economics and other disciplines.

In Britain the history of political thought has been a particular focus since the late 1960s and is associated especially with historians at Cambridge, such as John Dunn and Quentin Skinner. They studied European political thought in its historical context, emphasizing the emergence and development of such concepts as the state and freedom. Skinner in particular is renowned for his provocative methodological essays, which were and are widely read by philosophers and practitioners of other humanistic disciplines, and did much to give prominence to the practice of intellectual history.

In the United States, intellectual history is understood more broadly to encompass many different forms of intellectual output, not just the history of political ideas, and it includes such fields as the history of historical thought, associated especially with Anthony Grafton of Princeton University and J.G.A. Pocock of Johns Hopkins University. Formalized in 2010, the History and Culture Ph.D. at Drew University is one of a few graduate programs in the US currently specializing in intellectual history, both in its American and European contexts. Despite the prominence of early modern intellectual historians (those studying the age from the Renaissance to the Enlightenment), the intellectual history of the modern period has also been the locus of intense and creative output on both sides of the Atlantic. Prominent examples of such work include Louis Menand's "" and Martin Jay's "The Dialectical Imagination".

In continental Europe, equivalents of intellectual history can be found. An example is Reinhart Koselleck’s "Begriffsgeschichte" (history of concepts), though there are methodological differences between the work of Koselleck and his followers and the work of Anglo-American intellectual historians.

In the 21st century, the field of global intellectual history has received increased attention. In 2013, Samuel Moyn and Andrew Sartori published the anthology "Global Intellectual History".

In 2016, the Routledge journal "Global Intellectual History" (ed. Richard Whatmore) was established. J. G. A. Pocock and John Dunn are among those who recently have argued for a more global approach to intellectual history in contrast to Eurocentrism.







</doc>
<doc id="26711608" url="https://en.wikipedia.org/wiki?curid=26711608" title="School of Letters">
School of Letters

The School of Letters was a summer institute and degree-granting (M.A. and Ph.D. minor) program at Indiana University, Bloomington. The School moved from Kenyon College in 1951 following the withdrawal of funding of the School of English by the Rockefeller Foundation. I.U. President Herman B. Wells obtained funding from the University and located the School under the administration of Dean John W. Ashton of the College of Arts and Sciences. The School opened under the direction of Prof. Richard B. Hudson and then transitioned to Prof. Newton P. 'Stalky' Stallknecht until his retirement and the School's dissolution in 1972.

When Indiana University moved the School from Kenyon to Bloomington they maintained John Crowe Ransom, Lionel Trilling, Philip Rahv, Austin Warren, and Allen Tate as senior fellows, all well-known literary scholars. The Kenyon School of English was founded by three senior fellows, John Crowe Ransom, F. O. Matthiessen and Lionel Trilling and was held during a summer session at Kenyon College from 1948 until 1950. The first session of the School of Letters in Bloomington ran from June 21 to August 1, 1951.

Students at the School of Letters included James M. Cox '51 (later would become a faculty member at Indiana University and the School), Martha Banta '62, Bruce Jackson '62, Paul Lauter '55, and Geoffrey H. Hartman '51. The School existed during a period of major change within the field of literary studies from the dominance of New Criticism until the rise of post-structuralism. During each session of the School high profile academics, poets, and critics were brought to Bloomington to teach seminars and deliver weekly forum lectures. These instructors included Northrop Frye, William Empson, John Berryman, Robert Lowell, Leslie Fiedler, and R. P. Blackmur.


</doc>
<doc id="81724" url="https://en.wikipedia.org/wiki?curid=81724" title="Oral tradition">
Oral tradition

Oral tradition, or oral lore, is a form of human communication wherein knowledge, art, ideas and cultural material is received, preserved and transmitted orally from one generation to another. The transmission is through speech or song and may include folktales, ballads, chants, prose or verses. In this way, it is possible for a society to transmit oral history, oral literature, oral law and other knowledge across generations without a writing system, or in parallel to a writing system. Religions such as Buddhism, Hinduism, Catholicism, and Jainism, for example, have used an oral tradition, in parallel to a writing system, to transmit their canonical scriptures, rituals, hymns and mythologies from one generation to the next.

Oral tradition is information, memories and knowledge held in common by a group of people, over many generations, and it is not the same as testimony or oral history. In a general sense, "oral tradition" refers to the recall and transmission of a specific, preserved textual and cultural knowledge through vocal utterance. As an academic discipline, it refers both to a set of objects of study and a method by which they are studied.

The study of oral tradition is distinct from the academic discipline of oral history, which is the recording of personal memories and histories of those who experienced historical eras or events. Oral tradition is also distinct from the study of orality defined as thought and its verbal expression in societies where the technologies of literacy (especially writing and print) are unfamiliar to most of the population. A folklore is a type of oral tradition, but knowledge other than folklore has been orally transmitted and thus preserved in human history.

According to John Foley, oral tradition has been an ancient human tradition found in "all corners of the world". Modern archaeology has been unveiling evidence of the human efforts to preserve and transmit arts and knowledge that depended completely or partially on an oral tradition, across various cultures:
In Asia, the transmission of folklore, mythologies as well as scriptures in ancient India, in different Indian religions, was by oral tradition, preserved with precision with the help of elaborate mnemonic techniques.; Quote: The early Buddhist texts are also generally believed to be of oral tradition, with the first by comparing inconsistencies in the transmitted versions of literature from various oral societies such as the Greek, Serbia and other cultures, then noting that the Vedic literature is too consistent and vast to have been composed and transmitted orally across generations, without being written down. According to Goody, the Vedic texts likely involved both a written and oral tradition, calling it a "parallel products of a literate society".
Australian Aboriginal culture has thrived on oral traditions and oral histories passed down through thousands of years.
In a study published in February 2020, new evidence showed that both Budj Bim and Tower Hill volcanoes erupted between 34,000 and 40,000 years ago. Significantly, this is a "minimum age constraint for human presence in Victoria", and also could be interpreted as evidence for the oral histories of the Gunditjmara people, an Aboriginal Australian people of south-western Victoria, which tell of volcanic eruptions being some of the oldest oral traditions in existence. An axe found underneath volcanic ash in 1947 had already proven that humans inhabited the region before the eruption of Tower Hill.

All ancient Greek literature, states Steve Reece, was to some degree oral in nature, and the earliest literature was completely so. Homer's epic poetry, states Michael Gagarin, was largely composed, performed and transmitted orally. As folklores and legends were performed in front of distant audiences, the singers would substitute the names in the stories with local characters or rulers to give the stories a local flavor and thus connect with the audience, but making the historicity embedded in the oral tradition as unreliable. The lack of surviving texts about the Greek and Roman religious traditions have led scholars to presume that these were ritualistic and transmitted as oral traditions, but some scholars disagree that the complex rituals in the ancient Greek and Roman civilizations were an exclusive product of an oral tradition. The Torah and other ancient Jewish literature, the Judeo-Christian Bible and texts of early centuries of Christianity are rooted in an oral tradition, and the term "People of the Book" is a medieval construct. This is evidenced, for example, by the multiple scriptural statements by Paul admitting "previously remembered tradition which he received" orally.

Writing systems are not known to exist among Native North Americans before contact with Europeans. Oral storytelling traditions flourished in a context without the use of writing to record and preserve history, scientific knowledge, and social practices. While some stories were told for amusement and leisure, most functioned as practical lessons from tribal experience applied to immediate moral, social, psychological, and environmental issues. Stories fuse fictional, supernatural, or otherwise exaggerated characters and circumstances with real emotions and morals as a means of teaching. Plots often reflect real life situations and may be aimed at particular people known by the story's audience. In this way, social pressure could be exerted without directly causing embarrassment or social exclusion. For example, rather than yelling, Inuit parents might deter their children from wandering too close to the water's edge by telling a story about a sea monster with a pouch for children within its reach. One single story could provide dozens of lessons. Stories were also used as a means to assess whether traditional cultural ideas and practices are effective in tackling contemporary circumstances or if they should be revised.

Native American storytelling is a collaborative experience between storyteller and listeners. Native American tribes generally have not had professional tribal storytellers marked by social status. Stories could and can be told by anyone, with each storyteller using their own vocal inflections, word choice, content, or form. Storytellers not only draw upon their own memories, but also upon a collective or tribal memory extending beyond personal experience but nevertheless representing a shared reality. Native languages have in some cases up to twenty words to describe physical features like rain or snow and can describe the spectra of human emotion in very precise ways, allowing storytellers to offer their own personalized take on a story based on their own lived experiences. Fluidity in story deliverance allowed stories to be applied to different social circumstances according to the storyteller's objective at the time. One's rendition of a story was often considered a response to another's rendition, with plot alterations suggesting alternative ways of applying traditional ideas to present conditions. Listeners might have heard the story told many times, or even may have told the same story themselves. This does not take away from a story's meaning, as curiosity about what happens next was less of a priority than hearing fresh perspectives on well-known themes and plots. Elder storytellers generally were not concerned with discrepancies between their version of historical events and neighboring tribes' version of similar events, such as in origin stories. Tribal stories are considered valid within the tribe's own frame of reference and tribal experience.

Stories are used to preserve and transmit both tribal history and environmental history, which are often closely linked. Native oral traditions in the Pacific Northwest, for example, describe natural disasters like earthquakes and tsunamis. Various cultures from Vancouver Island and Washington have stories describing a physical struggle between a Thunderbird and a Whale. One such story tells of the Thunderbird, which can create thunder by moving just a feather, piercing the Whale's flesh with its talons, causing the Whale to dive to the bottom of the ocean, bringing the Thunderbird with it. Another depicts the Thunderbird lifting the Whale from the Earth then dropping it back down. Regional similarities in themes and characters suggests that these stories mutually describe the lived experience of earthquakes and floods within tribal memory. According to one story from the Suquamish Tribe, Agate Pass was created when an earthquake expanded the channel as a result of an underwater battle between a serpent and bird. Other stories in the region depict the formation of glacial valleys and moraines and the occurrence of landslides, with stories being used in at least one case to identify and date earthquakes that occurred in CE 900 and 1700. Further examples include Arikara origin stories of emergence form an "underworld" of persistent darkness, which may represent the remembrance of life in the Arctic Circle during the last ice age, and stories involving a "deep crevice", which may refer to the Grand Canyon. Despite such examples of agreement between geological and archeological records on one hand and Native oral records on the other, some scholars have cautioned against the historical validity of oral traditions because of their susceptibility to detail alteration over time and lack of precise dates. The Native American Graves Protection and Repatriation Act considers oral traditions as a viable source of evidence for establishing the affiliation between cultural objects and Native Nations.

Oral traditions face the challenge of accurate transmission and verifiability of the accurate version, particularly when the culture lacks written language or has limited access to writing tools. Oral cultures have employed various strategies that achieve this without writing. For example, a heavily rhythmic speech filled with mnemonic devices enhances memory and recall. A few useful mnemonic devices include alliteration, repetition, assonance, and proverbial sayings. In addition, the verse is often metrically composed with an exact number of syllables or morae - such as with Greek and Latin prosody and in Chandas found in Hindu and Buddhist texts. The verses of the epic or text are typically designed wherein the long and short syllables are repeated by certain rules, so that if an error or inadvertent change is made, an internal examination of the verse reveals the problem. Oral Traditions are able to be passed on through means of plays and acting which can be shown in the modern day Cameroon by the Graffis or Grasslanders who act out and deliver speeches to spread their history in the manner of Oral Tradition. Such strategies help facilitate transmission of information from individual to individual without a written intermediate, and they can also be applied to oral governance.

Rudyard Kipling's "The Jungle Book" provides an excellent demonstration of oral governance in the Law of the Jungle. Not only does grounding rules in oral proverbs allow for simple transmission and understanding, but it also legitimizes new rulings by allowing extrapolation. These stories, traditions, and proverbs are not static, but are often altered upon each transmission barring the overall meaning remains intact. In this way, the rules that govern the people are modified by the whole and not authored by a single entity.

Ancient texts of Hinduism, Buddhism and Jainism were preserved and transmitted by an oral tradition. For example, the śrutis of Hinduism called the Vedas, the oldest of which trace back to the second millennium BCE. Michael Witzel explains this oral tradition as follows:

Ancient Indians developed techniques for listening, memorization and recitation of their knowledge, in schools called Gurukul, while maintaining exceptional accuracy of their knowledge across the generations. Many forms of recitation or "paths" were designed to aid accuracy in recitation and the transmission of the "Vedas" and other knowledge texts from one generation to the next. All hymns in each Veda were recited in this way; for example, all 1,028 hymns with 10,600 verses of the Rigveda was preserved in this way; as were all other Vedas including the Principal Upanishads, as well as the Vedangas. Each text was recited in a number of ways, to ensure that the different methods of recitation acted as a cross check on the other. Pierre-Sylvain Filliozat summarizes this as follows:

These extraordinary retention techniques guaranteed an accurate Śruti, fixed across the generations, not just in terms of unaltered word order but also in terms of sound. That these methods have been effective, is testified to by the preservation of the most ancient Indian religious text, the "" (ca. 1500 BCE).

Research by Milman Parry and Albert Lord indicates that the verse of the Greek poet Homer has been passed down (at least in the Serbo-Croatian epic tradition) not by rote memorization but by "Oral-formulaic composition". In this process extempore composition is aided by use of stock phrases or "formulas" (expressions that are used regularly "under the same metrical conditions, to express a particular essential idea"). In the case of the work of Homer, formulas included "eos rhododaktylos" ("rosy fingered dawn") and "oinops pontos" ("winedark sea") which fit in a modular fashion into the poetic form (in this case six-colon Greek hexameter). Since the development of this theory, of Oral-formulaic composition has been "found in many different time periods and many different cultures", and according to another source (John Miles Foley) "touch[ed] on" over 100 "ancient, medieval and modern traditions."

The most recently revealed of the world's great religions, Islam had its two major sources of divine revelation — hadith and the Quran — compiled in written form relatively shortly after being revealed:

Nonetheless, few disagree that the oral milieu the sources were revealed in, and their oral form in general were/are important. The Arab poetry that preceded the Quran and the hadith were orally transmitted. Few Arabs were literate at the time and paper was not available in the Middle East.

The written Quran is said to have been created in part from what had been memorized by Muhammad's companions, and the decision to create a standard written work is said to have come after the death in battle (Yamama) of a large number of Muslims who had memorized the work.

For centuries copies of the Qurans/"mushaf" were written by hand, not printed, and their scarcity and expense made reciting the Quran from memory, not reading, the predominant mode of teaching it to others. To this day the Quran is memorized by millions and its recitation can be heard throughout the Muslim world from recordings and mosque loudspeakers (during Ramadan). Muslims state that some who teach memorization/recitation of the Quran constitute the end of an "un-broken chain" whose original teacher was Muhammad himself.
It has been argued that "the Qur’an’s rhythmic style and eloquent expression make it easy to memorize," and was made so to facilitate the "preservation and remembrance" of the work.

Islamic doctrine holds that from the time it was revealed to the present day, the Quran has not been altered, 
its continuity from divine revelation to its current written form insured by the large numbers of Muhammad's supporters who had reverently memorized the work, a careful compiling process and divine intervention. (Muslim scholars agree that although scholars have worked hard to separate the corrupt and uncorrupted hadith, this other source of revelation is not nearly so free of corruption because of the hadith's great political and theological influence.)

But at least a couple of non-Muslim scholars (Alan Dundes and Andrew G. Bannister) have examined the possibility that the Quran was not just "recited orally, but actually composed orally". Bannister postulates that some parts of the Quran — such as the seven re-tellings of the story of the Iblis and Adam, and the repeated phrases “which of the favours of your Lord will you deny?” in sura 55 — make more sense addressed to listeners than readers.

Banister, Dundes and other scholars (Shabbir Akhtar, Angelika Neuwirth, Islam Dayeh) have also noted the large amount of "formulaic" phraseology in the Quran consistent with "oral-formulaic composition" mentioned above. The most common formulas are the attributes of Allah — all-mighty, all-wise, all-knowing, all-high, etc. — often found as doublets at the end of a verse. Among the other repeated phrases are "Allah created the heavens and the earth" (found 19 times in the Quran).

As much as one third of the Quran is made up of "oral formulas", according to Dundes' estimates. Bannister, using a computer database of (the original Arabic) words of the Quran and of their "grammatical role, root, number, person, gender and so forth", estimates that depending on the length of the phrase searched, somewhere between 52% (three word phrases) and 23% (five word phrases) are oral formulas. Dundes reckons his estimates confirm "that the Quran was orally transmitted from its very beginnings". Bannister believes his estimates "provide strong corroborative evidence that oral composition should be seriously considered as we reflect upon how the qur’anic text was generated."

Dundes argues oral-formulaic composition is consistent with "the cultural context of Arabic oral tradition", quoting researchers who have found poetry reciters in the Najd (the region next to where the Quran was revealed) using "a common store of themes, motives, stock images, phraseology and prosodical options", and "a discursive and loosely structured" style "with no fixed beginning or end" and "no established sequence in which the episodes must follow".

The Catholic Church upholds that its teaching contained in its deposit of faith is transmitted not only through scripture, but as well as through sacred tradition. The Second Vatican Council affirmed in "Dei verbum" that the teachings of Jesus Christ was initially passed on to early Christians by "the Apostles who, by their oral preaching, by example, and by observances handed on what they had received from the lips of Christ, from living with Him, and from what He did". The Catholic Church asserts that this mode of transmission of the faith persists through current-day bishops, who by right of apostolic succession, have continued the oral passing of what had been revealed through Christ through their preaching as teachers. 

The following overview draws upon "Oral-Formulaic Theory and Research: An Introduction and Annotated Bibliography", (NY: Garland Publishing, 1985, 1986, 1989); additional material is summarized from the overlapping prefaces to the following volumes: "The Theory of Oral Composition: History and Methodology", (Indiana University Press, 1988, 1992); "Immanent Art: From Structure to Meaning in Traditional Oral Epic" (Bloomington: Indiana University Press, 1991); "The Singer of Tales in Performance" (Bloomington: Indiana University Press, 1995) and "Comparative Research on Oral Traditions: A Memorial for Milman Parry" (Columbus, Ohio: Slavica Publishers, 1987). in the work of the Serb scholar Vuk Stefanović Karadžić (1787–1864), a contemporary and friend of the Brothers Grimm. Vuk pursued similar projects of "salvage folklore" (similar to rescue archaeology) in the cognate traditions of the Southern Slavic regions which would later be gathered into Yugoslavia, and with the same admixture of romantic and nationalistic interests (he considered all those speaking the Eastern Herzegovinian dialect as Serbs). Somewhat later, but as part of the same scholarly enterprise of nationalist studies in folklore, the turcologist Vasily Radlov (1837–1918) would study the songs of the Kara-Kirghiz in what would later become the Soviet Union; Karadzic and Radloff would provide models for the work of Parry.

In a separate development, the media theorist Marshall McLuhan (1911–1980) would begin to focus attention on the ways that communicative media shape the nature of the content conveyed. He would serve as mentor to the Jesuit, Walter Ong (1912–2003), whose interests in cultural history, psychology and rhetoric would result in "Orality and Literacy" (Methuen, 1980) and the important but less-known "Fighting for Life: Contest, Sexuality and Consciousness" (Cornell, 1981) These two works articulated the contrasts between cultures defined by primary orality, writing, print, and the secondary orality of the electronic age.

Ong's works also made possible an integrated theory of oral tradition which accounted for both production of content (the chief concern of Parry-Lord theory) and its reception. This approach, like McLuhan's, kept the field open not just to the study of aesthetic culture but to the way physical and behavioral artifacts of oral societies are used to store, manage and transmit knowledge, so that oral tradition provides methods for investigation of cultural differences, other than the purely verbal, between oral and literate societies.

The most-often studied section of "Orality and Literacy" concerns the "psychodynamics of orality" This chapter seeks to define the fundamental characteristics of 'primary' orality and summarizes a series of descriptors (including but not limited to verbal aspects of culture) which might be used to index the relative orality or literacy of a given text or society.

In advance of Ong's synthesis, John Miles Foley began a series of papers based on his own fieldwork on South Slavic oral genres, emphasizing the dynamics of performers and audiences. Foley effectively consolidated oral tradition as an academic field when he compiled "Oral-Formulaic Theory and Research" in 1985. The bibliography gives a summary of the progress scholars made in evaluating the oral tradition up to that point, and includes a list of all relevant scholarly articles relating to the theory of Oral-Formulaic Composition. He also both established both the journal "Oral Tradition" and founded the "Center for Studies in Oral Tradition" (1986) at the University of Missouri. Foley developed Oral Theory beyond the somewhat mechanistic notions presented in earlier versions of Oral-Formulaic Theory, by extending Ong's interest in cultural features of oral societies beyond the verbal, by drawing attention to the agency of the bard and by describing how oral traditions bear meaning.

The bibliography would establish a clear underlying methodology which accounted for the findings of scholars working in the separate Linguistics fields (primarily Ancient Greek, Anglo-Saxon and Serbo-Croatian). Perhaps more importantly, it would stimulate conversation among these specialties, so that a network of independent but allied investigations and investigators could be established.

Foley's key works include "The Theory of Oral Composition" (1988); "Immanent Art" (1991); "Traditional Oral Epic: The Odyssey, Beowulf and the Serbo-Croatian Return-Song" (1993); "The Singer of Tales in Performance" (1995); "Teaching Oral Traditions" (1998); "How to Read an Oral Poem" (2002). His Pathways Project (2005–2012) draws parallels between the media dynamics of oral traditions and the Internet.

The theory of oral tradition would undergo elaboration and development as it grew in acceptance. While the number of formulas documented for various traditions proliferated, the concept of the formula remained lexically-bound. However, numerous innovations appeared, such as the "formulaic system" with structural "substitution slots" for syntactic, morphological and narrative necessity (as well as for artistic invention). Sophisticated models such as Foley's "word-type placement rules" followed. Higher levels of formulaic composition were defined over the years, such as "ring composition", "responsion" and the "type-scene" (also called a "theme" or "typical scene"). Examples include the "Beasts of Battle" and the "Cliffs of Death". Some of these characteristic patterns of narrative details, (like "the arming sequence;" "the hero on the beach"; "the traveler recognizes his goal") would show evidence of global distribution.

At the same time, the fairly rigid division between oral and literate was replaced by recognition of transitional and compartmentalized texts and societies, including models of diglossia (Brian Stock Franz Bäuml, and Eric Havelock). Perhaps most importantly, the terms and concepts of "orality" and "literacy" came to be replaced with the more useful and apt "traditionality" and "textuality". Very large units would be defined (The Indo-European Return Song) and areas outside of military epic would come under investigation: women's song, riddles and other genres.

The methodology of oral tradition now conditions a large variety of studies, not only in folklore, literature and literacy, but in philosophy, communication theory, Semiotics, and including a very broad and continually expanding variety of languages and ethnic groups, and perhaps most conspicuously in biblical studies, in which Werner Kelber has been especially prominent. The annual bibliography is indexed by 100 areas, most of which are ethnolinguistic divisions.

Present developments explore the implications of the theory for rhetoric and composition, interpersonal communication, cross-cultural communication, postcolonial studies, rural community development, popular culture and film studies, and many other areas. The most significant areas of theoretical development at present may be the construction of systematic hermeneutics and aesthetics specific to oral traditions.

The theory of oral tradition encountered early resistance from scholars who perceived it as potentially supporting either one side or another in the controversy between what were known as "unitarians" and "analysts" – that is, scholars who believed Homer to have been a single, historical figure, and those who saw him as a conceptual "author function," a convenient name to assign to what was essentially a repertoire of traditional narrative. A much more general dismissal of the theory and its implications simply described it as "unprovable" Some scholars, mainly outside the field of oral tradition, represent (either dismissively or with approval) this body of theoretical work as reducing the great epics to children's party games like "telephone" or "Chinese whispers". While games provide amusement by showing how messages distort content via uncontextualized transmission, Parry's supporters argue that the theory of oral tradition reveals how oral methods optimized the signal-to-noise ratio and thus improved the quality, stability and integrity of content transmission.

There were disputes concerning particular findings of the theory. For example, those trying to support or refute Crowne's hypothesis found the "Hero on the Beach" formula in numerous Old English poems. Similarly, it was also discovered in other works of Germanic origin, Middle English poetry, and even an Icelandic prose saga. J.A. Dane, in an article characterized as "polemics without rigor" claimed that the appearance of the theme in Ancient Greek poetry, a tradition without known connection to the Germanic, invalidated the notion of "an autonomous theme in the baggage of an oral poet."

Within Homeric studies specifically, Lord's "The Singer of Tales", which focused on problems and questions that arise in conjunction with applying oral-formulaic theory to problematic texts such as the "Iliad", "Odyssey", and even "Beowulf", influenced nearly all of the articles written on Homer and oral-formulaic composition thereafter. However, in response to Lord, Geoffrey Kirk published "The Songs of Homer", questioning Lord's extension of the oral-formulaic nature of Serbian and Croatian literature (the area from which the theory was first developed) to Homeric epic. Kirk argues that Homeric poems differ from those traditions in their "metrical strictness", "formular system[s]", and creativity. In other words, Kirk argued that Homeric poems were recited under a system that gave the reciter much more freedom to choose words and passages to get to the same end than the Serbo-Croatian poet, who was merely "reproductive". Shortly thereafter, Eric Havelock's "Preface to Plato" revolutionized how scholars looked at Homeric epic by arguing not only that it was the product of an oral tradition, but also that the oral-formulas contained therein served as a way for ancient Greeks to preserve cultural knowledge across many different generations. Adam Parry, in his 1966 work "Have we Homer's "Iliad"?", theorized the existence of the most fully developed oral poet to his time, a person who could (at his discretion) creatively and intellectually create nuanced characters in the context of the accepted, traditional story. In fact, he discounted the Serbo-Croatian tradition to an "unfortunate" extent, choosing to elevate the Greek model of oral-tradition above all others. Lord reacted to Kirk's and Parry's essays with "Homer as Oral Poet", published in 1968, which reaffirmed Lord's belief in the relevance of Yugoslav poetry and its similarities to Homer and downplayed the intellectual and literary role of the reciters of Homeric epic.

Many of the criticisms of the theory have been absorbed into the evolving field as useful refinements and modifications. For example, in what Foley called a "pivotal" contribution, Larry Benson introduced the concept of "written-formulaic" to describe the status of some Anglo-Saxon poetry which, while demonstrably written, contains evidence of oral influences, including heavy reliance on formulas and themes A number of individual scholars in many areas continue to have misgivings about the applicability of the theory or the aptness of the South Slavic comparison, and particularly what they regard as its implications for the creativity which may legitimately be attributed to the individual artist. However, at present, there seems to be little systematic or theoretically coordinated challenge to the fundamental tenets of the theory; as Foley put it, ""there have been numerous suggestions for revisions or modifications of the theory, but the majority of controversies have generated further understanding.





</doc>
<doc id="18933569" url="https://en.wikipedia.org/wiki?curid=18933569" title="Library science">
Library science

Library science (often termed library studies, bibliothecography, library economy, and informatics) is an interdisciplinary or multidisciplinary field that applies the practices, perspectives, and tools of management, information technology, education, and other areas to libraries; the collection, organization, preservation, and dissemination of information resources; and the political economy of information. Martin Schrettinger, a Bavarian librarian, coined the discipline within his work (1808–1828) "Versuch eines vollständigen Lehrbuchs der Bibliothek-Wissenschaft oder Anleitung zur vollkommenen Geschäftsführung eines Bibliothekars". Rather than classifying information based on nature-oriented elements, as was previously done in his Bavarian library, Schrettinger organized books in alphabetical order. The first American school for library science was founded by Melvil Dewey at Columbia University in 1887.

Historically, library science has also included archival science. This includes how information resources are organized to serve the needs of selected user groups, how people interact with classification systems and technology, how information is acquired, evaluated and applied by people in and outside libraries as well as cross-culturally, how people are trained and educated for careers in libraries, the ethics that guide library service and organization, the legal status of libraries and information resources, and the applied science of computer technology used in documentation and records management.

There is no generally agreed-upon distinction between the terms "library science",and "librarianship", and to a certain extent they are interchangeable, perhaps differing most significantly in connotation. The term "library and information studies" (LIS) is most often used; most librarians consider it as only a terminological variation, intended to emphasize the scientific and technical foundations of the subject and its relationship with information science. LIS should not be confused with information theory, the mathematical study of the concept of information. "Library philosophy" has been contrasted with "library science" as the study of the aims and justifications of librarianship as opposed to the development and refinement of techniques.

The earliest text on library operations, "Advice on Establishing a Library" was published in 1627 by French librarian and scholar Gabriel Naudé.
Naudé wrote prolifically, producing works on many subjects including politics, religion, history, and the supernatural. He put into practice all the ideas put forth in "Advice" when given the opportunity to build and maintain the library of Cardinal Jules Mazarin.

Martin Schrettinger wrote the second textbook (the first in Germany) on the subject from 1808 to 1829.

Thomas Jefferson, whose library at Monticello consisted of thousands of books, devised a classification system inspired by the Baconian method, which grouped books more or less by subject rather than alphabetically, as it was previously done.

The Jefferson collection provided the start of what became the Library of Congress.

The first American school of librarianship opened at Columbia University under the leadership of Melvil Dewey, noted for his 1876 decimal classification, on 5 January 1887 as the School of Library Economy. The term "library economy" was common in the U.S. until 1942, with the "library science" predominant through much of the 20th century.

Later, the term was used in the title of S. R. Ranganathan's "The Five Laws of Library Science", published in 1931, and in the title of Lee Pierce Butler's 1933 book, "An introduction to library science" (University of Chicago Press).

S. R. Ranganathan conceived the five laws of library science and the development of the first major analytico-synthetic classification system, the colon classification. 
In the United States, Lee Pierce Butler's new approach advocated research using quantitative methods and ideas in the social sciences with the aim of using librarianship to address society's information needs. He was one of the first faculty at the University of Chicago Graduate Library School, which changed the structure and focus of education for librarianship in the twentieth century. This research agenda went against the more procedure-based approach of "library economy," which was mostly confined to practical problems in the administration of libraries.

William Stetson Merrill's "A Code for Classifiers", released in several editions from 1914 to 1939, is an example of a more pragmatic approach, where arguments stemming from in-depth knowledge about each field of study are employed to recommend a system of classification. While Ranganathan's approach was philosophical, it was also tied more to the day-to-day business of running a library. A reworking of Ranganathan's laws was published in 1995 which removes the constant references to books. Michael Gorman's "Our Enduring Values: Librarianship in the 21st Century" features his eight principles necessary by library professionals and incorporate knowledge and information in all their forms, allowing for digital information to be considered.

In more recent years, with the growth of digital technology, the field has been greatly influenced by information science concepts. In the English speaking world the term "library science" seems to have been used for the first time in India in the 1916 book "Punjab Library Primer", written by Asa Don Dickinson and published by the University of the Punjab, Lahore, Pakistan. This university was the first in Asia to begin teaching "library science". The "Punjab Library Primer" was the first textbook on library science published in English anywhere in the world. The first textbook in the United States was the "Manual of Library Economy", published in 1929. In 1923, C. C. Williamson, who was appointed by the Carnegie Corporation, published an assessment of library science education entitled "The Williamson Report," which designated that universities should provide library science training. This report had a significant impact on library science training and education. Library research and practical work, the area of information science, has remained largely distinct both in training and in research interests.

The digital age has transformed how information is accessed and retrieved. "The library is now a part of a complex and dynamic educational, recreational, and informational infrastructure." Mobile devices and applications with wireless networking, high-speed computers and networks, and the computing cloud have deeply impacted and developed information science and information services. The evolution of the library sciences maintains its mission of access equity and community space, as well as the new means for information retrieval called information literacy skills. All catalogues, databases, and a growing number of books are all available on the Internet. In addition, the expanding free access to open source journals and sources such as Wikipedia have fundamentally impacted how information is accessed. Information literacy is the ability to "determine the extent of information needed, access the needed information effectively and efficiently, evaluate information and its sources critically, incorporate selected information into one’s knowledge base, use information effectively to accomplish a specific purpose, and understand the economic, legal, and social issues surrounding the use of information, and access and use information ethically and legally."

Academic courses in library science include collection management, information systems and technology, research methods, information literacy, cataloging and classification, , reference, statistics and management. Library science is constantly evolving, incorporating new topics like database management, information architecture and information management, among others. With the mounting acceptance of Wikipedia as a valued and reliable reference source, many libraries, museums and archives have introduced the role of Wikipedian in residence. As a result, some universities are including coursework relating to Wikipedia and Knowledge Management in their MLIS programs.

Most schools in the US only offer a master's degree in library science or an MLIS and do not offer an undergraduate degree in the subject. About fifty schools have this graduate program, and seven are still being ranked. Many have online programs, which makes attending more convenient if the college is not in a student's immediate vicinity. According to "US News" online journal, University of Illinois is at the top of the list of best MLIS programs provided by universities. Second is University of North Carolina and third is University of Washington. All the listings can be found here.

Most professional library jobs require a professional post-baccalaureate degree in library science, or one of its equivalent terms. In the United States and Canada the certification usually comes from a master's degree granted by an ALA-accredited institution, so even non-scholarly librarians have an originally academic background. In the United Kingdom, however, there have been moves to broaden the entry requirements to professional library posts, such that qualifications in, or experience of, a number of other disciplines have become more acceptable. In Australia, a number of institutions offer degrees accepted by the ALIA (Australian Library and Information Association). Global standards of accreditation or certification in librarianship have yet to be developed.

In academic regalia in the United States, the color for library science is lemon.

The Master of Library Science (MLIS) is the master's degree that is required for most professional librarian positions in the United States and Canada. The MLIS is a relatively recent degree; an older and still common degree designation for librarians to acquire is the Master of Library Science (MLS), or Master of Science in Library Science (MSLS) degree. According to the American Library Association (ALA), "The master’s degree in library and information studies is frequently referred to as the MLS; however, ALA-accredited degrees have various names such as Master of Arts, Master of Librarianship, Master of Library and Information Studies, or Master of Science. The degree name is determined by the program. The [ALA] Committee for Accreditation evaluates programs based on their adherence to the Standards for Accreditation of Master's Programs in Library and Information Studies, not based on the name of the degree

According to 'U.S. News & World Report', library and information science ranked as one of the "Best Careers of 2008." The median annual salary for 2017 was reported by the U.S. Bureau of Labor Statistics as $58,520 in the United States. Additional salary breakdowns available by metropolitan area show that the San Jose-Sunnyvale-Santa Clara metropolitan area has the highest average salary at $86,380. In December 2016, the BLS projected growth for the field at "9 percent between 2016 and 2026", which is "as fast as the average for all occupations". The 2010-2011 Occupational Outlook Handbook states, "Workers in this occupation tend to be older than workers in the rest of the economy. As a result, there may be more workers retiring from this occupation than other occupations. However, relatively large numbers of graduates from MLS programs may cause competition in some areas and for some jobs."

Librarianship manifests a dual career structure for men and women in the United States. While the ratio of female to male librarians remains roughly 4:1, top positions are more often held by men. In large academic libraries, there is less of a discrepancy; however, overall, throughout the profession, men tend to hold higher or leadership positions. Women, however, have made continuous progress toward equality. Women have also been largely left out of standard histories of U.S. librarianship, but Suzanne Hildenbrand's scholarly assessment of the work done by women has expanded the historical record. See also "The Role of women in librarianship, 1876–1976: the entry, advancement, and struggle for equalization in one profession", by Kathleen Weibel, Kathleen de la Peña McCook, and Dianne J. Ellsworth (1979), Phoenix, Ariz: Oryx Press.

There was a "Women's Meeting" at the 1882 14th American Libraries Conference, where issues concerning the salaries of women librarians and what female patrons do in reading rooms were discussed.

During the first 35 years of the American Library Association its presidency was held by men. In 1911 Theresa Elmendorf became the first woman elected president of the ALA. She was ALA president from May 24, 1911, until July 2, 1912.

In 1919, an ALA resolution promoting equal pay and opportunities for women in librarianship was defeated by a large margin.

In 1970, Betty Wilson brought forth a resolution that would have had the ALA refrain from using facilities that discriminate against women. That resolution was also defeated by the membership.

In 1977, the ALA took a stand for the Equal Rights Amendment. The organization stated that they would no longer hold conferences in states that did not ratify the amendment, with the boycott measure set to take place in 1981. An ERA Task Force was formed in 1979 towards this goal and a sum of $25,000 was allocated towards task force operations in unratified states. At the time, a number of state library associations passed pro-ERA resolutions and formed committees on women in libraries.

In 2013–2014, 82% of graduates in Master of Library Science (MLS) programs were female.
In 2016, Carla Hayden became the first female Librarian of Congress.

There are multiple groups within the American Library Association, dedicated to discussing, critiquing, and furthering gender-related and feminist issues within the profession.

In 1969 the first women's rights task force was founded: the National Women's Liberation Front for Librarians (NWFFL or New-Waffle). It was also in 1969 that children's librarians, after being unable to find children's books that included working mothers, worked to remedy the situation and succeeded in their efforts.

The American Library Association's Social Responsibilities Round Table Feminist Task Force (FTF) was founded in 1970 by women who wished to address sexism in libraries and librarianship. FTF was the first ALA group to focus on women's issues. In recent years during Women's History Month (March), the FTF has dedicated their efforts to expanding women's library history online, using the website Women of Library History. The FTF also publishes the annual Amelia Bloomer Project list, which includes some of the best feminist young adult literature of the year.

The Committee on the Status of Women in Librarianship (COSWL) of the American Library Association, founded in 1976, represents the diversity of women's interest within ALA and ensures that the Association considers the rights of the majority (women) in the library field, and promotes and initiates the collection, analysis, dissemination, and coordination of information on the status of women in librarianship. The bibliographic history of women in U.S. librarianship and women librarians developing services for women has been well-documented in the series of publications initially issued by the Social Responsibilities Round Table Task Force on Women and later continued by COSWL.

The ALA also has the Women & Gender Studies Section (WGSS) of its Division "Association of College & Research Libraries"; this section was formed to discuss, promote, and support women's studies collections and services in academic and research libraries.

Finally, the ALA has the Gay, Lesbian, Bisexual, and Transgender Roundtable (GLBTRT). While the GLBTRT deals with sexuality, different than gender identity, much of the roundtable's work is arguably feminist in nature, and concerned with issues of gender. The GLBTRT is committed to serving the information needs of the GLBT professional library community, and the GLBT information and access needs of individuals at large.

Many scholars within the profession have taken up gender and its relationship to the discipline of library and information science. Scholars like Hope A. Olson and Sanford Berman have directed efforts at the problematic nature of cataloging and classification standards and schemes that are obscuring or exclusionary to marginalized groups. Others have written about the implications of gendered stereotypes in librarianship, particularly as they relate to library instruction. Library instruction also intersects with feminist pedagogy, and scholars such as Maria Accardi have written about feminist pedagogical practices in libraries. Library scholars have also dealt with issues of gender and leadership, having equitable gender representation in library collection development, and issues of gender and young adult and children's librarianship.

The ALA Policy Manual states under "B.2.1.15 Access to Library Resources and Services Regardless of Sex, Gender Identity, Gender Expression, or Sexual Orientation (Old Number 53.1.15):" "The American Library Association stringently and unequivocally maintains that libraries and librarians have an obligation to resist efforts that systematically exclude materials dealing with any subject matter, including sex, gender identity or expression, or sexual orientation. The Association also encourages librarians to proactively support the First Amendment rights of all library users, regardless of sex, sexual orientation, or gender identity or expression. Adopted 1993, amended 2000, 2004, 2008, 2010." It also states under "B.2.12 Threats to Library Materials Related to Sex, Gender Identity, or Sexual Orientation (Old Number 53.12)", "The American Library Association supports the inclusion in library collections of materials that reflect the diversity of our society, including those related to sex, sexual orientation, and gender identity or expression. ALA encourages all American Library Association chapters to take active stands against all legislative or other government attempts to proscribe materials related to sex, sexual orientation, and gender identity or expression; and encourages all libraries to acquire and make available materials representative of all the people in our society. Adopted 2005, Amended 2009, 2010." 

The field of library science seeks to provide a diverse working environment in libraries across the United States. Ways to change the status quo include diversifying the job field with regards to age, class, disabilities, ethnicity, gender identity, race, sex, and sexual orientation. The demographics of America are changing; those who were once minorities will become the majority. Library facilities can best represent their communities by hiring diverse staffs. The American Library Association and many libraries around the country realize the issue of diversity in the workplace and are addressing this problem.

The majority of librarians working in the U.S. are female, between the ages of 55–64, and Caucasian. A 2014 study by the American Library Association of research done from 2009 to 2010 shows that 98,273 of credentialed librarians were female while 20,393 were male. 15,335 of the total 111,666 were 35 and younger and only 6,222 were 65 or older. 104,393 were white; 6,160 African American, 3,260 American Pacific Islander; 185 Native American including Alaskan; 1,008 of two or more races, and 3,661 Latino. (ALA).

To help change the lack of diversity in library jobs in the U.S., more scholarships and grants are emerging. Most library and information science students do not belong to an underrepresented group and as a reaction to these research statistics, the field is creating ways to encourage more diversity in the classroom.

The ALA Annual Research Diversity Grant Program is a way to encourage innovation in scholars and professionals to provide insight into how to diversify the field. The ALA Grant is directed toward those who have valuable and original research ideas that can add to the knowledge of diversity in the field of librarianship. The program awards up to three individuals once a year with a grant of $2,500 each. The applicants have submission guidelines, are given a timeline, and are shown the evaluation process online.

One way to nurture cultural diversity in the library field is with cultural competencies. Scholars recommend defining skills needed to serve and work with others who belong to different cultures. It is suggested that these definitions be posted in job listings and be referred to when promoting and giving raises. In library and information science graduate programs, it is also suggested by scholars that there is a lack of classes teaching students cultural competences. It is important for more classes to teach about diversity and measure the outcomes.

Another strategy is to create interest in the field of library and information science from a young age. If minorities do not desire to become librarians, they will not seek to obtain an MLS or MLIS and therefore will not fill high job roles in libraries. A recommended solutions are to create a great experience for all racial group's early on in life. This may inspire more young children to become interested in this field.

ALA Office for Diversity

The Office for Diversity is a sector of the American Library Association whose purpose is to aid libraries in providing a diverse workforce, gathering data, and teaching others about the issue of diversity related to the field of library and information science.

American Indian Library Association

The American Indian Library Association (AILA) was created in 1979. It publishes a newsletter twice a year and educates individuals and groups about Indian culture.

Black Caucus of the American Library Association

BCALA promotes not only library services that can be enjoyed by the African American community but also the emergence of African American librarians and library professionals. By joining the association, patrons have access to newsletters, the entirety of their website, and networking boards.

CALA

The Chinese American Librarians Association (CALA) began March on 31, 1973. It was formerly known as the Mid-West Chinese American Librarians Association. It has members not only in America but in China, Hong Kong, Canada, and more. The organization promotes the Chinese culture through the outlet of libraries and communicates with others in the profession of librarianship.

Reforma

Reforma is the national library association to promote library and information services to Latino and the Spanish speaking, created in 1971. The association has pushed for Spanish collections in libraries, gives out yearly scholarships, and sends out quarterly newsletters. One of Reforma's main goals is to recruit Latinos into professional positions of the library.

Deaf people have the same needs as any other library visitors, and often have more difficulty accessing materials and services. Over the last few decades, libraries in the United States have begun to implement services and collections for deaf and HoH patrons and are working to make more of their collections, services, their communities, and even the world more accessible to this group of underserved people.

The history of the role of libraries in the Deaf community in the United States is a sordid one. The American Library Association readily admits that disabled people belong to a minority that is often overlooked and underrepresented by people in the library, and the Deaf community belongs in this minority group. However, in the last few decades, libraries across the United States have made great strides in the mission of making libraries more accessible to disabled people in general and to the Deaf community specifically. The Library Bill of Rights preamble states that "all libraries are forums for information and ideas" and as such libraries need to remove the physical and technological barriers which in turn would allow persons with disabilities full access to the resources available.

One notable American activist in the library community working toward accessibility for the deaf was Alice Lougee Hagemeyer.

Australian librarian Karen McQuigg stated in 2003 that "even ten years ago, when I was involved in a project looking at what public libraries could offer the deaf, it seemed as if the gap between the requirements of this group and what public libraries could offer was too great for public libraries to be able to serve them effectively." Clearly, not even so long ago, there was quite a dearth of information for or about the deaf community available in libraries across the nation and around the globe.

New guidelines from library organizations such as International Federation of Library Associations and Institutions (IFLA) and the ALA were written in order to help libraries make their information more accessible to people with disabilities, and in some cases, specifically the deaf community. IFLA's "Guidelines for Library Services to Deaf People" is one such set of guidelines, was published to inform libraries of the services that should be provided for deaf patrons. Most of the guidelines pertain to ensuring that deaf patrons have equal access to all available library services. Other guidelines include training library staff to provide services for the deaf community, availability of text telephones or TTYs not only to assist patrons with reference questions but also for making outside calls, using the most recent technology in order to communicate more effectively with deaf patrons, including closed captioning services for any television services, and developing a collection that would interest the members of the deaf community.

Over the years, library services have begun to evolve in order to accommodate the needs and desires of local deaf communities. There is now a Library Service to People Who Are Deaf or Hard of Hearing Forum for libraries to look at to find out what they can do to better serve their Deaf/HoH users. At the Queen Borough Public Library (QBPL) in New York, the staff implemented new and innovative ideas in order to involve the community and library staff with the deaf people in their community. The QBPL hired a deaf librarian, Lori Stambler, to train the library staff about deaf culture, to teach sign language classes for family members and people who are involved with deaf people, and to teach literacy classes for deaf patrons. In working with the library, Stambler was able to help the community reach out to its deaf neighbors, and helped other deaf people become more active in their outside community.

The library at Gallaudet University, the only deaf liberal arts university in the United States, was founded in 1876. The library's collection has grown from a small number of reference books to the world's largest collection of deaf-related materials, with over 234,000 books and thousands of other materials in different formats. The collection is so large that the library had to create a hybrid classification system based on the Dewey Decimal Classification System in order to make cataloging and location within the library easier for both library staff and users. The library also houses the university's archives, which holds some of the oldest deaf-related books and documents in the world.

In Nashville, Tennessee, Sandy Cohen manages the Library Services for the Deaf and Hard of Hearing (LSDHH). The program was created in 1979 in response to information accessibility issues for the deaf in the Nashville area. Originally, the only service provided was the news via a teletypewriter or TTY, but today, the program has expanded to serving the entire state of Tennessee by providing all different types of information and material on deafness, deaf culture, and information for family members of deaf people, as well as a historical and reference collection.

Many practicing librarians do not contribute to LIS scholarship, but focus on daily operations within their own libraries or library systems. Other practicing librarians, particularly in academic libraries, do perform original scholarly LIS research and contribute to the academic end of the field.

Whether or not individual professional librarians contribute to scholarly research and publication, many are involved with and contribute to the advancement of the profession and of library science through local, state, regional, national and international library or information organizations.

Library science is very closely related to issues of knowledge organization; however, the latter is a broader term which covers how knowledge is represented and stored (computer science/linguistics), how it might be automatically processed (artificial intelligence), and how it is organized outside the library in global systems such as the internet. In addition, library science typically refers to a specific community engaged in managing holdings as they are found in university and government libraries, while knowledge organization in general refers to this and also to other communities (such as publishers) and other systems (such as the Internet). The library system is thus one socio-technical structure for knowledge organization.

The terms information organization and knowledge organization are often used synonymously. The fundamentals of their study (particularly theory relating to indexing and classification) and many of the main tools used by the disciplines in modern times to provide access to digital resources (abstracting, metadata, resource description, systematic and alphabetic subject description, and terminology) originated in the 19th century and were developed, in part, to assist in making humanity's intellectual output accessible by recording, identifying, and providing bibliographic control of printed knowledge. 

Information has been published which analyses the relations between philosophy of information (PI), library and information science (LIS), and social epistemology (SE).

The study of librarianship for public libraries covers issues such as cataloging; collection development for a diverse community; information literacy; readers' advisory; community standards; public services-focused librarianship; serving a diverse community of adults, children, and teens; intellectual freedom; censorship; and legal and budgeting issues. The public library as a commons or public sphere based on the work of Jürgen Habermas has become a central metaphor in the 21st century.

Most people are familiar with municipal public libraries, but there are many different types of public libraries that exist. There are four different types of public libraries: association libraries, municipal public libraries, school district libraries and special district public libraries. It is very important to be able to distinguish among the four. Each receives its funding through different sources. Each is established by a different set of voters. And, not all are subject to municipal civil service governance. Listed below is a chart from the New York State Library's library development website. This chart lists all of the information about the different public libraries.

The study of school librarianship covers library services for children in schools through secondary school. In some regions, the local government may have stricter standards for the education and certification of school librarians (who are often considered a special case of teacher), than for other librarians, and the educational program will include those local criteria. School librarianship may also include issues of intellectual freedom, pedagogy, information literacy, and how to build a cooperative curriculum with the teaching staff.

The study of academic librarianship covers library services for colleges and universities. Issues of special importance to the field may include copyright; technology, digital libraries, and digital repositories; academic freedom; open access to scholarly works; as well as specialized knowledge of subject areas important to the institution and the relevant reference works. Librarians often divide focus individually as liaisons on particular schools within a college or university.

Some academic librarians are considered faculty, and hold similar academic ranks to those of professors, while others are not. In either case, the minimal qualification is a Master of Arts in Library Studies or Masters of Arts in Library Science. Some academic libraries may only require a master's degree in a specific academic field or a related field, such as educational technology.

The study of archives includes the training of archivists, librarians specially trained to maintain and build archives of records intended for historical preservation. Special issues include physical preservation, conservation and restoration of materials and mass deacidification; specialist catalogs; solo work; access; and appraisal. Many archivists are also trained historians specializing in the period covered by the archive.

The archival mission includes three major goals: To identify papers and records that have enduring value, to preserve the identified papers, and to make the papers available to others.

There are significant differences between libraries and archives, including differences in collections, records creation, item acquisition, and preferred behavior in the institution. The major difference in collections is that library collections typically comprise published items (books, magazines, etc.), while archival collections are usually unpublished works (letters, diaries, etc.) In managing their collections, libraries will categorize items individually, but archival items never stand alone. An archival record gains its meaning and importance from its relationship to the entire collection; therefore archival items are usually received by the archive in a group or batch. Library collections are created by many individuals, as each author and illustrator creates their own publication; in contrast, an archive usually collects the records of one person, family, institution, or organization, and so the archival items will have fewer source authors.

Another difference between a library and an archive, is that library materials are created explicitly by authors or others who are working intentionally. They choose to write and publish a book, for example, and that occurs. Archival materials are not created intentionally. Instead, the items in an archive are what remain after a business, institution, or person conducts their normal business practices.The collection of letters, documents, receipts, ledger books, etc. were created with intention to perform daily tasks, they were not created in order to populate a future archive.

As for item acquisition, libraries receive items individually, but archival items will usually become part of the archive's collection as a cohesive group.

Behavior in an archive differs from behavior in a library, as well. In most libraries, patrons are allowed and encouraged to browse the stacks, because the books are openly available to the public. Archival items almost never circulate, and someone interested in viewing documents must request them of the archivist and may only view them in a closed reading room. Those who wish to visit an archive will usually begin with an entrance interview. This is an opportunity for the archivist to register the researcher, confirm their identity, and determine their research needs. This is also the opportune time for the archivist to review reading room rules, which vary but typically include policies on privacy, photocopying, the use of finding aids, and restrictions on food, drinks, and other activities or items that could damage the archival materials.

Special libraries are libraries established to meet the highly specialised requirements of professional or business groups. A library is special depending on whether it covers a specialised collection, a special subject, or a particular group of users or even the type of parent organization. A library can be special if it only serves a particular group of users such as lawyers, doctors, nurses, etc. These libraries are called professional libraries and special librarians include almost any other form of librarianship, including those who serve in medical libraries (and hospitals or medical schools), corporations, news agencies, government organizations, or other special collections. The issues at these libraries are specific to the industries they inhabit, but may include solo work, corporate financing, specialized collection development, and extensive self-promotion to potential patrons. Special librarians have their own professional organization, the Special Library Association.

National Center for Atmospheric Research (NCAR) is considered a special library. Its mission is to support, preserve, make accessible, and collaborate in the scholarly research and educational outreach activities of UCAR/NCAR.

Another is the Federal Bureau of Investigations Library. According to its website, "The FBI Library supports the FBI in its statutory mission to uphold the law through investigation of violations of federal criminal law; to protect the United States from foreign intelligence and terrorist activities; and to provide leadership and law enforcement assistance to federal, state, local, and international agencies.

Preservation librarians most often work in academic libraries. Their focus is on the management of preservation activities that seek to maintain access to content within books, manuscripts, archival materials, and other library resources. Examples of activities managed by preservation librarians include binding, conservation, digital and analog reformatting, digital preservation, and environmental monitoring.





</doc>
<doc id="489094" url="https://en.wikipedia.org/wiki?curid=489094" title="Area studies">
Area studies

Area studies (also regional studies) are interdisciplinary fields of research and scholarship pertaining to particular geographical, national/federal, or cultural regions. The term exists primarily as a general description for what are, in the practice of scholarship, many heterogeneous fields of research, encompassing both the social sciences and the humanities. Typical area study programs involve international relations, strategic studies, history, political science, political economy, cultural studies, languages, geography, literature, and other related disciplines. In contrast to cultural studies, area studies often include diaspora and emigration from the area.

Interdisciplinary area studies became increasingly common in the United States of America and in Western scholarship after World War II. Before that war American universities had just a few faculty who taught or conducted research on the non-Western world. Foreign-area studies were virtually nonexistent. After the war, liberals and conservatives alike were concerned about the U.S. ability to respond effectively to perceived external threats from the Soviet Union and China in the context of the emerging Cold War, as well as to the fall-out from the Decolonization of Africa and Asia.

In this context, the Ford Foundation, the Rockefeller Foundation, and the Carnegie Corporation of New York convened a series of meetings producing a broad consensus that to address this knowledge deficit, the U.S. must invest in international studies. Therefore, the foundations of the field are strongly rooted in America. Participants argued that a large brain trust of internationally oriented political scientists and economists was an urgent national priority. There was a central tension, however, between those who felt strongly that, instead of applying Western models, social scientists should develop culturally and historically contextualized knowledge of various parts of the world by working closely with humanists, and those who thought social scientists should seek to develop overarching macrohistorial theories that could draw connections between patterns of change and development across different geographies. The former became area-studies advocates, the latter proponents of modernization theory.

The Ford Foundation would eventually become the dominant player in shaping the area-studies program in the United States.

In 1950 the foundation established the prestigious Foreign Area Fellowship Program (FAFP), the first large-scale national competition in support of area-studies training in the United States. From 1953 to 1966 it contributed $270 million to 34 universities for area and language studies. Also during this period, it poured millions of dollars into the committees run jointly by the Social Science Research Council and the American Council of Learned Societies for field-development workshops, conferences, and publication programs. Eventually, the SSRC-ACLS joint committees would take over the administration of FAFP.

Other large and important programs followed Ford's. Most notably, the National Defense Education Act of 1957, renamed the Higher Education Act in 1965, allocated funding for some 125 university-based area-studies units known as National Resource Center programs at U.S. universities, as well as for Foreign Language and Area Studies fellowships for graduate students.

Meanwhile, area studies were also developed in the Soviet Union.

Since their inception, area studies have been subject to criticism—including by area specialists themselves. Many of them alleged that because area studies were connected to the Cold War agendas of the CIA, the FBI, and other intelligence and military agencies, participating in such programs was tantamount to serving as an agent of the state. Some argue that there is the notion that U.S concerns and research priorities will define the intellectual terrain of area studies. Others insisted, however, that once they were established on university campuses, area studies began to encompass a much broader and deeper intellectual agenda than the one foreseen by government agencies, thus not American centric.

Arguably, one of the greatest threats to the area studies project was the rise of rational choice theory in political science and economics. To mock one of the most outspoken rational choice theory critics, Japan scholar Chalmers Johnson asked: Why do you need to know Japanese or anything about Japan's history and culture if the methods of rational choice will explain why Japanese politicians and bureaucrats do the things they do?

Following the demise of the Soviet Union, philanthropic foundations and scientific bureaucracies moved to attenuate their support for area studies, emphasizing instead interregional themes like "development and democracy". When the Social Science Research Council and the American Council of Learned Societies, which had long served as the national nexus for raising and administering funds for area studies, underwent their first major restructuring in thirty years, closing down their area committees, scholars interpreted this as a massive signal about the changing research environment.

Fields are defined differently from university to university, and from department to department, but common area-studies fields include:
Due to an increasing interest in studying translocal, transregional, transnational and transcontinental phenomena, a Potsdam-based research network has recently coined the term "TransArea Studies" (POINTS – Potsdam International Network for TransArea Studies).

Other interdisciplinary research fields such as women's studies (also known as gender studies), disability studies, and ethnic studies (including African American studies, Asian American studies, Latino/a studies, and Native American studies) are not part of area studies but are sometimes included in discussion along with it.

Area studies is sometimes known as regional studies. The Regional Studies Association is an international association focusing on these interdisciplinary fields.

Some entire institutions of higher education (tertiary education) are devoted solely to area studies such as School of Oriental and African Studies, part of the University of London, or the Tokyo University of Foreign Studies in Japan. At the University of Oxford, the School of Interdisciplinary Area Studies (SIAS)School of Interdiscplinary Area Studies, Oxford and St Antony's College specialise in area studies, and hosts a number of post-graduate teaching programmes and research centres covering various regions of the world.
Jawaharlal Nehru University, New Delhi, is the only institution with immense contribution towards popularising area studies in India.
An institution which exclusively deals with Area Studies is the GIGA (German Institute of Global Area Studies) in Germany. Additionally, Lund University in Sweden offers the largest Asian Studies masters program in Northern Europe and is dedicated to promoting studies related to South Asia through its SASNet.





</doc>
<doc id="34375832" url="https://en.wikipedia.org/wiki?curid=34375832" title="Vorlage">
Vorlage

A Vorlage (; from the German for "prototype" or "template") is a prior version or manifestation of a text under consideration. It may refer to such a version of a text itself, a particular manuscript of the text, or a more complex manifestation of the text (e.g., a group of copies, or a group of excerpts). Thus, the original-language version of a text which a translator then works into a translation is called the "Vorlage" of that translation. For example, the Luther Bible is a translation of the Textus Receptus, so the Textus Receptus is the "Vorlage" of the Luther Bible. 

Sometimes the "Vorlage" of a translation may be lost to history. In some of these cases the "Vorlage" may be reconstructed from the translation. Such a reconstructed "Vorlage" may be called a "retroversion", and it invariably is made with some amount of uncertainty. Nevertheless, the "Vorlage" may still be reconstructed in some parts at such a level of confidence that the translation and its retroversion can be used as a witness for the purposes of textual criticism. This reconstructed "Vorlage" may stand on its own as the sole witness of the original-language text, or it may be compared and used along with other witnesses. Thus, for example, scholars use the reconstructed "Vorlage" of the Greek Septuagint translation of the Hebrew Bible at parts to correct the Hebrew Masoretic version when trying to determine oldest version of the Hebrew Bible that they can infer. Or, as another example, the Coptic fragments of Plato's Republic included among the Nag Hammadi library are used to help attest to the original Greek text which Plato himself wrote. For the bulk of the Gospel of Thomas, the "Vorlage" exists only as a retroversion of the Coptic translation, as no other witness to the original Greek text for most of the sayings recorded therein is known.


</doc>
<doc id="35036752" url="https://en.wikipedia.org/wiki?curid=35036752" title="Outline of the humanities">
Outline of the humanities

The following outline is provided as an overview of and topical guide to the humanities:

Humanities – academic disciplines that study the human condition, using methods that are primarily analytical, critical, or speculative, as distinguished from the mainly empirical approaches of the natural sciences.

The humanities can be described as all of the following:














List of humanities journals





</doc>
<doc id="35885810" url="https://en.wikipedia.org/wiki?curid=35885810" title="Somatic theory">
Somatic theory

Somatic theory is a theory of human social behavior based loosely on the somatic marker hypothesis of António Damásio, which proposes a mechanism by which emotional processes can guide (or bias) behavior, particularly decision-making, as well as the attachment theory of John Bowlby and the self psychology of Heinz Kohut, especially as consolidated by Allan Schore. 

It draws on various philosophical models from "On the Genealogy of Morals" of Friedrich Nietzsche through Martin Heidegger on "das Man", Maurice Merleau-Ponty on the lived body, and Ludwig Wittgenstein on social practices to Michel Foucault on discipline, as well as theories of performativity emerging out of the speech act theory of J. L. Austin, especially as developed by Judith Butler and Shoshana Felman; some somatic theorists have also tied somaticity to performance in the schools of actor training developed by Konstantin Stanislavski and Bertolt Brecht.

Barbara Sellers-Young applies Damasio’s somatic-marker hypothesis to critical thinking as an embodied performance, and provides a review of the theoretical literature in performance studies that supports something like Damasio’s approach:


Edward Slingerland applies Damasio's somatic-marker hypothesis to the cognitive linguistics of Gilles Fauconnier and Mark Turner and George Lakoff and Mark Johnson, especially Fauconnier and Turner's theory of conceptual blending and Lakoff and Johnson's embodied mind theory of metaphor. His goal in importing somatic theory into cognitive linguistics is to show that

Douglas Robinson first began developing a somatic theory of language for a keynote presentation at the 9th American Imagery Conference in Los Angeles, October, 1985, based on Ahkter Ahsen's theory of somatic response to images as the basis for therapeutic transformations; in contradistinction to Ahsen's model, which rejected Freud's "talking cure" on the grounds that words do not awaken somatic responses, Robinson argued that there is a very powerful somatics of language. He later incorporated this notion into "The Translator's Turn" (1991), drawing on the (passing) somatic theories of William James, Ludwig Wittgenstein, and Kenneth Burke in order to argue that somatic response may be "idiosomatic" (somatically idiosyncratic) but typically is "ideosomatic" (somatically ideological, or shaped and guided by society), and that the ideosomatics of language explains how language remains stable enough for communication to be possible. This work preceded the Damasio group's first scientific publication on the somatic-marker hypothesis in 1991, and Robinson did not begin to incorporate Damasio's somatic-marker hypothesis into his somatic theory until later in the 1990s.

In "Translation and Taboo" (1996) Robinson drew on the protosomatic theories of Sigmund Freud, Jacques Lacan, and Gregory Bateson to explore the ways in which the ideosomatics of taboo structure (and partly sanction and conceal) the translation of sacred texts. His first book to draw on Damasio's somatic-marker hypothesis is "Performative Linguistics" (2003); there he draws on J. L. Austin's theory of speech acts, Jacques Derrida's theory of iterability, and Mikhail Bakhtin's theory of dialogism to argue that performativity as an activity of the speaking body is grounded in somaticity. He also draws on Daniel Simeoni's application of Pierre Bourdieu's theory of "habitus" in order to argue that his somatics of translation as developed in "The Translator's Turn" actually explains translation norms more fully than Gideon Toury in "Descriptive Translation Studies and beyond" (1995).

In 2005 Robinson began writing a series of books exploring somatic theory in different communicative contexts: modernist/formalist theories of estrangement (Robinson 2008), translation as ideological pressure (Robinson 2011), first-year writing (Robinson 2012), and the refugee experience, (de)colonization, and the intergenerational transmission of trauma (Robinson 2013).

In Robinson's articulation, somatic theory has four main planks:


In addition, he has added concepts along the way: the proprioception of the body politic as a homeostatic balancing between too much familiarity and too much strangeness (Robinson 2008); tensions between loconormativity and xenonormativity, the exosomatization of places, objects, and skin color, and paleosomaticity (Robinson 2013); ecosis and icosis (unpublished work).



</doc>
<doc id="19552" url="https://en.wikipedia.org/wiki?curid=19552" title="Media studies">
Media studies

Researchers may also develop and employ theories and methods from disciplines including cultural studies, rhetoric (including digital rhetoric), philosophy, literary theory, psychology, political science, political economy, economics, sociology, anthropology, social theory, art history and criticism, film theory, and information theory.

For a history of the field, see "History of media studies".The first Media Studies M.A. program in the U.S. was introduced by John Culkin at The New School in 1975, which has since graduated more than 2,000 students. Culkin was responsible for bringing Marshall McLuhan to Fordham in 1968 and subsequently founded the Center for Understanding Media, which became the New School program.

Media is studied as a broad subject in most states in Australia, with the state of Victoria being world leaders in curriculum development . Media studies in Australia was first developed as an area of study in Victorian universities in the early 1960s, and in secondary schools in the mid 1960s.

Today, almost all Australian universities teach media studies. According to the Government of Australia's "Excellence in Research for Australia" report, the leading universities in the country for media studies (which were ranked well above World standards by the report's scoring methodology) are Monash University, QUT, RMIT, University of Melbourne, University of Queensland and UTS.

In secondary schools, an early film studies course first began being taught as part of the Victorian junior secondary curriculum during the mid 1960s. And, by the early 1970s, an expanded media studies course was being taught. The course became part of the senior secondary curriculum (later known as the Victorian Certificate of Education or "VCE") in the 1980s. It has since become, and continues to be, a strong component of the VCE. Notable figures in the development of the Victorian secondary school curriculum were the long time Rusden College media teacher Peter Greenaway (not the British film director), Trevor Barr (who authored one of the first media text books "Reflections of Reality") and later John Murray (who authored "The Box in the Corner", "In Focus", and "10 Lessons in Film Appreciation").

Today, Australian states and territories that teach media studies at a secondary level are Australian Capital Territory, Northern Territory, Queensland, South Australia, Victoria and Western Australia. Media studies does not appear to be taught in the state of New South Wales at a secondary level.

In Victoria, the VCE media studies course is structured as: Unit 1 - Representation, Technologies of Representation, and New Media; Unit 2 - Media Production, Australian Media Organisations; Unit 3 - Narrative Texts, Production Planning; and Unit 4 - Media Process, Social Values, and Media Influence. Media studies also form a major part of the primary and junior secondary curriculum, and includes areas such as photography, print media and television.

Victoria also hosts the peak media teaching body known as ATOM which publishes "Metro" and "Screen Education" magazines.

In Canada, media studies and communication studies are incorporated in the same departments and cover a wide range of approaches (from critical theory to organizations to research-creation and political economy, for example). Over time, research developed to employ theories and methods from cultural studies, philosophy, political economy, gender, sexuality and race theory, management, rhetoric, film theory, sociology, and anthropology. Harold Innis and Marshall McLuhan are famous Canadian scholars for their contributions to the fields of media ecology and political economy in the 20th century. They were both important members of the Toronto School of Communication at the time. More recently, the School of Montreal and its founder James R. Taylor significantly contributed to the field of organizational communication by focusing on the ontological processes of organizations.

Carleton University and the University of Western Ontario, 1945 and 1946 prospectively, created Journalism specific programs or schools. A Journalism specific program was also created at Ryerson in 1950. The first communication programs in Canada were started at Ryerson and Concordia Universities. The Radio and Television Arts program at Ryerson were started in the 1950s, while the Film, Media Studies/Media Arts, and Photography programs also originated from programs started in the 1950s. The Communication studies department at Concordia was created in the late 1960s. Ryerson's Radio and Television, Film, Media and Photography programs were renowned by the mid 1970s, and its programs were being copied by other colleges and universities nationally and Internationally.

Today, most universities offer undergraduate degrees in Media and Communication Studies, and many Canadian scholars actively contribute to the field, among which: Brian Massumi (philosophy, cultural studies), Kim Sawchuk (cultural studies, feminist, ageing studies), Carrie Rentschler (feminist theory), and François Cooren (organizational communication).

In his book “Understanding Media, The Extensions of Man”, media theorist Marshall McLuhan suggested that "the medium is the message", and that all human artefacts and technologies are media. His book introduced the usage of terms such as “media” into our language along with other precepts, among them “global village” and “Age of Information”. A medium is anything that mediates our interaction with the world or other humans. Given this perspective, media study is not restricted to just media of communications but all forms of technology. Media and their users form an ecosystem and the study of this ecosystem is known as media ecology.

McLuhan says that the “technique of fragmentation that is the essence of machine technology” shaped the restructuring of human work and association and “the essence of automation technology is the opposite”. He uses an example of the electric light to make this connection and to explain “the medium is the message”. The electric light is pure information and it is a medium without a message unless it is used to spell out some verbal ad or a name. The characteristic of all media means the “content” of any medium is always another medium. For example, the content of writing is speech, the written word is the content of print, and print is the content of the telegraph. The change that the medium or technology introduces into human affairs is the “message”. If the electric light is used for Friday night football or to light up your desk you could argue that the content of the electric light is these activities. The fact that it is the medium that shapes and controls the form of human association and action makes it the message. The electric light is over looked as a communication medium because it doesn't have any content. It is not until the electric light is used to spell a brand name that it is recognized as medium. Similar to radio and other mass media electric light eliminates time and space factors in human association creating deeper involvement. McLuhan compared the “content” to a juicy piece of meat being carried by a burglar to distract the “watchdog of the mind”. The effect of the medium is made strong because it is given another media “content”. The content of a movie is a book, play or maybe even an opera.

McLuhan talks about media being “hot” or “cold” and touches on the principle that distinguishes them from one another. A hot medium (i.e., radio or Movie) extends a single sense in “high definition”. High definition means the state of being well filled with data. A cool medium (i.e., Telephone and TV) is considered “low definition” because a small amount of data/information is given and has to be filled in. Hot media are low in participation and cool media are high in participation. Hot media are low in participation because it is giving most of the information and it excludes. Cool media are high in participation because it gives you information but you have to fill in the blanks and it is inclusive. He used lecturing as an example for hot media and seminars as an example for low media. If you use a hot medium in a hot or cool culture makes a difference.

There are two universities in China that specialize in media studies. Communication University of China, formerly known as the Beijing Broadcasting Institute, that dates back to 1954. CUC has 15,307 full-time students, including 9264 undergraduates, 3512 candidates for doctor and master's degrees and 16780 students in programs of continuing education. The other university known for media studies in China is Zhejiang University of Media and Communications (ZUMC) which has campuses in Hangzhou and Tongxiang. Almost 10,000 full-time students are currently studying in over 50 programs at the 13 Colleges and Schools of ZUMC. Both institutions have produced some of China's brightest broadcasting talents for television as well as leading journalists at magazines and newspapers.

There is no university specialized on journalism and media studies, but there are seven public universities which have a department of media studies. Three biggest are based in Prague (Charles University), Brno (Masaryk University) and Olomouc (Palacký University). There are another nine private universities and colleges which has media studies department.

One prominent French media critic is the sociologist Pierre Bourdieu who wrote among other books "On Television" (New Press, 1999). Bourdieu's analysis is that television provides far less autonomy, or freedom, than we think. In his view, the market (which implies the hunt for higher advertising revenue) not only imposes uniformity and banality, but also a form of invisible censorship. When, for example, television producers "pre-interview" participants in news and public affairs programs, to ensure that they will speak in simple, attention-grabbing terms, and when the search for viewers leads to an emphasis on the sensational and the spectacular, people with complex or nuanced views are not allowed a hearing.

In Germany two main branches of media theory or media studies can be identified.

The first major branch of media theory has its roots in the humanities and cultural studies, such as film studies ("Filmwissenschaft"), theater studies ("Theaterwissenschaft") and German language and literature studies ("Germanistik") as well as Comparative Literature Studies ("Komparatistik"). This branch has broadened out substantially since the 1990s. And it is on this initial basis that a culturally-based media studies (often emphasised more recently through the disciplinary title "Medienkulturwissenschaft") in Germany has primarily developed and established itself.

This plurality of perspectives make it difficult to single out one particular site where this branch of Medienwissenschaft originated. While the Frankfurt-based theatre scholar, Hans-Theis Lehmanns term "post dramatic theater" points directly to the increased blending of co-presence and mediatized material in the German theater (and elsewhere) since the 1970s, the field of theater studies from the 1990s onwards at the Freie Universität Berlin, led in particular by Erika Fischer-Lichte, showed particular interest in the ways in which theatricality influenced notions of performativity in aesthetic events. Within the field of Film Studies, again, both Frankfurt and Berlin were dominant in the development of new perspectives on moving image media. Heide Schlüpman in Frankfurt and Gertrud Koch, first in Bochum then in Berlin, were key theorists contributing to an aesthetic theory of the cinema (Schlüpmann) as "dispositif" and the moving image as medium, particularly in the context of illusion (Koch). Many scholars who became known as media scholars in Germany originally were scholars of German, such as Friedrich Kittler, who taught at the Humboldt Universität zu Berlin, completed both his dissertation and habilitation in the context of "Germanistik". One of the early publications in this new direction is a volume edited by Helmut Kreuzer, "Literature Studies - Media Studies" ("Literaturwissenschaft – Medienwissenschaft"), which summarizes the presentations given at the Düsseldorfer Germanistentag 1976.

The second branch of media studies in Germany is comparable to Communication Studies. Pioneered by Elisabeth Noelle-Neumann in the 1940s, this branch studies mass media, its institutions and its effects on society and individuals. The German Institute for Media and Communication Policy, founded in 2005 by media scholar Lutz Hachmeister, is one of the few independent research institutions that is dedicated to issues surrounding media and communications policies.

The term "Wissenschaft" cannot be translated straightforwardly as "studies", as it calls to mind both scientific methods and the humanities. Accordingly, German media theory combines philosophy, psychoanalysis, history, and scienctific studies with media-specific research.

"Medienwissenschaften" is currently one of the most popular courses of study at universities in Germany, with many applicants mistakenly assuming that studying it will automatically lead to a career in TV or other media. This has led to widespread disillusionment, with students blaming the universities for offering highly theoretical course content. The universities maintain that practical journalistic training is not the aim of the academic studies they offer.

Media Studies is a fast growing academic field in India, with several dedicated departments and research institutes. With a view to making the best use of communication facilities for information, publicity and development, the Government of India in 1962-63 sought the advice of the Ford Foundation/UNESCO team of internationally known mass communication specialists who recommended the setting up of a national institute for training, teaching and research in mass communication. Anna University was the first university to start Master of Science in Electronic Media programmes. It offers a five-year integrated programme and a two-year programme in Electronic Media. The Department of Media Sciences was started in January 2002, branching off from the UGC's Educational Multimedia Research Centre (EMMRC). National Institute of Open Schooling, the world's largest open schooling system, offers Mass Communication as a subject of studies at senior secondary level. All the major universities in the country have mass media and journalism studies departments. Centre for the Study of Developing Societies (CSDS), Delhi has media studies as one of their major emphasis.

In the Netherlands, media studies are split into several academic courses such as (applied) communication sciences, communication- and information sciences, communication and media, media and culture or theater, film and television sciences. Whereas communication sciences focuses on the way people communicate, be it mediated or unmediated, media studies tends to narrow the communication down to just mediated communication. However, it would be a mistake to consider media studies a specialism of communication sciences, since media make up just a small portion of the overall course. Indeed, both studies tend to borrow elements from one another.

Communication sciences (or a derivative thereof) can be studied at Erasmus University Rotterdam, Radboud University, Tilburg University, University of Amsterdam, University of Groningen, University of Twente, Roosevelt Academy, University of Utrecht, VU University Amsterdam and Wageningen University and Research Centre.

Media studies (or something similar) can be studied at the University of Amsterdam, VU University Amsterdam, Erasmus University Rotterdam, University of Groningen and the University of Utrecht.

Eight Dutch universities collaborate in the overarching Netherlands Research school for Media Studies (RMeS), which acts as a platform for graduate students in media research.

Media studies in New Zealand is healthy, especially due to renewed activity in the country's film industry and is taught at both secondary and tertiary education institutes. Media studies in NZ can be regarded as a singular success, with the subject well-established in the tertiary sector (such as Screen and Media Studies at the University of Waikato; Media Studies, Victoria University of Wellington; Film, Television and Media Studies, University of Auckland; Media Studies, Massey University; Communication Studies, University of Otago). 

Different Media Studies courses can offer students a range of specialisations- such as cultural studies, media theory and analysis, practical film-making, journalism and communications studies. But what makes the case of New Zealand particularly significant in respect of Media Studies is that for more than a decade it has been a nationally mandated and very popular subject in secondary (high) schools, taught across three years in a very structured and developmental fashion, with Scholarship in Media Studies available for academically gifted students. According to the New Zealand Ministry of Education Subject Enrolment figures 229 New Zealand schools offered Media Studies as a subject in 2016, representing more than 14,000 students.

In Pakistan, media studies programs are widely offered. University of the Punjab Lahore is the oldest department. Later on University of Karachi, Peshawar University, BZU Multaan, Islamia University Bahwalpur also started communication programs. Now, newly established universities are also offering mass communication program in which University of Gujrat emerged as a leading department. Bahria University which is established by Pakistan Navy is also offering BS in media studies.

In Switzerland, media and communication studies are offered by several higher education institutions including the International University in Geneva, Zurich University of Applied Sciences, University of Lugano, University of Fribourg and others.

In the United Kingdom, media studies developed in the 1960s from the academic study of English, and from literary criticism more broadly. The key date, according to Andrew Crisell, is 1959:

When Joseph Trenaman left the BBC's Further Education Unit to become the first holder of the Granada Research Fellowship in Television at Leeds University. Soon after in 1966, the Centre for Mass Communication Research was founded at Leicester University, and degree programmes in media studies began to sprout at polytechnics and other universities during the 1970s and 1980s.

James Halloran at Leicester University is credited with much influence in the development of media studies and communication studies, as the head of the university's Centre for Mass Communication Research, and founder of the International Association for Media and Communication Research. Media Studies is now taught all over the UK. It is taught at Key Stages 1– 3, Entry Level, GCSE and at A level and the Scottish Qualifications Authority offers formal qualifications at a number of different levels. It is offered through a large area of exam boards including AQA and WJEC.

Much research in the field of news media studies has been led by the Reuters Institute for the Study of Journalism. Details of the research projects and results are published in the RISJ annual report.

Mass communication, Communication studies or simply 'Communication' may be more popular names than “media studies” for academic departments in the United States. However, the focus of such programs sometimes excludes certain media—film, book publishing, video games, etc. The title “media studies” may be used alone, to designate film studies and rhetorical or critical theory, or it may appear in combinations like “media studies and communication” to join two fields or emphasize a different focus. It involves the study of many emerging, contemporary media and platforms, with social media having boomed in recent years. Broadcast and cable TV is no longer the primary form of entertainment, with various screens offering worldwide events and pastimes around the clock.
In 1999, the MIT Comparative Media Studies program started under the leadership of Henry Jenkins, since growing into a graduate program, MIT's largest humanities major, and, following a 2012 merger with the Writing and Humanistic Studies program, a roster of twenty faculty, including Pulitzer Prize-winning author Junot Diaz, science fiction writer Joe Haldeman, games scholar T. L. Taylor, and media scholars William Uricchio (a CMS co-founder), Edward Schiappa, and Heather Hendershot. Now named Comparative Media Studies/Writing, the department places an emphasis on what Jenkins and colleagues had termed "applied humanities": it hosts several research groups for civic media, digital humanities, games, computational media, documentary, and mobile design, and these groups are used to provide graduate students with research assistantships to cover the cost of tuition and living expenses. The incorporation of Writing and Humanistic Studies also placed MIT's Science Writing program, Writing Across the Curriculum, and Writing and Communications Center under the same roof.

Formerly an interdisciplinary major at the University of Virginia the Department of Media Studies was officially established in 2001 and has quickly grown to wide recognition. This is partly thanks to the acquisition of Professor Siva Vaidhyanathan, a cultural historian and media scholar, as well as the Inaugural Verklin Media Policy and Ethics Conference, endowed by the CEO of Canoe Ventures and UVA alumnus David Verklin. In 2010, a group of undergraduate students in the Media Studies Department established the Movable Type Academic Journal, the first ever undergraduate academic journal of its kind. The department is expanding rapidly and doubled in size in 2011.

Brooklyn College, part of the City University of New York, has been offering graduate studies in television and media since 1961. Currently, the Department of Television and Radio administers an MS in Media Studies, and hosts the Center for the Study of World Television.

The University of Southern California has three distinct centers for media studies: the Center for Visual Anthropology (founded in 1984), the Institute for Media Literacy at the School of Cinematic Arts (founded in 1998) and the Annenberg School for Communication and Journalism (founded in 1971).

University of California, Irvine had in Mark Poster one of the first and foremost theorists of media culture in the US, and can boast a strong Department of Film & Media Studies. University of California, Berkeley has three institutional structures within which media studies can take place: the department of Film and Media (formerly Film Studies Program), including famous theorists as Mary Ann Doane and Linda Williams, the Center for New Media, and a long established interdisciplinary program formerly titled Mass Communications, which recently changed its name to Media Studies, dropping any connotations which accompany the term “Mass” in the former title. Until recently, Radford University in Virginia used the title "media studies" for a department that taught practitioner-oriented major concentrations in journalism, advertising, broadcast production and Web design. In 2008, those programs were combined with a previous department of communication (speech and public relations) to create a School of Communication. (A media studies major at Radford still means someone concentrating on journalism, broadcasting, advertising or Web production.)

The University of Denver has a renowned program for digital media studies. It is an interdisciplinary program combining Communications, Computer Science, and the arts.




</doc>
<doc id="104952" url="https://en.wikipedia.org/wiki?curid=104952" title="Tragicomedy">
Tragicomedy

Tragicomedy is a literary genre that blends aspects of both tragic and comic forms. Most often seen in dramatic literature, the term can describe either a tragic play which contains enough comic elements to lighten the overall mood or a serious play with a happy ending. Tragicomedy, as its name implies, invokes the intended response of both the tragedy and the comedy in the audience, the former being a genre based on human suffering that invokes an accompanying catharsis or pleasure and the latter being a genre intended to be humorous or amusing by inducing laughter.

There is no complete formal definition of tragicomedy from the classical age. It appears that the Greek philosopher Aristotle had something like the Renaissance meaning of the term (that is, a serious action with a happy ending) in mind when, in "Poetics", he discusses tragedy with a dual ending. In this respect, a number of Greek and Roman plays, for instance "Alcestis", may be called tragicomedies, though without any definite attributes outside of plot. The word itself originates with the Roman comic playwright Plautus, who coined the term somewhat facetiously in the prologue to his play "Amphitryon". The character Mercury, sensing the indecorum of the inclusion of both kings and gods alongside servants in a comedy, declares that the play had better be a "tragicomoedia":

Plautus's comment had an arguably excessive impact on Renaissance aesthetic theory, which had largely transformed Aristotle's comments on drama into a rigid theory. For "rule mongers" (the term is Giordano Bruno's), "mixed" works such as those mentioned above, more recent "romances" such as "Orlando Furioso", and even "The Odyssey" were at best puzzles; at worst, mistakes. Two figures helped to elevate tragicomedy to the status of a regular genre, by which is meant one with its own set of rigid rules. Giovanni Battista Giraldi Cinthio, in the mid-sixteenth century, both argued that the tragedy-with-comic-ending ("tragedia de lieto fin") was most appropriate to modern times and produced his own examples of such plays. Even more important was Giovanni Battista Guarini. Guarini's "Il Pastor Fido", published in 1590, provoked a fierce critical debate in which Guarini's spirited defense of generic innovation eventually carried the day. Guarini's tragicomedy offered modulated action that never drifted too far either to comedy or tragedy, mannered characters, and a pastoral setting. All three became staples of continental tragicomedy for a century and more.

In England, where practice ran ahead of theory, the situation was quite different. In the sixteenth century, "tragicomedy" meant the native sort of romantic play that violated the unities of time, place, and action, that glibly mixed high- and low-born characters, and that presented fantastic actions. These were the features Philip Sidney deplored in his complaint against the "mungrell Tragy-comedie" of the 1580s, and of which Shakespeare's Polonius offers famous testimony: "The best actors in the world, either for tragedy, comedy, history, pastoral, pastoral-comical, historical-pastoral, tragical-historical, tragical-comical-historical-pastoral, scene individuable, or poem unlimited: Seneca cannot be too heavy, nor Plautus too light. For the law of writ and the liberty, these are the only men." Some aspects of this romantic impulse remain even in the work of more sophisticated playwrights: Shakespeare's last plays, which may well be called tragicomedies, have often been called romances.

By the early Stuart period, some English playwrights had absorbed the lessons of the Guarini controversy. John Fletcher's "The Faithful Shepherdess", an adaptation of Guarini's play, was produced in 1608. In the printed edition, Fletcher offered an interesting definition of the term, worth quoting at length: "A tragi-comedie is not so called in respect of mirth and killing, but in respect it wants deaths, which is enough to make it no tragedy, yet brings some neere it, which is inough to make it no comedie." Fletcher's definition focuses primarily on events: a play's genre is determined by whether or not people die in it, and in a secondary way on how close the action comes to a death. But, as Eugene Waith showed, the tragicomedy Fletcher developed in the next decade also had unifying stylistic features: sudden and unexpected revelations, outré plots, distant locales, and a persistent focus on elaborate, artificial rhetoric.

Some of Fletcher's contemporaries, notably Philip Massinger and James Shirley, wrote successful and popular tragicomedies. Richard Brome also essayed the form, but with less success. And many of their contemporary writers, ranging from John Ford to Lodowick Carlell to Sir Aston Cockayne, made attempts in the genre.

Tragicomedy remained fairly popular up to the closing of the theaters in 1642, and Fletcher's works were popular in the Restoration as well. The old styles were cast aside as tastes changed in the eighteenth century; the "tragedy with a happy ending" eventually developed into melodrama, in which form it still flourishes.

"Landgartha" (1640) by Henry Burnell, the first play by an Irish playwright to be performed in an Irish theatre, was explicitly described by its author as a tragicomedy. Critical reaction to the play was universally hostile, partly it seems because the ending was neither happy nor unhappy. Burnell in his introduction to the printed edition of the play attacked his critics for their ignorance, pointing out that as they should know perfectly well, many plays are neither tragedy nor comedy, but "something between".

The more subtle criticism that developed after the Renaissance stressed the thematic and formal aspects of tragicomedy, rather than plot. Gotthold Ephraim Lessing defined it as a mixture of emotions in which "seriousness stimulates laughter, and pain pleasure." Even more commonly, tragicomedy's affinity with satire and "dark" comedy have suggested a tragicomic impulse in modern theatre with Luigi Pirandello who influenced Beckett. Also it can be seen in absurdist drama. Friedrich Dürrenmatt, the Swiss dramatist, suggested that tragicomedy was the inevitable genre for the twentieth century; he describes his play "The Visit" (1956) as a tragicomedy. Tragicomedy is a common genre in post-World War II British theatre, with authors as varied as Samuel Beckett, Tom Stoppard, John Arden, Alan Ayckbourn and Harold Pinter writing in this genre. Many writers of the metamodernist and postmodernist movements have made use of tragicomedy and/or gallows humor. A notable example of a metamodernist tragicomedy is David Foster Wallace's 1996 magnum opus, "Infinite Jest".

Beginning with BoJack Horseman in 2014, the tragicomic genre has spread into the television medium. Television shows that are considered tragicomedies that have come after Bojack are "One Mississippi", "Horace and Pete", "Fleabag", "Corporate", "Barry", "Dead to Me", and "Kidding" starring Jim Carrey.

Several tragicomic plays have been adapted for the big screen including Shakespeare's "The Merchant of Venice". Tom Stoppard has also adapted his own play "Rosencrantz and Guildenstern Are Dead" into a movie of the same name".

Film director Taika Waititi has made a name out of making tragicomedy films including his 2010 film "Boy" and his 2019 Adolf Hitler satire, "Jojo Rabbit".

Other notable examples of tragicomedy films include "Little Miss Sunshine", "Inside Llewyn Davis", "Eternal Sunshine of the Spotless Mind", "Life Is Beautiful", "Good Bye, Lenin!", and "Parasite".




</doc>
<doc id="18856114" url="https://en.wikipedia.org/wiki?curid=18856114" title="Integrated human studies">
Integrated human studies

Integrated human studies is an emerging educational field that equips people with knowledge and competencies across a range of disciplines to enable them to address the challenges facing human beings this century. It differs from other interdisciplinary educational initiatives in that its curriculum is purpose designed rather than simply an amalgamation of existing disciplines.

Kyoto University in Japan has offered a formal course in Integrated Human Studies since 1992 when it reorganized its College of Liberal Arts and Sciences and renamed it the Faculty of Integrated Human Studies. This was subsequently (in 2003) integrated with the Graduate School of Human and Environmental Studies to create the new Graduate School of Human and Environmental Studies.

The University of Western Australia established the Center for Integrated Human Studies in early 2008. This center brings together the sciences, social sciences, arts and humanities to focus on the nature and future of humankind. Its fundamental concern is to promote human well-being at an individual, local and global level within a sustainable environment.

Integration of disciplinary fields has arisen as a response to the “increasing specialization of [university] courses to meet the demands of technological progress, economic growth and vocational training” resulting in the development of ever narrower fields of study at tertiary level. Proponents of integrated human studies believe that a broader, interdisciplinary approach is needed to enable future decision-makers to grasp the complexities of the issues facing humankind in the 21st century and craft workable solutions.


</doc>
<doc id="40138324" url="https://en.wikipedia.org/wiki?curid=40138324" title="Humanities Indicators">
Humanities Indicators

The Humanities Indicators is a project of the American Academy of Arts and Sciences that equips researchers and policymakers, universities, foundations, museums, libraries, humanities councils and other public institutions with statistical tools for answering basic questions about primary and secondary humanities education, undergraduate and graduate education in the humanities, the humanities workforce, levels and sources of program funding, public understanding and impact of the humanities, and other areas of concern in the humanities community.

Data from the Humanities Indicators has been widely discussed in recent conversations about a "crisis in the humanities", in light of a national decline in the number of college majors. To address questions about the workforce outcomes of humanities graduates (which are often cited as playing a role in the falling number of majors as of 2015), the Indicators issued "The State of the Humanities 2018: Graduates in the Workforce & Beyond", which examined not only their employment and earnings relative to other fields, but also graduates’ satisfaction with their work after graduation and their lives more generally. The data reveal that despite disparities in median earnings, humanities majors are quite similar to graduates from other fields with respect to their "perceived" well-being. The report was widely cited in the media as an important intervention in the discussion.



</doc>
<doc id="1495838" url="https://en.wikipedia.org/wiki?curid=1495838" title="Romance studies">
Romance studies

Romance studies (in French: "études romanes"; in Spanish: "filología románica") is an academic discipline that covers the study of the languages, literatures, and cultures of areas that speak a Romance language. Romance studies departments usually include the study of Spanish, French, Italian, and Portuguese. Additional languages of study include Romanian and Catalan, on one hand, and culture, history, and politics on the other hand.

Because most places in Latin America speak a Romance language, Latin America is also studied in Romance studies departments. As a result, non-Romance languages in use in Latin America, such as Quechua, are sometimes also taught in Romance studies departments.

Romance studies departments differ from single- or two-language departments in that they attempt to break down the barriers in scholarship among the various languages, through interdisciplinary or comparative work. These departments differ from Romance "language" departments in that they place a heavier emphasis on connections between language and literature, among others.




</doc>
<doc id="26826220" url="https://en.wikipedia.org/wiki?curid=26826220" title="Global intellectual history">
Global intellectual history

Global intellectual history is the history of thought in the world across the span of human history, often understood from the invention of writing to the present. The discipline is part of the field of intellectual history, also known as history of ideas, and can also be termed global history of ideas.

In recent years, historians such as C. A. Bayly have been calling for a "global intellectual history" to be written. They stress that to understand the history of ideas across time and space, it is necessary to study from a cosmopolitan or global point of view the connections and the parallels in intellectual development across the world. Yet these separate histories and their convergence in the modern period have yet to be brought together into a single historical narrative. Nonetheless, some global histories, like Bayly's own "Birth of the Modern World" or David Armitage's "The Declaration of Independence: A Global History" offer contributions to the huge and necessarily collaborative project of writing the history of thought in a comparative and especially connective way. Other examples of transnational intellectual histories include Albert Hourani's "Arabic Thought in the Liberal Age".

In 2013, Samuel Moyn and Andrew Sartori published the anthology "Global Intellectual History". In 2016, the Routledge journal "Global Intellectual History" (ed. Richard Whatmore) was established. In January 2019 the historian J. G. A. Pocock stated in that journal: "The beginnings of the ‘global’ critique are well known and may as well be accepted as common ground. They reduce to the assertion that ‘Cambridge’ scholarship in this field is ‘Eurocentric’; that is, that it has dealt exclusively with the ‘political thought’ generated in the Greco-Roman Mediterranean, transmitted to medieval and modern Europe, and taken up in the Euro-colonized Americas and a world (or ‘globe’) subjected to European or ‘western’ domination. This is obviously true, and calls for reformation." 

It has been argued that the historians of ideas Arthur O. Lovejoy and Hajime Nakamura should be read as modern founders of the discipline global intellectual history. Other recent contributors are Siep Stuurman, Sanjay Subrahamnyam, and Martin Mulsow.

The origins of human intellectual history arguably began before the invention of writing, but historians are by definition only concerned with the eras in which writing was present. In the spirit of a historiographic project that is relevant to all human beings and that has yet to be completed, the sections that follow briefly review currents of thought in pre-modern and modern history of the world, and are organized by geographic area (and within each section, chronologically).

The modern intellectual history of Europe cannot be separated from various bodies of ancient thought, from the works of classical Greek and Latin authors to the writings of the fathers of the Christian Church. Such a broad survey of topics is not attempted here, however. A debatable but defensible starting point for modern European thought might instead be identified with the birth of scholasticism and humanism in the 13th and 14th centuries. Both of these intellectual currents were associated with classical revivals (in the case of scholasticism, the rediscovery of Aristotle; in the case of humanism, of Latin antiquity, especially Cicero) and with prominent founders, Aquinas and Petrarch respectively. But they were both significantly original intellectual experiences, as well as self-consciously modern, so that they make an appropriate starting point for this survey.

What follows below is a selective and far from complete listing of significant trends and individuals in the history of European thought. While movements such as the Enlightenment or Romanticism are relatively imprecise approximations, rarely taken too seriously by scholars, they are good starting points for approaching the enormous complexity of the history of Europe's intellectual heritage. It is hoped that interested readers will pursue the listed topics in greater depth by consulting the respective articles and the suggestions for further reading.

The intellectual history of western Europe and the Americas includes:

"Pre-Modern East Asia"

The intellectual history of China is connected to the birth of scholarship in ancient China, the creation of Confucianism with its extensive exegesis of the texts of Confucius, and the active part of scholars in governments. In Korea, the yangban scholar movement drove the development of Korean intellectual history from the late Goryeo to the golden age of intellectual achievement in the Joseon Dynasty.

In China, "literati" referred to the government officials who formed the ruling class in China for over two thousand years. These "scholar-bureaucrats" were a status group of educated laymen, not ordained priests. They were not a hereditary group as their position depended on their knowledge of writing and literature. After 200 B.C. the system of selection of candidates was influenced by Confucianism and established its ethic among the literati.

Confucianism (儒家, literally "scholarly tradition") is a Chinese ethical and philosophical system originally developed from the teachings of the early Chinese sage Confucius. Confucius is seen as the founder of the teachings of Confucianism, although he claimed to follow the ways of people before him. Confucianism is a complex system of moral, social, political, philosophical, and religious thought which has had tremendous influence on the culture and history of East Asia. Some people in Europe have considered it to have been the "state religion" in East Asian countries because of governmental promotion of Confucianist values and needs.

Other ancient intellectual currents in East Asia include Buddhism and Daoism.

"Modern East Asia"

The modern intellectual history of China is considered to begin with the arrival of the Jesuits in the sixteenth century. The Jesuits brought with them new astronomical and cartographic knowledge, and were responsible for new developments in Chinese science. Science in modern China has been the subject of the work of the historian Benjamin Elman.

"Pre-Modern South Asia"

Indian thought is a broad topic that includes the ancient epics of South Asia, the development of what is now called Hinduism and Hindu philosophy and the rise of Buddhism, as well as many other topics relating to the political and artistic lives of pre-modern South Asia.

Ram Sharan Sharma's work "Aspects of Political Ideas and Institutions in India" (Motilal Banarsidass is the most authoritative account of ancient Indian political ideas and institutions. It deals with the intellectual standards in ancient India in terms of political institutions.

"Pre-Modern History"

The culture of the ancient Near East and eventually of much of Africa as well was modified significantly by the arrival of Islam beginning in the seventh century CE. The history of Islam has been the work of many scholars, both Muslim and non-Muslim, and including such luminaries as Ignác Goldziher, Marshall Hodgson and in more recent times Patricia Crone. Islamic culture is not a simple and unified entity. The history of Islam, like that of other religions, is a history of different interpretations and approaches to Islam. There is no a-historical Islam outside the process of historical development.

Islamic thought includes a variety of different intellectual disciplines, including theology, the study of the Qur'an, the study of Hadith, history, grammar, rhetoric and philosophy. For more information see the Islamic Golden Age.

Classical Islamic scholars and authors include:

Persian philosophy can be traced back as far as to Old Iranian philosophical traditions and thoughts which originated in ancient Indo-Iranian roots and were considerably influenced by Zarathustra's teachings. Throughout Iranian history and due to remarkable political and social changes such as the Macedonian, Arab and Mongol invasions of Persia a wide spectrum of schools of thoughts showed a variety of views on philosophical questions extending from Old Iranian and mainly Zoroastrianism-related traditions to schools appearing in the late pre-Islamic era such as Manicheism and Mazdakism as well as various post-Islamic schools. Iranian philosophy after Arab invasion of Persia, is characterized by different interactions with the Old Iranian philosophy, the Greek philosophy and with the development of Islamic philosophy. The Illumination School and the Transcendent Philosophy are regarded as two of the main philosophical traditions of that era in Persia.

"Modern Near and Middle East"

Islam and modernity encompass the relation and compatibility between the phenomenon of modernity, its related concepts and ideas, and the religion of Islam. In order to understand the relation between Islam and modernity, one point should be made in the beginning. Similarly, modernity is a complex and multidimensional phenomenon rather than a unified and coherent phenomenon. It has historically had different schools of thoughts moving in many directions.

Intellectual movements in Iran involve the Iranian experience of modernism, through which Iranian modernity and its associated art, science, literature, poetry, and political structures have been evolving since the 19th century. Religious intellectualism in Iran develops gradually and subtly. It reached its apogee during the Persian Constitutional Revolution (1906–11). The process involved numerous philosophers, sociologists, political scientists and cultural theorists. However the associated art, cinema and poetry remained to be developed.

"Modern Africa"

Recent concepts about African culture include the African Renaissance and Afrocentrism. The African Renaissance is a concept popularized by South African President Thabo Mbeki who called upon the African people and nations to solve the many problems troubling the African continent. It reached its height in the late 1990s but continues to be a key part of the post-apartheid intellectual agenda in South Africa. The concept however extends well beyond intellectual life to politics and economic development.

With the rise of Afrocentrism, the push away from Eurocentrism has led to the focus on the contributions of African people and their model of world civilization and history. Afrocentrism aims to shift the focus from a perceived European-centered history to an African-centered history. More broadly, Afrocentrism is concerned with distinguishing the influence of European and Oriental peoples from African achievements.

Notable modern African intellectual include:


</doc>
<doc id="53132" url="https://en.wikipedia.org/wiki?curid=53132" title="Humanities">
Humanities

Humanities are academic disciplines that study aspects of human society and culture. In the Renaissance, the term contrasted with divinity and referred to what is now called classics, the main area of secular study in universities at the time. Today, the humanities are more frequently contrasted with natural, and sometimes social sciences, as well as professional training.

The humanities use methods that are primarily critical, or speculative, and have a significant historical element—as distinguished from the mainly empirical approaches of the natural sciences, yet, unlike the sciences, it has no central discipline.
The humanities include the study of ancient and modern languages, literature, philosophy, history, human geography, law, politics, religion, and art.

Scholars in the humanities are "humanity scholars" or "humanists". The term "humanist" also describes the philosophical position of humanism, which some "antihumanist" scholars in the humanities reject. The Renaissance scholars and artists were also called humanists. Some secondary schools offer humanities classes usually consisting of literature, global studies and art.

Human disciplines like history, folkloristics, and cultural anthropology study subject matters that the manipulative experimental method does not apply to—and instead mainly use the comparative method and comparative research.

Anthropology is the holistic "science of humans", a science of the totality of human existence. The discipline deals with the integration of different aspects of the social sciences, humanities and human biology. In the twentieth century, academic disciplines have often been institutionally divided into three broad domains. The natural "sciences" seek to derive general laws through reproducible and verifiable experiments. The "humanities" generally study local traditions, through their history, literature, music, and arts, with an emphasis on understanding particular individuals, events, or eras. The "social sciences" have generally attempted to develop scientific methods to understand social phenomena in a generalizable way, though usually with methods distinct from those of the natural sciences.

The anthropological social sciences often develop nuanced descriptions rather than the general laws derived in physics or chemistry, or they may explain individual cases through more general principles, as in many fields of psychology. Anthropology (like some fields of history) does not easily fit into one of these categories, and different branches of anthropology draw on one or more of these domains. Within the United States, anthropology is divided into four sub-fields: archaeology, physical or biological anthropology, anthropological linguistics, and cultural anthropology. It is an area that is offered at most undergraduate institutions. The word "anthropos" (άνθρωπος) is from the Greek word for "human being" or "person". Eric Wolf described sociocultural anthropology as "the most scientific of the humanities, and the most humanistic of the sciences".

The goal of anthropology is to provide a holistic account of humans and human nature. This means that, though anthropologists generally specialize in only one sub-field, they always keep in mind the biological, linguistic, historic and cultural aspects of any problem. Since anthropology arose as a science in Western societies that were complex and industrial, a major trend within anthropology has been a methodological drive to study peoples in societies with more simple social organization, sometimes called "primitive" in anthropological literature, but without any connotation of "inferior". Today, anthropologists use terms such as "less complex" societies, or refer to specific modes of subsistence or production, such as "pastoralist" or "forager" or "horticulturalist", to discuss humans living in non-industrial, non-Western cultures, such people or folk ("ethnos") remaining of great interest within anthropology.

The quest for holism leads most anthropologists to study a people in detail, using biogenetic, archaeological, and linguistic data alongside direct observation of contemporary customs. In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. It is possible to view all human cultures as part of one large, evolving global culture. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.

Archaeology is the study of human activity through the recovery and analysis of material culture. The archaeological record consists of artifacts, architecture, biofacts or ecofacts, and cultural landscapes. Archaeology can be considered both a social science and a branch of the humanities. It has various goals, which range from understanding culture history to reconstructing past lifeways to documenting and explaining changes in human societies through time.

Archaeology is thought of as a branch of anthropology in the United States, while in Europe, it is viewed as a discipline in its own right, or grouped under other related disciplines such as history.

Classics, in the Western academic tradition, refers to the studies of the cultures of classical antiquity, namely Ancient Greek and Latin and the Ancient Greek and Roman cultures. Classical studies is considered one of the cornerstones of the humanities; however, its popularity declined during the 20th century. Nevertheless, the influence of classical ideas on many humanities disciplines, such as philosophy and literature, remains strong.

History is systematically collected information about the past. When used as the name of a field of study, "history" refers to the study and interpretation of the record of humans, societies, institutions, and any topic that has changed over time.

Traditionally, the study of history has been considered a part of the humanities. In modern academia, history is occasionally classified as a social science.

While the scientific study of language is known as linguistics and is generally considered a social science, a natural science or a cognitive science, the study of languages is still central to the humanities. A good deal of twentieth-century and twenty-first-century philosophy has been devoted to the analysis of language and to the question of whether, as Wittgenstein claimed, many of our philosophical confusions derive from the vocabulary we use; literary theory has explored the rhetorical, associative, and ordering features of language; and historical linguists have studied the development of languages across time. Literature, covering a variety of uses of language including prose forms (such as the novel), poetry and drama, also lies at the heart of the modern humanities curriculum. College-level programs in a foreign language usually include study of important works of the literature in that language, as well as the language itself.

 In common parlance, law means a rule that (unlike a rule of ethics) is enforceable through institutions. The study of law crosses the boundaries between the social sciences and humanities, depending on one's view of research into its objectives and effects. Law is not always enforceable, especially in the international relations context. It has been defined as a "system of rules", as an "interpretive concept" to achieve justice, as an "authority" to mediate people's interests, and even as "the command of a sovereign, backed by the threat of a sanction". However one likes to think of law, it is a completely central social institution. Legal policy incorporates the practical manifestation of thinking from almost every social science and discipline of the humanities. Laws are politics, because politicians create them. Law is philosophy, because moral and ethical persuasions shape their ideas. Law tells many of history's stories, because statutes, case law and codifications build up over time. And law is economics, because any rule about contract, tort, property law, labour law, company law and many more can have long-lasting effects on how productivity is organised and the distribution of wealth. The noun "law" derives from the late Old English "lagu", meaning something laid down or fixed, and the adjective "legal" comes from the Latin word "LEX".

 Literature is a term that does not have a universally accepted definition, but which has variably included all written work; writing that possesses literary merit; and language that foregrounds literariness, as opposed to ordinary language. Etymologically the term derives from Latin "literatura/litteratura" "writing formed with letters", although some definitions include spoken or sung texts. Literature can be classified according to whether it is fiction or non-fiction, and whether it is poetry or prose; it can be further distinguished according to major forms such as the novel, short story or drama; and works are often categorised according to historical periods, or according to their adherence to certain aesthetic features or expectations (genre).

Philosophy—etymologically, the "love of wisdom"—is generally the study of problems concerning matters such as existence, knowledge, justification, truth, justice, right and wrong, beauty, validity, mind, and language. Philosophy is distinguished from other ways of addressing these issues by its critical, generally systematic approach and its reliance on reasoned argument, rather than experiments (experimental philosophy being an exception).

Philosophy used to be a very comprehensive term, including what have subsequently become separate disciplines, such as physics. (As Immanuel Kant noted, "Ancient Greek philosophy was divided into three sciences: physics, ethics, and logic.") Today, the main fields of philosophy are logic, ethics, metaphysics, and epistemology. Still, it continues to overlap with other disciplines. The field of semantics, for example, brings philosophy into contact with linguistics.

Since the early twentieth century, philosophy in English-speaking universities has moved away from the humanities and closer to the formal sciences, becoming much more "analytic." Analytic philosophy is marked by emphasis on the use of logic and formal methods of reasoning, conceptual analysis, and the use of symbolic and/or mathematical logic, as contrasted with the Continental style of philosophy. This method of inquiry is largely indebted to the work of philosophers such as Gottlob Frege, Bertrand Russell, G.E. Moore, Georg Wilhelm Friedrich Hegel and Ludwig Wittgenstein.

New philosophies and religions arose in both the East and West, particularly around the 6th century BC. Over time, a great variety of religions developed around the world, with Hinduism, Jainism, and Buddhism in India, and Zoroastrianism in Persia being some of the earliest major faiths. In the east, three schools of thought were to dominate Chinese thinking until the modern day. These were Taoism, Legalism, and Confucianism. The Confucian tradition, which would attain predominance, looked not to the force of law, but to the power and example of tradition for political morality. In the west, the Greek philosophical tradition, represented by the works of Plato and Aristotle, was diffused throughout Europe and the Middle East by the conquests of Alexander of Macedon in the 4th century BC.

Abrahamic religions are those religions deriving from a common ancient tradition and traced by their adherents to Abraham (circa 1900 BCE), a patriarch whose life is narrated in the Hebrew Bible/Old Testament, where he is described as a prophet (Genesis 20:7), and in the Quran, where he also appears as a prophet. This forms a large group of related largely monotheistic religions, generally held to include Judaism, Christianity, and Islam, and comprises over half of the world's religious adherents.

The performing arts differ from the visual arts in so far as the former uses the artist's own body, face, and presence as a medium, and the latter uses materials such as clay, metal, or paint, which can be molded or transformed to create some art object. Performing arts include acrobatics, busking, comedy, dance, film, magic, music, opera, juggling, marching arts, such as brass bands, and theatre.

Artists who participate in these arts in front of an audience are called performers, including actors, comedians, dancers, musicians, and singers. Performing arts are also supported by workers in related fields, such as songwriting and stagecraft. Performers often adapt their appearance, such as with costumes and stage makeup, etc. There is also a specialized form of fine art in which the artists "perform" their work live to an audience. This is called Performance art. Most performance art also involves some form of plastic art, perhaps in the creation of props. Dance was often referred to as a "plastic art" during the Modern dance era.

Musicology as an academic discipline can take a number of different paths, including historical musicology, music literature, ethnomusicology and music theory. Undergraduate music majors generally take courses in all of these areas, while graduate students focus on a particular path. In the liberal arts tradition, musicology is also used to broaden skills of non-musicians by teaching skills such as concentration and listening.

Theatre (or theater) (Greek "theatron", "θέατρον") is the branch of the performing arts concerned with acting out stories in front of an audience using combinations of speech, gesture, music, dance, sound and spectacle — indeed any one or more elements of the other performing arts. In addition to the standard narrative dialogue style, theatre takes such forms as opera, ballet, mime, kabuki, classical Indian dance, Chinese opera, mummers' plays, and pantomime.

Dance (from Old French "dancier", perhaps from Frankish) generally refers to human movement either used as a form of expression or presented in a social, spiritual or performance setting. Dance is also used to describe methods of non-verbal communication (see body language) between humans or animals (bee dance, mating dance), and motion in inanimate objects ("the leaves danced in the wind"). Choreography is the art of creating dances, and the person who does this is called a choreographer.

Definitions of what constitutes dance are dependent on social, cultural, aesthetic, artistic, and moral constraints and range from functional movement (such as Folk dance) to codified, virtuoso techniques such as ballet.

The great traditions in art have a foundation in the art of one of the ancient civilizations, such as Ancient Japan, Greece and Rome, China, India, Greater Nepal, Mesopotamia and Mesoamerica.

Ancient Greek art saw a veneration of the human physical form and the development of equivalent skills to show musculature, poise, beauty and anatomically correct proportions. Ancient Roman art depicted gods as idealized humans, shown with characteristic distinguishing features (e.g., Zeus' thunderbolt).

In Byzantine and Gothic art of the Middle Ages, the dominance of the church insisted on the expression of biblical and not material truths. The Renaissance saw the return to valuation of the material world, and this shift is reflected in art forms, which show the corporeality of the human body, and the three-dimensional reality of landscape.

Eastern art has generally worked in a style akin to Western medieval art, namely a concentration on surface patterning and local colour (meaning the plain colour of an object, such as basic red for a red robe, rather than the modulations of that colour brought about by light, shade and reflection). A characteristic of this style is that the local colour is often defined by an outline (a contemporary equivalent is the cartoon). This is evident in, for example, the art of India, Tibet and Japan.

Religious Islamic art forbids iconography, and expresses religious ideas through geometry instead. The physical and rational certainties depicted by the 19th-century Enlightenment were shattered not only by new discoveries of relativity by Einstein and of unseen psychology by Freud, but also by unprecedented technological development. Increasing global interaction during this time saw an equivalent influence of other cultures into Western art.

Drawing is a means of making a picture, using any of a wide variety of tools and techniques. It generally involves making marks on a surface by applying pressure from a tool, or moving a tool across a surface. Common tools are graphite pencils, pen and ink, inked brushes, wax color pencils, crayons, charcoals, pastels, and markers. Digital tools that simulate the effects of these are also used. The main techniques used in drawing are: line drawing, hatching, crosshatching, random hatching, scribbling, stippling, and blending. A computer aided designer who excels in technical drawing is referred to as a "draftsman" or "draughtsman".

Painting taken literally is the practice of applying pigment suspended in a carrier (or medium) and a binding agent (a glue) to a surface (support) such as paper, canvas or a wall. However, when used in an artistic sense it means the use of this activity in combination with drawing, composition and other aesthetic considerations in order to manifest the expressive and conceptual intention of the practitioner. Painting is also used to express spiritual motifs and ideas; sites of this kind of painting range from artwork depicting mythological figures on pottery to The Sistine Chapel to the human body itself.

Colour is highly subjective, but has observable psychological effects, although these can differ from one culture to the next. Black is associated with mourning in the West, but elsewhere white may be. Some painters, theoreticians, writers and scientists, including Goethe, Kandinsky, Isaac Newton, have written their own colour theories. Moreover, the use of language is only a generalization for a colour equivalent. The word "red", for example, can cover a wide range of variations on the pure red of the spectrum. There is not a formalized register of different colours in the way that there is agreement on different notes in music, such as C or C# in music, although the Pantone system is widely used in the printing and design industry for this purpose.

Modern artists have extended the practice of painting considerably to include, for example, collage. This began with cubism and is not painting in strict sense. Some modern painters incorporate different materials such as sand, cement, straw or wood for their texture. Examples of this are the works of Jean Dubuffet or Anselm Kiefer. Modern and contemporary art has moved away from the historic value of craft in favour of concept; this has led some to say that painting, as a serious art form, is dead, although this has not deterred the majority of artists from continuing to practise it either as whole or part of their work.

The word "humanities" is derived from the Renaissance Latin expression "studia humanitatis", or "study of "humanitas"" (a classical Latin word meaning—in addition to "humanity"—"culture, refinement, education" and, specifically, an "education befitting a cultivated man"). In its usage in the early 15th century, the "studia humanitatis" was a course of studies that consisted of grammar, poetry, rhetoric, history, and moral philosophy, primarily derived from the study of Latin and Greek classics. The word "humanitas" also gave rise to the Renaissance Italian neologism "umanisti", whence "humanist", "Renaissance humanism".

In the West, the study of the humanities can be traced to ancient Greece, as the basis of a broad education for citizens. During Roman times, the concept of the seven liberal arts evolved, involving grammar, rhetoric and logic (the trivium), along with arithmetic, geometry, astronomy and music (the quadrivium). These subjects formed the bulk of medieval education, with the emphasis being on the humanities as skills or "ways of doing".

A major shift occurred with the Renaissance humanism of the fifteenth century, when the humanities began to be regarded as subjects to study rather than practice, with a corresponding shift away from traditional fields into areas such as literature and history. In the 20th century, this view was in turn challenged by the postmodernist movement, which sought to redefine the humanities in more egalitarian terms suitable for a democratic society since the Greek and Roman societies in which the humanities originated were not at all democratic. This was in keeping with the postmodernists' nuanced view of themselves as the culmination of history.

For many decades, there has been a growing public perception that a humanities education inadequately prepares graduates for employment. The common belief is that graduates from such programs face underemployment and incomes too low for a humanities education to be worth the investment.

In fact, humanities graduates find employment in a wide variety of management and professional occupations. In Britain, for example, over 11,000 humanities majors found employment in the following occupations:
Many humanities graduates finish university with no career goals in mind. Consequently, many spend the first few years after graduation deciding what to do next, resulting in lower incomes at the start of their career; meanwhile, graduates from career-oriented programs experience more rapid entry into the labour market. However, usually within five years of graduation, humanities graduates find an occupation or career path that appeals to them.

There is empirical evidence that graduates from humanities programs earn less than graduates from other university programs. However, the empirical evidence also shows that humanities graduates still earn notably higher incomes than workers with no postsecondary education, and have job satisfaction levels comparable to their peers from other fields. Humanities graduates also earn more as their careers progress; ten years after graduation, the income difference between humanities graduates and graduates from other university programs is no longer statistically significant. Humanities graduates can earn even higher incomes if they obtain advanced or professional degrees.

The Humanities Indicators, unveiled in 2009 by the American Academy of Arts and Sciences, are the first comprehensive compilation of data about the humanities in the United States, providing scholars, policymakers and the public with detailed information on humanities education from primary to higher education, the humanities workforce, humanities funding and research, and public humanities activities. Modeled after the National Science Board's Science and Engineering Indicators, the Humanities Indicators are a source of reliable benchmarks to guide analysis of the state of the humanities in the United States.

If "The STEM Crisis Is a Myth", statements about a "crisis" in the humanities are also misleading and ignore data of the sort collected by the Humanities Indicators.

The 1980 United States Rockefeller Commission on the Humanities described the humanities in its report, "The Humanities in American Life":
Through the humanities we reflect on the fundamental question: What does it mean to be human? The humanities offer clues but never a complete answer. They reveal how people have tried to make moral, spiritual, and intellectual sense of a world where irrationality, despair, loneliness, and death are as conspicuous as birth, friendship, hope, and reason.

In 1950, a little over 1 percent of 22-year-olds in the United States had earned a humanities degrees (defined as a degree in English, language, history, philosophy); in 2010, this had doubled to about 2 and a half percent. In part, this is because there was an overall rise in the number of Americans who have any kind of college degree. (In 1940, 4.6 percent had a four-year degree; in 2016, 33.4 percent had one.) As a percentage of the type of degrees awarded, however, the humanities seem to be declining. Harvard University provides one example. In 1954, 36 percent of Harvard undergraduates majored in the humanities, but in 2012, only 20 percent took that course of study. Professor Benjamin Schmidt of Northeastern University has documented that between 1990 and 2008, degrees in English, history, foreign languages, and philosophy have decreased from 8 percent to just under 5 percent of all U.S. college degrees.

The Commission on the Humanities and Social Sciences 2013 report "The Heart of the Matter" supports the notion of a broad "liberal arts education", which includes study in disciplines from the natural sciences to the arts as well as the humanities.

Many colleges provide such an education; some require it. The University of Chicago and Columbia University were among the first schools to require an extensive core curriculum in philosophy, literature, and the arts for all students. Other colleges with nationally recognized, mandatory programs in the liberal arts are Fordham University, St. John's College, Saint Anselm College and Providence College. Prominent proponents of liberal arts in the United States have included Mortimer J. Adler and E. D. Hirsch, Jr..

Researchers in the humanities have developed numerous large- and small-scale digital corporation, such as digitized collections of historical texts, along with the digital tools and methods to analyze them. Their aim is both to uncover new knowledge about corpora and to visualize research data in new and revealing ways. Much of this activity occurs in a field called the digital humanities.

Politicians in the United States currently espouse a need for increased funding of the STEM fields, science, technology, engineering, mathematics. Federal funding represents a much smaller fraction of funding for humanities than other fields such as STEM or medicine. The result was a decline of quality in both college and pre-college education in the humanities field.

Former four-term Louisiana Governor, Edwin Edwards (D), has recently acknowledged the importance of the humanities. In a video address to the academic conference, "Revolutions in Eighteenth-Century Sociability", Edwards said

The contemporary debate in the field of critical university studies centers around the declining value of the humanities. As in America, there is a perceived decline in interest within higher education policy in research that is qualitative and does not produce marketable products. This threat can be seen in a variety of forms across Europe, but much critical attention has been given to the field of research assessment in particular. For example, the UK [Research Excellence Framework] has been subject to criticism due to its assessment criteria from across the humanities, and indeed, the social sciences. In particular, the notion of "impact" has generated significant debate.

In India, there are many institutions that offer undergraduate UG or bachelor's degree/diploma and postgraduate PG or master's degree/diploma as well as doctoral PhD and postdoctoral studies and research, in this academic discipline.

Since the late 19th century, a central justification for the humanities has been that it aids and encourages self-reflection—a self-reflection that, in turn, helps develop personal consciousness or an active sense of civic duty.

Wilhelm Dilthey and Hans-Georg Gadamer centered the humanities' attempt to distinguish itself from the natural sciences in humankind's urge to understand its own experiences. This understanding, they claimed, ties like-minded people from similar cultural backgrounds together and provides a sense of cultural continuity with the philosophical past.

Scholars in the late 20th and early 21st centuries extended that "narrative imagination" to the ability to understand the records of lived experiences outside of one's own individual social and cultural context. Through that narrative imagination, it is claimed, humanities scholars and students develop a conscience more suited to the multicultural world we live in. That conscience might take the form of a passive one that allows more effective self-reflection or extend into active empathy that facilitates the dispensation of civic duties a responsible world citizen must engage in. There is disagreement, however, on the level of influence humanities study can have on an individual and whether or not the understanding produced in humanistic enterprise can guarantee an "identifiable positive effect on people."

There are three major branches of knowledge: natural sciences, social sciences, and the humanities. Technology is the practical extension of the natural sciences, as politics is the extension of the social sciences. Similarly, the humanities have their own practical extension, sometimes called "transformative humanities" (transhumanities) or "culturonics" (Mikhail Epstein's term):
Technology, politics and culturonics are designed to transform what their respective disciplines study: nature, society, and culture. The field of transformative humanities includes various practicies and technologies, for example, language planning, the construction of new languages, like Esperanto, and invention of new artistic and literary genres and movements in the genre of manifesto, like Romanticism, Symbolism, or Surrealism. Humanistic invention in the sphere of culture, as a practice complementary to scholarship, is an important aspect of the humanities.

The divide between humanistic study and natural sciences informs arguments of meaning in humanities as well. What distinguishes the humanities from the natural sciences is not a certain subject matter, but rather the mode of approach to any question. Humanities focuses on understanding meaning, purpose, and goals and furthers the appreciation of singular historical and social phenomena—an interpretive method of finding "truth"—rather than explaining the causality of events or uncovering the truth of the natural world. Apart from its societal application, narrative imagination is an important tool in the (re)production of understood meaning in history, culture and literature.

Imagination, as part of the tool kit of artists or scholars, helps create meaning that invokes a response from an audience. Since a humanities scholar is always within the nexus of lived experiences, no "absolute" knowledge is theoretically possible; knowledge is instead a ceaseless procedure of inventing and reinventing the context a text is read in. Poststructuralism has problematized an approach to the humanistic study based on questions of meaning, intentionality, and authorship. In the wake of the death of the author proclaimed by Roland Barthes, various theoretical currents such as deconstruction and discourse analysis seek to expose the ideologies and rhetoric operative in producing both the purportedly meaningful objects and the hermeneutic subjects of humanistic study. This exposure has opened up the interpretive structures of the humanities to criticism that humanities scholarship is "unscientific" and therefore unfit for inclusion in modern university curricula because of the very nature of its changing contextual meaning.

Some, like Stanley Fish, have claimed that the humanities can defend themselves best by refusing to make any claims of utility. (Fish may well be thinking primarily of literary study, rather than history and philosophy.) Any attempt to justify the humanities in terms of outside benefits such as social usefulness (say increased productivity) or in terms of ennobling effects on the individual (such as greater wisdom or diminished prejudice) is ungrounded, according to Fish, and simply places impossible demands on the relevant academic departments. Furthermore, critical thinking, while arguably a result of humanistic training, can be acquired in other contexts. And the humanities do not even provide any more the kind of social cachet (what sociologists sometimes call "cultural capital") that was helpful to succeed in Western society before the age of mass education following World War II.

Instead, scholars like Fish suggest that the humanities offer a unique kind of pleasure, a pleasure based on the common pursuit of knowledge (even if it is only disciplinary knowledge). Such pleasure contrasts with the increasing privatization of leisure and instant gratification characteristic of Western culture; it thus meets Jürgen Habermas' requirements for the disregard of social status and rational problematization of previously unquestioned areas necessary for an endeavor which takes place in the bourgeois public sphere. In this argument, then, only the academic pursuit of pleasure can provide a link between the private and the public realm in modern Western consumer society and strengthen that public sphere that, according to many theorists, is the foundation for modern democracy.

Others, like Mark Bauerlein, argue that professors in the humanities have increasingly abandoned proven methods of epistemology ("I care only about the quality of your arguments, not your conclusions.") in favor of indoctrination ("I care only about your conclusions, not the quality of your arguments."). The result is that professors and their students adhere rigidly to a limited set of viewpoints, and have little interest in, or understanding of, opposing viewpoints. Once they obtain this intellectual self-satisfaction, persistent lapses in learning, research, and evaluation are common.

Implicit in many of these arguments supporting the humanities are the makings of arguments against public support of the humanities. Joseph Carroll asserts that we live in a changing world, a world where "cultural capital" is replaced with "scientific literacy", and in which the romantic notion of a Renaissance humanities scholar is obsolete. Such arguments appeal to judgments and anxieties about the essential uselessness of the humanities, especially in an age when it is seemingly vitally important for scholars of literature, history and the arts to engage in "collaborative work with experimental scientists or even simply to make "intelligent use of the findings from empirical science." 

Despite many humanities based arguments against the humanities some within the exact sciences have called for their return. In 2017, Science popularizer Bill Nye retracted previous claims about the supposed 'uselessness' of philosophy. As Bill Nye states, “People allude to Socrates and Plato and Aristotle all the time, and I think many of us who make those references don’t have a solid grounding,” he said. “It’s good to know the history of philosophy.” Scholars, such as biologist Scott F. Gilbert, make the claim that it is in fact the increasing predominance, leading to exclusivity, of scientific ways of thinking that need to be tempered by historical and social context. Gilbert worries that the commercialization that may be inherent in some ways of conceiving science (pursuit of funding, academic prestige etc.) need to be examined externally. Gilbert argues "First of all, there is a very successful alternative to science as a commercialized march to “progress.” This is the approach taken by the liberal arts college, a model that takes pride in seeing science in context and in integrating science with the humanities and social sciences."




</doc>
<doc id="4633449" url="https://en.wikipedia.org/wiki?curid=4633449" title="Literary nonsense">
Literary nonsense

Literary nonsense (or nonsense literature) is a broad categorization of literature that balances elements that make sense with some that do not, with the effect of subverting language conventions or logical reasoning. Even though the most well-known form of literary nonsense is nonsense verse, the genre is present in many forms of literature.

The effect of nonsense is often caused by an excess of meaning, rather than a lack of it. Its humor is derived from its nonsensical nature, rather than wit or the "joke" of a punchline.

Literary nonsense, as recognized since the nineteenth century, comes from a combination of two broad artistic sources. The first and older source is the oral folk tradition, including games, songs, dramas, and rhymes, such as the nursery rhyme "Hey Diddle Diddle". The literary figure Mother Goose represents common incarnations of this style of writing.

The second, newer source of literary nonsense is in the intellectual absurdities of court poets, scholars, and intellectuals of various kinds. These writers often created sophisticated nonsense forms of Latin parodies, religious travesties, and political satire, though these texts are distinguished from more pure satire and parody by their exaggerated nonsensical effects.

Today's literary nonsense comes from a combination of both sources. Though not the first to write this hybrid kind of nonsense, Edward Lear developed and popularized it in his many limericks (starting with "A Book of Nonsense", 1846) and other famous texts such as "The Owl and the Pussycat", "The Dong with a Luminous Nose," "" and "The Story of the Four Little Children Who Went Around the World". Lewis Carroll continued this trend, making literary nonsense a worldwide phenomenon with "Alice's Adventures in Wonderland" (1865) and "Through the Looking-Glass" (1871). Carroll's poem "Jabberwocky", which appears in the latter book, is often considered quintessential nonsense literature.

In literary nonsense, certain formal elements of language and logic that facilitate meaning are balanced by elements that negate meaning. These formal elements include semantics, syntax, phonetics, context, representation, and formal diction. The genre is most easily recognizable by the various techniques or devices it uses to create this balance of meaning and lack of meaning, such as faulty cause and effect, portmanteau, neologism, reversals and inversions, imprecision (including gibberish), simultaneity, picture/text incongruity, arbitrariness, infinite repetition, negativity or mirroring, and misappropriation. Nonsense tautology, reduplication, and absurd precision have also been used in the nonsense genre. For a text to be within the genre of literary nonsense, it must have an abundance of nonsense techniques woven into the fabric of the piece. If the text employs only occasional nonsense devices, then it may not be classified as literary nonsense, though there may be a nonsensical effect to certain portions of the work. Laurence Sterne's "Tristram Shandy", for instance, employs the nonsense device of imprecision by including a blank page, but this is only one nonsense device in a novel that otherwise makes sense. In Flann O'Brien's "The Third Policeman", on the other hand, many of the devices of nonsense are present throughout, and thus it could be considered a nonsense novel.

Gibberish, light verse, fantasy, and jokes and riddles are sometimes mistaken for literary nonsense, and the confusion is greater because nonsense can sometimes inhabit these (and many other) forms and genres.

Pure gibberish, as in the "hey diddle diddle" of nursery rhyme, is a device of nonsense, but it does not make a text, overall, literary nonsense. If there is not significant sense to balance out such devices, then the text dissolves into literal (as opposed to literary) nonsense.

Light verse, which is generally speaking humorous verse meant to entertain, may share humor, inconsequentiality, and playfulness, with nonsense, but it usually has a clear point or joke, and does not have the requisite tension between meaning and lack of meaning.

Nonsense is distinct from fantasy, though there are sometimes resemblances between them. While nonsense may employ the strange creatures, other worldly situations, magic, and talking animals of fantasy, these supernatural phenomena are not nonsensical if they have a discernible logic supporting their existence. The distinction lies in the coherent and unified nature of fantasy. Everything follows logic within the rules of the fantasy world; the nonsense world, on the other hand, has no comprehensive system of logic, although it may imply the existence of an inscrutable one, just beyond our grasp. The nature of magic within an imaginary world is an example of this distinction. Fantasy worlds employ the presence of magic to logically explain the impossible. In nonsense literature, magic is rare but when it does occur, its nonsensical nature only adds to the mystery rather than logically explaining anything. An example of nonsensical magic occurs in Carl Sandburg's "Rootabaga Stories", when Jason Squiff, in possession of a magical "gold buckskin whincher", has his hat, mittens, and shoes turn into popcorn because, according to the "rules" of the magic, "You have a letter Q in your name and because you have the pleasure and happiness of having a Q in your name you must have a popcorn hat, popcorn mittens and popcorn shoes".

Riddles only appear to be nonsense until the answer is found. The most famous nonsense riddle is only so because it originally had no answer. In Carroll's "Alice in Wonderland", the Mad Hatter asks Alice "Why is a raven like a writing-desk?" When Alice gives up, the Hatter replies that he does not know either, creating a nonsensical riddle. Some seemingly nonsense texts are actually riddles, such as the popular 1940s song "Mairzy Doats", which at first appears to have little discernible meaning but has a discoverable message. Jokes are not nonsense because their humor comes from their making sense, from our "getting" it, while nonsense is funny because it does "not" make sense, we do not "get" it.

While most contemporary nonsense has been written for children, the form has an extensive history in adult configurations before the nineteenth century. Figures such as John Hoskyns, Henry Peacham, John Sandford, and John Taylor lived in the early seventeenth century and were noted nonsense authors in their time. Nonsense was also an important element in the works of Flann O'Brien and Eugène Ionesco. Literary nonsense, as opposed to the folk forms of nonsense that have always existed in written history, was only first written for children in the early nineteenth century. It was popularized by Edward Lear and then later by Lewis Carroll. Today literary nonsense enjoys a shared audience of adults and children.

"Note: None of these writers is considered "exclusively" a "nonsense writer". Some of them wrote texts considered to be in the genre (as in Lear, Carroll, Gorey, Lennon, Sandburg), while others only use nonsense as an occasional device (as in Joyce, Juster). All of these writers wrote outside of the nonsense genre also."


Writers of nonsense from other languages include:

Bob Dylan wrote some lyrics that contain nonsense techniques, especially around the mid-1960s, in songs like "Bob Dylan's 115th Dream" and "Tombstone Blues".<br>

David Byrne, frontman of the art rock/new wave group Talking Heads, employed nonsensical techniques in songwriting. Byrne often combined coherent yet unrelated phrases to make up nonsensical lyrics in songs such as: "Burning Down the House", "Making Flippy Floppy" and "Girlfriend Is Better". This tendency formed the basis of the title for the Talking Heads concert movie, "Stop Making Sense". More recently, Byrne published "Arboretum" (2006), a volume of tree-like diagrams that are, "mental maps of imaginary territory". He continues, explaining the aspect of nonsense: "Irrational logic – [...]. The application of logical scientific rigor and form to basically irrational premises. To proceed, carefully and deliberately, from nonsense, with a straight face, often arriving at a new kind of sense."

Syd Barrett, one-time frontman and founder of Pink Floyd, was known for his often nonsensical songwriting influenced by Lear and Carroll that featured heavily on Pink Floyd's first album, "The Piper at the Gates of Dawn".

Glen Baxter's comic work is often nonsense, relying on the baffling interplay between word and image.

"Zippy the Pinhead", by Bill Griffith, is an American strip that mixes philosophy, including what has been called "Heideggerian disruptions" and pop culture in its nonsensical processes.


_________. "The Complete Works of Lewis Carroll". London: Nonesuch Press, 1940.
_________. "Your Disgusting Head: The Darkest, Most Offensive—and Moist—Secrets of Your Ears, Mouth and Nose, Volume 2., 2004.
_________. "Animals of the Ocean, In particular the giant squid", Volume 3, 2006
_________. "Cold Fusion", Volume 4, 2008
_________. "Amphigorey too". New York: Perigee, 1975.
_________. "Amphigorey Also". Harvest, 1983.
_________. "Amphigorey Again". Barnes & Noble, 2002.
_________. "The Writings of John Lennon: In His Own Write, A Spaniard in the Works" New York: Simon and Schuster, 1964, 1965.
_________. "Captain Slaughterboard Drops Anchor". London: Country Life Book, 1939.
_________. "Rhymes Without Reason". Eyre & Spottiswoode, 1944.
_________. "Titus Groan". London:, London: Methuen, 1946.
_________. "Wish You Were Here", Chennai: Tara Publishing, 2003.
_________. "Today is My Day", illus. Piet Grobler, Chennai: Tara Publishing, 2003.
_________. "Tirra Lirra: Rhymes Old and New", illus. Marguerite Davis. London: George G. Harrap, 1933.
_________. "More Rootabaga Stories".


_________. "Edward Lear's Limericks and the Reversals of Nonsense," Victorian Poetry, 29 (1988): 285–299.
_________. "The Limerick and the Space of Metaphor," Genre, 21 (Spring 1988): 65–91.
_________. "Society and the Self in the Limericks of Lear," The Review of English Studies, 177 (1994): 42–62.
_________. "Edward Lear's Limericks and Their Illustrations" in Explorations in the Field of Nonsense, ed. Wim Tigges (Amsterdam: Rodopi, 1987), pp. 101–116.
_________. "An Introduction to the Nonsense Literature of Edward Lear and Lewis Carroll" in Explorations in the Field of Nonsense, ed. Wim Tigges (Amsterdam: Rodopi, 1987), pp. 47–60.
_________. "Edward Lear: Eccentricity and Victorian Angst," Victorian Poetry, 16 (1978): 112–122.
_________. "A New Defense of Nonsense; or, 'Where is his phallus?' and other questions not to ask" in Children's Literature Association Quarterly, Winter 1999–2000. Volume 24, Number 4 (186–194)
_________. "An Indian Nonsense Naissance" in "The Tenth Rasa: An Anthology of Indian Nonsense", edited by Michael Heyman, with Sumanyu Satpathy and Anushka Ravishankar. New Delhi: Penguin, 2007.<br>
_________. "Nonsense", with Kevin Shortsleeve, in "Keywords for Children's Literature". eds. Philip Nel and Lissa Paul. New York: NYU Press, 2011.<br>
_________. "The Perils and Nonpereils of Literary Nonsense Translation." Words Without Borders. June 2, 2014.
_________. "Edward Lear, 1812–1888". London: Weidenfeld & Nicolson, 1985.
_________. "The Limerick: The Sonnet of Nonsense?" "Dutch Quarterly Review", 16 (1986): 220–236.
_________. ed., "Explorations in the Field of Nonsense". Amsterdam: Rodopi, 1987.



</doc>
<doc id="44183472" url="https://en.wikipedia.org/wiki?curid=44183472" title="Spatial turn">
Spatial turn

Spatial turn is an intellectual movement that places emphasis on place and space in social science and the humanities. It is closely linked with quantitative studies of history, literature, cartography, and other studies of society. The movement has been influential in providing mass amounts of data for study of cultures, regions, and specific locations.

Academics such as Ernst Cassirer and Lewis Mumford helped to define a sense of "community" and "commons" in their studies, forming the first part of a "spatial turn." The turn developed more comprehensively in the later twentieth century in French academic theories, such as those of Michel Foucault. 

Technologies have also played an important role in "turns." The introduction of Geographic Information Systems (GIS) has also been instrumental in quantifying data in the humanities for study by its place.


</doc>
<doc id="22900852" url="https://en.wikipedia.org/wiki?curid=22900852" title="History of art criticism">
History of art criticism

The history of art criticism, as part of art history, is the study of objects of art in their historical development and stylistic contexts, i.e. genre, design, format, and style, which include aesthetic considerations. This includes the "major" arts of painting, sculpture, and architecture as well as the "minor" arts of ceramics, furniture, and other decorative objects.

As a term, the history of art history (also history of art) encompasses several methods of studying and assessing the visual arts; in common usage referring to works of art and architecture. Aspects of the discipline overlap. As the art historian Ernst Gombrich once observed, "the field of art history [is] much like Caesar's Gaul, divided in three parts inhabited by three different, though not necessarily hostile tribes: (i) the connoisseurs, (ii) the critics, and (iii) the academic art historians".

As a discipline, the history of art criticism is distinguished from art criticism, which is concerned with establishing a relative artistic value upon individual works with respect to others of comparable style, or sanctioning an entire style or movement from the standpoint of its history and of its major scholars. It is also distinguished from art theory or "philosophy of art", which is concerned with the fundamental nature of art. One branch of this area of study is aesthetics, which includes investigating the enigma of the sublime and determining the essence of beauty. Technically, art history is not these things, because the art historian uses historical method to answer the questions: How did the artist come to create the work?, Who were the patrons?, Who were his or her teachers?, Who was the audience?, Who were his or her disciples?, What historical forces shaped the artist's oeuvre, and How did he or she and the creation, in turn, affect the course of artistic, political, and social events? It is, however, questionable whether many questions of this kind can be answered satisfactorily without also considering basic questions about the nature of art. Unfortunately the current disciplinary gap between art history and the philosophy of art (aesthetics) often hinders this.

The history of art criticism is not only a biographical endeavor. The history of art criticism often roots its studies in the scrutiny of individual objects. It attempt to answer in historically specific ways, questions such as: What are key features of this style?, What meaning did this object convey?, How does it function visually?, Did the artist meet their goals well?, What symbols are involved?, and Does it function discursively?

The historical backbone of the discipline is a celebratory chronology of beautiful creations commissioned by public or religious bodies or wealthy individuals in western Europe. Such a "canon" remains prominent, as indicated by the selection of objects present in art history textbooks. Nonetheless, since the 20th century there has been an effort to re-define the discipline to be more inclusive of non-Western art, art made by women, and vernacular creativity.

The history of art criticism as we know it in the 21st century began in the 19th century but has precedents that date to the ancient world. Like the analysis of historical trends in politics, literature, and the sciences, the discipline benefits from the clarity and portability of the written word, but art historians also rely on formal analysis, semiotics, psychoanalysis and iconography. Advances in photographic reproduction and printing techniques after World War II increased the ability of reproductions of artworks. Such technologies have helped to advance the discipline in profound ways, as they have enabled easy comparisons of objects. The study of visual art thus described, can be a practice that involves understanding context, form, and social significance.

Art historians, in performing their assessment within the history of art criticism, employ a number of methods in their research into the ontology and history of objects.

Practitioners of art criticism often examine work in the context of its time. At best, this is done in a manner which respects its creator's motivations and imperatives; with consideration of the desires and prejudices of its patrons and sponsors; with a comparative analysis of themes and approaches of the creator's colleagues and teachers; and with consideration of iconography and symbolism. In short, this approach examines the work of art in the context of the world within which it was created.

Practitioners of art criticism also often examine work through an analysis of form; that is, the creator's use of line, shape, color, texture, and composition. This approach examines how the artist uses a two-dimensional picture plane or the three dimensions of sculptural or architectural space to create his or her art. The way these individual elements are employed results in representational or non-representational art. Is the artist imitating an object or image found in nature? If so, it is representational. The closer the art hews to perfect imitation, the more the art is "realistic". Is the artist not imitating, but instead relying on symbolism, or in an important way striving to capture nature's essence, rather than copy it directly? If so the art is non-representational—also called abstract. Realism and abstraction exist on a continuum. Impressionism is an example of a representational style that was not directly imitative, but strove to create an "impression" of nature. If the work is not representational and is an expression of the artist's feelings, longings and aspirations, or is a search for ideals of beauty and form, the work is non-representational or a work of expressionism.

An iconographical analysis is one which focuses on particular design elements of an object. Through a close reading of such elements, it is possible to trace their lineage, and with it draw conclusions regarding the origins and trajectory of these motifs. In turn, it is possible to make any number of observations regarding the social, cultural, economic, and aesthetic values of those responsible for producing the object.

Many practitioners of art criticism use critical theory to frame their inquiries into objects. Theory is most often used when dealing with more recent objects, those from the late 19th century onward. Critical theory in art history is often borrowed from literary scholars, and it involves the application of a non-artistic analytical framework to the study of art objects. Feminist, Marxist, critical race, queer, and postcolonial theories are all well established in the discipline. As in literary studies, there is an interest among scholars in nature and the environment, but the direction that this will take in the discipline has yet to be determined.

More recently, media and digital technology introduced possibilities of visual, spatial and experiential analyses. The relevant forms vary from movies, to interactive forms, including virtual environments, augmented environments, situated media, networked media, etc. The methods enabled by such techniques are in active development and promise to include qualitative approaches that can emphasize narrative, dramatic, emotional and ludic characteristics of history and art.

The earliest surviving writing on art that can be classified as art history or art criticism are the passages in Pliny the Elder's "Natural History" (c. AD 77-79), concerning the development of Greek sculpture and painting. From them it is possible to trace the ideas of Xenokrates of Sicyon (c. 280 BC), a Greek sculptor who was perhaps the first art historian. Pliny's work, while mainly an encyclopaedia of the sciences, has thus been influential from the Renaissance onwards. (Passages about techniques used by the painter Apelles c. (332-329 BC), have been especially well-known.) Similar, though independent, developments occurred in the 6th century China, where a canon of worthy artists was established by writers in the scholar-official class. These writers, being necessarily proficient in calligraphy, were artists themselves. The artists are described in the "Six Principles of Painting" formulated by Xie He.

While personal reminiscences of art and artists have long been written and read (see Lorenzo Ghiberti "Commentarii," for the best early example), it was Giorgio Vasari, the Tuscan painter, sculptor and author of the "Lives of the Painters", who wrote the first true "history" of art. He emphasized art's progression and development, which was a milestone in this field. His was a personal and a historical account, featuring biographies of individual Italian artists, many of whom were his contemporaries and personal acquaintances. The most renowned of these was Michelangelo, and Vasari's account is enlightening, though biased in places.

Vasari's ideas about art were enormously influential, and served as a model for many, including in the north of Europe Karel van Mander's "Schilder-boeck" and Joachim von Sandrart's "Teutsche Akademie". Vasari's approach held sway until the 18th century, when criticism was leveled at his biographical account of history.

Scholars such as Johann Joachim Winckelmann (1717–1768), criticised Vasari's "cult" of artistic personality, and they argued that the real emphasis in the study of art should be the views of the learned beholder and not the unique viewpoint of the charismatic artist. Winckelmann's writings thus were the beginnings of art criticism. His two most notable works that introduced the concept of art criticism were "Gedanken über die Nachahmung der griechischen Werke in der Malerei und Bildhauerkunst, published in 1755, shortly before he left for Rome (Fuseli published an English translation in 1765 under the title Reflections on the Painting and Sculpture of the Greeks), and Geschichte der Kunst des Altertums (History of Art in Antiquity), published in 1764 (this is the first occurrence of the phrase ‘history of art’ in the title of a book)". Winckelmann critiqued the artistic excesses of Baroque and Rococo forms, and was instrumental in reforming taste in favor of the more sober Neoclassicism. Jacob Burckhardt (1818–1897), one of the founders of art history, noted that Winckelmann was 'the first to distinguish between the periods of ancient art and to link the history of style with world history'. From Winckelmann until the mid-20th century, the field of art history was dominated by German-speaking academics. Winckelmann's work thus marked the entry of art history into the high-philosophical discourse of German culture.

Winckelmann was read avidly by Johann Wolfgang Goethe and Friedrich Schiller, both of whom began to write on the history of art, and his account of the Laocoön group occasioned a response by Lessing. The emergence of art as a major subject of philosophical speculation was solidified by the appearance of Immanuel Kant's "Critique of Judgment" in 1790, and was furthered by Hegel's "Lectures on Aesthetics". Hegel's philosophy served as the direct inspiration for Karl Schnaase's work. Schnaase's "Niederländische Briefe" established the theoretical foundations for art history as an autonomous discipline, and his "Geschichte der bildenden Künste", one of the first historical surveys of the history of art from antiquity to the Renaissance, facilitated the teaching of art history in German-speaking universities. Schnaase's survey was published contemporaneously with a similar work by Franz Theodor Kugler.

Heinrich Wölfflin (1864–1945), who studied under Burckhardt in Basel, is the "father" of modern art history. Wölfflin taught at the universities of Berlin, Basel, Munich, and Zurich. A number of students went on to distinguished careers in art history, including Jakob Rosenberg and Frida Schottmuller. He introduced a scientific approach to the history of art, focusing on three concepts. Firstly, he attempted to study art using psychology, particularly by applying the work of Wilhelm Wundt. He argued, among other things, that art and architecture are good if they resemble the human body. For example, houses were good if their façades looked like faces. Secondly, he introduced the idea of studying art through comparison. By comparing individual paintings to each other, he was able to make distinctions of style. His book "Renaissance and Baroque" developed this idea, and was the first to show how these stylistic periods differed from one another. In contrast to Giorgio Vasari, Wölfflin was uninterested in the biographies of artists. In fact he proposed the creation of an "art history without names." Finally, he studied art based on ideas of nationhood. He was particularly interested in whether there was an inherently "Italian" and an inherently "German" style. This last interest was most fully articulated in his monograph on the German artist Albrecht Dürer.

Contemporaneous with Wölfflin's career, a major school of art-historical thought developed at the University of Vienna. The first generation of the Vienna School was dominated by Alois Riegl and Franz Wickhoff, both students of Moritz Thausing, and was characterized by a tendency to reassess neglected or disparaged periods in the history of art. Riegl and Wickhoff both wrote extensively on the art of late antiquity, which before them had been considered as a period of decline from the classical ideal. Riegl also contributed to the revaluation of the Baroque.

The next generation of professors at Vienna included Max Dvořák, Julius von Schlosser, Hans Tietze, Karl Maria Swoboda, and Josef Strzygowski. A number of the most important twentieth-century art historians, including Ernst Gombrich, received their degrees at Vienna at this time. The term "Second Vienna School" (or "New Vienna School") usually refers to the following generation of Viennese scholars, including Hans Sedlmayr, Otto Pächt, and Guido Kaschnitz von Weinberg. These scholars began in the 1930s to return to the work of the first generation, particularly to Riegl and his concept of "Kunstwollen", and attempted to develop it into a full-blown art-historical methodology. Sedlmayr, in particular, rejected the minute study of iconography, patronage, and other approaches grounded in historical context, preferring instead to concentrate on the aesthetic qualities of a work of art. As a result, the Second Vienna School gained a reputation for unrestrained and irresponsible formalism, and was furthermore colored by Sedlmayr's overt racism and membership in the Nazi party. This latter tendency was, however, by no means shared by all members of the school; Pächt, for example, was himself Jewish, and was forced to leave Vienna in the 1930s.

Our 21st-century understanding of the symbolic content of art comes from a group of scholars who gathered in Hamburg in the 1920s. The most prominent among them were Erwin Panofsky, Aby Warburg, and Fritz Saxl. Together they developed much of the vocabulary that continues to be used in the 21st century by art historians. "Iconography"—with roots meaning "symbols from writing" refers to subject matter of art derived from written sources—especially scripture and mythology. "Iconology" is a broader term that referred to all symbolism, whether derived from a specific text or not. Today art historians sometimes use these terms interchangeably.

Panofsky, in his early work, also developed the theories of Riegl, but became eventually more preoccupied with iconography, and in particular with the transmission of themes related to classical antiquity in the Middle Ages and Renaissance. In this respect his interests coincided with those of Warburg, the son of a wealthy family who had assembled an impressive library in Hamburg devoted to the study of the classical tradition in later art and culture. Under Saxl's auspices, this library was developed into a research institute, affiliated with the University of Hamburg, where Panofsky taught.

Warburg died in 1929, and in the 1930s Saxl and Panofsky, both Jewish, were forced to leave Hamburg. Saxl settled in London, bringing Warburg's library with him and establishing the Warburg Institute. Panofsky settled in Princeton at the Institute for Advanced Study. In this respect they were part of an extraordinary influx of German art historians into the English-speaking academy in the 1930s. These scholars were largely responsible for establishing art history as a legitimate field of study in the English-speaking world, and the influence of Panofsky's methodology, in particular, determined the course of American art history for a generation.

Heinrich Wölfflin was not the only scholar to invoke psychological theories in the study of art. Psychoanalyst Sigmund Freud wrote a book on the artist Leonardo da Vinci, in which he used Leonardo's paintings to interrogate the artist's psyche and sexual orientation. Freud inferred from his analysis that Leonardo was probably homosexual.

Though the use of posthumous material to perform psychoanalysis is controversial among art historians, especially since the sexual mores of Leonardo's time and Freud's are different, it is often attempted. One of the best-known psychoanalytic scholars is Laurie Schneider Adams, who wrote a popular textbook, "Art Across Time", and a book "Art and Psychoanalysis".

An unsuspecting turn for the history of art criticism came in 1914 when Sigmund Freud published a psychoanalytical interpretation of Michelangelo’s Moses titled Der Moses des Michelangelo as one of the first psychology based analyses on a work of art. Freud first published this work shortly after reading Vasari’s "Lives". For unknown purposes, Freud originally published the article anonymously.

Carl Jung also applied psychoanalytic theory to art. C.G. Jung was a Swiss psychiatrist, an influential thinker, and founder of analytical psychology. Jung's approach to psychology emphasized understanding the psyche through exploring the worlds of dreams, art, mythology, world religion and philosophy. Much of his life's work was spent exploring Eastern and Western philosophy, alchemy, astrology, sociology, as well as literature and the arts. His most notable contributions include his concept of the psychological archetype, the collective unconscious, and his theory of synchronicity. Jung believed that many experiences perceived as coincidence were not merely due to chance but, instead, suggested the manifestation of parallel events or circumstances reflecting this governing dynamic. He argued that a collective unconscious and archetypal imagery were detectable in art. His ideas were particularly popular among American Abstract expressionists in the 1940s and 1950s. His work inspired the surrealist concept of drawing imagery from dreams and the unconscious.

Jung emphasized the importance of balance and harmony. He cautioned that modern humans rely too heavily on science and logic and would benefit from integrating spirituality and appreciation of the unconscious realm. His work not only triggered analytical work by art historians, but it became an integral part of art-making. Jackson Pollock, for example, famously created a series of drawings to accompany his psychoanalytic sessions with his Jungian psychoanalyst, Dr. Joseph Henderson. Henderson who later published the drawings in a text devoted to Pollock's sessions realized how powerful the drawings were as a therapeutic tool.

The legacy of psychoanalysis in art history has been profound, and extends beyone Freud and Jung. The prominent feminist art historian Griselda Pollock, for example, draws upon psychoanalysis both in her reading into contemporary art and in her rereading of modernist art. With Griselda Pollock's reading of French feminist psychoanalysis and in particular the writings of Julia Kristeva and Bracha L. Ettinger, as with Rosalind Krauss readings of Jacques Lacan and Jean-François Lyotard and Catherine de Zegher's curatorial rereading of art, Feminist theory written in the fields of French feminism and Psychoanalysis has strongly informed the reframing of both men and women artists in art history.

During the mid-20th century, art historians embraced social history by using critical approaches. The goal was to show how art interacts with power structures in society. One critical approach that art historians used was Marxism. Marxist art history attempted to show how art was tied to specific classes, how images contain information about the economy, and how images can make the status quo seem natural (ideology).

Perhaps the best-known Marxist was Clement Greenberg, who came to prominence during the late 1930s with his essay "Avant-Garde and Kitsch". In the essay Greenberg claimed that the avant-garde arose in order to defend aesthetic standards from the decline of taste involved in consumer society, and seeing kitsch and art as opposites. Greenberg further claimed that avant-garde and Modernist art was a means to resist the leveling of culture produced by capitalist propaganda. Greenberg appropriated the German word 'kitsch' to describe this consumerism, although its connotations have since changed to a more affirmative notion of leftover materials of capitalist culture. Greenberg later became well known for examining the formal properties of modern art.

Meyer Schapiro is one of the best-remembered Marxist art historians of the mid-20th century. Although he wrote about numerous time periods and themes in art, he is best remembered for his commentary on sculpture from the late Middle Ages and early Renaissance, at which time he saw evidence of capitalism emerging and feudalism declining.

Arnold Hauser wrote the first Marxist survey of Western Art, entitled "The Social History of Art". He attempted to show how class consciousness was reflected in major art periods. The book was controversial when published during the 1950s since it makes generalizations about entire eras, a strategy now called "vulgar Marxism".

Marxist Art History was refined in the department of Art History at UCLA with scholars such as T.J. Clark, O.K. Werckmeister, David Kunzle, Theodor W. Adorno, and Max Horkheimer. T.J. Clark was the first art historian writing from a Marxist perspective to abandon vulgar Marxism. He wrote Marxist art histories of several impressionist and realist artists, including Gustave Courbet and Édouard Manet. These books focused closely on the political and economic climates in which the art was created.

Linda Nochlin's essay "Why have there been no great women artists?" helped to ignite feminist art history during the 1970s and remains one of the most widely read essays about female artists. In it she applies a feminist critical framework to show systematic exclusion of women from art training. Nochlin argues that exclusion from practicing art as well as the canonical history of art was the consequence of cultural conditions which curtailed and restricted women from art producing fields. The few who did succeed were treated as anomalies and did not provide a model for subsequent success.Griselda Pollock is another prominent feminist art historian, whose use of psychoanalytic theory is described above. While feminist art history can focus on any time period and location, much attention has been given to the Modern era. Some of this scholarship centers on the feminist art movement, which referred specifically to the experience of women.

As opposed to iconography which seeks to identify meaning, semiotics is concerned with how meaning is created. Roland Barthes’s connoted and denoted meanings are paramount to this examination. In any particular work of art, an interpretation depends on the identification of denoted meaning—the recognition of a visual sign, and the connoted meaning—the instant cultural associations that come with recognition. The main concern of the semiotic art historian is to come up with ways to navigate and interpret connoted meaning.

Semiotic art history seeks to uncover the codified meaning or meanings in an aesthetic object by examining its connectedness to a collective consciousness. Art historians do not commonly commit to any one particular brand of semiotics but rather construct an amalgamated version which they incorporate into their collection of analytical tools. For example, Meyer Schapiro borrowed Saussure’s differential meaning in effort to read signs as they exist within a system. According to Schapiro, to understand the meaning of frontality in a specific pictorial context, it must be differentiated from, or viewed in relation to, alternate possibilities such as a profile, or a three-quarter view. Schapiro combined this method with the work of Charles Sanders Peirce whose object, sign, and interpretant provided a structure for his approach. Alex Potts demonstrates the application of Peirce’s concepts to visual representation by examining them in relation to the Mona Lisa. By seeing the Mona Lisa, for example, as something beyond its materiality is to identify it as a sign. It is then recognized as referring to an object outside of itself, a woman, or Mona Lisa. The image does not seem to denote religious meaning and can therefore be assumed to be a portrait. This interpretation leads to a chain of possible interpretations: who was the sitter in relation to Leonardo? What significance did she have to him? Or, maybe she is an icon for all of womankind. This chain of interpretation, or “unlimited semiosis” is endless; the art historian’s job is to place boundaries on possible interpretations as much as it is to reveal new possibilities.

Semiotics operates under the theory that an image can only be understood from the viewer’s perspective. The artist is supplanted by the viewer as the purveyor of meaning, even to the extent that an interpretation is still valid regardless of whether the creator had intended it. Rosalind Krauss espoused this concept in her essay “In the Name of Picasso.” She denounced the artist’s monopoly on meaning and insisted that meaning can only be derived after the work has been removed from its historical and social context. Mieke Bal argued similarly that meaning does not even exist until the image is observed by the viewer. It is only after acknowledging this that meaning can become opened up to other possibilities such as feminism or psychoanalysis.

Aspects of the subject which have come to the fore in recent decades include interest in the patronage and consumption of art, including the economics of the art market, the role of collectors, the intentions and aspirations of those commissioning works, and the reactions of contemporary and later viewers and owners. Museum studies, including the history of museum collecting and display, is now a specialized field of study, as is the history of collecting.

Scientific advances have made possible much more accurate investigation of the materials and techniques used to create works, especially infra-red and x-ray photographic techniques which have allowed many underdrawings of paintings to be seen again. Proper analysis of pigments used in paint is now possible, which has upset many attributions. Dendrochronology for panel paintings and radio-carbon dating for old objects in organic materials have allowed scientific methods of dating objects to confirm or upset dates derived from stylistic analysis or documentary evidence. The development of good colour photography, now held digitally and available on the internet or by other means, has transformed the study of many types of art, especially those covering objects existing in large numbers which are widely dispersed among collections, such as illuminated manuscripts and Persian miniatures, and many types of archaeological artworks.

The field of Art History is traditionally divided into specializations or concentrations based on eras and regions, with further sub-division based on media. Thus, someone might specialize in "19th-century German architecture" or in "16th-century Tuscan sculpture." Sub-fields are often included under a specialization. For example, the Ancient Near East, Greece, Rome, and Egypt are all typically considered special concentrations of Ancient art. In some cases, these specializations may be closely allied (as Greece and Rome, for example), while in others such alliances are far less natural (Indian art versus Korean art, for example).

Non-Western art is a relative newcomer to the Art Historical canon. Recent revisions of the semantic division between art and artifact have recast objects created in non-Western cultures in more aesthetic terms. Relative to those studying Ancient Rome or the Italian Renaissance, scholars specializing in Africa, the Ancient Americas and Asia are a growing minority.

Contemporary Art History refers to research into the period from the 1960s until today reflecting the break from the assumptions of modernism brought by artists of the neo-avant-garde and a continuity in contemporary art in terms of practice based on conceptualist and post-conceptualist practices.

In the United States, the most important art history organization is the College Art Association. It organizes an annual conference and publishes the "Art Bulletin" and "Art Journal". Similar organizations exist in other parts of the world, as well as for specializations, such as architectural history and Renaissance art history. In the UK, for example, the Association of Art Historians is the premiere organization, and it publishes a journal titled "Art History".





</doc>
<doc id="48656171" url="https://en.wikipedia.org/wiki?curid=48656171" title="Disclosing New Worlds">
Disclosing New Worlds

Disclosing New Worlds: Entrepreneurship, Democratic Action, and the Cultivation of Solidarity (1997) is a book co-authored by Fernando Flores, Hubert Dreyfus and Charles Spinosa (a consultant philosopher specializing in commercial innovation). It is a philosophical proposal intended to restore or energize democracy by social constructionism via an argument style of world disclosure but which philosophy is distinct from:

Nevertheless, the authors build on these ideas and seek to reformulate the relationship between democratic rights and economic progress when persistent technological advance obscures an uncertain future for humanity threatened by multiple issues such as peak oil, global warming and environmental degradation. The authors concentrate on three practical activities:


The authors reason that human beings are at their best when engaged in imaginative and practical innovation rather than in abstract reflection, and thus challenging accepted wisdom and conventional practices within their particular environment, or as the authors claim, when they are making history. History-making, in this account, refers not to political power changes, wars or violent revolution, but to changes in the way people understand their personal qualities and deal with their particular situations.

World disclosure (German: "Erschlossenheit", literally "development or comprehension") is a phenomenon first described by the German philosopher Martin Heidegger in his landmark book "Being and Time". As well as the authors of this work, the idea of disclosing has also been discussed by philosophers such as John Dewey, Jürgen Habermas, Nikolas Kompridis and Charles Taylor. It refers to how things become intelligible and meaningfully relevant to ordinary people.

The authors reiterate the importance of history making and identify three types of actors:

Each has to overcome resistance to change, but do so in different ways. Solving puzzles request of each a clear strategic objective, but use different tactics to overcome obstructions. By understanding and disclosing our objectives, and discovering the sort of role we may have to play according to particular situations to reach those goals, all of us can be "history makers" and make changes to our society by changing the shared narrative that binds our particular culture. The authors quote already changed collective attitudes which have recently become much less tolerant of road accidents, various forms of discrimination and repressive public education of young people.

They promote a change to the overarching consumerism as the western world's social and economic order and ideology that encourages the acquisition of goods and services in ever-increasing amounts, in favor of sustainable development within individual communities as a precursor to disclosing a new world order that supports human equality



</doc>
<doc id="49733801" url="https://en.wikipedia.org/wiki?curid=49733801" title="Women in musicology">
Women in musicology

Women in musicology describes the role of women professors, scholars and researchers in postsecondary education musicology departments at postsecondary education institutions, including universities, colleges and music conservatories. Traditionally, the vast majority of major musicologists and music historians have been men. Nevertheless, some women musicologists have reached the top ranks of the profession. Carolyn Abbate (born 1956) is an American musicologist who did her PhD at Princeton University. She has been described by the "Harvard Gazette" as "one of the world's most accomplished and admired music historians".

Susan McClary (born 1946) is a musicologist associated with the "New Musicology" who incorporates feminist music criticism in her work. McClary holds a PhD from Harvard University. One of her best known works is "Feminine Endings" (1991), which covers musical constructions of gender and sexuality, gendered aspects of traditional music theory, gendered sexuality in musical narrative, music as a gendered discourse and issues affecting women musicians. In the book, McClary suggests that the sonata form (used in symphonies and string quartets) may be a sexist or misogynistic procedure that constructs of gender and sexual identity. McClary's "Conventional Wisdom" (2000) argues that the traditional musicological assumption of the existence of "purely musical" elements, divorced from culture and meaning, the social and the body, is a conceit used to veil the social and political imperatives of the worldview that produces the classical canon most prized by supposedly objective musicologists.

American musicologist Marcia Citron has asked "[w]hy is music composed by women so marginal to the standard 'classical' repertoire?" Citron "examines the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works." She argues that in the 1800s, women composers typically wrote art songs for performance in small recitals rather than symphonies intended for performance with an orchestra in a large hall, with the latter works being seen as the most important genre for composers; since women composers did not write many symphonies, they were deemed to be not notable as composers.

Other notable women scholars include:


Ethnomusicologists study the many musics around the world that emphasize their cultural, social, material, cognitive, biological, and other dimensions or contexts instead of or in addition to its isolated sound component or any particular repertoire.
Ethnomusicology – a term coined by Jaap Kunst from the Greek words ἔθνος ("ethnos", "nation") and μουσική ("mousike", "music") – is often described as the anthropology or ethnography of music. Initially, ethnomusicology was almost exclusively oriented toward non-Western music, but now includes the study of Western music from anthropological, sociological and intercultural perspectives.

Notable ethnomusicologists include:


</doc>
<doc id="2318626" url="https://en.wikipedia.org/wiki?curid=2318626" title="German studies">
German studies

German studies is the field of humanities that researches, documents, and disseminates German language and literature in both its historic and present forms. Academic departments of German studies often include classes on German culture, German history, and German politics in addition to the language and literature component. Common German names for the field are , , and . In English the terms Germanistics or Germanics are sometimes used (mostly by Germans), but the subject is more often referred to as "German studies", "German language and literature", or "German philology".

Modern German studies is usually seen as a combination of two sub-disciplines: German linguistics and Germanophone literature studies.

German linguistics is traditionally called philology in Germany, as there is something of a difference between philologists and linguists. It is roughly divided as follows:

In addition, the discipline examines German under various aspects: the way it is spoken and written, i.e., spelling; declination; vocabulary; sentence structure; texts; etc. It compares the various manifestations such as social groupings (slang, written texts, etc.) and geographical groupings (dialects, etc.).

The study German literature is divided into two parts: "Ältere Deutsche Literaturwissenschaft" deals with the period from the beginnings of German in the early Middle Ages up to post-Medieval times around AD 1750, while the modern era is covered by "Neuere Deutsche Literaturwissenschaft". The field systematically examines German literature in terms of genre, form, content, and motifs as well as looking at it historically by author and epoch. Important areas include edition philology, history of literature, and textual interpretation. The relationships of German literature to the literatures of other languages (e.g. reception and mutual influences) and historical contexts are also important areas of concentration. "The Penguin Dictionary of Literary Terms and Literary Theory: Fourth Edition" () is printed in English but contains many German-language literary terms that apply cross-culturally in the field of literary criticism; quite a few of the in terms in the book originated in German but have since been adopted by English-language critics and scholars.

At least in Germany and Austria, German studies in academia play a central role in the education of German school teachers. Their courses usually cover four fields:
Several universities offer specialized curricula for school teachers, usually called "'". In Germany, they are leading to a two step exam and certificate by the federal states of Germany cultural authorities, called the ' ("state exam").

In recent years, German has looked for links with the fields of communications, cultural studies and media studies. In addition, the sub-branch of film studies has established itself.

As an unsystematic field of interest for individual scholars, German studies can be traced back to Tacitus' "Germania". The publication and study of legal and historical source material, such as Medieval Bible translations, were all undertaken during the German Renaissance of the sixteenth century, truly initiating the field of German studies. As an independent university subject, German studies was introduced at the beginning of the nineteenth century by Georg Friedrich Benecke, the Brothers Grimm, and Karl Lachmann.




"German studies" is taught at many German universities. Some examples are:







</doc>
<doc id="8074243" url="https://en.wikipedia.org/wiki?curid=8074243" title="The Word and the World">
The Word and the World

The Word and the World Project of the Stanford University's Learning Lab developed a large lecture, Introduction to Humanities (IHUM) course adopting pedagogical strategies and technologies designed to enhance learning. The course was given in 1997 and 1998. The goal of the curriculum innovations was to transform a large lecture course into a learning community. Professors: Larry Friedlander (English), Haun Saussy (East Asian Studies), and Tim Lenior (History); teaching fellows: Carlos Seligo and Margo Denman and lab staff: Charles Kerns and George Toye worked together to develop a holistic curriculum mediated through a website center for the course.

This course was developed in response to the shortcomings of earlier large lecture courses. This type of course typically rated poorly in student evaluations and often led students to behaviors that inhibited learning: students skipped lectures and did not read assignments or prepare for meetings; students crammed for exams and waited until the last minute to write papers with a focus on grades and not learning. They were passive participants in a system that did not foster active engagement. Often there was a lack of continuity between lecture and section. Students had widely varying levels of knowledge about the texts. Faculty had very little information about the students’ knowledge as the course proceeded and the students had little feedback on their performance.

The curriculum was based on the reading of five texts, Genesis, Blade Runner, Hamlet, Descartes' Meditations, from the viewpoint of the scholar of history, literature, or philosophy. The course emphasized methods of reading and critical and interpretive approaches, rather than content.
This first year course met weekly in two one-hour lectures for all 100 students and two one-hour discussion section meetings of 15 students each. Students and instructors engaged in web activities including structured reading assignments and asynchronous discussion forums. The web site provided rich resources to supplement the texts. Students worked cross-section group projects and on panel discussions. There was no final exam for the course; a fair was held in which students exhibited their project web sites.

A site that had a calendar, announcements, faculty bios, the asynchronous discussion forums, assignments, and student projects
The course web site linked rich resources supporting each text to engage students from novices to those well-informed on the texts. It included explanatory information, annotations, cultural information, critical analyses, visual interpretations of the texts including paintings, video recordings of multiple performances, and different cuts of the films.
The web site also had a series of on-line assignments that structured the learners’ reading activities. These on-line activities drew upon the students’ previous knowledge and guided them as they approached each text. In the first year of the project in 1997 students responses were recorded in a personal on-line, shareable portfolio, but after noting little use of the portfolio, the design was changed in the second iteration of the course in 1998 so that assignments were displayed directly in the discussion Forum.

The Forum had special features to support large lecture courses such as photographs of the posting student next to his/her message, organization into sections, and notification systems to alert faculty and groups to special postings. The forum was used to discuss lectures; it was where assignments were posted (they were not sent to an instructor); and was used for communication in forming project groups. The Forum was enhanced in the second year to allow direct linkage to assignments, faculty-controlled subject organization, and threaded discussion.

The students in the course completed projects in the second half of the course that demonstrated their understanding of the texts. Projects were constructed in many media the first year but were limited to posters and web sites in the second. Faculty met often with students helping them to plan their projects. All projects from the previous year were available as models when the course was repeated.

The project employed several modes of project and technology evaluation: questionnaires, interviews, video interaction analysis, peer review, and ethnographic studies. Findings:

-The Forum provided a venue for discussion by quiet students and accommodated different learning styles.
-The Forum and assignments helped students prepare for face-to-face meetings by discussing basic issues before class. 
-It added to workload of teaching staff.
-There was a positive correlation between scores on papers and number of Forum postings.



</doc>
<doc id="30695401" url="https://en.wikipedia.org/wiki?curid=30695401" title="Variantology">
Variantology

Variantology has been conceived as an international research project with the aim of developing a critical appraisal of the established concepts of “media”. The concept of a medium is thus opened up to approaches and disciplines that up to now have remained outside the contemporary discourse on media, such as theology, various musicology, aspects of natural sciences, fine arts or classical philology. Furthermore, it is opened up to cultures of knowledge that have long been excluded from the western discourse, like the oriental and Arabic-Islamic culture. Variantology also attempts to explore how reciprocally these disciplines then become open to thinking in categories and terms of media and communication. Consequently, the network of research that constitutes the Variantology project involves scholars based in academic institutions as well as artists, musicians and authors.

To come to a different understanding of media, a central part of research is the development of a network of scientists, artists and scholars who are engaged with the "deep time relations“ between arts, sciences, and technologies. The term "deep time relations“ refers to the notion of being a plurality of traversals through the genealogy of what we call media today. The underlying theoretical center is Michel Foucault’s concept of genealogy, which he developed from Friedrich Nietzsche’s thinking about morality as a historical- and social-generated construction. However, Foucault differs from his predecessor in that Nietzsche believed these constructions were rooted in psychological drives based on differing psychological types. In this sense, Foucault borrows from Nietzsche but diverges from Nietzsche's philosophical naturalism. 

Foucault's approach in this connection was to comprehend history as a constitution of knowledge, of discourses, of objectification and so on, detached from an idea of historical subjects and previously unquestioned categories of Western (Eurocentric) culture and power. This is not to say that ethnocentrism is only a Western phenomenon; there is debate as to whether eurocentrism is an ethnocentric term, given that Western models of thought have aimed to incorporate non-native elements to a much larger degree than have non-Western cultures and systems of power. What has changed is the focus: how media have developed within a Western context (e.g. technological advancements, socio-political effects) is studied alongside the histories, practices, and effects in non-Western countries. This study is complicated within media studies as distinctions between Western and non-Western cultures continue to diminish under the influence of international corporations, law, trade, and communications. But how regional cultures incorporate non-native elements while maintaining their local cultural identity remains an important area of research. 

An integral part of the project is the annual international workshop. The first three Variantology workshops were held at Academy of Media Arts Cologne, the fourth one at UdK in Berlin, and the 5th at Biblioteca Nazionale Vittorio Emanuele III in Naples, Italy.


</doc>
<doc id="7873885" url="https://en.wikipedia.org/wiki?curid=7873885" title="History by period">
History by period

This history by period summarizes significant eras in the history of the world, from the ancient world to the present day.

Ancient history refers to the time period in which scientists have found the earliest remains of human activity, approximately 60,000 BC. It ends with the fall of several significant empires, such as the Western Roman Empire in the Mediterranean, the Han Dynasty in China, and the Gupta Empire in India, collectively around 650 AD.

The Bronze Age is the time period in which humans around the world began to use bronze as a major metal in tools. It is generally accepted as starting around 3600 BC and ending with the advent of iron in 1000 BC.

The Iron Age is often called Antiquity or the Classical Era, but these periods more commonly refer to only one region. It begins around 1000 BC with the widespread use of iron in tools. It is often accepted to end at approximately 650 AD, with the fall of the aforementioned major civilizations.

Note that BC and BCE refer to the same time period. BCE is an abbreviation for Before Common Era, and BC for Before Christ. AD is Anno Domini, and CE is Common Era. This is done in order to standardize time periods across the world (ISO 8601).

The Postclassical Era, also referred to as the Medieval period or, for Europe, the Middle Ages, begins around 500 CE after the fall of major civilizations, covering the advent of Islam. The period ends around 1450–1500, with events like the rise of moveable-type printing in Europe, the voyages of Christopher Columbus, and the Ottoman Empire's conquest of Constantinople.


The Modern Period covers human history from the creation of a more global network (i.e. the colonization of the Americas by Europeans) to present day.

The Early Modern Period is the first third of the Modern Period and is often used with the parent categorization. It starts with the invention of the printing press, covering the voyage of Christopher Columbus in 1492 and, more generally, the establishment of a more global network. It ends in 1750 with the beginning of British industrialization.


The Age of Revolution is a less commonly used period, but appropriately covers the time between the early modern and contemporary. It begins around 1750 with European industrialization and is marked by several political revolutions. It ends around 1945, with the relative advancement of industrialization in Europe, the United States, Japan, and Russia, and the end of World War II. 

The Contemporary Period generally covers history still in living memory, approximately 100 years behind the current year. However, for all intents and purposes, the period will be used here as spanning from the second world war in 1945 to present day, as it is considered separate from the past eras and the newest stage of world history.





</doc>
<doc id="351227" url="https://en.wikipedia.org/wiki?curid=351227" title="Transparency (behavior)">
Transparency (behavior)

Transparency, as used in science, engineering, business, the humanities and in other social contexts, is operating in such a way that it is easy for others to see what actions are performed. Transparency implies openness, communication, and accountability.

Transparency is practiced in companies, organizations, administrations, and communities. For example, a cashier making change after a point of sale transaction by offering a record of the items purchased (e.g., a receipt) as well as counting out the customer's change on the counter demonstrates one type of transparency.

The term "transparency" has a very different meaning in information security where it is used to describe security mechanisms that are intentionally in-detectable or hidden from view. Examples include hiding utilities and tools which the user does not need to know in order to do their job, like keeping the remote re-authentication operations of Challenge-Handshake Authentication Protocol hidden from the user. 

In Norway and in Sweden, tax authorities annually release the "skatteliste", "taxeringskalendern", or "tax list"; official records showing the annual income and overall wealth of nearly every taxpayer.

Regulations in Hong Kong require banks to list their top earners – without naming them – by pay band.

In 2009, the Spanish government for the first time released information on how much each cabinet member is worth, but data on ordinary citizens is private.

Radical transparency is a management method where nearly all decision making is carried out publicly. All draft documents, all arguments for and against a proposal, all final decisions, and the decision making process itself are made public and remain publicly archived. This approach has grown in popularity with the rise of the Internet. Two examples of organizations utilizing this style are the GNU/Linux community and Indymedia.

Corporate transparency, a form of radical transparency, is the concept of removing all barriers to —and the facilitating of— free and easy public access to corporate information and the laws, rules, social connivance and processes that facilitate and protect those individuals and corporations that freely join, develop, and improve the process.

Accountability and transparency are of high relevance for non-governmental organisations (NGOs). In view of their responsibilities to stakeholders, including donors, sponsors, programme beneficiaries, staff, states and the public, they are considered to be of even greater importance to them than to commercial undertakings. Yet these same values are often found to be lacking in NGOs.

The "International NGO Accountability Charter", linked to the Global Reporting Initiative, documents the commitment of its members international NGOs to accountability and transparency, requiring them to submit an annual report, among others. Signed in 2006 by 11 NGOs active in the area of humanitarian rights, the INGO Accountability Charter has been referred to as the “first global accountability charter for the non-profit sector”. In 1997, the One World Trust created an "NGO Charter", a code of conduct comprising commitment to accountability and transparency.

Media transparency is the concept of determining how and why information is conveyed through various means.

If the media and the public knows everything that happens in all authorities and county administrations there will be a lot of questions, protests and suggestions coming from media and the public. People who are interested in a certain issue will try to influence the decisions. Transparency creates an everyday participation in the political processes by media and the public. One tool used to increase everyday participation in political processes is freedom of information legislation and requests. Modern democracy builds on such participation of the people and media.

There are, for anybody who is interested, many ways to influence the decisions at all levels in society.

The right and the means to examine the process of decision making is known as transparency.
In politics, transparency is used as a means of holding public officials accountable and fighting corruption. When a government's meetings are open to the press and the public, its budgets may be reviewed by anyone, and its laws and decisions are open to discussion, it is seen as transparent. It is not clear however if this provides less opportunity for the authorities to abuse the system for their own interests.

When military authorities classify their plans as secret, transparency is absent. This can be seen as either positive or negative; positive because it can increase national security, negative because it can lead to corruption and, in extreme cases, a military dictatorship.

While a liberal democracy can be a plutocracy, where decisions are made behind locked doors and the people have fewer possibilities to influence politics between the elections, a participative democracy is more closely connected to the will of the people. Participative democracy, built on transparency and everyday participation, has been used officially in northern Europe for decades. In the northern European country Sweden, public access to government documents became a law as early as 1766. It has officially been adopted as an ideal to strive for by the rest of EU, leading to measures like freedom of information laws and laws for lobby transparency.

To promote transparency in politics, Hans Peter Martin, Paul van Buitenen (Europa Transparant) and Ashley Mote decided to cooperate under the name Platform for Transparency (PfT) in 2005. Similar organizations that promotes transparency are Transparency International and the Sunlight Foundation.

A recent political movement to emerge in conjunction with the demands for transparency is the Pirate Party, a label for a number of political parties across different countries who advocate freedom of information, direct democracy, network neutrality, and the free sharing of knowledge.

21st century culture affords a higher level of public transparency than ever before, and actually requires it in many cases. Modern technology and associated culture shifts have changed how government works (see WikiLeaks), what information people can find out about each other, and the ability of politicians to stay in office if they are involved in sex scandals. Due to the digital revolution, people no longer have a high level of control over what is public information, leading to a tension between the values of transparency and privacy.

Scholarly research in any academic discipline may also be labeled as (partly) transparent (or open research) if some or all relevant aspects of the research are open in the sense of open source, open access and open data, thereby facilitating social recognition and accountability of the scholars who did the research and replication by others interested in the matters addressed by it.

Some mathematicians and scientists are critical of using closed source mathematical software such as Mathematica for mathematical proofs, because these do not provide transparency, and thus are not verifiable. Open-source software such as SageMath aims to solve this problem.

In the computer software world, open source software concerns the creation of software, to which access to the underlying source code is freely available. This permits use, study, and modification without restriction.

In computer security, the debate is ongoing as to the relative merits of the full disclosure of security vulnerabilities, versus a security-by-obscurity approach.

There is a different (perhaps almost opposite) sense of transparency in human-computer interaction, whereby a system after change adheres to its previous external interface as much as possible while changing its internal behaviour. That is, a change in a system is transparent to its users if the change is unnoticeable to them.

Sports has become a global business over the last century, and here, too, initiatives ranging from mandatory drug testing to the fighting of sports-related corruption are gaining ground based on the transparent activities in other domains.

Sigmund Freud following Friedrich Nietzsche ("On Truth and Lie in a Nonmoral Sense") regularly argues that transparency is impossible because of the occluding function of the unconscious.

Among philosophical and literary works that have examined the idea of transparency are Michel Foucault's "Discipline and Punish" or David Brin's "The Transparent Society". 
The German philosopher and media theorist Byung-Chul Han in his 2012 work "Transparenzgesellschaft" sees transparency as a cultural norm created by neoliberal market forces, which he understands as the insatiable drive toward voluntary disclosure bordering on the pornographic. According to Han, the dictates of transparency enforce a totalitarian system of openness at the expense of other social values such as shame, secrecy, and trust. He was criticized for his concepts, as they would suggest corrupt politics and for referring to the anti-democratic Carl Schmitt.

Anthropologists have long explored ethnographically the relation between revealed and concealed knowledges, and have increasingly taken up the topic in relation to accountability, transparency and conspiracy theories and practices today. Todd Sanders and Harry West for example suggest not only that realms of the revealed and concealed require each other, but also that transparency in practice produces the very opacities it claims to obviate.

Clare Birchall, Christina Gaarsten, Mikkel Flyverbom, and Mark Fenster among others, write in the vein of 'Critical Transparency Studies' which attempts to challenge particular orthodoxies concerning transparency. 
Birchall, assessed in an article "[...] whether the ascendance of transparency as an ideal limits political thinking, particularly for western socialists and radicals struggling to seize opportunities for change [...]". She argues that the promotion of 'datapreneurial' activity through open data initiatives outsources and interrupts the political contract between governed and government. She is concerned that the dominant model of governmental data-driven transparency produces neoliberal subjectivities that reduce the possibility of politics as an arena of dissent between real alternatives. She suggests that the radical Left might want to work with and reinvent secrecy as an alternative to neoliberal transparency.

Researchers at University of Oxford and Warwick Business School found that transparency can also have significant unintended consequences in the field of medical care. McGivern and Fischer found 'media spectacles' and transparent regulation combined to create 'spectacular transparency' which some perverse effects on doctors' practice and increased defensive behaviour in doctors and their staff. Similarly, in a four-year organizational study, Fischer and Ferlie found that transparency in the context of a clinical risk management can act perversely to undermine ethical behavior, leading to organizational crisis and even collapse.





</doc>
