<doc id="1973470" url="https://en.wikipedia.org/wiki?curid=1973470" title="Fuzzy concept">
Fuzzy concept

A fuzzy concept is a concept of which the boundaries of application can vary considerably according to context or conditions, instead of being fixed once and for all. This means the concept is vague in some way, lacking a fixed, precise meaning, without however being unclear or meaningless altogether. It has a definite meaning, which can be made more precise only through further elaboration and specification - including a closer definition of the context in which the concept is used. The study of the characteristics of fuzzy concepts and fuzzy language is called "fuzzy semantics". The inverse of a "fuzzy concept" is a "crisp concept" (i.e. a precise concept).

A fuzzy concept is understood by scientists as a concept which is "to an extent applicable" in a situation. That means the concept has "gradations" of significance or "unsharp" (variable) boundaries of application. A fuzzy statement is a statement which is true "to some extent", and that extent can often be represented by a scaled value. The best known example of a fuzzy concept around the world is an amber traffic light, and indeed fuzzy concepts are widely used in traffic control systems. The term is also used these days in a more general, popular sense - in contrast to its technical meaning - to refer to a concept which is "rather vague" for any kind of reason.

In the past, the very idea of reasoning with fuzzy concepts faced considerable resistance from academic elites. They did not want to endorse the use of imprecise concepts in research or argumentation. Yet although people might not be aware of it, the use of fuzzy concepts has risen gigantically in all walks of life from the 1970s onward. That is mainly due to advances in electronic engineering, fuzzy mathematics and digital computer programming. The new technology allows very complex inferences about "variations on a theme" to be anticipated and fixed in a program.

The new neuro-fuzzy computational methods make it possible, to identify, to measure and respond to fine gradations of significance, with great precision. It means that practically useful concepts can be coded and applied to all kinds of tasks, even if, ordinarily, these concepts are never precisely defined. Nowadays engineers, statisticians and programmers often represent fuzzy concepts mathematically, using fuzzy logic, fuzzy values, fuzzy variables and fuzzy sets.

Problems of vagueness and fuzziness have probably always existed in human experience. The boundary between different things can appear blurry. Sometimes people have to think, when they are not in the best frame of mind to do it, or, they have to talk about something out there, which just isn't sharply defined. Across time, however, philosophers and scientists began to reflect about those kinds of problems, in much more systematic ways.

The ancient Sorites paradox first raised the logical problem of how we could exactly define the threshold at which a change in quantitative gradation turns into a qualitative or categorical difference. With some physical processes this threshold is relatively easy to identify. For example, water turns into steam at 100 °C or 212 °F (the boiling point depends partly on atmospheric pressure, which decreases at higher altitudes).

With many other processes and gradations, however, the point of change is much more difficult to locate, and remains somewhat vague. Thus, the boundaries between qualitatively different things may be "unsharp": we know that there are boundaries, but we cannot define them exactly.

According to the modern idea of the continuum fallacy, the fact that a statement is to an extent vague, does not automatically mean that it is invalid. The problem then becomes one of how we could ascertain the kind of validity that the statement does have.

The Nordic myth of Loki's wager suggested that concepts that lack precise meanings or precise boundaries of application cannot be usefully discussed at all. However, the 20th-century idea of "fuzzy concepts" proposes that "somewhat vague terms" can be operated with, since we can explicate and define the variability of their application, by assigning numbers to gradations of applicability. This idea sounds simple enough, but it had large implications.

The intellectual origins of the species of fuzzy concepts as a logical category have been traced back to a diversity of famous and less well-known thinkers, including (among many others) Eubulides, Plato, Cicero, Georg Wilhelm Friedrich Hegel, Karl Marx and Friedrich Engels, Friedrich Nietzsche, Hugh MacColl, Charles S. Peirce, Max Black, Jan Łukasiewicz, Emil Leon Post, Alfred Tarski, Georg Cantor, Nicolai A. Vasiliev, Kurt Gödel, Stanisław Jaśkowski and Donald Knuth.

Across at least two and a half millennia, all of them had something to say about graded concepts with unsharp boundaries. This suggests at least that the awareness of the existence of concepts with "fuzzy" characteristics, in one form or another, has a very long history in human thought. Quite a few logicians and philosophers have also tried to "analyze" the characteristics of fuzzy concepts as a recognized species, sometimes with the aid of some kind of many-valued logic or substructural logic.

An early attempt in the post-WW2 era to create a theory of sets where set membership is a matter of degree was made by Abraham Kaplan and Hermann Schott in 1951. They intended to apply the idea to empirical research. Kaplan and Schott measured the degree of membership of empirical classes using real numbers between 0 and 1, and they defined corresponding notions of intersection, union, complementation and subset. However, at the time, their idea "fell on stony ground". J. Barkley Rosser Sr. published a treatise on many-valued logics in 1952, anticipating "many-valued sets". Another treatise was published in 1963 by Aleksandr A. Zinov'ev and others

In 1964, the American philosopher William Alston introduced the term "degree vagueness" to describe vagueness in an idea that results from the absence of a definite cut-off point along an implied scale (in contrast to "combinatory vagueness" caused by a term that has a number of logically independent conditions of application).

The German mathematician Dieter Klaua published a German-language paper on fuzzy sets in 1965, but he used a different terminology (he referred to "many-valued sets", not "fuzzy sets").

Two popular introductions to many-valued logic in the late 1960s were by Robert J. Ackermann and Nicholas Rescher respectively. Rescher's book includes a bibliography on fuzzy theory up to 1965, which was extended by Robert Wolf for 1966-1974. Haack provides references to significant works after 1974. Bergmann provides a more recent (2008) introduction to fuzzy reasoning.

Usually the Iranian-born American computer scientist Lotfi A. Zadeh (1921-2017) is credited with inventing the specific idea of a "fuzzy concept" in his seminal 1965 paper on fuzzy sets, because he gave a formal mathematical presentation of the phenomenon that was widely accepted by scholars.<ref>Lotfi A. Zadeh, "Fuzzy sets". In: "Information and Control", Vol. 8, June 1965, pp. 338–353.



</doc>
<doc id="37673" url="https://en.wikipedia.org/wiki?curid=37673" title="Symbol">
Symbol

A symbol is a mark, sign or word that indicates, signifies, or is understood as representing an idea, object, or relationship. Symbols allow people to go beyond what is known or seen by creating linkages between otherwise very different concepts and experiences. All communication (and data processing) is achieved through the use of symbols. Symbols take the form of words, sounds, gestures, ideas or visual images and are used to convey other ideas and beliefs. For example, a red octagon may be a symbol for "STOP". On a map, a blue line might represent a river. Numerals are symbols for numbers. Alphabetic letters may be symbols for sounds. Personal names are symbols representing individuals. A red rose may symbolize love and compassion. The variable 'x', in a mathematical equation, may symbolize the position of a particle in space.

The academic study of symbols is semiotics. In cartography, an organized collection of symbols forms a legend for a map.

The word "symbol" derives from the Greek σύμβολον "symbolon", meaning "token, watchword" from σύν "syn" "together" and βάλλω "bállō" " "I throw, put." The sense evolution in Greek is from "throwing things together" to "contrasting" to "comparing" to "token used in comparisons to determine if something is genuine." Hence, "outward sign" of something. The meaning "something which stands for something else" was first recorded in 1590, in Edmund Spenser's "Faerie Queene".

Symbols are a means of complex communication that often can have multiple levels of meaning. Symbols are the basis of all human understanding and serve as vehicles of conception for all human knowledge. Symbols facilitate understanding of the world in which we live, thus serving as the grounds upon which we make judgments. In this way, people use symbols not only to make sense of the world around them, but also to identify and cooperate in society through constitutive rhetoric.

Human cultures use symbols to express specific ideologies and social structures and to represent aspects of their specific culture. Thus, symbols carry meanings that depend upon one's cultural background; in other words, the meaning of a symbol is not inherent in the symbol itself but is culturally learned.

Heinrich Zimmer gives a concise overview of the nature, and perennial relevance, of symbols.
In the book "Signs and Symbols", it is stated that

Semiotics is the study of signs, symbols, and signification as communicative behavior. Semiotics studies focus on the relationship of the signifier and the signified, also taking into account interpretation of visual cues, body language, sound, and other contextual clues. Semiotics is linked with both linguistics and psychology. Semioticians thus not only study what a symbol implies, but also how it got its meaning and how it functions to make meaning in society. Symbols allow the human brain continuously to create meaning using sensory input and decode symbols through both denotation and connotation.

An alternative definition of "symbol", distinguishing it from the term "sign" was proposed by Swiss psychoanalyst Carl Jung. In his studies on what is now called Jungian archetypes, a sign stands for something known, as a word stands for its referent. He contrasted a sign with a "symbol": something that is unknown and that cannot be made clear or precise. An example of a symbol in this sense is Christ as a symbol of the archetype called "self".

Kenneth Burke described "Homo sapiens" as a "symbol-using, symbol making, and symbol misusing animal" to suggest that a person creates symbols as well as misuses them. One example he uses to indicate what he means by the misuse of symbol is the story of a man who, when told that a particular food item was whale blubber, could barely keep from throwing it up. Later, his friend discovered it was actually just a dumpling. But the man's reaction was a direct consequence of the symbol of "blubber" representing something inedible in his mind. In addition, the symbol of "blubber" was created by the man through various kinds of learning.

Burke goes on to describe symbols as also being derived from Sigmund Freud's work on condensation and displacement, further stating that symbols are not just relevant to the theory of dreams but also to "normal symbol systems". He says they are related through "substitution", where one word, phrase, or symbol is substituted for another in order to change the meaning. In other words, if one person does not understand a certain word or phrase, another person may substitute a synonym or symbol in order to get the meaning across. However, upon learning the new way of interpreting a specific symbol, the person may change his or her already-formed ideas to incorporate the new information.

Jean Dalby Clift says that people not only add their own interpretations to symbols, they also create personal symbols that represent their own understanding of their lives: what she calls "core images" of the person. She argues that symbolic work with these personal symbols or core images can be as useful as working with dream symbols in psychoanalysis or counseling.

William Indick suggests that the symbols that are commonly found in myth, legend, and fantasy fulfil psychological functions and hence are why archetypes such as "the hero," "the princess" and "the witch" have remained popular for centuries.

Symbols can carry symbolic value in three primary forms: Ideological, comparative, and isomorphic. Ideological symbols such as religious and state symbols convey complex sets of beliefs and ideas that indicate "the right thing to do". Comparative symbols such as prestigious office addresses, fine art, and prominent awards indicate answers to questions of "better or worse" and "superior or inferior". Isomorphic symbols blend in with the surrounding cultural environment such that they enable individuals and organizations to conform to their surroundings and evade social and political scrutiny. Examples of symbols with isomorphic value include wearing professional dress during business meetings, shaking hands to greet others in the West, or bowing to greet others in the East. A single symbol can carry multiple distinct meanings such that it provides multiple types of symbolic value.

Paul Tillich argued that, while signs are invented and forgotten, symbols are born and die. There are, therefore, dead and living symbols. A living symbol can reveal to an individual hidden levels of meaning and transcendent or religious realities. For Tillich a symbol always "points beyond itself" to something that is unquantifiable and mysterious; symbols open up the "depth dimension of reality itself". Symbols are complex, and their meanings can evolve as the individual or culture evolves. When a symbol loses its meaning and power for an individual or culture, it becomes a dead symbol.
When a symbol becomes identified with the deeper reality to which it refers, it becomes idolatrous as the "symbol is taken for reality." The symbol itself is substituted for the deeper meaning it intends to convey. The unique nature of a symbol is that it gives access to deeper layers of reality which are otherwise inaccessible.

A symbol's meaning may be modified by various factors including popular usage, history, and contextual intent.

The history of a symbol is one of many factors in determining a particular symbol's apparent meaning. Consequently, symbols with emotive power carry problems analogous to false etymologies.

The context of a symbol may change its meaning. Similar five-pointed stars might signify a law enforcement officer or a member of the armed services, depending upon the uniform.

Symbols are used in cartography to communicate geographical information (generally as point, line, or area features). As with other symbols, visual variables such as size, shape, orientation, texture, and pattern provide meaning to the symbol. According to semiotics, map symbols are "read" by map users when they make a connection between the graphic mark on the map (the "sign"), a general concept (the "interpretant"), and a particular feature of the real world (the "referent"). Map symbols can thus be categorized by how they suggest this connection:


A symbolic action is an action that has no, or little, practical effect but symbolizes, or signals, what the actor wants or believes. The action conveys meaning to the viewers. Symbolic action may overlap with symbolic speech, such as the use of flag burning to express hostility or saluting the flag to express patriotism. In response to intense public criticism, businesses, organizations, and governments may take symbolic actions rather than, or in addition to, directly addressing the identified problems.



</doc>
<doc id="387403" url="https://en.wikipedia.org/wiki?curid=387403" title="Nonsense">
Nonsense

Nonsense is a communication, via speech, writing, or any other symbolic system, that lacks any coherent meaning. Sometimes in ordinary usage, nonsense is synonymous with absurdity or the ridiculous. Many poets, novelists and songwriters have used nonsense in their works, often creating entire works using it for reasons ranging from pure comic amusement or satire, to illustrating a point about language or reasoning. In the philosophy of language and philosophy of science, nonsense is distinguished from sense or meaningfulness, and attempts have been made to come up with a coherent and consistent method of distinguishing sense from nonsense. It is also an important field of study in cryptography regarding separating a signal from noise.

The phrase "Colorless green ideas sleep furiously" was coined by Noam Chomsky as an example of nonsense. However, this can easily be confused with poetic symbolism. The individual "words" make sense and are arranged according to proper grammatical rules, yet the result is nonsense. The inspiration for this attempt at creating verbal nonsense came from the idea of contradiction and seemingly irrelevant and/or incompatible characteristics, which conspire to make the phrase meaningless, but are open to interpretation. The phrase "the square root of Tuesday" operates on the latter principle. This principle is behind the inscrutability of the "kōan" "What is the sound of one hand clapping?", where one hand would presumably be insufficient for clapping without the intervention of another.

James Joyce's final novel "Finnegans Wake" also uses nonsense: full of portmanteau and strong words, it "appears" to be pregnant with multiple layers of meaning, but in many passages it is difficult to say whether any one human's interpretation of a text could be the intended or unintended one.

"Jabberwocky", a poem (of nonsense verse) found in "Through the Looking-Glass, and What Alice Found There" by Lewis Carroll (1871), is a nonsense poem written in the English language. The word "jabberwocky" is also occasionally used as a synonym of nonsense.

Nonsense verse is the verse form of literary nonsense, a genre that can manifest in many other ways. Its best-known exponent is Edward Lear, author of "The Owl and the Pussycat" and hundreds of limericks.

Nonsense verse is part of a long line of tradition predating Lear: the nursery rhyme "Hey Diddle Diddle" could also be termed a nonsense verse. There are also some works which "appear" to be nonsense verse, but actually are not, such as the popular 1940s song Mairzy Doats.

Lewis Carroll, seeking a nonsense riddle, once posed the question "How is a raven like a writing desk?". Someone answered him, "Because Poe wrote on both". However, there are other possible answers (e.g. "both have inky quills").

Lines of nonsense frequently figure in the refrains of folksongs, where nonsense riddles and knock-knock jokes are often encountered.

The first verse of "Jabberwocky" by Lewis Carroll;
The first four lines of "On the Ning Nang Nong" by Spike Milligan;
The first verse of "Spirk Troll-Derisive" by James Whitcomb Riley;
The first four lines of "The Mayor of Scuttleton" by Mary Mapes Dodge;
"Oh Freddled Gruntbuggly" by Prostetnic Vogon Jeltz; a creation of Douglas Adams

In the philosophy of language and the philosophy of science, nonsense refers to a lack of sense or meaning. Different technical definitions of meaning delineate sense from nonsense.

In Ludwig Wittgenstein's writings, the word "nonsense" carries a special technical meaning which differs significantly from the normal use of the word. In this sense, "nonsense" does not refer to meaningless gibberish, but rather to the lack of sense in the context of sense and reference. In this context, logical tautologies, and purely mathematical propositions may be regarded as "nonsense". For example, "1+1=2" is a nonsensical proposition. Wittgenstein wrote in Tractatus Logico Philosophicus that some of the propositions contained in his own book should be regarded as nonsense. Used in this way, "nonsense" does not necessarily carry negative connotations.

Starting from Wittgenstein, but through an original perspective, the Italian philosopher Leonardo Vittorio Arena, in his book "Nonsense as the meaning", highlights this positive meaning of nonsense to undermine every philosophical conception which does not take note of the absolute lack of meaning of the world and life. Nonsense implies the destruction of all views or opinions, on the wake of the Indian Buddhist philosopher Nagarjuna. In the name of nonsense, it is finally refused the conception of duality and the Aristotelian formal logic.

The problem of distinguishing sense from nonsense is important in cryptography and other intelligence fields. For example, they need to distinguish signal from noise. Cryptanalysts have devised algorithms to determine whether a given text is in fact nonsense or not. These algorithms typically analyze the presence of repetitions and redundancy in a text; in meaningful texts, certain frequently used words recur, for example, "the", "is" and "and" in a text in the English language. A random scattering of letters, punctuation marks and spaces do not exhibit these regularities. Zipf's law attempts to state this analysis mathematically. By contrast, cryptographers typically seek to make their cipher texts resemble random distributions, to avoid telltale repetitions and patterns which may give an opening for cryptanalysis.

It is harder for cryptographers to deal with the presence or absence of meaning in a text in which the level of redundancy and repetition is "higher" than found in natural languages (for example, in the mysterious text of the Voynich manuscript).

Scientists have attempted to teach machines to produce nonsense. The Markov chain technique is one method which has been used to generate texts by algorithm and randomizing techniques that seem meaningful. Another method is sometimes called the "Mad Libs" method: it involves creating templates for various sentence structures and filling in the blanks with noun phrases or verb phrases; these phrase-generation procedures can be looped to add recursion, giving the output the appearance of greater complexity and sophistication. Racter was a computer program which generated nonsense texts by this method; however, Racter's book, "The Policeman’s Beard is Half Constructed", proved to have been the product of heavy human editing of the program's output.





</doc>
<doc id="55063170" url="https://en.wikipedia.org/wiki?curid=55063170" title="Afro-pessimism">
Afro-pessimism

Afro-pessimism is a field of thought which takes seriously the historical reality that blackness is politically and ontologically coterminous with slaveness. According to the 2018 Oxford Bibliography entry on Afro-pessimism written by Patrice Douglass, Selamawit D. Terrefe, and Frank B. Wilderson III, afro-pessimism can be understood as “a lens of interpretation that accounts for civil society’s dependence on anti-black violence—a regime of violence that positions black people as internal enemies of civil society.” This violence, they argue, “cannot be analogized with the regimes of violence that disciplines the Marxist subaltern, the postcolonial subaltern, the colored but nonblack Western immigrant, the nonblack queer, or the nonblack woman.” According to Wilderson, the scholar who coined the term as it functions most popularly today, afro-pessimism theorizes blackness as a position of, using the language of scholar Saidiya Hartman, "accumulation and fungibility"; that is, as a condition of —or relation to—ontological death, as opposed to a cultural identity or human subjectivity.
As opposed to humanist anthropologists, historians, sociologists, and political scientists who engage the history of Black subjectivity as one of entrenched political discrimination and social ostracization, afro-pessimists across disciplines have argued that Black people are constitutively excluded from the category of the self-possessing, rights-bearing human being of modernity. As Wilderson writes, “Blacks do not function as political subjects; instead, our flesh and energies are instrumentalized for postcolonial, immigrant, feminist, LGBT, and workers’ agendas.”




</doc>
<doc id="6880483" url="https://en.wikipedia.org/wiki?curid=6880483" title="Philosophy of mind">
Philosophy of mind

Philosophy of mind is a branch of philosophy that studies the ontology and nature of the mind and its relationship with the body. The mind–body problem is a paradigm issue in philosophy of mind, although other issues are addressed, such as the hard problem of consciousness, and the nature of particular mental states. Aspects of the mind that are studied include mental events, mental functions, mental properties, consciousness, the ontology of the mind, the nature of thought, and the relationship of the mind to the body.

Dualism and monism are the two central schools of thought on the mind–body problem, although nuanced views have arisen that do not fit one or the other category neatly.


Most modern philosophers of mind adopt either a reductive physicalist or non-reductive physicalist position, maintaining in their different ways that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, especially in the fields of sociobiology, computer science (specifically, artificial intelligence), evolutionary psychology and the various neurosciences. Reductive physicalists assert that all mental states and properties will eventually be explained by scientific accounts of physiological processes and states. Non-reductive physicalists argue that although the mind is not a separate substance, mental properties supervene on physical properties, or that the predicates and vocabulary used in mental descriptions and explanations are indispensable, and cannot be reduced to the language and lower-level explanations of physical science. Continued neuroscientific progress has helped to clarify some of these issues; however, they are far from being resolved. Modern philosophers of mind continue to ask how the subjective qualities and the intentionality of mental states and properties can be explained in naturalistic terms.

The mind–body problem concerns the explanation of the relationship that exists between minds, or mental processes, and bodily states or processes. The main aim of philosophers working in this area is to determine the nature of the mind and mental states/processes, and how—or even if—minds are affected by and can affect the body.

Our perceptual experiences depend on stimuli that arrive at our various sensory organs from the external world, and these stimuli cause changes in our mental states, ultimately causing us to feel a sensation, which may be pleasant or unpleasant. Someone's desire for a slice of pizza, for example, will tend to cause that person to move his or her body in a specific manner and in a specific direction to obtain what he or she wants. The question, then, is how it can be possible for conscious experiences to arise out of a lump of gray matter endowed with nothing but electrochemical properties.

A related problem is how someone's propositional attitudes (e.g. beliefs and desires) cause that individual's neurons to fire and muscles to contract. These comprise some of the puzzles that have confronted epistemologists and philosophers of mind from at least the time of René Descartes.

Dualism is a set of views about the relationship between mind and matter (or body). It begins with the claim that mental phenomena are, in some respects, non-physical. One of the earliest known formulations of mind–body dualism was expressed in the eastern Sankhya and Yoga schools of Hindu philosophy (c. 650 BCE), which divided the world into purusha (mind/spirit) and prakriti (material substance). Specifically, the Yoga Sutra of Patanjali presents an analytical approach to the nature of the mind.

In Western Philosophy, the earliest discussions of dualist ideas are in the writings of Plato who maintained that humans' "intelligence" (a faculty of the mind or soul) could not be identified with, or explained in terms of, their physical body. However, the best-known version of dualism is due to René Descartes (1641), and holds that the mind is a non-extended, non-physical substance, a "res cogitans". Descartes was the first to clearly identify the mind with consciousness and self-awareness, and to distinguish this from the brain, which was the seat of intelligence. He was therefore the first to formulate the mind–body problem in the form in which it still exists today.

The most frequently used argument in favor of dualism appeals to the common-sense intuition that conscious experience is distinct from inanimate matter. If asked what the mind is, the average person would usually respond by identifying it with their self, their personality, their soul, or another related entity. They would almost certainly deny that the mind simply is the brain, or vice versa, finding the idea that there is just one ontological entity at play to be too mechanistic, or simply unintelligible. Many modern philosophers of mind think that these intuitions are misleading and that we should use our critical faculties, along with empirical evidence from the sciences, to examine these assumptions to determine whether there is any real basis to them.

Another important argument in favor of dualism is that the mental and the physical seem to have quite different, and perhaps irreconcilable, properties. Mental events have a subjective quality, whereas physical events do not. So, for example, one can reasonably ask what a burnt finger feels like, or what a blue sky looks like, or what nice music sounds like to a person. But it is meaningless, or at least odd, to ask what a surge in the uptake of glutamate in the dorsolateral portion of the prefrontal cortex feels like.

Philosophers of mind call the subjective aspects of mental events "qualia" or "raw feels". There is something that it is like to feel pain, to see a familiar shade of blue, and so on. There are qualia involved in these mental events that seem particularly difficult to reduce to anything physical. David Chalmers explains this argument by stating that we could conceivably know all the objective information about something, such as the brain states and wavelengths of light involved with seeing the color red, but still not know something fundamental about the situation – what it is like to see the color red.

If consciousness (the mind) can exist independently of physical reality (the brain), one must explain how physical memories are created concerning consciousness. Dualism must therefore explain how consciousness affects physical reality. One possible explanation is that of a miracle, proposed by Arnold Geulincx and Nicolas Malebranche, where all mind–body interactions require the direct intervention of God.

Another possible argument that has been proposed by C. S. Lewis is the Argument from Reason: if, as monism implies, all of our thoughts are the effects of physical causes, then we have no reason for assuming that they are also the consequent of a reasonable ground. Knowledge, however, is apprehended by reasoning from ground to consequent. Therefore, if monism is correct, there would be no way of knowing this—or anything else—we could not even suppose it, except by a fluke.

The zombie argument is based on a thought experiment proposed by Todd Moody, and developed by David Chalmers in his book "The Conscious Mind". The basic idea is that one can imagine one's body, and therefore conceive the existence of one's body, without any conscious states being associated with this body. Chalmers' argument is that it seems possible that such a being could exist because all that is needed is that all and only the things that the physical sciences describe about a zombie must be true of it. Since none of the concepts involved in these sciences make reference to consciousness or other mental phenomena, and any physical entity can be by definition described scientifically via physics, the move from conceivability to possibility is not such a large one. Others such as Dennett have argued that the notion of a philosophical zombie is an incoherent, or unlikely, concept. It has been argued under physicalism that one must either believe that anyone including oneself might be a zombie, or that no one can be a zombie—following from the assertion that one's own conviction about being (or not being) a zombie is a product of the physical world and is therefore no different from anyone else's. This argument has been expressed by Dennett who argues that "Zombies think they are conscious, think they have qualia, think they suffer pains—they are just 'wrong' (according to this lamentable tradition) in ways that neither they nor we could ever discover!"
See also the problem of other minds.

Interactionist dualism, or simply interactionism, is the particular form of dualism first espoused by Descartes in the "Meditations". In the 20th century, its major defenders have been Karl Popper and John Carew Eccles. It is the view that mental states, such as beliefs and desires, causally interact with physical states.

Descartes' famous argument for this position can be summarized as follows: Seth has a clear and distinct idea of his mind as a thinking thing that has no spatial extension (i.e., it cannot be measured in terms of length, weight, height, and so on). He also has a clear and distinct idea of his body as something that is spatially extended, subject to quantification and not able to think. It follows that mind and body are not identical because they have radically different properties.

At the same time, however, it is clear that Seth's mental states (desires, beliefs, etc.) have causal effects on his body and vice versa: A child touches a hot stove (physical event) which causes pain (mental event) and makes her yell (physical event), this in turn provokes a sense of fear and protectiveness in the caregiver (mental event), and so on.

Descartes' argument crucially depends on the premise that what Seth believes to be "clear and distinct" ideas in his mind are necessarily true. Many contemporary philosophers doubt this. For example, Joseph Agassi suggests that several scientific discoveries made since the early 20th century have undermined the idea of privileged access to one's own ideas. Freud claimed that a psychologically-trained observer can understand a person's unconscious motivations better than the person himself does. Duhem has shown that a philosopher of science can know a person's methods of discovery better than that person herself does, while Malinowski has shown that an anthropologist can know a person's customs and habits better than the person whose customs and habits they are. He also asserts that modern psychological experiments that cause people to see things that are not there provide grounds for rejecting Descartes' argument, because scientists can describe a person's perceptions better than the person herself can.

Psychophysical parallelism, or simply parallelism, is the view that mind and body, while having distinct ontological statuses, do not causally influence one another. Instead, they run along parallel paths (mind events causally interact with mind events and brain events causally interact with brain events) and only seem to influence each other. This view was most prominently defended by Gottfried Leibniz. Although Leibniz was an ontological monist who believed that only one type of substance, the monad, exists in the universe, and that everything is reducible to it, he nonetheless maintained that there was an important distinction between "the mental" and "the physical" in terms of causation. He held that God had arranged things in advance so that minds and bodies would be in harmony with each other. This is known as the doctrine of pre-established harmony.

Occasionalism is the view espoused by Nicholas Malebranche as well as Islamic philosophers such as Abu Hamid Muhammad ibn Muhammad al-Ghazali that asserts that all supposedly causal relations between physical events, or between physical and mental events, are not really causal at all. While body and mind are different substances, causes (whether mental or physical) are related to their effects by an act of God's intervention on each specific occasion.

Property dualism is the view that the world is constituted of just one kind of substance – the physical kind – and there exist two distinct kinds of properties: physical properties and mental properties. In other words, it is the view that non-physical, mental properties (such as beliefs, desires and emotions) inhere in some physical bodies (at least, brains). How mental and physical properties relate causally depends on the variety of property dualism in question, and is not always a clear issue. Sub-varieties of property dualism include:


Dual aspect theory or dual-aspect monism is the view that the mental and the physical are two aspects of, or perspectives on, the same substance. (Thus it is a mixed position, which is monistic in some respects). In modern philosophical writings, the theory's relationship to neutral monism has become somewhat ill-defined, but one proffered distinction says that whereas neutral monism allows the context of a given group of neutral elements and the relationships into which they enter to determine whether the group can be thought of as mental, physical, both, or neither, dual-aspect theory suggests that the mental and the physical are manifestations (or aspects) of some underlying substance, entity or process that is itself neither mental nor physical as normally understood. Various formulations of dual-aspect monism also require the mental and the physical to be complementary, mutually irreducible and perhaps inseparable (though distinct).

This is a philosophy of mind that regards the degrees of freedom between mental and physical well-being as not necessarily synonymous thus implying an experiential dualism between body and mind. An example of these disparate degrees of freedom is given by Allan Wallace who notes that it is "experientially apparent that one may be physically uncomfortable—for instance, while engaging in a strenuous physical workout—while mentally cheerful; conversely, one may be mentally distraught while experiencing physical comfort". Experiential dualism notes that our subjective experience of merely seeing something in the physical world seems qualitatively different than mental processes like grief that comes from losing a loved one. This philosophy also is a proponent of causal dualism which is defined as the dual ability for mental states and physical states to affect one another. Mental states can cause changes in physical states and vice versa.

However, unlike cartesian dualism or some other systems, experiential dualism does not posit two fundamental substances in reality: mind and matter. Rather, experiential dualism is to be understood as a conceptual framework that gives credence to the qualitative difference between the experience of mental and physical states. Experiential dualism is accepted as the conceptual framework of Madhyamaka Buddhism.

Madhayamaka Buddhism goes even further, finding fault with the monist view of physicalist philosophies of mind as well in that these generally posit matter and energy as the fundamental substance of reality. Nonetheless, this does not imply that the cartesian dualist view is correct, rather Madhyamaka regards as error any affirming view of a fundamental substance to reality.In denying the independent self-existence of all the phenomena that make up the world of our experience, the Madhyamaka view departs from both the substance dualism of Descartes and the substance monism—namely, physicalism—that is characteristic of modern science. The physicalism propounded by many contemporary scientists seems to assert that the real world is composed of physical things-in-themselves, while all mental phenomena are regarded as mere appearances, devoid of any reality in and of themselves. Much is made of this difference between appearances and reality.
Indeed, physicalism, or the idea that matter is the only fundamental substance of reality, is explicitly rejected by Buddhism.In the Madhyamaka view, mental events are no more or less real than physical events. In terms of our common-sense experience, differences of kind do exist between physical and mental phenomena. While the former commonly have mass, location, velocity, shape, size, and numerous other physical attributes, these are not generally characteristic of mental phenomena. For example, we do not commonly conceive of the feeling of affection for another person as having mass or location. These physical attributes are no more appropriate to other mental events such as sadness, a recalled image from one's childhood, the visual perception of a rose, or consciousness of any sort. Mental phenomena are, therefore, not regarded as being physical, for the simple reason that they lack many of the attributes that are uniquely characteristic of physical phenomena. Thus, Buddhism has never adopted the physicalist principle that regards only physical things as real.

In contrast to dualism, monism does not accept any fundamental divisions. The fundamentally disparate nature of reality has been central to forms of eastern philosophies for over two millennia. In Indian and Chinese philosophy, monism is integral to how experience is understood. Today, the most common forms of monism in Western philosophy are physicalist. Physicalistic monism asserts that the only existing substance is physical, in some sense of that term to be clarified by our best science. However, a variety of formulations (see below) are possible. Another form of monism, idealism, states that the only existing substance is mental. Although pure idealism, such as that of George Berkeley, is uncommon in contemporary Western philosophy, a more sophisticated variant called panpsychism, according to which mental experience and properties may be at the foundation of physical experience and properties, has been espoused by some philosophers such as Alfred North Whitehead and David Ray Griffin.

Phenomenalism is the theory that representations (or sense data) of external objects are all that exist. Such a view was briefly adopted by Bertrand Russell and many of the logical positivists during the early 20th century. A third possibility is to accept the existence of a basic substance that is neither physical nor mental. The mental and physical would then both be properties of this neutral substance. Such a position was adopted by Baruch Spinoza and was popularized by Ernst Mach in the 19th century. This neutral monism, as it is called, resembles property dualism.

Behaviorism dominated philosophy of mind for much of the 20th century, especially the first half. In psychology, behaviorism developed as a reaction to the inadequacies of introspectionism. Introspective reports on one's own interior mental life are not subject to careful examination for accuracy and cannot be used to form predictive generalizations. Without generalizability and the possibility of third-person examination, the behaviorists argued, psychology cannot be scientific. The way out, therefore, was to eliminate the idea of an interior mental life (and hence an ontologically independent mind) altogether and focus instead on the description of observable behavior.

Parallel to these developments in psychology, a philosophical behaviorism (sometimes called logical behaviorism) was developed. This is characterized by a strong verificationism, which generally considers unverifiable statements about interior mental life pointless. For the behaviorist, mental states are not interior states on which one can make introspective reports. They are just descriptions of behavior or dispositions to behave in certain ways, made by third parties to explain and predict another's behavior.

Philosophical behaviorism has fallen out of favor since the latter half of the 20th century, coinciding with the rise of cognitivism.

Type physicalism (or type-identity theory) was developed by John Smart and Ullin Place as a direct reaction to the failure of behaviorism. These philosophers reasoned that, if mental states are something material, but not behavioral, then mental states are probably identical to internal states of the brain. In very simplified terms: a mental state "M" is nothing other than brain state "B". The mental state "desire for a cup of coffee" would thus be nothing more than the "firing of certain neurons in certain brain regions".
Despite its initial plausibility, the identity theory faces a strong challenge in the form of the thesis of multiple realizability, first formulated by Hilary Putnam. For example, not only humans, but many different species of animals can experience pain. However, it seems highly unlikely that all of these diverse organisms with the same pain experience are in the identical brain state. And if this is the case, then pain cannot be identical to a specific brain state. The identity theory is thus empirically unfounded.

On the other hand, even granted the above, it does not follow that identity theories of all types must be abandoned. According to token identity theories, the fact that a certain brain state is connected with only one mental state of a person does not have to mean that there is an absolute correlation between types of mental state and types of brain state. The type–token distinction can be illustrated by a simple example: the word "green" contains four types of letters (g, r, e, n) with two tokens (occurrences) of the letter "e" along with one each of the others.
The idea of token identity is that only particular occurrences of mental events are identical with particular occurrences or tokenings of physical events. Anomalous monism (see below) and most other non-reductive physicalisms are token-identity theories. Despite these problems, there is a renewed interest in the type identity theory today, primarily due to the influence of Jaegwon Kim.

Functionalism was formulated by Hilary Putnam and Jerry Fodor as a reaction to the inadequacies of the identity theory. Putnam and Fodor saw mental states in terms of an empirical computational theory of the mind. At about the same time or slightly after, D.M. Armstrong and David Kellogg Lewis formulated a version of functionalism that analyzed the mental concepts of folk psychology in terms of functional roles. Finally, Wittgenstein's idea of meaning as use led to a version of functionalism as a theory of meaning, further developed by Wilfrid Sellars and Gilbert Harman. Another one, psychofunctionalism, is an approach adopted by the naturalistic philosophy of mind associated with Jerry Fodor and Zenon Pylyshyn.

What all these different varieties of functionalism share in common is the thesis that mental states are characterized by their causal relations with other mental states and with sensory inputs and behavioral outputs. That is, functionalism abstracts away from the details of the physical implementation of a mental state by characterizing it in terms of non-mental functional properties. For example, a kidney is characterized scientifically by its functional role in filtering blood and maintaining certain chemical balances. From this point of view, it does not really matter whether the kidney be made up of organic tissue, plastic nanotubes or silicon chips: it is the role that it plays and its relations to other organs that define it as a kidney.

Non-reductionist philosophers hold firmly to two essential convictions with regard to mind–body relations: 1) Physicalism is true and mental states must be physical states, but 2) All reductionist proposals are unsatisfactory: mental states cannot be reduced to behavior, brain states or functional states. Hence, the question arises whether there can still be a non-reductive physicalism. Donald Davidson's anomalous monism is an attempt to formulate such a physicalism. He "thinks that when one runs across what are traditionally seen as absurdities of Reason, such as akrasia or self-deception, the personal psychology framework is not to be given up in favor of the subpersonal one, but rather must be enlarged or extended so that the rationality set out by the principle of charity can be found elsewhere."

Davidson uses the thesis of supervenience: mental states supervene on physical states, but are not reducible to them. "Supervenience" therefore describes a functional dependence: there can be no change in the mental without some change in the physical–causal reducibility between the mental and physical without ontological reducibility.

Because non-reductive physicalist theories attempt to both retain the ontological distinction between mind and body and try to solve the "surfeit of explanations puzzle" in some way; critics often see this as a paradox and point out the similarities to epiphenomenalism, in that it is the brain that is seen as the root "cause" not the mind, and the mind seems to be rendered inert.

Epiphenomenalism regards one or more mental states as the byproduct of physical brain states, having no influence on physical states. The interaction is one-way (solving the "surfeit of explanations puzzle") but leaving us with non-reducible mental states (as a byproduct of brain states) – causally reducible, but ontologically irreducible to physical states. Pain would be seen by epiphenomenalists as being caused by the brain state but as not having effects on other brain states, though it might have effects on other mental states (i.e. cause distress).

Weak emergentism is a form of "non-reductive physicalism" that involves a layered view of nature, with the layers arranged in terms of increasing complexity and each corresponding to its own special science. Some philosophers hold that emergent properties causally interact with more fundamental levels, while others maintain that higher-order properties simply supervene over lower levels without direct causal interaction. The latter group therefore holds a less strict, or "weaker", definition of emergentism, which can be rigorously stated as follows: a property P of composite object O is emergent if it is metaphysically impossible for another object to lack property P if that object is composed of parts with intrinsic properties identical to those in O and has those parts in an identical configuration.

Sometimes emergentists use the example of water having a new property when Hydrogen H and Oxygen O combine to form HO (water). In this example there "emerges" a new property of a transparent liquid that would not have been predicted by understanding hydrogen and oxygen as gases. This is analogous to physical properties of the brain giving rise to a mental state. Emergentists try to solve the notorious mind–body gap this way. One problem for emergentism is the idea of "causal closure" in the world that does not allow for a mind-to-body causation.

If one is a materialist and believes that all aspects of our common-sense psychology will find reduction to a mature cognitive neuroscience, and that non-reductive materialism is mistaken, then one can adopt a final, more radical position: eliminative materialism.

There are several varieties of eliminative materialism, but all maintain that our common-sense "folk psychology" badly misrepresents the nature of some aspect of cognition. Eliminativists such as Patricia and Paul Churchland argue that while folk psychology treats cognition as fundamentally sentence-like, the non-linguistic vector/matrix model of neural network theory or connectionism will prove to be a much more accurate account of how the brain works.

The Churchlands often invoke the fate of other, erroneous popular theories and ontologies that have arisen in the course of history. For example, Ptolemaic astronomy served to explain and roughly predict the motions of the planets for centuries, but eventually this model of the solar system was eliminated in favor of the Copernican model. The Churchlands believe the same eliminative fate awaits the "sentence-cruncher" model of the mind in which thought and behavior are the result of manipulating sentence-like states called "propositional attitudes".

Some philosophers take an epistemic approach and argue that the mind–body problem is currently unsolvable, and perhaps will always remain unsolvable to human beings. This is usually termed New mysterianism. Colin McGinn holds that human beings are cognitively closed in regards to their own minds. According to McGinn human minds lack the concept-forming procedures to fully grasp how mental properties such as consciousness arise from their causal basis. An example would be how an elephant is cognitively closed in regards to particle physics.

A more moderate conception has been expounded by Thomas Nagel, which holds that the mind–body problem is currently unsolvable at the present stage of scientific development and that it might take a future scientific paradigm shift or revolution to bridge the explanatory gap. Nagel posits that in the future a sort of "objective phenomenology" might be able to bridge the gap between subjective conscious experience and its physical basis.

Each attempt to answer the mind–body problem encounters substantial problems. Some philosophers argue that this is because there is an underlying conceptual confusion. These philosophers, such as Ludwig Wittgenstein and his followers in the tradition of linguistic criticism, therefore reject the problem as illusory. They argue that it is an error to ask how mental and biological states fit together. Rather it should simply be accepted that human experience can be described in different ways—for instance, in a mental and in a biological vocabulary. Illusory problems arise if one tries to describe the one in terms of the other's vocabulary or if the mental vocabulary is used in the wrong contexts. This is the case, for instance, if one searches for mental states of the brain. The brain is simply the wrong context for the use of mental vocabulary—the search for mental states of the brain is therefore a category error or a sort of fallacy of reasoning.

Today, such a position is often adopted by interpreters of Wittgenstein such as Peter Hacker. However, Hilary Putnam, the originator of functionalism, has also adopted the position that the mind–body problem is an illusory problem which should be dissolved according to the manner of Wittgenstein.

The thesis of physicalism is that the mind is part of the material (or physical) world. Such a position faces the problem that the mind has certain properties that no other material thing seems to possess. Physicalism must therefore explain how it is possible that these properties can nonetheless emerge from a material thing. The project of providing such an explanation is often referred to as the "naturalization of the mental". Some of the crucial problems that this project attempts to resolve include the existence of qualia and the nature of intentionality.

Many mental states seem to be experienced subjectively in different ways by different individuals. And it is characteristic of a mental state that it has some experiential "quality", e.g. of pain, that it hurts. However, the sensation of pain between two individuals may not be identical, since no one has a perfect way to measure how much something hurts or of describing exactly how it feels to hurt. Philosophers and scientists therefore ask where these experiences come from. The existence of cerebral events, in and of themselves, cannot explain why they are accompanied by these corresponding qualitative experiences. The puzzle of why many cerebral processes occur with an accompanying experiential aspect in consciousness seems impossible to explain.

Yet it also seems to many that science will eventually have to explain such experiences. This follows from an assumption about the possibility of reductive explanations. According to this view, if an attempt can be successfully made to explain a phenomenon reductively (e.g., water), then it can be explained why the phenomenon has all of its properties (e.g., fluidity, transparency). In the case of mental states, this means that there needs to be an explanation of why they have the property of being experienced in a certain way.

The 20th-century German philosopher Martin Heidegger criticized the ontological assumptions underpinning such a reductive model, and claimed that it was impossible to make sense of experience in these terms. This is because, according to Heidegger, the nature of our subjective experience and its "qualities" is impossible to understand in terms of Cartesian "substances" that bear "properties". Another way to put this is that the very concept of qualitative experience is incoherent in terms of—or is semantically incommensurable with the concept of—substances that bear properties.

This problem of explaining introspective first-person aspects of mental states and consciousness in general in terms of third-person quantitative neuroscience is called the explanatory gap. There are several different views of the nature of this gap among contemporary philosophers of mind. David Chalmers and the early Frank Jackson interpret the gap as ontological in nature; that is, they maintain that qualia can never be explained by science because physicalism is false. There are two separate categories involved and one cannot be reduced to the other. An alternative view is taken by philosophers such as Thomas Nagel and Colin McGinn. According to them, the gap is epistemological in nature. For Nagel, science is not yet able to explain subjective experience because it has not yet arrived at the level or kind of knowledge that is required. We are not even able to formulate the problem coherently. For McGinn, on other hand, the problem is one of permanent and inherent biological limitations. We are not able to resolve the explanatory gap because the realm of subjective experiences is cognitively closed to us in the same manner that quantum physics is cognitively closed to elephants. Other philosophers liquidate the gap as purely a semantic problem. This semantic problem, of course, led to the famous "Qualia Question", which is: "Does Red cause Redness"?

Intentionality is the capacity of mental states to be directed towards ("about") or be in relation with something in the external world. This property of mental states entails that they have contents and semantic referents and can therefore be assigned truth values. When one tries to reduce these states to natural processes there arises a problem: natural processes are not true or false, they simply happen. It would not make any sense to say that a natural process is true or false. But mental ideas or judgments are true or false, so how then can mental states (ideas or judgments) be natural processes? The possibility of assigning semantic value to ideas must mean that such ideas are about facts. Thus, for example, the idea that Herodotus was a historian refers to Herodotus and to the fact that he was a historian. If the fact is true, then the idea is true; otherwise, it is false. But where does this relation come from? In the brain, there are only electrochemical processes and these seem not to have anything to do with Herodotus.

Philosophy of perception is concerned with the nature of perceptual experience and the status of perceptual objects, in particular how perceptual experience relates to appearances and beliefs about the world. The main contemporary views within philosophy of perception include naive realism, enactivism and representational views.

Humans are corporeal beings and, as such, they are subject to examination and description by the natural sciences. Since mental processes are intimately related to bodily processes, the descriptions that the natural sciences furnish of human beings play an important role in the philosophy of mind. There are many scientific disciplines that study processes related to the mental. The list of such sciences includes: biology, computer science, cognitive science, cybernetics, linguistics, medicine, pharmacology, and psychology.

The theoretical background of biology, as is the case with modern natural sciences in general, is fundamentally materialistic. The objects of study are, in the first place, physical processes, which are considered to be the foundations of mental activity and behavior. The increasing success of biology in the explanation of mental phenomena can be seen by the absence of any empirical refutation of its fundamental presupposition: "there can be no change in the mental states of a person without a change in brain states."

Within the field of neurobiology, there are many subdisciplines that are concerned with the relations between mental and physical states and processes: Sensory neurophysiology investigates the relation between the processes of perception and stimulation. Cognitive neuroscience studies the correlations between mental processes and neural processes. Neuropsychology describes the dependence of mental faculties on specific anatomical regions of the brain. Lastly, evolutionary biology studies the origins and development of the human nervous system and, in as much as this is the basis of the mind, also describes the ontogenetic and phylogenetic development of mental phenomena beginning from their most primitive stages. Evolutionary biology furthermore places tight constraints on any philosophical theory of the mind, as the gene-based mechanism of natural selection does not allow any giant leaps in the development of neural complexity or neural software but only incremental steps over long time periods.

The methodological breakthroughs of the neurosciences, in particular the introduction of high-tech neuroimaging procedures, has propelled scientists toward the elaboration of increasingly ambitious research programs: one of the main goals is to describe and comprehend the neural processes which correspond to mental functions (see: neural correlate). Several groups are inspired by these advances.

Computer science concerns itself with the automatic processing of information (or at least with physical systems of symbols to which information is assigned) by means of such things as computers. From the beginning, computer programmers have been able to develop programs that permit computers to carry out tasks for which organic beings need a mind. A simple example is multiplication. It is not clear whether computers could be said to have a mind. Could they, someday, come to have what we call a mind? This question has been propelled into the forefront of much philosophical debate because of investigations in the field of artificial intelligence (AI).

Within AI, it is common to distinguish between a modest research program and a more ambitious one: this distinction was coined by John Searle in terms of a weak AI and strong AI. The exclusive objective of "weak AI", according to Searle, is the successful simulation of mental states, with no attempt to make computers become conscious or aware, etc. The objective of strong AI, on the contrary, is a computer with consciousness similar to that of human beings. The program of strong AI goes back to one of the pioneers of computation Alan Turing. As an answer to the question "Can computers think?", he formulated the famous Turing test. Turing believed that a computer could be said to "think" when, if placed in a room by itself next to another room that contained a human being and with the same questions being asked of both the computer and the human being by a third party human being, the computer's responses turned out to be indistinguishable from those of the human. Essentially, Turing's view of machine intelligence followed the behaviourist model of the mind—intelligence is as intelligence does. The Turing test has received many criticisms, among which the most famous is probably the Chinese room thought experiment formulated by Searle.

The question about the possible sensitivity (qualia) of computers or robots still remains open. Some computer scientists believe that the specialty of AI can still make new contributions to the resolution of the "mind–body problem". They suggest that based on the reciprocal influences between software and hardware that takes place in all computers, it is possible that someday theories can be discovered that help us to understand the reciprocal influences between the human mind and the brain (wetware).

Psychology is the science that investigates mental states directly. It uses generally empirical methods to investigate concrete mental states like joy, fear or obsessions. Psychology investigates the laws that bind these mental states to each other or with inputs and outputs to the human organism.

An example of this is the psychology of perception. Scientists working in this field have discovered general principles of the perception of forms. A law of the psychology of forms says that objects that move in the same direction are perceived as related to each other. This law describes a relation between visual input and mental perceptual states. However, it does not suggest anything about the nature of perceptual states. The laws discovered by psychology are compatible with all the answers to the mind–body problem already described.

Cognitive science is the interdisciplinary scientific study of the mind and its processes. It examines what cognition is, what it does, and how it works. It includes research on intelligence and behavior, especially focusing on how information is represented, processed, and transformed (in faculties such as perception, language, memory, reasoning, and emotion) within nervous systems (human or other animal) and machines (e.g. computers). Cognitive science consists of multiple research disciplines, including psychology, artificial intelligence, philosophy, neuroscience, linguistics, anthropology, sociology, and education. It spans many levels of analysis, from low-level learning and decision mechanisms to high-level logic and planning; from neural circuitry to modular brain organisation. Rowlands argues that cognition is enactive, embodied, embedded, affective and (potentially) extended. The position is taken that the "classical sandwich" of cognition sandwiched between perception and action is artificial; cognition has to be seen as a product of a strongly coupled interaction that cannot be divided this way.

Most of the discussion in this article has focused on one style or tradition of philosophy in modern Western culture, usually called analytic philosophy (sometimes described as Anglo-American philosophy). Many other schools of thought exist, however, which are sometimes subsumed under the broad (and vague) label of continental philosophy. In any case, though topics and methods here are numerous, in relation to the philosophy of mind the various schools that fall under this label (phenomenology, existentialism, etc.) can globally be seen to differ from the analytic school in that they focus less on language and logical analysis alone but also take in other forms of understanding human existence and experience. With reference specifically to the discussion of the mind, this tends to translate into attempts to grasp the concepts of thought and perceptual experience in some sense that does not merely involve the analysis of linguistic forms.

Immanuel Kant's "Critique of Pure Reason", first published in 1781 and presented again with major revisions in 1787, represents a significant intervention into what will later become known as the philosophy of mind. Kant's first critique is generally recognized as among the most significant works of modern philosophy in the West. Kant is a figure whose influence is marked in both continental and analytic/Anglo-American philosophy. Kant's work develops an in-depth study of transcendental consciousness, or the life of the mind as conceived through universal categories of consciousness.

In Georg Wilhelm Friedrich Hegel's "Philosophy of Mind" (frequently translated as "Philosophy of Spirit" or Geist), the third part of his "Encyclopedia of the Philosophical Sciences", Hegel discusses three distinct types of mind: the "subjective mind/spirit", the mind of an individual; the "objective mind/spirit", the mind of society and of the State; and the "Absolute mind/spirit", the position of religion, art, and philosophy. See also Hegel's "The Phenomenology of Spirit". Nonetheless, Hegel's work differs radically from the style of Anglo-American philosophy of mind.

In 1896, Henri Bergson made in "Matter and Memory" "Essay on the relation of body and spirit" a forceful case for the ontological difference of body and mind by reducing the problem to the more definite one of memory, thus allowing for a solution built on the "empirical test case" of aphasia.

In modern times, the two main schools that have developed in response or opposition to this Hegelian tradition are phenomenology and existentialism. Phenomenology, founded by Edmund Husserl, focuses on the contents of the human mind (see noema) and how processes shape our experiences. Existentialism, a school of thought founded upon the work of Søren Kierkegaard, focuses on Human predicament and how people deal with the situation of being alive. Existential-phenomenology represents a major branch of continental philosophy (they are not contradictory), rooted in the work of Husserl but expressed in its fullest forms in the work of Martin Heidegger, Jean-Paul Sartre, Simone de Beauvoir and Maurice Merleau-Ponty. See Heidegger's "Being and Time", Merleau-Ponty's "Phenomenology of Perception", Sartre's "Being and Nothingness", and Simone de Beauvoir's "The Second Sex".

There are countless subjects that are affected by the ideas developed in the philosophy of mind. Clear examples of this are the nature of death and its definitive character, the nature of emotion, of perception and of memory. Questions about what a person is and what his or her identity consists of also have much to do with the philosophy of mind. There are two subjects that, in connection with the philosophy of the mind, have aroused special attention: free will and the self.

In the context of philosophy of mind, the problem of free will takes on renewed intensity. This is certainly the case, at least, for materialistic determinists. According to this position, natural laws completely determine the course of the material world. Mental states, and therefore the will as well, would be material states, which means human behavior and decisions would be completely determined by natural laws. Some take this reasoning a step further: people cannot determine by themselves what they want and what they do. Consequently, they are not free.

This argumentation is rejected, on the one hand, by the compatibilists. Those who adopt this position suggest that the question "Are we free?" can only be answered once we have determined what the term "free" means. The opposite of "free" is not "caused" but "compelled" or "coerced". It is not appropriate to identify freedom with indetermination. A free act is one where the agent could have done otherwise if it had chosen otherwise. In this sense a person can be free even though determinism is true. The most important compatibilist in the history of the philosophy was David Hume. More recently, this position is defended, for example, by Daniel Dennett.

On the other hand, there are also many incompatibilists who reject the argument because they believe that the will is free in a stronger sense called libertarianism. These philosophers affirm the course of the world is either a) not completely determined by natural law where natural law is intercepted by physically independent agency, b) determined by indeterministic natural law only, or c) determined by indeterministic natural law in line with the subjective effort of physically non-reducible agency. Under Libertarianism, the will does not have to be deterministic and, therefore, it is potentially free. Critics of the second proposition (b) accuse the incompatibilists of using an incoherent concept of freedom. They argue as follows: if our will is not determined by anything, then we desire what we desire by pure chance. And if what we desire is purely accidental, we are not free. So if our will is not determined by anything, we are not free.

The philosophy of mind also has important consequences for the concept of "self". If by "self" or "I" one refers to an essential, immutable nucleus of the "person", some modern philosophers of mind, such as Daniel Dennett believe that no such thing exists. According to Dennett and other contemporaries, the self is considered an illusion. The idea of a self as an immutable essential nucleus derives from the idea of an immaterial soul. Such an idea is unacceptable to modern philosophers with physicalist orientations and their general skepticism of the concept of "self" as postulated by David Hume, who could never catch himself "not" doing, thinking or feeling anything. However, in the light of empirical results from developmental psychology, developmental biology and neuroscience, the idea of an essential inconstant, material nucleus—an integrated representational system distributed over changing patterns of synaptic connections—seems reasonable.



</doc>
<doc id="154170" url="https://en.wikipedia.org/wiki?curid=154170" title="Intuition">
Intuition

Intuition is the ability to acquire knowledge without recourse to conscious reasoning. Different writers give the word "intuition" a great variety of different meanings, ranging from direct access to unconscious knowledge, unconscious cognition, inner sensing, inner insight to unconscious pattern-recognition and the ability to understand something instinctively, without the need for conscious reasoning.

The word "intuition" comes from the Latin verb "intueri" translated as "consider" or from the late middle English word "intuit", "to contemplate".

Both Eastern and Western philosophers have studied the concept in great detail. Philosophy of mind deals with the concept.

In the East intuition is mostly intertwined with religion and spirituality, and various meanings exist from different religious texts.

In Hinduism various attempts have been made to interpret the Vedic and other esoteric texts.

For Sri Aurobindo intuition comes under the realms of knowledge by identity; he describes the psychological plane in humans (often referred to as mana in sanskrit) having two arbitrary natures, the first being imprinting of psychological experiences which is constructed through sensory information (mind seeking to become aware of external world). The second nature being the action when it seeks to be aware of itself, resulting in humans being aware of their existence or aware of being angry & aware of other emotions. He terms this second nature as knowledge by identity.
He finds that at present as the result of evolution the mind has accustomed itself to depend upon certain physiological functioning and their reactions as its normal means of entering into relations with the outer material world. As a result, when we seek to know about the external world the dominant habit is through arriving at truths about things via what our senses convey to us. However, knowledge by identity, which we currently only give the awareness of human beings' existence, can be extended further to outside of ourselves resulting in intuitive knowledge.

He finds this intuitive knowledge was common to older humans (Vedic) and later was taken over by reason which currently organises our perception, thoughts and actions resulting from Vedic to metaphysical philosophy and later to experimental science. He finds that this process, which seems to be decent, is actually a circle of progress, as a lower faculty is being pushed to take up as much from a higher way of working. He finds when self-awareness in the mind is applied to one's self and the outer (other) -self, results in luminous self-manifesting identity; the reason also converts itself into the form of the self-luminous intuitional knowledge.

Osho believed consciousness of human beings to be in increasing order from basic animal instincts to intelligence and intuition, and humans being constantly living in that conscious state often moving between these states depending on their affinity. He also suggests living in the state of intuition is one of the ultimate aims of humanity.

Advaita vedanta (a school of thought) takes intuition to be an experience through which one can come in contact with an experience Brahman.

Buddhism finds intuition to be a faculty in the mind of immediate knowledge and puts the term intuition beyond the mental process of conscious thinking, as the conscious thought cannot necessarily access subconscious information, or render such information into a communicable form. In Zen Buddhism various techniques have been developed to help develop one's intuitive capability, such as koans – the resolving of which leads to states of minor enlightenment (satori). In parts of Zen Buddhism intuition is deemed a mental state between the Universal mind and one's individual, discriminating mind.

In Islam there are various scholars with varied interpretations of intuition (often termed as hadas (Arabic: حدس), hitting correctly on a mark), sometimes relating the ability of having intuitive knowledge to prophethood. 
Siháb al Din-al Suhrawadi, in his book "Philosophy Of Illumination" (ishraq), finds that intuition is knowledge acquired through illumination, is mystical in nature, and also suggests mystical contemplation (mushahada) to bring about correct judgment. Ibn Sīnā finds the ability of having intuition as a "prophetic capacity" and describes it as knowledge obtained without intentionally acquiring it. He finds that regular knowledge is based on imitation while intuitive knowledge is based on intellectual certitude.

In the West, intuition does not appear as a separate field of study, and early mentions and definitions can be traced back to Plato. In his book "Republic" he tries to define intuition as a fundamental capacity of human reason to comprehend the true nature of reality. In his works "Meno" and "Phaedo", he describes intuition as a pre-existing knowledge residing in the "soul of eternity", and a phenomenon by which one becomes conscious of pre-existing knowledge. He provides an example of mathematical truths, and posits that they are not arrived at by reason. He argues that these truths are accessed using a knowledge already present in a dormant form and accessible to our intuitive capacity. This concept by Plato is also sometimes referred to as anamnesis. The study was later continued by his followers.

In his book "Meditations on First Philosophy", Descartes refers to an “intuition” as a pre-existing knowledge gained through rational reasoning or discovering truth through contemplation. This definition is commonly referred to as rational intuition. Later philosophers, such as Hume, have more ambiguous interpretations of intuition. Hume claims intuition is a recognition of relationships (relation of time, place, and causation) while he states that "the resemblance" (recognition of relations) "will strike the eye" (which would not require further examination) but goes on to state, "or rather in mind"—attributing intuition to power of mind, contradicting the theory of empiricism.

Immanuel Kant’s notion of “intuition” differs considerably from the Cartesian notion, and consists of the basic sensory information provided by the cognitive faculty of sensibility (equivalent to what might loosely be called perception). Kant held that our mind casts all of our external intuitions in the form of space, and all of our internal intuitions (memory, thought) in the form of time. Intuitionism is a position advanced by Luitzen Egbertus Jan Brouwer in philosophy of mathematics derived from Kant's claim that all mathematical knowledge is knowledge of the pure forms of the intuition—that is, intuition that is not empirical. Intuitionistic logic was devised by Arend Heyting to accommodate this position (and has been adopted by other forms of constructivism in general). It is characterized by rejecting the law of excluded middle: as a consequence it does not in general accept rules such as double negation elimination and the use of reductio ad absurdum to prove the existence of something.

Intuitions are customarily appealed to independently of any particular theory of how intuitions provide evidence for claims, and there are divergent accounts of what sort of mental state intuitions are, ranging from mere spontaneous judgment to a special presentation of a necessary truth. In recent years a number of philosophers, especially George Bealer have tried to defend appeals to intuition against Quinean doubts about conceptual analysis. A different challenge to appeals to intuition has recently come from experimental philosophers, who argue that appeals to intuition must be informed by the methods of social science.

The metaphilosophical assumption that philosophy ought to depend on intuitions has recently been challenged by experimental philosophers (e.g., Stephen Stich). One of the main problems adduced by experimental philosophers is that intuitions differ, for instance, from one culture to another, and so it seems problematic to cite them as evidence for a philosophical claim. Timothy Williamson has responded to such objections against philosophical methodology by arguing that intuition plays no special role in philosophy practice, and that skepticism about intuition cannot be meaningfully separated from a general skepticism about judgment. On this view, there are no qualitative differences between the methods of philosophy and common sense, the sciences or mathematics. Others like Ernest Sosa seek to support intuition by arguing that the objections against intuition merely highlight a verbal disagreement.

According to Sigmund Freud, knowledge could only be attained through the intellectual manipulation of carefully made observations and rejected any other means of acquiring knowledge such as intuition, and his findings could have been an analytic turn of his mind towards the subject.

In Carl Jung's theory of the ego, described in 1916 in "Psychological Types", intuition is an "irrational function", opposed most directly by sensation, and opposed less strongly by the "rational functions" of thinking and feeling. Jung defined intuition as "perception via the unconscious": using sense-perception only as a starting point, to bring forth ideas, images, possibilities, ways out of a blocked situation, by a process that is mostly unconscious.

Jung said that a person in whom intuition is dominant, an "intuitive type", acts not on the basis of rational judgment but on sheer intensity of perception. An extraverted intuitive type, "the natural champion of all minorities with a future", orients to new and promising but unproven possibilities, often leaving to chase after a new possibility before old ventures have borne fruit, oblivious to his or her own welfare in the constant pursuit of change. An introverted intuitive type orients by images from the unconscious, ever exploring the psychic world of the archetypes, seeking to perceive the meaning of events, but often having no interest in playing a role in those events and not seeing any connection between the contents of the psychic world and him- or herself. Jung thought that extraverted intuitive types were likely entrepreneurs, speculators, cultural revolutionaries, often undone by a desire to escape every situation before it becomes settled and constraining—even repeatedly leaving lovers for the sake of new romantic possibilities. His introverted intuitive types were likely mystics, prophets, or cranks, struggling with a tension between protecting their visions from influence by others and making their ideas comprehensible and reasonably persuasive to others—a necessity for those visions to bear real fruit.

In more-recent psychology, intuition can encompass the ability to know valid solutions to problems and decision making. For example, the recognition primed decision (RPD) model explains how people can make relatively fast decisions without having to compare options. Gary Klein found that under time pressure, high stakes, and changing parameters, experts used their base of experience to identify similar situations and intuitively choose feasible solutions. Thus, the RPD model is a blend of intuition and analysis. The intuition is the pattern-matching process that quickly suggests feasible courses of action. The analysis is the mental simulation, a conscious and deliberate review of the courses of action.

Instinct is often misinterpreted as intuition and its reliability considered to be dependent on past knowledge and occurrences in a specific area. For example, someone who has had more experiences with children will tend to have a better instinct about what they should do in certain situations with them. This is not to say that one with a great amount of experience is always going to have an accurate intuition.

Intuitive abilities were quantitatively tested at Yale University in the 1970s. While studying nonverbal communication, researchers noted that some subjects were able to read nonverbal facial cues before reinforcement occurred. In employing a similar design, they noted that highly intuitive subjects made decisions quickly but could not identify their rationale. Their level of accuracy, however, did not differ from that of non-intuitive subjects.

According to the works of Daniel Kahneman, intuition is the ability to automatically generate solutions without long logical arguments or evidence.

Intuition, as a gut feeling based on experience, has been found to be useful for business leaders for making judgement about people, culture and strategy. Law enforcement officers often claim to observe suspects and immediately "know" that they possess a weapon or illicit narcotic substances, which could also be action of instincts. Often unable to articulate why they reacted or what prompted them at the time of the event, they sometimes retrospectively can plot their actions based upon what had been clear and present danger signals. Such examples liken intuition to "gut feelings" and when viable illustrate preconscious activity.

Intuition Peak in Antarctica is so named "in appreciation of the role of scientific intuition for the advancement of human knowledge."



</doc>
<doc id="25754129" url="https://en.wikipedia.org/wiki?curid=25754129" title="Theory of forms">
Theory of forms

The theory of Forms or theory of Ideas is a philosophical theory, concept, or world-view, attributed to Plato, that the physical world is not as real or true as timeless, absolute, unchangeable ideas. According to this theory, ideas in this sense, often capitalized and translated as "Ideas" or "Forms", are the non-physical essences of all things, of which objects and matter in the physical world are merely imitations. Plato speaks of these entities only through the characters (primarily Socrates) of his dialogues who sometimes suggest that these Forms are the only objects of study that can provide knowledge. The theory itself is contested from within Plato's dialogues, and it is a general point of controversy in philosophy. Whether the theory represents Plato's own views is held in doubt by modern scholarship. Nonetheless the theory is considered to be a classical solution to the problem of universals.

The early Greek concept of form precedes attested philosophical usage and is represented by a number of words mainly having to do with vision, sight, and appearance. Plato uses these aspects of sight and appearance from the early Greek concept of the form in his dialogues to explain the Forms and the Good.

The meaning of the term ("eidos"), "visible form", and related terms μορφή ("morphē"), "shape", and φαινόμενα ("phainomena"), "appearances", from φαίνω ("phainō"), "shine", Indo-European ""*bʰeh₂-"" or "*bhā-" remained stable over the centuries until the beginning of philosophy, when they became equivocal, acquiring additional specialized philosophic meanings. The pre-Socratic philosophers, starting with Thales, noted that appearances change, and began to ask what the thing that changes "really" is. The answer was substance, which stands under the changes and is the actually existing thing being seen. The status of appearances now came into question. What is the form really and how is that related to substance?

The Forms are expounded upon in Plato's dialogues and general speech, in that every object or quality in reality has a form: dogs, human beings, mountains, colors, courage, love, and goodness. Form answers the question, "What is that?" Plato was going a step further and asking what Form itself is. He supposed that the object was essentially or "really" the Form and that the phenomena were mere shadows mimicking the Form; that is, momentary portrayals of the Form under different circumstances. The problem of universals – how can one thing in general be many things in particular – was solved by presuming that Form was a distinct singular thing but caused plural representations of itself in particular objects. For example, in the dialogue Parmenides, Socrates states: "Nor, again, if a person were to show that all is one by partaking of one, and at the same time many by partaking of many, would that be very astonishing. But if he were to show me that the absolute one was many, or the absolute many one, I should be truly amazed." Matter is considered particular in itself. For Plato, forms, such as beauty, are more real than any objects that imitate them. Though the forms are timeless and unchanging, physical things are in a constant change of existence. Where forms are unqualified perfection, physical things are qualified and conditioned.

These Forms are the essences of various objects: they are that without which a thing would not be the kind of thing it is. For example, there are countless tables in the world but the Form of tableness is at the core; it is the essence of all of them. Plato's Socrates held that the world of Forms is transcendent to our own world (the world of substances) and also is the essential basis of reality. Super-ordinate to matter, Forms are the most pure of all things. Furthermore, he believed that true knowledge/intelligence is the ability to grasp the world of Forms with one's mind.

A Form is "aspatial" (transcendent to space) and "atemporal" (transcendent to time). Atemporal means that it does not exist within any time period, rather it provides the formal basis for time. It therefore formally grounds beginning, persisting and ending. It is neither eternal in the sense of existing forever, nor mortal, of limited duration. It exists transcendent to time altogether. Forms are aspatial in that they have no spatial dimensions, and thus no orientation in space, nor do they even (like the point) have a location. They are non-physical, but they are not in the mind. Forms are extra-mental (i.e. real in the strictest sense of the word).

A Form is an objective "blueprint" of perfection. The Forms are perfect and unchanging representations of objects and qualities. For example, the Form of beauty or the Form of a triangle. For the form of a triangle say there is a triangle drawn on a blackboard. A triangle is a polygon with 3 sides. The triangle as it is on the blackboard is far from perfect. However, it is only the intelligibility of the Form "triangle" that allows us to know the drawing on the chalkboard is a triangle, and the Form "triangle" is perfect and unchanging. It is exactly the same whenever anyone chooses to consider it; however, time only effects the observer and not of the triangle. It follows that the same attributes would exist for the Form of beauty and for all Forms.

The words, εἶδος ("eidos") and ἰδέα ("idea") come from the Indo-European root or "*weid-" "see" (cognate with Sanskrit "vétti"). "Eidos" (though not "idea") is already attested in texts of the Homeric era, the earliest Greek literature. This transliteration and the translation tradition of German and Latin lead to the expression "theory of Ideas." The word is however not the English "idea," which is a mental concept only.

The theory of matter and form (today's hylomorphism) started with Plato and possibly germinal in some of the presocratic writings. The forms were considered as being "in" something else, which Plato called nature ("physis"). The latter seemed as carved "wood", ὕλη ("hyle") in Greek, corresponding to "materia" in Latin, from which the English word "matter" is derived, shaped by receiving (or exchanging) forms.

The English word "form" may be used to translate two distinct concepts that concerned Plato—the outward "form" or appearance of something, and "Form" in a new, technical nature, that never...assumes a form like that of any of the things which enter into her; ... But the forms which enter into and go out of her are the likenesses of real existences modelled after their patterns in a wonderful and inexplicable manner... The objects that are seen, according to Plato, are not real, but literally "mimic" the real Forms. In the Allegory of the Cave expressed in "Republic", the things that are ordinarily perceived in the world are characterized as shadows of the real things, which are not perceived directly. That which the observer understands when he views the world mimics the archetypes of the many types and properties (that is, of universals) of things observed.

Plato often invokes, particularly in his dialogues "Phaedo", "Republic" and "Phaedrus", poetic language to illustrate the mode in which the Forms are said to exist. Near the end of the "Phaedo", for example, Plato describes the world of Forms as a pristine region of the physical universe located above the surface of the Earth ("Phd." 109a-111c). In the "Phaedrus" the Forms are in a "place beyond heaven" ("huperouranios topos") ("Phdr." 247c ff); and in the "Republic" the sensible world is contrasted with the intelligible realm ("noēton topon") in the famous Allegory of the Cave.

It would be a mistake to take Plato's imagery as positing the intelligible world as a literal physical space apart from this one. Plato emphasizes that the Forms are not beings that extend in space (or time), but subsist apart from any physical space whatsoever. Thus we read in the "Symposium" of the Form of Beauty: "It is not anywhere in another thing, as in an animal, or in earth, or in heaven, or in anything else, but itself by itself with itself," (211b). And in the "Timaeus" Plato writes: "Since these things are so, we must agree that that which keeps its own form unchangingly, which has not been brought into being and is not destroyed, which neither receives into itself anything else from anywhere else, "nor itself enters into anything anywhere", is one thing," (52a, emphasis added).

According to Plato, Socrates postulated a world of ideal Forms, which he admitted were impossible to know. Nevertheless, he formulated a very specific description of that world, which did not match his metaphysical principles. Corresponding to the world of Forms is our world, that of the shadows, an imitation of the real one. Just as shadows exist only because of the light of a fire, our world exists as, "the offspring of the good". Our world is modeled after the patterns of the Forms. The function of humans in our world is therefore to imitate the ideal world as much as possible which, importantly, includes imitating the good, i.e. acting morally.

Plato lays out much of this theory in the "Republic" where, in an attempt to define Justice, he considers many topics including the constitution of the ideal state. While this state, and the Forms, do not exist on earth, because their imitations do, Plato says we are able to form certain well-founded opinions about them, through a theory called recollection.

The republic is a greater imitation of Justice:Our aim in founding the state was not the disproportional happiness of any one class, but the greatest happiness of the whole; we thought that in a state ordered with a view to the good of the whole we should be most likely to find justice.

The key to not know how such a state might come into existence is the word "founding" ("oikidzomen"), which is used of colonization. It was customary in such instances to receive a constitution from an elected or appointed lawgiver; however in Athens, lawgivers were appointed to reform the constitution from time to time (for example, Draco, Solon). In speaking of reform, Socrates uses the word "purge" ("diakathairountes") in the same sense that Forms exist purged of matter.

The purged society is a regulated one presided over by philosophers educated by the state, who maintain three non-hereditary classes as required: the tradesmen (including merchants and professionals), the guardians (militia and police) and the philosophers (legislators, administrators and the philosopher-king). Class is assigned at the end of education, when the state institutes individuals in their occupation. Socrates expects class to be hereditary but he allows for mobility according to natural ability. The criteria for selection by the academics is ability to perceive forms (the analog of English "intelligence") and martial spirit as well as predisposition or aptitude.

The views of Socrates on the proper order of society are certainly contrary to Athenian values of the time and must have produced a shock effect, intentional or not, accounting for the animosity against him. For example, reproduction is much too important to be left in the hands of untrained individuals: "... the possession of women and the procreation of children ... will ... follow the general principle that friends have all things in common, ..." The family is therefore to be abolished and the children – whatever their parentage – to be raised by the appointed mentors of the state.

Their genetic fitness is to be monitored by the physicians: "... he (Asclepius, a culture hero) did not want to lengthen out good-for-nothing lives, or have weak fathers begetting weaker sons – if a man was not able to live in the ordinary way he had no business to cure him ..." Physicians minister to the healthy rather than cure the sick: "... (Physicians) will minister to better natures, giving health both of soul and of body; but those who are diseased in their bodies they will leave to die, and the corrupt and incurable souls they will put an end to themselves." Nothing at all in Greek medicine so far as can be known supports the airy (in the Athenian view) propositions of Socrates. Yet it is hard to be sure of Socrates' real views considering that there are no works written by Socrates himself. There are two common ideas pertaining to the beliefs and character of Socrates: the first being the Mouthpiece Theory where writers use Socrates in dialogue as a mouthpiece to get their own views across. However, since most of what we know about Socrates comes from plays, most of the Platonic plays are accepted as the more accurate Socrates since Plato was a direct student of Socrates.

Perhaps the most important principle is that just as the Good must be supreme so must its image, the state, take precedence over individuals in everything. For example, guardians "... will have to be watched at every age in order that we may see whether they preserve their resolution and never, under the influence either of force or enchantment, forget or cast off their sense of duty to the state." This concept of requiring guardians of guardians perhaps suffers from the Third Man weakness (see below): guardians require guardians require guardians, ad infinitum. The ultimate trusty guardian is missing. Socrates does not hesitate to face governmental issues many later governors have found formidable: "Then if anyone at all is to have the privilege of lying, the rulers of the state should be the persons, and they ... may be allowed to lie for the public good."

Plato's conception of Forms actually differs from dialogue to dialogue, and in certain respects it is never fully explained, so many aspects of the theory are open to interpretation. Forms are first introduced in the Phaedo, but in that dialogue the concept is simply referred to as something the participants are already familiar with, and the theory itself is not developed. Similarly, in the Republic, Plato relies on the concept of Forms as the basis of many of his arguments but feels no need to argue for the validity of the theory itself or to explain precisely what Forms are. Commentators have been left with the task of explaining what Forms are and how visible objects participate in them, and there has been no shortage of disagreement. Some scholars advance the view that Forms are paradigms, perfect examples on which the imperfect world is modeled. Others interpret Forms as universals, so that the Form of Beauty, for example, is that quality that all beautiful things share. Yet others interpret Forms as "stuffs," the conglomeration of all instances of a quality in the visible world. Under this interpretation, we could say there is a little beauty in one person, a little beauty in another—all the beauty in the world put together is the Form of Beauty. Plato himself was aware of the ambiguities and inconsistencies in his Theory of Forms, as is evident from the incisive criticism he makes of his own theory in the Parmenides.

Plato's main evidence for the existence of Forms is intuitive only and is as follows.

We call both the sky and blue jeans by the same color, blue. However, clearly a pair of jeans and the sky are not the same color; moreover, the wavelengths of light reflected by the sky at every location and all the millions of blue jeans in every state of fading constantly change, and yet we somehow have a consensus of the basic form Blueness as it applies to them. Says Plato:But if the very nature of knowledge changes, at the time when the change occurs there will be no knowledge, and, according to this view, there will be no one to know and nothing to be known: but if that which knows and that which is known exist ever, and the beautiful and the good and every other thing also exist, then I do not think that they can resemble a process of flux, as we were just now supposing.

Plato believed that long before our bodies ever existed, our souls existed and inhabited heaven, where they became directly acquainted with the forms themselves. Real knowledge, to him, was knowledge of the forms. But knowledge of the forms cannot be gained through sensory experience because the forms are not in the physical world. Therefore, our real knowledge of the forms must be the memory of our initial acquaintance with the forms in heaven. Therefore, what we seem to learn is in fact just remembering.

No one has ever seen a perfect circle, nor a perfectly straight line, yet everyone knows what a circle and a straight line are. Plato utilizes the tool-maker's blueprint as evidence that Forms are real:... when a man has discovered the instrument which is naturally adapted to each work, he must express this natural form, and not others which he fancies, in the material ...

Perceived circles or lines are not exactly circular or straight, and true circles and lines could never be detected since by definition they are sets of infinitely small points. But if the perfect ones were not real, how could they direct the manufacturer?

Plato was well aware of the limitations of the theory, as he offered his own criticisms of it in his dialogue "Parmenides". There Socrates is portrayed as a young philosopher acting as junior counterfoil to aged Parmenides. To a certain extent it is tongue-in-cheek as the older Socrates will have solutions to some of the problems that are made to puzzle the younger.

The dialogue does present a very real difficulty with the Theory of Forms, which Plato most likely only viewed as problems for later thought. These criticisms were later emphasized by Aristotle in rejecting an independently existing world of Forms. It is worth noting that Aristotle was a pupil and then a junior colleague of Plato; it is entirely possible that the presentation of "Parmenides" "sets up" for Aristotle; that is, they agreed to disagree.

One difficulty lies in the conceptualization of the "participation" of an object in a form (or Form). The young Socrates conceives of his solution to the problem of the universals in another metaphor, which though wonderfully apt, remains to be elucidated:
Nay, but the idea may be like the day which is one and the same in many places at once, and yet continuous with itself; in this way each idea may be one and the same in all at the same time.

But exactly how is a Form like the day in being everywhere at once? The solution calls for a distinct form, in which the particular instances, which are not identical to the form, participate; i.e., the form is shared out somehow like the day to many places. The concept of "participate", represented in Greek by more than one word, is as obscure in Greek as it is in English. Plato hypothesized that distinctness meant existence as an independent being, thus opening himself to the famous third man argument of Parmenides, which proves that forms cannot independently exist and be participated.

If universal and particulars – say man or greatness – all exist and are the same then the Form is not one but is multiple. If they are only like each other then they contain a form that is the same and others that are different. Thus if we presume that the Form and a particular are alike then there must be another, or third Form, man or greatness by possession of which they are alike. An infinite regression would then result; that is, an endless series of third men. The ultimate participant, greatness, rendering the entire series great, is missing. Moreover, any Form is not unitary but is composed of infinite parts, none of which is the proper Form.

The young Socrates (some may say the young Plato) did not give up the Theory of Forms over the Third Man but took another tack, that the particulars do not exist as such. Whatever they are, they "mime" the Forms, appearing to be particulars. This is a clear dip into representationalism, that we cannot observe the objects as they are in themselves but only their representations. That view has the weakness that if only the mimes can be observed then the real Forms cannot be known at all and the observer can have no idea of what the representations are supposed to represent or that they are representations.

Socrates' later answer would be that men already know the Forms because they were in the world of Forms before birth. The mimes only recall these Forms to memory. The comedian Aristophanes wrote a play, "The Clouds", poking fun of Socrates with his head in the clouds.

The topic of Aristotle's criticism of Plato's Theory of Forms is a large one and continues to expand. Rather than quote Plato, Aristotle often summarized. Classical commentaries thus recommended Aristotle as an introduction to Plato. As a historian of prior thought, Aristotle was invaluable, however this was secondary to his own dialectic and in some cases he treats purported implications as if Plato had actually mentioned them, or even defended them. In examining Aristotle's criticism of The Forms, it is helpful to understand Aristotle's own hylomorphic forms, by which he intends to salvage much of Plato's theory.

In the summary passage quoted above Plato distinguishes between real and non-real "existing things", where the latter term is used of substance. The figures that the artificer places in the gold are not substance, but gold is. Aristotle stated that, for Plato, all things studied by the sciences have Form and asserted that Plato considered only substance to have Form. Uncharitably, this leads him to something like a contradiction: Forms existing as the objects of science, but not-existing as non-substance. Scottish philosopher W.D. Ross objects to this as a mischaracterization of Plato.

Plato did not claim to know where the line between Form and non-Form is to be drawn. As Cornford points out, those things about which the young Socrates (and Plato) asserted "I have often been puzzled about these things" (in reference to Man, Fire and Water), appear as Forms in later works. However, others do not, such as Hair, Mud, Dirt. Of these, Socrates is made to assert, "it would be too absurd to suppose that they have a Form."

Ross also objects to Aristotle's criticism that Form Otherness accounts for the differences between Forms and purportedly leads to contradictory forms: the Not-tall, the Not-beautiful, etc. That particulars participate in a Form is for Aristotle much too vague to permit analysis. By one way in which he unpacks the concept, the Forms would cease to be of one essence due to any multiple participation. As Ross indicates, Plato didn't make that leap from "A is not B" to "A is Not-B." Otherness would only apply to its own particulars and not to those of other Forms. For example, there is no Form Not-Greek, only "particulars" of Form Otherness that somehow "suppress" Form Greek.

Regardless of whether Socrates meant the particulars of Otherness yield Not-Greek, Not-tall, Not-beautiful, etc., the particulars would operate specifically rather than generally, each somehow yielding only one exclusion.

Plato had postulated that we know Forms through a remembrance of the soul's past lives and Aristotle's arguments against this treatment of epistemology are compelling. For Plato, particulars somehow do not exist, and, on the face of it, "that which is non-existent cannot be known". See "Metaphysics" III 3–4.

Nominalism (from Latin "nomen", "name") says that ideal universals are mere names, human creations; the blueness shared by sky and blue jeans is a shared concept, communicated by our word "blueness". Blueness is held not to have any existence beyond that which it has in instances of blue things. This concept arose in the Middle Ages, as part of Scholasticism.

Scholasticism was a highly multinational, polyglottal school of philosophy, and the nominalist argument may be more obvious if an example is given in more than one language. For instance, colour terms are strongly variable by language; some languages consider blue and green the same colour, others have monolexemic terms for several shades of blue, which are considered different; other, like the Mandarin "qing" denote both blue and black. The German word "Stift" means a pen or a pencil, and also anything of the same shape. English does not have such a word. The English "pencil" originally meant "small paintbrush"; the term later included the silver rod used for silverpoint. The German "Bleistift" and "Silberstift" can both be called "Stift", but this term also includes felt-tip pens, which are clearly not pencils.

The shifting and overlapping nature of these concepts makes it easy to imagine them as mere names, with meanings not rigidly defined, but specific enough to be useful for communication. Given a group of objects, how is one to decide if it contains only instances of a single Form, or several mutually-exclusive Forms?

The theory is presented in the following dialogues:





</doc>
<doc id="59350035" url="https://en.wikipedia.org/wiki?curid=59350035" title="Mind in eastern philosophy">
Mind in eastern philosophy

The study of the mind in Eastern philosophy has parallels to the Western study of the Philosophy of mind as a branch of philosophy that studies the nature of the mind. Dualism and monism are the two central schools of thought on the mind–body problem in the Western tradition, although nuanced views have arisen that do not fit one or the other category neatly. Dualism is found in both Eastern and Western traditions (in the Sankhya and Yoga schools of Hindu philosophy as well as Plato) but its entry into Western philosophy was thanks to René Descartes in the 17th century. This article on mind in eastern philosophy deals with this subject from the standpoint of eastern philosophy which is historically strongly separated from the Western tradition and its approach to the Western philosophy of mind.

Substance Dualism is a common feature of several orthodox Hindu schools including the Sāṅkhya, Nyāya, Yoga and Dvaita Vedanta. In these schools a clear difference is drawn between matter and a non-material soul, which is eternal and undergoes samsara, a cycle of death and rebirth. The Nyāya school argued that qualities such as cognition and desire are inherent qualities which are not possessed by anything solely material, and therefore by process of elimination must belong to a non-material self, the atman. Many of these schools see their spiritual goal as moksha, liberation from the cycle of reincarnation.

In the Advaita Vedanta of the 8th century Indian philosopher Śaṅkara, the mind, body and world are all held to be appearances of the same unchanging eternal conscious entity called Brahman. Advaita, which means non-dualism, holds the view that all that exists is pure absolute consciousness. The fact that the world seems to be made up of changing entities is an illusion, or Maya. The only thing that exists is Brahman, which is described as Satchitananda (Being, consciousness and bliss). Advaita Vedanta is best described by a verse which states "Brahman is alone True, and this world of plurality is an error; the individual self is not different from Brahman."

Another form of monistic Vedanta is Vishishtadvaita (qualified non-dualism) as posited by the eleventh century philosopher Ramanuja. Ramanuja criticized Advaita Vedanta by arguing that consciousness is always intentional and that it is also always a property of something. Ramanuja's Brahman is defined by a multiplicity of qualities and properties in a single monistic entity. This doctrine is called "samanadhikaranya" (several things in a common substrate).

Arguably the first exposition of empirical materialism in the history of philosophy is in the Cārvāka school (also called Lokāyata). The Cārvāka school rejected the existence of anything but matter (which they defined as being made up of the four elements), including God and the soul. Therefore, they held that even consciousness was nothing but a construct made up of atoms. A section of the Cārvāka school believed in a material soul made up of air or breath, but since this also was a form of matter, it was not said to survive death.

Buddhist teachings describe that the mind manifests moment-to-moment as sense impressions and mental phenomena that are continuously changing. The moment-by-moment manifestation of the mind-stream has been described as happening in every person all the time, even in a scientist who analyses various phenomena in the world, or analyses the material body including the organ brain. The manifestation of the mind-stream is also described as being influenced by physical laws, biological laws, psychological laws, volitional laws, and universal laws.

A salient feature of Buddhist philosophy which sets it apart from Indian orthodoxy is the centrality of the doctrine of not-self (Pāli. anatta, Skt. anātman). The Buddha's not-self doctrine sees humans as an impermanent composite of five psychological and physical aspects instead of a single fixed self. In this sense, what is called ego or the self is merely a convenient fiction, an illusion that does not apply to anything real but to an erroneous way of looking at the ever-changing stream of five interconnected aggregate factors. The relationship between these aggregates is said to be one of dependent-arising (pratītyasamutpāda). This means that all things, including mental events, arise co-dependently from a plurality of other causes and conditions. This seems to reject both causal determinist and epiphenomenalist conceptions of mind.

Three centuries after the death of the Buddha (c. 150 BCE) saw the growth of a large body of literature called the Abhidharma in several contending Buddhist schools. In the Abhidharmic analysis of mind, the ordinary thought is defined as prapañca ('conceptual proliferation'). According to this theory, perceptual experience is bound up in multiple conceptualizations (expectations, judgments and desires). This proliferation of conceptualizations form our illusory superimposition of concepts like self and other upon an ever-changing stream of aggregate phenomena.
In this conception of mind no strict distinction is made between the conscious faculty and the actual sense perception of various phenomena. Consciousness is instead said to be divided into six sense modalities, five for the five senses and sixth for perception of mental phenomena. The arising of cognitive awareness is said to depend on sense perception, awareness of the mental faculty itself which is termed mental or 'introspective awareness' ("manovijñāna") and attention ("āvartana"), the picking out of objects out of the constantly changing stream of sensory impressions.

Rejection of a permanent agent eventually led to the philosophical problems of the seeming continuity of mind and also of explaining how rebirth and karma continue to be relevant doctrines without an eternal mind. This challenge was met by the Theravāda school by introducing the concept of mind as a factor of existence. This "life-stream" (Bhavanga-sota) is an undercurrent forming the condition of being. The continuity of a karmic "person" is therefore assured in the form of a mindstream (citta-santana), a series of flowing mental moments arising from the subliminal life-continuum mind (Bhavanga-citta), mental content, and attention.

The Sautrāntika school held a form of phenomenalism that saw the world as imperceptible. It held that external objects exist only as a support for cognition, which can only apprehend mental representations. This influenced the later Yogācāra school of Mahayana Buddhism. The Yogācāra school is often called the mind-only school because of its internalist stance that consciousness is the ultimate existing reality. The works of Vasubandhu have often been interpreted as arguing for some form of Idealism. Vasubandhu uses the dream argument and a mereological refutation of atomism to attack the reality of external objects as anything other than mental entities. Scholarly interpretations of Vasubandhu's philosophy vary widely, and include phenomenalism, neutral monism and realist phenomenology.

The Indian Mahayana schools were divided on the issue of the possibility of reflexive awareness ("svasaṃvedana"). Dharmakīrti accepted the idea of reflexive awareness as expounded by the Yogācāra school, comparing it to a lamp that illuminates itself while also illuminating other objects. This was strictly rejected by Mādhyamika scholars like Candrakīrti. Since in the philosophy of the Mādhyamika all things and mental events are characterized by emptiness, they argued that consciousness could not be an inherently reflexive ultimate reality since that would mean it was self-validating and therefore not characterized by emptiness. These views were ultimately reconciled by the 8th century thinker Śāntarakṣita. In Śāntarakṣita's synthesis he adopts the idealist Yogācāra views of reflexive awareness as a conventional truth into the structure of the two truths doctrine. Thus he states: "By relying on the Mind-Only system, know that external entities do not exist. And by relying on this Middle Way system, know that no self exists at all, even in that [mind]." 

The Yogācāra school also developed the theory of the repository consciousness ("ālayavijñāna") to explain continuity of mind in rebirth and accumulation of karma. This repository consciousness acts as a storehouse for karmic seeds (bija) when all other senses are absent during the process of death and rebirth as well as being the causal potentiality of dharmic phenomena. Thus according to B. Alan Wallace: 
No constituents of the body—in the brain or elsewhere—transform into mental states and processes. Such subjective experiences do not emerge from the body, but neither do they emerge from nothing. Rather, all objective mental appearances arise from the substrate, and all subjective mental states and processes arise from the substrate consciousness.

Tibetan Buddhist theories of mind evolved directly from the Indian Mahayana views. Thus the founder of the Gelug school, Je Tsongkhapa discusses the Yogācāra system of the Eight Consciousnesses in his "Explanation of the Difficult Points". He would later come to repudiate Śāntarakṣita's pragmatic idealism. 
According to the 14th Dalai Lama the mind can be defined "as an entity that has the nature of mere experience, that is, 'clarity and knowing'. It is the knowing nature, or agency, that is called mind, and this is non-material." The simultaneously dual nature of mind is as follows:
The 14th Dalai Lama has also explicitly laid out his theory of mind as experiential dualism which is described above under the different types of dualism.

Because Tibetan philosophy of mind is ultimately soteriological, it focuses on meditative practices such as Dzogchen and Mahamudra that allow a practitioner to experience the true reflexive nature of their mind directly. This unobstructed knowledge of one's primordial, empty and non-dual Buddha nature is called rigpa. The mind's innermost nature is described among various schools as pure luminosity or "clear light" ('od gsal) and is often compared to a crystal ball or a mirror. Sogyal Rinpoche speaks of mind thus:
"Imagine a sky, empty, spacious, and pure from the beginning; its essence is like this. Imagine a sun, luminous, clear, unobstructed, and spontaneously present; its nature is like this."

The central issue in Chinese Zen philosophy of mind is in the difference between the pure and awakened mind and the defiled mind. Chinese Chan master Huangpo described the mind as without beginning and without form or limit while the defiled mind was that which was obscured by attachment to form and concepts. The pure Buddha-mind is thus able to see things "as they truly are", as absolute and non-dual "thusness" (Tathatā). This non-conceptual seeing also includes the paradoxical fact that there is no difference between a defiled and a pure mind, as well as no difference between samsara and nirvana.

In the Shobogenzo, the Japanese philosopher Dogen argued that body and mind are neither ontologically nor phenomenologically distinct but are characterized by a oneness called "shin jin" (bodymind). According to Dogen, "casting off body and mind" ("Shinjin datsuraku") in zazen will allow one to experience things-as-they-are ("genjokoan") which is the nature of original enlightenment ("hongaku").



</doc>
<doc id="161999" url="https://en.wikipedia.org/wiki?curid=161999" title="Idea">
Idea

In philosophy, ideas (E I D I E A)are usually taken as mental representational images of some object. Ideas can also be abstract concepts that do not present as mental images. Many philosophers have considered ideas to be a fundamental ontological category of being. The capacity to create and understand the meaning of ideas is considered to be an essential and defining feature of human beings. In a popular sense, an idea arises in a reflexive, spontaneous manner, even without thinking or serious reflection, for example, when we talk about the "idea" of a person or a place. A new or original idea can often lead to innovation.

The word "idea" comes from Greek ἰδέα "idea" "form, pattern," from the root of ἰδεῖν "idein", "to see." 

One view on the nature of ideas is that there exist some ideas (called "innate ideas") which are so general and abstract that they could not have arisen as a representation of an object of our perception but rather were in some sense always present. These are distinguished from "adventitious ideas" which are images or concepts which are accompanied by the judgment that they are caused or occasioned by an external object.

Another view holds that we only discover ideas in the same way that we discover the real world, from personal experiences. The view that humans acquire all or almost all their behavioral traits from nurture (life experiences) is known as "tabula rasa" ("blank slate"). Most of the confusions in the way ideas arise is at least in part due to the use of the term "idea" to cover both the representation perceptics and the object of conceptual thought. This can be always illustrated in terms of the scientific doctrines of innate ideas, "concrete ideas versus abstract ideas", as well as "simple ideas versus complex ideas".

Plato in Ancient Greece was one of the earliest philosophers to provide a detailed discussion of ideas and of the thinking process (in Plato's Greek the word "idea" carries a rather different sense from our modern English term). Plato argued in dialogues such as the "Phaedo", "Symposium", "Republic", and "Timaeus" that there is a realm of ideas or forms ("eidei"), which exist independently of anyone who may have thoughts on these ideas, and it is the ideas which distinguish mere opinion from knowledge, for unlike material things which are transient and liable to contrary properties, ideas are unchanging and nothing but just what they are. Consequently, Plato seems to assert forcefully that material things can only be the objects of opinion; real knowledge can only be had of unchanging ideas. Furthermore, ideas for Plato appear to serve as universals; consider the following passage from the "Republic":
Descartes often wrote of the meaning of "idea" as an image or representation, often but not necessarily "in the mind", which was well known in the vernacular. Despite that Descartes is usually credited with the invention of the non-Platonic use of the term, he at first followed this vernacular use. In his "Meditations on First Philosophy" he says, "Some of my thoughts are like images of things, and it is to these alone that the name 'idea' properly belongs." He sometimes maintained that ideas were innate and uses of the term "idea" diverge from the original primary scholastic use. He provides multiple non-equivalent definitions of the term, uses it to refer to as many as six distinct kinds of entities, and divides "ideas" inconsistently into various genetic categories. For him knowledge took the form of ideas and philosophical investigation is the deep consideration of these entities.

In striking contrast to Plato's use of idea is that of John Locke. In his Introduction to An Essay Concerning Human Understanding, Locke defines "idea" as "that term which, I think, serves best to stand for whatsoever is the object of the understanding when a man thinks, I have used it to express whatever is meant by phantasm, notion, species, or whatever it is which the mind can be employed about in thinking; and I could not avoid frequently using it." He said he regarded the book necessary to examine our own abilities and see what objects our understandings were, or were not, fitted to deal with. In his philosophy other outstanding figures followed in his footsteps — Hume and Kant in the 18th century, Arthur Schopenhauer in the 19th century, and Bertrand Russell, Ludwig Wittgenstein, and Karl Popper in the 20th century. Locke always believed in "good sense" — not pushing things to extremes and on taking fully into account the plain facts of the matter. He considered his common-sense ideas "good-tempered, moderate, and down-to-earth."

As John Locke studied humans in his work “An Essay Concerning Human Understanding” he continually referenced Descartes for ideas as he asked this fundamental question: “When we are concerned with something about which we have no certain knowledge, what rules or standards should guide how confident we allow ourselves to be that our opinions are right?” A simpler way of putting it is how do humans know ideas, and what are the different types of ideas. An idea to Locke “can simply mean some sort of brute experience.” He shows that there are “No innate principles in the mind.”. Thus, he concludes that “our ideas are all experiential in nature.” An experience can either be a sensation or a reflection: “consider whether there are any innate ideas in the mind before any are brought in by the impression from sensation or reflection.” Therefore, an idea was an experience in which the human mind apprehended something.

In a Lockean view, there are really two types of ideas: complex and simple. Simple ideas are the building blocks for much more complex ideas, and “While the mind is wholly passive in the reception of simple ideas, it is very active in the building of complex ideas…” Complex ideas, therefore, can either be modes, substances, or relations. Modes are when ideas are combined in order to convey new information. For instance, David Banach gives the example of beauty as a mode. He says that it is the combination of color and form. Substances, however, is different. Substances are certain objects, that can either be dogs, cats, or tables. And relations represent the relationship between two or more ideas. In this way, Locke did, in fact, answer his own questions about ideas and humans.

Hume differs from Locke by limiting "idea" to the more or less vague mental reconstructions of perceptions, the perceptual process being described as an "impression." Hume shared with Locke the basic empiricist premise that it is only from life experiences (whether their own or others') that humans' knowledge of the existence of anything outside of themselves can be ultimately derived, that they shall carry on doing what they are prompted to do by their emotional drives of varying kinds. In choosing the means to those ends, they shall follow their accustomed associations of ideas. Hume has contended and defended the notion that "reason alone is merely the 'slave of the passions'." 

Immanuel Kant defines an "idea" as opposed to a "concept". "Regulative ideas" are ideals that one must tend towards, but by definition may not be completely realized. Liberty, according to Kant, is an idea. The autonomy of the rational and universal subject is opposed to the determinism of the empirical subject. Kant felt that it is precisely in knowing its limits that philosophy exists. The business of philosophy he thought was not to give rules, but to analyze the private judgement of good common sense.

Whereas Kant declares limits to knowledge ("we can never know the thing in itself"), in his epistemological work, Rudolf Steiner sees "ideas" as "objects of experience" which the mind apprehends, much as the eye apprehends light. In "Goethean Science" (1883), he declares, "Thinking ... is no more and no less an organ of perception than the eye or ear. Just as the eye perceives colors and the ear sounds, so thinking perceives ideas." He holds this to be the premise upon which Goethe made his natural-scientific observations.

Wundt widens the term from Kant's usage to include "conscious representation of some object or process of the external world". In so doing, he includes not only ideas of memory and imagination, but also perceptual processes, whereas other psychologists confine the term to the first two groups. One of Wundt's main concerns was to investigate conscious processes in their own context by experiment and introspection. He regarded both of these as "exact methods", interrelated in that experimentation created optimal conditions for introspection. Where the experimental method failed, he turned to other "objectively valuable aids", specifically to "those products of cultural communal life which lead one to infer particular mental motives. Outstanding among these are speech, myth, and social custom." Wundt designed the basic mental activity apperception — a unifying function which should be understood as an activity of the will. Many aspects of his empirical physiological psychology are used today. One is his principles of mutually enhanced contrasts and of assimilation and dissimilation (i.e. in color and form perception and his advocacy of "objective" methods of expression and of recording results, especially in language. Another is the principle of heterogony of ends — that multiply motivated acts lead to unintended side effects which in turn become motives for new actions.

C. S. Peirce published the first full statement of pragmatism in his important works "" (1878) and "" (1877). In "How to Make Our Ideas Clear" he proposed that a "clear idea" (in his study he uses concept and "idea" as synonymic) is defined as one, when it is apprehended such as it will be recognized wherever it is met, and no other will be mistaken for it. If it fails of this clearness, it is said to be obscure. He argued that to understand an idea clearly we should ask ourselves what difference its application would make to our evaluation of a proposed solution to the problem at hand. Pragmatism (a term he appropriated for use in this context), he defended, was a method for ascertaining the meaning of terms (as a theory of meaning). The originality of his ideas is in their rejection of what was accepted as a view and understanding of knowledge by scientists for some 250 years, i.e. that, he pointed, knowledge was an impersonal fact. Peirce contended that we acquire knowledge as "participants", not as "spectators". He felt "the real", sooner or later, is information acquired through ideas and knowledge with the application of logical reasoning would finally result in. He also published many papers on logic in relation to "ideas".

G. F. Stout and J. M. Baldwin, in the "Dictionary of Philosophy and Psychology", define "idea" as "the reproduction with a more or less adequate image, of an object not actually present to the senses." They point out that an idea and a perception are by various authorities contrasted in various ways. "Difference in degree of intensity", "comparative absence of bodily movement on the part of the subject", "comparative dependence on mental activity", are suggested by psychologists as characteristic of an idea as compared with a perception.

It should be observed that an idea, in the narrower and generally accepted sense of a mental reproduction, is frequently composite. That is, as in the example given above of the idea of a chair, a great many objects, differing materially in detail, all call a single idea. When a man, for example, has obtained an idea of chairs in general by comparison with which he can say "This is a chair, that is a stool", he has what is known as an "abstract idea" distinct from the reproduction in his mind of any particular chair (see abstraction). Furthermore, a complex idea may not have any corresponding physical object, though its particular constituent elements may severally be the reproductions of actual perceptions. Thus the idea of a centaur is a complex mental picture composed of the ideas of man and horse, that of a mermaid of a woman and a fish.

Diffusion studies explore the spread of ideas from culture to culture. Some anthropological theories hold that all cultures imitate ideas from one or a few original cultures, the Adam of the Bible, or several cultural circles that overlap. Evolutionary diffusion theory holds that cultures are influenced by one another but that similar ideas can be developed in isolation.

In the mid-20th century, social scientists began to study how and why ideas spread from one person or culture to another. Everett Rogers pioneered diffusion of innovations studies, using research to prove factors in adoption and profiles of adopters of ideas. In 1976, in his book "The Selfish Gene", Richard Dawkins suggested applying biological evolutionary theories to the spread of ideas. He coined the term "meme" to describe an abstract unit of selection, equivalent to the gene in evolutionary biology.

James Boswell recorded Samuel Johnson's opinion about ideas. Johnson claimed that they are mental images or internal visual pictures. As such, they have no relation to words or the concepts which are designated by verbal names.

To protect the cause of invention and innovation, the legal constructions of Copyrights and Patents were established. Patent law regulates various aspects related to the functional manifestation of inventions based on new ideas or incremental improvements to existing ones. Thus, patents have a direct relationship to ideas.

In some cases, authors can be granted limited legal monopolies on the manner in which certain works are expressed. This is known colloquially as copyright, although the term intellectual property is used mistakenly in place of "copyright". Copyright law regulating the aforementioned monopolies generally does not cover the actual ideas. The law does not bestow the legal status of property upon ideas per se. Instead, laws purport to regulate events related to the usage, copying, production, sale and other forms of exploitation of the fundamental expression of a work, that may or may not carry ideas. Copyright law is fundamentally different from patent law in this respect: patents do grant monopolies on ideas (more on this below).

A copyright is meant to regulate some aspects of the usage of expressions of a work, "not" an idea. Thus, copyrights have a negative relationship to ideas.

Work means a tangible medium of expression. It may be an original or derivative work of art, be it literary, dramatic, musical recitation, artistic, related to sound recording, etc. In (at least) countries adhering to the Berne Convention, copyright automatically starts covering the work upon the original creation and fixation thereof, without any extra steps. While creation usually involves an idea, the idea in itself does not suffice for the purposes of claiming copyright. 
Confidentiality and nondisclosure agreements are legal instruments that assist corporations and individuals in keeping ideas from escaping to the general public. Generally, these instruments are covered by contract law.




</doc>
<doc id="37080" url="https://en.wikipedia.org/wiki?curid=37080" title="Thought">
Thought

Thought encompasses an "aim-oriented flow of ideas and associations that can lead to a reality-oriented conclusion". Although thinking is an activity of an existential value for humans, there is still no consensus as to how it is adequately defined or understood.

Because thought underlies many human actions and interactions, understanding its physical and metaphysical origins and its effects has been a longstanding goal of many academic disciplines including philosophy, linguistics, psychology, neuroscience, artificial intelligence, biology, sociology and cognitive science.

Thinking allows humans to make sense of, interpret, represent or model the world they experience, and to make predictions about that world. It is therefore helpful to an organism with needs, objectives, and desires as it makes plans or otherwise attempts to accomplish those goals.

The word "thought" comes from Old English "þoht", or "geþoht", from stem of "þencan" "to conceive of in the mind, consider".

The word "thought" may mean:

Definitions may or may not require that thought

Definitions of thought may also be derived directly or indirectly from theories of thought.


The phenomenology movement in philosophy saw a radical change in the way in which we understand thought. Martin Heidegger's phenomenological analyses of the existential structure of man in "Being and Time" cast new light on the issue of thinking, unsettling traditional cognitive or rational interpretations of man which affect the way we understand thought. The notion of the fundamental role of non-cognitive understanding in rendering possible thematic consciousness informed the discussion surrounding artificial intelligence (AI) during the 1970s and 1980s.

Phenomenology, however, is not the only approach to thinking in modern Western philosophy. Philosophy of mind is a branch of philosophy that studies the nature of the mind, mental events, mental functions, mental properties, consciousness and their relationship to the physical body, particularly the brain. The mind–body problem, i.e. the relationship of the mind to the body, is commonly seen as the central issue in philosophy of mind, although there are other issues concerning the nature of the mind that do not involve its relation to the physical body.

The mind–body problem concerns the explanation of the relationship that exists between minds, or mental processes, and bodily states or processes. The main aim of philosophers working in this area is to determine the nature of the mind and mental states/processes, and how—or even if—minds are affected by and can affect the body.

Human perceptual experiences depend on stimuli which arrive at one's various sensory organs from the external world and these stimuli cause changes in one's mental state, ultimately causing one to feel a sensation, which may be pleasant or unpleasant. Someone's desire for a slice of pizza, for example, will tend to cause that person to move his or her body in a specific manner and in a specific direction to obtain what he or she wants. The question, then, is how it can be possible for conscious experiences to arise out of a lump of gray matter endowed with nothing but electrochemical properties. A related problem is to explain how someone's propositional attitudes (e.g. beliefs and desires) can cause that individual's neurons to fire and his muscles to contract in exactly the correct manner. These comprise some of the puzzles that have confronted epistemologists and philosophers of mind from at least the time of René Descartes.

The above reflects a classical, functional description of how we work as cognitive, thinking systems. However the apparently irresolvable mind–body problem is said to be overcome, and bypassed, by the embodied cognition approach, with its roots in the work of Heidegger, Piaget, Vygotsky, Merleau-Ponty and the pragmatist John Dewey.

This approach states that the classical approach of separating the mind and analysing its processes is misguided: instead, we should see that the mind, actions of an embodied agent, and the environment it perceives and envisions, are all parts of a whole which determine each other. Therefore, functional analysis of the mind alone will always leave us with the mind–body problem which cannot be solved.

A neuron (also known as a neurone or nerve cell) is an excitable cell in the nervous system that processes and transmits information by electrochemical signaling. Neurons are the core components of the brain, the vertebrate spinal cord, the invertebrate ventral nerve cord and the peripheral nerves. A number of specialized types of neurons exist: sensory neurons respond to touch, sound, light and numerous other stimuli affecting cells of the sensory organs that then send signals to the spinal cord and brain. Motor neurons receive signals from the brain and spinal cord that cause muscle contractions and affect glands. Interneurons connect neurons to other neurons within the brain and spinal cord. Neurons respond to stimuli, and communicate the presence of stimuli to the central nervous system, which processes that information and sends responses to other parts of the body for action. Neurons do not go through mitosis and usually cannot be replaced after being destroyed, although astrocytes have been observed to turn into neurons, as they are sometimes pluripotent.

Psychologists have concentrated on thinking as an intellectual exertion aimed at finding an answer to a question or the solution of a practical problem. Cognitive psychology is a branch of psychology that investigates internal mental processes such as problem solving, memory, and language. The school of thought arising from this approach is known as cognitivism, which is interested in how people mentally represent information processing. It had its foundations in the Gestalt psychology of Max Wertheimer, Wolfgang Köhler, and Kurt Koffka, and in the work of Jean Piaget, who provided a theory of stages/phases that describes children's cognitive development.

Cognitive psychologists use psychophysical and experimental approaches to understand, diagnose, and solve problems, concerning themselves with the mental processes which mediate between stimulus and response. They study various aspects of thinking, including the psychology of reasoning, and how people make decisions and choices, solve problems, as well as engage in creative discovery and imaginative thought. Cognitive theory contends that solutions to problems either take the form of algorithms: rules that are not necessarily understood but promise a solution, or of heuristics: rules that are understood but that do not always guarantee solutions. Cognitive science differs from cognitive psychology in that algorithms that are intended to simulate human behavior are implemented or implementable on a computer. In other instances, solutions may be found through insight, a sudden awareness of relationships.

In developmental psychology, Jean Piaget was a pioneer in the study of the development of thought from birth to maturity. In his theory of cognitive development, thought is based on actions on the environment. That is, Piaget suggests that the environment is understood through assimilations of objects in the available schemes of action and these accommodate to the objects to the extent that the available schemes fall short of the demands. As a result of this interplay between assimilation and accommodation, thought develops through a sequence of stages that differ qualitatively from each other in mode of representation and complexity of inference and understanding. That is, thought evolves from being based on perceptions and actions at the sensorimotor stage in the first two years of life to internal representations in early childhood. Subsequently, representations are gradually organized into logical structures which first operate on the concrete properties of the reality, in the stage of concrete operations, and then operate on abstract principles that organize concrete properties, in the stage of formal operations. In recent years, the Piagetian conception of thought was integrated with information processing conceptions. Thus, thought is considered as the result of mechanisms that are responsible for the representation and processing of information. In this conception, speed of processing, cognitive control, and working memory are the main functions underlying thought. In the neo-Piagetian theories of cognitive development, the development of thought is considered to come from increasing speed of processing, enhanced cognitive control, and increasing working memory.

Positive psychology emphasizes the positive aspects of human psychology as equally important as the focus on mood disorders and other negative symptoms. In "Character Strengths and Virtues", Peterson and Seligman list a series of positive characteristics. One person is not expected to have every strength, nor are they meant to fully capsulate that characteristic entirely. The list encourages positive thought that builds on a person's strengths, rather than how to "fix" their "symptoms".

The "id", "ego" and "super-ego" are the three parts of the "psychic apparatus" defined in Sigmund Freud's structural model of the psyche; they are the three theoretical constructs in terms of whose activity and interaction mental life is described. According to this model, the uncoordinated instinctual trends are encompassed by the "id", the organized realistic part of the psyche is the "ego", and the critical, moralizing function is the "super-ego".

For psychoanalysis, the unconscious does not include all that is not conscious, rather only what is actively repressed from conscious thought or what the person is averse to knowing consciously. In a sense this view places the self in relationship to their unconscious as an adversary, warring with itself to keep what is unconscious hidden. If a person feels pain, all he can think of is alleviating the pain. Any of his desires, to get rid of pain or enjoy something, command the mind what to do. For Freud, the unconscious was a repository for socially unacceptable ideas, wishes or desires, traumatic memories, and painful emotions put out of mind by the mechanism of psychological repression. However, the contents did not necessarily have to be solely negative. In the psychoanalytic view, the unconscious is a force that can only be recognized by its effects—it expresses itself in the symptom.

Social psychology is the study of how people and groups interact. Scholars in this interdisciplinary area are typically either psychologists or sociologists, though all social psychologists employ both the individual and the group as their units of analysis.

Despite their similarity, psychological and sociological researchers tend to differ in their goals, approaches, methods, and terminology. They also favor separate academic journals and professional societies. The greatest period of collaboration between sociologists and psychologists was during the years immediately following World War II. Although there has been increasing isolation and specialization in recent years, some degree of overlap and influence remains between the two disciplines.

The collective unconscious, sometimes known as collective subconscious, is a term of analytical psychology, coined by Carl Jung. It is a part of the unconscious mind, shared by a society, a people, or all humanity, in an interconnected system that is the product of all common experiences and contains such concepts as science, religion, and morality. While Freud did not distinguish between "individual psychology" and "collective psychology", Jung distinguished the collective unconscious from the personal subconscious particular to each human being. The collective unconscious is also known as "a reservoir of the experiences of our species".

In the "Definitions" chapter of Jung's seminal work "Psychological Types", under the definition of "collective" Jung references "representations collectives", a term coined by Lucien Lévy-Bruhl in his 1910 book "How Natives Think". Jung says this is what he describes as the collective unconscious. Freud, on the other hand, did not accept the idea of a collective unconscious.





</doc>
<doc id="2118843" url="https://en.wikipedia.org/wiki?curid=2118843" title="Progress">
Progress

Progress is the movement towards a refined, improved, or otherwise desired state. In the context of progressivism, it refers to the proposition that advancements in technology, science, and social organization have resulted, and by extension will continue to result, in an improved human condition; the latter may happen as a result of direct human action, as in social enterprise or through activism, or as a natural part of sociocultural evolution.

The concept of progress was introduced in the early 19th-century social theories, especially social evolution as described by Auguste Comte and Herbert Spencer. It was present in the Enlightenment's philosophies of history. As a goal, social progress has been advocated by varying realms of political ideologies with different theories on how it is to be achieved.

Specific indicators for measuring progress can range from economic data, technical innovations, change in the political or legal system, and questions bearing on individual life chances, such as life expectancy and risk of disease and disability.

GDP growth has become a key orientation for politics and is often taken as a key figure to evaluate a politician's performance. However, GDP has a number of flaws that make it a bad measure of progress, especially for developed countries. For example, environmental damage is not taken into account nor is the sustainability of economic activity. Wikiprogress has been set up to share information on evaluating societal progress. It aims to facilitate the exchange of ideas, initiatives and knowledge. HumanProgress.org is another online resource that seeks to compile data on different measures of societal progress.

Our World in Data is a scientific online publication, based at the University of Oxford, that studies how to make progress against large global problems such as poverty, disease, hunger, climate change, war, existential risks, and inequality.
The mission of Our World in Data is to present "research and data to make progress against the world’s largest problems".
The Social Progress Index is a tool developed by the International Organization Imperative Social Progress, which measures the extent to which countries cover social and environmental needs of its citizenry. There are fifty-two indicators in three areas or dimensions: Basic Human Needs, and Foundations of Wellbeing and Opportunities which show the relative performance of nations.

Indices that can be used to measure progress include:

Scientific progress is the idea that the scientific community learns more over time, which causes a body of scientific knowledge to accumulate. The chemists in the 19th century knew less about chemistry than the chemists in the 20th century, and they in turn knew less than the chemists in the 21st century. Looking forward, today's chemists reasonably expect that chemists in future centuries will know more than they do. 

This process differs from non-science fields, such as human languages or history: the people who spoke a now-extinct language, or who lived through a historical time period, can be said to have known different things from the scholars who studied it later, but they cannot be said to know less about their lives than the modern scholars. Some valid knowledge is lost through the passage of time, and other knowledge is gained, with the result that the non-science fields do not make scientific progress towards understanding their subject areas.

From the 18th century through late 20th century, the history of science, especially of the physical and biological sciences, was often presented as a progressive accumulation of knowledge, in which true theories replaced false beliefs. Some more recent historical interpretations, such as those of Thomas Kuhn, tend to portray the history of science in terms of competing paradigms or conceptual systems in a wider matrix of intellectual, cultural, economic and political trends. These interpretations, however, have met with opposition for they also portray the history of science as an incoherent system of incommensurable paradigms, not leading to any scientific progress, but only to the illusion of progress.

Aspects of social progress, as described by Condorcet, have included the disappearance of slavery, the rise of literacy, the lessening of inequalities between the sexes, reforms of harsh prisons and the decline of poverty. The social progress of a society can be measured based on factors such as its ability to address fundamental human needs, help citizens improve their quality of life, and provide opportunities for citizens to succeed.

Social progress is often improved by increases in GDP, although other factors are also relevant. An imbalance between economic and social progress hinders further economic progress, and can lead to political instability.

How progress improved the status of women in traditional society was a major theme of historians starting in the Enlightenment and continuing to today. British theorists William Robertson (1721–1793) and Edmund Burke (1729–1797), along with many of their contemporaries, remained committed to Christian- and republican-based conceptions of virtue, while working within a new Enlightenment paradigm. The political agenda related beauty, taste, and morality to the imperatives and needs of modern societies of a high level of sophistication and differentiation. Two themes in the work of Robertson and Burke—the nature of women in 'savage' and 'civilized' societies and 'beauty in distress'—reveals how long-held convictions about the character of women, especially with regard to their capacity and right to appear in the public domain, were modified and adjusted to the idea of progress and became central to modern European civilization.

Classics experts have examined the status of women in the ancient world, concluding that in the Roman Empire, with its superior social organization, internal peace, and rule of law, allowed women to enjoy a somewhat better standing than in ancient Greece, where women were distinctly inferior. The inferior status of women in traditional China has raised the issue of whether the idea of progress requires a thoroughgoing reject of traditionalism—a belief held by many Chinese reformers in the early 20th century.

Historians Leo Marx and Bruce Mazlish asking, "Should we in fact abandon the idea of progress as a view of the past," answer that there is no doubt "that the status of women has improved markedly" in cultures that have adopted the Enlightenment idea of progress.

Modernization was promoted by classical liberals in the 19th and 20th centuries, who called for the rapid modernization of the economy and society to remove the traditional hindrances to free markets and free movements of people. During the Enlightenment in Europe social commentators and philosophers began to realize that people "themselves" could change society and change their way of life. Instead of being made completely by gods, there was increasing room for the idea that people themselves "made their own society"—and not only that, as Giambattista Vico argued, "because" people made their own society, they could also fully comprehend it. This gave rise to new sciences, or proto-sciences, which claimed to provide new scientific knowledge about what society was like, and how one may change it for the better.

In turn, this gave rise to progressive opinion, in contrast with conservational opinion. The social conservationists were skeptical about panaceas for social ills. According to conservatives, attempts to radically remake society normally make things worse. Edmund Burke was the leading exponent of this, although later-day liberals like Hayek have espoused similar views. They argue that society changes organically and naturally, and that grand plans for the remaking of society, like the French Revolution, National Socialism and Communism hurt society by removing the traditional constraints on the exercise of power.

The scientific advances of the 16th and 17th centuries provided a basis for Francis Bacon's book the New Atlantis. In the 17th century, Bernard le Bovier de Fontenelle described progress with respect to arts and the sciences, saying that each age has the advantage of not having to rediscover what was accomplished in preceding ages. The epistemology of John Locke provided further support and was popularized by the Encyclopedists Diderot, Holbach, and Condorcet. Locke had a powerful influence on the American Founding Fathers. The first complete statement of progress is that of Turgot, in his "A Philosophical Review of the Successive Advances of the Human Mind" (1750). For Turgot, progress covers not only the arts and sciences but, on their base, the whole of culture—manner, mores, institutions, legal codes, economy, and society. Condorcet predicted the disappearance of slavery, the rise of literacy, the lessening of inequalities between the sexes, reforms of harsh prisons and the decline of poverty.

John Stuart Mill's (1806–1873) ethical and political thought demonstrated faith in the power of ideas and of intellectual education for improving human nature or behavior. For those who do not share this faith the idea of progress becomes questionable.

Alfred Marshall (1842–1924), a British economist of the early 20th century, was a proponent of classical liberalism. In his highly influential "Principles of Economics" (1890), he was deeply interested in human progress and in what is now called "sustainable development." For Marshall, the importance of wealth lay in its ability to promote the physical, mental, and moral health of the general population. After World War II, the modernization and development programs undertaken in the Third World were typically based on the idea of progress.

In Russia the notion of progress was first imported from the West by Peter the Great (1672–1725). An absolute ruler, he used the concept to modernize Russia and to legitimize his monarchy (unlike its usage in Western Europe, where it was primarily associated with political opposition). By the early 19th century, the notion of progress was being taken up by Russian intellectuals and was no longer accepted as legitimate by the tsars. Four schools of thought on progress emerged in 19th-century Russia: conservative (reactionary), religious, liberal, and socialist—the latter winning out in the form of Bolshevist materialism.

The intellectual leaders of the American Revolution, such as Benjamin Franklin, Thomas Paine, Thomas Jefferson and John Adams, were immersed in Enlightenment thought and believed the idea of progress meant that they could reorganize the political system to the benefit of the human condition; both for Americans and also, as Jefferson put it, for an "Empire of Liberty" that would benefit all mankind. In particular, Adams wrote “I must study politics and war, that our sons may have liberty to study mathematics and philosophy. Our sons ought to study mathematics and philosophy, geography, natural history and naval architecture, navigation, commerce and agriculture in order to give their children a right to study painting, poetry, music, architecture, statuary, tapestry and porcelain.”

Juan Bautista Alberdi (1810–1884) was one of the most influential political theorists in Argentina. Economic liberalism was the key to his idea of progress. He promoted faith in progress, while chiding fellow Latin Americans for blind copying of American and European models. He hoped for progress through promotion of immigration, education, and a moderate type of federalism and republicanism that might serve as a transition in Argentina to true democracy.

In Mexico, José María Luis Mora (1794–1850) was a leader of classical liberalism in the first generation after independence, leading the battle against the conservative trinity of the army, the church, and the "hacendados". He envisioned progress as both a process of human development by the search for philosophical truth and as the introduction of an era of material prosperity by technological advancement. His plan for Mexican reform demanded a republican government bolstered by widespread popular education free of clerical control, confiscation and sale of ecclesiastical lands as a means of redistributing income and clearing government debts, and effective control of a reduced military force by the government. Mora also demanded the establishment of legal equality between native Mexicans and foreign residents. His program, untried in his lifetime, became the key element in the Mexican Constitution of 1857.

In Italy, the idea that progress in science and technology would lead to solutions for human ills was connected to the nationalism that united the country in 1860. The Piedmontese Prime Minister Camillo Cavour envisaged the railways as a major factor in the modernization and unification of the Italian peninsula. The new Kingdom of Italy, formed in 1861, worked to speed up the processes of modernization and industrialization that had begun in the north, but were slow to arrive in the Papal States and central Italy, and were nowhere in sight in the "Mezzogiorno" (that is, Southern Italy, Sicily, and Sardinia). The government sought to combat the backwardness of the poorer regions in the south and work towards augmenting the size and quality of the newly created Italian army so that it could compete on an equal footing with the powerful nations of Europe. In the same period, the government was legislating in favour of public education to fight the great problem of illiteracy, upgrade the teaching classes, improve existing schools, and procure the funds needed for social hygiene and care of the body as factors in the physical and moral regeneration of the race.

In China, in the 20th century the Kuomintang or Nationalist party, which ruled from the 1920s to the 1940s, advocated progress. The Communists under Mao Zedong adopted western models and their ruinous projects caused mass famines. After Mao's death, however, the new regime led by Deng Xiaoping (1904–1997) and his successors aggressively promoted modernization of the economy using capitalist models and imported western technology. This was termed the "Opening of China" in the west, and more broadly encompasses Chinese economic reform.

Among environmentalists, there is a continuum between two opposing poles. The one pole is optimistic, progressive, and business-oriented, and endorses the classic idea of progress. For example, bright green environmentalism endorses the idea that new designs, social innovations and green technologies can solve critical environmental challenges. The other is pessimistic in respect of technological solutions, warning of impending global crisis (through climate change or peak oil, for example) and tends to reject the very idea of modernity and the myth of progress that is so central to modernization thinking. Similarly, Kirkpatrick Sale, wrote about progress as a myth benefiting the few, and a pending environmental doomsday for everyone. An example is the philosophy of Deep Ecology.

Sociologist Robert Nisbet said that "No single idea has been more important than ... the Idea of Progress in Western civilization for three thousand years", and defines five "crucial premises" of the idea of progress:

Sociologist P. A. Sorokin said, "The ancient Chinese, Babylonian, Hindu, Greek, Roman, and most of the medieval thinkers supporting theories of rhythmical, cyclical or trendless movements of social processes were much nearer to reality than the present proponents of the linear view". Unlike Confucianism and to a certain extent Taoism, that both search for an ideal past, the Judeo-Christian-Islamic tradition believes in the fulfillment of history, which was translated into the idea of progress in the modern age. Therefore, Chinese proponents of modernization have looked to western models. According to Thompson, the late Qing dynasty reformer, Kang Youwei, believed he had found a model for reform and "modernisation" in the Ancient Chinese Classics.

Philosopher Karl Popper said that progress was not fully adequate as a scientific explanation of social phenomena.
More recently, Kirkpatrick Sale, a self-proclaimed neo-luddite author, wrote exclusively about progress as a myth, in an essay entitled "Five Facets of a Myth".

Iggers (1965) says that proponents of progress underestimated the extent of man's destructiveness and irrationality, while critics misunderstand the role of rationality and morality in human behavior.

In 1946, psychoanalyst Charles Baudouin claimed modernity has retained the "corollary" of the progress myth, the idea that the present is superior to the past, while at the same time insisting that it is free of the myth:
A cyclical theory of history was adopted by Oswald Spengler (1880–1936), a German historian who wrote "The Decline of the West" in 1920. World War I, World War II, and the rise of totalitarianism demonstrated that progress was not automatic and that technological improvement did not necessarily guarantee democracy and moral advancement. British historian Arnold J. Toynbee (1889–1975) felt that Christianity would help modern civilization overcome its challenges.

The Jeffersonians said that history is not exhausted but that man may begin again in a new world. Besides rejecting the lessons of the past, they Americanized the idea of progress by democratizing and vulgarizing it to include the welfare of the common man as a form of republicanism. As Romantics deeply concerned with the past, collecting source materials and founding historical societies, the Founding Fathers were animated by clear principles. They saw man in control of his destiny, saw virtue as a distinguishing characteristic of a republic, and were concerned with happiness, progress, and prosperity. Thomas Paine, combining the spirit of rationalism and romanticism, pictured a time when America's innocence would sound like a romance, and concluded that the fall of America could mark the end of 'the noblest work of human wisdom.'

Historian J. B. Bury wrote in 1920:

In the postmodernist thought steadily gaining ground from the 1980s, the grandiose claims of the modernizers are steadily eroded, and the very concept of social progress is again questioned and scrutinized. In the new vision, radical modernizers like Joseph Stalin and Mao Zedong appear as totalitarian despots, whose vision of social progress is held to be totally deformed. Postmodernists question the validity of 19th-century and 20th-century notions of progress—both on the capitalist and the Marxist side of the spectrum. They argue that both capitalism and Marxism over-emphasize technological achievements and material prosperity while ignoring the value of inner happiness and peace of mind. Postmodernism posits that both dystopia and utopia are one and the same, overarching grand narratives with impossible conclusions.
Some 20th-century authors refer to the "Myth of Progress" to refer to the idea that the human condition will inevitably improve. In 1932, English physician Montague David Eder wrote: "The myth of progress states that civilization has moved, is moving, and will move in a desirable direction. Progress is inevitable... Philosophers, men of science and politicians have accepted the idea of the inevitability of progress." Eder argues that the advancement of civilization is leading to greater unhappiness and loss of control in the environment. The strongest critics of the idea of progress complain that it remains a dominant idea in the 21st century, and shows no sign of diminished influence. As one fierce critic, British historian John Gray (b. 1948), concludes:
Recently the idea of progress has been generalized to psychology, being related with the concept of a goal, that is, progress is understood as "what counts as a means of advancing towards the end result of a given defined goal."

Historian J. B. Bury said that thought in ancient Greece was dominated by the theory of world-cycles or the doctrine of eternal return, and was steeped in a belief parallel to the Judaic "fall of man," but rather from a preceding "Golden Age" of innocence and simplicity. Time was generally regarded as the enemy of humanity which depreciates the value of the world. He credits the Epicureans with having had a potential for leading to the foundation of a theory of progress through their materialistic acceptance of the atomism of Democritus as the explanation for a world without an intervening deity.
Robert Nisbet and Gertrude Himmelfarb have attributed a notion of progress to other Greeks. Xenophanes said "The gods did not reveal to men all things in the beginning, but men through their own search find in the course of time that which is better." Plato's Book III of "The Laws" depicts humanity's progress from a state of nature to the higher levels of culture, economy, and polity. Plato's "The Statesman" also outlines a historical account of the progress of mankind.

During the Medieval period, science was to a large extent based on Scholastic (a method of thinking and learning from the Middle Ages) interpretations of Aristotle's work. The Renaissance of the 15th, 16th and 17th Centuries changed the mindset in Europe towards an empirical view, based on a pantheistic interpretation of Plato. This induced a revolution in curiosity about nature in general and scientific advance, which opened the gates for technical and economic advance. Furthermore, the individual potential was seen as a never-ending quest for being God-like, paving the way for a view of Man based on unlimited perfection and progress.

In the Enlightenment, French historian and philosopher Voltaire (1694–1778) was a major proponent of progress. At first Voltaire's thought was informed by the idea of progress coupled with rationalism. His subsequent notion of the historical idea of progress saw science and reason as the driving forces behind societal advancement.

Immanuel Kant (1724–1804) argued that progress is neither automatic nor continuous and does not measure knowledge or wealth, but is a painful and largely inadvertent passage from barbarism through civilization toward enlightened culture and the abolition of war. Kant called for education, with the education of humankind seen as a slow process whereby world history propels mankind toward peace through war, international commerce, and enlightened self-interest.

Scottish theorist Adam Ferguson (1723–1816) defined human progress as the working out of a divine plan, though he rejected predestination. The difficulties and dangers of life provided the necessary stimuli for human development, while the uniquely human ability to evaluate led to ambition and the conscious striving for excellence. But he never adequately analyzed the competitive and aggressive consequences stemming from his emphasis on ambition even though he envisioned man's lot as a perpetual striving with no earthly culmination. Man found his happiness only in effort.

Some scholars consider the idea of progress that was affirmed with the Enlightenment, as a secularization of ideas from early Christianity, and a reworking of ideas from ancient Greece.

In the 19th century, Romantic critics charged that progress did not automatically better the human condition, and in some ways could make it worse. Thomas Malthus (1766–1834) reacted against the concept of progress as set forth by William Godwin and Condorcet because he believed that inequality of conditions is "the best (state) calculated to develop the energies and faculties of man". He said, "Had population and food increased in the same ratio, it is probable that man might never have emerged from the savage state". He argued that man's capacity for improvement has been demonstrated by the growth of his intellect, a form of progress which offsets the distresses engendered by the law of population.

German philosopher Friedrich Nietzsche (1844–1900) criticized the idea of progress as the 'weakling's doctrines of optimism,' and advocated undermining concepts such as faith in progress, to allow the strong individual to stand above the plebeian masses. An important part of his thinking consists of the attempt to use the classical model of 'eternal recurrence of the same' to dislodge the idea of progress.

Iggers (1965) argues there was general agreement in the late 19th century that the steady accumulation of knowledge and the progressive replacement of conjectural, that is, theological or metaphysical, notions by scientific ones was what created progress. Most scholars concluded this growth of scientific knowledge and methods led to the growth of industry and the transformation of warlike societies into industrial and pacific ones. They agreed as well that there had been a systematic decline of coercion in government, and an increasing role of liberty and of rule by consent. There was more emphasis on impersonal social and historical forces; progress was increasingly seen as the result of an inner logic of society.

Marx developed a theory of historical materialism. He describes the mid-19th-century condition in "The Communist Manifesto" as follows:

Furthermore, Marx described the process of social progress, which in his opinion is based on the interaction between the productive forces and the relations of production:

Capitalism is thought by Marx as a process of continual change, in which the growth of markets dissolve all fixities in human life, and Marx admits that capitalism is progressive and non-reactionary. Marxism further states that capitalism, in its quest for higher profits and new markets, will inevitably sow the seeds of its own destruction. Marxists believe that, in the future, capitalism will be replaced by socialism and eventually communism.

Many advocates of capitalism such as Schumpeter agreed with Marx's analysis of capitalism as a process of continual change through creative destruction, but, unlike Marx, believed and hoped that capitalism could essentially go on forever.

Thus, by the beginning of the 20th century, two opposing schools of thought—Marxism and liberalism—believed in the possibility and the desirability of continual change and improvement. Marxists strongly opposed capitalism and the liberals strongly supported it, but the one concept they could both agree on was progress, which affirms the power of human beings to make, improve and reshape their society, with the aid of scientific knowledge, technology and practical experimentation. "Modernity" denotes cultures that embrace that concept of progress. (This is not the same as modernism, which was the artistic and philosophical response to modernity, some of which embraced technology while rejecting individualism, but more of which rejected modernity entirely.)



</doc>
<doc id="1287236" url="https://en.wikipedia.org/wiki?curid=1287236" title="Innatism">
Innatism

Innatism is a philosophical and epistemological doctrine that holds that the mind is born with ideas/knowledge, and that therefore the mind is not a "blank slate" at birth, as early empiricists such as John Locke claimed. It asserts that not all knowledge is gained from experience and the senses. Plato and Descartes are prominent philosophers in the development of innatism and the notion that the mind is already born with ideas, knowledge and beliefs. Both philosophers emphasize that experiences are the key to unlocking this knowledge but not the source of the knowledge itself. Essentially, no knowledge is derived exclusively from one's experiences as empiricists like John Locke suggested.

In general usage, the terms "innatism" and "nativism" are synonymous as they both refer to notions of preexisting ideas present in the mind. However, more correctly, innatism refers to the philosophy of Plato and Descartes, who assumed that a God or a similar being or process placed innate ideas and principles in the human mind.

Nativism represents an adaptation of this, grounded in the fields of genetics, cognitive psychology, and psycholinguistics. Nativists hold that innate beliefs are in some way genetically programmed to arise in our mind—that innate beliefs are the phenotypes of certain genotypes that all humans share in common.

Nativism is a modern view rooted in innatism. The advocates of nativism are mainly philosophers who also work in the field of cognitive psychology or psycholinguistics: most notably Noam Chomsky and Jerry Fodor (although the latter has adopted a more critical attitude towards nativism in his later writings). The nativist's general objection against empiricism is still the same as was raised by the rationalists; the human mind of a newborn child is not a "tabula rasa", but equipped with an inborn structure.

In philosophy and psychology, an innate idea is a concept or item of knowledge which is said to be "universal to all humanity"—that is, something people are born with rather than something people have "learned" through experience.

The issue is controversial, and can be said to be an aspect of a long-running nature versus nurture debate, albeit one localized to the question of understanding human cognition.

Although individual human beings obviously vary due to cultural, racial, linguistic and era-specific influences, innate ideas are said to belong to a more fundamental level of human cognition. For example, the philosopher René Descartes theorized that knowledge of God is innate in everybody as a product of the faculty of faith.

Other philosophers, most notably the empiricists, were critical of the theory and denied the existence of any innate ideas, saying all human knowledge was founded on experience, rather than "a priori" reasoning.

Philosophically, the debate over innate ideas is central to the conflict between rationalist and empiricist epistemologies. While rationalists believe that certain ideas exist independently of experience, empiricism claims that all knowledge is derived from experience.

Immanuel Kant was a German philosopher who is regarded as having ended the impasse in modern philosophy between rationalists and empiricists, and is widely held to have synthesized these two early modern traditions in his thought.

Plato argues that if there are certain concepts that we know to be true but did not learn from experience then it must be because we have an innate knowledge of it and this knowledge must have been gained before birth. In Plato's "Meno", he recalls a situation in which Socrates, his mentor, questioned a slave boy about a geometry theorem. Though the slave boy had no previous experience with geometry, he was able to generate the right responses to the questions he was asked. Plato reasoned that this was possible as Socrates' questions sparked the innate knowledge of math the boy had had from birth.

Plato produced a tripartite to help explain what is contained within soul, which man is able to tap into for higher decision-making. Normally man is good but able to be confused and have the conscience become distorted with irrelevance’s or illogical reasoning. While Socrates believed no man does evil knowingly, Plato was skeptical. For we have to make conscious decisions with relation to other parts of our nature, that isn’t the same as reason. There are separations (which a person may have predominance of by nature or may rationally choose), each of course have a role to play and its up to our reason to rule, hence the title "Plato: The Rule of Reason". The tripartite may be categorized as:

"Individual material things are known by the senses, whereas forms are known by the intellect.". The forms have real independent existence.

Descartes conveys the idea that innate knowledge or ideas is something inborn such as one would say, that a certain disease might be 'innate' to signify that a person might be at risk of contracting such a disease. He suggests that something that is 'innate' is effectively present from birth and while it may not reveal itself then, is more than likely to present itself later in life. Descartes comparison of innate knowledge to an innate disease, whose symptoms may only show up later in life, unless prohibited by a factor like age or puberty, suggests that if an event occurs prohibiting someone from exhibiting an innate behaviour or knowledge, it doesn't mean the knowledge did not exist at all but rather it wasn't expressed – they were not able to acquire that knowledge. In other words, innate beliefs, ideas and knowledge require experiences to be triggered or they may never be expressed. Experiences are not the source of knowledge as proposed by John Locke, but catalysts to the uncovering of knowledge.

The main antagonist to the concept of innate ideas is John Locke, a contemporary of Leibniz. Locke argued that the mind is in fact devoid of all knowledge or ideas at birth; it is a blank sheet or "tabula rasa". He argued that all our ideas are constructed in the mind via a process of constant composition and decomposition of the input that we receive through our senses.

Locke, in "An Essay Concerning Human Understanding", suggests that the concept of universal assent in fact proves nothing, except perhaps that everyone is in agreement; in short universal assent proves that there is universal assent and nothing else. Moreover, Locke goes on to suggest that in fact there "is" no universal assent. Even a phrase such as "What is, is" is not universally assented to; infants and severely handicapped adults do not generally acknowledge this truism. Locke also attacks the idea that an innate idea can be imprinted on the mind without the owner realizing it. For Locke, such reasoning would allow one to conclude the absurd: “all the Truths a Man ever comes to know, will, by this account, be, every one of them, innate.” To return to the musical analogy, we may not be able to recall the entire melody until we hear the first few notes, but we were aware of the fact that we knew the melody and that upon hearing the first few notes we would be able to recall the rest.

Locke ends his attack upon innate ideas by suggesting that the mind is a "tabula rasa" or "blank slate", and that all ideas come from experience; all our knowledge is founded in sensory experience.

Essentially, the same knowledge thought to be "a priori" by Leibniz is in fact, according to Locke, the result of empirical knowledge, which has a lost origin [been forgotten] in respect to the inquirer. However, the inquirer is not cognizant of this fact; thus, he experiences what he believes to be "a priori" knowledge.

1) The theory of innate knowledge is excessive. Even innatists accept that most of our knowledge is learned through experience, but if that can be extended to account for all knowledge, we learn colour through seeing it, so therefore, there is no need for a theory about an innate understanding of colour.

2) No ideas are universally held. Do we all possess the idea of God? Do we all believe in justice and beauty? Do we all understand the law of identity? If not, it may not be the case that we have acquired these ideas through impressions/experience/social interaction (this is the children's and idiot's criticism).

3) Even if there are some universally agreed statements, it is just the ability of the human brain to organize learned ideas/words, that is, innate. An "ability to organize" is not the same as "possessing propositional knowledge" (e.g., a computer with no saved files has all the operations programmed in but has an empty memory).

Gottfried Wilhelm Leibniz suggested that we are born with certain innate ideas, the most identifiable of these being mathematical truisms. The idea that "1 + 1 = 2" is evident to us without the necessity for empirical evidence. Leibniz argues that empiricism can only show us that concepts are true in the present; the observation of one apple and then another in one instance, and in that instance only, leads to the conclusion that one and another equals two. However, the suggestion that one and another will always equal two require an innate idea, as that would be a suggestion of things unwitnessed.

Leibniz called such concepts as mathematical truisms "necessary truths". Another example of such may be the phrase, "what is, is" or "it is impossible for the same thing to be and not to be". Leibniz argues that such truisms are universally assented to (acknowledged by all to be true); this being the case, it must be due to their status as innate ideas. Often there are ideas that are acknowledged as necessarily true but are not universally assented to. Leibniz would suggest that this is simply because the person in question has not become aware of the innate idea, not because they do not possess it. Leibniz argues that empirical evidence can serve to bring to the surface certain principles that are already innately embedded in our minds. This is similar to needing to hear only the first few notes in order to recall the rest of the melody.

In his "Meno", Plato raises an important epistemological quandary: How is it that we have certain ideas which are not conclusively derivable from our environments? Noam Chomsky has taken this problem as a philosophical framework for the scientific enquiry into innatism. His linguistic theory, which derives from 18th century classical-liberal thinkers such as Wilhelm von Humboldt, attempts to explain in cognitive terms how we can develop knowledge of systems which are said, by supporters of innatism, to be too rich and complex to be derived from our environment. One such example is our linguistic faculty. Our linguistic systems contain a systemic complexity which supposedly could not be empirically derived: the environment seems too poor, variable and indeterminate, according to Chomsky, to explain the extraordinary ability to learn complex concepts possessed by very young children. Essentially, their accurate grammatical knowledge cannot have originated from their experiences as their experiences are not adequate. It follows that humans must be born with a universal innate grammar, which is determinate and has a highly organized directive component, and enables the language learner to ascertain and categorize language heard into a system. Chomsky states that the ability to learn how to properly construct sentences or know which sentences are grammatically incorrect is an ability gained from innate knowledge. Noam Chomsky cites as evidence for this theory, the apparent invariability, according to his views, of human languages at a fundamental level. In this way, linguistics may provide a window into the human mind, and establish scientific theories of innateness which otherwise would remain merely speculative.

One implication of Noam Chomsky's innatism, if correct, is that at least a part of human knowledge consists in cognitive predispositions, which are triggered and developed by the environment, but not determined by it. Chomsky suggests that we can look at how a belief is acquired as an input-output situation. He supports the doctrine of innatism as he states that human beliefs gathered from sensory experience are much richer and complex than the experience itself. He asserts that the extra information gathered is from the mind itself as it cannot solely be from experiences. Humans derive excess amount of information from their environment so some of that information must be pre-determined.

Parallels can then be drawn, on a purely speculative level, between our moral faculties and language, as has been done by sociobiologists such as E. O. Wilson and evolutionary psychologists such as Steven Pinker. The relative consistency of fundamental notions of morality across cultures seems to produce convincing evidence for these theories. In psychology, notions of archetypes such as those developed by Carl Jung, suggest determinate identity perceptions.

Evidence for innatism is being found by neuroscientists working on the Blue Brain Project. They discovered that neurons transmit signals despite an individual's experience. It had been previously assumed that neuronal circuits are made when the experience of an individual is imprinted in the brain, making memories. Researchers at Blue Brain discovered a network of about fifty neurons which they believed were building blocks of more complex knowledge but contained basic innate knowledge that could be combined in different more complex ways to give way to acquired knowledge, like memory.

Scientists ran tests on the neuronal circuits of several rats and ascertained that if the neuronal circuits had only been formed based on an individual's experience, the tests would bring about very different characteristics for each rat. However, the rats all displayed similar characteristics which suggests that their neuronal circuits must have been established previously to their experiences – it must be inborn and created prior to their experiences. The Blue Brain Project research suggests that some of the "building blocks" of knowledge are genetic and present at birth.

There are two ways in which animals can gain knowledge. The first of these two ways is learning. This is when an animal gathers information about its surrounding environment and then proceeds to use this information. For example, if an animal eats something that hurts its stomach, it has learned not to eat this again. The second way that an animal can acquire knowledge is through innate knowledge. This knowledge is genetically inherited. The animal automatically knows it without any prior experience. An example of this is when a horse is born and can immediately walk. The horse has not learned this behavior; it simply knows how to do it. In some scenarios, innate knowledge is more beneficial than learned knowledge. However, in other scenarios the opposite is true.

In a changing environment, an animal must constantly be gaining new information in order to survive. However, in a stable environment this same individual need only to gather the information it needs once and rely on it for the duration of its life. Therefore, there are different scenarios in which learning or innate knowledge is better suited.
Essentially, the cost of obtaining certain knowledge versus the benefit of having it determined whether an animal evolved to learn in a given situation or whether it innately knew the information. If the cost of gaining the knowledge outweighed the benefit of having it, then the individual would not have evolved to learn in this scenario; instead, non-learning would evolve. However, if the benefit of having certain information outweighed the cost of obtaining it, then the animal would be far more likely to evolve to have to learn this information.

Non-learning is more likely to evolve in two scenarios. If an environment is static and change does not or rarely occurs then learning would simply be unnecessary. Because there is no need for learning in this scenario – and because learning could prove to be disadvantageous due to the time it took to learn the information – non-learning evolves. However, if an environment were in a constant state of change then learning would also prove to be disadvantageous. Anything learned would immediately become irrelevant because of the changing environment. The learned information would no longer apply. Essentially, the animal would be just as successful if it took a guess as if it learned. In this situation, non-learning would evolve.

However, in environments where change occurs but is not constant, learning is more likely to evolve. Learning is beneficial in these scenarios because an animal can adapt to the new situation, but can still apply the knowledge that it learns for a somewhat extended period of time. Therefore, learning increases the chances of success as opposed to guessing and adapts to changes in the environment as opposed to innate knowledge.






</doc>
<doc id="1632214" url="https://en.wikipedia.org/wiki?curid=1632214" title="Great Conversation">
Great Conversation

The Great Conversation is the ongoing process of writers and thinkers referencing, building on, and refining the work of their predecessors. This process is characterized by writers in the Western canon making comparisons and allusions to the works of earlier writers. As such it is a name used in the promotion of the "Great Books of the Western World" published by Encyclopædia Britannica Inc. in 1952. It is also the title of (i) the first volume of the first edition of this set of books, written by the educational theorist Robert Maynard Hutchins, and (ii) an accessory volume to the second edition (1990), written by the philosopher Mortimer J. Adler. 

According to Hutchins, "The tradition of the West is embodied in the Great Conversation that began in the dawn of history and that continues to the present day". Adler said, What binds the authors together in an intellectual community is the great conversation in which they are engaged. In the works that come later in the sequence of years, we find authors listening to what their predecessors have had to say about this idea or that, this topic or that. They not only harken to the thought of their predecessors, they also respond to it by commenting on it in a variety of ways.




</doc>
<doc id="6978" url="https://en.wikipedia.org/wiki?curid=6978" title="Concept">
Concept

Concepts are defined as abstract ideas or general notions that occur in the mind, in speech, or in thought. They are understood to be the fundamental building blocks of thoughts and beliefs. They play an important role in all aspects of cognition. As such, concepts are studied by several disciplines, such as linguistics, psychology, and philosophy, and these disciplines are interested in the logical and psychological structure of concepts, and how they are put together to form thoughts and sentences. The study of concepts has served as an important flagship of an emerging interdisciplinary approach called cognitive science.

In contemporary philosophy, there are at least three prevailing ways to understand what a concept is:


Concepts can be organized into a hierarchy, higher levels of which are termed "superordinate" and lower levels termed "subordinate". Additionally, there is the "basic" or "middle" level at which people will most readily categorize a concept. For example, a basic-level concept would be "chair", with its superordinate, "furniture", and its subordinate, "easy chair".

A concept is instantiated (reified) by all of its actual or potential instances, whether these are things in the real world or other ideas.

Concepts are studied as components of human cognition in the cognitive science disciplines of linguistics, psychology and, philosophy, where an ongoing debate asks whether all cognition must occur through concepts. Concepts are used as formal tools or models in mathematics, computer science, databases and artificial intelligence where they are sometimes called classes, schema or categories. In informal use the word "concept" often just means any idea.

A central question in the study of concepts is the question of what they "are". Philosophers construe this question as one about the ontology of concepts—what they are like. The ontology of concepts determines the answer to other questions, such as how to integrate concepts into a wider theory of the mind, what functions are allowed or disallowed by a concept's ontology, etc. There are two main views of the ontology of concepts: (1) Concepts are abstract objects, and (2) concepts are mental representations.

Within the framework of the representational theory of mind, the structural position of concepts can be understood as follows: Concepts serve as the building blocks of what are called "mental representations" (colloquially understood as "ideas in the mind"). Mental representations, in turn, are the building blocks of what are called "propositional attitudes" (colloquially understood as the stances or perspectives we take towards ideas, be it "believing", "doubting", "wondering", "accepting", etc.). And these propositional attitudes, in turn, are the building blocks of our understanding of thoughts that populate everyday life, as well as folk psychology. In this way, we have an analysis that ties our common everyday understanding of thoughts down to the scientific and philosophical understanding of concepts.

In a physicalist theory of mind, a concept is a mental representation, which the brain uses to denote a class of things in the world. This is to say that it is literally, a symbol or group of symbols together made from the physical material of the brain. Concepts are mental representations that allow us to draw appropriate inferences about the type of entities we encounter in our everyday lives. Concepts do not encompass all mental representations, but are merely a subset of them. The use of concepts is necessary to cognitive processes such as categorization, memory, decision making, learning, and inference.

Concepts are thought to be stored in long term cortical memory, in contrast to episodic memory of the particular objects and events which they abstract, which are stored in hippocampus. Evidence for this separation comes from hippocampal damaged patients such as patient HM. The abstraction from the day's hippocampal events and objects into cortical concepts is often considered to be the computation underlying (some stages of) sleep and dreaming. Many people (beginning with Aristotle) report memories of dreams which appear to mix the day's events with analogous or related historical concepts and memories, and suggest that they were being sorted or organised into more abstract concepts. ("Sort" is itself another word for concept, and "sorting" thus means to organise into concepts.)

The semantic view of concepts suggests that concepts are abstract objects. In this view, concepts are abstract objects of a category out of a human's mind rather than some mental representations. 

There is debate as to the relationship between concepts and natural language. However, it is necessary at least to begin by understanding that the concept "dog" is philosophically distinct from the things in the world grouped by this concept—or the reference class or extension. Concepts that can be equated to a single word are called "lexical concepts".

Study of concepts and conceptual structure falls into the disciplines of linguistics, philosophy, psychology, and cognitive science.

In the simplest terms, a concept is a name or label that regards or treats an abstraction as if it had concrete or material existence, such as a person, a place, or a thing. It may represent a natural object that exists in the real world like a tree, an animal, a stone, etc. It may also name an artificial (man-made) object like a chair, computer, house, etc. Abstract ideas and knowledge domains such as freedom, equality, science, happiness, etc., are also symbolized by concepts. It is important to realize that a concept is merely a symbol, a representation of the abstraction. The word is not to be mistaken for the thing. For example, the word "moon" (a concept) is not the large, bright, shape-changing object up in the sky, but only "represents" that celestial object. Concepts are created (named) to describe, explain and capture reality as it is known and understood.

Kant maintained the view that human minds possess pure or "a priori" concepts. Instead of being abstracted from individual perceptions, like empirical concepts, they originate in the mind itself. He called these concepts categories, in the sense of the word that means predicate, attribute, characteristic, or quality. But these pure categories are predicates of things "in general", not of a particular thing. According to Kant, there are twelve categories that constitute the understanding of phenomenal objects. Each category is that one predicate which is common to multiple empirical concepts. In order to explain how an "a priori" concept can relate to individual phenomena, in a manner analogous to an "a posteriori" concept, Kant employed the technical concept of the schema. He held that the account of the concept as an abstraction of experience is only partly correct. He called those concepts that result from abstraction "a posteriori concepts" (meaning concepts that arise out of experience). An empirical or an "a posteriori" concept is a general representation ("Vorstellung") or non-specific thought of that which is common to several specific perceived objects (Logic, I, 1., §1, Note 1)

A concept is a common feature or characteristic. Kant investigated the way that empirical "a posteriori" concepts are created.
In cognitive linguistics, abstract concepts are transformations of concrete concepts derived from embodied experience. The mechanism of transformation is structural mapping, in which properties of two or more source domains are selectively mapped onto a blended space (Fauconnier & Turner, 1995; see conceptual blending). A common class of blends are metaphors. This theory contrasts with the rationalist view that concepts are perceptions (or "recollections", in Plato's term) of an independently existing world of ideas, in that it denies the existence of any such realm. It also contrasts with the empiricist view that concepts are abstract generalizations of individual experiences, because the contingent and bodily experience is preserved in a concept, and not abstracted away. While the perspective is compatible with Jamesian pragmatism, the notion of the transformation of embodied concepts through structural mapping makes a distinct contribution to the problem of concept formation.

Platonist views of the mind construe concepts as abstract objects.Plato was the starkest proponent of the realist thesis of universal concepts. By his view, concepts (and ideas in general) are innate ideas that were instantiations of a transcendental world of pure forms that lay behind the veil of the physical world. In this way, universals were explained as transcendent objects. Needless to say this form of realism was tied deeply with Plato's ontological projects. This remark on Plato is not of merely historical interest. For example, the view that numbers are Platonic objects was revived by Kurt Gödel as a result of certain puzzles that he took to arise from the phenomenological accounts.

Gottlob Frege, founder of the analytic tradition in philosophy, famously argued for the analysis of language in terms of sense and reference. For him, the sense of an expression in language describes a certain state of affairs in the world, namely, the way that some object is presented. Since many commentators view the notion of sense as identical to the notion of concept, and Frege regards senses as the linguistic representations of states of affairs in the world, it seems to follow that we may understand concepts as the manner in which we grasp the world. Accordingly, concepts (as senses) have an ontological status .

According to Carl Benjamin Boyer, in the introduction to his "The History of the Calculus and its Conceptual Development", concepts in calculus do not refer to perceptions. As long as the concepts are useful and mutually compatible, they are accepted on their own. For example, the concepts of the derivative and the integral are not considered to refer to spatial or temporal perceptions of the external world of experience. Neither are they related in any way to mysterious limits in which quantities are on the verge of nascence or evanescence, that is, coming into or going out of existence. The abstract concepts are now considered to be totally autonomous, even though they originated from the process of abstracting or taking away qualities from perceptions until only the common, essential attributes remained.

The classical theory of concepts, also referred to as the empiricist theory of concepts, is the oldest theory about the structure of concepts (it can be traced back to Aristotle), and was prominently held until the 1970s. The classical theory of concepts says that concepts have a definitional structure. Adequate definitions of the kind required by this theory usually take the form of a list of features. These features must have two important qualities to provide a comprehensive definition. Features entailed by the definition of a concept must be both "necessary" and "sufficient" for membership in the class of things covered by a particular concept. A feature is considered necessary if every member of the denoted class has that feature. A feature is considered sufficient if something has all the parts required by the definition. For example, the classic example "bachelor" is said to be defined by "unmarried" and "man". An entity is a bachelor (by this definition) if and only if it is both unmarried and a man. To check whether something is a member of the class, you compare its qualities to the features in the definition. Another key part of this theory is that it obeys the "law of the excluded middle", which means that there are no partial members of a class, you are either in or out.

The classical theory persisted for so long unquestioned because it seemed intuitively correct and has great explanatory power. It can explain how concepts would be acquired, how we use them to categorize and how we use the structure of a concept to determine its referent class. In fact, for many years it was one of the major activities in philosophy—concept analysis. Concept analysis is the act of trying to articulate the necessary and sufficient conditions for the membership in the referent class of a concept. For example, Shoemaker's classic "Time Without Change" explored whether the concept of the flow of time can include flows where no changes take place, though change is usually taken as a definition of time.

Given that most later theories of concepts were born out of the rejection of some or all of the classical theory, it seems appropriate to give an account of what might be wrong with this theory. In the 20th century, philosophers such as Wittgenstein and Rosch argued against the classical theory. There are six primary arguments summarized as follows:

Prototype theory came out of problems with the classical view of conceptual structure. Prototype theory says that concepts specify properties that members of a class tend to possess, rather than must possess. Wittgenstein, Rosch, Mervis, Berlin, Anglin, and Posner are a few of the key proponents and creators of this theory. Wittgenstein describes the relationship between members of a class as "family resemblances". There are not necessarily any necessary conditions for membership; a dog can still be a dog with only three legs. This view is particularly supported by psychological experimental evidence for prototypicality effects. Participants willingly and consistently rate objects in categories like 'vegetable' or 'furniture' as more or less typical of that class. It seems that our categories are fuzzy psychologically, and so this structure has explanatory power. We can judge an item's membership of the referent class of a concept by comparing it to the typical member—the most central member of the concept. If it is similar enough in the relevant ways, it will be cognitively admitted as a member of the relevant class of entities. Rosch suggests that every category is represented by a central exemplar which embodies all or the maximum possible number of features of a given category. Lech, Gunturkun, and Suchan explain that categorization involves many areas of the brain. Some of these are: visual association areas, prefrontal cortex, basal ganglia, and temporal lobe.

The Prototype perspective is proposed as an alternative view to the Classical approach. While the Classical theory requires an all-or-nothing membership in a group, prototypes allow for more fuzzy boundaries and are characterized by attributes. Lakeoff stresses that experience and cognition are critical to the function of language, and Labov's experiment found that the function that an artifact contributed to what people categorized it as. For example, a container holding mashed potatoes versus tea swayed people toward classifying them as a bowl and a cup, respectively. This experiment also illuminated the optimal dimensions of what the prototype for "cup" is.

Prototypes also deal with the essence of things and to what extent they belong to a category. There have been a number of experiments dealing with questionnaires asking participants to rate something according to the extent to which it belongs to a category. This question is contradictory to the Classical Theory because something is either a member of a category or is not. This type of problem is paralleled in other areas of linguistics such as phonology, with an illogical question such as "is /i/ or /o/ a better vowel?" The Classical approach and Aristotelian categories may be a better descriptor in some cases.

Theory-theory is a reaction to the previous two theories and develops them further. This theory postulates that categorization by concepts is something like scientific theorizing. Concepts are not learned in isolation, but rather are learned as a part of our experiences with the world around us. In this sense, concepts' structure relies on their relationships to other concepts as mandated by a particular mental theory about the state of the world. How this is supposed to work is a little less clear than in the previous two theories, but is still a prominent and notable theory. This is supposed to explain some of the issues of ignorance and error that come up in prototype and classical theories as concepts that are structured around each other seem to account for errors such as whale as a fish (this misconception came from an incorrect theory about what a whale is like, combining with our theory of what a fish is). When we learn that a whale is not a fish, we are recognizing that whales don't in fact fit the theory we had about what makes something a fish.Theory-theory also postulates that people's theories about the world are what inform their conceptual knowledge of the world. Therefore, analysing people's theories can offer insights into their concepts. In this sense, "theory" means an individual's mental explanation rather than scientific fact. This theory criticizes classical and prototype theory as relying too much on similarities and using them as a sufficient constraint. It suggests that theories or mental understandings contribute more to what has membership to a group rather than weighted similarities, and a cohesive category is formed more by what makes sense to the perceiver. Weights assigned to features have shown to fluctuate and vary depending on context and experimental task demonstrated by Tversky. For this reason, similarities between members may be collateral rather than causal.

According to the theory of ideasthesia (or "sensing concepts"), activation of a concept may be the main mechanism responsible for creation of phenomenal experiences. Therefore, understanding how the brain processes concepts may be central to solving the mystery of how conscious experiences (or qualia) emerge within a physical system e.g., the sourness of the sour taste of lemon. This question is also known as the hard problem of consciousness. Research on ideasthesia emerged from research on synesthesia where it was noted that a synesthetic experience requires first an activation of a concept of the inducer. Later research expanded these results into everyday perception.

There is a lot of discussion on the most effective theory in concepts. Another theory is semantic pointers, which use perceptual and motor representations and these representations are like symbols.

The term "concept" is traced back to 1554–60 (Latin "" – "something conceived").



</doc>
<doc id="12825560" url="https://en.wikipedia.org/wiki?curid=12825560" title="Choiceless awareness">
Choiceless awareness

Choiceless awareness is a major topic in the exposition of Indian philosopher Jiddu Krishnamurti (18951986). Beginning in the 1930s, he often commented on the subject, which became a recurring theme in his work. He is considered to have been mainly responsible for the subsequent interest in both the term and the concept.

Krishnamurti held that outside of strictly practical, technical matters, the presence and action of choice indicates confusion and subtle bias: an individual who perceives a given situation in an unbiased manner, without distortion, and therefore with complete awareness, will immediately, naturally, act according to this awarenessthe action will be the manifestation and result of this awareness, rather than the result of choice. Such action (and quality of mind) is inherently without conflict.

He did not offer any method to achieve such awareness; in his view application of technique cannot possibly evolve into, or result in, true choicelessnessjust as unceasing application of effort leads to illusory effortlessness, in reality the action of habit. Additionally, in his opinion all methods introduce potential or actual conflict, generated by the practitioner's efforts to comply. According to this analysis, all practices towards achieving choiceless awareness have the opposite effect: they inhibit its action in the present by treating it as a future, premeditated result, and moreover one that is conditioned by the practitioner's implied or expressed expectations. 

Krishnamurti stated that for true choicelessness to be realized, choiceimplicit or explicithas to simply, irrevocably, stop; however, this ceasing of choice is not the result of decision-making, but implies the ceasing of the functioning of the chooser or self as a psychological entity. He proposed that such a state might be approached through inquiry based on total attentiveness: identity is then dissolved in complete, all-encompassing attention. Therefore, he asserted that choiceless awareness is a natural attribute of non-self-centered perception, which he called "observation without the observer".

Accordingly, Krishnamurti advised against following any doctrine, discipline, teacher, guru, or authority, including himself. He also advised against following one's own psychological knowledge and experience, which he considered integral parts of the observer. He denied the usefulness of all meditation techniques and methods, but not of meditation itself, which he called "perhaps the greatest" art in life; and stated that insight into choiceless awareness could be shared through open dialogue.

Krishnamurti's ideas on choiceless awareness were discussed by among others, influential Hindu spiritual teacher Ramana Maharshi (18791950) and, following wide publication of his books, they attracted the attention of psychologists and psychoanalysts in the 1950s; in subsequent decades Krishnamurti held a number of discussions on this and related subjects with practicing psychotherapists and with researchers in the field. His views on the subject have been included in scholarly papers on existential therapy, education theory, and peace research, but they have also been discussed in less formal or structured settings.

In late1980, almost half a century after he started discussing it, Krishnamurti included the concept in "The Core of Krishnamurti's Teaching", a pivotal statement of his philosophy: "Freedom is found in the choiceless awareness of our daily existence and activity.

In contrast with Krishnamurti's approach, other articulations commonly include choiceless awareness (or related ideas and terms) as part, or as the hoped-for result, of specific methodologies and meditation techniques. Similar concepts and terms appeared or developed in various traditional and contemporary religious or spiritual doctrines and texts, and also within secular disciplines such as psychotherapy, rehabilitation medicine, and counseling. Choiceless awareness has been examined within the context of philosophy of perception and behavior, while studies have cited its possible role in job performance. Other studies have linked meditation based on the concept (among others), with neural activity consistent with increased attentiveness, considered a factor of well-being and happiness.

One term that is often used as a near-synonym is mindfulness, which as a concept has similarities to or may include choiceless awareness. Initially part of Buddhist meditation practice, it has been adapted and utilized for contemporary psychological treatment, and has been applied as a component of integrative medicine programs.

Kindred themes can be found in the doctrine and meditation practices (such as Vipassanā) associated with the Theravada school of Buddhism; and also in 20th-century offshoots such as the Thai Forest Tradition and the Vipassana movement. Within these and similar fields, for example the Shikantaza practice in Zen Buddhism, choiceless (or effortless) awareness is considered to frequently be the result of a mature progression of practice.

The concept has been included in the discourse of transpersonal philosopher Ken Wilber (b. 1949), and also of independent Indian spiritual teacher Osho (Rajneesh) (19311990). Tibetan Buddhism teacher Chögyam Trungpa (19391987), who engaged in dialogue with Krishnamurti, used the term to describe the experience of shunyata (Śūnyatā)in Sanskrit, "emptiness", or "ego-less perception".

Among other fields, the term has appeared in dispute resolution theory and practice, and has found application in artistic endeavors. In dramatic theory, theater criticism, and acting, it has been used to denote spontaneous creativity and related practices or attempts; it has additionally appeared in music works. Author J. D. Salinger (19192010), who was interested in spirituality and alternative religions, was reputedly an adherent of Ramana Maharshi's ideas on choiceless awareness.

Contrary to press reports published in mid-20th-century, later interest in practices related to, or influenced by, choiceless awareness, has resulted in unambiguously favorable mentions in the popular press. Additionally, mass market general interest titles covering the subject have been published.








</doc>
<doc id="10188326" url="https://en.wikipedia.org/wiki?curid=10188326" title="B-theory of time">
B-theory of time

The B-theory of time is the name given to one of two positions regarding philosophy of time. B-theorists argue that the flow of time is an illusion, that the past, present, and future are equally real, and that time is tenseless. This would mean that temporal becoming is not an objective feature of reality.

B-theory is often drawn upon in theoretical physics, and in theories such as eternalism.

The labels, A-theory and B-theory, first coined by Richard Gale in 1966, are derived from the analysis of time and change developed by Cambridge philosopher J. M. E. McTaggart in "The Unreality of Time" (1908), in which events are ordered via a tensed A-series or a tenseless B-series. It is popularly assumed that the A theory represents time like an A-series, while the B theory represents time like a B-series. The terms A and B theory are sometimes used as synonyms to the terms presentism and eternalism, but arguably presentism does not represent time being like an A-series since it denies that there is a future and past in which events can be located.

Events (or "times"), McTaggart observed, may be characterized in two distinct, but related, ways. On the one hand they can be characterized as past, present or future, normally indicated in natural languages such as English by the verbal inflection of tenses or auxiliary adverbial modifiers. Alternatively events may be described as earlier than, simultaneous with, or later than others. Philosophers are divided as to whether the tensed or tenseless mode of expressing temporal fact is fundamental. Some philosophers have criticised hybrid theories, where one holds a tenseless view of time, but asserts that the present has special properties, as falling foul of McTaggart's paradox. For a thorough discussion of McTaggart's paradox, see R. D. Ingthorsson (2016).

The debate between A-theorists and B-theorists is a continuation of a metaphysical dispute reaching back to the ancient Greek philosophers Heraclitus and Parmenides. Parmenides thought that reality is timeless and unchanging. Heraclitus, in contrast, believed that the world is a process of ceaseless change or flux. Reality for Heraclitus is dynamic and ephemeral. Indeed, the world is so fleeting, according to Heraclitus, that it is impossible to step twice into the same river. The metaphysical issues that continue to divide A-theorists and B-theorists concern the reality of the past, the reality of the future, and the ontological status of the present.

The difference between A-theorists and B-theorists is often described as a dispute about temporal passage or 'becoming' and 'progressing'. B-theorists argue that this notion is purely psychological. Many A-theorists argue that in rejecting temporal 'becoming', B-theorists reject time's most vital and distinctive characteristic. It is common (though not universal) to identify A-theorists' views with belief in temporal passage. Another way to characterise the distinction revolves around what is known as the "principle of temporal parity", the thesis that contrary to what appears to be the case, all times really exist in parity. The A-theory (and especially presentism) denies that all times exist in parity, while the B-theory insists all times exist in parity.

B-theorists such as D. H. Mellor and J. J. C. Smart wish to eliminate all talk of past, present and future in favour of a tenseless ordering of events, believing the past, present, and future to be equally real, opposing the idea that they are irreducible foundations of temporality. B-theorists also argue that the past, present, and future feature very differently in deliberation and reflection. For example, we remember the past and anticipate the future, but not vice versa. 
B-theorists maintain that the fact that we know much less about the future simply reflects an epistemological difference between the future and the past: the future is no less real than the past; we just know less about it.

The B-theory of time has received support from the physics community. This is likely due to its compatibility with physics and the fact that many theories such as special relativity, the ADD model, and brane cosmology, point to a theory of time similar to B-theory.

In special relativity, the relativity of simultaneity shows that there is no unique present, and that each point in the universe can have a different set of events that are in its present moment.

Many of special relativity's now-proven counter-intuitive predictions, such as length contraction and time dilation, are a result of this.
Relativity of simultaneity is often taken to imply eternalism (and hence a B-theory of time), where the present for different observers is a time slice of the four dimensional universe. This is demonstrated in the Rietdijk–Putnam argument and additionally in an advanced form of this argument called the Andromeda paradox, created by mathematical physicist Roger Penrose.

It is therefore common (though not universal), for B-theorists to be four-dimensionalists, that is, to believe that objects are extended in time as well as in space and therefore have temporal as well as spatial parts. This is sometimes called a time-slice ontology.

In 'Presentism and the Space-Time Manifold', Dean Zimmerman notes that A-theory is 'almost certainly a minority view among philosophers', while B-theory has 'achieved broad acceptance'; despite this there are still a number of philosophers who maintain opposition for B-theory.

Earlier B-theorists argued that one could paraphrase tensed sentences (such as "the sun is now shining") into tenseless sentences (such as "on September 28, the sun shines") without loss of meaning. Later B-theorists argued that tenseless sentences could give the truth conditions of tensed sentences or their tokens. Quentin Smith states that "now" cannot be reduced to descriptions of dates and times, because all date and time descriptions, and therefore truth conditionals, are relative to certain events. Tensed sentences, on the other hand, do not have such truth conditionals. The B-theorist could argue that "now" is reducible to a token-reflexive phrase such as "simultaneous with this utterance," yet Smith states that even such an argument fails to eliminate tense. One can think the statement "I am not uttering anything now," and such a statement would be true. The statement "I am not uttering anything simultaneous with this utterance" is self-contradictory, and cannot be true even when one thinks the statement. Finally, while tensed statements can express token-independent truth values, no token-reflexive statement can do so (by definition of the term "token-reflexive"). Quentin Smith claims that current proponents of the B-theory argue that the inability to translate tensed sentences into tenseless sentences does not prove the A-theory of time.

Noted logician and philosopher Arthur Prior (originator of tense logic) has also drawn a distinction between what he calls A-facts and B-facts. The latter are facts about tenseless relations, such as the fact that the year 2025 is 25 years later than the year 2000. The former are tensed facts, such as the Jurassic age being in the past, or the end of the universe being in the future. Prior asks the reader to imagine having a headache, and after the headache subsides, saying "thank goodness that's over." Prior argues that the B-theory cannot make sense of this sentence. It seems bizarre to be thankful that a headache is earlier than one's utterance, anymore than being thankful that the headache is later than one's utterance. Indeed, most people who say "thank goodness that's over" are not even thinking of their own utterance. Therefore, when people say "thank goodness that's over," they are thankful for an A-fact, and not a B-fact. Yet, A-facts are only possible on the A-theory of time. (See also: Further facts.)

Opponents also charge the B-theory with being unable to explain persistence of objects. The two leading explanations for this phenomenon are endurantism and perdurantism. The former states that an object is wholly present at every moment of its existence. The latter states that objects are extended in time and therefore have temporal parts. Hales and Johnson explain endurantism as follows: "something is an enduring object only if it is wholly present at each time in which it exists. An object is wholly present at a time if all of its parts co-exist at that time." Under endurantism, all objects must exist as wholes at each point in time. However, an object such as a rotting fruit will have the property of being not rotten one day and being rotten on another. On eternalism, and hence the B-theory, it seems that one is committed to two conflicting states for the same object. The spacetime (Minkowskian) interpretation of relativity adds an additional problem for endurantism under the B-theory. On the spacetime interpretation, an object may appear as a whole at its rest frame. On an inertial frame, however, that same object will have proper parts at different positions, and therefore will have different parts at different times. Hence, it will not exist as a whole at any point in time, contradicting the thesis of endurantism.

Opponents will then charge perdurantism with having numerous difficulties of its own. First, it is controversial whether perdurantism can be formulated coherently. An object is defined as a collection of spatio-temporal parts, which are defined as pieces of a perduring object. If objects have temporal parts, this leads to difficulties. For example, the rotating discs argument asks the reader to imagine a world containing nothing more than a homogeneous spinning disk. Under endurantism, the same disc endures despite that it is rotating. The perdurantist supposedly has a difficult time explaining what it means for such a disk to have a determinate state of rotation. Temporal parts also seem to act unlike physical parts. A piece of chalk can be broken into two physical halves, but it seems nonsensical to talk about breaking it into two temporal halves. Chisholm argued that someone who hears the bird call "Bob White" knows "that his experience of hearing 'Bob' and his experience of hearing 'White' were not also had by two other things, each distinct from himself and from each other. The endurantist can explain the experience as "There exists an x such that x hears 'Bob' and then x hears 'White'" but the perdurantist cannot give such an account. Peter van Inwagen asks the reader to consider Descartes as a four-dimensional object that extends from 1596–1650. If Descartes had lived a much shorter life, he would have had a radically different set of temporal parts. This diminished Descartes, he argues, could not have been the same person on perdurantism, since their temporal extents and parts are so different.



</doc>
<doc id="21402758" url="https://en.wikipedia.org/wiki?curid=21402758" title="Qualia">
Qualia

In philosophy and certain models of psychology, qualia ( or ; singular form: quale) are defined as individual instances of subjective, conscious experience. The term "qualia" derives from the Latin neuter plural form ("qualia") of the Latin adjective "quālis" () meaning "of what sort" or "of what kind" in a specific instance, such as "what it is like to taste a specific apple, this particular apple now".

Examples of qualia include the perceived sensation of "pain" of a headache, the "taste" of wine, as well as the "redness" of an evening sky. As qualitative characters of sensation, qualia stand in contrast to "propositional attitudes", where the focus is on beliefs about experience rather than what it is directly like to be experiencing.

Philosopher and cognitive scientist Daniel Dennett once suggested that "qualia" was "an unfamiliar term for something that could not be more familiar to each of us: the ways things seem to us".

Much of the debate over their importance hinges on the definition of the term, and various philosophers emphasize or deny the existence of certain features of qualia. Consequently, the nature and existence of various definitions of qualia remain controversial because they are not verifiable.

There are many definitions of qualia, which have changed over time. One of the simpler, broader definitions is: "The 'what it is like' character of mental states. The way it feels to have mental states such as pain, seeing red, smelling a rose, etc."

Charles Sanders Peirce introduced the term quale in philosophy in 1866. 

Clarence Irving Lewis, in his book "Mind and the World Order" (1929), was the first to use the term "qualia" in its generally agreed upon modern sense.
Frank Jackson later defined qualia as "...certain features of the bodily sensations especially, but also of certain perceptual experiences, which no amount of purely physical information includes". 

Daniel Dennett identifies four properties that are commonly ascribed to qualia. According to these, qualia are:

If qualia of this sort exist, then a normally sighted person who sees red would be unable to describe the experience of this perception in such a way that a listener who has never experienced color will be able to know everything there is to know about that experience. Though it is possible to make an analogy, such as "red looks hot", or to provide a description of the conditions under which the experience occurs, such as "it's the color you see when light of 700-nm wavelength is directed at you", supporters of this kind of qualia contend that such a description is incapable of providing a complete description of the experience.

Another way of defining qualia is as "raw feels". A "raw feel" is a perception in and of itself, considered entirely in isolation from any effect it might have on behavior and behavioral disposition. In contrast, a "cooked feel" is that perception seen as existing in terms of its effects. For example, the perception of the taste of wine is an ineffable, raw feel, while the experience of warmth or bitterness caused by that taste of wine would be a cooked feel. Cooked feels are not qualia.

According to an argument put forth by Saul Kripke in his paper "Identity and Necessity" (1971), one key consequence of the claim that such things as raw feels can be meaningfully discussed—that qualia exist—is that it leads to the logical possibility of two entities exhibiting identical behavior in all ways despite one of them entirely lacking qualia. While very few ever claim that such an entity, called a philosophical zombie, actually exists, the mere possibility is claimed to be sufficient to refute physicalism.

Arguably, the idea of hedonistic utilitarianism, where the ethical value of things is determined from the amount of subjective pleasure or pain they cause, is dependent on the existence of qualia.

Since it is by definition impossible to convey qualia verbally, it is also impossible to demonstrate them directly in an argument; so a more tangential approach is needed. Arguments for qualia generally come in the form of thought experiments designed to lead one to the conclusion that qualia exist.

Although it does not actually mention the word "qualia", Thomas Nagel's paper "What Is it Like to Be a Bat?" is often cited in debates over qualia. Nagel argues that consciousness has an essentially subjective character, a what-it-is-like aspect. He states that "an organism has conscious mental states if and only if there is something that it is like to "be" that organism—something it is like "for" the organism." Nagel also suggests that the subjective aspect of the mind may not ever be sufficiently accounted for by the objective methods of reductionistic science. He claims that "if we acknowledge that a physical theory of mind must account for the subjective character of experience, we must admit that no presently available conception gives us a clue about how this could be done." Furthermore, he states that "it seems unlikely that any physical theory of mind can be contemplated until more thought has been given to the general problem of subjective and objective."

The inverted spectrum thought experiment, originally developed by John Locke, invites us to imagine that we wake up one morning and find that for some unknown reason all the colors in the world have been inverted, i.e. swapped to the hue on the opposite side of a color wheel. Furthermore, we discover that no physical changes have occurred in our brains or bodies that would explain this phenomenon. Supporters of the existence of qualia argue that since we can imagine this happening without contradiction, it follows that we are imagining a change in a property that determines the way things look to us, but that has no physical basis. In more detail:

The argument thus claims that if we find the inverted spectrum plausible, we must admit that qualia exist (and are non-physical). Some philosophers find it absurd that an armchair argument can prove something to exist, and the detailed argument does involve a lot of assumptions about conceivability and possibility, which are open to criticism. Perhaps it is not possible for a given brain state to produce anything other than a given quale in our universe, and that is all that matters.

The idea that an inverted spectrum would be undetectable in practice is also open to criticism on more scientific grounds (see main article). There is an actual experiment—albeit somewhat obscure—that parallels the inverted spectrum argument. George M. Stratton, professor of psychology at the University of California, Berkeley, performed an experiment in which he wore special prism glasses that caused the external world to appear upside down. After a few days of continually wearing the glasses, an adaptation occurred and the external world appeared righted. When the glasses were removed, the external world again appeared inverted. After a similar period, perception of the external world returned to the "normal" perceptual state. If this argument provides indicia that qualia exist, it does not necessarily follow that they must be non-physical, because that distinction should be considered a separate epistemological issue.

A similar argument holds that it is conceivable (or not inconceivable) that there could be physical duplicates of people, called "philosophical zombies", without any qualia at all. These "zombies" would demonstrate outward behavior precisely similar to that of a normal human, but would not have a subjective phenomenology. It is worth noting that a necessary condition for the possibility of philosophical zombies is that there be no specific part or parts of the brain that directly give rise to qualia—the zombie can only exist if subjective consciousness is causally separate from the physical brain.

"Are zombies possible? They're not just possible, they're actual. We're all zombies. Nobody is conscious." - Daniel Dennett ("Consciousness Explained,1991")

Joseph Levine's paper "Conceivability, Identity, and the Explanatory Gap" takes up where the criticisms of conceivability arguments, such as the inverted spectrum argument and the zombie argument, leave off. Levine agrees that conceivability is flawed as a means of establishing metaphysical realities, but points out that even if we come to the "metaphysical" conclusion that qualia are physical, there is still an "explanatory" problem.
However, such an epistemological or explanatory problem might indicate an underlying metaphysical issue—the non-physicality of qualia, even if not proven by conceivability arguments is far from ruled out.

Frank Jackson offers what he calls the "knowledge argument" for qualia. One example runs as follows:
This thought experiment has two purposes. First, it is intended to show that qualia exist. If one agrees with the thought experiment, we believe that Mary gains something after she leaves the room—that she acquires knowledge of a particular thing that she did not possess before. That knowledge, Jackson argues, is knowledge of the quale that corresponds to the experience of seeing red, and it must thus be conceded that qualia are real properties, since there is a difference between a person who has access to a particular quale and one who does not.

The second purpose of this argument is to refute the physicalist account of the mind. Specifically, the knowledge argument is an attack on the physicalist claim about the completeness of physical truths. The challenge posed to physicalism by the knowledge argument runs as follows:

First Jackson argued that qualia are epiphenomenal: not causally efficacious with respect to the physical world. Jackson does not give a positive justification for this claim—rather, he seems to assert it simply because it defends qualia against the classic problem of dualism. Our natural assumption would be that qualia must be causally efficacious in the physical world, but some would ask how we could argue for their existence if they did not affect our brains. If qualia are to be non-physical properties (which they must be in order to constitute an argument against physicalism), some argue that it is almost impossible to imagine how they could have a causal effect on the physical world. By redefining qualia as epiphenomenal, Jackson attempts to protect them from the demand of playing a causal role.

Later, however, he rejected epiphenomenalism. This, he argues, is because when Mary first sees red, she says "wow", so it must be Mary's qualia that cause her to say "wow". This contradicts epiphenomenalism. Since the Mary's room thought experiment seems to create this contradiction, there must be something wrong with it. This is often referred to as the "there must be a reply" reply.

In "Consciousness Explained" (1991) and "Quining Qualia" (1988), Daniel Dennett offers an argument against qualia by claiming that the above definition breaks down when one tries to make a practical application of it. In a series of thought experiments, which he calls "intuition pumps", he brings qualia into the world of neurosurgery, clinical psychology, and psychological experimentation. His argument states that, once the concept of qualia is so imported, it turns out that we can either make no use of it in the situation in question, or that the questions posed by the introduction of qualia are unanswerable precisely because of the special properties defined for qualia.

In Dennett's updated version of the inverted spectrum thought experiment, "alternative neurosurgery", you again awake to find that your qualia have been inverted—grass appears red, the sky appears orange, etc. According to the original account, you should be immediately aware that something has gone horribly wrong. Dennett argues, however, that it is impossible to know whether the diabolical neurosurgeons have indeed inverted your qualia (by tampering with your optic nerve, say), or have simply inverted your connection to memories of past qualia. Since both operations would produce the same result, you would have no means on your own to tell which operation has actually been conducted, and you are thus in the odd position of not knowing whether there has been a change in your "immediately apprehensible" qualia.

Dennett's argument revolves around the central objection that, for qualia to be taken seriously as a component of experience—for them to even make sense as a discrete concept—it must be possible to show that

Dennett attempts to show that we cannot satisfy (a) either through introspection or through observation, and that qualia's very definition undermines its chances of satisfying (b).

Supporters of qualia could point out that in order for you to notice a change in qualia, you must compare your current qualia with your memories of past qualia. Arguably, such a comparison would involve immediate apprehension of your current qualia "and" your memories of past qualia, but not the past qualia "themselves". Furthermore, modern functional brain imaging has increasingly suggested that the memory of an experience is processed in similar ways and in similar zones of the brain as those originally involved in the original perception. This may mean that there would be asymmetry in outcomes between altering the mechanism of perception of qualia and altering their memories. If the diabolical neurosurgery altered the immediate perception of qualia, you might not even notice the inversion directly, since the brain zones which re-process the memories would themselves invert the qualia remembered. On the other hand, alteration of the qualia memories themselves would be processed without inversion, and thus you would perceive them as an inversion. Thus, you might know immediately if memory of your qualia had been altered, but might not know if immediate qualia were inverted or whether the diabolical neurosurgeons had done a sham procedure.

Dennett also has a response to the "Mary the color scientist" thought experiment. He argues that Mary would not, in fact, learn something new if she stepped out of her black and white room to see the color red. Dennett asserts that if she already truly knew "everything about color", that knowledge would include a deep understanding of why and how human neurology causes us to sense the "quale" of color. Mary would therefore already know exactly what to expect of seeing red, before ever leaving the room. Dennett argues that the misleading aspect of the story is that Mary is supposed to not merely be knowledgeable about color but to actually know "all" the physical facts about it, which would be a knowledge so deep that it exceeds what can be imagined, and twists our intuitions.

If Mary really does know everything physical there is to know about the experience of color, then this effectively grants her almost omniscient powers of knowledge. Using this, she will be able to deduce her own reaction, and figure out exactly what the experience of seeing red will feel like.

Dennett finds that many people find it difficult to see this, so he uses the case of RoboMary to further illustrate what it would be like for Mary to possess such a vast knowledge of the physical workings of the human brain and color vision. RoboMary is an intelligent robot who, instead of the ordinary color camera-eyes, has a software lock such that she is only able to perceive black and white and shades in-between.

RoboMary can examine the computer brain of similar non-color-locked robots when they look at a red tomato, and see exactly how they react and what kinds of impulses occur. RoboMary can also construct a simulation of her own brain, unlock the simulation's color-lock and, with reference to the other robots, simulate exactly how this simulation of herself reacts to seeing a red tomato. RoboMary naturally has control over all of her internal states except for the color-lock. With the knowledge of her simulation's internal states upon seeing a red tomato, RoboMary can put her own internal states directly into the states they would be in upon seeing a red tomato. In this way, without ever seeing a red tomato through her cameras, she will know exactly what it is like to see a red tomato.

Dennett uses this example to show us that Mary's all-encompassing physical knowledge makes her own internal states as transparent as those of a robot or computer, and it is almost straightforward for her to figure out exactly how it feels to see red.

Perhaps Mary's failure to learn exactly what seeing red feels like is simply a failure of language, or a failure of our ability to describe experiences. An alien race with a different method of communication or description might be perfectly able to teach their version of Mary exactly how seeing the color red would feel. Perhaps it is simply a uniquely human failing to communicate first-person experiences from a third-person perspective. Dennett suggests that the description might even be possible using English. He uses a simpler version of the Mary thought experiment to show how this might work. What if Mary was in a room without triangles and was prevented from seeing or making any triangles? An English-language description of just a few words would be sufficient for her to imagine what it is like to see a triangle—she can simply and directly visualize a triangle in her mind. Similarly, Dennett proposes, it is perfectly, logically possible that the quale of what it is like to see red could eventually be described in an English-language description of millions or billions of words.

In "Are we explaining consciousness yet?" (2001), Dennett approves of an account of qualia defined as the deep, rich collection of individual neural responses that are too fine-grained for language to capture. For instance, a person might have an alarming reaction to yellow because of a yellow car that hit her previously, and someone else might have a nostalgic reaction to a comfort food. These effects are too individual-specific to be captured by English words. "If one dubs this inevitable residue "qualia", then qualia are guaranteed to exist, but they are just more of the same, dispositional properties that have not yet been entered in the catalog [...]."

According to Paul Churchland, Mary might be considered to be like a feral child. Feral children have suffered extreme isolation during childhood. Technically when Mary leaves the room, she would not have the ability to see or know what the color red is. A brain has to learn and develop how to see colors. Patterns need to form in the V4 section of the visual cortex. These patterns are formed from exposure to wavelengths of light. This exposure is needed during the early stages of brain development. In Mary's case, the identifications and categorizations of color will only be in respect to representations of black and white.

In his book "Good and Real" (2006), Gary Drescher compares qualia with "gensyms" (generated symbols) in Common Lisp. These are objects that Lisp treats as having no properties or components and which can only be identified as equal or not equal to other objects. Drescher explains, "we have no introspective access to whatever internal properties make the "red" gensym recognizably distinct from the "green" [...] even though we know the sensation when we experience it." Under this interpretation of qualia, Drescher responds to the Mary thought experiment by noting that "knowing about red-related cognitive structures and the dispositions they engender—even if that knowledge were implausibly detailed and exhaustive—would not necessarily give someone who lacks prior color-experience the slightest clue whether the card now being shown is of the color called red." This does not, however, imply that our experience of red is non-mechanical; "on the contrary, gensyms are a routine feature of computer-programming languages".

David Lewis has an argument that introduces a new hypothesis about types of knowledge and their transmission in qualia cases. Lewis agrees that Mary cannot learn what red looks like through her monochrome physicalist studies. But he proposes that this doesn't matter. Learning transmits information, but experiencing qualia doesn't transmit information; instead it communicates abilities. When Mary sees red, she doesn't get any new information. She gains new abilities—now she can remember what red looks like, imagine what other red things might look like and recognize further instances of redness. Lewis states that Jackson's thought experiment uses the "Phenomenal Information Hypothesis"—that is, the new knowledge that Mary gains upon seeing red is phenomenal information. Lewis then proposes a different "Ability Hypothesis" that differentiates between two types of knowledge: knowledge that (information) and knowledge how (abilities). Normally the two are entangled; ordinary learning is also an experience of the subject concerned, and people both learn information (for instance, that Freud was a psychologist) and gain ability (to recognize images of Freud). However, in the thought experiment, Mary can only use ordinary learning to gain know-that knowledge. She is prevented from using experience to gain the know-how knowledge that would allow her to remember, imagine and recognize the color red.

We have the intuition that Mary has been deprived of some vital data to do with the experience of redness. It is also uncontroversial that some things cannot be learned inside the room; for example, we do not expect Mary to learn how to ski within the room. Lewis has articulated that information and ability are potentially different things. In this way, physicalism is still compatible with the conclusion that Mary gains new knowledge. It is also useful for considering other instances of qualia; "being a bat" is an ability, so it is know-how knowledge.

The artificial intelligence researcher Marvin Minsky thinks the problems posed by qualia are essentially issues of complexity, or rather of mistaking complexity for simplicity.

Michael Tye holds the opinion there are no qualia, no "veils of perception" between us and the referents of our thought. He describes our experience of an object in the world as "transparent". By this he means that no matter what private understandings and/or misunderstandings we may have of some public entity, it is still there before us in reality. The idea that qualia intervene between ourselves and their origins he regards as "a massive error"; as he says, "it is just not credible that visual experiences are systematically misleading in this way"; "the only objects of which you are aware are the external ones making up the scene before your eyes"; there are "no such things as the qualities of experiences" for "they are qualities of external surfaces (and volumes and films) if they are qualities of anything." This insistence permits him to take our experience as having a reliable base since there is no fear of losing contact with the realness of public objects.

In Tye's thought there is no question of qualia without information being contained within them; it is always "an awareness that", always "representational". He characterizes the perception of children as a misperception of referents that are undoubtedly as present for them as they are for grown-ups. As he puts it, they may not know that "the house is dilapidated", but there is no doubt about their seeing the house. After-images are dismissed as presenting no problem for the Transparency Theory because, as he puts it, after-images being illusory, there is nothing that one sees.

Tye proposes that phenomenal experience has five basic elements, for which he has coined the acronym PANIC—Poised, Abstract, Nonconceptual, Intentional Content. It is "Poised" in the sense that the phenomenal experience is always presented to the understanding, whether or not the agent is able to apply a concept to it. Tye adds that the experience is "maplike" in that, in most cases, it reaches through to the distribution of shapes, edges, volumes, etc. in the world—you may not be reading the "map" but, as with an actual map there is a reliable match with what it is mapping. It is "Abstract" because it is still an open question in a particular case whether you are in touch with a concrete object (someone may feel a pain in a "left leg" when that leg has actually been amputated). It is "Nonconceptual" because a phenomenon can exist although one does not have the concept by which to recognize it. Nevertheless, it is "Intentional" in the sense that it represents something, again whether or not the particular observer is taking advantage of that fact; this is why Tye calls his theory "representationalism". This last makes it plain that Tye believes that he has retained a direct contact with what produces the phenomena and is therefore not hampered by any trace of a "veil of perception".

Roger Scruton, whilst sceptical of the idea that neurobiology can tell us a great amount about consciousness, is of the opinion that the idea of qualia is incoherent, and that Wittgenstein's famous private language argument effectively disproves it. Scruton writes, The belief that these essentially private features of mental states exist, and that they form the introspectible essence of whatever possesses them, is grounded in a confusion, one that Wittgenstein tried to sweep away in his arguments against the possibility of a private language. When you judge that I am in pain, it is on the basis of my circumstances and behavior, and you could be wrong. When I ascribe a pain to myself, I don’t use any such evidence. I don’t find out that I am in pain by observation, nor can I be wrong. But that is not because there is some other fact about my pain, accessible only to me, which I consult in order to establish what I am feeling. For if there were this inner private quality, I could misperceive it; I could get it wrong, and I would have to find out whether I am in pain. To describe my inner state, I would also have to invent a language, intelligible only to me – and that, Wittgenstein plausibly argues, is impossible. The conclusion to draw is that I ascribe pain to myself not on the basis of some inner quale but on no basis at all.
In his book "On Human Nature", Scruton does pose a potential line of criticism to this, which is that whilst Wittgenstein's private language argument does disprove the concept of reference to qualia, or the idea that we can talk even to ourselves of their nature, it does not disprove its existence altogether. Scruton believes that this is a valid criticism, and this is why he stops short of actually saying that quales don't exist, and instead merely suggests that we should abandon them as a concept. However, he does give a quote by Wittgenstein as a response: "Whereof one cannot speak, thereof one must be silent."

David Chalmers formulated the hard problem of consciousness, raising the issue of qualia to a new level of importance and acceptance in the field. In his paper "Absent Qualia, Fading Qualia, Dancing Qualia", he also argued for what he called "the principle of organizational invariance". In this paper, he argues that if a system such as one of appropriately configured computer chips reproduces the functional organization of the brain, it will also reproduce the qualia associated with the brain.

E. J. Lowe, of Durham University, denies that holding to indirect realism (in which we have access only to sensory features internal to the brain) necessarily implies a Cartesian dualism. He agrees with Bertrand Russell that our "retinal images"—that is, the distributions across our retinas—are connected to "patterns of neural activity in the cortex" (Lowe 1986). He defends a version of the Causal Theory of Perception in which a causal path can be traced between the external object and the perception of it. He is careful to deny that we do any inferring from the sensory field, a view which he believes allows us to found an access to knowledge on that causal connection. In a later work he moves closer to the non-epistemic theory in that he postulates "a wholly non-conceptual component of perceptual experience", but he refrains from analyzing the relation between the perceptual and the "non-conceptual". Most recently he has drawn attention to the problems that hallucination raises for the direct realist and to their disinclination to enter the discussion on the topic.

John Barry Maund, an Australian philosopher of perception at the University of Western Australia, draws attention to a key distinction of qualia. Qualia are open to being described on two levels, a fact that he refers to as "dual coding". Using the Television Analogy (which, as the non-epistemic argument shows, can be shorn of its objectionable aspects), he points out that, if asked what we see on a television screen there are two answers that we might give:

The states of the screen during a football match are unquestionably different from those of the screen during a chess game, but there is no way available to us of describing the ways in which they are different except by reference to the play, moves and pieces in each game.

He has refined the explanation by shifting to the example of a "Movitype" screen, often used for advertisements and announcements in public places. A Movitype screen consists of a matrix—or "raster" as the neuroscientists prefer to call it (from the Latin "rastrum", a "rake"; think of the lines on a TV screen as "raked" across)—that is made up of an array of tiny light-sources. A computer-led input can excite these lights so as to give the impression of letters passing from right to left, or even, on the more advanced forms now commonly used in advertisements, to show moving pictures. Maund's point is as follows. It is obvious that there are two ways of describing what you are seeing. We could either adopt the everyday public language and say "I saw some sentences, followed by a picture of a 7-Up can." Although that is a perfectly adequate way of describing the sight, nevertheless, there is a scientific way of describing it which bears no relation whatsoever to this commonsense description. One could ask the electronics engineer to provide us with a computer print-out staged across the seconds that you were watching it of the point-states of the raster of lights. This would no doubt be a long and complex document, with the state of each tiny light-source given its place in the sequence. The interesting aspect of this list is that, although it would give a comprehensive and point-by-point-detailed description of the state of the screen, nowhere in that list would there be a mention of "English sentences" or "a 7-Up can".

What this makes clear is that there are two ways to describe such a screen, (1) the "commonsense" one, in which publicly recognizable objects are mentioned, and (2) an accurate point-by-point account of the actual state of the field, but makes no mention of what any passer-by would or would not make of it. This second description would be non-epistemic from the common sense point of view, since no objects are mentioned in the print-out, but perfectly acceptable from the engineer's point of view. Note that, if one carries this analysis across to human sensing and perceiving, this rules out Daniel Dennett's claim that all qualiaphiles must regard qualia as "ineffable", for at this second level they are in principle quite "effable"—indeed, it is not ruled out that some neurophysiologist of the future might be able to describe the neural detail of qualia at this level.

Maund has also extended his argument particularly with reference of color. Color he sees as a dispositional property, not an objective one, an approach which allows for the facts of difference between person and person, and also leaves aside the claim that external objects are colored. Colors are therefore "virtual properties", in that it is as if things possessed them; although the naïve view attributes them to objects, they are intrinsic, non-relational inner experiences.

In his book "Sensing the World", Moreland Perkins argues that qualia need not be identified with their objective sources: a smell, for instance, bears no direct resemblance to the molecular shape that gives rise to it, nor is a toothache actually in the tooth. He is also like Hobbes in being able to view the process of sensing as being something complete in itself; as he puts it, it is not like "kicking a football" where an external object is required—it is more like "kicking a kick", an explanation which entirely avoids the familiar Homunculus Objection, as adhered to, for example, by Gilbert Ryle. Ryle was quite unable even to entertain this possibility, protesting that "in effect it explained the having of sensations as the not having of sensations." However, A.J. Ayer in a rejoinder identified this objection as "very weak" as it betrayed an inability to detach the notion of eyes, indeed any sensory organ, from the neural sensory experience.

Vilayanur S. Ramachandran and William Hirstein proposed three laws of qualia (with a fourth later added), which are "functional criteria that need to be fulfilled in order for certain neural events to be associated with qualia" by philosophers of the mind:
They proposed that the phenomenal nature of qualia could be communicated (as in "oh "that" is what salt tastes like") if brains could be appropriately connected with a "cable of neurons". If this turned out to be possible this would scientifically prove or objectively demonstrate the existence and the nature of qualia.

Howard Robinson is a philosopher who has concentrated his research within the philosophy of mind. Taking what has been through the latter part of the last century an unfashionable stance, he has consistently argued against those explanations of sensory experience that would reduce them to physical origins. He has never regarded the theory of sense-data as refuted, but has set out to refute in turn the objections which so many have considered to be conclusive. The version of the theory of sense-data he defends takes what is before consciousness in perception to be qualia as mental presentations that are causally linked to external entities, but which are not physical in themselves. Unlike the philosophers so far mentioned, he is therefore a dualist, one who takes both matter and mind to have real and metaphysically distinct natures. In one of his most recent articles he takes the physicalist to task for ignoring the fact that sensory experience can be entirely free of representational character. He cites phosphenes as a stubborn example (phosphenes are flashes of neural light that result either from sudden pressure in the brain—as induced, for example, by intense coughing, or through direct physical pressure on the retina), and points out that it is grossly counter-intuitive to argue that these are not visual experiences on a par with open-eye seeing.

William Robinson (no relation) takes a very similar view to that of his namesake. In his most recent book, "Understanding Phenomenal Consciousness", he is unusual as a dualist in calling for research programs that investigate the relation of qualia to the brain. The problem is so stubborn, he says, that too many philosophers would prefer "to explain it away", but he would rather have it explained and does not see why the effort should not be made. However, he does not expect there to be a straightforward scientific reduction of phenomenal experience to neural architecture; on the contrary he regards this as a forlorn hope. The "Qualitative Event Realism" that Robinson espouses sees phenomenal consciousness as caused by brain events but not identical with them, being non-material events.

It is noteworthy that he refuses to set aside the vividness—and commonness—of mental images, both visual and aural, standing here in direct opposition to Daniel Dennett, who has difficulty in crediting the experience in others. He is similar to Moreland Perkins in keeping his investigation wide enough to apply to all the senses.

Edmond Wright is a philosopher who considers the intersubjective aspect of perception. From Locke onwards it had been normal to frame perception problems in terms of a single subject S looking at a single entity E with a property p. However, if we begin with the facts of the differences in sensory registration from person to person, coupled with the differences in the criteria we have learned for distinguishing what we together call "the same" things, then a problem arises of how two persons align their differences on these two levels so that they can still get a practical overlap on parts of the real about them—and, in particular, update each other about them.

Wright mentions being struck with the hearing difference between himself and his son, discovering that his son could hear sounds up to nearly 20 kilohertz while his range only reached to 14 kHz or so. This implies that a difference in qualia could emerge in human action (for example, the son could warn the father of a high-pitched escape of a dangerous gas kept under pressure, the sound-waves of which would be producing no qualia evidence at all for the father). The relevance for language thus becomes critical, for an informative statement can best be understood as an updating of a perception—and this may involve a radical re-selection from the qualia fields viewed as non-epistemic, even perhaps of the presumed singularity of "the" referent, a fortiori if that "referent" is the self. Here he distinguishes his view from that of Revonsuo, who too readily makes his "virtual space" "egocentric".

Wright's particular emphasis has been on what he asserts is a core feature of communication, that, in order for an updating to be set up and made possible, both speaker and hearer have to behave as if they have identified "the same singular thing", which, he notes, partakes of the structure of a joke or a story. Wright says that this systematic ambiguity seems to opponents of qualia to be a sign of fallacy in the argument (as ambiguity is in pure logic) whereas, on the contrary, it is sign—in talk about "what" is perceived—of something those speaking to each other have to learn to take advantage of. In extending this analysis, he has been led to argue for an important feature of human communication being the degree and character of the faith maintained by the participants in the dialogue, a faith that has priority over what has before been taken to be the key virtues of language, such as "sincerity", "truth", and "objectivity". Indeed, he considers that to prioritize them over faith is to move into superstition.

Erwin Schrödinger, a theoretical physicist and one of the leading pioneers of quantum mechanics, also published in the areas of colorimetry and color perception. In several of his philosophical writings, he defends the notion that qualia are not physical.
He continues on to remark that subjective experiences do not form a one-to-one correspondence with stimuli. For example, light of wavelength in the neighborhood of 590 nm produces the sensation of yellow, whereas exactly the same sensation is produced by mixing red light, with wavelength 760 nm, with green light, at 535 nm. From this he concludes that there is no "numerical connection with these physical, objective characteristics of the waves" and the sensations they produce.

Schrödinger concludes with a proposal of how it is that we might arrive at the mistaken belief that a satisfactory theoretical account of qualitative experience has been—or might ever be—achieved:

When looked at philosophically, qualia become a tipping point between physicality and the metaphysical, which polarizes the discussion, as we've seen above, into "Do they or do they not exist?" and "Are they physical or beyond the physical?" However, from a strictly neurological perspective, they can both exist, and be very important to the organism's survival, and be the result of strict neuronal oscillation, and still not rule out the metaphysical. A good example of this pro/con blending is in Rodolfo Llinás's "I of the Vortex" (MIT Press, 2002, pp. 202–207). Llinás argues that qualia are ancient and necessary for an organism's survival "and" a product of neuronal oscillation. Llinás gives the evidence of anesthesia of the brain and subsequent stimulation of limbs to demonstrate that qualia can be "turned off" with changing only the variable of neuronal oscillation (local brain electrical activity), while all other connections remain intact, arguing strongly for an oscillatory—electrical origin of qualia, or important aspects of them.

Roger Orpwood, an engineer with a strong background in studying neural mechanisms, proposed a neurobiological model that gives rise to qualia and ultimately, consciousness. As advancements in cognitive and computational neuroscience continue to grow, the need to study the mind, and qualia, from a scientific perspective follows. Orpwood does not deny the existence of qualia, nor does he intend to debate its physical or non-physical existence. Rather, he suggests that qualia are created through the neurobiological mechanism of re-entrant feedback in cortical systems 

Orpwood develops his mechanism by first addressing the issue of information. One unsolved aspect of qualia is the concept of the fundamental information involved in creating the experience. He does not address a position on the metaphysics of the information underlying the experience of qualia, nor does he state what information actually is. However, Orpwood does suggest that information in general is of two types: the information structure and information message. Information structures are defined by the physical vehicles and structural, biological patterns encoding information. That encoded information is the information message; a source describing "what" that information is. The neural mechanism or network receives input information structures, completes a designated instructional task (firing of the neuron or network), and outputs a modified information structure to downstream regions. The information message is the purpose and meaning of the information structure and causally exists as a result of that particular information structure. Modification of the information structure changes the meaning of the information message, but the message itself cannot be directly altered.

Local cortical networks have the capacity to receive feedback from their own output information structures. This form of local feedback continuously cycles part of the networks output structures as its next input information structure. Since the output structure must represent the information message derived from the input structure, each consecutive cycle that is fed-back will represent the output structure the network just generated. As the network of mechanisms cannot recognize the information message, but only the input information structure, the network is unaware that it is representing its own previous outputs. The neural mechanisms are merely completing their instructional tasks and outputting any recognizable information structures. Orpwood proposes that these local networks come into an attractor state that consistently outputs exactly the same information structure as the input structure. Instead of only representing the information message derived from the input structure, the network will now represent its own output and thereby its own information message. As the input structures are fed-back, the network identifies the previous information structure as being a previous representation of the information message. As Orpwood states, 

Representation of the networks own output structures, by which represents its own information message, is Orpwood's explanation that grounds the manifestation of qualia via neurobiological mechanisms. These mechanisms are particular to networks of pyramidal neurons. Although computational neuroscience still has much to investigate regarding pyramidal neurons, their complex circuitry is relatively unique. Research shows that the complexity of pyramidal neuron networks is directly related to the increase in the functional capabilities of a species. When human pyramidal networks are compared with other primate species and species with less intricate behavioral and social interactions, the complexity of these neural networks drastically decline. The complexity of these networks are also increased in frontal brain regions. These regions are often associated with conscious assessment and modification of one's immediate environment; often referred to as "executive functions". Sensory input is necessary to gain information from the environment, and perception of that input is necessary for navigating and modifying interactions with the environment. This suggests that frontal regions containing more complex pyramidal networks are associated with an increased perceptive capacity. As perception is necessary for conscious thought to occur, and since the experience of qualia is derived from consciously recognizing some perception, qualia may indeed be specific to the functional capacity of pyramidal networks. This derives Orpwood's notion that the mechanisms of re-entrant feedback may not only create qualia, but also be the foundation to consciousness.

It is possible to apply a criticism similar to Nietzsche's criticism of Kant's "thing in itself" to qualia: Qualia are unobservable in others and unquantifiable in us. We cannot possibly be sure, when discussing individual qualia, that we are even discussing the same phenomena. Thus, any discussion of them is of indeterminate value, as descriptions of qualia are necessarily of indeterminate accuracy. Qualia can be compared to "things in themselves" in that they have no publicly demonstrable properties; this, along with the impossibility of being sure that we are communicating about the same qualia, makes them of indeterminate value and definition in any philosophy in which proof relies upon precise definition. On the other hand, qualia could be considered akin to Kantian phenomena since they are held to be seemings of appearances. Revonsuo, however, considers that, within neurophysiological inquiry, a definition at the level of the fields may become possible (just as we can define a television picture at the level of liquid crystal pixels).

Whether or not qualia or consciousness can play any causal role in the physical world remains an open question, with epiphenomenalism acknowledging the existence of qualia while denying it any causal power. The position has been criticized by a number of philosophers, if only because our own consciousness seem to be causally active. In order to avoid epiphenomenalism, one who believes that qualia are nonphysical would need to embrace something like interactionist dualism; or perhaps emergentism, the claim that there are as yet unknown causal relations between the mental and physical. This in turn would imply that qualia can be detected by an external agency through their causal powers.

To illustrate: one might be tempted to give as examples of qualia "the pain of a headache, the taste of wine, or the redness of an evening sky". But this list of examples already prejudges a central issue in the current debate on qualia. An analogy might make this clearer. Suppose someone wants to know the nature of the liquid crystal pixels on a television screen, those tiny elements that provide all the distributions of color that go to make up the picture. It would not suffice as an answer to say that they are the "redness of an evening sky" as it appears on the screen. We would protest that their real character was being ignored. One can see that relying on the list above assumes that we must tie sensations not only to the notion of given objects in the world (the "head", "wine", "an evening sky"), but also to the properties with which we characterize the experiences themselves ("redness", for example).

Nor is it satisfactory to print a little red square as at the top of the article, for, since each person has a slightly different registration of the light-rays, it confusingly suggests that we all have the same response. Imagine in a television shop seeing "a red square" on twenty screens at once, each slightly different—something of vital importance would be overlooked if a single example were to be taken as defining them all.

Yet it has been argued whether or not identification with the external object should still be the core of a correct approach to sensation, for there are many who state the definition thus because they regard the link with external reality as crucial. If sensations are defined as "raw feels", there arises a palpable threat to the reliability of knowledge. The reason has been given that, if one sees them as neurophysiological happenings in the brain, it is difficult to understand how they could have any connection to entities, whether in the body or the external world. It has been declared, by John McDowell for example, that to countenance qualia as a "bare presence" prevents us ever gaining a certain ground for our knowledge. The issue is thus fundamentally an epistemological one: it would appear that access to knowledge is blocked if one allows the existence of qualia as fields in which only virtual constructs are before the mind.

His reason is that it puts the entities about which we require knowledge behind a "veil of perception", an occult field of "appearance" which leaves us ignorant of the reality presumed to be beyond it. He is convinced that such uncertainty propels into the dangerous regions of relativism and solipsism: relativism sees all truth as determined by the single observer; solipsism, in which the single observer is the only creator of and legislator for his or her own universe, carries the assumption that no one else exists. These accusations constitute a powerful ethical argument against qualia being something going on in the brain, and these implications are probably largely responsible for the fact that in the 20th century it was regarded as not only freakish, but also dangerously misguided to uphold the notion of sensations as going on inside the head. The argument was usually strengthened with mockery at the very idea of "redness" being in the brain: the question was—and still is—"How can there be red neurons in the brain?" which strikes one as a justifiable appeal to common sense.

To maintain a philosophical balance, the argument for "raw feels" needs to be set side by side with the claim above. Viewing sensations as "raw feels" implies that initially they have not yet—to carry on the metaphor—been "cooked", that is, unified into "things" and "persons", which is something the mind does after the sensation has responded to the blank input, that response driven by motivation, that is, initially by pain and pleasure, and subsequently, when memories have been implanted, by desire and fear. Such a "raw-feel" state has been more formally identified as "non-epistemic". In support of this view, the theorists cite a range of empirical facts. The following can be taken as representative. There are brain-damaged persons, known as "agnosics" (literally "not-knowing") who still have vivid visual sensations but are quite unable to identify any entity before them, including parts of their own body. There is also the similar predicament of persons, formerly blind, who are given sight for the first time—and consider what it is a newborn baby must experience. A German psychologist of the 19th century, Hermann von Helmholtz, proposed a simple experiment to demonstrate the non-epistemic nature of qualia: his instructions were to stand in front of a familiar landscape, turn your back on it, bend down and look at the landscape between your legs—you will find it difficult in the upside-down view to recognize what you found familiar before.

These examples suggest that a "bare presence"—that is, knowledgeless sensation that is no more than evidence—may really occur. Present supporters of the non-epistemic theory thus regard sensations as only data in the sense that they are "given" (Latin "datum", "given") and fundamentally involuntary, which is a good reason for not regarding them as basically mental. In the last century they were called "sense-data" by the proponents of qualia, but this led to the confusion that they carried with them reliable proofs of objective causal origins. For instance, one supporter of qualia was happy to speak of the redness and bulginess of a cricket ball as a typical "sense-datum", though not all of them were happy to define qualia by their relation to external entities (see Roy Wood Sellars). The modern argument, following Sellars' lead, centers on how we learn under the regime of motivation to interpret the sensory evidence in terms of "things", "persons", and "selves" through a continuing process of feedback.

The definition of qualia thus is governed by one's point of view, and that inevitably brings with it philosophical and neurophysiological presuppositions. The question, therefore, of what qualia can be raises profound issues in the philosophy of mind, since some materialists want to deny their existence altogether: on the other hand, if they are accepted, they cannot be easily accounted for as they raise the difficult problem of consciousness. There are committed dualists such as Richard L. Amoroso or John Hagelin who believe that the mental and the material are two distinct aspects of physical reality like the distinction between the classical and quantum regimes. In contrast, there are direct realists for whom the thought of qualia is unscientific as there appears to be no way of making them fit within the modern scientific picture; and there are committed proselytizers for a final truth who reject them as forcing knowledge out of reach.




</doc>
<doc id="19312" url="https://en.wikipedia.org/wiki?curid=19312" title="Meme">
Meme

A meme ( ) is an idea, behavior, or style that spreads by means of imitation from person to person within a culture—often with the aim of conveying a particular phenomenon, theme, or meaning represented by the meme. A meme acts as a unit for carrying cultural ideas, symbols, or practices, that can be transmitted from one mind to another through writing, speech, gestures, rituals, or other imitable phenomena with a mimicked theme. Supporters of the concept regard memes as cultural analogues to genes in that they self-replicate, mutate, and respond to selective pressures.

Proponents theorize that memes are a viral phenomenon that may evolve by natural selection in a manner analogous to that of biological evolution. Memes do this through the processes of variation, mutation, competition, and inheritance, each of which influences a meme's reproductive success. Memes spread through the behavior that they generate in their hosts. Memes that propagate less prolifically may become extinct, while others may survive, spread, and (for better or for worse) mutate. Memes that replicate most effectively enjoy more success, and some may replicate effectively even when they prove to be detrimental to the welfare of their hosts.

A field of study called memetics arose in the 1990s to explore the concepts and transmission of memes in terms of an evolutionary model. Criticism from a variety of fronts has challenged the notion that academic study can examine memes empirically. However, developments in neuroimaging may make empirical study possible. Some commentators in the social sciences question the idea that one can meaningfully categorize culture in terms of discrete units, and are especially critical of the biological nature of the theory's underpinnings. Others have argued that this use of the term is the result of a misunderstanding of the original proposal.

The word "meme" is a neologism coined by Richard Dawkins. It originated from Dawkins's 1976 book "The Selfish Gene". Dawkins's own position is somewhat ambiguous: he welcomed N. K. Humphrey's suggestion that "memes should be considered as living structures, not just metaphorically" and proposed to regard memes as "physically residing in the brain". Later, he argued that his original intentions, presumably before his approval of Humphrey's opinion, had been simpler.

The word "meme" is a shortening (modeled on "gene") of "mimeme" (from Ancient Greek "mīmēma", "imitated thing", from "mimeisthai", "to imitate", from "mimos", "mime") coined by British evolutionary biologist Richard Dawkins in "The Selfish Gene" (1976) as a concept for discussion of evolutionary principles in explaining the spread of ideas and cultural phenomena. Examples of memes given in the book included melodies, catchphrases, fashion, and the technology of building arches. Kenneth Pike had in 1954 coined the related terms "emic" and "etic", generalizing the linguistic units of phoneme, morpheme, grapheme, lexeme, and tagmeme (as set out by Leonard Bloomfield), distinguishing insider and outside views of communicative behavior.

The word "meme" originated with Richard Dawkins' 1976 book "The Selfish Gene". Dawkins cites as inspiration the work of geneticist L. L. Cavalli-Sforza, anthropologist F. T. Cloak and ethologist J. M. Cullen. Dawkins wrote that evolution depended not on the particular chemical basis of genetics, but only on the existence of a self-replicating unit of transmission—in the case of biological evolution, the gene. For Dawkins, the meme exemplified another self-replicating unit with potential significance in explaining human behavior and cultural evolution. Although Dawkins invented the term 'meme' and developed meme theory, the possibility that ideas were subject to the same pressures of evolution as were biological attributes was discussed in Darwin's time. T. H. Huxley claimed that 'The struggle for existence holds as much in the intellectual as in the physical world. A theory is a species of thinking, and its right to exist is coextensive with its power of resisting extinction by its rivals.'
Dawkins used the term to refer to any cultural entity that an observer might consider a replicator. He hypothesized that one could view many cultural entities as replicators, and pointed to melodies, fashions and learned skills as examples. Memes generally replicate through exposure to humans, who have evolved as efficient copiers of information and behavior. Because humans do not always copy memes perfectly, and because they may refine, combine or otherwise modify them with other memes to create new memes, they can change over time. Dawkins likened the process by which memes survive and change through the evolution of culture to the natural selection of genes in biological evolution.

Dawkins defined the "meme" as a unit of cultural transmission, or a unit of imitation and replication, but later definitions would vary. The lack of a consistent, rigorous, and precise understanding of what typically makes up one unit of cultural transmission remains a problem in debates about memetics. In contrast, the concept of genetics gained concrete evidence with the discovery of the biological functions of DNA. Meme transmission requires a physical medium, such as photons, sound waves, touch, taste, or smell because memes can be transmitted only through the senses.

Dawkins noted that in a society with culture a person need not have descendants to remain influential in the actions of individuals thousands of years after their death:

But if you contribute to the world's culture, if you have a good idea...it may live on, intact, long after your genes have dissolved in the common pool. Socrates may or may not have a gene or two alive in the world today, as G.C. Williams has remarked, but who cares? The meme-complexes of Socrates, Leonardo, Copernicus and Marconi are still going strong.

Although Dawkins invented the term "meme", he has not claimed that the idea was entirely novel, and there have been other expressions for similar ideas in the past. In 1904, Richard Semon published "Die Mneme" (which appeared in English in 1924 as "The Mneme"). The term "mneme" was also used in Maurice Maeterlinck's "The Life of the White Ant" (1926), with some parallels to Dawkins's concept.

Memes, analogously to genes, vary in their aptitude to replicate; successful memes remain and spread, whereas unfit ones stall and are forgotten. Thus memes that prove more effective at replicating and surviving are selected in the meme pool.

Memes first need retention. The longer a meme stays in its hosts, the higher its chances of propagation are. When a host uses a meme, the meme's life is extended. The reuse of the neural space hosting a certain meme's copy to host different memes is the greatest threat to that meme's copy.

A meme which increases the longevity of its hosts will generally survive longer. On the contrary, a meme which shortens the longevity of its hosts will tend to disappear faster. However, as hosts are mortal, retention is not sufficient to perpetuate a meme in the long term; memes also need transmission.

Life-forms can transmit information both vertically (from parent to child, via replication of genes) and horizontally (through viruses and other means).
Memes can replicate vertically or horizontally within a single biological generation. They may also lie dormant for long periods of time.

Memes reproduce by copying from a nervous system to another one, either by communication or imitation. Imitation often involves the copying of an observed behavior of another individual. Communication may be direct or indirect, where memes transmit from one individual to another through a copy recorded in an inanimate source, such as a book or a musical score. Adam McNamara has suggested that memes can be thereby classified as either internal or external memes (i-memes or e-memes).

Some commentators have likened the transmission of memes to the spread of contagions. Social contagions such as fads, hysteria, copycat crime, and copycat suicide exemplify memes seen as the contagious imitation of ideas. Observers distinguish the contagious imitation of memes from instinctively contagious phenomena such as yawning and laughing, which they consider innate (rather than socially learned) behaviors.

Aaron Lynch described seven general patterns of meme transmission, or "thought contagion":


Dawkins initially defined "meme" as a noun that "conveys the idea of a unit of cultural transmission, or a unit of "imitation"". John S. Wilkins retained the notion of meme as a kernel of cultural imitation while emphasizing the meme's evolutionary aspect, defining the meme as "the least unit of sociocultural information relative to a selection process that has favorable or unfavorable selection bias that exceeds its endogenous tendency to change". The meme as a unit provides a convenient means of discussing "a piece of thought copied from person to person", regardless of whether that thought contains others inside it, or forms part of a larger meme. A meme could consist of a single word, or a meme could consist of the entire speech in which that word first occurred. This forms an analogy to the idea of a gene as a single unit of self-replicating information found on the self-replicating chromosome.

While the identification of memes as "units" conveys their nature to replicate as discrete, indivisible entities, it does not imply that thoughts somehow become quantized or that "atomic" ideas exist that cannot be dissected into smaller pieces. A meme has no given size. Susan Blackmore writes that melodies from Beethoven's symphonies are commonly used to illustrate the difficulty involved in delimiting memes as discrete units. She notes that while the first four notes of Beethoven's Fifth Symphony () form a meme widely replicated as an independent unit, one can regard the entire symphony as a single meme as well.

The inability to pin an idea or cultural feature to quantifiable key units is widely acknowledged as a problem for memetics. It has been argued however that the traces of memetic processing can be quantified utilizing neuroimaging techniques which measure changes in the connectivity profiles between brain regions." Blackmore meets such criticism by stating that memes compare with genes in this respect: that while a gene has no particular size, nor can we ascribe every phenotypic feature directly to a particular gene, it has value because it encapsulates that key unit of inherited expression subject to evolutionary pressures. To illustrate, she notes evolution selects for the gene for features such as eye color; it does not select for the individual nucleotide in a strand of DNA. Memes play a comparable role in understanding the evolution of imitated behaviors.

The 1981 book "Genes, Mind, and Culture: The Coevolutionary Process" by Charles J. Lumsden and E. O. Wilson proposed the theory that genes and culture co-evolve, and that the fundamental biological units of culture must correspond to neuronal networks that function as nodes of semantic memory. They coined their own word, "culturgen", which did not catch on. Coauthor Wilson later acknowledged the term "meme" as the best label for the fundamental unit of cultural inheritance in his 1998 book "", which elaborates upon the fundamental role of memes in unifying the natural and social sciences.

Dawkins noted the three conditions that must exist for evolution to occur:
Dawkins emphasizes that the process of evolution naturally occurs whenever these conditions co-exist, and that evolution does not apply only to organic elements such as genes. He regards memes as also having the properties necessary for evolution, and thus sees meme evolution as not simply analogous to genetic evolution, but as a real phenomenon subject to the laws of natural selection. Dawkins noted that as various ideas pass from one generation to the next, they may either enhance or detract from the survival of the people who obtain those ideas, or influence the survival of the ideas themselves. For example, a certain culture may develop unique designs and methods of tool-making that give it a competitive advantage over another culture. Each tool-design thus acts somewhat similarly to a biological gene in that some populations have it and others do not, and the meme's function directly affects the presence of the design in future generations. In keeping with the thesis that in evolution one can regard organisms simply as suitable "hosts" for reproducing genes, Dawkins argues that one can view people as "hosts" for replicating memes. Consequently, a successful meme may or may not need to provide any benefit to its host.

Unlike genetic evolution, memetic evolution can show both Darwinian and Lamarckian traits. Cultural memes will have the characteristic of Lamarckian inheritance when a host aspires to replicate the given meme through inference rather than by exactly copying it. Take for example the case of the transmission of a simple skill such as hammering a nail, a skill that a learner imitates from watching a demonstration without necessarily imitating every discrete movement modeled by the teacher in the demonstration, stroke for stroke. Susan Blackmore distinguishes the difference between the two modes of inheritance in the evolution of memes, characterizing the Darwinian mode as "copying the instructions" and the Lamarckian as "copying the product."

Clusters of memes, or "memeplexes" (also known as "meme complexes" or as "memecomplexes"), such as cultural or political doctrines and systems, may also play a part in the acceptance of new memes. Memeplexes comprise groups of memes that replicate together and coadapt. Memes that fit within a successful memeplex may gain acceptance by "piggybacking" on the success of the memeplex.
As an example, John D. Gottsch discusses the transmission, mutation and selection of religious memeplexes and the theistic memes contained. Theistic memes discussed include the "prohibition of aberrant sexual practices such as incest, adultery, homosexuality, bestiality, castration, and religious prostitution", which may have increased vertical transmission of the parent religious memeplex. Similar memes are thereby included in the majority of religious memeplexes, and harden over time; they become an "inviolable canon" or set of dogmas, eventually finding their way into secular law. This could also be referred to as the propagation of a taboo.

The discipline of memetics, which dates from the mid-1980s, provides an approach to evolutionary models of cultural information transfer based on the concept of the meme. Memeticists have proposed that just as memes function analogously to genes, memetics functions analogously to genetics. Memetics attempts to apply conventional scientific methods (such as those used in population genetics and epidemiology) to explain existing patterns and transmission of cultural ideas.

Principal criticisms of memetics include the claim that memetics ignores established advances in other fields of cultural study, such as sociology, cultural anthropology, cognitive psychology, and social psychology. Questions remain whether or not the meme concept counts as a validly disprovable scientific theory. This view regards memetics as a theory in its infancy: a protoscience to proponents, or a pseudoscience to some detractors.

An objection to the study of the evolution of memes in genetic terms (although not to the existence of memes) involves a perceived gap in the gene/meme analogy: the cumulative evolution of genes depends on biological selection-pressures neither too great nor too small in relation to mutation-rates. There seems no reason to think that the same balance will exist in the selection pressures on memes.

Luis Benitez-Bribiesca M.D., a critic of memetics, calls the theory a "pseudoscientific dogma" and "a dangerous idea that poses a threat to the serious study of consciousness and cultural evolution". As a factual criticism, Benitez-Bribiesca points to the lack of a "code script" for memes (analogous to the DNA of genes), and to the excessive instability of the meme mutation mechanism (that of an idea going from one brain to another), which would lead to a low replication accuracy and a high mutation rate, rendering the evolutionary process chaotic.

British political philosopher John Gray has characterized Dawkins's memetic theory of religion as "nonsense" and "not even a theory... the latest in a succession of ill-judged Darwinian metaphors", comparable to Intelligent Design in its value as a science.

Another critique comes from semiotic theorists such as Deacon and Kull. This view regards the concept of "meme" as a primitivized concept of "sign". The meme is thus described in memetics as a sign lacking a triadic nature. Semioticians can regard a meme as a "degenerate" sign, which includes only its ability of being copied. Accordingly, in the broadest sense, the objects of copying are memes, whereas the objects of translation and interpretation are signs.

Fracchia and Lewontin regard memetics as reductionist and inadequate. Evolutionary biologist Ernst Mayr disapproved of Dawkins's gene-based view and usage of the term "meme", asserting it to be an "unnecessary synonym" for "concept", reasoning that concepts are not restricted to an individual or a generation, may persist for long periods of time, and may evolve.

Opinions differ as to how best to apply the concept of memes within a "proper" disciplinary framework. One view sees memes as providing a useful philosophical perspective with which to examine cultural evolution. Proponents of this view (such as Susan Blackmore and Daniel Dennett) argue that considering cultural developments from a meme's-eye view—"as if" memes themselves respond to pressure to maximise their own replication and survival—can lead to useful insights and yield valuable predictions into how culture develops over time. Others such as Bruce Edmonds and Robert Aunger have focused on the need to provide an empirical grounding for memetics to become a useful and respected scientific discipline.

A third approach, described by Joseph Poulshock, as "radical memetics" seeks to place memes at the centre of a materialistic theory of mind and of personal identity.

Prominent researchers in evolutionary psychology and anthropology, including Scott Atran, Dan Sperber, Pascal Boyer, John Tooby and others, argue the possibility of incompatibility between modularity of mind and memetics. In their view, minds structure certain communicable aspects of the ideas produced, and these communicable aspects generally trigger or elicit ideas in other minds through inference (to relatively rich structures generated from often low-fidelity input) and not high-fidelity replication or imitation. Atran discusses communication involving religious beliefs as a case in point. In one set of experiments he asked religious people to write down on a piece of paper the meanings of the Ten Commandments. Despite the subjects' own expectations of consensus, interpretations of the commandments showed wide ranges of variation, with little evidence of consensus. In another experiment, subjects with autism and subjects without autism interpreted ideological and religious sayings (for example, "Let a thousand flowers bloom" or "To everything there is a season"). People with autism showed a significant tendency to closely paraphrase and repeat content from the original statement (for example: "Don't cut flowers before they bloom"). Controls tended to infer a wider range of cultural meanings with little replicated content (for example: "Go with the flow" or "Everyone should have equal opportunity"). Only the subjects with autism—who lack the degree of inferential capacity normally associated with aspects of theory of mind—came close to functioning as "meme machines".

In his book "The Robot's Rebellion", Stanovich uses the memes and memeplex concepts to describe a program of cognitive reform that he refers to as a "rebellion". Specifically, Stanovich argues that the use of memes as a descriptor for cultural units is beneficial because it serves to emphasize transmission and acquisition properties that parallel the study of epidemiology. These properties make salient the sometimes parasitic nature of acquired memes, and as a result individuals should be motivated to reflectively acquire memes using what he calls a "Neurathian bootstrap" process.

Although social scientists such as Max Weber sought to understand and explain religion in terms of a cultural attribute, Richard Dawkins called for a re-analysis of religion in terms of the evolution of self-replicating ideas "apart from" any resulting biological advantages they might bestow.
He argued that the role of key replicator in cultural evolution belongs not to genes, but to memes replicating thought from person to person by means of imitation. These replicators respond to selective pressures that may or may not affect biological reproduction or survival.

In her book "The Meme Machine", Susan Blackmore regards religions as particularly tenacious memes. Many of the features common to the most widely practiced religions provide built-in advantages in an evolutionary context, she writes. For example, religions that preach of the value of faith over evidence from everyday experience or reason inoculate societies against many of the most basic tools people commonly use to evaluate their ideas. By linking altruism with religious affiliation, religious memes can proliferate more quickly because people perceive that they can reap societal as well as personal rewards. The longevity of religious memes improves with their documentation in revered religious texts.

Aaron Lynch attributed the robustness of religious memes in human culture to the fact that such memes incorporate multiple modes of meme transmission. Religious memes pass down the generations from parent to child and across a single generation through the meme-exchange of proselytism. Most people will hold the religion taught them by their parents throughout their life. Many religions feature adversarial elements, punishing apostasy, for instance, or demonizing infidels. In "Thought Contagion" Lynch identifies the memes of transmission in Christianity as especially powerful in scope. Believers view the conversion of non-believers both as a religious duty and as an act of altruism. The promise of heaven to believers and threat of hell to non-believers provide a strong incentive for members to retain their belief. Lynch asserts that belief in the Crucifixion of Jesus in Christianity amplifies each of its other replication advantages through the indebtedness believers have to their Savior for sacrifice on the cross. The image of the crucifixion recurs in religious sacraments, and the proliferation of symbols of the cross in homes and churches potently reinforces the wide array of Christian memes.

Although religious memes have proliferated in human cultures, the modern scientific community has been relatively resistant to religious belief. Robertson (2007) reasoned that if evolution is accelerated in conditions of propagative difficulty, then we would expect to encounter variations of religious memes, established in general populations, addressed to scientific communities. Using a memetic approach, Robertson deconstructed two attempts to privilege religiously held spirituality in scientific discourse. Advantages of a memetic approach as compared to more traditional "modernization" and "supply side" theses in understanding the evolution and propagation of religion were explored.

In "Cultural Software: A Theory of Ideology", Jack Balkin argued that memetic processes can explain many of the most familiar features of ideological thought. His theory of "cultural software" maintained that memes form narratives, social networks, metaphoric and metonymic models, and a variety of different mental structures. Balkin maintains that the same structures used to generate ideas about free speech or free markets also serve to generate racistic beliefs. To Balkin, whether memes become harmful or maladaptive depends on the environmental context in which they exist rather than in any special source or manner to their origination. Balkin describes racist beliefs as "fantasy" memes that become harmful or unjust "ideologies" when diverse peoples come together, as through trade or competition.

In "A Theory of Architecture", Nikos Salingaros speaks of memes as "freely propagating clusters of information" which can be beneficial or harmful. He contrasts memes to patterns and true knowledge, characterizing memes as "greatly simplified versions of patterns" and as "unreasoned matching to some visual or mnemonic prototype". Taking reference to Dawkins, Salingaros emphasizes that they can be transmitted due to their own communicative properties, that "the simpler they are, the faster they can proliferate", and that the most successful memes "come with a great psychological appeal".

Architectural memes, according to Salingaros, can have destructive power. "Images portrayed in architectural magazines representing buildings that could not possibly accommodate everyday uses become fixed in our memory, so we reproduce them unconsciously." He lists various architectural memes that circulated since the 1920s and which, in his view, have led to contemporary architecture becoming quite decoupled from human needs. They lack connection and meaning, thereby preventing "the creation of true connections necessary to our understanding of the world". He sees them as no different from antipatterns in software design—as solutions that are false but are re-utilized nonetheless.

An "Internet meme" is a concept that spreads rapidly from person to person via the Internet, largely through Internet-based E-mailing, blogs, forums, imageboards like 4chan, social networking sites like Facebook, Instagram, or Twitter, instant messaging, social news sites or thread sites like Reddit, and video hosting services like YouTube and Twitch.

In 2013, Richard Dawkins characterized an Internet meme as one deliberately altered by human creativity, distinguished from Dawkins's original idea involving mutation "by random change and a form of Darwinian selection".




</doc>
<doc id="4102640" url="https://en.wikipedia.org/wiki?curid=4102640" title="Meaning (philosophy of language)">
Meaning (philosophy of language)

In the philosophy of language, the nature of meaning, its definition, elements, and types, was discussed by philosophers Aristotle, Augustine, and Aquinas. According to them "meaning is a relationship between two sorts of things: "signs" and the kinds of things they "mean" (intend, express or signify)". One term in the relationship of meaning necessarily causes something else to come to the mind. In other words: "a sign is defined as an entity that indicates another entity to some agent for some purpose". As Augustine states, a sign is "something that shows itself to the senses and something other than itself to the mind" ("Signum est quod se ipsum sensui et praeter se aliquid animo ostendit"; "De dial.", 1975, 86).

The types of meanings vary according to the types of the thing that is being represented. Namely:

All subsequent inquiries emphasize some particular perspectives within the general AAA framework.

The major contemporary positions of meaning come under the following partial definitions of meaning:

The evaluation of meaning according to each one of the five major substantive theories of meaning and truth is presented below. The question of what is a proper basis for deciding how words, symbols, ideas and beliefs may properly be considered to truthfully denote meaning, whether by a single person or an entire society, is dealt with by the five most prevalent substantive theories listed below. Each theory of meaning as evaluated by these respective theories of truth are each further researched by the individual scholars supporting each one of the respective theories of truth and meaning.

Both hybrid theories of meaning and alternative theories of meaning and truth have also been researched, and are subject to further assessment according to their respective and relative merits.

Correspondence theories emphasise that true beliefs and true statements of meaning correspond to the actual state of affairs and that associated meanings must be in agreement with these beliefs and statements. This type of theory stresses a relationship between thoughts or statements on one hand, and things or objects on the other. It is a traditional model tracing its origins to ancient Greek philosophers such as Socrates, Plato, and Aristotle. This class of theories holds that the truth or the falsity of a representation is determined in principle entirely by how it relates to "things", by whether it accurately describes those "things." An example of correspondence theory is the statement by the Thirteenth Century philosopher/theologian Thomas Aquinas: "Veritas est adaequatio rei et intellectus" ("Truth is the equation [or adequation] of things and intellect"), a statement which Aquinas attributed to the Ninth Century neoplatonist Isaac Israeli. Aquinas also restated the theory as: "A judgment is said to be true when it conforms to the external reality".

Correspondence theory centres heavily around the assumption that truth and meaning are a matter of accurately copying what is known as "objective reality" and then representing it in thoughts, words and other symbols. Many modern theorists have stated that this ideal cannot be achieved without analysing additional factors. For example, language plays a role in that all languages have words to represent concepts that are virtually undefined in other languages. The German word "Zeitgeist" is one such example: one who speaks or understands the language may "know" what it means, but any translation of the word apparently fails to accurately capture its full meaning (this is a problem with many abstract words, especially those derived in agglutinative languages). Thus, some words add an additional parameter to the construction of an accurate truth predicate. Among the philosophers who grappled with this problem is Alfred Tarski, whose semantic theory is summarized further below in this article.

For coherence theories in general, the assessment of meaning and truth requires a proper fit of elements within a whole system. Very often, though, coherence is taken to imply something more than simple logical consistency; often there is a demand that the propositions in a coherent system lend mutual inferential support to each other. So, for example, the completeness and comprehensiveness of the underlying set of concepts is a critical factor in judging the validity and usefulness of a coherent system. A pervasive tenet of coherence theories is the idea that truth is primarily a property of whole systems of propositions, and can be ascribed to individual propositions only according to their coherence with the whole. Among the assortment of perspectives commonly regarded as coherence theory, theorists differ on the question of whether coherence entails many possible true systems of thought or only a single absolute system.

Some variants of coherence theory are claimed to describe the essential and intrinsic properties of formal systems in logic and mathematics. However, formal reasoners are content to contemplate axiomatically independent and sometimes mutually contradictory systems side by side, for example, the various alternative geometries. On the whole, coherence theories have been rejected for lacking justification in their application to other areas of truth, especially with respect to assertions about the natural world, empirical data in general, assertions about practical matters of psychology and society, especially when used without support from the other major theories of truth.

Coherence theories distinguish the thought of rationalist philosophers, particularly of Spinoza, Leibniz, and G.W.F. Hegel, along with the British philosopher F.H. Bradley. Other alternatives may be found among several proponents of logical positivism, notably Otto Neurath and Carl Hempel.

Social constructivism holds that meaning and truth are constructed by social processes, is historically and culturally specific, and that it is in part shaped through the power struggles within a community. Constructivism views all of our knowledge as "constructed," because it does not reflect any external "transcendent" realities (as a pure correspondence theory might hold). Rather, perceptions of truth are viewed as contingent on convention, human perception, and social experience. It is believed by constructivists that representations of physical and biological reality, including race, sexuality, and gender, are socially constructed.

Giambattista Vico was among the first to claim that history and culture along with their meaning were man-made. Vico's epistemological orientation gathers the most diverse rays and unfolds in one axiom"verum ipsum factum""truth itself is constructed". Hegel and Marx were among the other early proponents of the premise that truth is, or can be, socially constructed. Marx, like many critical theorists who followed, did not reject the existence of objective truth but rather distinguished between true knowledge and knowledge that has been distorted through power or ideology. For Marx, scientific and true knowledge is "in accordance with the dialectical understanding of history" and ideological knowledge is "an epiphenomenal expression of the relation of material forces in a given economic arrangement".

Consensus theory holds that meaning and truth are whatever is agreed upon, or in some versions, might come to be agreed upon, by some specified group. Such a group might include all human beings, or a subset thereof consisting of more than one person.

Among the current advocates of consensus theory as a useful accounting of the concept of "truth" is the philosopher Jürgen Habermas. Habermas maintains that truth is what would be agreed upon in an ideal speech situation. Among the current strong critics of consensus theory is the philosopher Nicholas Rescher.

The three most influential forms of the "pragmatic theory of truth" and meaning were introduced around the turn of the 20th century by Charles Sanders Peirce, William James, and John Dewey. Although there are wide differences in viewpoint among these and other proponents of pragmatic theory, they hold in common that meaning and truth are verified and confirmed by the results of putting one's concepts into practice.

Peirce defines truth as follows: "Truth is that concordance of an abstract statement with the ideal limit towards which endless investigation would tend to bring scientific belief, which concordance the abstract statement may possess by virtue of the confession of its inaccuracy and one-sidedness, and this confession is an essential ingredient of truth." This statement stresses Peirce's view that ideas of approximation, incompleteness, and partiality, what he describes elsewhere as "fallibilism" and "reference to the future", are essential to a proper conception of meaning and truth. Although Peirce uses words like "concordance" and "correspondence" to describe one aspect of the pragmatic sign relation, he is also quite explicit in saying that definitions of truth based on mere correspondence are no more than "nominal" definitions, which he accords a lower status than "real" definitions.

William James's version of pragmatic theory, while complex, is often summarized by his statement that "the 'true' is only the expedient in our way of thinking, just as the 'right' is only the expedient in our way of behaving." By this, James meant that truth is a "quality", the value of which is confirmed by its effectiveness when applying concepts to practice (thus, "pragmatic").

John Dewey, less broadly than James but more broadly than Peirce, held that inquiry, whether scientific, technical, sociological, philosophical or cultural, is self-corrective over time "if" openly submitted for testing by a community of inquirers in order to clarify, justify, refine and/or refute proposed meanings and truths.

Though not widely known, a new variation of the pragmatic theory was defined and wielded successfully from the 20th century forward. Defined and named by William Ernest Hocking, this variation is known as "negative pragmatism". Essentially, what works may or may not be true, but what fails cannot be true because the truth and its meaning always works. James and Dewey's ideas also ascribe meaning and truth to repeated testing which is "self-corrective" over time.

Pragmatism and negative pragmatism are also closely aligned with the coherence theory of truth in that any testing should not be isolated but rather incorporate knowledge from all human endeavors and experience. The universe is a whole and integrated system, and testing should acknowledge and account for its diversity. As Feynman said, "... if it disagrees with experiment, it is wrong."

Some have asserted that meaning is nothing substantially more or less than the truth conditions they involve. For such theories, an emphasis is placed upon reference to actual things in the world to account for meaning, with the caveat that reference more or less explains the greater part (or all) of meaning itself.

The logical positivists argued that the meaning of a statement arose from how it is verified.

In his paper "Über Sinn und Bedeutung" (now usually translated as "On Sense and Reference"), Gottlob Frege argued that proper names present at least two problems in explaining meaning.


Frege can be interpreted as arguing that it was therefore a mistake to think that the meaning of a name is the thing it refers to. Instead, the meaning must be something else—the "sense" of the word. Two names for the same person, then, can have different senses (or meanings): one referent might be picked out by more than one sense. This sort of theory is called a mediated reference theory. Frege argued that, ultimately, the same bifurcation of meaning must apply to most or all linguistic categories, such as to quantificational expressions like "All boats float".

Logical analysis was further advanced by Bertrand Russell and Alfred North Whitehead in their groundbreaking "Principia Mathematica", which attempted to produce a formal language with which the truth of all mathematical statements could be demonstrated from first principles.

Russell differed from Frege greatly on many points, however. He rejected Frege's sense-reference distinction. He also disagreed that language was of fundamental significance to philosophy, and saw the project of developing formal logic as a way of eliminating all of the confusions caused by ordinary language, and hence at creating a perfectly transparent medium in which to conduct traditional philosophical argument. He hoped, ultimately, to extend the proofs of the "Principia" to all possible true statements, a scheme he called logical atomism. For a while it appeared that his pupil Wittgenstein had succeeded in this plan with his "Tractatus Logico-Philosophicus".

Russell's work, and that of his colleague G. E. Moore, developed in response to what they perceived as the nonsense dominating British philosophy departments at the turn of the 20th century, which was a kind of British Idealism most of which was derived (albeit very distantly) from the work of Hegel. In response Moore developed an approach ("Common Sense Philosophy") which sought to examine philosophical difficulties by a close analysis of the language used in order to determine its meaning. In this way Moore sought to expunge philosophical absurdities such as "time is unreal". Moore's work would have significant, if oblique, influence (largely mediated by Wittgenstein) on Ordinary language philosophy.

The Vienna Circle, a famous group of logical positivists from the early 20th century (closely allied with Russell and Frege), adopted the verificationist theory of meaning. The verificationist theory of meaning (in at least one of its forms) states that to say that an expression is meaningful is to say that there are some conditions of experience that could exist to show that the expression is true. As noted, Frege and Russell were two proponents of this way of thinking.

A semantic theory of truth was produced by Alfred Tarski for formal semantics. According to Tarski's account, meaning consists of a recursive set of rules that end up yielding an infinite set of sentences, "'p' is true if and only if p", covering the whole language. His innovation produced the notion of propositional functions discussed on the section on universals (which he called "sentential functions"), and a model-theoretic approach to semantics (as opposed to a proof-theoretic one). Finally, some links were forged to the correspondence theory of truth (Tarski, 1944).

Perhaps the most influential current approach in the contemporary theory of meaning is that sketched by Donald Davidson in his introduction to the collection of essays "Truth and Meaning" in 1967. There he argued for the following two theses:

The result is a theory of meaning that rather resembles, by no accident, Tarski's account.

Davidson's account, though brief, constitutes the first systematic presentation of truth-conditional semantics. He proposed simply translating natural languages into first-order predicate calculus in order to reduce meaning to a function of truth.

Saul Kripke examined the relation between sense and reference in dealing with possible and actual situations. He showed that one consequence of his interpretation of certain systems of modal logic was that the reference of a proper name is "necessarily" linked to its referent, but that the sense is not. So for instance "Hesperus" necessarily refers to Hesperus, even in those imaginary cases and worlds in which perhaps Hesperus is not the evening star. That is, Hesperus is necessarily Hesperus, but only contingently the morning star.

This results in the curious situation that part of the meaning of a name — that it refers to some particular thing — is a necessary fact about that name, but another part — that it is used in some particular way or situation — is not.

Kripke also drew the distinction between speaker's meaning and semantic meaning, elaborating on the work of ordinary language philosophers Paul Grice and Keith Donnellan. The speaker's meaning is what the speaker intends to refer to by saying something; the semantic meaning is what the words uttered by the speaker mean according to the language.

In some cases, people do not say what they mean; in other cases, they say something that is in error. In both these cases, the speaker's meaning and the semantic meaning seem to be different. Sometimes words do not actually express what the speaker wants them to express; so words will mean one thing, and what people intend to convey by them might mean another. The meaning of the expression, in such cases, is ambiguous.

W.V. Quine attacked both verificationism and the very notion of meaning in his famous essay, "Two Dogmas of Empiricism". In it, he suggested that meaning was nothing more than a vague and dispensable notion. Instead, he asserted, what was more interesting to study was the synonymy between signs. He also pointed out that verificationism was tied to the distinction between analytic and synthetic statements, and asserted that such a divide was defended ambiguously. He also suggested that the unit of analysis for any potential investigation into the world (and, perhaps, meaning) would be the entire body of statements taken as a collective, not just individual statements on their own.

Other criticisms can be raised on the basis of the limitations that truth-conditional theorists themselves admit to. Tarski, for instance, recognized that truth-conditional theories of meaning only make sense of statements, but fail to explain the meanings of the lexical parts that make up statements. Rather, the meaning of the parts of statements is presupposed by an understanding of the truth-conditions of a whole statement, and explained in terms of what he called "satisfaction conditions".

Still another objection (noted by Frege and others) was that some kinds of statements don't seem to have any truth-conditions at all. For instance, "Hello!" has no truth-conditions, because it doesn't even attempt to tell the listener anything about the state of affairs in the world. In other words, different propositions have different grammatical moods.

Deflationist accounts of truth, sometimes called 'irrealist' accounts, are the staunchest source of criticism of truth-conditional theories of meaning. According to them, "truth" is a word with no serious meaning or function in discourse. For instance, for the deflationist, the sentences "It's true that Tiny Tim is trouble" and "Tiny Tim is trouble" are equivalent. In consequence, for the deflationist, any appeal to truth as an account of meaning has little explanatory power.

The sort of truth-theories presented here can also be attacked for their formalism both in practice and principle. The principle of formalism is challenged by the informalists, who suggest that language is largely a construction of the speaker, and so, not compatible with formalization. The practice of formalism is challenged by those who observe that formal languages (such as present-day quantificational logic) fail to capture the expressive power of natural languages (as is arguably demonstrated in the awkward character of the quantificational explanation of definite description statements, as laid out by Bertrand Russell).

Finally, over the past century, forms of logic have been developed that are not dependent exclusively on the notions of truth and falsity. Some of these types of logic have been called modal logics. They explain how certain logical connectives such as "if-then" work in terms of necessity and possibility. Indeed, modal logic was the basis of one of the most popular and rigorous formulations in modern semantics called the Montague grammar. The successes of such systems naturally give rise to the argument that these systems have captured the natural meaning of connectives like if-then far better than an ordinary, truth-functional logic ever could.

Throughout the 20th century, English philosophy focused closely on analysis of language. This style of analytic philosophy became very influential and led to the development of a wide range of philosophical tools.

The philosopher Ludwig Wittgenstein was originally an artificial language philosopher, following the influence of Russell and Frege. In his "Tractatus Logico-Philosophicus" he had supported the idea of an ideal language built up from atomic statements using logical connectives (see picture theory of meaning and logical atomism). However, as he matured, he came to appreciate more and more the phenomenon of natural language. "Philosophical Investigations", published after his death, signalled a sharp departure from his earlier work with its focus upon ordinary language use (see use theory of meaning and ordinary language philosophy). His approach is often summarised by the aphorism "the meaning of a word is its use in a language". However, following in Frege's footsteps, in the "Tractatus", Wittgenstein declares: "... Only in the context of a proposition has a name meaning."

His work would come to inspire future generations and spur forward a whole new discipline, which explained meaning in a new way. Meaning in a natural language was seen as primarily a question of how the speaker uses words within the language to express intention.

This close examination of natural language proved to be a powerful philosophical technique. Practitioners who were influenced by Wittgenstein's approach have included an entire tradition of thinkers, featuring P. F. Strawson, Paul Grice, R. M. Hare, R. S. Peters, and Jürgen Habermas.

At around the same time Ludwig Wittgenstein was re-thinking his approach to language, reflections on the complexity of language led to a more expansive approach to meaning. Following the lead of George Edward Moore, J. L. Austin examined the use of words in great detail. He argued against fixating on the meaning of words. He showed that dictionary definitions are of limited philosophical use, since there is no simple "appendage" to a word that can be called its meaning. Instead, he showed how to focus on the way in which words are used in order to do things. He analysed the structure of utterances into three distinct parts: locutions, illocutions and perlocutions. His pupil John Searle developed the idea under the label "speech acts". Their work greatly influenced pragmatics.

Past philosophers had understood reference to be tied to words themselves. However, Sir Peter Strawson disagreed in his seminal essay, "On Referring", where he argued that there is nothing true about statements on their own; rather, only the uses of statements could be considered to be true or false.

Indeed, one of the hallmarks of the ordinary use perspective is its insistence upon the distinctions between meaning and use. "Meanings", for ordinary language philosophers, are the "instructions" for usage of words — the common and conventional definitions of words. "Usage", on the other hand, is the actual meanings that individual speakers have — the things that an individual speaker in a particular context wants to refer to. The word "dog" is an example of a meaning, but pointing at a nearby dog and shouting "This dog smells foul!" is an example of usage. From this distinction between usage and meaning arose the divide between the fields of Pragmatics and Semantics.

Yet another distinction is of some utility in discussing language: "mentioning". "Mention" is when an expression refers to itself as a linguistic item, usually surrounded by quotation marks. For instance, in the expression "'Opopanax' is hard to spell", what is referred to is the word itself ("opopanax") and not what it means (an obscure gum resin). Frege had referred to instances of mentioning as "opaque contexts".

In his essay, "Reference and Definite Descriptions", Keith Donnellan sought to improve upon Strawson's distinction. He pointed out that there are two uses of definite descriptions: "attributive" and "referential". Attributive uses provide a description of whoever is being referred to, while referential uses point out the actual referent. Attributive uses are like mediated references, while referential uses are more directly referential.

The philosopher Paul Grice, working within the ordinary language tradition, understood "meaning" — in his 1957 article — to have two kinds: "natural" and "non-natural". "Natural meaning" had to do with cause and effect, for example with the expression "these spots mean measles". "Non-natural" meaning, on the other hand, had to do with the intentions of the speaker in communicating something to the listener.

In his essay, "Logic and Conversation", Grice went on to explain and defend an explanation of how conversations work. His guiding maxim was called the "cooperative principle", which claimed that the speaker and the listener will have mutual expectations of the kind of information that will be shared. The principle is broken down into four maxims: "Quality" (which demands truthfulness and honesty), "Quantity" (demand for just enough information as is required), "Relation" (relevance of things brought up), and "Manner" (lucidity). This principle, if and when followed, lets the speaker and listener figure out the meaning of certain implications by way of inference.

The works of Grice led to an avalanche of research and interest in the field, both supportive and critical. One spinoff was called Relevance theory, developed by Dan Sperber and Deirdre Wilson during the mid-1980s, whose goal was to make the notion of "relevance" more clear. Similarly, in his work, "Universal pragmatics", Jürgen Habermas began a program that sought to improve upon the work of the ordinary language tradition. In it, he laid out the goal of a valid conversation as a pursuit of mutual understanding.

Although he has focused on the structure and functioning of human syntax, in many works Noam Chomsky has discussed many philosophical problems too, including the problem of meaning and reference in human language. Chomsky has formulated a strong criticism against both the externalist notion of reference (reference consists in a direct or causal relation among words and objects) and the internalist one (reference is a mind-mediated relation holding among words and reality). According to Chomsky, both these notions (and many others widely used in philosophy, such as that of truth) are basically inadequate for the naturalistic (= scientific) inquiry on human mind: they are common sense notions, not scientific notions, which cannot, as such, enter in the scientific discussion. Chomsky argues that the notion of reference can be used only when we deal with scientific (i.e. artificial) languages, whose symbols refers to specific things or entities; but when we consider human language expressions, we immediately understand that their reference is vague, in the sense that they can be used to denote many things. For example, the word “book” can be used to denote an abstract object (e.g., “he is reading the book”) or a concrete one (e.g., “the book is on the chair”); the name “London” can denote at the same time a set of buildings, the air of a place and the character of a population (think to the sentence “London is so gray, polluted and sad”). These and other cases induce Chomsky to argue that the only plausible (although not scientific) notion of reference is that of act of reference, a complex phenomenon of language use (performance) which includes many factors (linguistic and not: i.e. beliefs, desires, assumptions about the world, premises, etc.). As Chomsky himself has pointed out 
, this conception of meaning is very close to that adopted by John Austin, Peter Strawson and the late Wittgenstein.

Michael Dummett argued against the kind of truth-conditional semantics presented by Davidson. Instead, he argued that basing semantics on "assertion conditions" avoids a number of difficulties with truth-conditional semantics, such as the transcendental nature of certain kinds of truth condition. He leverages work done in proof-theoretic semantics to provide a kind of inferential role semantics, where:
A semantics based upon assertion conditions is called a verificationist semantics: cf. the verificationism of the Vienna Circle.

This work is closely related, though not identical, to one-factor theories of conceptual role semantics.

Sometimes between the 1950-1990s, cognitive scientist Jerry Fodor said that use theories (of the Wittgensteinian kind) seem to assume that language is solely a public phenomenon, that there is no such thing as a "private language". Fodor thinks it is necessary to create or describe the "language of thought", which would seemingly require the existence of a "private language".

In the 1960s, David Kellogg Lewis described meaning as use, a feature of a social convention and conventions as regularities of a specific sort. Lewis' work was an application of game theory in philosophical topics. Conventions, he argued, are a species of coordination equilibria.

The idea theory of meaning (also ideational theory of meaning), most commonly associated with the British empiricist John Locke, claims that meanings are mental representations provoked by signs. 

The term "ideas" is used to refer to either mental representations, or to mental activity in general. Those who seek an explanation for meaning in the former sort of account endorse a stronger sort of idea theory of mind than the latter. Those who seek an explanation for meaning in the former sort of account endorse a stronger sort of idea theory of meaning than the latter.

Each idea is understood to be necessarily "about" something external and/or internal, real or imaginary. For example, in contrast to the abstract meaning of the universal "dog", the referent "this dog" may mean a particular real life chihuahua. In both cases, the word is about something, but in the former it is about the class of dogs as generally understood, while in the latter it is about a very real and particular dog in the real world.

John Locke, considered all ideas to be both imaginable objects of sensation and the very "un"imaginable objects of reflection. Locke said in his "Essay Concerning Human Understanding", that words are used both as signs for ideas—but also to signify the lack of certain ideas. David Hume held that thoughts were kinds of imaginable entities. (See Hume's "Enquiry Concerning Human Understanding", section 2). Hume argued that any words that could not call upon any past experience were without meaning.

Nonetheless, George Berkeley and Ludwig Wittgenstein held, in contrast to Locke and Hume, that ideas alone are unable to account for the different variations within a general meaning. For example, any hypothetical image of the meaning of "dog" has to include such varied images as a chihuahua, a pug, and a Black Lab; and this seems impossible to imagine, all of those particular breeds looking very different from one another. Another way to see this point is to question why it is that, if we have an image of a specific type of dog (say of a chihuahua), it should be entitled to represent the entire concept.

Another criticism is that some meaningful words, known as non-lexical items, don't have any meaningfully associated image. For example, the word "the" has a meaning, but one would be hard-pressed to find a mental representation that fits it. Still another objection lies in the observation that certain linguistic items name something in the real world, and are meaningful, yet which we have no mental representations to deal with. For instance, it is not known what Newton's father looked like, yet the phrase "Newton's father" still has meaning.

Another problem is that of composition — that it is difficult to explain how words and phrases combine into sentences if only ideas were involved in meaning.

Eleanor Rosch and George Lakoff advanced the theory of prototypes, which suggests that many lexical categories, at least on the face of things, have "radial structures". That is to say, there are some ideal member(s) in the category that seem to represent the category better than other members. For example, the category of "birds" may feature the "robin" as the prototype, or the ideal kind of bird. With experience, subjects might come to evaluate membership in the category of "bird" by comparing candidate members to the prototype and evaluating for similarities. So, for example, a penguin or an ostrich would sit at the fringe of the meaning of "bird", because a penguin is unlike a robin.

Intimately related to these researches is the notion of a "psychologically basic level", which is both the first level named and understood by children, and "the highest level at which a single mental image can reflect the entire category". (Lakoff 1987:46) The "basic level" of cognition is understood by Lakoff as crucially drawing upon "image-schemas" along with various other cognitive processes.

The philosophers (Ned Block, Gilbert Harman, H. Field) and the cognitive scientists (G. Miller and P. Johnson-Laird) say that the meaning of a term can be found by investigating its role in relation to other concepts and mental states. They endorse a view called "conceptual role semantics". Those proponents of this view who understand meanings to be exhausted by the content of mental states can be said to endorse "one-factor" accounts of conceptual role semantics. and thus fit within the tradition of idea theories.




</doc>
<doc id="43854" url="https://en.wikipedia.org/wiki?curid=43854" title="Reality">
Reality

Reality is the sum or aggregate of all that is real or existent within a system, as opposed to that which is only imaginary. The term is also used to refer to the ontological status of things, indicating their existence. In physical terms, reality is the totality of a system, known and unknown. Philosophical questions about the nature of reality or existence or being are considered under the rubric of ontology, which is a major branch of metaphysics in the Western philosophical tradition. Ontological questions also feature in diverse branches of philosophy, including the philosophy of science, philosophy of religion, philosophy of mathematics, and philosophical logic. These include questions about whether only physical objects are real (i.e., Physicalism), whether reality is fundamentally immaterial (e.g., Idealism), whether hypothetical unobservable entities posited by scientific theories exist, whether God exists, whether numbers and other abstract objects exist, and whether possible worlds exist.

A common colloquial usage would have "reality" mean "perceptions, beliefs, and attitudes toward reality", as in "My reality is not your reality." This is often used just as a colloquialism indicating that the parties to a conversation agree, or should agree, not to quibble over deeply different conceptions of what is real. For example, in a religious discussion between friends, one might say (attempting humor), "You might disagree, but in my reality, everyone goes to heaven."

Reality can be defined in a way that links it to worldviews or parts of them (conceptual frameworks): Reality is the totality of all things, structures (actual and conceptual), events (past and present) and phenomena, whether observable or not. It is what a world view (whether it be based on individual or shared human experience) ultimately attempts to describe or map.

Certain ideas from physics, philosophy, sociology, literary criticism, and other fields shape various theories of reality. One such belief is that there simply and literally "is" no reality beyond the perceptions or beliefs we each have about reality. Such attitudes are summarized in the popular statement, "Perception is reality" or "Life is how you perceive reality" or "reality is what you can get away with" (Robert Anton Wilson), and they indicate anti-realism – that is, the view that there is no objective reality, whether acknowledged explicitly or not.

Many of the concepts of science and philosophy are often defined culturally and socially. This idea was elaborated by Thomas Kuhn in his book "The Structure of Scientific Revolutions" (1962). "The Social Construction of Reality", a book about the sociology of knowledge written by Peter L. Berger and Thomas Luckmann, was published in 1966. It explained how knowledge is acquired and used for the comprehension of reality. Out of all the realities, the reality of everyday life is the most important one since our consciousness requires us to be completely aware and attentive to the experience of everyday life.

Philosophy addresses two different aspects of the topic of reality: the nature of reality itself, and the relationship between the mind (as well as language and culture) and reality.

On the one hand, ontology is the study of being, and the central topic of the field is couched, variously, in terms of being, existence, "what is", and reality. The task in ontology is to describe the most general categories of reality and how they are interrelated. If a philosopher wanted to proffer a positive definition of the concept "reality", it would be done under this heading. As explained above, some philosophers draw a distinction between reality and existence. In fact, many analytic philosophers today tend to avoid the term "real" and "reality" in discussing ontological issues. But for those who would treat "is real" the same way they treat "exists", one of the leading questions of analytic philosophy has been whether existence (or reality) is a property of objects. It has been widely held by analytic philosophers that it is "not" a property at all, though this view has lost some ground in recent decades.

On the other hand, particularly in discussions of objectivity that have feet in both metaphysics and epistemology, philosophical discussions of "reality" often concern the ways in which reality is, or is not, in some way "dependent upon" (or, to use fashionable jargon, "constructed" out of) mental and cultural factors such as perceptions, beliefs, and other mental states, as well as cultural artifacts, such as religions and political movements, on up to the vague notion of a common cultural world view, or .

The view that there is a reality independent of any beliefs, perceptions, etc., is called realism. More specifically, philosophers are given to speaking about "realism "about"" this and that, such as realism about universals or realism about the external world. Generally, where one can identify any class of object, the existence or essential characteristics of which is said not to depend on perceptions, beliefs, language, or any other human artifact, one can speak of "realism "about"" that object.

One can also speak of "anti"-realism about the same objects. "Anti-realism" is the latest in a long series of terms for views opposed to realism. Perhaps the first was idealism, so called because reality was said to be in the mind, or a product of our "ideas". Berkeleyan idealism is the view, propounded by the Irish empiricist George Berkeley, that the objects of perception are actually ideas in the mind. In this view, one might be tempted to say that reality is a "mental construct"; this is not quite accurate, however, since, in Berkeley's view, perceptual ideas are created and coordinated by God. By the 20th century, views similar to Berkeley's were called phenomenalism. Phenomenalism differs from Berkeleyan idealism primarily in that Berkeley believed that minds, or souls, are not merely ideas nor made up of ideas, whereas varieties of phenomenalism, such as that advocated by Russell, tended to go farther to say that the mind itself is merely a collection of perceptions, memories, etc., and that there is no mind or soul over and above such mental events. Finally, anti-realism became a fashionable term for "any" view which held that the existence of some object depends upon the mind or cultural artifacts. The view that the so-called external world is really merely a social, or cultural, artifact, called social constructionism, is one variety of anti-realism. Cultural relativism is the view that social issues such as morality are not absolute, but at least partially cultural artifact.

A correspondence theory of knowledge about what exists claims that "true" knowledge of reality represents accurate correspondence of statements about and images of reality with the actual reality that the statements or images are attempting to represent. For example, the scientific method can verify that a statement is true based on the observable evidence that a thing exists. Many humans can point to the Rocky Mountains and say that this mountain range exists, and continues to exist even if no one is observing it or making statements about it.

The nature of being is a perennial topic in metaphysics. For, instance Parmenides taught that reality was a single unchanging Being, whereas Heraclitus wrote that all things flow. The 20th century philosopher Heidegger thought previous philosophers have lost sight the question of Being (qua Being) in favour of the questions of beings (existing things), so that a return to the Parmenidean approach was needed. An ontological catalogue is an attempt to list the fundamental constituents of reality. The question of whether or not existence is a predicate has been discussed since the Early Modern period, not least in relation to the ontological argument for the existence of God. Existence, "that" something is, has been contrasted with "essence", the question of "what" something is.
Since existence without essence seems blank, it associated with nothingness by philosophers such as Hegel. Nihilism represents an extremely negative view of being, the absolute a positive one.

The question of direct or "naïve" realism, as opposed to indirect or "representational" realism, arises in the philosophy of perception and of mind out of the debate over the nature of conscious experience; the epistemological question of whether the world we see around us is the real world itself or merely an internal perceptual copy of that world generated by neural processes in our brain. Naïve realism is known as "direct" realism when developed to counter "indirect" or representative realism, also known as epistemological dualism, the philosophical position that our conscious experience is not of the real world itself but of an internal representation, a miniature virtual-reality replica of the world.

Timothy Leary coined the influential term Reality Tunnel, by which he means a kind of representative realism. The theory states that, with a subconscious set of mental filters formed from their beliefs and experiences, every individual interprets the same world differently, hence "Truth is in the eye of the beholder". His ideas influenced the work of his friend Robert Anton Wilson.

The status of abstract entities, particularly numbers, is a topic of discussion in mathematics.

In the philosophy of mathematics, the best known form of realism about numbers is Platonic realism, which grants them abstract, immaterial existence. Other forms of realism identify mathematics with the concrete physical universe.

Anti-realist stances include formalism and fictionalism.

Some approaches are selectively realistic about some mathematical objects but not others. Finitism rejects infinite quantities. Ultra-finitism accepts finite quantities up to a certain amount. Constructivism and intuitionism are realistic about objects that can be explicitly constructed, but reject the use of the principle of the excluded middle to prove existence by reductio ad absurdum.

The traditional debate has focused on whether an abstract (immaterial, intelligible) realm of numbers has existed "in addition to" the physical (sensible, concrete) world. A recent development is the mathematical universe hypothesis, the theory that "only" a mathematical world exists, with the finite, physical world being an illusion within it.

An extreme form of realism about mathematics is the mathematical multiverse hypothesis advanced by Max Tegmark. Tegmark's sole postulate is: "All structures that exist mathematically also exist physically". That is, in the sense that "in those [worlds] complex enough to contain self-aware substructures [they] will subjectively perceive themselves as existing in a physically 'real' world". The hypothesis suggests that worlds corresponding to different sets of initial conditions, physical constants, or altogether different equations should be considered real. The theory can be considered a form of Platonism in that it posits the existence of mathematical entities, but can also be considered a mathematical monism in that it denies that anything exists except mathematical objects.

The problem of universals is an ancient problem in metaphysics about whether universals exist. Universals are general or abstract qualities, characteristics, properties, kinds or relations, such as being male/female, solid/liquid/gas or a certain colour, that can be predicated of individuals or particulars or that individuals or particulars can be regarded as sharing or participating in. For example, Scott, Pat, and Chris have in common the universal quality of "being human" or "humanity".

The realist school claims that universals are real – they exist and are distinct from the particulars that instantiate them. There are various forms of realism. Two major forms are Platonic realism and Aristotelian realism. "Platonic realism" is the view that universals are real entities and they exist independent of particulars. "Aristotelian realism", on the other hand, is the view that universals are real entities, but their existence is dependent on the particulars that exemplify them.

Nominalism and conceptualism are the main forms of anti-realism about universals.

A traditional realist position in ontology is that time and space have existence apart from the human mind. Idealists deny or doubt the existence of objects independent of the mind. Some anti-realists whose ontological position is that objects outside the mind do exist, nevertheless doubt the independent existence of time and space.

Kant, in the "Critique of Pure Reason", described time as an "a priori" notion that, together with other "a priori" notions such as space, allows us to comprehend sense experience. Kant denies that either space or time are substance, entities in themselves, or learned by experience; he holds rather that both are elements of a systematic framework we use to structure our experience. Spatial measurements are used to quantify how far apart objects are, and temporal measurements are used to quantitatively compare the interval between (or duration of) events. Although space and time are held to be "transcendentally ideal" in this sense, they are also "empirically real", i.e. not mere illusions.

Idealist writers such as J. M. E. McTaggart in "The Unreality of Time" have argued that time is an illusion.

As well as differing about the reality of time as a whole, metaphysical theories of time can differ in their ascriptions of reality to the past, present and future separately.

Time, and the related concepts of process and evolution are central to the system-building metaphysics of A. N. Whitehead and Charles Hartshorne.

The term "possible world" goes back to Leibniz's theory of possible worlds, used to analyse necessity, possibility, and similar modal notions. Modal realism is the view, notably propounded by David Kellogg Lewis, that all possible worlds are as real as the actual world. In short: the actual world is regarded as merely one among an infinite set of logically possible worlds, some "nearer" to the actual world and some more remote. Other theorists may use the Possible World framework to express and explore problems without committing to it ontologically.
Possible world theory is related to alethic logic: a proposition is "necessary" if it is true in all possible worlds, and "possible" if it is true in at least one. The many worlds interpretation of quantum mechanics is a similar idea in science.

The philosophical implications of a physical TOE are frequently debated. For example, if philosophical physicalism is true, a physical TOE will coincide with a philosophical theory of everything.

The "system building" style of metaphysics attempts to answer "all" the important questions in a coherent way, providing a complete picture of the world. Plato and Aristotle could be said to be early examples of comprehensive systems. In the early modern period (17th and 18th centuries), the system-building "scope" of philosophy is often linked to the rationalist "method" of philosophy, that is the technique of deducing the nature of the world by pure "a priori" reason. Examples from the early modern period include the Leibniz's Monadology, Descartes's Dualism, Spinoza's Monism. Hegel's Absolute idealism and Whitehead's Process philosophy were later systems.

Other philosophers do not believe its techniques can aim so high. Some scientists think a more mathematical approach than philosophy is needed for a TOE, for instance Stephen Hawking wrote in "A Brief History of Time" that even if we had a TOE, it would necessarily be a set of equations. He wrote, "What is it that breathes fire into the equations and makes a universe for them to describe?"

On a much broader and more subjective level, private experiences, curiosity, inquiry, and the selectivity involved in personal interpretation of events shapes reality as seen by one and only one individual and hence is called phenomenological. While this
form of reality might be common to others as well, it could at times also be so unique to oneself as to never be experienced or agreed upon by anyone else. Much of the kind of experience deemed spiritual occurs on this level of reality.

Phenomenology is a philosophical method developed in the early years of the twentieth century by Edmund Husserl and a circle of followers at the universities of Göttingen and Munich in Germany. Subsequently, phenomenological themes were taken up by philosophers in France, the United States, and elsewhere, often in contexts far removed from Husserl's work.

The word "phenomenology" comes from the Greek "phainómenon", meaning "that which appears", and "lógos", meaning "study". In Husserl's conception, phenomenology is primarily concerned with making the structures of consciousness, and the phenomena which appear in acts of consciousness, objects of systematic reflection and analysis. Such reflection was to take place from a highly modified "first person" viewpoint, studying phenomena not as they appear to "my" consciousness, but to any consciousness whatsoever. Husserl believed that phenomenology could thus provide a firm basis for all human knowledge, including scientific knowledge, and could establish philosophy as a "rigorous science".

Husserl's conception of phenomenology has been criticised and developed not only by himself, but also by his student and assistant Martin Heidegger, by existentialists, such as Maurice Merleau-Ponty, Jean-Paul Sartre, and by other philosophers, such as Paul Ricoeur, Emmanuel Levinas, and Dietrich von Hildebrand.

Skeptical hypotheses in philosophy suggest that reality is very different from what we think it is; or at least that we cannot prove it is not. Examples include:


Jain philosophy postulates that seven tattva (truths or fundamental principles) constitute reality. These seven "tattva" are:

Scientific realism is, at the most general level, the view that the world described by science (perhaps ideal science) is the real world, as it is, independent of what we might take it to be. Within philosophy of science, it is often framed as an answer to the question "how is the success of science to be explained?" The debate over what the success of science involves centers primarily on the status of entities that are not directly observable discussed by scientific theories. Generally, those who are scientific realists state that one can make reliable claims about these entities (viz., that they have the same ontological status) as directly observable entities, as opposed to instrumentalism. The most used and studied scientific theories today state more or less the truth.

"Realism" in the sense used by physicists does not equate to realism in metaphysics.
The latter is the claim that the world is mind-independent: that even if the results of a measurement do not pre-exist the act of measurement, that does not require that they are the creation of the observer. Furthermore, a mind-independent property does not have to be the value of some physical variable such as position or momentum. A property can be "dispositional" (or potential), i.e. it can be a tendency: in the way that glass objects tend to break, or are disposed to break, even if they do not "actually" break. Likewise, the mind-independent properties of quantum systems could consist of a tendency to respond to particular measurements with particular values with ascertainable probability. Such an ontology would be metaphysically realistic, without being realistic in the physicist's sense of "local realism" (which would require that a single value be produced with certainty).

A closely related term is counterfactual definiteness (CFD), used to refer to the claim that one can meaningfully speak of the definiteness of results of measurements that have not been performed (i.e. the ability to assume the existence of objects, and properties of objects, even when they have not been measured).

Local realism is a significant feature of classical mechanics, of general relativity, and of electrodynamics; but quantum mechanics has shown that quantum entanglement is possible. This was rejected by Einstein, who proposed the EPR paradox, but it was subsequently quantified by Bell's inequalities. If Bell's inequalities are violated, either local realism "or" counterfactual definiteness must be incorrect; but some physicists dispute that experiments have demonstrated Bell's violations, on the grounds that the sub-class of inhomogeneous Bell inequalities has not been tested or due to experimental limitations in the tests. Different interpretations of quantum mechanics violate different parts of local realism and/or counterfactual definiteness.

The quantum mind–body problem refers to the philosophical discussions of the mind–body problem in the context of quantum mechanics. Since quantum mechanics involves quantum superpositions, which are not perceived by observers, some interpretations of quantum mechanics place conscious observers in a special position.

The founders of quantum mechanics debated the role of the observer, and of them, Wolfgang Pauli and Werner Heisenberg believed that it was the observer that produced collapse. This point of view, which was never fully endorsed by Niels Bohr, was denounced as mystical and anti-scientific by Albert Einstein. Pauli accepted the term, and described quantum mechanics as "lucid mysticism".

Heisenberg and Bohr always described quantum mechanics in logical positivist terms. Bohr also took an active interest in the philosophical implications of quantum theories such as his complementarity, for example. He believed quantum theory offers a complete description of nature, albeit one that is simply ill-suited for everyday experiences – which are better described by classical mechanics and probability. Bohr never specified a demarcation line above which objects cease to be quantum and become classical. He believed that it was not a question of physics, but one of philosophy.

Eugene Wigner reformulated the "Schrödinger's cat" thought experiment as "Wigner's friend" and proposed that the consciousness of an observer is the demarcation line which precipitates collapse of the wave function, independent of any realist interpretation. Commonly known as "consciousness causes collapse", this interpretation of quantum mechanics states that observation by a conscious observer is what makes the wave function collapse.

The multiverse is the hypothetical set of multiple possible universes (including the historical universe we consistently experience) that together comprise everything that exists: the entirety of space, time, matter, and energy as well as the physical laws and constants that describe them. The term was coined in 1895 by the American philosopher and psychologist William James. In the many-worlds interpretation (MWI), one of the mainstream interpretations of quantum mechanics, there are an infinite number of universes and every possible quantum outcome occurs in at least one universe.

The structure of the multiverse, the nature of each universe within it and the relationship between the various constituent universes, depend on the specific multiverse hypothesis considered. Multiverses have been hypothesized in cosmology, physics, astronomy, religion, philosophy, transpersonal psychology and fiction, particularly in science fiction and fantasy. In these contexts, parallel universes are also called "alternative universes", "quantum universes", "interpenetrating dimensions", "parallel dimensions", "parallel worlds", "alternative realities", "alternative timelines", and "dimensional planes", among others.

A theory of everything (TOE) is a putative theory of theoretical physics that fully explains and links together all known physical phenomena, and predicts the outcome of "any" experiment that could be carried out "in principle". The theory of everything is also called the final theory. Many candidate theories of everything have been proposed by theoretical physicists during the twentieth century, but none have been confirmed experimentally. The primary problem in producing a TOE is that general relativity and quantum mechanics are hard to unify. This is one of the unsolved problems in physics.

Initially, the term "theory of everything" was used with an ironic connotation to refer to various overgeneralized theories. For example, a great-grandfather of Ijon Tichy, a character from a cycle of Stanisław Lem's science fiction stories of the 1960s, was known to work on the "General Theory of Everything". Physicist John Ellis claims to have introduced the term into the technical literature in an article in "Nature" in 1986. Over time, the term stuck in popularizations of quantum physics to describe a theory that would unify or explain through a single model the theories of all fundamental interactions and of all particles of nature: general relativity for gravitation, and the standard model of elementary particle physics – which includes quantum mechanics – for electromagnetism, the two nuclear interactions, and the known elementary particles.

Current candidates for a theory of everything include string theory, M theory, and loop quantum gravity.

Virtual reality (VR) is a computer-simulated environment that can simulate physical presence in places in the real world, as well as in imaginary worlds.

The Virtuality Continuum is a continuous scale ranging between the completely virtual, a Virtuality, and the completely real: Reality. The reality-virtuality continuum therefore encompasses all possible variations and compositions of real and virtual objects. It has been described as a concept in new media and computer science, but in fact it could be considered a matter of anthropology. The concept was first introduced by Paul Milgram.

The area between the two extremes, where both the real and the virtual are mixed, is the so-called Mixed reality. This in turn is said to consist of both Augmented Reality, where the virtual augments the real, and Augmented virtuality, where the real augments the virtual.
Cyberspace, the world's computer systems considered as an interconnected whole, can be thought of as a virtual reality; for instance, it is portrayed as such in the cyberpunk fiction of William Gibson and others. Second life and MMORPGs such as "World of Warcraft" are examples of artificial environments or virtual worlds (falling some way short of full virtual reality) in cyberspace.

On the Internet, "real life" refers to life in the real world. It generally references life or consensus reality, in contrast to an environment seen as fiction or fantasy, such as virtual reality, lifelike experience, dreams, novels, or movies. Online, the acronym "IRL" stands for "in real life", with the meaning "not on the Internet". Sociologists engaged in the study of the Internet have determined that someday, a distinction between online and real-life worlds may seem "quaint", noting that certain types of online activity, such as sexual intrigues, have already made a full transition to complete legitimacy and "reality". The abbreviation "RL" stands for "real life". For example, one can speak of "meeting in RL" someone whom one has met in a chat or on an Internet forum. It may also be used to express an inability to use the Internet for a time due to "RL problems".






</doc>
<doc id="1666427" url="https://en.wikipedia.org/wiki?curid=1666427" title="Transcendence (philosophy)">
Transcendence (philosophy)

In philosophy, transcendence conveys the basic ground concept from the word's literal meaning (from Latin), of climbing or going beyond, albeit with varying connotations in its different historical and cultural stages. It includes philosophies, systems, and approaches that describe the fundamental structures of being, not as an ontology (theory of being), but as the framework of emergence and validation of knowledge of being. "Transcendental" is a word derived from the scholastic, designating the extra-categorical attributes of beings.

In religion, transcendence refers to the aspect of God's nature and power which is wholly independent of the material universe, beyond all physical laws. This is contrasted with immanence, where a god is said to be fully present in the physical world and thus accessible to creatures in various ways. In religious experience transcendence is a state of being that has overcome the limitations of physical existence and by some definitions has also become independent of it. This is typically manifested in prayer, séance, meditation, psychedelics and paranormal "visions".

It is affirmed in various religious traditions' concept of the divine, which contrasts with the notion of a god (or, the Absolute) that exists exclusively in the physical order (immanentism), or indistinguishable from it (pantheism). Transcendence can be attributed to the divine not only in its being, but also in its knowledge. Thus, God may transcend both the universe and knowledge (is beyond the grasp of the human mind).

Although transcendence is defined as the opposite of immanence, the two are not necessarily mutually exclusive. Some theologians and metaphysicians of various religious traditions affirm that a god is both within and beyond the universe (panentheism); in it, but not of it; simultaneously pervading it and surpassing it.

In modern philosophy, Immanuel Kant introduced a new term, "transcendental", thus instituting a new, third meaning. In his theory of knowledge, this concept is concerned with the condition of possibility of knowledge itself. He also opposed the term "transcendental" to the term "transcendent", the latter meaning "that which goes beyond" (transcends) any possible knowledge of a human being. For him "transcendental" meant knowledge about our cognitive faculty with regard to how objects are possible "a priori". "I call all knowledge "transcendental" if it is occupied, not with objects, but with the way that we can possibly know objects even before we experience them." Therefore, metaphysics, as a fundamental and universal theory, turns out to be an epistemology. Transcendental philosophy, consequently, is not considered a traditional ontological form of metaphysics.

Kant also equated "transcendental" with that which is "...in respect of the subject's faculty of cognition." Something is transcendental if it plays a role in the way in which the mind "constitutes" objects and makes it possible for us to experience them as objects in the first place. Ordinary knowledge is knowledge of objects; transcendental knowledge is knowledge of how it is possible for us to experience those objects as objects. This is based on Kant's acceptance of David Hume's argument that certain general features of objects (e.g. persistence, causal relationships) cannot be derived from the sense impressions we have of them. Kant argues that the mind must contribute those features and make it possible for us to experience objects as objects. In the central part of his "Critique of Pure Reason", the "Transcendental Deduction of the Categories", Kant argues for a deep interconnection between the ability to have consciousness of self and the ability to experience a world of objects. Through a process of synthesis, the mind generates both the structure of objects and its own unity.

A metaphilosophical question discussed by many Kantian scholars is how transcendental reflection is itself possible. Stephen Palmquist interprets Kant's appeal to faith as his most effective solution to this problem.

For Kant, the "transcendent", as opposed to the "transcendental", is that which lies beyond what our faculty of knowledge can legitimately know. Hegel's counter-argument to Kant was that to know a boundary is also to be aware of what it bounds and as such what lies beyond it – in other words, to have already transcended it.

In phenomenology, the "transcendent" is that which transcends our own consciousness: that which is objective rather than only a phenomenon of consciousness. Noema is employed in phenomenology to refer to the terminus of an intention as given for consciousness. 

Jean-Paul Sartre also speaks of transcendence in his works. In "Being and Nothingness", Sartre uses transcendence to describe the relation of the self to the object oriented world, as well as our concrete relations with others. For Sartre, the for-itself is sometimes called a transcendence. Additionally if the other is viewed strictly as an object, much like any other object, then the other is, for the for-itself, a transcendence-transcended. When the for-itself grasps the other in the others world, and grasps the subjectivity that the other has, it is referred to as transcending-transcendence. Thus, Sartre defines relations with others in terms of transcendence.

Contemporary transcendental philosophy is developed by German philosopher Harald Holz with a holistic approach. Holz liberated transcendental philosophy from the convergence of neo-Kantianism, he critically discussed transcendental pragmatism and the relation between transcendental philosophy, neo-empiricism and the so-called postmodernism.

In everyday language, "transcendence" means "going beyond", and "self-transcendence" means going beyond a prior form or state of oneself. Mystical experience is thought of as a particularly advanced state of self-transcendence, in which the sense of a separate self is abandoned. "Self-transcendence" is believed to be psychometrically measurable, and (at least partially) inherited, and has been incorporated as a personality dimension in the Temperament and Character Inventory. The discovery of this is described in the book "The God Gene" by Dean Hamer, although this has been criticized by commentators such as Carl Zimmer.





</doc>
<doc id="30746" url="https://en.wikipedia.org/wiki?curid=30746" title="Theory">
Theory

A theory is a contemplative and rational type of abstract or generalizing thinking about a phenomenon, or the results of such thinking. The process of contemplative and rational thinking often is associated with such processes like observational study, research. Theories may either be scientific or other than scientific (or scientific to less extent). Depending on the context, the results might, for example, include generalized explanations of how nature works. The word has its roots in ancient Greek, but in modern use it has taken on several related meanings.

In modern science, the term "theory" refers to scientific theories, a well-confirmed type of explanation of nature, made in a way consistent with scientific method, and fulfilling the criteria required by modern science. Such theories are described in such a way that scientific tests should be able to provide empirical support for, or empirically contradict ("falsify") it. Scientific theories are the most reliable, rigorous, and comprehensive form of scientific knowledge, in contrast to more common uses of the word "theory" that imply that something is unproven or speculative (which in formal terms is better characterized by the word "hypothesis"). Scientific theories are distinguished from hypotheses, which are individual empirically testable conjectures, and from scientific laws, which are descriptive accounts of the way nature behaves under certain conditions.

Theories guide the enterprise of finding facts rather than of reaching goals, and are neutral concerning alternatives among values. A theory can be a body of knowledge, which may or may not be associated with particular explanatory models. To theorize is to develop this body of knowledge.

The word theory or "in theory" is more or less often used erroneously by people to explain something which they individually did not experience or tested before. In those instances, semantically, it is being substituted for another concept, a hypothesis. Instead of using the word hypothetically, it is replaced by a phrase: "in theory". In some instances the theory's credibility could be contested by calling it "just a theory" (implying that the idea has not even been tested). Hence, that word "theory" is very often contrasted to "practice" (from Greek "", πρᾶξις) a Greek term for "doing", which is opposed to theory. A "classical example" of the distinction between "theoretical" and "practical" uses the discipline of medicine: medical theory involves trying to understand the causes and nature of health and sickness, while the practical side of medicine is trying to make people healthy. These two things are related but can be independent, because it is possible to research health and sickness without curing specific patients, and it is possible to cure a patient without knowing how the cure worked.

The English word "theory" derives from a technical term in philosophy in Ancient Greek. As an everyday word, "theoria", , meant "a looking at, viewing, beholding", but in more technical contexts it came to refer to contemplative or speculative understandings of natural things, such as those of natural philosophers, as opposed to more practical ways of knowing things, like that of skilled orators or artisans. English-speakers have used the word "theory" since at least the late 16th century. Modern uses of the word "theory" derive from the original definition, but have taken on new shades of meaning, still based on the idea of a theory as a thoughtful and rational explanation of the general nature of things.

Although it has more mundane meanings in Greek, the word apparently developed special uses early in the recorded history of the Greek language. In the book "From Religion to Philosophy", Francis Cornford suggests that the Orphics used the word "theoria" to mean "passionate sympathetic contemplation". Pythagoras changed the word to mean a passionate sympathetic contemplation of mathematical knowledge, because he considered this intellectual pursuit the way to reach the highest plane of existence. Pythagoras emphasized subduing emotions and bodily desires to help the intellect function at the higher plane of theory. Thus, it was Pythagoras who gave the word "theory" the specific meaning that led to the classical and modern concept of a distinction between theory (as uninvolved, neutral thinking) and practice.

Aristotle's terminology, as already mentioned, contrasts theory with "praxis" or practice, and this contrast exists till today. For Aristotle, both practice and theory involve thinking, but the aims are different. Theoretical contemplation considers things humans do not move or change, such as nature, so it has no human aim apart from itself and the knowledge it helps create. On the other hand, "praxis" involves thinking, but always with an aim to desired actions, whereby humans cause change or movement themselves for their own ends. Any human movement that involves no conscious choice and thinking could not be an example of "praxis" or doing.

Theories are analytical tools for understanding, explaining, and making predictions about a given subject matter. There are theories in many and varied fields of study, including the arts and sciences. A formal theory is syntactic in nature and is only meaningful when given a semantic component by applying it to some content (e.g., facts and relationships of the actual historical world as it is unfolding). Theories in various fields of study are expressed in natural language, but are always constructed in such a way that their general form is identical to a theory as it is expressed in the formal language of mathematical logic. Theories may be expressed mathematically, symbolically, or in common language, but are generally expected to follow principles of rational thought or logic.

Theory is constructed of a set of sentences that are entirely true statements about the subject under consideration. However, the truth of any one of these statements is always relative to the whole theory. Therefore, the same statement may be true with respect to one theory, and not true with respect to another. This is, in ordinary language, where statements such as "He is a terrible person" cannot be judged as true or false without reference to some interpretation of who "He" is and for that matter what a "terrible person" is under the theory.

Sometimes two theories have exactly the same explanatory power because they make the same predictions. A pair of such theories is called indistinguishable or observationally equivalent, and the choice between them reduces to convenience or philosophical preference.

The form of theories is studied formally in mathematical logic, especially in model theory. When theories are studied in mathematics, they are usually expressed in some formal language and their statements are closed under application of certain procedures called rules of inference. A special case of this, an axiomatic theory, consists of axioms (or axiom schemata) and rules of inference. A theorem is a statement that can be derived from those axioms by application of these rules of inference. Theories used in applications are abstractions of observed phenomena and the resulting theorems provide solutions to real-world problems. Obvious examples include arithmetic (abstracting concepts of number), geometry (concepts of space), and probability (concepts of randomness and likelihood).

Gödel's incompleteness theorem shows that no consistent, recursively enumerable theory (that is, one whose theorems form a recursively enumerable set) in which the concept of natural numbers can be expressed, can include all true statements about them. As a result, some domains of knowledge cannot be formalized, accurately and completely, as mathematical theories. (Here, formalizing accurately and completely means that all true propositions—and only true propositions—are derivable within the mathematical system.) This limitation, however, in no way precludes the construction of mathematical theories that formalize large bodies of scientific knowledge.

A theory is "underdetermined" (also called "indeterminacy of data to theory") if a rival, inconsistent theory is at least as consistent with the evidence. Underdetermination is an epistemological issue about the relation of evidence to conclusions.

A theory that lacks supporting evidence is generally, more properly, referred to as a hypothesis.

If a new theory better explains and predicts a phenomenon than an old theory (i.e., it has more explanatory power), we are justified in believing that the newer theory describes reality more correctly. This is called an "intertheoretic reduction" because the terms of the old theory can be reduced to the terms of the new one. For instance, our historical understanding about "sound", "light" and "heat" have been reduced to "wave compressions and rarefactions", "electromagnetic waves", and "molecular kinetic energy", respectively. These terms, which are identified with each other, are called "intertheoretic identities." When an old and new theory are parallel in this way, we can conclude that the new one describes the same reality, only more completely.

When a new theory uses new terms that do not reduce to terms of an older theory, but rather replace them because they misrepresent reality, it is called an "intertheoretic elimination." For instance, the obsolete scientific theory that put forward an understanding of heat transfer in terms of the movement of caloric fluid was eliminated when a theory of heat as energy replaced it. Also, the theory that phlogiston is a substance released from burning and rusting material was eliminated with the new understanding of the reactivity of oxygen.

Theories are distinct from theorems. A "theorem" is derived deductively from axioms (basic assumptions) according to a formal system of rules, sometimes as an end in itself and sometimes as a first step toward being tested or applied in a concrete situation; theorems are said to be true in the sense that the conclusions of a theorem are logical consequences of the axioms. "Theories" are abstract and conceptual, and are supported or challenged by observations in the world. They are 'rigorously tentative', meaning that they are proposed as true and expected to satisfy careful examination to account for the possibility of faulty inference or incorrect observation. Sometimes theories are incorrect, meaning that an explicit set of observations contradicts some fundamental objection or application of the theory, but more often theories are corrected to conform to new observations, by restricting the class of phenomena the theory applies to or changing the assertions made. An example of the former is the restriction of classical mechanics to phenomena involving macroscopic length scales and particle speeds much lower than the speed of light.

In science, the term "theory" refers to "a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment." Theories must also meet further requirements, such as the ability to make falsifiable predictions with consistent accuracy across a broad area of scientific inquiry, and production of strong evidence in favor of the theory from multiple independent sources (consilience).

The strength of a scientific theory is related to the diversity of phenomena it can explain, which is measured by its ability to make falsifiable predictions with respect to those phenomena. Theories are improved (or replaced by better theories) as more evidence is gathered, so that accuracy in prediction improves over time; this increased accuracy corresponds to an increase in scientific knowledge. Scientists use theories as a foundation to gain further scientific knowledge, as well as to accomplish goals such as inventing technology or curing diseases.

The United States National Academy of Sciences defines scientific theories as follows:The formal scientific definition of "theory" is quite different from the everyday meaning of the word. It refers to a comprehensive explanation of some aspect of nature that is supported by a vast body of evidence. Many scientific theories are so well established that no new evidence is likely to alter them substantially. For example, no new evidence will demonstrate that the Earth does not orbit around the sun (heliocentric theory), or that living things are not made of cells (cell theory), that matter is not composed of atoms, or that the surface of the Earth is not divided into solid plates that have moved over geological timescales (the theory of plate tectonics) ... One of the most useful properties of scientific theories is that they can be used to make predictions about natural events or phenomena that have not yet been observed.

From the American Association for the Advancement of Science:
A scientific theory is a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment. Such fact-supported theories are not "guesses" but reliable accounts of the real world. The theory of biological evolution is more than "just a theory." It is as factual an explanation of the universe as the atomic theory of matter or the germ theory of disease. Our understanding of gravity is still a work in progress. But the phenomenon of gravity, like evolution, is an accepted fact.

The term "theory" is not appropriate for describing scientific models or untested, but intricate hypotheses.

The logical positivists thought of scientific theories as "deductive theories"—that a theory's content is based on some formal system of logic and on basic axioms. In a deductive theory, any sentence which is a logical consequence of one or more of the axioms is also a sentence of that theory. This is called the received view of theories.

In the semantic view of theories, which has largely replaced the received view, theories are viewed as scientific models. A model is a logical framework intended to represent reality (a "model of reality"), similar to the way that a map is a graphical model that represents the territory of a city or country. In this approach, theories are a specific category of models that fulfill the necessary criteria. (See Theories as models for further discussion.)

In physics the term "theory" is generally used for a mathematical framework—derived from a small set of basic postulates (usually symmetries, like equality of locations in space or in time, or identity of electrons, etc.)—which is capable of producing experimental predictions for a given category of physical systems. One good example is classical electromagnetism, which encompasses results derived from gauge symmetry (sometimes called gauge invariance) in a form of a few equations called Maxwell's equations. The specific mathematical aspects of classical electromagnetic theory are termed "laws of electromagnetism", reflecting the level of consistent and reproducible evidence that supports them. Within electromagnetic theory generally, there are numerous hypotheses about how electromagnetism applies to specific situations. Many of these hypotheses are already considered adequately tested, with new ones always in the making and perhaps untested.

Acceptance of a theory does not require that all of its major predictions be tested, if it is already supported by sufficiently strong evidence. For example, certain tests may be infeasible or technically difficult. As a result, theories may make predictions that have not yet been confirmed or proven incorrect; in this case, the predicted results may be described informally using the term "theoretical." These predictions can be tested at a later time, and if they are incorrect, this may lead to revision, invalidation, or rejection of the theory.
In mathematics the use of the term "theory" is different, necessarily so, since mathematics contains no explanations of natural phenomena, "per se", even though it may help provide insight into natural systems or be inspired by them. In the general sense, a mathematical "theory" is a branch of or topic in mathematics, such as Set theory, Number theory, Group theory, Probability theory, Game theory, Control theory, Perturbation theory, etc., such as might be appropriate for a single textbook.

In the same sense, but more specifically, the word "theory" is an extensive, structured collection of theorems, organized so that the proof of each theorem only requires the theorems and axioms that preceded it (no circular proofs), occurs as early as feasible in sequence (no postponed proofs), and the whole is as succinct as possible (no redundant proofs). Ideally, the sequence in which the theorems are presented is as easy to understand as possible, although illuminating a branch of mathematics is the purpose of textbooks, rather than the mathematical theory they might be written to cover.

A theory can be either "descriptive" as in science, or "prescriptive" (normative) as in philosophy. The latter are those whose subject matter consists not of empirical data, but rather of ideas. At least some of the elementary theorems of a philosophical theory are statements whose truth cannot necessarily be scientifically tested through empirical observation.

A field of study is sometimes named a "theory" because its basis is some initial set of assumptions describing the field's approach to the subject. These assumptions are the elementary theorems of the particular theory, and can be thought of as the axioms of that field. Some commonly known examples include set theory and number theory; however literary theory, critical theory, and music theory are also of the same form.

One form of philosophical theory is a "metatheory" or "meta-theory". A metatheory is a theory whose subject matter is some other theory or set of theories. In other words, it is a theory about theories. Statements made in the metatheory about the theory are called metatheorems.

A political theory is an ethical theory about the law and government. Often the term "political theory" refers to a general view, or specific ethic, political belief or attitude, about politics.

In social science, jurisprudence is the philosophical theory of law. Contemporary philosophy of law addresses problems internal to law and legal systems, and problems of law as a particular social institution.

Most of the following are scientific theories. Some are not, but rather encompass a body of knowledge or art, such as Music theory and Visual Arts Theories.





</doc>
<doc id="19378" url="https://en.wikipedia.org/wiki?curid=19378" title="Mind">
Mind

The mind is the set of cognitive faculties including consciousness, imagination, perception, thinking, judgement, language and memory, which is housed in the brain (sometimes including the central nervous system). It is usually defined as the faculty of an entity's thoughts and consciousness. It holds the power of imagination, recognition, and appreciation, and is responsible for processing feelings and emotions, resulting in attitudes and actions.

There is a lengthy tradition in philosophy, religion, psychology, and cognitive science about what constitutes a mind and what are its distinguishing properties.

One open question regarding the nature of the mind is the mind–body problem, which investigates the relation of the mind to the physical brain and nervous system. Older viewpoints included dualism and idealism, which considered the mind somehow non-physical. Modern views often center around physicalism and functionalism, which hold that the mind is roughly identical with the brain or reducible to physical phenomena such as neuronal activity, though dualism and idealism continue to have many supporters. Another question concerns which types of beings are capable of having minds (New Scientist 8 September 2018 p10). For example, whether mind is exclusive to humans, possessed also by some or all animals, by all living things, whether it is a strictly definable characteristic at all, or whether mind can also be a property of some types of human-made machines.

Whatever its nature, it is generally agreed that mind is that which enables a being to have subjective awareness and intentionality towards their environment, to perceive and respond to stimuli with some kind of agency, and to have consciousness, including thinking and feeling.

The concept of mind is understood in many different ways by many different cultural and religious traditions. Some see mind as a property exclusive to humans whereas others ascribe properties of mind to non-living entities (e.g. panpsychism and animism), to animals and to deities. Some of the earliest recorded speculations linked mind (sometimes described as identical with soul or spirit) to theories concerning both life after death, and cosmological and natural order, for example in the doctrines of Zoroaster, the Buddha, Plato, Aristotle, and other ancient Greek, Indian and, later, Islamic and medieval European philosophers.

Important philosophers of mind include Plato, Patanjali, Descartes, Leibniz, Locke, Berkeley, Hume, Kant, Hegel, Schopenhauer, Searle, Dennett, Fodor, Nagel, and Chalmers. Psychologists such as Freud and James, and computer scientists such as Turing and Putnam developed influential theories about the nature of the mind. The possibility of nonbiological minds is explored in the field of artificial intelligence, which works closely in relation with cybernetics and information theory to understand the ways in which information processing by nonbiological machines is comparable or different to mental phenomena in the human mind.

The mind is also portrayed as the stream of consciousness where sense impressions and mental phenomena are constantly changing.

The original meaning of Old English "gemynd" was the faculty of memory, not of thought in general. Hence "call to mind", "come to mind", "keep in mind", "to have mind of", etc. The word retains this sense in Scotland. Old English had other words to express "mind", such as "hyge" "mind, spirit".

The meaning of "memory" is shared with Old Norse, which has "munr". The word is originally from a PIE verbal root "", meaning "to think, remember", whence also Latin" mens" "mind", Sanskrit "" "mind" and Greek μένος "mind, courage, anger".

The generalization of "mind" to include all mental faculties, thought, volition, feeling and memory, gradually develops over the 14th and 15th centuries.

The attributes that make up the mind are debated. Some psychologists argue that only the "higher" intellectual functions constitute mind, particularly reason and memory. In this view the emotions — love, hate, fear, and joy — are more "primitive "or subjective in nature and should be seen as different from the mind as such. Others argue that various rational and emotional states cannot be so separated, that they are of the same nature and origin, and should therefore be considered all part of it as mind.

In popular usage, "mind "is frequently synonymous with "thought": the private conversation with ourselves that we carry on "inside our heads." Thus we "make up our minds," "change our minds" or are "of two minds" about something. One of the key attributes of the mind in this sense is that it is a private sphere to which no one but the owner has access. No one else can "know our mind." They can only interpret what we consciously or unconsciously communicate.

Broadly speaking, mental faculties are the various functions of the mind, or things the mind can "do".

Thought is a mental act that allows humans to make sense of things in the world, and to represent and interpret them in ways that are significant, or which accord with their needs, attachments, goals, commitments, plans, ends, desires, etc. Thinking involves the symbolic or semiotic mediation of ideas or data, as when we form concepts, engage in problem solving, reasoning, and making decisions. Words that refer to similar concepts and processes include deliberation, cognition, ideation, discourse and imagination.

Thinking is sometimes described as a "higher" cognitive function and the analysis of thinking processes is a part of cognitive psychology. It is also deeply connected with our capacity to make and use tools; to understand cause and effect; to recognize patterns of significance; to comprehend and disclose unique contexts of experience or activity; and to respond to the world in a meaningful way.

Memory is the ability to preserve, retain, and subsequently recall, knowledge, information or experience. Although memory has traditionally been a persistent theme in philosophy, the late nineteenth and early twentieth centuries also saw the study of memory emerge as a subject of inquiry within the paradigms of cognitive psychology. In recent decades, it has become one of the pillars of a new branch of science called cognitive neuroscience, a marriage between cognitive psychology and neuroscience.

Imagination is the activity of generating or evoking novel situations, images, ideas or other qualia in the mind. It is a characteristically subjective "activity", rather than a direct or passive experience. The term is technically used in psychology for the process of reviving in the mind percepts of objects formerly given in sense perception. Since this use of the term conflicts with that of ordinary language, some psychologists have preferred to describe this process as "imaging" or "imagery" or to speak of it as "reproductive" as opposed to "productive" or "constructive" imagination. Things imagined are said to be seen in the "mind's eye". Among the many practical functions of imagination are the ability to project possible futures (or histories), to "see" things from another's perspective, and to change the way something is perceived, including to make decisions to respond to, or enact, what is imagined.

Consciousness in mammals (this includes humans) is an aspect of the mind generally thought to comprise qualities such as subjectivity, sentience, and the ability to perceive the relationship between oneself and one's environment. It is a subject of much research in philosophy of mind, psychology, neuroscience, and cognitive science. Some philosophers divide consciousness into phenomenal consciousness, which is subjective experience itself, and access consciousness, which refers to the global availability of information to processing systems in the brain. Phenomenal consciousness has many different experienced qualities, often referred to as qualia. Phenomenal consciousness is usually consciousness "of" something or "about" something, a property known as intentionality in philosophy of mind.

Mental contents are those items that are thought of as being "in" the mind, and capable of being formed and manipulated by mental processes and faculties. Examples include thoughts, concepts, memories, emotions, percepts and intentions. Philosophical theories of mental content include internalism, externalism, representationalism and intentionality.

Memetics is a theory of mental content based on an analogy with Darwinian evolution, which was originated by Richard Dawkins and Douglas Hofstadter in the 1980s. It is an evolutionary model of cultural information transfer. A meme, analogous to a gene, is an idea, belief, pattern of behaviour (etc.) "hosted" in one or more individual minds, and can reproduce itself from mind to mind. Thus what would otherwise be regarded as one individual influencing another to adopt a belief, is seen memetically as a meme reproducing itself.

In animals, the brain, or "encephalon" (Greek for "in the head"), is the control center of the central nervous system, responsible for thought. In most animals, the brain is located in the head, protected by the skull and close to the primary sensory apparatus of vision, hearing, equilibrioception, taste and olfaction. While all vertebrates have a brain, most invertebrates have either a centralized brain or collections of individual ganglia. Primitive animals such as sponges do not have a brain at all. Brains can be extremely complex. For example, the human brain contains around 86 billion neurons, each linked to as many as 10,000 others.

Understanding the relationship between the brain and the mind – mind–body problem is one of the central issues in the history of philosophy – is a challenging problem both philosophically and scientifically. There are three major philosophical schools of thought concerning the answer: dualism, materialism, and idealism. Dualism holds that the mind exists independently of the brain; materialism holds that mental phenomena are identical to neuronal phenomena; and idealism holds that only mental phenomena exist.

Through most of history many philosophers found it inconceivable that cognition could be implemented by a physical substance such as brain tissue (that is neurons and synapses). Descartes, who thought extensively about mind-brain relationships, found it possible to explain reflexes and other simple behaviors in mechanistic terms, although he did not believe that complex thought, and language in particular, could be explained by reference to the physical brain alone.

The most straightforward scientific evidence of a strong relationship between the physical brain matter and the mind is the impact physical alterations to the brain have on the mind, such as with traumatic brain injury and psychoactive drug use. Philosopher Patricia Churchland notes that this drug-mind interaction indicates an intimate connection between the brain and the mind.

In addition to the philosophical questions, the relationship between mind and brain involves a number of scientific questions, including understanding the relationship between mental activity and brain activity, the exact mechanisms by which drugs influence cognition, and the neural correlates of consciousness.

Theoretical approaches to explain how mind emerges from the brain include connectionism, computationalism and Bayesian brain.

The evolution of human intelligence refers to several theories that aim to describe how human intelligence has evolved in relation to the evolution of the human brain and the origin of language.

The timeline of human evolution spans some 7 million years, from the separation of the genus "Pan" until the emergence of behavioral modernity by 50,000 years ago. Of this timeline, the first 3 million years concern "Sahelanthropus", the following 2 million concern "Australopithecus", while the final 2 million span the history of actual "Homo" species (the Paleolithic).

Many traits of human intelligence, such as empathy, theory of mind, mourning, ritual, and the use of symbols and tools, are already apparent in great apes although in lesser sophistication than in humans.

There is a debate between supporters of the idea of a sudden emergence of intelligence, or "Great leap forward" and those of a gradual or continuum hypothesis.

Theories of the evolution of intelligence include:

Philosophy of mind is the branch of philosophy that studies the nature of the mind, mental events, mental functions, mental properties, consciousness and their relationship to the physical body. The "mind–body problem", i.e. the relationship of the mind to the body, is commonly seen as the central issue in philosophy of mind, although there are other issues concerning the nature of the mind that do not involve its relation to the physical body. José Manuel Rodriguez Delgado writes, "In present popular usage, soul and mind are not clearly differentiated and some people, more or less consciously, still feel that the soul, and perhaps the mind, may enter or leave the body as independent entities."

"Dualism" and "monism" are the two major schools of thought that attempt to resolve the mind–body problem. Dualism is the position that mind and body are in some way separate from each other. It can be traced back to Plato, Aristotle and the Nyaya, Samkhya and Yoga schools of Hindu philosophy, but it was most precisely formulated by René Descartes in the 17th century. "Substance dualists" argue that the mind is an independently existing substance, whereas "Property dualists" maintain that the mind is a group of independent properties that emerge from and cannot be reduced to the brain, but that it is not a distinct substance.

The 20th century philosopher Martin Heidegger suggested that subjective experience and activity (i.e. the "mind") cannot be made sense of in terms of Cartesian "substances" that bear "properties" at all (whether the mind itself is thought of as a distinct, separate kind of substance or not). This is because the nature of subjective, "qualitative" experience is incoherent in terms of – or semantically incommensurable with the concept of – substances that bear properties. This is a fundamentally ontological argument.

The philosopher of cognitive science Daniel Dennett, for example, argues there is no such thing as a narrative center called the "mind", but that instead there is simply a collection of sensory inputs and outputs: different kinds of "software" running in parallel. Psychologist B.F. Skinner argued that the mind is an explanatory fiction that diverts attention from environmental causes of behavior; he considered the mind a "black box" and thought that mental processes may be better conceived of as forms of covert verbal behavior.

Philosopher David Chalmers has argued that the third person approach to uncovering mind and consciousness is not effective, such as looking into other's brains or observing human conduct, but that a first person approach is necessary. Such a first person perspective indicates that the mind must be conceptualized as something distinct from the brain.

The mind has also been described as manifesting from moment to moment, one thought moment at a time as a fast flowing stream, where sense impressions and mental phenomena are constantly changing.

"Monism" is the position that mind and body are not physiologically and ontologically distinct kinds of entities. This view was first advocated in Western Philosophy by Parmenides in the 5th Century BC and was later espoused by the 17th Century rationalist Baruch Spinoza. According to Spinoza's dual-aspect theory, mind and body are two aspects of an underlying reality which he variously described as "Nature" or "God".

The most common monisms in the 20th and 21st centuries have all been variations of physicalism; these positions include behaviorism, the type identity theory, anomalous monism and functionalism.

Many modern philosophers of mind adopt either a "reductive" or "non-reductive physicalist" position, maintaining in their different ways that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, e.g. in the fields of sociobiology, computer science, evolutionary psychology and the various neurosciences. Other philosophers, however, adopt a non-physicalist position which challenges the notion that the mind is a purely physical construct.

Continued progress in neuroscience has helped to clarify many of these issues, and its findings have been taken by many to support physicalists' assertions. Nevertheless, our knowledge is incomplete, and modern philosophers of mind continue to discuss how subjective qualia and the intentional mental states can be naturally explained. Then, of course, there is the problem of Quantum Mechanics, which is best understood as a form of perspectivism.

Neuroscience studies the nervous system, the physical basis of the mind. At the systems level, neuroscientists investigate how biological neural networks form and physiologically interact to produce mental functions and content such as reflexes, multisensory integration, motor coordination, circadian rhythms, emotional responses, learning, and memory. The underlying physical basis of learning and memory is likely dynamic changes in gene expression that occur in brain neurons. Such expression changes are introduced by epigenetic mechanisms. Epigenetic regulation of gene expression ordinarily involves chemical modification of DNA or DNA-associated histone proteins. Such chemical modifications can cause long-lasting changes in gene expression. Epigenetic mechanisms employed in learning and memory include the methylation and demethylation of neuronal DNA as well as methylation, acetylation and deacetylation of neuronal histone proteins. 

At a larger scale, efforts in computational neuroscience have developed large-scale models that simulate simple, functioning brains. As of 2012, such models include the thalamus, basal ganglia, prefrontal cortex, motor cortex, and occipital cortex, and consequentially simulated brains can learn, respond to visual stimuli, coordinate motor responses, form short-term memories, and learn to respond to patterns. Currently, researchers aim to program the hippocampus and limbic system, hypothetically imbuing the simulated mind with long-term memory and crude emotions.

By contrast, affective neuroscience studies the neural mechanisms of personality, emotion, and mood primarily through experimental tasks.

Cognitive science examines the mental functions that give rise to information processing, termed cognition. These include perception, attention, working memory, long-term memory, producing and understanding language, learning, reasoning, problem solving, and decision making. Cognitive science seeks to understand thinking "in terms of representational structures in the mind and computational procedures that operate on those structures".

Psychology is the scientific study of human behavior, mental functioning, and experience. As both an academic and applied discipline, Psychology involves the scientific study of mental processes such as perception, cognition, emotion, personality, as well as environmental influences, such as social and cultural influences, and interpersonal relationships, in order to devise theories of human behavior. Psychological patterns can be understood as low cost ways of information processing. Psychology also refers to the application of such knowledge to various spheres of human activity, including problems of individuals' daily lives and the treatment of mental health problems.

Psychology differs from the other social sciences (e.g. anthropology, economics, political science, and sociology) due to its focus on experimentation at the scale of the individual, or individuals in small groups as opposed to large groups, institutions or societies. Historically, psychology differed from biology and neuroscience in that it was primarily concerned with mind rather than brain. Modern psychological science incorporates physiological and neurological processes into its conceptions of perception, cognition, behaviour, and mental disorders.

By analogy with the health of the body, one can speak metaphorically of a state of health of the mind, or mental health. Merriam-Webster defines mental health as "A state of emotional and psychological well-being in which an individual is able to use his or her cognitive and emotional capabilities, function in society, and meet the ordinary demands of everyday life." According to the World Health Organization (WHO), there is no one "official" definition of mental health. Cultural differences, subjective assessments, and competing professional theories all affect how "mental health" is defined. In general, most experts agree that "mental health" and "mental disorder" are not opposites. In other words, the absence of a recognized mental disorder is not necessarily an indicator of mental health.

One way to think about mental health is by looking at how effectively and successfully a person functions. Feeling capable and competent; being able to handle normal levels of stress, maintaining satisfying relationships, and leading an independent life; and being able to "bounce back," or recover from difficult situations, are all signs of mental health.

Psychotherapy is an interpersonal, relational intervention used by trained psychotherapists to aid clients in problems of living. This usually includes increasing individual sense of well-being and reducing subjective discomforting experience. Psychotherapists employ a range of techniques based on experiential relationship building, dialogue, communication and behavior change and that are designed to improve the mental health of a client or patient, or to improve group relationships (such as in a family). Most forms of psychotherapy use only spoken conversation, though some also use various other forms of communication such as the written word, art, drama, narrative story, or therapeutic touch. Psychotherapy occurs within a structured encounter between a trained therapist and client(s). Purposeful, theoretically based psychotherapy began in the 19th century with psychoanalysis; since then, scores of other approaches have been developed and continue to be created.

Animal cognition, or cognitive ethology, is the title given to a modern approach to the mental capacities of animals. It has developed out of comparative psychology, but has also been strongly influenced by the approach of ethology, behavioral ecology, and evolutionary psychology. Much of what used to be considered under the title of "animal intelligence" is now thought of under this heading. Animal language acquisition, attempting to discern or understand the degree to which animal cognition can be revealed by linguistics-related study, has been controversial among cognitive linguists.

 In 1950 Alan M. Turing published "Computing machinery and intelligence" in "Mind", in which he proposed that machines could be tested for intelligence using questions and answers. This process is now named the Turing Test. The term Artificial Intelligence (AI) was first used by John McCarthy who considered it to mean "the science and engineering of making intelligent machines". It can also refer to intelligence as exhibited by an artificial ("man-made", "non-natural", "manufactured") entity. AI is studied in overlapping fields of computer science, psychology, neuroscience and engineering, dealing with intelligent behavior, learning and adaptation and usually developed using customized machines or computers.

Research in AI is concerned with producing machines to automate tasks requiring intelligent behavior. Examples include control, planning and scheduling, the ability to answer diagnostic and consumer questions, handwriting, natural language, speech and facial recognition. As such, the study of AI has also become an engineering discipline, focused on providing solutions to real life problems, knowledge mining, software applications, strategy games like computer chess and other video games. One of the biggest limitations of AI is in the domain of actual machine comprehension. Consequentially natural language understanding and connectionism (where behavior of neural networks is investigated) are areas of active research and development.

The debate about the nature of the mind is relevant to the development of artificial intelligence. If the mind is indeed a thing separate from or higher than the functioning of the brain, then hypothetically it would be much more difficult to recreate within a machine, if it were possible at all. If, on the other hand, the mind is no more than the aggregated functions of the brain, then it will be possible to create a machine with a recognisable mind (though possibly only with computers much different from today's), by simple virtue of the fact that such a machine already exists in the form of the human brain.

Many religions associate spiritual qualities to the human mind. These are often tightly connected to their mythology and ideas of afterlife.

The Indian philosopher-sage Sri Aurobindo attempted to unite the Eastern and Western psychological traditions with his integral psychology, as have many philosophers and New religious movements. Judaism teaches that "moach shalit al halev", the mind rules the heart. Humans can approach the Divine intellectually, through learning and behaving according to the Divine Will as enclothed in the Torah, and use that deep logical understanding to elicit and guide emotional arousal during prayer. Christianity has tended to see the mind as distinct from the soul (Greek "nous") and sometimes further distinguished from the spirit. Western esoteric traditions sometimes refer to a mental body that exists on a plane other than the physical. Hinduism's various philosophical schools have debated whether the human soul (Sanskrit "atman") is distinct from, or identical to, "Brahman", the divine reality. Taoism sees the human being as contiguous with natural forces, and the mind as not separate from the body. Confucianism sees the mind, like the body, as inherently perfectible.

Buddhist teachings explain the moment-to-moment manifestation of the mind-stream. The components that make up the mind are known as the five aggregates (i.e., material form, feelings, perception, volition, and sensory consciousness), which arise and pass away continuously. The arising and passing of these aggregates in the present moment is described as being influenced by five causal laws: biological laws, psychological laws, physical laws, volitional laws, and universal laws. The Buddhist practice of mindfulness involves attending to this constantly changing mind-stream.

According to Buddhist philosopher Dharmakirti, the mind has two fundamental qualities: "clarity and cognizes". If something is not those two qualities, it cannot validly be called mind. "Clarity" refers to the fact that mind has no color, shape, size, location, weight, or any other physical characteristic, and "cognizes" that it functions to know or perceive objects. "Knowing" refers to the fact that mind is aware of the contents of experience, and that, in order to exist, mind must be cognizing an object. You cannot have a mind – whose function is to cognize an object – existing without cognizing an object.

Mind, in Buddhism, is also described as being "space-like" and "illusion-like". Mind is space-like in the sense that it is not physically obstructive. It has no qualities which would prevent it from existing. In Mahayana Buddhism, mind is illusion-like in the sense that it is empty of inherent existence. This does not mean it does not exist, it means that it exists in a manner that is counter to our ordinary way of misperceiving how phenomena exist, according to Buddhism. When the mind is itself cognized properly, without misperceiving its mode of existence, it appears to exist like an illusion. There is a big difference however between being "space and illusion" and being "space-like" and "illusion-like". Mind is not composed of space, it just shares some descriptive similarities to space. Mind is not an illusion, it just shares some descriptive qualities with illusions.

Buddhism posits that there is no inherent, unchanging identity (Inherent I, Inherent Me) or phenomena (Ultimate self, inherent self, Atman, Soul, Self-essence, Jiva, Ishvara, humanness essence, etc.) which is the experiencer of our experiences and the agent of our actions. In other words, human beings consist of merely a body and a mind, and nothing extra. Within the body there is no part or set of parts which is – by itself or themselves – the person. Similarly, within the mind there is no part or set of parts which are themselves "the person". A human being merely consists of five aggregates, or "skandhas" and nothing else.

In the same way, "mind" is what can be validly conceptually labelled onto our mere experience of clarity and knowing. There is something separate and apart from clarity and knowing which is "Awareness", in Buddhism. "Mind" is that part of experience the sixth sense door, which can be validly referred to as mind by the concept-term "mind". There is also not "objects out there, mind in here, and experience somewhere in-between". There is a third thing called "awareness" which exists being aware of the contents of mind and what mind cognizes. There are five senses (arising of mere experience: shapes, colors, the components of smell, components of taste, components of sound, components of touch) and mind as the sixth institution; this means, expressly, that there can be a third thing called "awareness" and a third thing called "experiencer who is aware of the experience". This awareness is deeply related to "no-self" because it does not judge the experience with craving or aversion.

Clearly, the experience arises and is known by mind, but there is a third thing calls Sati what is the "real experiencer of the experience" that sits apart from the experience and which can be aware of the experience in 4 levels. (Maha Sathipatthana Sutta.)

To be aware of these four levels one needs to cultivate equanimity toward Craving and Aversion. This is Called Vipassana which is different from the way of reacting with Craving and Aversion. This is the state of being aware and equanimous to the complete experience of here and now. This is the way of Buddhism, with regards to mind and the ultimate nature of minds (and persons).

Due to the mind–body problem, a lot of interest and debate surrounds the question of what happens to one's conscious mind as one's body dies. During brain death all brain function permanently ceases. According to some neuroscientific views which see these processes as the physical basis of mental phenomena, the mind fails to survive brain death and ceases to exist. This permanent loss of consciousness after death is sometimes called "eternal oblivion". The belief that some spiritual or incorporeal component (soul) exists and that it is preserved after death is described by the term "afterlife".

Parapsychology is a study of certain types of paranormal phenomena, or of phenomena which appear to be paranormal or not have any scientific basis , for instance, precognition, telekinesis and telepathy.

The term is based on the Greek para (beside/beyond), psyche (soul/mind), and logos (account/explanation) and was coined by psychologist Max Dessoir in or before 1889. J.B. Rhine tried to popularize "parapsychology" using fraudulent techniques as a replacement for the earlier term "psychical research", during a shift in methodologies which brought experimental methods to the study of psychic phenomena. Parapsychology is not accepted among the scientific community as science, as psychic abilities have not been demonstrated to exist. The status of parapsychology as a science has also been disputed, with many scientists regarding the discipline as pseudoscience.




</doc>
<doc id="630795" url="https://en.wikipedia.org/wiki?curid=630795" title="Ecstasy (emotion)">
Ecstasy (emotion)

Ecstasy (from Ancient Greek ἔκστασις "ékstasis", meaning 'outside of oneself') is a subjective experience of total involvement of the subject, with an object of their awareness. In classical Greek literature it refers to removal of the mind or body "from its normal place of function."

Total involvement with an object of interest is not an ordinary experience because of being aware of other objects, thus ecstasy is an example of an altered state of consciousness characterized by diminished awareness of other objects or the total lack of the awareness of surroundings and everything around the object. The word is also used to refer to any heightened state of consciousness or intensely pleasant experience. It is also used more specifically to denote states of awareness of non-ordinary mental spaces, which may be perceived as spiritual (the latter type of ecstasy often takes the form of religious ecstasy).

From a psychological perspective, ecstasy is a loss of self-control and sometimes a temporary loss of consciousness, which is often associated with religious mysticism, sexual intercourse and the use of certain drugs.
For the duration of the ecstasy the ecstatic is out of touch with ordinary life and is capable neither of communication with other people nor of undertaking normal actions. The experience can be brief in physical time, or it can go on for hours. Subjective perception of time, space or self may strongly change or disappear during ecstasy. For instance, if one is concentrating on a physical task, then any intellectual thoughts may cease. On the other hand, making a spirit journey in an ecstatic trance involves the cessation of voluntary bodily movement.

Ecstasy can be deliberately induced using religious or creative activities, meditation, music, dancing, breathing exercises, physical exercise, sexual intercourse or consumption of psychotropic drugs. The particular technique that an individual uses to induce ecstasy is usually also associated with that individual's particular religious and cultural traditions. Sometimes an ecstatic experience takes place due to occasional contact with something or somebody perceived as extremely beautiful or holy, or without any known reason. "In some cases, a person might obtain an ecstatic experience 'by mistake'. Maybe the person unintentionally triggers one of the, probably many, physiological mechanisms through which such an experience can be reached. In such cases, it is not rare to find that the person later, by reading, looks for an interpretation and maybe finds it within a tradition."

People interpret the experience afterward according to their culture and beliefs (as a revelation from God, a trip to the world of spirits or a psychotic episode). "When a person is using an ecstasy technique, he usually does so within a tradition. When he reaches an experience, a traditional interpretation of it already exists." The experience together with its subsequent interpretation may strongly and permanently change the value system and the worldview of the subject (e.g. to cause religious conversion).

In 1925, James Leuba wrote: "Among most uncivilized populations, as among civilized peoples, certain ecstatic conditions are regarded as divine possession or as union with the Divine. These states are induced by means of drugs, by physical excitement, or by psychical means. But, however produced and at whatever level of culture they may be found, they possess certain common features which suggest even to the superficial observer some profound connection. Always described as delightful beyond expression, these awesome ecstatic experiences end commonly in mental quiescence or even in total unconsciousness." He prepares his readers "... to recognize a continuity of impulse, of purpose, of form and of result between the ecstatic intoxication of the savage and the absorption in God of the Christian mystic."

"In everyday language, the word 'ecstasy' denotes an intense, euphoric experience. For obvious reasons, it is rarely used in a scientific context; it is a concept that is extremely hard to define."




</doc>
<doc id="1134" url="https://en.wikipedia.org/wiki?curid=1134" title="Analysis">
Analysis

Analysis is the process of breaking a complex topic or substance into smaller parts in order to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384–322 B.C.), though "analysis" as a formal concept is a relatively recent development.

The word comes from the Ancient Greek ἀνάλυσις ("análisis", "a breaking-up" or "an untying;" from "ana-" "up, throughout" and "lisis" "a loosening").

As a formal concept, the method has variously been ascribed to Alhazen, René Descartes ("Discourse on the Method"), and Galileo Galilei. It has also been ascribed to Isaac Newton, in the form of a practical method of physical discovery (which he did not name).

The field of chemistry uses analysis in at least three ways: to identify the components of a particular chemical compound (qualitative analysis), to identify the proportions of components in a mixture (quantitative analysis), and to break down chemical processes and examine chemical reactions between elements of matter. For an example of its use, analysis of the concentration of elements is important in managing a nuclear reactor, so nuclear scientists will analyse neutron activation to develop discrete measurements within vast samples. A matrix can have a considerable effect on the way a chemical analysis is conducted and the quality of its results. Analysis can be done manually or with a device. Chemical analysis is an important element of national security among the major world powers with materials

Types Of Analysis:

A) Qualitative Analysis: It is concerned with which components are in a given sample or compound.

Example: Precipitation reaction

B) Quantitative Analysis: It is to determine the quantity of individual component present in a given sample or compound.

Example:To find concentration by uv-spectrophotometer.

Chemists can use isotope analysis to assist analysts with issues in anthropology, archeology, food chemistry, forensics, geology, and a host of other questions of physical science. Analysts can discern the origins of natural and man-made isotopes in the study of environmental radioactivity.




Analysts in the field of engineering look at requirements, structures, mechanisms, systems and dimensions. Electrical engineers analyse systems in electronics. Life cycles and system failures are broken down and studied by engineers. It is also looking at different factors incorporated within the design.

The field of intelligence employs analysts to break down and understand a wide array of questions. Intelligence agencies may use heuristics, inductive and deductive reasoning, social network analysis, dynamic network analysis, link analysis, and brainstorming to sort through problems they face. Military intelligence may explore issues through the use of game theory, Red Teaming, and wargaming. Signals intelligence applies cryptanalysis and frequency analysis to break codes and ciphers. Business intelligence applies theories of competitive intelligence analysis and competitor analysis to resolve questions in the marketplace. Law enforcement intelligence applies a number of theories in crime analysis.

Linguistics looks at individual languages and language in general. It breaks language down and analyses its component parts: theory, sounds and their meaning, utterance usage, word origins, the history of words, the meaning of words and word combinations, sentence construction, basic construction beyond the sentence level, stylistics, and conversation. It examines the above using statistics and modeling, and semantics. It analyses language in context of anthropology, biology, evolution, geography, history, neurology, psychology, and sociology. It also takes the applied approach, looking at individual language development and clinical issues.

Literary criticism is the analysis of literature. The focus can be as diverse as the analysis of Homer or Freud. While not all literary-critical methods are primarily analytical in nature, the main approach to the teaching of literature in the west since the mid-twentieth century, literary formal analysis or close reading, is. This method, rooted in the academic movement labelled The New Criticism, approaches texts – chiefly short poems such as sonnets, which by virtue of their small size and significant complexity lend themselves well to this type of analysis – as units of discourse that can be understood in themselves, without reference to biographical or historical frameworks. This method of analysis breaks up the text linguistically in a study of prosody (the formal analysis of meter) and phonic effects such as alliteration and rhyme, and cognitively in examination of the interplay of syntactic structures, figurative language, and other elements of the poem that work to produce its larger effects.

Modern mathematical analysis is the study of infinite processes. It is the branch of mathematics that includes calculus. It can be applied in the study of classical concepts of mathematics, such as real numbers, complex variables, trigonometric functions, and algorithms, or of non-classical concepts like constructivism, harmonics, infinity, and vectors.

Florian Cajori explains in (1893) the difference between modern and ancient mathematical analysis, as distinct from logical analysis, as follows:

The terms "synthesis" and "analysis" are used in mathematics in a more special sense than in logic. In ancient mathematics they had a different meaning from what they now have. The oldest definition of mathematical analysis as opposed to synthesis is that given in [appended to] Euclid, XIII. 5, which in all probability was framed by Eudoxus: "Analysis is the obtaining of the thing sought by assuming it and so reasoning up to an admitted truth; synthesis is the obtaining of the thing sought by reasoning up to the inference and proof of it." 

The analytic method is not conclusive, unless all operations involved in it are known to be reversible. To remove all doubt, the Greeks, as a rule, added to the analytic process a synthetic one, consisting of a reversion of all operations occurring in the analysis. Thus the aim of analysis was to aid in the discovery of synthetic proofs or solutions.
James Gow uses a similar argument as Cajori, with the following clarification, in his "A Short History of Greek Mathematics" (1884):
The synthetic proof proceeds by shewing that the proposed new truth involves certain admitted truths. An analytic proof begins by an assumption, upon which a synthetic reasoning is founded. The Greeks distinguished "theoretic" from "problematic" analysis. A theoretic analysis is of the following kind. To "prove" that A is B, "assume" first that A is B. If so, then, since B is C and C is D and D is E, therefore A is E. If this be known a falsity, A is not B. But if this be a known truth and all the intermediate propositions be convertible, then the reverse process, A is E, E is D, D is C, C is B, therefore A is B, constitutes a synthetic proof of the original theorem. Problematic analysis is applied in all cases where it is proposed to construct a figure which is assumed to satisfy a given condition. The problem is then converted into some theorem which is involved in the condition and which is proved synthetically, and the steps of this synthetic proof taken backwards are a synthetic solution of the problem.




In statistics, the term "analysis" may refer to any method used
for data analysis. Among the many such methods, some are:





</doc>
<doc id="184483" url="https://en.wikipedia.org/wiki?curid=184483" title="Intentionality">
Intentionality

Intentionality is a philosophical concept defined as "the power of minds to be about, to represent, or to stand for, things, properties and states of affairs". The idea fell out of discussion with the end of the medieval scholastic period, but in recent times was resurrected by Franz Brentano and later adopted by Edmund Husserl. Today, intentionality is a live concern among philosophers of mind and language. The earliest theory of intentionality is associated with St. Anselm's ontological argument for the existence of God, and with his tenets distinguishing between objects that exist in the understanding and objects that exist in reality.

The concept of intentionality was reintroduced in 19th-century contemporary philosophy by Franz Brentano (a German philosopher and psychologist who is generally regarded as the founder of act psychology, also called intentionalism) in his work "Psychology from an Empirical Standpoint" (1874). Brentano described intentionality as a characteristic of all acts of consciousness that are thus "psychical" or "mental" phenomena, by which they may be set apart from "physical" or "natural" phenomena.
Brentano coined the expression "intentional inexistence" to indicate the peculiar ontological status of the contents of mental phenomena. According to some interpreters the "in-" of "in-existence" is to be read as locative, i.e. as indicating that "an intended object ... exists in or has "in-existence", existing not externally but in the psychological state" (Jacquette 2004, p. 102), while others are more cautious, stating: "It is not clear whether in 1874 this ... was intended to carry any ontological commitment" (Chrudzimski and Smith 2004, p. 205).

A major problem within discourse on intentionality is that participants often fail to make explicit whether or not they use the term to imply concepts such as agency or desire, i.e. whether it involves teleology. Dennett (see below) explicitly invokes teleological concepts in the "intentional stance". However, most philosophers use "intentionality" to mean something with no teleological import. Thus, a thought of a chair can be about a chair without any implication of an intention or even a belief relating to the chair. For philosophers of language, what is meant by intentionality is largely an issue of how symbols can have meaning. This lack of clarity may underpin some of the differences of view indicated below.

To bear out further the diversity of sentiment evoked from the notion of intentionality, Husserl followed on Brentano, and gave the concept of intentionality more widespread attention, both in continental and analytic philosophy. In contrast to Brentano's view, French philosopher Jean-Paul Sartre ("Being and Nothingness") identified intentionality with consciousness, stating that the two were indistinguishable. German philosopher Martin Heidegger ("Being and Time"), defined intentionality as "care" ("Sorge"), a sentient condition where an individual's existence, facticity, and being in the world identifies their ontological significance, in contrast to that which is merely ontic ("thinghood").

Other 20th-century philosophers such as Gilbert Ryle and A.J. Ayer were critical of Husserl's concept of intentionality and his many layers of consciousness. Ryle insisted that perceiving is not a process, and Ayer that describing one's knowledge is not to describe mental processes. The effect of these positions is that consciousness is so fully intentional that the mental act has been emptied of all content, and that the idea of pure consciousness is that it is nothing. (Sartre also referred to "consciousness" as "nothing").

Platonist Roderick Chisholm has revived the Brentano thesis through linguistic analysis, distinguishing two parts to Brentano's concept, the ontological aspect and the psychological aspect. Chisholm's writings have attempted to summarize the suitable and unsuitable criteria of the concept since the Scholastics, arriving at a criterion of intentionality identified by the two aspects of Brentano's thesis and defined by the logical properties that distinguish language describing psychological phenomena from language describing non-psychological phenomena. Chisholm's criteria for the intentional use of sentences are: existence independence, truth-value indifference, and referential opacity.

In current artificial intelligence and philosophy of mind, intentionality is sometimes linked with questions of semantic inference, with both skeptical and supportive adherents. John Searle argued for this position with the Chinese room thought experiment, according to which no syntactic operations that occurred in a computer would provide it with semantic content. Others are more skeptical of the human ability to make such an assertion, arguing that the kind of intentionality that emerges from self-organizing networks of automata will always be undecidable because it will never be possible to make our subjective introspective experience of intentionality and decision making coincide with our objective observation of the behavior of a self-organizing machine.

Daniel Dennett offers a taxonomy of the current theories about intentionality in Chapter 10 of his book "The Intentional Stance". Most, if not all, current theories on intentionality accept Brentano's thesis of the irreducibility of intentional idiom. From this thesis the following positions emerge:

Roderick Chisholm (1956), G.E.M. Anscombe (1957), Peter Geach (1957), and Charles Taylor (1964) all adhere to the former position, namely that intentional idiom is problematic and cannot be integrated with the natural sciences. Members of this category also maintain realism in regard to intentional objects, which may imply some kind of dualism (though this is debatable).

The latter position, which maintains the unity of intentionality with the natural sciences, is further divided into three standpoints:

Proponents of the eliminative materialism, understand intentional idiom, such as "belief", "desire", and the like, to be replaceable either with behavioristic language (e.g. Quine) or with the language of neuroscience (e.g. Churchland).

Holders of realism argue that there is a deeper fact of the matter to both translation and belief attribution. In other words, manuals for translating one language into another cannot be set up in different yet behaviorally identical ways and ontologically there are intentional objects. Famously, Fodor has attempted to ground such realist claims about intentionality in a language of thought. Dennett comments on this issue, Fodor "attempt[s] to make these irreducible realities acceptable to the physical sciences by grounding them (somehow) in the 'syntax' of a system of physically realized mental representations" (Dennett 1987, 345).

Those who adhere to the so-called Quinean double standard (namely that "ontologically there is nothing intentional, but that the language of intentionality is indispensable"), accept Quine's thesis of the indeterminacy of radical translation and its implications, while the other positions so far mentioned do not. As Quine puts it, indeterminacy of radical translation is the thesis that "manuals for translating one language into another can be set up in divergent ways, all compatible with the totality of speech dispositions, yet incompatible with one another" (Quine 1960, 27). Quine (1960) and Wilfrid Sellars (1958) both comment on this intermediary position. One such implication would be that there is, in principle, no deeper fact of the matter that could settle two interpretative strategies on what belief to attribute to a physical system. In other words, the behavior (including speech dispositions) of any physical system, in theory, could be interpreted by two different predictive strategies and both would be equally warranted in their belief attribution. This category can be seen to be a medial position between the realists and the eliminativists since it attempts to blend attributes of both into a theory of intentionality. Dennett, for example, argues in "True Believers" (1981) that intentional idiom (or "folk psychology") is a predictive strategy and if such a strategy successfully and voluminously predicts the actions of a physical system, then that physical system can be said to have those beliefs attributed to it. Dennett calls this predictive strategy the intentional stance.

They are further divided into two theses:

Advocates of the former, the Normative Principle, argue that attributions of intentional idioms to physical systems should be the propositional attitudes that the physical system ought to have in those circumstances (Dennett 1987, 342). However, exponents of this view are still further divided into those who make an Assumption of Rationality and those who adhere to the Principle of Charity. Dennett (1969, 1971, 1975), Cherniak (1981, 1986), and the more recent work of Putnam (1983) recommend the Assumption of Rationality, which unsurprisingly assumes that the physical system in question is rational. Donald Davidson (1967, 1973, 1974, 1985) and Lewis (1974) defend the Principle of Charity.

The latter is advocated by Grandy (1973) and Stich (1980, 1981, 1983, 1984), who maintain that attributions of intentional idioms to any physical system (e.g. humans, artifacts, non-human animals, etc.) should be the propositional attitude (e.g. "belief", "desire", etc.) that one would suppose one would have in the same circumstances (Dennett 1987, 343).

Working on the intentionality of vision, belief, and knowledge, Pierre Le Morvan (2005) has distinguished between three basic kinds of intentionality that he dubs "transparent", "translucent", and "opaque" respectively. The threefold distinction may be explained as follows. Let's call the "intendum" what an intentional state is about, and the "intender" the subject who is in the intentional state. An intentional state is transparent if it satisfies the following two conditions: (i) it is genuinely relational in that it entails the existence of not just the intender but the intendum as well, and (ii) substitutivity of identicals applies to the intendum (i.e. if the intentional state is about a, and a = b, then the intentional state is about b as well). An intentional state is translucent if it satisfies (i) but not (ii). An intentional state is opaque if it satisfies neither (i) nor (ii).

The claim that all mental states are intentional is called intentionalism, the contrary being anti-intentionalism.

Some anti-intentionalism, such as that of Ned Block, is based on the argument that phenomenal conscious experience or qualia is also a vital component of consciousness, and that it is not intentional. (The latter claim is itself disputed by Michael Tye.)

Another form of anti-intentionalism associated with John Searle regards phenomenality itself as the "mark of the mental" and sidelines intentionality.

A further form argues that some unusual states of consciousness are non-intentional, although an individual might live a lifetime without experiencing them. Robert K.C. Forman argues that some of the unusual states of consciousness typical of mystical experience are pure consciousness events in which awareness exists, but has no object, is not awareness "of" anything.

Several authors have attempted to construct philosophical models describing how intentionality relates to the human capacity to be self-conscious. Cedric Evans contributed greatly to the discussion with his "The Subject of Self-Consciousness" in 1970. He centered his model on the idea that executive attention need not be propositional in form.





</doc>
<doc id="89532" url="https://en.wikipedia.org/wiki?curid=89532" title="Identity (philosophy)">
Identity (philosophy)

In philosophy, identity, from ("sameness"), is the relation each thing bears only to itself. The notion of identity gives rise to many philosophical problems, including the identity of indiscernibles (if "x" and "y" share all their properties, are they one and the same thing?), and questions about change and personal identity over time (what has to be the case for a person "x" at one time and a person "y" at a later time to be one and the same person?).

The philosophical concept of identity is distinct from the more well-known notion of identity in use in psychology and the social sciences. The philosophical concept concerns a "relation", specifically, a relation that "x" and "y" stand in if, and only if they are one and the same thing, or "identical to" each other (i.e. if, and only if "x" = "y"). The sociological notion of identity, by contrast, has to do with a person's self-conception, social presentation, and more generally, the aspects of a person that make them unique, or qualitatively different from others (e.g. cultural identity, gender identity, national identity, online identity and processes of identity formation).

Metaphysicians and philosophers of language and mind ask other questions:
The law of identity originates from classical antiquity. The modern formulation of identity is that of Gottfried Leibniz, who held that "x" is the same as "y" if and only if every predicate true of "x" is true of "y" as well.

Leibniz's ideas have taken root in the philosophy of mathematics, where they have influenced the development of the predicate calculus as Leibniz's law. Mathematicians sometimes distinguish identity from equality. More mundanely, an "identity" in mathematics may be an "equation" that holds true for all values of a variable. Hegel argued that things are inherently self-contradictory and that the notion of something being self-identical only made sense if it were not also not-identical or different from itself and did not also imply the latter. In Hegel's words, "Identity is the identity of identity and non-identity." More recent metaphysicians have discussed trans-world identity—the notion that there can be the same object in different possible worlds. An alternative to trans-world identity is the counterpart relation in Counterpart theory. It is a similarity relation that rejects trans-world individuals and instead defends an objects counterpart - the most similar object.

Some philosophers have denied that there is such a relation as identity. Thus Ludwig Wittgenstein writes ("Tractatus" 5.5301): "That identity is not a relation between objects is obvious." At 5.5303 he elaborates: "Roughly speaking: to say of two things that they are identical is nonsense, and to say of one thing that it is identical with itself is to say nothing." Bertrand Russell had earlier voiced a worry that seems to be motivating Wittgenstein's point ("The Principles of Mathematics" §64): "[I]dentity, an objector may urge, cannot be anything at all: two terms plainly are not identical, and one term cannot be, for what is it identical with?" Even before Russell, Gottlob Frege, at the beginning of "On Sense and Reference," expressed a worry with regard to identity as a relation: "Equality gives rise to challenging questions which are not altogether easy to answer. Is it a relation?" More recently, C. J. F. Williams has suggested that identity should be viewed as a second-order relation, rather than a relation between objects, and Kai Wehmeier has argued that appealing to a binary relation that every object bears to itself, and to no others, is both logically unnecessary and metaphysically suspect.

Kind-terms, or sortals give a criterion of identity and non-identity among items of their kind.





</doc>
<doc id="36797" url="https://en.wikipedia.org/wiki?curid=36797" title="Occam's razor">
Occam's razor

Occam's razor (also Ockham's razor or Ocham's razor: ; or law of parsimony: ) is the problem-solving principle that states that "Entities should not be multiplied without necessity." The idea is attributed to English Franciscan friar William of Ockham ( 1287–1347), a scholastic philosopher and theologian who used a preference for simplicity to defend the idea of divine miracles. It is sometimes paraphrased by a statement like "the simplest solution is most likely the right one". Occam's razor says that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions, and it is not meant to be a way of choosing between hypotheses that make different predictions.

Similarly, in science, Occam's razor is used as an abductive heuristic in the development of theoretical models rather than as a rigorous arbiter between candidate models. In the scientific method, Occam's razor is not considered an irrefutable principle of logic or a scientific result; the preference for simplicity in the scientific method is based on the falsifiability criterion. For each accepted explanation of a phenomenon, there may be an extremely large, perhaps even incomprehensible, number of possible and more complex alternatives. Since one can always burden failing explanations with "ad hoc" hypotheses to prevent them from being falsified, simpler theories are preferable to more complex ones because they are more testable.

The phrase "Occam's razor" did not appear until a few centuries after William of Ockham's death in 1347. Libert Froidmont, in his "On Christian Philosophy of the Soul", takes credit for the phrase, speaking of "novacula occami". Ockham did not invent this principle, but the "razor"—and its association with him—may be due to the frequency and effectiveness with which he used it. Ockham stated the principle in various ways, but the most popular version, "Entities are not to be multiplied without necessity" () was formulated by the Irish Franciscan philosopher John Punch in his 1639 commentary on the works of Duns Scotus.

The origins of what has come to be known as Occam's razor are traceable to the works of earlier philosophers such as John Duns Scotus (1265–1308), Robert Grosseteste (1175–1253), Maimonides (Moses ben-Maimon, 1138–1204), and even Aristotle (384–322 BC). Aristotle writes in his "Posterior Analytics", "We may assume the superiority [other things being equal] of the demonstration which derives from fewer postulates or hypotheses." Ptolemy () stated, "We consider it a good principle to explain the phenomena by the simplest hypothesis possible."

Phrases such as "It is vain to do with more what can be done with fewer" and "A plurality is not to be posited without necessity" were commonplace in 13th-century scholastic writing. Robert Grosseteste, in "Commentary on" [Aristotle's] "the Posterior Analytics Books" ("Commentarius in Posteriorum Analyticorum Libros") (c. 1217–1220), declares: "That is better and more valuable which requires fewer, other circumstances being equal... For if one thing were demonstrated from many and another thing from fewer equally known premises, clearly that is better which is from fewer because it makes us know quickly, just as a universal demonstration is better than particular because it produces knowledge from fewer premises. Similarly in natural science, in moral science, and in metaphysics the best is that which needs no premises and the better that which needs the fewer, other circumstances being equal."

The "Summa Theologica" of Thomas Aquinas (1225–1274) states that "it is superfluous to suppose that what can be accounted for by a few principles has been produced by many." Aquinas uses this principle to construct an objection to God's existence, an objection that he in turn answers and refutes generally (cf. "quinque viae"), and specifically, through an argument based on causality. Hence, Aquinas acknowledges the principle that today is known as Occam's razor, but prefers causal explanations to other simple explanations (cf. also Correlation does not imply causation).

William of Ockham ("circa" 1287–1347) was an English Franciscan friar and theologian, an influential medieval philosopher and a nominalist. His popular fame as a great logician rests chiefly on the maxim attributed to him and known as Occam's razor. The term "razor" refers to distinguishing between two hypotheses either by "shaving away" unnecessary assumptions or cutting apart two similar conclusions.

While it has been claimed that Occam's razor is not found in any of William's writings, one can cite statements such as ("Plurality must never be posited without necessity"), which occurs in his theological work on the "Sentences of Peter Lombard" ("Quaestiones et decisiones in quattuor libros Sententiarum Petri Lombardi"; ed. Lugd., 1495, i, dist. 27, qu. 2, K).

Nevertheless, the precise words sometimes attributed to William of Ockham, (Entities must not be multiplied beyond necessity), are absent in his extant works; this particular phrasing comes from John Punch, who described the principle as a "common axiom" ("axioma vulgare") of the Scholastics. William of Ockham's contribution seems to restrict the operation of this principle in matters pertaining to miracles and God's power; so, in the Eucharist, a plurality of miracles is possible, simply because it pleases God.

This principle is sometimes phrased as ("Plurality should not be posited without necessity"). In his "Summa Totius Logicae", i. 12, William of Ockham cites the principle of economy, ("It is futile to do with more things that which can be done with fewer"; Thorburn, 1918, pp. 352–53; Kneale and Kneale, 1962, p. 243.)

To quote Isaac Newton, "We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances. Therefore, to the same natural effects we must, as far as possible, assign the same causes."

Bertrand Russell offers a particular version of Occam's razor: "Whenever possible, substitute constructions out of known entities for inferences to unknown entities."

Around 1960, Ray Solomonoff founded the theory of universal inductive inference, the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. This theory is a mathematical formalization of Occam's razor.

Another technical approach to Occam's razor is ontological parsimony. Parsimony means spareness and is also referred to as the Rule of Simplicity. This is considered a strong version of Occam's razor. A variation used in medicine is called the "Zebra": a physician should reject an exotic medical diagnosis when a more commonplace explanation is more likely, derived from Theodore Woodward's dictum "When you hear hoofbeats, think of horses not zebras".

Ernst Mach formulated the stronger version of Occam's razor into physics, which he called the Principle of Economy stating: "Scientists must use the simplest means of arriving at their results and exclude everything not perceived by the senses."

This principle goes back at least as far as Aristotle, who wrote "Nature operates in the shortest way possible." The idea of parsimony or simplicity in deciding between theories, though not the intent of the original expression of Occam's razor, has been assimilated into our culture as the widespread layman's formulation that "the simplest explanation is usually the correct one."

Prior to the 20th century, it was a commonly held belief that nature itself was simple and that simpler hypotheses about nature were thus more likely to be true. This notion was deeply rooted in the aesthetic value that simplicity holds for human thought and the justifications presented for it often drew from theology. Thomas Aquinas made this argument in the 13th century, writing, "If a thing can be done adequately by means of one, it is superfluous to do it by means of several; for we observe that nature does not employ two instruments [if] one suffices."

Beginning in the 20th century, epistemological justifications based on induction, logic, pragmatism, and especially probability theory have become more popular among philosophers.

Occam's razor has gained strong empirical support in helping to converge on better theories (see "Applications" section below for some examples).

In the related concept of overfitting, excessively complex models are affected by statistical noise (a problem also known as the bias-variance trade-off), whereas simpler models may capture the underlying structure better and may thus have better predictive performance. It is, however, often difficult to deduce which part of the data is noise (cf. model selection, test set, minimum description length, Bayesian inference, etc.).

The razor's statement that "other things being equal, simpler explanations are generally better than more complex ones" is amenable to empirical testing. Another interpretation of the razor's statement would be that "simpler hypotheses are generally better than the complex ones". The procedure to test the former interpretation would compare the track records of simple and comparatively complex explanations. If one accepts the first interpretation, the validity of Occam's razor as a tool would then have to be rejected if the more complex explanations were more often correct than the less complex ones (while the converse would lend support to its use). If the latter interpretation is accepted, the validity of Occam's razor as a tool could possibly be accepted if the simpler hypotheses led to correct conclusions more often than not.

Some increases in complexity are sometimes necessary, so there remains a justified general bias toward the simpler of two competing explanations. To understand why, consider that for each accepted explanation of a phenomenon, there is always an infinite number of possible, more complex, and ultimately incorrect, alternatives. This is so because one can always burden a failing explanation with an ad hoc hypothesis. Ad hoc hypotheses are justifications that prevent theories from being falsified. Even other empirical criteria, such as consilience, can never truly eliminate such explanations as competition. Each true explanation, then, may have had many alternatives that were simpler and false, but also an infinite number of alternatives that were more complex and false. But if an alternative ad hoc hypothesis were indeed justifiable, its implicit conclusions would be empirically verifiable. On a commonly accepted repeatability principle, these alternative theories have never been observed and continue to escape observation. In addition, one does not say an explanation is true if it has not withstood this principle.

Put another way, any new, and even more complex, theory can still possibly be true. For example, if an individual makes supernatural claims that leprechauns were responsible for breaking a vase, the simpler explanation would be that he is mistaken, but ongoing ad hoc justifications (e.g. "... and that's not me on the film; they tampered with that, too") successfully prevent outright disproval. This endless supply of elaborate competing explanations, called saving hypotheses, cannot be ruled out – except by using Occam's razor. A study of the predictive validity of Occam's razor found 32 published papers that included 97 comparisons of economic forecasts from simple and complex forecasting methods. None of the papers provided a balance of evidence that complexity of method improved forecast accuracy. In the 25 papers with quantitative comparisons, complexity increased forecast errors by an average of 27 percent.

One justification of Occam's razor is a direct result of basic probability theory. By definition, all assumptions introduce possibilities for error; if an assumption does not improve the accuracy of a theory, its only effect is to increase the probability that the overall theory is wrong.

There have also been other attempts to derive Occam's razor from probability theory, including notable attempts made by Harold Jeffreys and E. T. Jaynes. The probabilistic (Bayesian) basis for Occam's razor is elaborated by David J. C. MacKay in chapter 28 of his book "Information Theory, Inference, and Learning Algorithms", where he emphasizes that a prior bias in favour of simpler models is not required.

William H. Jefferys and James O. Berger (1991) generalize and quantify the original formulation's "assumptions" concept as the degree to which a proposition is unnecessarily accommodating to possible observable data. They state, "A hypothesis with fewer adjustable parameters will automatically have an enhanced posterior probability, due to the fact that the predictions it makes are sharp." The model they propose balances the precision of a theory's predictions against their sharpness—preferring theories that sharply make correct predictions over theories that accommodate a wide range of other possible results. This, again, reflects the mathematical relationship between key concepts in Bayesian inference (namely marginal probability, conditional probability, and posterior probability).

The bias–variance tradeoff is a framework that incorporates the Occam's razor principal in its balance between overfitting (i.e. variance minimization) and underfitting (i.e. bias minimization).

Karl Popper argues that a preference for simple theories need not appeal to practical or aesthetic considerations. Our preference for simplicity may be justified by its falsifiability criterion: we prefer simpler theories to more complex ones "because their empirical content is greater; and because they are better testable". The idea here is that a simple theory applies to more cases than a more complex one, and is thus more easily falsifiable. This is again comparing a simple theory to a more complex theory where both explain the data equally well.

The philosopher of science Elliott Sober once argued along the same lines as Popper, tying simplicity with "informativeness": The simplest theory is the more informative, in the sense that it requires less information to a question. He has since rejected this account of simplicity, purportedly because it fails to provide an epistemic justification for simplicity. He now believes that simplicity considerations (and considerations of parsimony in particular) do not count unless they reflect something more fundamental. Philosophers, he suggests, may have made the error of hypostatizing simplicity (i.e., endowed it with a "sui generis" existence), when it has meaning only when embedded in a specific context (Sober 1992). If we fail to justify simplicity considerations on the basis of the context in which we use them, we may have no non-circular justification: "Just as the question 'why be rational?' may have no non-circular answer, the same may be true of the question 'why should simplicity be considered in evaluating the plausibility of hypotheses?'"

Richard Swinburne argues for simplicity on logical grounds:

According to Swinburne, since our choice of theory cannot be determined by data (see Underdetermination and Duhem-Quine thesis), we must rely on some criterion to determine which theory to use. Since it is absurd to have no logical method for settling on one hypothesis amongst an infinite number of equally data-compliant hypotheses, we should choose the simplest theory: "Either science is irrational [in the way it judges theories and predictions probable] or the principle of simplicity is a fundamental synthetic a priori truth." (Swinburne 1997).

From the "Tractatus Logico-Philosophicus":


and on the related concept of "simplicity":


In science, Occam's razor is used as a heuristic to guide scientists in developing theoretical models rather than as an arbiter between published models. In physics, parsimony was an important heuristic in Albert Einstein's formulation of special relativity, in the development and application of the principle of least action by Pierre Louis Maupertuis and Leonhard Euler, and in the development of quantum mechanics by Max Planck, Werner Heisenberg and Louis de Broglie.

In chemistry, Occam's razor is often an important heuristic when developing a model of a reaction mechanism. Although it is useful as a heuristic in developing models of reaction mechanisms, it has been shown to fail as a criterion for selecting among some selected published models. In this context, Einstein himself expressed caution when he formulated Einstein's Constraint: "It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience". An often-quoted version of this constraint (which cannot be verified as posited by Einstein himself) says "Everything should be kept as simple as possible, but not simpler."

In the scientific method, parsimony is an epistemological, metaphysical or heuristic preference, not an irrefutable principle of logic or a scientific result. As a logical principle, Occam's razor would demand that scientists accept the simplest possible theoretical explanation for existing data. However, science has shown repeatedly that future data often support more complex theories than do existing data. Science prefers the simplest explanation that is consistent with the data available at a given time, but the simplest explanation may be ruled out as new data become available. That is, science is open to the possibility that future experiments might support more complex theories than demanded by current data and is more interested in designing experiments to discriminate between competing theories than favoring one theory over another based merely on philosophical principles.

When scientists use the idea of parsimony, it has meaning only in a very specific context of inquiry. Several background assumptions are required for parsimony to connect with plausibility in a particular research problem. The reasonableness of parsimony in one research context may have nothing to do with its reasonableness in another. It is a mistake to think that there is a single global principle that spans diverse subject matter.

It has been suggested that Occam's razor is a widely accepted example of extraevidential consideration, even though it is entirely a metaphysical assumption. There is little empirical evidence that the world is actually simple or that simple accounts are more likely to be true than complex ones.

Most of the time, Occam's razor is a conservative tool, cutting out "crazy, complicated constructions" and assuring "that hypotheses are grounded in the science of the day", thus yielding "normal" science: models of explanation and prediction. There are, however, notable exceptions where Occam's razor turns a conservative scientist into a reluctant revolutionary. For example, Max Planck interpolated between the Wien and Jeans radiation laws and used Occam's razor logic to formulate the quantum hypothesis, even resisting that hypothesis as it became more obvious that it was correct.

Appeals to simplicity were used to argue against the phenomena of meteorites, ball lightning, continental drift, and reverse transcriptase. One can argue for atomic building blocks for matter, because it provides a simpler explanation for the observed reversibility of both mixing and chemical reactions as simple separation and rearrangements of atomic building blocks. At the time, however, the atomic theory was considered more complex because it implied the existence of invisible particles that had not been directly detected. Ernst Mach and the logical positivists rejected John Dalton's atomic theory until the reality of atoms was more evident in Brownian motion, as shown by Albert Einstein.

In the same way, postulating the aether is more complex than transmission of light through a vacuum. At the time, however, all known waves propagated through a physical medium, and it seemed simpler to postulate the existence of a medium than to theorize about wave propagation without a medium. Likewise, Newton's idea of light particles seemed simpler than Christiaan Huygens's idea of waves, so many favored it. In this case, as it turned out, neither the wave—nor the particle—explanation alone suffices, as light behaves like waves and like particles.

Three axioms presupposed by the scientific method are realism (the existence of objective reality), the existence of natural laws, and the constancy of natural law. Rather than depend on provability of these axioms, science depends on the fact that they have not been objectively falsified. Occam's razor and parsimony support, but do not prove, these axioms of science. The general principle of science is that theories (or models) of natural law must be consistent with repeatable experimental observations. This ultimate arbiter (selection criterion) rests upon the axioms mentioned above.

There are examples where Occam's razor would have favored the wrong theory given the available data. Simplicity principles are useful philosophical preferences for choosing a more likely theory from among several possibilities that are all consistent with available data. A single instance of Occam's razor favoring a wrong theory falsifies the razor as a general principle. Michael Lee and others provide cases in which a parsimonious approach does not guarantee a correct conclusion and, if based on incorrect working hypotheses or interpretations of incomplete data, may even strongly support a false conclusion.

If multiple models of natural law make exactly the same testable predictions, they are equivalent and there is no need for parsimony to choose a preferred one. For example, Newtonian, Hamiltonian and Lagrangian classical mechanics are equivalent. Physicists have no interest in using Occam's razor to say the other two are wrong. Likewise, there is no demand for simplicity principles to arbitrate between wave and matrix formulations of quantum mechanics. Science often does not demand arbitration or selection criteria between models that make the same testable predictions.

Biologists or philosophers of biology use Occam's razor in either of two contexts both in evolutionary biology: the units of selection controversy and systematics. George C. Williams in his book "Adaptation and Natural Selection" (1966) argues that the best way to explain altruism among animals is based on low-level (i.e., individual) selection as opposed to high-level group selection. Altruism is defined by some evolutionary biologists (e.g., R. Alexander, 1987; W. D. Hamilton, 1964) as behavior that is beneficial to others (or to the group) at a cost to the individual, and many posit individual selection as the mechanism that explains altruism solely in terms of the behaviors of individual organisms acting in their own self-interest (or in the interest of their genes, via kin selection). Williams was arguing against the perspective of others who propose selection at the level of the group as an evolutionary mechanism that selects for altruistic traits (e.g., D. S. Wilson & E. O. Wilson, 2007). The basis for Williams' contention is that of the two, individual selection is the more parsimonious theory. In doing so he is invoking a variant of Occam's razor known as Morgan's Canon: "In no case is an animal activity to be interpreted in terms of higher psychological processes, if it can be fairly interpreted in terms of processes which stand lower in the scale of psychological evolution and development." (Morgan 1903).

However, more recent biological analyses, such as Richard Dawkins' "The Selfish Gene", have contended that Morgan's Canon is not the simplest and most basic explanation. Dawkins argues the way evolution works is that the genes propagated in most copies end up determining the development of that particular species, i.e., natural selection turns out to select specific genes, and this is really the fundamental underlying principle that automatically gives individual and group selection as emergent features of evolution.

Zoology provides an example. Muskoxen, when threatened by wolves, form a circle with the males on the outside and the females and young on the inside. This is an example of a behavior by the males that seems to be altruistic. The behavior is disadvantageous to them individually but beneficial to the group as a whole and was thus seen by some to support the group selection theory. Another interpretation is kin selection: if the males are protecting their offspring, they are protecting copies of their own alleles. Engaging in this behavior would be favored by individual selection if the cost to the male musk ox is less than half of the benefit received by his calf – which could easily be the case if wolves have an easier time killing calves than adult males. It could also be the case that male musk oxen would be individually less likely to be killed by wolves if they stood in a circle with their horns pointing out, regardless of whether they were protecting the females and offspring. That would be an example of regular natural selection – a phenomenon called "the selfish herd".

Systematics is the branch of biology that attempts to establish patterns of genealogical relationship among biological taxa. It is also concerned with their classification. There are three primary camps in systematics: cladists, pheneticists, and evolutionary taxonomists. The cladists hold that genealogy alone should determine classification, pheneticists contend that overall similarity is the determining criterion, while evolutionary taxonomists say that both genealogy and similarity count in classification.

It is among the cladists that Occam's razor is to be found, although their term for it is "cladistic parsimony". Cladistic parsimony (or maximum parsimony) is a method of phylogenetic inference in the construction of types of phylogenetic trees (more specifically, cladograms). Cladograms are branching, tree-like structures used to represent hypotheses of relative degree of relationship, based on shared, derived character states. Cladistic parsimony is used to select as the preferred hypothesis of relationships the cladogram that requires the fewest implied character state transformations. Critics of the cladistic approach often observe that for some types of tree, parsimony consistently produces the wrong results, regardless of how much data is collected (this is called statistical inconsistency, or long branch attraction). However, this criticism is also potentially true for any type of phylogenetic inference, unless the model used to estimate the tree reflects the way that evolution actually happened. Because this information is not empirically accessible, the criticism of statistical inconsistency against parsimony holds no force. For a book-length treatment of cladistic parsimony, see Elliott Sober's "Reconstructing the Past: Parsimony, Evolution, and Inference" (1988). For a discussion of both uses of Occam's razor in biology, see Sober's article "Let's Razor Ockham's Razor" (1990).

Other methods for inferring evolutionary relationships use parsimony in a more traditional way. Likelihood methods for phylogeny use parsimony as they do for all likelihood tests, with hypotheses requiring few differing parameters (i.e., numbers of different rates of character change or different frequencies of character state transitions) being treated as null hypotheses relative to hypotheses requiring many differing parameters. Thus, complex hypotheses must predict data much better than do simple hypotheses before researchers reject the simple hypotheses. Recent advances employ information theory, a close cousin of likelihood, which uses Occam's razor in the same way.

Francis Crick has commented on potential limitations of Occam's razor in biology. He advances the argument that because biological systems are the products of (an ongoing) natural selection, the mechanisms are not necessarily optimal in an obvious sense. He cautions: "While Ockham's razor is a useful tool in the physical sciences, it can be a very dangerous implement in biology. It is thus very rash to use simplicity and elegance as a guide in biological research."

In biogeography, parsimony is used to infer ancient migrations of species or populations by observing the geographic distribution and relationships of existing organisms. Given the phylogenetic tree, ancestral migrations are inferred to be those that require the minimum amount of total movement.

In the philosophy of religion, Occam's razor is sometimes applied to the existence of God. William of Ockham himself was a Christian. He believed in God, and in the authority of Scripture; he writes that "nothing ought to be posited without a reason given, unless it is self-evident (literally, known through itself) or known by experience or proved by the authority of Sacred Scripture." Ockham believed that an explanation has no sufficient basis in reality when it does not harmonize with reason, experience, or the Bible. However, unlike many theologians of his time, Ockham did not believe God could be logically proven with arguments. To Ockham, science was a matter of discovery, but theology was a matter of revelation and faith. He states: "only faith gives us access to theological truths. The ways of God are not open to reason, for God has freely chosen to create a world and establish a way of salvation within it apart from any necessary laws that human logic or rationality can uncover."

St. Thomas Aquinas, in the "Summa Theologica", uses a formulation of Occam's razor to construct an objection to the idea that God exists, which he refutes directly with a counterargument:

Further, it is superfluous to suppose that what can be accounted for by a few principles has been produced by many. But it seems that everything we see in the world can be accounted for by other principles, supposing God did not exist. For all natural things can be reduced to one principle which is nature; and all voluntary things can be reduced to one principle which is human reason, or will. Therefore there is no need to suppose God's existence.

In turn, Aquinas answers this with the "quinque viae", and addresses the particular objection above with the following answer:

Since nature works for a determinate end under the direction of a higher agent, whatever is done by nature must needs be traced back to God, as to its first cause. So also whatever is done voluntarily must also be traced back to some higher cause other than human reason or will, since these can change or fail; for all things that are changeable and capable of defect must be traced back to an immovable and self-necessary first principle, as was shown in the body of the Article.

Rather than argue for the necessity of a god, some theists base their belief upon grounds independent of, or prior to, reason, making Occam's razor irrelevant. This was the stance of Søren Kierkegaard, who viewed belief in God as a leap of faith that sometimes directly opposed reason. This is also the doctrine of Gordon Clark's presuppositional apologetics, with the exception that Clark never thought the leap of faith was contrary to reason (see also Fideism).

Various arguments in favor of God establish God as a useful or even necessary assumption. Contrastingly some anti-theists hold firmly to the belief that assuming the existence of God introduces unnecessary complexity (Schmitt 2005, e.g., the Ultimate Boeing 747 gambit).

Another application of the principle is to be found in the work of George Berkeley (1685–1753). Berkeley was an idealist who believed that all of reality could be explained in terms of the mind alone. He invoked Occam's razor against materialism, stating that matter was not required by his metaphysic and was thus eliminable. One potential problem with this belief is that it's possible, given Berkeley's position, to find solipsism itself more in line with the razor than a God-mediated world beyond a single thinker.

Occam's razor may also be recognized in the apocryphal story about an exchange between Pierre-Simon Laplace and Napoleon. It is said that in praising Laplace for one of his recent publications, the emperor asked how it was that the name of God, which featured so frequently in the writings of Lagrange, appeared nowhere in Laplace's. At that, he is said to have replied, "It's because I had no need of that hypothesis." Though some point to this story as illustrating Laplace's atheism, more careful consideration suggests that he may instead have intended merely to illustrate the power of methodological naturalism, or even simply that the fewer logical premises one assumes, the stronger is one's conclusion.

In his article "Sensations and Brain Processes" (1959), J. J. C. Smart invoked Occam's razor with the aim to justify his preference of the mind-brain identity theory over spirit-body dualism. Dualists state that there are two kinds of substances in the universe: physical (including the body) and spiritual, which is non-physical. In contrast, identity theorists state that everything is physical, including consciousness, and that there is nothing nonphysical. Though it is impossible to appreciate the spiritual when limiting oneself to the physical, Smart maintained that identity theory explains all phenomena by assuming only a physical reality. Subsequently, Smart has been severely criticized for his use (or misuse) of Occam's razor and ultimately retracted his advocacy of it in this context. Paul Churchland (1984) states that by itself Occam's razor is inconclusive regarding duality. In a similar way, Dale Jacquette (1994) stated that Occam's razor has been used in attempts to justify eliminativism and reductionism in the philosophy of mind. Eliminativism is the thesis that the ontology of folk psychology including such entities as "pain", "joy", "desire", "fear", etc., are eliminable in favor of an ontology of a completed neuroscience.

In penal theory and the philosophy of punishment, parsimony refers specifically to taking care in the distribution of punishment in order to avoid excessive punishment. In the utilitarian approach to the philosophy of punishment, Jeremy Bentham's "parsimony principle" states that any punishment greater than is required to achieve its end is unjust. The concept is related but not identical to the legal concept of proportionality. Parsimony is a key consideration of the modern restorative justice, and is a component of utilitarian approaches to punishment, as well as the prison abolition movement. Bentham believed that true parsimony would require punishment to be individualised to take account of the sensibility of the individual—an individual more sensitive to punishment should be given a proportionately lesser one, since otherwise needless pain would be inflicted. Later utilitarian writers have tended to abandon this idea, in large part due to the impracticality of determining each alleged criminal's relative sensitivity to specific punishments.

Marcus Hutter's universal artificial intelligence builds upon Solomonoff's mathematical formalization of the razor to calculate the expected value of an action.

There are various papers in scholarly journals deriving formal versions of Occam's razor from probability theory, applying it in statistical inference, and using it to come up with criteria for penalizing complexity in statistical inference. Papers have suggested a connection between Occam's razor and Kolmogorov complexity.

One of the problems with the original formulation of the razor is that it only applies to models with the same explanatory power (i.e., it only tells us to prefer the simplest of equally good models). A more general form of the razor can be derived from Bayesian model comparison, which is based on Bayes factors and can be used to compare models that don't fit the observations equally well. These methods can sometimes optimally balance the complexity and power of a model. Generally, the exact Occam factor is intractable, but approximations such as Akaike information criterion, Bayesian information criterion, Variational Bayesian methods, false discovery rate, and Laplace's method are used. Many artificial intelligence researchers are now employing such techniques, for instance through work on Occam Learning or more generally on the Free energy principle.

Statistical versions of Occam's razor have a more rigorous formulation than what philosophical discussions produce. In particular, they must have a specific definition of the term "simplicity", and that definition can vary. For example, in the Kolmogorov–Chaitin minimum description length approach, the subject must pick a Turing machine whose operations describe the basic operations "believed" to represent "simplicity" by the subject. However, one could always choose a Turing machine with a simple operation that happened to construct one's entire theory and would hence score highly under the razor. This has led to two opposing camps: one that believes Occam's razor is objective, and one that believes it is subjective.

The minimum instruction set of a universal Turing machine requires approximately the same length description across different formulations, and is small compared to the Kolmogorov complexity of most practical theories. Marcus Hutter has used this consistency to define a "natural" Turing machine of small size as the proper basis for excluding arbitrarily complex instruction sets in the formulation of razors. Describing the program for the universal program as the "hypothesis", and the representation of the evidence as program data, it has been formally proven under Zermelo–Fraenkel set theory that "the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized." Interpreting this as minimising the total length of a two-part message encoding model followed by data given model gives us the minimum message length (MML) principle.

One possible conclusion from mixing the concepts of Kolmogorov complexity and Occam's razor is that an ideal data compressor would also be a scientific explanation/formulation generator. Some attempts have been made to re-derive known laws from considerations of simplicity or compressibility.

According to Jürgen Schmidhuber, the appropriate mathematical theory of Occam's razor already exists, namely, Solomonoff's theory of optimal inductive inference and its extensions. See discussions in David L. Dowe's "Foreword re C. S. Wallace" for the subtle distinctions between the algorithmic probability work of Solomonoff and the MML work of Chris Wallace, and see Dowe's "MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness" both for such discussions and for (in section 4) discussions of MML and Occam's razor. For a specific example of MML as Occam's razor in the problem of decision tree induction, see Dowe and Needham's "Message Length as an Effective Ockham's Razor in Decision Tree Induction".

Occam's razor is not an embargo against the positing of any kind of entity, or a recommendation of the simplest theory come what may. Occam's razor is used to adjudicate between theories that have already passed "theoretical scrutiny" tests and are equally well-supported by evidence. Furthermore, it may be used to prioritize empirical testing between two equally plausible but unequally testable hypotheses; thereby minimizing costs and wastes while increasing chances of falsification of the simpler-to-test hypothesis.

Another contentious aspect of the razor is that a theory can become more complex in terms of its structure (or syntax), while its ontology (or semantics) becomes simpler, or vice versa. Quine, in a discussion on definition, referred to these two perspectives as "economy of practical expression" and "economy in grammar and vocabulary", respectively.

Galileo Galilei lampooned the "misuse" of Occam's razor in his "Dialogue". The principle is represented in the dialogue by Simplicio. The telling point that Galileo presented ironically was that if one really wanted to start from a small number of entities, one could always consider the letters of the alphabet as the fundamental entities, since one could construct the whole of human knowledge out of them.

Occam's razor has met some opposition from people who have considered it too extreme or rash. Walter Chatton (c. 1290–1343) was a contemporary of William of Ockham who took exception to Occam's razor and Ockham's use of it. In response he devised his own "anti-razor": "If three things are not enough to verify an affirmative proposition about things, a fourth must be added, and so on." Although there have been a number of philosophers who have formulated similar anti-razors since Chatton's time, no one anti-razor has perpetuated in as much notability as Chatton's anti-razor, although this could be the case of the Late Renaissance Italian motto of unknown attribution ("Even if it is not true, it is well conceived") when referred to a particularly artful explanation.

Anti-razors have also been created by Gottfried Wilhelm Leibniz (1646–1716), Immanuel Kant (1724–1804), and Karl Menger (1902–1985). Leibniz's version took the form of a principle of plenitude, as Arthur Lovejoy has called it: the idea being that God created the most varied and populous of possible worlds. Kant felt a need to moderate the effects of Occam's razor and thus created his own counter-razor: "The variety of beings should not rashly be diminished."

Karl Menger found mathematicians to be too parsimonious with regard to variables, so he formulated his Law Against Miserliness, which took one of two forms: "Entities must not be reduced to the point of inadequacy" and "It is vain to do with fewer what requires more." A less serious but (some might say) even more extremist anti-razor is 'Pataphysics, the "science of imaginary solutions" developed by Alfred Jarry (1873–1907). Perhaps the ultimate in anti-reductionism, "'Pataphysics seeks no less than to view each event in the universe as completely unique, subject to no laws but its own." Variations on this theme were subsequently explored by the Argentine writer Jorge Luis Borges in his story/mock-essay "Tlön, Uqbar, Orbis Tertius". There is also Crabtree's Bludgeon, which cynically states that "[n]o set of mutually inconsistent observations can exist for which some human intellect cannot conceive a coherent explanation, however complicated."





</doc>
<doc id="1137736" url="https://en.wikipedia.org/wiki?curid=1137736" title="Principle of sufficient reason">
Principle of sufficient reason

The principle of sufficient reason states that everything must have a reason or a cause. The modern formulation of the principle is usually attributed to Gottfried Leibniz, although the idea was conceived of and utilized by various philosophers who preceded him, including Anaximander, Parmenides, Archimedes, Plato and Aristotle, Cicero, Avicenna, Thomas Aquinas, and Spinoza. Some philosophers have associated the principle of sufficient reason with "ex nihilo nihil fit". Hamilton identified the laws of inference modus ponens with the "law of Sufficient Reason, or of Reason and Consequent" and modus tollens with its contrapositive expression.

The principle has a variety of expressions, all of which are perhaps best summarized by the following:


A sufficient explanation may be understood either in terms of "reasons" or "causes," for like many philosophers of the period, Leibniz did not carefully distinguish between the two. The resulting principle is very different, however, depending on which interpretation is given.

It is an open question whether the principle of sufficient reason can be applied to axioms within a logic construction like a mathematical or a physical theory, because axioms are propositions accepted as having no justification possible within the system
The principle declares that all propositions considered to be true within a system should be deducible from the set axioms at the base of the construction (with some theoretical exceptions: see Gödel's theorem).

Leibniz identified two kinds of truth, necessary and contingent truths. He believed necessary mathematical truths to be derived from the law of identity (and the principle of non-contradiction): "Necessary truths are those that can be demonstrated through an analysis of terms, so that in the end they become identities, just as in Algebra an equation expressing an identity ultimately results from the substitution of values [for variables]. That is, necessary truths depend upon the principle of contradiction." Leibniz states that the sufficient reason for necessary truths is that their negation is a contradiction.

Leibniz admitted contingent truths on the basis of infinitary reasons, to which God had access but humans did not:
In contingent truths, even though the predicate is in the subject, this can never be demonstrated, nor can a proposition ever be reduced to an equality or to an identity, but the resolution proceeds to infinity, God alone seeing, not the end of the resolution, of course, which does not exist, but the connection of the terms or the containment of the predicate in the subject, since he sees whatever is in the series.Without this qualification, the principle can be seen as a description of a certain notion of closed system, in which there is no 'outside' to provide unexplained events with causes. It is also in tension with the paradox of Buridan's ass. Leibniz denied that the paradox of Buridan's ass could ever occur, saying:

Leibniz also used the principle of sufficient reason to refute the idea of absolute space:

I say then, that if space is an absolute being, there would be something for which it would be impossible there should be a sufficient reason. Which is against my axiom. And I prove it thus. Space is something absolutely uniform; and without the things placed in it, one point in space does not absolutely differ in any respect whatsoever from another point of space. Now from hence it follows, (supposing space to be something in itself, beside the order of bodies among themselves,) that 'tis impossible that there should be a reason why God, preserving the same situation of bodies among themselves, should have placed them in space after one particular manner, and not otherwise; why everything was not placed the quite contrary way, for instance, by changing East into West.

The principle was one of the four recognised laws of thought, that held a place in European pedagogy of logic and reasoning (and, to some extent, philosophy in general) in the 18th and 19th centuries. It was influential in the thinking of Leo Tolstoy, amongst others, in the elevated form that history could not be accepted as random.

A sufficient reason is sometimes described as the coincidence of every single thing that is needed for the occurrence of an effect (i.e. of the so-called "necessary conditions"). Such view could perhaps be also applied to indeterministic systems, as long as randomness is in a way incorporated in the preconditions.

Here is how Hamilton, circa 1837–1838, expressed his "fourth law" in his LECT. V. LOGIC. 60–61:

According to Schopenhauer's "On the Fourfold Root of the Principle of Sufficient Reason", there are four distinct forms of the principle.

First Form: The Principle of Sufficient Reason of Becoming (principium rationis sufficientis fiendi); appears as the law of causality in the understanding.

Second Form: The Principle of Sufficient Reason of Knowing (principium rationis sufficientis cognoscendi); asserts that if a judgment is to express a piece of knowledge, it must have a sufficient ground or reason, in which case it receives the predicate true.

Third Form: The Principle of Sufficient Reason of Being (principium rationis sufficientis essendi); the law whereby the parts of space and time determine one another as regards those relations. Example in arithmetic: Each number presupposes the preceding numbers as grounds or reasons of its being; "I can reach ten only by going through all the preceding numbers; and only by virtue of this insight into the ground of being, do I know that where there are ten, so are there eight, six, four."

"Now just as the subjective correlative to the first class of representations is the understanding, that to the second the faculty of reason, and that to the third pure sensibility, so is the subjective correlative to this fourth class found to be the inner sense, or generally self-consciousness." 

Fourth Form: The Principle of Sufficient Reason of Acting (principium rationis sufficientis agendi); briefly known as the law of motivation. "Any judgment that does not follow its previously existing ground or reason" or any state that cannot be explained away as falling under the three previous headings "must be produced by an act of will which has a motive." As his proposition in 43 states, "Motivation is causality seen from within."

Several proofs have been prepared in order to demonstrate that the universe is at bottom causal, i.e. works in accord with the principle in question; perhaps not in every single case (randomness might still play a part here and there), but that causality must be the way it works at least "in general", in most of what we see; and that our minds are aware of the principle even before any experience. The two famous arguments or proofs were proposed by Immanuel Kant (from the form of Time, temporal ordering of events and "directionality" of time) and by Arthur Schopenhauer (by demonstrating how all perception depends on causality and the intellect).

Once it is agreed (e.g. from a kind of an "arrow of time") that causal interconnections, as a form of principle of sufficient reason, indeed must in general exist everywhere in the universe (at least in the large scale), "backwards" causality in general might then be precluded using a form of the paradox of free will (i.e. an event that has a future source might cause us to remove that source quick enough and thus causality would not work).




</doc>
<doc id="21350938" url="https://en.wikipedia.org/wiki?curid=21350938" title="Absolute (philosophy)">
Absolute (philosophy)

In idealist philosophy, the Absolute is "the sum of all being, actual and potential". In monistic idealism, it serves as a concept for the "unconditioned reality which is either the spiritual ground of all being or the whole of things considered as a spiritual unity. 

The concept of "the absolute" was introduced in modern philosophy, notably by Hegel, for "the sum of all being, actual and potential". For Hegel, states the philosophy scholar Martin Heidegger, the Absolute is "the spirit, that which is present to itself in the certainty of unconditional self-knowing". According to Hegel, states Frederick Copleston – a historian of philosophy, "Logic studies the Absolute 'in itself'; the philosophy of Nature studies the Absolute 'for itself'; and the philosophy of Spirit studies the Absolute 'in and for itself'. The concept is also found in the works of F.W.J. Schelling, and was anticipated by Johann Gottlieb Fichte. In English philosophy, F. H. Bradley has distinguished the concept of Absolute from God, while Josiah Royce, the founder of American idealism school of philosophy, has equated them.

The concept of the Absolute has been used to interpret the early texts of the Indian religions such as those attributed to Yajnavalkya, Nagarjuna and Adi Shankara.

In Jainism, Absolute Knowledge or Kewalya Gnan, is said to be attained by the Arihantas and Teerthankaras, who reflects in their knowing, the 360 degrees of the truth and events of past, present and future. All 24 Teerthankaras and many others are Kewalya Gnani or Carriers of Absolute Knowledge.

According to Takeshi Umehara, some ancient texts of Buddhism state that the "truly Absolute and the truly Free must be nothingness", the "void". Yet, the early Buddhist scholar Nagarjuna, states Paul Williams, does not present "emptiness" as some kind of Absolute, rather it is "the very absence (a pure non-existence) of inherent existence" in Mādhyamaka school of the Buddhist philosophy. 

According to Glyn Richards, the early texts of Hinduism state that the Brahman or the nondual Brahman–Atman is the Absolute.

The term has also been adopted by Aldous Huxley in his perennial philosophy to interpret various religious traditions, including Indian religions, and influenced other strands of nondualistic and New Age thought.


</doc>
<doc id="2289941" url="https://en.wikipedia.org/wiki?curid=2289941" title="Terror management theory">
Terror management theory

Terror management theory (TMT) is both a social and evolutionary psychology theory originally proposed by Jeff Greenberg, Sheldon Solomon, and Tom Pyszczynski and codified in their book "The Worm at the Core: On the Role of Death in Life" (2015). It proposes that a basic psychological conflict results from having a self-preservation instinct while realizing that death is inevitable and to some extent unpredictable. This conflict produces terror, and the terror is then managed by embracing cultural beliefs, or symbolic systems that act to counter biological reality with more durable forms of meaning and value.

The most obvious examples of cultural values that assuage death anxiety are those that purport to offer literal immortality (e.g. belief in afterlife, religion). However, TMT also argues that other cultural values – including those that are seemingly unrelated to death – offer symbolic immortality. For example, values of national identity, posterity, cultural perspectives on sex, and human superiority over animals have been linked to death concerns. In many cases these values are thought to offer symbolic immortality either a) by providing the sense that one is part of something greater that will ultimately outlive the individual (e.g. country, lineage, species), or b) by making one's symbolic identity superior to biological nature (i.e. you are a personality, which makes you more than a glob of cells).

Because cultural values determine that which is meaningful, they are also the foundation for self-esteem. TMT describes self-esteem as being the personal, subjective measure of how well an individual is living up to their cultural values.

TMT is derived from anthropologist Ernest Becker's 1973 Pulitzer Prize-winning work of nonfiction "The Denial of Death", in which Becker argues most human action is taken to ignore or avoid the inevitability of death. The terror of absolute annihilation creates such a profound – albeit subconscious – anxiety in people that they spend their lives attempting to make sense of it. On large scales, societies build symbols: laws, religious meaning systems, cultures, and belief systems to explain the significance of life, define what makes certain characteristics, skills, and talents extraordinary, reward others whom they find exemplify certain attributes, and punish or kill others who do not adhere to their cultural worldview. On an individual level, self-esteem provides a buffer against death-related anxiety.

Cultural anthropologist Ernest Becker asserted in his 1973 book "The Denial of Death" that humans, as intelligent animals, are able to grasp the inevitability of death. They therefore spend their lives building and believing in cultural elements that illustrate how to make themselves stand out as individuals and give their lives significance and meaning. Death creates an anxiety in humans; it strikes at unexpected and random moments, and its nature is essentially unknowable, causing people to spend most of their time and energy to explain, forestall, and avoid it.

Becker expounded upon the previous writings of Sigmund Freud, Søren Kierkegaard, Norman O. Brown, and Otto Rank. According to clinical psychiatrist Morton Levitt, Becker replaces the Freudian preoccupation with sexuality with the fear of death as the primary motivation in human behavior.

People desire to think of themselves as beings of value and worth with a feeling of permanence, a concept in psychology known as self-esteem. This feeling counters the cognitive dissonance created by an individual's realization that they may be no more important than any other living thing. Becker refers to high self-esteem as heroism: 

The rationale behind decisions regarding one's own health can be explored through a terror management model. A 2008 research article in Psychological Review proposes a three-part model for understanding how awareness of death can ironically subvert health-promoting behaviors by redirecting one's focus towards behaviors that build self-esteem instead: "Proposition 1 suggests that conscious thoughts about death can instigate health-oriented responses aimed at removing death-related thoughts from current focal attention. Proposition 2 suggests that the unconscious resonance of death-related cognition promotes self-oriented defenses directed toward maintaining, not one's health, but a sense of meaning and self-esteem. The last proposition suggests that confrontations with the physical body may undermine symbolic defenses and thus present a previously unrecognized barrier to health promotion activities."

Terror management theorists consider TMT to be compatible with the theory of evolution: Valid fears of dangerous things have an adaptive function that helped facilitate the survival of our ancestors' genes. However, generalized existential anxiety resulting from the clash between a desire for life and awareness of the inevitability of death is neither adaptive nor selected for. TMT views existential anxiety as an unfortunate byproduct of these two highly adaptive human proclivities rather than as an adaptation that the evolutionary process selected for its advantages. Just as human bipedalism confers advantages as well as disadvantages, death anxiety is an inevitable part of our intelligence and awareness of dangers.

Anxiety in response to the inevitability of death threatened to undermine adaptive functioning and therefore needed amelioration. TMT posits that humankind used the same intellectual capacities that gave rise to this problem to fashion cultural beliefs and values that provided protection against this potential anxiety. TMT considers these cultural beliefs (even unpleasant and frightening ones, such as ritual human sacrifice) when they manage potential death anxiety in a way that promotes beliefs and behaviors which facilitated the functioning and survival of the collective.

Hunter-gatherers used their emerging cognitive abilities to facilitate solving practical problems, such as basic needs for nutrition, mating, and tool-making. As these abilities evolved, an explicit awareness of death also emerged. But once this awareness materialized, the potential for terror that it created put pressure on emerging conceptions of reality. Any conceptual formation that was to be widely accepted by the group needed to provide a means of managing this terror.

Originally, the emergence of morality evolved to facilitate co-existence within groups. Together with language, morality served pragmatic functions that extended survival. The struggle to deny the finality of death co-opted and changed the function of these cultural inventions. For example, Neanderthals might have begun burying their dead as a means of avoiding unpleasant odors, disease-infested parasites, or dangerous scavengers. But during the Upper Paleolithic era, these pragmatic burial practices appear to have become imbued with layers of ritual performance and supernatural beliefs, suggested by the elaborate decoration of bodies with thousands of beads or other markers. Food and other necessities were also included within the burial chamber, indicating the potential for a belief system that included life after death. In many human cultures today, funerals are viewed primarily as cultural events, viewed through the lens of morality and language, with little thought given to the utilitarian origins of burying the dead.

Evolutionary history also indicates that "the costs of ignoring threats have outweighed the costs of ignoring opportunities for self-development." This reinforces the concept that abstract needs for individual and group self-esteem may continue to be selected for by evolution, even when they sometimes confer risks to physical health and well-being.

Self-esteem lies at the heart of TMT and is a fundamental aspect of its core paradigms. TMT fundamentally seeks to elucidate the causes and consequences of a need for self-esteem. Theoretically, it draws heavily from Ernest Becker's conceptions of culture and self-esteem (Becker, 1971; Becker, 1973). TMT not only attempts to explain the concept of self-esteem, it also tries to explain "why" we need self-esteem. One explanation is that self-esteem is used as a coping mechanism for anxiety. It helps people control their sense of terror and nullify the realization that humans are just animals trying to manage the world around them. According to TMT, self-esteem is a sense of personal value that is created by beliefs in the validity of one's cultural worldview, and the belief that one is living up to the cultural standards created by that worldview.

Critically, Hewstone "et al." (2002) have questioned the causal direction between self-esteem and death anxiety, evaluating whether one's self-esteem comes from their desire to reduce their death anxiety, or if death anxiety arises from a lack of self-esteem. In other words, an individual's suppression of death anxiety may arise from their overall need to increase their self-esteem in a positive manner.

Research has demonstrated that self-esteem can play an important role in physical health. In some cases, people may be so concerned with their physical appearance and boosting their self-esteem that they ignore problems or concerns with their own physical health. Arndt "et al." (2009) conducted three studies to examine how peer perceptions and social acceptance of smokers contributes to their quitting, as well as if, and why these people continue smoking for outside reasons, even when faced with thoughts of death and anti-smoking prompts. Tanning and exercising were also looked at in the researchers' studies. The studies found that people are influenced by the situations around them. Specifically, Arndt "et al." (2009) found in terms of their self-esteem and health, that participants who saw someone exercising were more likely to increase their intentions to exercise. In addition, the researchers found in study two that how participants reacted to an anti-smoking commercial was affected by their motivation for smoking and the situation which they were in. For instance, people who smoked for extrinsic reasons and were previously prompted with death reminders were more likely to be compelled by the anti-smoking message.

An individual's level of self-consciousness can affect their views on life and death. To a point, increasing self-consciousness is adaptive in that it helps prevent awareness of danger. However, research has demonstrated that there may be diminishing returns from this phenomenon. Individuals with higher levels of self-consciousness sometimes have increased death cognition, and a more negative outlook on life, than those with reduced self-consciousness.

Conversely, self-esteem can work in the opposite manner. Research has confirmed that individuals with higher self-esteem, particularly in regard to their behavior, have a more positive attitude towards their life. Specifically, death cognition in the form of anti-smoking warnings weren't effective for smokers and in fact, increased their already positive attitudes towards the behavior. The reasons behind individuals' optimistic attitudes towards smoking after mortality was made salient, indicate that people use positivity as a buffer against anxiety. Continuing to hold certain beliefs even after they are shown to be flawed creates cognitive dissonance regarding current information and past behavior, and the way to alleviate this is to simply reject new information. Therefore, anxiety buffers such as self-esteem allow individuals to cope with their fears more easily. Death cognition may in fact cause negative reinforcement that leads people to further engage in dangerous behaviors (smoking in this instance) because accepting the new information would lead to a loss of self-esteem, increasing vulnerability and awareness of mortality.

The mortality salience hypothesis (MS) states that if indeed one's cultural worldview, or one's self-esteem, serves a death-denying function, then threatening these constructs should produce defenses aimed at restoring psychological equanimity (i.e., returning the individual to a state of feeling invulnerable). In the MS paradigm, these "threats" are simply experiential reminders of one's own death. This can, and has, taken many different forms in a variety of study paradigms (e.g., asking participants to write about their own death; conducting the experiment near funeral homes or cemeteries; having participants watch graphic depictions of death, etc.). Like the other TMT hypotheses, the literature supporting the MS hypothesis is vast and diverse. For a meta analysis of MS research, see Burke "et al." (2010).

Experimentally, the MS hypothesis has been tested in close to 200 empirical articles. After participants in an experiment are asked to write about their own death (vs. a neutral, non-death control topic, such as dental pain), and then following a brief delay (distal, worldview/self-esteem defenses work the best after a delay; see Greenberg "et al." (1994) for a discussion), the participants' defenses are measured. In one early TMT study assessing the MS hypothesis, Greenberg "et al." (1990) had Christian participants evaluate other Christian and Jewish students that were similar demographically, but differed in their religious affiliation. After being reminded of their death (experimental MS induction), Christian participants evaluated fellow Christians more positively, and Jewish participants more negatively, relative to the control condition. Conversely, bolstering self-esteem in these scenarios leads to less worldview defense and derogation of dissimilar others.

Mortality salience has an influence on individuals and their decisions regarding their health. Cox "et al." (2009) discuss mortality salience in terms of suntanning. Specifically, the researchers found that participants who were prompted with the idea that pale was more socially attractive along with mortality reminders, tended to lean towards decisions that resulted in more protective measures from the sun. The participants were placed in two different conditions; one group of participants were given an article relating to the fear of death, while the control group received an article unrelated to death dealing with the fear of public speaking. Additionally, they gave one group an article pertaining to the message that "bronze is beautiful," one relating to the idea that "pale is pretty," and one neutral article that did not speak of tan or pale skin tones. Finally, after introducing a delay activity, the researchers gave the participants a five-item questionnaire asking them about their future sun-tanning behaviors. The study illustrated that when tan skin was associated with attractiveness, mortality salience positively affected people's intentions to suntan; however, when pale skin was associated with attractiveness people's intentions to tan decreased.

Studies have shown that mortality and self-esteem are important factors of the terror management theory. Jessop "et al." (2008) study this relationship within four studies that all examine how people react when they are given information on risks, specifically, in terms of the mortality related to the risks of driving. More specifically, the researchers were exploring how participants acted in terms of self-esteem, and its impact on how mortality-related health-risk information would be received. Overall, Jessop "et al." (2008) found that even when mortality is prominent, people who engage in certain behaviors to improve their self-esteem have a greater chance of continuing with these activities. Mortality and self-esteem are both factors that influence people's behaviors and decision-making regarding their health. Furthermore, individuals who are involved in behaviors and possess motivation to enhance their self-worth are less likely to be affected by the importance placed on health risks, in terms of mortality.

Self-esteem is important when mortality is made salient. It can allow people a coping mechanism, one that can cushion individuals' fears; and thus, impacting one's attitudes towards a given behavior. Individuals who have higher levels of self-esteem regarding their behavior(s) are less likely to have their attitudes, and thus their behaviors changed regardless of mortality salient or death messages. People will use their self-esteem to hide behind their fears of dying. In terms of smoking behaviors, people with higher smoking-based self-esteem are less susceptible to anti-smoking messages that relate to death; therefore, mortality salience and death warnings afford them with an even more positive outlook on their behavior, or in this instance their smoking.

In the Hansen "et al." (2010) experiment the researchers manipulated mortality salience. In the experiment, Hansen "et al." (2010) examined smokers' attitudes towards the behavior of smoking. Actual warning labels were utilized to create mortality salience in this specific experiment. The researchers first gave participants a questionnaire to measure their smoking-based self-esteem. Following the questionnaire, participants were randomly assigned to two different conditions; the first were given anti-smoking warning labels about death and the second control group were exposed to anti-smoking warning labels not dealing with death. Before the participants' attitudes towards smoking were taken the researchers introduced an unrelated question to provide a delay. Further research has demonstrated that delays allow mortality salience to emerge because thoughts of death become non-conscious. Finally, participants were asked questions regarding their intended future smoking behavior. However, one weakness in their conduction was that the final questionnaire addressed opinions and behavioral questions, as opposed to the participants level of persuasion regarding the different anti-smoking warning labels.

Many people are more motivated by social pressures, rather than health risks. Specifically for younger people, mortality salience is stronger in eliciting changes of one's behavior when it brings awareness to the immediate loss of social status or position, rather than a loss, such as death that one can not imagine and feels far off. However, there are many different factors to take into consideration, such as how strongly an individual feels toward a decision, his or her level of self-esteem, and the situation around the individual. Particularly with people's smoking behaviors, self-esteem and mortality salience have different effects on individuals' decisions. In terms of the longevity of their smoking decisions, it has been seen that individuals' smoking habits are affected, in the short-term sense, when they are exposed to mortality salience that interrelates with their own self-esteem. Moreover, people who viewed social exclusion prompts were more likely to quit smoking in the long run than those who were simply shown health-effects of smoking. More specifically, it was demonstrated that when individuals had high levels of self-esteem they were more likely to quit smoking following the social pressure messages, rather than the health risk messages. In this specific instance, terror management, and specifically mortality salience is showing how people are more motivated by the social pressures and consequences in their environment, rather than consequences relating to their health. This is mostly seen in young adult smokers with higher smoking-based self-esteems who are not thinking of their future health and the less-immediate effects of smoking on their health.

Another paradigm that TMT researchers use to get at unconscious concerns about death is what is known as the death thought accessibility (DTA) hypothesis. Essentially, the DTA hypothesis states that if individuals are motivated to avoid cognitions about death, and they avoid these cognitions by espousing a worldview or by buffering their self-esteem, then when threatened, an individual should possess more death-related cognitions (e.g., thoughts about death, and death-related stimuli) than they would when not threatened.

The DTA hypothesis has its origins in work by Greenberg "et al." (1994) as an extension of their earlier terror management hypotheses (i.e., the anxiety buffer hypothesis and the mortality salience hypothesis). The researchers reasoned that if, as indicated by Wegner's research on thought suppression (1994; 1997), thoughts that are purposely suppressed from conscious awareness are often brought back with ease, then following a delay death-thought cognitions should be more available to consciousness than (a) those who keep the death-thoughts in their consciousness the whole time, and (b) those who suppress the death-thoughts but are not provided a delay. That is precisely what they found. However, other psychologists have failed to replicate these findings.

In these initial studies (i.e., Greenberg "et al." (2004); Arndt "et al." (1997)), and in numerous subsequent DTA studies, the main measure of DTA is a word fragment task, whereby participants can complete word fragments in distinctly death-related ways (e.g., coff_ _ as coffin, not coffee) or in non death-related ways (e.g., sk_ _l as skill, not skull). If death-thoughts are indeed more available to consciousness, then it stands to reason that the word fragments should be completed in a way that is semantically related to death.

The introduction of this hypothesis has refined TMT, and led to new avenues of research that formerly could not be assessed due to the lack of an empirically validated way of measuring death-related cognitions. Also, the differentiation between proximal (conscious, near, and threat-focused) and distal (unconscious, distant, symbolic) defenses that have been derived from DTA studies have been extremely important in understanding how people deal with their terror.

It is important to note how the DTA paradigm subtly alters, and expands, TMT as a motivational theory. Instead of solely manipulating mortality and witnessing its effects (e.g., nationalism, increased prejudice, risky sexual behavior, etc.), the DTA paradigm allows a measure of the death-related cognitions that result from various affronts to the self. Examples include threats to self-esteem and to one's worldview; the DTA paradigm can therefore assess the role of death-thoughts in self-esteem and worldview defenses. Furthermore, the DTA hypothesis lends support to TMT in that it corroborates its central hypothesis that death is uniquely problematic for human beings, and that it is fundamentally different in its effects than meaning threats, (i.e., Heine "et al.", 2006) and that is death itself, and not uncertainty and lack of control associated with death; Fritsche "et al." (2008) explore this idea.

Since its inception, the DTA hypothesis had been rapidly gaining ground in TMT investigations, and as of 2009, has been employed in over 60 published papers, with a total of more than 90 empirical studies.

How people respond to their fears and anxiety of death is investigated in TMT. Moreover, Taubman-Ben-Ari and Noy (2010) examine the idea that a person's level of self-awareness and self-consciousness should be considered in relation to their responses to their anxiety and death cognitions. The more an individual is presented with their death or death cognitions in general, the more fear and anxiety one may have; therefore, to combat said anxiety one may implement anxiety buffers.

Due to a change in people's lifestyles, in the direction of more unhealthy behaviors, the leading causes of death now, being cancer and heart disease, most definitely are related to individuals' unhealthy behaviors (though the statement is over-generalising and certainly cannot be applied to every case). Age and death anxiety both are factors that should be considered in the terror management theory, in relation to health-promoting behaviors. Age undoubtedly plays some kind of role in people's health-promoting behaviors; however, an actual age related effect on death anxiety and health-promoting behaviors has yet to be seen. Although, research has demonstrated that for young adults only, when they were prompted with death related scenarios, they yielded more health-promoting behaviors, compared to those participants in their sixties. In addition, death anxiety has been found to have an effect for young adults, on their behaviors of health promotion.

The terror management health model (TMHM) explores the role that death plays on one's health and behavior. Goldenberg and Arndt (2008) state that the TMHM proposes the idea that death, despite its threatening nature, is in fact instrumental and purposeful in the conditioning of one's behavior towards the direction of a longer life.

According to Goldenberg and Arndt (2008), certain health behaviors such as breast self-exams (BSEs) can consciously activate and facilitate people to think of death, especially their own death. While death can be instrumental for individuals, in some cases, when breast self-exams activate people's death thoughts an obstacle can present itself, in terms of health promotion, because of the experience of fear and threat. Abel and Kruger (2009) have suggested that the stress caused by increased awareness of mortality when celebrating one's birthday might explain the birthday effect, where mortality rates seem to spike around these days.

On the other hand, death and thoughts of death can serve as a way of empowering the self, not as threats. Researchers, Cooper "et al." (2011) explored TMHM in terms of empowerment, specifically using BSEs under two conditions; when death thoughts were prompted, and when thoughts of death were non-conscious. According to TMHM, people's health decisions, when death thoughts are not conscious, should be based on their motivations to act appropriately, in terms of the self and identity. Cooper "et al." (2011) found that when mortality and death thoughts were primed, women reported more empowerment feelings than those who were not prompted before performing a BSE.

Additionally, TMHM suggests that mortality awareness and self-esteem are important factors in individuals' decision making and behaviors relating to their health. TMHM explores how people will engage in behaviors, whether positive or negative, even with the heightened awareness of mortality, in the attempt to conform to society's expectations and improve their self-esteem. The TMHM is useful in understanding what motivates individuals regarding their health decisions and behaviors.

In terms of smoking behaviors and attitudes, the impact of warnings with death messages depends on:

People with low self-esteem, but not high self-esteem, have more negative emotions when reminded of death. This is believed to be because these individuals lack the very defenses that TMT argues protect people from mortality concerns (e.g., solid worldviews). In contrast, positive mood states are not impacted by death thoughts for people of low or high self-esteem.

It has been suggested that culture provides meaning, organization, and a coherent world view that diminishes the psychological terror caused by the knowledge of eventual death. The terror management theory can help to explain why a leader's popularity can grow substantially during times of crisis. When a follower's mortality is made prominent they will tend to show a strong preference for iconic leaders. An example of this occurred when George W. Bush's approval rating jumped almost 50 percent following the September 11 attacks in the United States. As Forsyth (2009) posits, this tragedy made U.S. citizens aware of their mortality, and Bush provided an antidote to these existential concerns by promising to bring justice to the terrorist group responsible for the attacks.

Researchers Cohen "et al." (2004), in their particular study on TMT, tested the preferences for different types of leaders, while reminding people of their mortality. Three different candidates were presented to participants. The three leaders were of three different types: task-oriented (emphasized setting goals, strategic planning, and structure), relationship-oriented (emphasized compassion, trust, and confidence in others), and charismatic. The participants were then placed in one of two conditions: mortality salient or control group. In the former condition the participants were asked to describe the emotions surrounding their own death, as well as the physical act of the death itself, whereas the control group were asked similar questions about an upcoming exam. The results of the study were that the charismatic leader was favored more, and the relationship-oriented leader was favored less, in the mortality-salient condition. Further research has shown that mortality salient individuals also prefer leaders who are members of the same group, as well as men rather than women (Hoyt "et al." 2010). This has links to social role theory.

TMT posits that religion was created as a means for humans to cope with their own mortality. Supporting this, arguments in favor of life after death, and simply being religious, reduce the effects of mortality salience on worldview defense. Thoughts of death have also been found to increase religious beliefs. At an implicit, subconscious level, this is the case even for people who claim to be nonreligious.

Psychologists, especially evolutionary psychologists, have argued against terror management theory. One scholar commented that the field of psychology would be advanced by a study of paralyzed states caused by anxiety that would only be alleviated with the reworking of a person's mental state. These authors instead explain human behavior is selected to urge people to avoid situations likely to lead to death. This suggests that mortality salience effects reflect adaptive responses to solve specific life-threats rather than an unconscious attempt to avoid this realization.

Since findings on mortality salience and worldview defense were first published, other researchers have claimed that the effects may have been obtained due to reasons other than death itself, such as anxiety, fear, or other aversive stimuli such as pain. Other studies have found effects similar to those that MS results in – for example, thinking about difficult personal choices to be made, being made to respond to open-ended questions regarding uncertainty, thinking about being robbed, thinking about being socially isolated, and being told that one's life lacks meaning. While these cases exist, thoughts of death have since been compared to various aversive experimental controls, such as (but not limited to) thinking about: failure, writing a critical exam, public speaking with a considerable audience, being excluded, paralysis, dental pain, intense physical pain, etc.

With regards to the studies that found similar effects, TMT theorists have argued that in the previously mentioned studies where death was not the subject thought about, the subjects would quite easily be related to death in an individual's mind due to "linguistic or experiential connection with mortality" (p. 332) For example, being robbed invokes thoughts of violence and being unsafe in one's own home – many people have died trying to protect their property and family. A second possible explanation for these results involves the death-thought accessibility hypothesis: these threats somehow sabotage crucial anxiety-buffering aspects of an individual's worldview or self-esteem, which increases their DTA. For example, one study found increased DTA in response to thoughts of antagonistic relations with attachment figures.

The meaning maintenance model (MMM) was initially introduced as a comprehensive motivational theory that claimed to subsume TMT, with alternative explanations for TMT findings. Essentially, it posits that people automatically give meaning to things, and when those meanings are somehow disrupted, it causes anxiety. In response, people concentrate on "meaning maintenance to reestablish their sense of symbolic unity" and that such "meaning maintenance often involves the compensatory reaffirmation of alternative meaning structures". These meanings, among other things, should "provide a basis for prediction and control of our...environments, help [one] to cope with tragedy and trauma...and the symbolic cheating of death via adherence to the enduring values that these cultures provide".

TMT theorists argue that MMM cannot describe why different sets of meaning are preferred for a symbol by different people, and that while they may exist, "different [(i.e., more concrete)] types of meaning have different psychological functions". For example, MMM theorists argue that all types of meaning are basically equal, and yet one could not compare the likelihood of defensive responses resulting from exposure to a deck of cards containing black hearts with something like the September 11 attacks. TMT theorists argue, essentially, that unless something is an important element of a person's anxiety-buffering worldview or self-esteem, it will not require broad meaning maintenance.

In sum, TMT theorists believe that MMM cannot accurately claim to be an alternative to TMT because it does not seem to be able to explain the current breadth of TMT evidence. As an example, TMT theorists assert that mortality salience would not be a threat to meaning, since our eventual demise is a necessary condition of life. Therefore, it should not cause an individual to engage in general meaning maintenance. MMM also makes no attempt to explain why threatening meaning increases DTA.

Some theorists have argued that it is not the idea of death and nonexistence that is unsettling to people, but the fact that uncertainty is involved. For example, these researchers posited that people defend themselves by altering their fear responses from uncertainty to an enthusiasm approach. Other researchers argue for distinguishing fear of death from fear of dying and, therein, posit that ultimately the fear of death has more to do with some other fear (e.g., fear of pain) or reflects fear of the unknown.

TMT theorists agree that uncertainty can be disconcerting in some cases and it may even result in defense responses, but note that they believe the inescapability of death and the possibility of its finality regarding one's existence is most unsettling. They ask, "'Would death be any less frightening if you knew for certain that it would come next Tuesday at 5:15 p.m., and that your hopes for an afterlife were illusory?'...Would you rather be certain that death is the end, or live with the uncertainty that it might not be?" They also note that people actually seek out some types of uncertainty, and that being uncertain is not always very unpleasant. In contrast, there is substantial evidence that, all things being equal, uncertainty and the unknown represent fundamental fears and are only experienced as pleasant when there is sufficient contextual certainty. For example, a surprise involves uncertainty, but is only perceived as pleasant if there is sufficient certainty that the surprise will be pleasant. Consider a box received on a birthday from a trusted family member as compared to the box received at the end of the film "Seven" (which contains a severed head).

Though TMT theorists acknowledge that many responses to mortality salience involve greater approaches (zealousness) towards important worldviews, they also note examples of mortality salience which resulted in the opposite, which offensive defensiveness cannot account for: when negative features of a group to which participants belong were made salient, people actively distanced themselves from that group under mortality salience.

Several critiques have been proposed against TMT from evolutionary psychologists – for reasons including that fear is an adaptive response in individuals that has come about as a result of natural selection; without these adaptations human beings would have never been able to avoid maladaptive situations. Thus, it is unlikely that people would have psychological ways of slowing-down anxiety. In response, TMT theorists argue that this critique is mixing up fear related to immediate danger with anxiety related to thoughts of threats that will or may occur eventually. TMT is talking about the protection that self-esteem and cultural worldviews offer against the threat of unavoidable death in the "future". While anxiety may be adaptive in avoiding entering a dangerous place (e.g. because a predator may be waiting), this doesn't mean that anxiety must be adaptive in all cases. For a more comprehensive review of TMT and evolutionary psychology, see Landau "et al.", 2007. Similar evolutionary critiques have been raised by researchers exploring uncertainty and unknowns (see for reviews,).

Coalitional psychology (CP) is presented as another alternative to TMT, which proposes that there is an evolutionary tendency to seek safety in groups (coalitions) as a reaction to adaptive threats. People already a part of coalitional groups seek to protect their membership by exhibiting their value to the group. TMT theorists answer by arguing that CP 1) cannot account for the fact that virtually all cultures have a supernatural dimension; 2) it does not explain why cultural worldview defense is symbolic, involving allegiance to both specific and general systems of abstract meaning unrelated to specific threats, rather than focused on the specific adaptive threats it supposedly evolved to deal with; 3) it dismisses TMT's dual process account of the underlying processes that generate MS effects without providing an alternative of any kind or attempting to account for the data relevant to this aspect of the TMT analysis and 4) the experiments testing hypotheses derived from CP do not provide compelling or unique support for CP, 5) it cannot account for a host of empirical findings supporting hypotheses derived from TMT that could never be deduced from CP.

In addition to the criticisms from alternative theoretical perspectives, the most damaging critique of TMT is empirical. A large scale effort to test the theory failed to obtain support for the mortality salience effect on worldview defense, i.e. a greater preference for an essay writer adopting a pro-U.S. argument than an essay writer adopting an anti-U.S. argument. The test is a multi-lab replication of Study 1 of Greenberg et al. (1994). Psychologists in 21 labs across the U.S. re-executed the original experiment, among a total of 2,200 participants. The results showed no reliable evidence for a mortality salience effect, adding to the replication crisis. Importantly, the results were not only close to zero for attempts by researchers who independently re-executed the experiment, but also for researchers who consulted with the original authors.



Discusses TMT at length

TMT and self-esteem


</doc>
<doc id="18958074" url="https://en.wikipedia.org/wiki?curid=18958074" title="Closure (business)">
Closure (business)

Closure is the term used to refer to the actions necessary when it is no longer necessary or possible for a business or other organization to continue to operate. Closure may be the result of a bankruptcy, where the organization lacks sufficient funds to continue operations, as a result of the proprietor of the business dying, as a result of a business being purchased by another organization (or a competitor) and shut down as superfluous, or because it is the non-surviving entity in a corporate merger. A closure may occur because the purpose for which the organization was created is no longer necessary.

While a closure is typically of a business or a non-profit organization, any entity which is created by human beings can be subject to a closure, from a single church to a whole religion, up to and including an entire country if, for some reason, it ceases to exist.

Closures are of two types, voluntary or involuntary. Voluntary closures of organizations are much rarer than involuntary ones, as, in the absence of some change making operations impossible or unnecessary, most operations will continue until something happens that causes a change requiring this situation.

The most common form of voluntary closure would be when those involved in an organization such as a social club, a band, or other non-profit organization decide to cease operating. Once the organization has paid any outstanding debts and completed any pending operations, closure may simply mean that the organization ceases to exist.

If an organization has debts that cannot be paid, it may be necessary to perform a liquidation of its assets. If there is anything left after the assets are converted to cash, in the case of a for-profit organization, the remainder is distributed to the stockholders; in the case of a non-profit, by law any remaining assets must be distributed to another non-profit.

If an organization has more debts than assets, it may have to declare bankruptcy. If the organization is viable, it may reorganizes itself as a result of the bankruptcy and continue operations. If it is not viable for the business to continue operating, then a closure occurs through a bankruptcy liquidation: its assets are liquidated, the creditors are paid from whatever assets could be liquidated, and the business ceases operations.

Possibly the largest "closure" in history (but more closely analogous to a demerger) was the split of the Soviet Union into its constituent countries. In comparison, the end of East Germany can be considered a merger rather than a closure as West Germany assumed all of the assets and liabilities of East Germany. The end of the Soviet Union was the equivalent of a closure through a bankruptcy liquidation, because while Russia assumed most of the assets and responsibilities of the former Soviet Union, it did not assume all of them. There have been issues over who is responsible for unpaid parking tickets accumulated by motor vehicles operated on behalf of diplomatic missions operated by the former Soviet Union in other countries, as Russia claims it is not responsible for them.

Several major business closures include the bankruptcy of the Penn Central railroad, the Enron scandals, and MCI Worldcom's bankruptcy and eventual merger into Verizon.


</doc>
<doc id="18166327" url="https://en.wikipedia.org/wiki?curid=18166327" title="Business acumen">
Business acumen

Business acumen ("Business savvy" and "business sense" are often used as synonyms) is keenness and quickness in understanding and dealing with a "business situation" (risks and opportunities) in a manner that is likely to lead to a good outcome. Additionally, business acumen has emerged as a vehicle for improving financial performance and leadership development. Consequently, several different types of strategies have developed around improving business acumen.

In his 2012 work, "Seeing the Big Picture, Business Acumen to Build Your Credibility, Career, and Company", Kevin R. Cope put forward that an individual who possesses business acumen views the business with an "executive mentality" - they understand how the moving parts of a company work together to make it successful and how financial metrics like profit margin, cash flow, and stock price reflect how well each of those moving parts is doing its job. Cope proposes that an individual who has the following five abilities could be described as someone having a strong sense of business acumen: 

Raymond R. Reilly of the Ross School of Business at the University of Michigan and Gregory P. Reilly of the University of Connecticut document traits that individuals with business acumen possess: 
Thus, developing stronger business acumen means a more thoughtful analysis, clearer logic underlying business decisions, closer attention to key dimensions of implementation and operation, and more disciplined performance management.

Financial literacy is a comprehensive understanding of the drivers of growth, profitability, and cash flow; an organization's financial statements; key performance measures; and the implications of decisions on value creation. In a SHRM article entitled, "Business Acumen Involves More Than Numbers" Chris Berger, member of human resources at CTPartners, explains that business acumen starts with the ability to understand how a company makes decisions, and that leaders must be financially literate and be able to understand numbers on company financial statements. It entails the ability to take the knowledge of business fundamentals and use it to think strategically and then take appropriate action.

According to E. Ted Prince, "Financial literacy is almost never the need for senior managers and high potentials. Most already possess degrees in business, including MBAs, and many have also had experience in the business sides of their professional roles. The need for these managers is to understand how their actions and their behavior impact their financial decision-making and how this in turn affects financial outcomes at the unit and the corporate level." It's evident that an individual with business acumen has some level of financial understanding and knowledge - but someone who is financially literate doesn't necessarily possess strong business acumen.

Bob Selden, faculty member of Mobilizing People, a leadership development program based in Switzerland, observes a complementary relationship between business acumen and leadership. Selden states the importance of nurturing both the development of strategic skills and that of good leadership and management skills in order for business leaders to achieve effectiveness.

A study published in Human Resource Management International Digest titled, Business acumen: a critical concern of modern leadership development: Global trends accelerate the move away from traditional approaches, reveals why traditional leadership development approaches, which rely on personality and competency assessments as the scientific core of their approach, are failing. The paper demonstrates the importance of business acumen in leadership-development approaches and contends that business acumen will have an increasing impact on leadership development and HR agendas. Research into this relationship resulted in the creation of the Perth Leadership Outcome Model, which links financial outcomes to individual leadership traits.

In a study that interviewed 55 global business leaders, business acumen was cited as the most critical competency area for global leaders.

In their 2011 work, "The Leadership Pipeline", Ram Charan, Stephen Drotter, and James Noel study the process and criteria for selecting a group manager, and suggest that the process and criteria are similar for selecting a CEO. According to them an obvious criterium for selecting a leader is well-developed business acumen.

An organization full of high business acumen individuals can expect to see leaders with a heightened perspective that translates into an ability to inspire and excite the organization to achieve its potential.

Programs designed to improve an individual or group's business acumen have supported the recognition of the concept as a significant topic in the corporate world. Executive Development Associates' 2009/2010 survey of Chief Learning Officers, Senior Vice Presidents of Human Resources, and Heads of Executive and Leadership Development listed business acumen as the second most significant trend in executive development. A 2011 report explores the impact of business acumen training on an organization in terms of intangibles and more tangible expressions of value. The findings support the notion that business acumen is a learned skill - developed on the job by learning the required skills from knowledge mentors while working in different employment positions. They also suggest that the learning process ranges widely, from structured internal company training programs, to an individual's self-chosen moves from one position to another.

The combination of these reports and surveys indicate that business acumen is a learned skill of increasing importance within the corporate world. There are different types of business acumen development programs available:

A business simulation is another corporate development tool used to increase business acumen. Several companies offer business simulations as a way to educate mid-level managers and non-financial leaders within their organization on cash flow and financial-decision-making processes. Their forms can vary from computer simulations to boardgame-style simulations.

The advent of personal assessments for business acumen is based in the emerging theories of behavioral finance and attempts to correlate innate personality traits with positive financial outcomes. This method approaches business acumen not as entirely based in either knowledge or experience, but on the combination of these and other factors which comprise an individual's financial personality or "signature." The results from this research have been limited, but noteworthy.


</doc>
<doc id="26085407" url="https://en.wikipedia.org/wiki?curid=26085407" title="Business interaction networks">
Business interaction networks

Business interaction networks are networks that allow businesses and their communities of interest to collaborate and do business online securely via the Internet. 

Mary Johnston Turner first discussed the concept in a "Network World" opinion piece in August 1995 and attributed the first advocacy for the concept to the now-defunct BBN Planet, the ISP division of BBN Technologies.


</doc>
<doc id="16416558" url="https://en.wikipedia.org/wiki?curid=16416558" title="Business idea">
Business idea

A business idea is a concept that can be used for financial gain that is usually centered on a product or service that can be offered for money. An idea is the base of the pyramid when it comes to the business as a whole.

The characteristics of a promising business idea are:

A business idea is often linked to its creator who needs to identify the business' value proposition in order to launch to market and establish competitive advantage.

For businesses this could mean: creating new ideas, new product development through research and development or improving existing services. Innovation can be the central focus of a business and this can help them to grow and become a market leader if they execute their ideas properly. Businesses that are focused on innovation are usually more efficient, cost effective and productive. Successful innovation should be built into the business strategy, where you can create a culture of innovation and drive forward creative problem solving.


These successful companies were built on sheer innovation and we can see how valuable they have become in the short time they have been around or have been focusing on innovation. When Tesla's value is compared to that of General Motors, we see that the market capitalization of General Motors is $53.98 billion today in which the company has been around since 1908 whereas Tesla was founded in 2003 and has achieved 50% of General Motors value within 12 years.

A unique selling point (USP) is the factor that makes a company or a product stand out from its competitors, whether it is through; pricing, quality, customer service or innovation.

Each successful company has a unique selling proposition (USP). A USP can be created through the element of being first to a market, for example Uber was the first company to allow for taxicab hailing via mobile app. Because Uber had reached this market first, it had a USP and therefore it received loyal customers. However; with fierce competition copying Uber's business model, Uber has had to develop its service through innovation. 

Business ideas that solve problems are fundamental to developing the world and companies such as Curemark are one of many who do this. Curemark is a biotech company founded by Joan Fallon, who noticed that a lot of the children she treated were low on an enzyme for processing protein and since then she has quit her job and has built Curemark to solve this problem. Curemark has now raised $50 million and is on its way to solving a problem that truly exists.

Profitability is a business's ability to generate earnings compared to its costs over a certain period of time. This is possibly the most important aspect of any business idea in the long term, as this is what makes a business survive in order to keep having the impact that it has. Profitable ideas need a strong revenue stream against its costs and this tends to create the success of the business, however some companies defy this and make losses to begin with, yet are still exceptional business ideas that are worth billions.




</doc>
<doc id="22697384" url="https://en.wikipedia.org/wiki?curid=22697384" title="Business sector">
Business sector

In business, the business sector or corporate sector - sometimes popularly called simply "business" - is "the part of the economy made up by companies".
It is a subset of the domestic economy,
excluding the economic activities of general government, of private households, and of non-profit organizations serving individuals.
An alternative analysis of economies, the three-sector theory, subdivides them into:


In the United States the business sector accounted for about 78 percent of the value of gross domestic product (GDP) . Kuwait and Tuvalu each had business sectors accounting for less than 40% of GDP .

The Oxford English Dictionary records the phrase "business sector" in the general sense from 1934.
Word usage suggests that the concept of a "business sector" came into wider use after 1940.
Related terms in previous times included "merchant class" and "merchant caste".





</doc>
<doc id="2886264" url="https://en.wikipedia.org/wiki?curid=2886264" title="Core business">
Core business

The core business of an organization is an idealized construct intended to express that organization's "main" or "essential" activity. 

The corporate trend in the mid-20th Century, of acquiring new enterprises and forming conglomerates, enabled corporations to reduce costs funds and similar investment vehicles, and sometimes the following of a popular trend among corporate management seeking to appeal to current and impress investors.

Core business process means that a business's success depends not only on how well each department performs its work, but also on how well the company manages to coordinate departmental activities to conduct the core business process, which is;

1. The market-sensing process
Meaning all activities in gathering marketing intelligence and acting on the information.

2. The new-offering realization process
Covering all activities in research, development and launching new quality offerings quickly and within budget.

3. The customer acquisition process
all the activities defining the target market and prospecting for new customers

4. The customer relationship management process
all the activities covering building deeper understanding, relationships and offerings to individual customers.

5. The fulfillment management process
all the activities in receiving and approving orders, shipping out on time and collecting payment.

To be successful, a business needs to look for competitive advantages beyond its own operations. The business needs to look at the competitiveness value chain of suppliers, distributors and customers. Many companies today have partnered with specific suppliers and distributors to create a superior value delivery network.

Kotler & Keller, Marketing management, 2009, p76

Core competency


</doc>
<doc id="2633364" url="https://en.wikipedia.org/wiki?curid=2633364" title="Registered office">
Registered office

A registered office is the official address of an incorporated company, association or any other legal entity. Generally it will form part of the public record and is required in most countries where the registered organization or legal entity is incorporated. A registered physical office address is required for incorporated organizations to receive official correspondence and formal notices from government departments, investors, banks, shareholders and the general public.

In the United Kingdom and many other common law countries, the registered office address does not have to be where the organization conducts its actual business or trade, and it is not unusual for law firms, accountants or incorporation agents to provide the official registered office address service. In the United Kingdom all statutory correspondence for an incorporated organization (e.g. formal notices, service of process, tax and government communications) is posted or hand delivered to the registered office address as recorded on the Companies House register. A registered physical office address is required for incorporated organizations to receive official correspondence and formal notices from government departments, investors, banks, shareholders and the general public.

Under regulations implemented in the UK on 1 October 2009, company directors may now also use a registered office address instead of their private home address for contact on the Companies House register.

The company's full registered name must be visible to the public on the premises. The Company's records previously had to be kept at the registered office and available for public inspection. Since 1 October 2009 it has been possible for companies to designate a single alternative inspection location (SAIL) as a place to keep their records which must be available for public inspection.

In many other countries the address with which a company is registered must be where its headquarters or seat is located, and this will often determine the subnational registry at which the company must be registered.



</doc>
<doc id="8613151" url="https://en.wikipedia.org/wiki?curid=8613151" title="Business economics">
Business economics

Business economics is a field in applied economics which uses economic theory and quantitative methods to analyze business enterprises and the factors contributing to the diversity of organizational structures and the relationships of firms with labour, capital and product markets. A professional focus of the journal "Business Economics" has been expressed as providing "practical information for people who apply economics in their jobs."

Business economics is an integral part of traditional economics and is an extension of economic concepts to the real business situations. It is an applied science in the sense of a tool of managerial decision-making and forward planning by management. In other words, business economics is concerned with the application of economic theory to business management. Business economics is based on microeconomics in two categories: positive and normative. 

Business economics focuses on the economic issues and problems related to business organization, management, and strategy. Issues and problems include: an explanation of why "corporate" firms emerge and exist; why they expand: horizontally, vertically and spacially; the role of entrepreneurs and entrepreneurship; the significance of organizational structure; the relationship of firms with employees, providers of capital, customers, and government; and interactions between firms and the business environment.

The term 'business economics' is used in a variety of ways. Sometimes it is used as synonymously with industrial economics/industrial organisation, managerial economics, and economics for business. Still, there may be substantial differences in the usage of 'economics for business' and 'managerial economics' with the latter used more narrowly.
One view of the distinctions between these would be that business economics is wider in its scope than industrial economics in that it would be concerned not only with "industry" but also businesses in the service sector. Economics for business looks at the major principles of economics but focuses on applying these economic principles to the real world of business. Managerial economics is the application of economic methods in the managerial decision-making process.

Many universities offer courses in business economics and offer a range of interpretations as to the meaning of the word. The Bachelor of Business Economics (BBE) Program at University of Delhi is designed to meet the growing need for an analytical and quantitative approach to problem solving in the changing corporate world by the application of the latest techniques evolved in the fields of economics and business. The Autonomous University of Barcelona (UAB), the Universidad Pública de Navarra (UPNa) and the University of the Balearic Islands (UIB) developed an official Master of Science in Management, Organization and Business Economics focused on management and business topics to train professionals in the study of organizations, on a conceptual and quantitative basis. To achieve this, advanced analysis tools are used from the fields of Neoclassical economics, New institutional economics, Statistics, Econometrics and Operations research. This focus is complemented with contributing ideas and theories to develop the necessary instruments to facilitate the management of sophisticated and complex organizations.

The program at Harvard University uses economic methods to analyze practical aspects of business, including business administration, management, and related fields of business economics.. The Universidad del Desarrollo, in Chile follows on Harvard University definition, adding entrepreurnship as a field of business. The University of Miami defines business economics as involving the study of how we use our resources for the production, distribution, and consumption of goods and services. This requires business economists to analyze social institutions, banks, the stock market, the government and their relationships with labor negotiations, taxes, international trade, and urban and environmental issues.

Courses at the University of Manchester interpret business economics to be concerned with the economic analysis of how businesses contribute to welfare of society rather than on the welfare of an individual or a business. This is done via an examination of the relationship between ownership, control and firm objectives; theories of the growth of the firm; the behavioural theory of the firm; theories of entrepreneurship; the factors that influence the structure, conduct and performance of business at the industry level.

Italian universities borrow their concept of business economics from the tradition of Gino Zappa, for example a standard course at the Politecnico di Milano involves studying corporate governance, accounting, investment analysis, budgeting and business strategy.

La Trobe University of Melbourne, Australia associates business economics with the process of demand, supply and equilibrium coordinating the behaviour of individuals and businesses in the market. Also, business economics extends to government policy, economic variables and international factors which influence business and competition.





</doc>
<doc id="47827092" url="https://en.wikipedia.org/wiki?curid=47827092" title="Business guru">
Business guru

A business guru or management guru is a leading authority on business practices and can be defined as 'a person with influential ideas or theories about business'. The earliest use of the term business guru can be tracked back to the 1960s being used in "Business Week". There are no existing qualifications that make someone a business guru. Anyone can become a business guru by making impact in a particular business field. It's also possible to claim to be a business guru at any time. It's not a title. The lists of people who have been accepted as business gurus have constantly changed over time. However, there are some people who have been accepted by a great majority as a business guru and also some organizations which have created their own lists of gurus. One English writer has described management gurus as "overwhelmingly a US phenomenon."

There is no definitive list of business gurus, but some writers have proposed "personal" lists. These lists are mostly created by organizations such as business magazines or management writers. There have been many business guru lists created through history.

A list consisting of people who are included in almost all of the lists created, collectively known as the "Famous Five", are: Frederick Winslow Taylor, Michael Porter, Alfred Sloan, Peter Drucker, and Douglas McGregor.

In 2001, "Harvard Business Review" asked the gurus to name their favorite gurus. The people named were Peter Drucker, James March and Herbert Simon.

Another list includes Peter Drucker, Michael Porter, and Tom Peters as the three leading gurus of our time.

There are also many gurus who have emerged and disappeared through history. For example, the Japanese were known for making improvements to the business world and bringing out gurus in the 1980s, which included Keniche Ohmae and Akio Morita. Then European gurus emerged, which included Yves Doz, Geert Hofstede, Manfred Kets De Vries and Charles Handy.

One management expert, Gary Hamel, says there have been "few genuine breakthroughs" since the work of Taylor and Max Weber. In his book, Hamel says that management is "stuck in a time warp." Similarly, even one of the authors of a book about management gurus warns that management theory is "not served well by fads," citing Enron as a "management fad for its supposed culture of innovation."



</doc>
<doc id="30781093" url="https://en.wikipedia.org/wiki?curid=30781093" title="Religion and business">
Religion and business

Religion and business have throughout history interacted in ways that relate to and affected one another, as well as influenced sociocultural evolution, political geographies, and labour laws. As businesses expand globally they seek new markets which leads to expanding their corporation’s norms and rules to encompass the new locations norms which most often involve religious rules and terms.

Some areas, countries or cities have an economy based on religious tourism. Examples include Islamic Hajj tourism and Vatican tourism. The hotels and markets of important religious places are a source of income to the locals.

The boards or shines sometimes receive so much in donations that governments to take it under control for proper utilization of resources and management. The annual revenues of most of the religious places are not regulated.






Judaism outlines requirements of accurate weights and measurements in commerce, as well as prohibitions on monetary deception, verbal deception and misrepresentation. Jewish business ethics believe that god is the best source of value, believes in centrality of the community, and promise that men and women can transform themselves. The concept of business is perceived as legitimate by Judaism. There is a huge push for social responsibility in any business venture as well as a charity obligation of both public and private business organizations.

Globally, halal products comprise a US$2 trillion industry.

As of 2003, the kosher industry had certified more than 100,000 products, which total approximately US$165 billion in sales annually.

United Kingdom labour law prohibits employer discrimination based on religion, belief, or any lack thereof.

In the United States, labor laws including Title VII of the Civil Rights Act of 1964 prohibit businesses from discriminating against employees based on the basis of religion. Business law is also at times applied to religious organizations, due to their status as incorporated entities.

Religious Freedoms Act of 1993:

Stops any agency, department, or official of the United States or any state from substantially burdening a person's exercise of religion even if the burden results from a rule of general applicability, except that the government may burden a person's exercise of religion only if it demonstrates that application of the burden to the person. 

Free Exercise Clause:

Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise.

Equal Protection Clause:

Governmental body may not deny people equal protection of its governing clause.

Equal Employment Opportunity Commission (EEOC):

A Federal Agency that pushes equal opportunity in employment through administrative and judicial enforcement of the federal civil rights laws.

1961 Braunfeld v. Brown (4-5):

Abraham Braunfeld owned a retail and clothing furnishing store in Philadelphia. As an Orthodox Jew he observes Sabbath and is not allowed to work. The Pennsylvania Blue Law only allowed certain laws to remain open for business on Sunday. Because Braunfeld needs to be open six days a week for economic reasons but he couldn't be open on Saturday due to his observation of the Sabbath. The U.S Supreme court found that the Pennsylvania Blue Law wasn't unconstitutional and didn't violate the free exercise clause. The law didn't make any religious practices unlawful. It was just a way find a statewide day of rest and it was unfortunate that it fell on Sunday.

1963 Sherbert v. Verner (7-1):

Adeil Sherbert was fired because she refused to work on Saturday, which was the day of her worship as she is a member of the Seventh-day Adventist Church. The Employment Security Commission ruled that people Sherbert was ineligible for unemployment benefits because not working on Saturday was not a good enough reason. The U.S Supreme Court sided with Sherbert, citing the free exercise clause.

1972 Wisconsin v. Yoder (0-9):

Jonas Yoder and Warren Miller members of the old order Amish religion, and Adin Yutzy a member of conservative Amish Mennonite Church. These three parents were prosecuted under Wisconsin law, which states that all children must attend public school till 16. The parents refused to send their children after 8th grade citing religious concerns. The U.S Supreme Court sided with Yoder, Miller, and Yutzy under the free exercise clause.

1977 Trans World Airlines, Inc. v. Hardison (7-2):

Larry Hardison was an employee at Trans World Airline. Hardison was a member of the Worldwide Church of God and refused to work on Saturdays which was his sabbath. TWA transferred his shift from night to during the day on Saturday. But he didn't keep the same seniority once he switched shifts and therefore didn't have Saturdays off. The Supreme court sided with the Trans World Airlines because the Equal Employment Opportunity Commission states there needs to be “reasonable” accommodations for religious exercise.

1990 Employment Division Department of Human Resources of Oregon v. Smith (6-3):

Two employees of a private drug rehabilitation organization ingested peyote as part of their religious ceremony at a Native American Church. The employees were fired and applied for unemployment benefit but had not granted them because they were fired from workplace misconduct. The U.S Supreme Court sided with the Employment Division of Oregon stating that even though the employees took peyote for religious reasons. Peyote is illegal in the United States.

2014 Burwell v. Hobby Lobby Stores (5-4):

Hobby Lobby owners have organized their stores around Christian faith. Under the Affordable Care Act (ACA) it talks about how for-profit businesses must provide contraceptives to all employees. The owners of Hobby Lobby sued the Department of Health and Human Services based on violation of the free exercise clause. The U.S Supreme Court sided with the Department of Health and Human Services citing there was no violation of the freedom of religion because religious beliefs must not infringe on third party people.

2018 Masterpiece Cakeshop Ltd v. Colorado Civil Rights Commission (7-2):

Masterpiece Cakeshop refused to make a cake for a wedding between two gay men, due to the businesses religious standing. The Colorado Civil rights Commission sided with the customers on the bases of discrimination on sexual orientation. The U.S Supreme Court reversed the Colorado Civil rights Commission's decision stating that they violated the business owner of Masterpiece Cakeshops right to their free exercise of religion.





</doc>
<doc id="328240" url="https://en.wikipedia.org/wiki?curid=328240" title="Marketing strategy">
Marketing strategy

Marketing strategy is a long-term, forward-looking approach with the fundamental goal of achieving a sustainable competitive advantage. 

Scholars continue to debate the precise meaning of marketing strategy. Consequently, the literature offers many different definitions. On close examination, however, these definitions appear to centre around the notion that strategy refers to a broad statement of what is to be achieved.

Strategic planning involves an analysis of the company's strategic initial situation prior to the formulation, evaluation and selection of market-oriented competitive position that contributes to the company's goals and marketing objectives.

Strategic marketing, as a distinct field of study emerged in the 1971s, and built on strategic management that preceded it. Marketing strategy highlights the role of marketing as a link between the organization and its customers.

The marketing mix is a crucial tool to help understand what the product or service can offer and how to plan for a successful product offering. The marketing mix is most commonly executed through the 4 P's of marketing: Price, Product, Promotion, and Place. Carefully considering the marketing mix will enable a business to understand how it can differentiate its product or service and thus build a marketing strategy to drive sales.

The distinction between “strategic” and “managerial” marketing is used to distinguish "two phases having different goals and based on different conceptual tools. Strategic marketing concerns the choice of policies aiming at improving the competitive position of the firm, taking account of challenges and opportunities proposed by the competitive environment. On the other hand, managerial marketing is focused on the implementation of specific targets." Marketing strategy is about "lofty visions translated into less lofty and practical goals [while marketing management] is where we start to get our hands dirty and make plans for things to happen." Marketing strategy is sometimes called "higher order" planning because it sets out the broad direction and provides guidance and structure for the marketing program.

Marketing scholars have suggested that strategic marketing arose in the late 1970s and its origins can be understood in terms of a distinct evolutionary path:

Marketing strategy involves mapping out the company's direction for the forthcoming planning period, whether that be three, five or ten years. It involves undertaking a 360° review of the firm and its operating environment with a view to identifying new business opportunities that the firm could potentially leverage for competitive advantage. Strategic planning may also reveal market threats that the firm may need to consider for long-term sustainability. Strategic planning makes no assumptions about the firm continuing to offer the same products to the same customers into the future. Instead, it is concerned with identifying the business opportunities that are likely to be successful and evaluates the firm's capacity to leverage such opportunities. It seeks to identify the "strategic gap"; that is the difference between where a firm is currently situated (the "strategic reality" or "inadvertent strategy") and where it should be situated for sustainable, long-term growth (the "strategic intent" or "deliberate strategy").

Strategic planning seeks to address three deceptively simple questions, specifically:

A fourth question may be added to the list, namely 'How do we know when we got there?' Due to increasing need for accountability, many marketing organisations use a variety of marketing metrics to track strategic performance, allowing for corrective action to be taken as required. On the surface, strategic planning seeks to address three simple questions, however, the research and analysis involved in strategic planning is very sophisticated and requires a great deal of skill and judgement.

Strategic analysis is designed to address the first strategic question, "Where are we now?" Traditional market research is less useful for strategic marketing because the analyst is not seeking insights about customer attitudes and preferences. Instead strategic analysts are seeking insights about the firm's operating environment with a view to identifying possible future scenarios, opportunities and threats.

Strategic planning focuses on the 3C's, namely: Customer, Corporation and Competitors. A detailed analysis of each factor is key to the success of strategy formulation. The 'competitors' element refers to an analysis of the strengths of the business relative to close rivals, and a consideration of competitive threats that might impinge on the business' ability to move in certain directions. The 'customer' element refers to an analysis of any possible changes in customer preferences that potentially give rise to new business opportunities. The 'corporation' element refers to a detailed analysis of the company's internal capabilities and its readiness to leverage market-based opportunities or its vulnerability to external threats.

Mintzberg suggests that the top planners spend most of their time engaged in analysis and are concerned with industry or competitive analyses as well as internal studies, including the use of computer models to analyze trends in the organization. Strategic planners use a variety of research tools and analytical techniques, depending on the environment complexity and the firm's goals. Fleitcher and Bensoussan, for instance, have identified some 200 qualitative and quantitative analytical techniques regularly used by strategic analysts while a recent publication suggests that 72 techniques are essential. No optimal technique can be identified as useful across all situations or problems. Determining which technique to use in any given situation rests with the skill of the analyst. The choice of tool depends on a variety of factors including: data availability; the nature of the marketing problem; the objective or purpose, the analyst's skill level as well as other constraints such as time or motivation.

The most commonly used tools and techniques include:

Research methods

Analytical techniques

Gap analysis is a type of higher order analysis that seeks to identify the difference between the organisation's current strategy and its desired strategy. This difference is sometimes known as the "strategic gap." Mintzberg identifies two types of strategy namely "deliberate strategy" and "inadvertent strategy." The deliberate strategy represents the firm's strategic intent or its desired path while the inadvertent strategy represents the path that the firm may have followed as it adjusted to environmental, competitive and market changes. Other scholars use the terms "realized strategy" versus "intended" strategy to refer to the same concepts. This type of analysis indicates whether an organisation has strayed from its desired path during the planning period. The presence of a large gap may indicate the organisation has become "stuck in the middle"; a recipe for strategic mediocrity and potential failure.

The category/brand development index is a method used to assess the sales potential for a region or market and identify market segments that can be developed (i.e. high CDI and high BDI). In addition, it may be used to identify markets where the category or brand is under-performing and may signal underlying marketing problems such as poor distribution (i.e. high CDI and low BDI).

BDI and CDI are calculated as follows:

Strategic planning typically begins with a scan of the business environment, both internal and external, this includes understanding strategic constraints. An understanding of the external operating environment, including political, economic, social and technological which includes demographic and cultural aspects, is necessary for the identification of business opportunities and threats. This analysis is called PEST; an acronym for Political, Economic, Social and Technological. A number of variants of the PEST analysis can be identified in literature, including: PESTLE analysis (Political, Economic, Social, Technological, Legal and Environmental); STEEPLE (adds ethics); STEEPLED (adds demographics) and STEER (adds regulatory).

The aim of the PEST analysis is to identify opportunities and threats in the wider operating environment. Firms try to leverage opportunities while trying to buffer themselves against potential threats. Basically, the PEST analysis guides strategic decision-making. The main elements of the PEST analysis are:

When carrying out a PEST analysis, planners and analysts may consider the operating environment at three levels, namely "the supranational"; "the national" and "subnational" or local level. As businesses become more globalized, they may need to pay greater attention to the supranational level.

In addition to the PEST analysis, firms carry out a Strengths, Weakness, Opportunities and Threats (SWOT) analysis. A SWOT analysis identifies: 

Typically the firm will attempt to leverage those opportunities that can be matched with internal strengths; that is to say the firm has a capability in any area where strengths are matched with external opportunities. It may need to build capability if it wishes to leverage opportunities in areas of weakness. An area of weakness that is matched with an external threat represents a vulnerability, and the firm may need to develop contingency plans.

The vision and mission address the second central question, 'Where are we going?' At the conclusion of the research and analysis stage, the firm will typically review its vision statement, mission statement and, if necessary, devise a new vision and mission for the outlook period. At this stage, the firm will also devise a generic competitive strategy as the basis for maintaining a sustainable competitive advantage for the forthcoming planning period.

A vision statement is a realistic, long term future scenario for the organisation. (Vision statements should not be confused with slogans or mottos.) A vision statement is designed to present a realistic long-term future scenario for the organisation. It is a "clearly articulated statement of the business scope." A strong vision statement typically includes the following:
Some scholars point out the market visioning is a skill or competency that encapsulates the planners' capacity "to link advanced technologies to market opportunities of the future, and to do so through a shared understanding of a given product market.

A mission statement is a clear and concise statement of the organisation’s reason for being and its scope of operations, while the generic strategy outlines how the company intends to achieve both its vision and mission.

Mission statements should include detailed information and must be more than a simple "motherhood statement". A mission statement typically includes the following:

The generic competitive strategy outlines the fundamental basis for obtaining a sustainable competitive advantage within a category. Firms can normally trace their competitive position to one of three factors:

It is essential that the internal analysis provide a frank and open evaluation of the firm's superiority in terms of skills, resources or market position since this will provide the basis for competing over the forthcoming planning period. For this reason, some companies engage external consultants, often advertising or marketing agencies, to provide an independent assessment of the firms capabilities and resources.

In 1980, Michael Porter developed an approach to strategy formulation that proved to be extremely popular with both scholars and practitioners. The approach became known as the "positioning school" because of its emphasis on locating a defensible "competitive position" within an industry or sector. In this approach, strategy formulation consists of three key strands of thinking: analysis of the five forces to determine the sources of competitive advantage; the selection of one of three possible positions which leverage the advantage and the value chain to implement the strategy. In this approach, the strategic choices involve decisions about whether to compete for a share of the total market or for a specific target group (competitive scope) and whether to compete on costs or product differences (competitive advantage). This type of thinking leads to three generic strategies:

According to Porter, these strategies are mutually exclusive and the firm must select one approach to the exclusion of all others. Firms that try to be all things to all people can present a confused market position which ultimately leads to below average returns. Any ambiguity about the firm's approach is a recipe for "strategic mediocrity" and any firm that tries to pursue two approaches simultaneously is said to be "stuck in the middle" and destined for failure.

Porter's approach was the dominant paradigm throughout the 1980s. However, the approach has attracted considerable criticism. One important criticism is that it is possible to identify successful companies that pursue a hybrid strategy - such as low cost position and a differentiated position simultaneously. Toyota is a classic example of this hybrid approach. Other scholars point to the simplistic nature of the analysis and the overly prescriptive nature of the strategic choices which limits strategies to just three options. Yet others point to research showing that many practitioners find the approach to be overly theoretical and not applicable to their business.

During the 1990s, the "resource-based view" (also known as the "resource-advantage theory") of the firm became the dominant paradigm. It is an inter-disciplinary approach that represents a substantial shift in thinking. It focuses attention on an organisation's internal resources as a means of organising processes and obtaining a competitive advantage. The resource-based view suggests that organisations must develop unique, firm-specific core competencies that will allow them to outperform competitors by doing things differently and in a superior manner.

Barney stated that for resources to hold potential as sources of sustainable competitive advantage, they should be valuable, rare and imperfectly imitable. A key insight arising from the resource-based view is that not all resources are of equal importance nor possess the potential to become a source of sustainable competitive advantage. The sustainability of any competitive advantage depends on the extent to which resources can be imitated or substituted. Barney and others point out that understanding the causal relationship between the sources of advantage and successful strategies can be very difficult in practice. Barney uses the term "causally ambiguous" which he describes as a situation when "the link between the resources controlled by the firm and the firm's sustained competitive advantage is not understood or understood only very imperfectly." Thus, a great deal of managerial effort must be invested in identifying, understanding and classifying core competencies. In addition, management must invest in organisational learning to develop and maintain key resources and competencies.

Market Based Resources include:

In the resource-based view, strategists select the strategy or competitive position that best exploits the internal resources and capabilities relative to external opportunities. Given that strategic resources represent a complex network of inter-related assets and capabilities, organisations can adopt many possible competitive positions. Although scholars debate the precise categories of competitive positions that are used, there is general agreement, within the literature, that the resource-based view is much more flexible than Porter's prescriptive approach to strategy formulation.

Hooley et al., suggest the following classification of competitive positions:

The choice of competitive strategy often depends on a variety of factors including: the firm's market position relative to rival firms, the stage of the product life cycle. A well-established firm in a mature market will likely have a different strategy than a start-up.

Growth of a business is critical for business success. A firm may grow by developing the market or by developing new products. The Ansoff product and market growth matrix illustrates the two broad dimensions for achieving growth. The Ansoff matrix identifies four specific growth strategies: market penetration, product development, market development and diversification. 

A horizontal integration strategy may be indicated in fast changing work environments as well as providing a broad knowledge base for the business and employees. A benefit of horizontal diversification is that it is an open platform for a business to expand and build away from the already existing market.

High levels of horizontal integration lead to high levels of communication within the business. Another benefit of using this strategy is that it leads to a larger market for merged businesses, and it is easier to build good reputations for a business when using this strategy. A disadvantage of using a diversification strategy is that the benefits could take a while to start showing, which could lead the business to believe that the strategy in ineffective. Another disadvantage or risk is, it has been shown that using the horizontal diversification method has become harmful for stock value, but using the vertical diversification had the best effects.

A disadvantage of using the horizontal integration strategy is that this limits and restricts the field of interest that the business. Horizontal integration can affect a business's reputation, especially after a merge has happened between two or more businesses. There are three main benefits to a business's reputation after a merge. A larger business helps the reputation and increases the severity of the punishment. As well as the merge of information after a merge has happened, this increases the knowledge of the business and marketing area they are focused on. The last benefit is more opportunities for deviation to occur in merged businesses rather than independent businesses.


Vertical integration is when business is expanded through the vertical production line on one business. An example of a vertically integrated business could be Apple. Apple owns all their own software, hardware, designs and operating systems instead of relying on other businesses to supply these. By having a highly vertically integrated business this creates different economies therefore creating a positive performance for the business. Vertical integration is seen as a business controlling the inputs of supplies and outputs of products as well as the distribution of the final product. Some benefits of using a Vertical integration strategy is that costs may be reduced because of the reducing transaction costs which include finding, selling, monitoring, contracting and negotiating with other firms. Also by decreasing outside businesses input it will increase the efficient use of inputs into the business. Another benefit of vertical integration is that it improves the exchange of information through the different stages of the production line. Some competitive advantages could include; avoiding foreclosures, improving the business marketing intelligence, and opens up opportunities to create different products for the market. Some disadvantages of using a Vertical Integration Strategy include the internal costs for the business and the need for overhead costs. Also if the business is not well organised and fully equipped and prepared the business will struggle using this strategy. There are also competitive disadvantages as well, which include; creates barriers for the business, and loses access to information from suppliers and distributors.

In terms of market position, firms may be classified as market leaders, market challengers, market followers or market nichers.

As the speed of change in the marketing environment quickens, time horizons are becoming shorter. Nevertheless, most firms carry out strategic planning every 3– 5 years and treat the process as a means of checking whether the company is on track to achieve its vision and mission. Ideally, strategies are both dynamic and interactive, partially planned and partially unplanned. Strategies are broad in their scope in order to enable a firm to react to unforeseen developments while trying to keep focused on a specific pathway. A key aspect of marketing strategy is to keep marketing consistent with a company's overarching mission statement.

Strategies often specify how to adjust the marketing mix; firms can use tools such as Marketing Mix Modeling to help them decide how to allocate scarce resources, as well as how to allocate funds across a portfolio of brands. In addition, firms can conduct analyses of performance, customer analysis, competitor analysis, and target market analysis.

Marketing strategies may differ depending on the unique situation of the individual business. According to Lieberman and Montgomery, every entrant into a market – whether it is new or not – is classified under a Market Pioneer, Close Follower or a Late follower

Market pioneers are known to often open a new market to consumers based off a major innovation. They emphasise these product developments, and in a significant number of cases, studies have shown that early entrants – or pioneers – into a market have serious market-share advantages above all those who enter later. Pioneers have the first-mover advantage, and in order to have this advantage, business’ must ensure they have at least one or more of three primary sources: Technological Leadership, Preemption of Assets or Buyer Switching Costs. Technological Leadership means gaining an advantage through either Research and Development or the “learning curve”. This lets a business use the research and development stage as a key point of selling due to primary research of a new or developed product. Preemption of Assets can help gain an advantage through acquiring scarce assets within a certain market, allowing the first-mover to be able to have control of existing assets rather than those that are created through new technology. Thus allowing pre-existing information to be used and a lower risk when first entering a new market. By being a first entrant, it is easy to avoid higher switching costs compared to later entrants. For example, those who enter later would have to invest more expenditure in order to encourage customers away from early entrants. However, while Market Pioneers may have the “highest probability of engaging in product development” and lower switching costs, to have the first-mover advantage, it can be more expensive due to product innovation being more costly than product imitation. It has been found that while Pioneers in both consumer goods and industrial markets have gained “significant sales advantages”, they incur larger disadvantages cost-wise.

Being a Market Pioneer can, more often than not, attract entrepreneurs and/or investors depending on the benefits of the market. If there is an upside potential and the ability to have a stable market share, many businesses would start to follow in the footsteps of these pioneers. These are more commonly known as Close Followers. These entrants into the market can also be seen as challengers to the Market Pioneers and the Late Followers. This is because early followers are more than likely to invest a significant amount in Product Research and Development than later entrants. By doing this, it allows businesses to find weaknesses in the products produced before, thus leading to improvements and expansion on the aforementioned product. Therefore, it could also lead to customer preference, which is essential in market success. Due to the nature of early followers and the research time being later than Market Pioneers, different development strategies are used as opposed to those who entered the market in the beginning, and the same is applied to those who are Late Followers in the market. By having a different strategy, it allows the followers to create their own unique selling point and perhaps target a different audience in comparison to that of the Market Pioneers. Early following into a market can often be encouraged by an established business’ product that is “threatened or has industry-specific supporting assets”.

Those who follow after the Close Followers are known as the Late Entrants. While being a Late Entrant can seem very daunting, there are some perks to being a latecomer. For example, Late Entrants have the ability to learn from those who are already in the market or have previously entered. Late Followers have the advantage of learning from their early competitors and improving the benefits or reducing the total costs. This allows them to create a strategy that could essentially mean gaining market share and most importantly, staying in the market. In addition to this, markets evolve, leading to consumers wanting improvements and advancements on products. Late Followers have the advantage of catching the shifts in customer needs and wants towards the products. When bearing in mind customer preference, customer value has a significant influence. Customer value means taking into account the investment of customers as well as the brand or product. It is created through the “perceptions of benefits” and the “total cost of ownership”. On the other hand, if the needs and wants of consumers have only slightly altered, Late Followers could have a cost advantage over early entrants due to the use of product imitation. However, if a business is switching markets, this could take the cost advantage away due to the expense of changing markets for the business. Late Entry into a market does not necessarily mean there is a disadvantage when it comes to market share, it depends on how the marketing mix is adopted and the performance of the business. If the marketing mix is not used correctly – despite the entrant time – the business will gain little to no advantages, potentially missing out on a significant opportunity.

The differentiated strategy

The customised target strategy
The requirements of individual customer markets are unique, and their purchases sufficient to make viable the design of a new marketing mix for each customer.

If a company adopts this type of market strategy, a separate marketing mix is to be designed for each customer.

Specific marketing mixes can be developed to appeal to most of the segments when market segmentation reveals several potential targets.

Whereas the vision and mission provide the framework, the "goals define targets within the mission, which, when achieved, should move the organization toward the performance of that mission." "Goals" are broad primary outcomes whereas, "objectives" are measurable steps taken to achieve a goal or strategy. In strategic planning, it is important for managers to translate the overall strategy into goals and objectives. Goals are designed to inspire action and focus attention on specific desired outcomes. Objectives, on the other hand, are used to measure an organisation's performance on specific dimensions, thereby providing the organisation with feedback on how well it is achieving its goals and strategies.

Managers typically establish objectives using the "balanced scorecard" approach. This means that objectives do not include desired financial outcomes exclusively, but also specify measures of performance for customers (e.g. satisfaction, loyalty, repeat patronage), internal processes (e.g., employee satisfaction, productivity) and innovation and improvement activities.

After setting the goals marketing strategy or marketing plan should be developed. The marketing strategy plan provides an outline of the specific actions to be taken over time to achieve the objectives. Plans can be extended to cover many years, with sub-plans for each year. Plans usually involve monitoring, to assess progress, and prepare for contingencies if problems arise. Simultaneous such as customer lifetime value models can be used to help marketers conduct "what-if" analyses to forecast what potential scenarios arising from possible actions, and to gauge how specific actions might affect such variables as the revenue-per-customer and the churn rate.

Developing competitive strategy requires significant judgement and is based on a deep understanding of the firm's current situation, its past history and its operating environment. No heuristics have yet been developed to assist strategists choose the optimal strategic direction. Nevertheless, some researchers and scholars have sought to classify broad groups of strategy approaches that might serve as broad frameworks for thinking about suitable choices.

In 2003, Raymond Miles proposed a detailed scheme using the categories:

Marketing warfare strategies are competitor-centered strategies drawn from analogies with the field of military science. Warfare strategies were popular in the 1980s, but interest in this approach has waned in the new era of relationship marketing. An increased awareness of the distinctions between business and military cultures also raises questions about the extent to which this type of analogy is useful. In spite of its limitations, the typology of marketing warfare strategies is useful for predicting and understanding competitor responses.

In the 1980s, Kotler and Singh developed a typology of marketing warfare strategies:


Marketing strategy and marketing mix are related elements of a comprehensive marketing plan. While marketing strategy is aligned with setting the direction of a company or product/service line, the marketing mix is majorly tactical in nature and is employed to carry out the overall marketing strategy. The 4P's of the marketing mix (Price, Product, Place and Promotion) represent the tools that marketers can leverage while defining their marketing strategy to create a marketing plan.




</doc>
<doc id="39206" url="https://en.wikipedia.org/wiki?curid=39206" title="Business">
Business

Business is the activity of making one's living or making money by producing or buying and selling products (such as goods and services). Simply put, it is "any activity or enterprise entered into for profit. It does not mean it is a company, a corporation, partnership, or have any such formal organization, but it can range from a street peddler to General Motors." 

Having a business name does not separate the business entity from the owner, which means that the owner of the business is responsible and liable for debts incurred by the business. If the business acquires debts, the creditors can go after the owner's personal possessions. A business structure does not allow for corporate tax rates. The proprietor is personally taxed on all income from the business.

The term is also often used colloquially (but not by lawyers or by public officials) to refer to a company. A company, on the other hand, is a separate legal entity and provides for limited liability, as well as corporate tax rates. A company structure is more complicated and expensive to set up, but offers more protection and benefits for the owner.

Forms of business ownership vary by jurisdiction, but several common entities exist:

"Less common types of companies are:"
Note that "Ltd after the company's name signifies limited company, and PLC (public limited company) indicates that its shares are widely held."

In legal parlance, the owners of a company are normally referred to as the "members". In a company limited or unlimited by shares (formed or incorporated with a share capital), this will be the shareholders. In a company limited by guarantee, this will be the guarantors. Some offshore jurisdictions have created special forms of offshore company in a bid to attract business for their jurisdictions. Examples include "segregated portfolio companies" and restricted purpose companies.

There are, however, many, many sub-categories of types of company that can be formed in various jurisdictions in the world.

Companies are also sometimes distinguished into public companies and private companies for legal and regulatory purposes. Public companies are companies whose shares can be publicly traded, often (although not always) on a stock exchange which imposes listing requirements/Listing Rules as to the issued shares, the trading of shares and a future issue of shares to help bolster the reputation of the exchange or particular market of exchange. Private companies do not have publicly traded shares, and often contain restrictions on transfers of shares. In some jurisdictions, private companies have maximum numbers of shareholders.

A parent company is a company that owns enough voting stock in another firm to control management and operations by influencing or electing its board of directors; the second company being deemed as a subsidiary of the parent company. The definition of a parent company differs by jurisdiction, with the definition normally being defined by way of laws dealing with companies in that jurisdiction.


Accounting is the measurement, processing, and communication of financial information about economic entities such as businesses and corporations. The modern field was established by the Italian mathematician Luca Pacioli in 1494. Accounting, which has been called the "language of business", measures the results of an organization's economic activities and conveys this information to a variety of users, including investors, creditors, management, and regulators. Practitioners of accounting are known as accountants. The terms "accounting" and "financial reporting" are often used as synonyms.

Finance is a field that deals with the study of investments. It includes the dynamics of assets and liabilities over time under conditions of different degrees of uncertainty and risk. Finance can also be defined as the science of money management. Finance aims to price assets based on their risk level and their expected rate of return. Finance can be broken into three different sub categories: public finance, corporate finance, and personal finance.

Manufacturing is the production of merchandise for use or sale using labour and machines, tools, chemical and biological processing, or formulation. The term may refer to a range of human activity, from handicraft to high tech, but is most commonly applied to industrial production, in which raw materials are transformed into finished goods on a large scale.

Marketing is defined by the American Marketing Association as "the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offerings that have value for customers, clients, partners, and society at large." The term developed from the original meaning which referred literally to going to a market to buy or sell goods or services. Marketing tactics include advertising as well as determining product pricing.

With the rise in technology, marketing is further divided into a class called digital marketing. It is marketing products and services using digital technologies.

Research and development refer to activities in connection with corporate or government innovation. Research and development constitute the first stage of development of a potential new service or product. Research and development are very difficult to manage since the defining feature of the research is that the researchers do not know in advance exactly how to accomplish the desired result. 

Injuries cost businesses billions of dollars annually. Studies have shown how company acceptance and implementation of comprehensive safety and health management systems reduce incidents, insurance costs, and workers' compensation claims. New technologies, like wearable safety devices and available online safety training, continue to be developed to encourage employers to invest in protection beyond the "canary in the coal mine" and reduce the cost to businesses of protecting their employees.

Sales are activity related to selling or the number of goods or services sold in a given time period. Sales are often integrated with all lines of business and are key to a companies' success.

The efficient and effective operation of a business, and study of this subject, is called management. The major branches of management are financial management, marketing management, human resource management, strategic management, production management, operations management, service management, and information technology management. 

Owners may manage their businesses themselves, or employ managers to do so for them. Whether they are owners or employees, managers administer three primary components of the business' value: financial resources, capital (tangible resources), and human resources. These resources are administered in at least six functional areas: legal contracting, manufacturing or service production, marketing, accounting, financing, and human resources.

In recent decades, states modeled some of their assets and enterprises after business enterprises. In 2003, for example, the People's Republic of China modeled 80% of its state-owned enterprises on a company-type management system. Many state institutions and enterprises in China and Russia have transformed into joint-stock companies, with part of their shares being listed on public stock markets.

Business process management (BPM) is a holistic management approach focused on aligning all aspects of an organization with the wants and needs of clients. BPM attempts to improve processes continuously. It can, therefore, be described as a "process optimization process". It is argued that BPM enables organizations to be more efficient, effective and capable of change than a functionally focused, traditional hierarchical management approach. 

Most legal jurisdictions specify the forms of ownership that a business can take, creating a body of commercial law for each type.

The major factors affecting how a business is organized are usually:


Many businesses are operated through a separate entity such as a corporation or a partnership (either formed with or without limited liability). Most legal jurisdictions allow people to organize such an entity by filing certain charter documents with the relevant Secretary of State or equivalent and complying with certain other ongoing obligations. The relationships and legal rights of shareholders, limited partners, or members are governed partly by the charter documents and partly by the law of the jurisdiction where the entity is organized. Generally speaking, shareholders in a corporation, limited partners in a limited partnership, and members in a limited liability company are shielded from personal liability for the debts and obligations of the entity, which is legally treated as a separate "person". This means that unless there is misconduct, the owner's own possessions are strongly protected in law if the business does not succeed.

Where two or more individuals own a business together but have failed to organize a more specialized form of vehicle, they will be treated as a general partnership. The terms of a partnership are partly governed by a partnership agreement if one is created, and partly by the law of the jurisdiction where the partnership is located. No paperwork or filing is necessary to create a partnership, and without an agreement, the relationships and legal rights of the partners will be entirely governed by the law of the jurisdiction where the partnership is located. A single person who owns and runs a business is commonly known as a "sole proprietor", whether that person owns it directly or through a formally organized entity. Depending on the business needs, an adviser can decide what kind is proprietorship will be most suitable.

A few relevant factors to consider in deciding how to operate a business include:


A very detailed and well-established body of rules that evolved over a very long period of time applies to commercial transactions. The need to regulate trade and commerce and resolve business disputes helped shape the creation of law and courts. The Code of Hammurabi dates back to about 1772 BC for example and contains provisions that relate, among other matters, to shipping costs and dealings between merchants and brokers. The word "corporation" derives from the Latin "corpus", meaning body, and the Maurya Empire in Iron-Age India accorded legal rights to business entities.

In many countries, it is difficult to compile all the laws that can affect a business into a single reference source. Laws can govern the treatment of labour and employee relations, worker protection and safety, discrimination on the basis of age, gender, disability, race, and in some jurisdictions, sexual orientation, and the minimum wage, as well as unions, worker compensation, and working hours and leave.

Some specialized businesses may also require licenses, either due to laws governing entry into certain trades, occupations or professions, that require special education or to raise revenue for local governments. Professions that require special licenses include law, medicine, piloting aircraft, selling liquor, radio broadcasting, selling investment securities, selling used cars, and roofing. Local jurisdictions may also require special licenses and taxes just to operate a business.

Some businesses are subject to ongoing special regulation, for example, public utilities, investment securities, banking, insurance, broadcasting, aviation, and health care providers. Environmental regulations are also very complex and can affect many businesses.

 When businesses need to raise money (called capital), they sometimes offer securities for sale.

Capital may be raised through private means, by an initial public offering or IPO on a stock exchange, or in other ways.

Major stock exchanges include the Shanghai Stock Exchange, Singapore Exchange, Hong Kong Stock Exchange, New York Stock Exchange and NASDAQ (the USA), the London Stock Exchange (UK), the Tokyo Stock Exchange (Japan), and Bombay Stock Exchange (India). Most countries with capital markets have at least one.

Businesses that have gone public are subject to regulations concerning their internal governance, such as how executive officers' compensation is determined, and when and how information is disclosed to shareholders and to the public. In the United States, these regulations are primarily implemented and enforced by the United States Securities and Exchange Commission (SEC). Other western nations have comparable regulatory bodies. The regulations are implemented and enforced by the China Securities Regulation Commission (CSRC) in China. In Singapore, the regulatory authority is the Monetary Authority of Singapore (MAS), and in Hong Kong, it is the Securities and Futures Commission (SFC).

The proliferation and increasing complexity of the laws governing business have forced increasing specialization in corporate law. It is not unheard of for certain kinds of corporate transactions to require a team of five to ten attorneys due to sprawling regulation. Commercial law spans general corporate law, employment and labor law, health-care law, securities law, mergers and acquisitions, tax law, employee benefit plans, food and drug regulation, intellectual property law on copyrights, patents, trademarks, telecommunications law, and financing.

Other types of capital sourcing include crowdsourcing on the Internet, venture capital, bank loans, and debentures.

Businesses often have important "intellectual property" that needs protection from competitors for the company to stay profitable. This could require patents, copyrights, trademarks, or preservation of trade secrets. Most businesses have names, logos, and similar branding techniques that could benefit from trademarking. Patents and copyrights in the United States are largely governed by federal law, while trade secrets and trademarking are mostly a matter of state law. Because of the nature of intellectual property, a business needs protection in every jurisdiction in which they are concerned about competitors. Many countries are signatories to international treaties concerning intellectual property, and thus companies registered in these countries are subject to national laws bound by these treaties. In order to protect trade secrets, companies may require employees to sign noncompete clauses which will impose limitations on an employee's interactions with stakeholders, and competitors.

A trade union (or labor union) is an organization of workers who have come together to achieve common goals such as protecting the integrity of its trade, improving safety standards, achieving higher pay and benefits such as health care and retirement, increasing the number of employees an employer assigns to complete the work, and better working conditions. The trade union, through its leadership, bargains with the employer on behalf of union members (rank and file members) and negotiates labor contracts (collective bargaining) with employers. The most common purpose of these associations or unions is "maintaining or improving the conditions of their employment". This may include the negotiation of wages, work rules, complaint procedures, rules governing hiring, firing, and promotion of workers, benefits, workplace safety and policies.


</doc>
<doc id="59563876" url="https://en.wikipedia.org/wiki?curid=59563876" title="OSTO System Model">
OSTO System Model

The OSTO System Model is based on the OSTO System Theory, which comprehends complex systems and organizations as living systems and maps these by means of the OSTO System Model. The model is cybernetic in nature and is deduced from the theory of closed loops. The basics of this theory have been formulated by David P. Hanna in the 1980’s and have been published initially in 1988. The model assumes that several central transformation processes take place on the inside of a complex organization. These are deeply influenced by mutual reactions between the inner life of the organization and the outside (environment). In terms of closed loop theory, the OSTO System Model depicts the essential elements of such a living system in its interconnectedness, dependencies, and reciprocal reactions. Thinking in network structures is, thus, a crucial part of the OSTO System Theory.

The acronym “OSTO” stands for open, sociotechnical, economic (German: “oekonomisch”) aspects of a system. With regard to organizations and economically working companies, the model takes into consideration the openness of systems towards their environments as well as the fact that they are multidimensional, socio-techno-economic structures. Taking into consideration these four aspects, the model displays the complexity of such a system in its numerous dimensions.

The OSTO System Model is a concrete model of the OSTO thought framework. In practice this model is used as a managerial and reflection tool.

Looking through the so-called “OSTO glasses” is to facilitate managing the steadily increasing dynamism and complexity of systems such as to find new action strategies by creating distance.
Consequently, for organizational development the method is applied in the field of change management. Companies use the methods in the area of 
diagnosis, design and redesign of organizations as well as in project management. 
OSTO has developed concepts for education and human resource development in line with the systemic qualification of managers (“SYMA®”). 
The approach is mainly taught at the University of Klagenfurt and at RWTH Aachen University. 
It is mainly concentrated in the institute of cybernetics as well as at the chair of information management in mechanical engineering and at the center 
for learning and knowledge management. Each year, more than 1000 students acquire knowledge of the OSTO System Theory during a mandatory course 
in their studies of mechanical engineering.

In the OSTO System approach, organizations are analyzed as open systems. In this context, the attribute “open” refers as well to the spatial and subject 
level as to the temporal aspect. On the spatial and subject level, not only intended but also unintended exchange with the environment is analyzed. 
Systems are hardly ever closed. In consequence of that, bidirectional reciprocal exchange between a system and its environment has to be monitored very 
closely. Internal relationships as well as external dependencies of the entire system have to be grasped in order to develop a long-term strategy that 
takes consequences into consideration.

The social side of the system comprises the classical areas of design and process organization, information and decision procedures, 
division of functions and tasks, as well as the reward and control system. This aspect does, however, also consider the motivation throughout the 
company and the relationships among employees and the overall organizational culture. For understanding this part and its influence on the whole 
system it is crucial to know that trust plays the most important part in all procedures and processes in which humans are involved.

The technical side of the system mainly focuses on the material aspects of companies, such as machines, equipment, internal and external architecture 
as well as tools and procedures. Additionally, it comprises the conceptualization of technology with regard to centralized and/or decentralized solutions. 
Another problem that is tackled within this part of the model is the question as to how technical concepts and tool further fragmentation of work or – if 
intended so- in how far they enable integrated wholesome work structures.

The economic side of the system describes all aspects which are directly linked to the economic efficiency of the organization, such as revenue trends, productivity development, controlling procedures, remuneration systems, investment and budget planning, fiscal aspects, lead times, cost structure, etc.

Complexity, with regard to entrepreneurial action was first described by organizational theorists in the years around 1975. Hence, different management 
schools and consultants tried to develop new forms of organizational development since: They intended to understand the internal and external complexities 
of companies by developing thought frameworks and creating models. 
Up to this point, there were models describing organizations as Tayloristic structures with subdivision of work (design and process organization). 
These models are still in use. However, they bear the disadvantage of being incapable of depicting the necessary flexibility. The systemic approach 
represents organizations as living organisms which need to flexibly adapt to new conditions. The new aspect of these models is the fact that they consider 
the internal and external complexity of an organization and the social psychological phenomena in and around an organization enter the scope of analysis.
The three most important models in this development are the viable system model (cf. Stafford Beer, 1959), the new St. Gallen Management Model 
(cf. Rüegg-Stürm, 2002) and the OSTO System Model. These models structure the (multidimensional) complexity of large organizational structures into 
illustrations of one or more dimensions. Another, rather non-famous model, is the “Sensitivity Model” by Frederic Vester.

The OSTO System Model is based on the “Organization Performance Model” which has been developed, tested and published by David P. Hanna in 1988 in his 
time as a consultant for Procter & Gamble. Further important participants in this development include Clark/Krone 1972, Krone 1974 and Krug 1992. 
Later on, the model has been further developed and systemized for science by Heijo Rieckmann (Klagenfurt University) and Klaus Henning (RWTH Aachen University) as well as for systemic consulting of organizations by Renate Henning.

Every organization is separated from its environments (at least) theoretically by differing borders. Possible forms of such borders are: Physical 
(e.g. buildings), temporal (e.g. work shifts), social (e.g. teams), or psychological (stereotypes, prejudices) system borders. In order to describe and define a system as accurately as possible it is necessary to determine the borders of a system very carefully. It is a current perception that system borders are partly permeable.

The environments of a system, i.e. everything outside its borders, have a strong influence on every organization. The model assumes that systems without 
any environment that they interact with cannot exist. A system that is hardly influenced by an environment described as an autarkical system. On the 
contrary, a system that is strongly formed by external influences is named a dependent system. In the context of companies an environment can be as 
diverse as to be the marketplace, customers, political conditions etc.

The reason for existing of a system – also to be grasped as the purpose – is the contractual, reciprocal relationship between the system and its environments. It describes which need of the environments is to be satisfied by the core processes of the organization. The Reason for Existing can never be defined unilaterally, which separates it from unilateral, personal interests. In its form it is not to be seen as static, but is also influenced in various ways from the inside and by external factors of the system such that a regular comparison with reality is important. In conjunction with the mission and the goals the Reason for Existing incorporates the overruling “company strategy”.

Next to the Reason for Existing, for every living system a sound mission that is oriented towards the future bears many advantages. The mission questions the long-term sensibility of the Reason for Existing. The internal motivation and identification on the one hand and the acceptance in society on the other hand are maintained through long-term, future oriented thinking. The mission focuses on sensibility with regard to sustainability based on individual, cultural, ethical and further aspects.

In applied practice, the Ultimate Anchor plays a minor role. It deeply analyses [basis and meta value, views on life and the world, images of people and gods which creates the framework of convictions and beliefs in which “questions for meaning” are shaped.] 

Mission as well as Ultimate Anchor were added to the OSTO System Model by Rieckmann and Henning in the second half of the 1980´s since both aspects become increasingly important under the influence of globalization and crisis in society.,

An appropriate depiction of the initial outputs is necessary for the organizational diagnosis. In that regard it is important that “Output” comprises both numerically graspable as well as qualitative aspects (e.g. work place satisfaction, motivation etc.). It is just as important to capture the factually or seemingly useless initial results and not only the “official” or “desired” results.

The term “Outcome” comprises all financial events of an organization: Income from product sales, R&D, etc. It is intended so that the term is slightly broader such that the organization under scrutiny utilizing the model decides itself whether “Outcome” includes prices, sales volume, return on investment (ROI), or other aspects.

The OSTO System Model points out that inside an organization the information from the environment, the Reason for Existing and the outputs/outcome are turned into real results through transformational processes. The model provides two explanations for that

The process version explains the processing of information from the environment, the reason for existing and the outputs/outcome by means of a transformational process. This process is made up of three central core processes. The term “core process” is to underline that only processes that go to the core of the subject matter, i.e. those which ensure the existence of the company, are relevant. There are three core process that are to be distinguished:

The task core process comprises all activities, communications, actions, etc. which aim at creating the system results (output).

An important basis for all processes of within a system is the energy (work power, performance) which each single person in a system provides and is capable of deploying towards the goals of a system. The systemic approach describes this by the term “Individual Core Process”.

Throughout the social core process, the humans in an organization work towards the goals of a system. In the SCP, the individual core process and the task core process are linked such that synergy effects are yielded from collaboration.

The Structural version explains the system’s internal transformation process through strategies, design elements and (system-) behavior. The transformational process is such structured by the design elements of the company. They are intended to concentrate and to structure all processes and structures within an organization:

In the OSTO map, the goals of a company belong to the internal design of an organization, i.e. the so-called transformational process. With respect to systemic theory, the goals are to be grasped as an internal specification and are derived from the reason for existing. They define the internal needs for actions. For reaching the goals it is necessary to formulate strategies that define how things are to be implemented such that the goals are achieved. The strategies are realized by appropriate adaptions to the design elements.

In every system/organization there is an abundance of very different behavioral patterns (e.g. leadership behavior, work behavior, etc.) which are produced by the design elements. This implies that the system behavior can only be influenced through the design elements. Since systems are dynamic processes, they are not oriented towards subjects but rather towards events. The overall system can have characteristics which are not inert to any of its component parts (loyal/cooperative characteristic) and does in general not behave like the sum of its parts. The OSTO System Model is currently the only model of this kind which closely analyses the behavior of an organization.

The design element human comprises the members of a company/organization and their roles (talents, qualifications etc.), expectations and needs of material character. Furthermore, the network of socio-emotional relationships and interaction circumstances (“climate”) is included in these conditions of collaboration.

Technology as a design element comprises the technical machines, the means of production, property plant and equipment, etc. and the relationships among them, i.e. all material and spatial conditions of a system.

The organizational structure describes the operating procedures and processes of an organization, i.e. the functions, hierarchies, reporting structures as well as the regulation of processes in temporal, spatial and subject aspects.

Functions and tasks as a design element is derived from the open property of companies/organizations. It comprises the description of the tasks which stem from customer needs as well as the division of tasks as specific work orders, expectations towards functions, jobs, etc. Consequently it is possible to develop processes in order to install and foster changes in organizations.

The decision making system describes where, how, to whom, on which level, at which spot and by means of which tools decisions are made. It furthermore describes which mechanisms, processes, rules etc. guide the decision processes.

The information system describes who receives or does not receive which information when, from whom and by which means. It also analyzes why this is so.

Amplifying and reduction systems of material and immaterial as well as formal and informal character are described through the reward and control system of an organization. As such it analyzes mechanisms and procedures which observe and guide human and technical behavior, results and processes. This includes amongst others remuneration structures and the unwritten rules of a company.

By means of the development and renewal system the flexibility as well as the performance and adaption capacities of an organization are maintained and increased. This can also revolve around a group of employees within the company which have the mandate to develop the company internally and externally. Generally, this can be subsumed under the term innovation management.

According to the model, feedback is of crucial importance for the survival of a system, i.e. of an organization that is confronted with turbulent environments. As systems are to be understood as open systems they rely on feedback loops in order to remain existent. The feedback consists of loops that have a guiding, stabilizing and renewing effect on the system. According to literature there are four types of feedback.

Quality feedbacks are reactions to the quality of the output. Usually, all types of quality management use quality feedback as a starting point of their methods.

Renewal feedbacks report reaction of the environment with regard to the reason for existing. This form of feedback is to analyze the demands of the environment (e.g. Development of new markets).

Responsibility feedbacks question the mission of the system. They refer to the long-term chances of survival of the system and its environment (earth-humanity-future-problem). In that regard the focus of this form of feedback is sustainability.

Awareness feedbacks contain information about basic “truths” – about humans, the way humans live together and transcendent values – and about absolute (“true”) values. In this context, the consequences for the system and the members of the system (e.g. through religious orientations) are reflected.

The OSTO System Model is, just like the St. Gallen Management Model and the Viable System Model, to be categorized into economic and sociological system theory. The difference from these rather production oriented models is the fact that the OSTO System Model is process oriented and assumes an open system which is guided by permanent feedback. In contrast to other models it analyzes systems independently from hierarchies and is not based on management ratios. It is also the only model which takes (system-) behavior into consideration. Additionally, conscious and unconscious goals and strategies are rendered visible and intended as well as unintended outputs are unveiled and taken into consideration.

The model assumes that the central transformational processes marked in the model take place on the inside of a complex organization. However, the concepts with regard to system and organization are not definitely pointed out in theory. On the part of system theory there is hardly any link to the modern theory of social systems with their core concepts of the observed observer and the autopoiesis. On the part of organizational theory, Karl E. Weick described the problem that the term organization leaves open in how far a special behavior of an involved person takes place at a certain place or refers to a certain place very early on. In the same manner it is not clear in how far behavior is controlled by an organization or contributes to the embodiment of an organization or if both or none of this is true. Gareth Morgan also alluded to the fact that organizations can be analyzed from different perspectives. Dependent upon the perspective the analysis yields pronouncedly different implications for the design, change, and the guiding- and leadership concepts.



</doc>
<doc id="59769241" url="https://en.wikipedia.org/wiki?curid=59769241" title="Performance effects">
Performance effects

Strategy researchers want to understand differences in firm performance. For example, what can explain performance differences between Toyota’s cars business and Samsung’s mobile phones business? Studies show that just three effects account for most performance differences between such businesses: the industry to which a business belongs (automotive industry vs electronics industry), the corporation it is part of (Toyota vs. Samsung), and the business itself.

Performance usually means financial performance, measured most often as return on assets (ROA) or less often as return on sales (ROS), return on invested capital (ROIC), or market share.

A performance effect is an observed difference in business performance. For example, it compares the performance of Toyota’s cars business and that of Samsung’s mobile phones business. A performance effect is "not" a causal effect. For example, it does not indicate what the performance of the mobile phone business would have been if Toyota instead of Samsung was the owner.

Performance effects occur at multiple level of analysis. 

Industry, corporate, business, and year effects are among the most investigated levels of analyses. An industry is a group of businesses that sell similar goods or services. For example, Toyota’s cars business belongs to the automotive industry and Samsung’s mobile phones business to the electronics industry. A corporation is the legal owner of the business. For example, Berkshire Hathaway owns many businesses including of clothing, building products, and insurance. Thus, a corporation can own more than one business. A business is then defined by what it does (i.e. industry) and by whom it is owned (i.e. corporation). Year refers to the year of performance.

An industry effect is the performance difference of businesses in an industry and those in other industries. A corporate effect is the performance difference of businesses of a corporation and those of other corporations. A business effect is the performance difference of a business and those of other businesses. A year effect is the performance difference of businesses in one year and those in another year.

Formally, we can write the performance (p) of a business in industry "i", corporation "c", and year "t" as:

Here "m" is the mean performance of all businesses across all years. I is the industry effect for industry "i" (the performance difference between industry "i" and the mean); C is the corporate effect for corporation "c" (the performance difference between corporation "c" and the mean); B is the business effect for a business in industry "i" and corporation "c" (the performance difference between that business and the mean); Y is the year effect for year "t" (the performance difference between year "t" and the mean); and e is an error term (the performance difference between a business and the mean that is not accounted for by industry, corporate, business, and year effects).

A meta-analysis finds that the strongest effects are business, then corporate, then industry, and then year. Figures 1 and 2 show the strength of each effect with effect sizes in variance and in standard deviation, respectively.

Other performance effects include CEO and geographical region or country.

An effect size is a measure of the magnitude of performance differences.

A common measure is the variance. A finding of 36% for business effects means that the variance in business effects is 36% of the total variance in performance. Conversely, the variance in performance is for about one third related to differences between business with the other two thirds related to other effects (e.g. different industries, different corporations, different year, and random differences). An upside of the variance measure is that the effects sum to 100%. A downside is that the variance uses squared distances so that large effects are amplified and small effects are shrunk.

Another measure is the standard deviation, which is the square root of the variance. An upside of this measure is that the standard deviation relates to linear distances so effects are not similarly amplified or shrunk. For example, business effects are greater than year effects by about factor 45 when using variance and by about only factor 8 when using standard deviation. Relatedly, the standard deviation measure has the same unit of measurement as performance. For example, if performance is in dollars, then the standard deviation is also in dollars (the variance would be in dollars squared). A downside is that the effects measured in standard deviations do not sum to 100%.

An alternative measure is the sum of squares measure. It seeks to attribute squared performance difference to the different effects. Because the sum of squares measure does not account for degrees of freedom, it is sensitive to sample dimensions. For example, sampling more businesses in the same number of industries will change the ratio of sum of squares due to industry and sum of squares due to business.

Different methods are used to estimate effect sizes, including hierarchical linear model, or analysis of variance (ANOVA), or variance components analysis (VCA).

• Strategy
• Strategic management

(2018 Award Recipient of The Dan and Mary Lou Schendel Best Paper Prize)


</doc>
<doc id="33616691" url="https://en.wikipedia.org/wiki?curid=33616691" title="Fayolism">
Fayolism

Fayolism was a theory of management that analyzed and synthesized the role of management in organizations, developed around 1900 by the French management theorist Henri Fayol (1841–1925). It was through Fayol's work as a philosopher of administration that he contributed most widely to the theory and practice of organizational management.
Fayol believed by focusing on managerial practices he could minimize misunderstandings and increase efficiency in organizations. He enlightened managers on how to accomplish their managerial duties, and the practices in which they should engage. In his book "General and Industrial Management" (published in French in 1916, then published in English in 1949), Fayol outlined his theory of general management, which he believed could be applied to the administration of myriad industries. His concern was with the administrative apparatus (or functions of administration), and to that end he presented his "administrative theory", that is, principles and elements of management.

His theories and ideas were ideally a result of his environment—a post revolutionized France with an emerging republic bourgeois. A bourgeois himself, he believed in controlling workers to achieve greater productivity over all other managerial considerations. However, through reading "General and Industrial Management," it is apparent that Fayol advocated a flexible approach to management, one he believed could be applied to any circumstance whether in the home, the workplace, or within the state. He stressed the importance and the practice of forecasting and planning in order to apply these ideas and techniques, which demonstrated his ability and emphasis in being able to adapt to any sort of situation. In "General and Industrial Management" he outlines an agenda whereby, under an accepted theory of management, every citizen is exposed and taught some form of management education and allowed to exercise management abilities first at school and later on in the workplace.

Fayol has been regarded by many as the father of the modern operational management theory, and his ideas have become a fundamental part of modern management concepts. Fayol is often compared to Frederick Winslow Taylor who developed Scientific Management. Taylor's Scientific Management deals with the efficient organization of production in the context of a competitive enterprise that is concerned with controlling its production costs. Taylor's system of scientific management is the cornerstone of classical theory. Fayol was also a classical theorist, and referred to Taylor in his writing and considered him a visionary and pioneer in the management of organizations.

However, Fayol differed from Taylor in his focus. Taylor's main focus was on the task, whereas Fayol was more concerned with management. Another difference between the two theorists is their treatment of workers. Fayol appears to have slightly more respect for the worker than Taylor had, as evidenced by Fayol's proclamation that workers may indeed be motivated by more than just money. Fayol also argued for equity in the treatment of workers.

According to Claude George (1968), a primary difference between Fayol and Taylor was that Taylor viewed management processes from the bottom up, while Fayol viewed it from the top down. In Fayol's book "General and Industrial Management," Fayol wrote that He suggests that Taylor has staff analysts and advisors working with individuals at lower levels of the organization to identify the ways to improve efficiency. According to Fayol, the approach results in a "negation of the principle of unity of command". Fayol criticized Taylor’s functional management in this way. Those eight, Fayol said, were 
This, he said, was an unworkable situation, and that Taylor must have somehow reconciled the dichotomy in some way not described in Taylor's works.

Fayol's desire for teaching a generalized theory of management stemmed from the belief that each individual of an organization at one point or another takes on duties that involve managerial decisions. Unlike Taylor, however, who believed management activity was the exclusive duty of an organizations dominant class. Fayol's approach was more in sync with his idea of Authority, which stated, "...that the right to give orders should not be considered without the acceptance and understanding of responsibility."

Noted as one of the early fathers of the "Human Relations" movements, Fayol expressed ideas and practices different from Taylor, in that they showed flexibility and adaptation, and stressed the importance of interpersonal interaction among employees.

During the early 20th century, Fayol developed 14 principles of management to help managers manage their affairs more effectively. Organizations in technologically advanced countries interpret these principles quite differently from the way they were interpreted during Fayol's time as well. These differences in interpretation are in part a result of the cultural challenges managers face when implementing this framework. The fourteen principles are: 
Within his theory, Fayol outlined five elements of management that depict the kinds of behaviors managers should engage in so that the goals and objectives of an organization are effectively met. The five elements of management are:


Fayol believed that animosity and unease within the workplace occurred among employees in different departments. Many of these "misunderstandings" were thought to be caused by improper communication, mainly through letters (or in present-day emails). Among scholars of organizational communication and psychology, letters were perceived to induce or solidify a hierarchical structure within the organization. Through this type of vertical communication, many individuals gained a false feeling of importance. Furthermore, it gave way to selfish thinking and eventual conflict among employees in the workplace.

This concept was expressed in Fayol's book, "General and Industrial Management", by stating," in some firms... employees in neighboring departments with numerous points of contact, or even employees within a department, who could quite easily meet, communicate with each other in writing... there is to be observed a certain amount of animosity prevailing between different departments or different employees within a department. The system of written communication usually brings this result. There is a way of putting an end to this deplorable system ... and that is to forbid all communication in writing which could easily and advantageously be replaced by verbal ones."

Fayol believed that managerial practices were key to predictability and efficiency in organizations. The administrative theory views communication as a necessary ingredient to successful management and many of Fayol's practices are still alive in today's workplace. The elements and principles of management can be found in modern organizations in several ways: as accepted practices in some industries, as revamped versions of the original principles or elements, or as remnants of the organization's history to which alternative practices and philosophies are being offered. The U.S. military is a prime example of an organization that has continued to use these principles.



</doc>
<doc id="1725756" url="https://en.wikipedia.org/wiki?curid=1725756" title="Business directory">
Business directory

A business directory is a website or printed listing of information which lists businesses within niche based categories. Businesses can be categorized by niche, location, activity, or size. Business may be compiled either manually or through an automated online search software. Online yellow pages are a type of business directory, as is the traditional phone book.
The details provided in a business directory may vary. They may include the business name, addresses, telephone numbers, location, contact information, type of service or products the business provides, the number of employees, the served region and any professional associations. 

Some directories include a section for user reviews, comments, and feedback. Business directories in the past would take a printed format but have recently been upgraded to websites due to the advent of the internet.

Many business directories offer complimentary listings in addition to the premium options. There are many business directories and some of these have moved over to the internet and away from printed format. Whilst not being search engines, business directories often have a search function, enabling users to search businesses by Zip Code, country, state, area or city.

Business directories can be in either hard copy or in digital format. Ease of use and distribution means that many trade directories have a digital version.



</doc>
<doc id="31092" url="https://en.wikipedia.org/wiki?curid=31092" title="Time management">
Time management

Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests and commitments with the finiteness of time. Using time effectively gives the person "choice" on spending/managing activities at their own time and expediency. Time management may be aided by a range of skills, tools, and techniques used to manage time when accomplishing specific tasks, projects, and goals complying with a due date. Initially, time management referred to just business or work activities, but eventually the term broadened to include personal activities as well. A time management system is a designed combination of processes, tools, techniques, and methods. Time management is usually a necessity in any project development as it determines the project completion time and scope. It is also important to understand that both technical and structural differences in time management exist due to variations in cultural concepts of time.

The major themes arising from the literature on time management include the following:

Time management is related to different concepts such as:

Organizational time management is the science of identifying, valuing and reducing time cost wastage within organizations. It identifies, reports and financially values sustainable time, wasted time and effective time within an organization and develops the business case to convert wasted time into productive time through the funding of products, services, projects or initiatives at a positive return on investment.

Differences in the way a culture views time can affect the way their time is managed. For example, a "linear time" view is a way of conceiving time as flowing from one moment to the next in a linear fashion. This linear perception of time is predominant in America along with most Northern European countries such as, Germany, Switzerland, and England. People in these cultures tend to place a large value on productive time management, and tend to avoid decisions or actions that would result in wasted time. This linear view of time correlates to these cultures being more “monochronic”, or preferring to do only one thing at a time. Generally speaking, this cultural view leads to a better focus on accomplishing a singular task and hence, more productive time management.

Another cultural time view is "multi-active time" view. In multi-active cultures, most people feel that the more activities or tasks being done at once the happier they are. Multi-active cultures are “polychronic” or prefer to do multiple tasks at once. This multi-active time view is prominent in most Southern European countries such as Spain, Portugal, and Italy. In these cultures, the people often tend to spend time on things they deem to be more important such as placing a high importance on finishing social conversations. In business environments, they often pay little attention to how long meetings last, rather, the focus is on having high quality meetings. In general, the cultural focus tends to be on synergy and creativity over efficiency.

A final cultural time view is a "cyclical time" view. In cyclical cultures, time is considered neither linear nor event related. Because days, months, years, seasons, and events happen in regular repetitive occurrences, time is viewed as cyclical. In this view, time is not seen as wasted because it will always come back later, hence, there is an unlimited amount of it. This cyclical time view is prevalent throughout most countries in Asia including Japan, China, and Tibet. It is more important in cultures with cyclical concepts of time to complete tasks correctly, therefore, most people will spend more time thinking about decisions and the impact they will have before acting on their plans. Most people in cyclical cultures tend to understand that other cultures have different perspectives of time and are cognizant of this when acting on a global stage.

Some time-management literature stresses tasks related to the creation of an environment conducive to "real" effectiveness. These strategies include principles such as:

In addition, the timing of tackling tasks is important as tasks requiring high levels of concentration and mental energy are often done in the beginning of the day when a person is more refreshed. Literature also focuses on overcoming chronic psychological issues such as procrastination.

Excessive and chronic inability to manage time effectively may result from Attention deficit hyperactivity disorder (ADHD) or attention deficit disorder (ADD). Diagnostic criteria include a sense of underachievement, difficulty getting organized, trouble getting started, trouble managing many simultaneous projects, and trouble with follow-through. Some authors focus on the prefrontal cortex which is the most recently evolved part of the brain. It controls the functions of attention span, impulse control, organization, learning from experience and self-monitoring, among others. Some authors argue that changing the way the prefrontal cortex works is possible and offer a solution.

Time management strategies are often associated with the recommendation to set personal goals. The literature stresses themes such as:

These goals are recorded and may be broken down into a project, an action plan, or a simple task list. For individual tasks or for goals, an importance rating may be established, deadlines may be set, and priorities assigned. This process results in a plan with a task list or a schedule or calendar of activities. Authors may recommend a daily, weekly, monthly or other planning periods associated with different scope of planning or review. This is done in various ways, as follows.

A technique that has been used in business management for a long time is the categorization of large data into groups. These groups are often marked A, B, and C—hence the name. Activities are ranked by these general criteria:
Each group is then rank-ordered by priority. To further refine the prioritization, some individuals choose to then force-rank all "B" items as either "A" or "C". ABC analysis can incorporate more than three groups.

ABC analysis is frequently combined with Pareto analysis.

"See also:" Pareto analysis

This is the idea that 80% of tasks can be completed in 20% of the disposable time, and the remaining 20% of tasks will take up 80% of the time. This principle is used to sort tasks into two parts. According to this form of Pareto analysis it is recommended that tasks that fall into the first category be assigned a higher priority.

The 80-20-rule can also be applied to increase productivity: it is assumed that 80% of the productivity can be achieved by doing 20% of the tasks. Similarly, 80% of results can be attributed to 20% of activity. If productivity is the aim of time management, then these tasks should be prioritized higher.

The "Eisenhower Method" stems from a quote attributed to Dwight D. Eisenhower: "I have two kinds of problems, the urgent and the important. The urgent are not important, and the important are never urgent." Note that Eisenhower does not claim this insight for his own, but attributes it to an (unnamed) "former college president."

Using the Eisenhower Decision Principle, tasks are evaluated using the criteria important/unimportant and urgent/not urgent, and then placed in according quadrants in an Eisenhower Matrix (also known as an "Eisenhower Box" or "Eisenhower Decision Matrix"). Tasks are then handled as follows:

Tasks in
This method is inspired by the above quote from U.S. President Dwight D. Eisenhower. Note, however, that Eisenhower seems to say that things are never both important and urgent, or neither: So he has two kinds of problems, the urgent and the important.

POSEC is an acronym for "Prioritize by Organizing, Streamlining, Economizing and Contributing". The method dictates a template which emphasizes an average individual's immediate sense of emotional and monetary security. It suggests that by attending to one's personal responsibilities first, an individual is better positioned to shoulder collective responsibilities.

Inherent in the acronym is a hierarchy of self-realization, which mirrors Abraham Maslow's hierarchy of needs:

Time management also covers how to eliminate tasks that do not provide value to the individual or organization.

According to Sandberg, task lists "aren't the key to productivity [that] they're cracked up to be". He reports an estimated "30% of listers spend more time managing their lists than [they do] completing what's on them".

Hendrickson asserts that rigid adherence to task lists can create a "tyranny of the to-do list" that forces one to "waste time on unimportant activities".

Any form of stress is considered to be debilitative for learning and life, even if adaptability could be acquired its effects are damaging. But stress is an unavoidable part of daily life and Reinhold Niebuhr suggests to face it, as if having "the serenity to accept the things one cannot change and having the courage to change the things one can."

Part of setting priorities and goals is the emotion "worry," and its function is to ignore the present to fixate on a future that never arrives, which leads to the fruitless expense of one's time and energy. It is an unnecessary cost or a false aspect that can interfere with plans due to human factors. The Eisenhower Method is a strategy used to compete worry and dull-imperative tasks. Worry as stress, is a reaction to a set of environmental factors; understanding this is not a part of the person gives the person possibilities to manage them. Athletes under a coach call this management as "putting the game face."

Change is hard and daily life patterns are the most deeply ingrained habits of all. To eliminate non-priorities in study time it is suggested to divide the tasks, capture the moments, review task handling method, postpone unimportant tasks (understood by its current relevancy and sense of urgency reflects wants of the person rather than importance), control life balance (rest, sleep, leisure), and cheat leisure and non productive time (hearing audio taping of lectures, going through presentations of lectures when in queue, etc.).

Certain unnecessary factors that affect time management are habits, lack of task definition (lack of clarity), over-protectiveness of the work, guilt of not meeting objectives and subsequent avoidance of present tasks, defining tasks with higher expectations than their worth (over-qualifying), focusing on matters that have an apparent positive outlook without assessing their importance to personal needs, tasks that require support and time, sectional interests and conflicts, etc. A habituated systematic process becomes a device that the person can use with ownership for effective time management.

A task list (also called a to-do list or "things-to-do") is a list of tasks to be completed, such as chores or steps toward completing a project. It is an inventory tool which serves as an alternative or supplement to memory.

Task lists are used in self-management, business management, project management, and software development. It may involve more than one list.

When one of the items on a task list is accomplished, the task is checked or crossed off. The traditional method is to write these on a piece of paper with a pen or pencil, usually on a note pad or clip-board. Task lists can also have the form of paper or software checklists.

Writer Julie Morgenstern suggests "do's and don'ts" of time management that include:

Numerous digital equivalents are now available, including personal information management (PIM) applications and most PDAs. There are also several web-based task list applications, many of which are free.

Task lists are often diarised and tiered. The simplest tiered system includes a general to-do list (or task-holding file) to record all the tasks the person needs to accomplish, and a daily to-do list which is created each day by transferring tasks from the general to-do list. An alternative is to create a "not-to-do list", to avoid unnecessary tasks.

Task lists are often prioritized:

Various writers have stressed potential difficulties with to-do lists such as the following:

Many companies use time tracking software to track an employee's working time, billable hours etc., e.g. law practice management software.

Many software products for time management support multiple users. They allow the person to give tasks to other users and use the software for communication.

Task list applications may be thought of as lightweight personal information manager or project management software.

Modern task list applications may have built-in task hierarchy (tasks are composed of subtasks which again may contain subtasks), may support multiple methods of filtering and ordering the list of tasks, and may allow one to associate arbitrarily long notes for each task.

In contrast to the concept of allowing the person to use multiple filtering methods, at least one software product additionally contains a mode where the software will attempt to dynamically determine the best tasks for any given moment.

Time management systems often include a time clock or web-based application used to track an employee's work hours. Time management systems give employers insights into their workforce, allowing them to see, plan and manage employees' time. Doing so allows employers to control labor costs and increase productivity. A time management system automates processes, which eliminates paper work and tedious tasks.

Getting Things Done was created by David Allen. The basic idea behind this method is to finish all the small tasks immediately and a big task is to be divided into smaller tasks to start completing now. The reasoning behind this is to avoid the information overload or "brain freeze" which is likely to occur when there are hundreds of tasks. The thrust of GTD is to encourage the user to get their tasks and ideas out and on paper and organized as quickly as possible so they're easy to manage and see.

Francesco Cirillo's "Pomodoro Technique" was originally conceived in the late 1980s and gradually refined until it was later defined in 1992. The technique is the namesake of a pomodoro (Italian for tomato) shaped kitchen timer initially used by Cirillo during his time at university. The "Pomodoro" is described as the fundamental metric of time within the technique and is traditionally defined as being 30 minutes long, consisting of 25 minutes of work and 5 minutes of break time. Cirillo also recommends a longer break of 15 to 30 minutes after every four Pomodoros. Through experimentation involving various work groups and mentoring activities, Cirillo determined the "ideal Pomodoro" to be 20–35 minutes long.


Book:

Systems:

Psychology/neuroscience

Psychiatry



</doc>
<doc id="62647441" url="https://en.wikipedia.org/wiki?curid=62647441" title="Dialdirect">
Dialdirect

Dialdirect is a South African insurance company. It is one of the short-term insurance companies that form Telesure Investment Holdings (TIH). Dialdirect was established in 2003 in South Africa originated from Dial Direct in the UK.  



</doc>
<doc id="9987" url="https://en.wikipedia.org/wiki?curid=9987" title="Outline of engineering">
Outline of engineering

The following outline is provided as an overview of and topical guide to engineering:

Engineering is the scientific discipline and profession that applies scientific theories, mathematical methods, and empirical evidence to design, create, and analyze technological solutions cognizant of safety, human factors, physical laws, regulations, practicality, and cost.


History of engineering









</doc>
<doc id="20166782" url="https://en.wikipedia.org/wiki?curid=20166782" title="PDF/E">
PDF/E

ISO 24517-1:2008 is an ISO Standard published in 2008.


This standard defines a format (PDF/E) for the creation of documents used in geospatial, construction and manufacturing workflows<ref name="PDF/E-ISO-Standard"> ISO 24517-1:2008 - Document management -- Engineering document format using PDF -- Part 1: Use of PDF 1.6 (PDF/E-1)</ref> and is based on the PDF Reference version 1.6 from Adobe Systems. The specification also supports interactive media, including animation and 3D.

PDF/E is a subset of PDF, designed to be an open and neutral exchange format for engineering and technical documentation.<ref name="pdf/e-ready-guide">Creating PDF/E-ready files</ref>

The PDF/E Standard specifies how the Portable Document Format (PDF) should be used for the creation of documents in engineering workflows.

Key benefits of PDF/E include:

The Standard does not define a method for the creation or conversion from paper or electronic documents to the PDF/E format.

The Committee managing ISO 24517 (PDF/E) needs subject-matter experts to assist in the development of Part 2 of the Standard.

ISO 24517 (PDF/E) was created to meet the needs of organizations who need to reliably create, exchange and review engineering documentation, however, the first part of the standard does not address 3D, video or other dynamic content, nor does it address integrated source data.




</doc>
<doc id="36535684" url="https://en.wikipedia.org/wiki?curid=36535684" title="Glossary of engineering">
Glossary of engineering

"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones."

This glossary of engineering terms is a list of definitions about the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering.

 

 

where formula_1 is the infinitesimal amount of heat absorbed by the system from the reservoir and formula_2 is the temperature of the external reservoir (surroundings) at a particular instant in time. In the special case of a reversible process, the equality holds. The reversible case is used to introduce the entropy state function. This is because in a cyclic process the variation of a state function is zero. In words, the Clausius statement states that it is impossible to construct a device whose sole effect is the transfer of heat from a cool reservoir to a hot reservoir. Equivalently, heat spontaneously flows from a hot body to a cooler one, not the other way around. The generalized "inequality of Clausius"

 

 

 

 

 

 
 

 

 

 

 

 

 

 

 

 

 

 



</doc>
<doc id="9251" url="https://en.wikipedia.org/wiki?curid=9251" title="Engineering">
Engineering

Engineering is the use of scientific principles to design and build machines, structures, and other items, including bridges, tunnels, roads, vehicles, and buildings. The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied mathematics, applied science, and types of application. See glossary of engineering.

The term "engineering" is derived from the Latin "ingenium", meaning "cleverness" and "ingeniare", meaning "to contrive, devise".

The American Engineers' Council for Professional Development (ECPD, the predecessor of ABET) has defined "engineering" as:
The creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation and safety to life and property.

Engineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley, etc.

The term "engineering" is derived from the word "engineer", which itself dates back to the 14th century when an "engine'er" (literally, one who builds or operates a "siege engine") referred to "a constructor of military engines." In this context, now obsolete, an "engine" referred to a military machine, "i.e.", a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, "e.g.", the U.S. Army Corps of Engineers.

The word "engine" itself is of even older origin, ultimately deriving from the Latin "ingenium" (c. 1250), meaning "innate quality, especially mental power, hence a clever invention."

Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering.

The pyramids in ancient Egypt, ziggurats of Mesopotamia, the Acropolis and Parthenon in Greece, the Roman aqueducts, Via Appia and Colosseum, Teotihuacán, and the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon and the Pharos of Alexandria, were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World.

The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times. The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC. The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale, and to move large objects in ancient Egyptian technology. The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia circa 3000 BC, and then in ancient Egyptian technology circa 2000 BC. The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC, and ancient Egypt during the Twelfth Dynasty (1991-1802 BC). The screw, the last of the simple machines to be invented, first appeared in Mesopotamia during the Neo-Assyrian period (911-609) BC. The Egyptian pyramids were built using three of the six simple machines, the inclined plane, the wedge, and the lever, to create structures like the Great Pyramid of Giza.

The earliest civil engineer known by name is Imhotep. As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630–2611 BC. The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC.

Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, an early known mechanical analog computer, and the mechanical inventions of Archimedes, are examples of Greek mechanical engineering. Some of Archimedes' inventions as well as the Antikythera mechanism required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are still widely used today in diverse fields such as robotics and automotive engineering.

Ancient Chinese, Greek, Roman and Hunnic armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century BC, the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed.

The earliest practical wind-powered machines, the windmill and wind pump, first appeared in the Muslim world during the Islamic Golden Age, in what are now Iran, Afghanistan, and Pakistan, by the 9th century AD. The earliest practical steam-powered machine was a steam jack driven by a steam turbine, described in 1551 by Taqi al-Din Muhammad ibn Ma'ruf in Ottoman Egypt.

The cotton gin was invented in India by the 6th century AD, and the spinning wheel was invented in the Islamic world by the early 11th century, both of which were fundamental to the growth of the cotton industry. The spinning wheel was also a precursor to the spinning jenny, which was a key development during the early Industrial Revolution in the 18th century. The crankshaft and camshaft were invented by Al-Jazari in Northern Mesopotamia circa 1206, and they later became central to modern machinery such as the steam engine, internal combustion engine and automatic controls.

The earliest programmable machines were developed in the Muslim world. A music sequencer, a programmable musical instrument, was the earliest type of programmable machine. The first music sequencer was an automated flute player invented by the Banu Musa brothers, described in their "Book of Ingenious Devices", in the 9th century. In 1206, Al-Jazari invented programmable automata/robots. He described four automaton musicians, including drummers operated by a programmable drum machine, where they could be made to play different rhythms and different drum patterns. The castle clock, a hydropowered mechanical astronomical clock invented by Al-Jazari, was the first programmable analog computer.

Before the development of modern engineering, mathematics was used by artisans and craftsmen, such as millwrights, clockmakers, instrument makers and surveyors. Aside from these professions, universities were not believed to have had much practical significance to technology.

A standard reference for the state of mechanical arts during the Renaissance is given in the mining engineering treatise "De re metallica" (1556), which also contains sections on geology, mining, and chemistry. "De re metallica" was the standard chemistry reference for the next 180 years.

The science of classical mechanics, sometimes called Newtonian mechanics, formed the scientific basis of much of modern engineering. With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering.

Canal building was an important engineering work during the early phases of the Industrial Revolution.

John Smeaton was the first self-proclaimed civil engineer and is often regarded as the "father" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbors, and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Using a model water wheel, Smeaton conducted experiments for seven years, determining ways to increase efficiency. Smeaton introduced iron axles and gears to water wheels. Smeaton also made mechanical improvements to the Newcomen steam engine. Smeaton designed the third Eddystone Lighthouse (1755–59) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain "hydraulicity" in lime; work which led ultimately to the invention of Portland cement.

Applied science lead to the development of the steam engine. The sequence of events began with the invention the barometer and the measurement of atmospheric pressure by Evangelista Torricelli in 1643, demonstration of the force of atmospheric pressure by Otto von Guericke using the Magdeburg hemispheres in 1656, laboratory experiments by Denis Papin, who built experimental model steam engines and demonstrated the use of a piston, which he published in 1707. Edward Somerset, 2nd Marquess of Worcester published a book of 100 inventions containing a method for raising waters similar to a coffee percolator. Samuel Morland, a mathematician and inventor who worked on pumps, left notes at the Vauxhall Ordinance Office on a steam pump design that Thomas Savery read. In 1698 Savery built a steam pump called "The Miner's Friend." It employed both vacuum and pressure. Iron merchant Thomas Newcomen, who built the first commercial piston steam engine in 1712, was not known to have any scientific training.
The application of steam-powered cast iron blowing cylinders for providing pressurized air for blast furnaces lead to a large increase in iron production in the late 18th century. The higher furnace temperatures made possible with steam-powered blast allowed for the use of more lime in blast furnaces, which enabled the transition from charcoal to coke. These innovations lowered the cost of iron, making horse railways and iron bridges practical. The puddling process, patented by Henry Cort in 1784 produced large scale quantities of wrought iron. Hot blast, patented by James Beaumont Neilson in 1828, greatly lowered the amount of fuel needed to smelt iron. With the development of the high pressure steam engine, the power to weight ratio of steam engines made practical steamboats and locomotives possible. New steel making processes, such as the Bessemer process and the open hearth furnace, ushered in an area of heavy engineering in the late 19th century.

One of the most famous engineers of the mid 19th century was Isambard Kingdom Brunel, who built railroads, dockyards and steamships.
The Industrial Revolution created a demand for machinery with metal parts, which led to the development of several machine tools. Boring cast iron cylinders with precision was not possible until John Wilkinson invented his boring machine, which is considered the first machine tool. Other machine tools included the screw cutting lathe, milling machine, turret lathe and the metal planer. Precision machining techniques were developed in the first half of the 19th century. These included the use of gigs to guide the machining tool over the work and fixtures to hold the work in the proper position. Machine tools and machining techniques capable of producing interchangeable parts lead to large scale factory production by the late 19th century.

The United States census of 1850 listed the occupation of "engineer" for the first time with a count of 2,000. There were fewer than 50 engineering graduates in the U.S. before 1865. In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890, there were 6,000 engineers in civil, mining, mechanical and electrical.

There was no chair of applied mechanism and applied mechanics at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier.

The foundations of electrical engineering in the 1800s included the experiments of Alessandro Volta, Michael Faraday, Georg Ohm and others and the invention of the electric telegraph in 1816 and the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.
Chemical engineering developed in the late nineteenth century. Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants. The role of the chemical engineer was the design of these chemical plants and processes.

Aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.

The first PhD in engineering (technically, "applied science and engineering") awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.

Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.

Engineering is a broad discipline that is often broken down into several sub-disciplines. Although an engineer will usually be trained in a specific discipline, he or she may become multi-disciplined through experience. Engineering is often characterized as having four main branches: chemical engineering, civil engineering, electrical engineering, and mechanical engineering.

Chemical engineering is the application of physics, chemistry, biology, and engineering principles in order to carry out chemical processes on a commercial scale, such as the manufacture of commodity chemicals, specialty chemicals, petroleum refining, microfabrication, fermentation, and biomolecule production.

Civil engineering is the design and construction of public and private works, such as infrastructure (airports, roads, railways, water supply, and treatment etc.), bridges, tunnels, dams, and buildings. Civil engineering is traditionally broken into a number of sub-disciplines, including structural engineering, environmental engineering, and surveying. It is traditionally considered to be separate from military engineering.

Electrical engineering is the design, study, and manufacture of various electrical and electronic systems, such as Broadcast engineering, electrical circuits, generators, motors, electromagnetic/electromechanical devices, electronic devices, electronic circuits, optical fibers, optoelectronic devices, computer systems, telecommunications, instrumentation, controls, and electronics.

Mechanical engineering is the design and manufacture of physical or mechanical systems, such as power and energy systems, aerospace/aircraft products, weapon systems, transportation products, engines, compressors, powertrains, kinematic chains, vacuum technology, vibration isolation equipment, manufacturing, robotics, turbines, audio equipments, and mechatronics.

Interdisciplinary engineering draws from more than one of the principle branches of the practice. Historically, naval engineering and mining engineering were major branches. Other engineering fields are manufacturing engineering, acoustical engineering, corrosion engineering, instrumentation and control, aerospace, automotive, computer, electronic, information engineering, petroleum, environmental, systems, audio, software, architectural, agricultural, biosystems, biomedical, geological, textile, industrial, materials, and nuclear engineering. These and other branches of engineering are represented in the 36 licensed member institutions of the UK Engineering Council.

New specialties sometimes combine with the traditional fields and form new branches – for example, Earth systems engineering and management involves a wide range of subject areas including engineering studies, environmental science, engineering ethics and philosophy of engineering.

Aerospace engineering studies design, manufacture aircraft, satellites, rockets, helicopters, and so on. It closely studies the pressure difference and aerodynamics of a vehicle to ensure safety and efficiency. Since most of the studies are related to fluids, it is applied to any moving vehicle, such as cars.

Marine engineering is associated with anything on or near the ocean. Examples are, but not limited to, ships, submarines, oil rigs, structure, watercraft propulsion, on-board design and development, plants, harbors, and so on. It requires a combined knowledge in mechanical engineering, electrical engineering, civil engineering,and some programming abilities.

One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur, European Engineer, or Designated Engineering Representative.

In the engineering design process, engineers apply mathematics and sciences such as physics to find novel solutions to problems or to improve existing solutions. Engineers need proficient knowledge of relevant sciences for their design projects. As a result, many engineers continue to learn new material throughout their career.

If multiple solutions exist, engineers weigh each design choice based on their merit and choose the solution that best matches the requirements. The task of the engineer is to identify, understand, and interpret the constraints on a design in order to yield a successful result. It is generally insufficient to build a technically successful product, rather, it must also meet further requirements.

Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.

Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a problem. Creating an appropriate mathematical model of a problem often allows them to analyze it (sometimes definitively), and to test potential solutions.

Usually, multiple reasonable solutions exist, so engineers must evaluate the different design choices on their merits and choose the solution that best meets their requirements. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of "low-level" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.

Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected.

Engineers take on the responsibility of producing designs that will perform as well as expected and will not cause unintended harm to the public at large. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure.

The study of failed products is known as forensic engineering and can help the product designer in evaluating his or her design in the light of real conditions. The discipline is of greatest value after disasters, such as bridge collapses, when careful analysis is needed to establish the cause or causes of the failure.

As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.
One of the most widely used design tools in the profession is computer-aided design (CAD) software. It enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.

These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.

There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and Architecture, engineering and construction (AEC) software for civil engineering.

In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).

The engineering profession engages in a wide range of activities, from large collaboration at the societal level, and also smaller individual projects. Almost all engineering projects are obligated to some sort of financing agency: a company, a set of investors, or a government. The few types of engineering that are minimally constrained by such issues are "pro bono" engineering and open-design engineering.

By its very nature engineering has interconnections with society, culture and human behavior. Every product or construction used by modern society is influenced by engineering. The results of engineering activity influence changes to the environment, society and economies, and its application brings with it a responsibility and public safety.

Engineering projects can be subject to controversy. Examples from different engineering disciplines include the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some western engineering companies have enacted serious corporate and social responsibility policies.

Engineering is a key driver of innovation and human development. Sub-Saharan Africa, in particular, has a very small engineering capacity which results in many African nations being unable to develop crucial infrastructure without outside aid. The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.

All overseas development and relief NGOs make considerable use of engineers to apply solutions in disaster and development scenarios. A number of charitable organizations aim to use engineering directly for the good of mankind:

Engineering companies in many established economies are facing significant challenges with regard to the number of professional engineers being trained, compared with the number retiring. This problem is very prominent in the UK where engineering has a poor image and low status. There are many negative economic and political issues that this can cause, as well as ethical issues. It is widely agreed that the engineering profession faces an "image crisis", rather than it being fundamentally an unattractive career. Much work is needed to avoid huge problems in the UK and other western economies.

Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large. The National Society of Professional Engineers code of ethics states:
In Canada, many engineers wear the Iron Ring as a symbol and reminder of the obligations and ethics associated with their profession.

There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.

Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely "engineering scientists".
In the book "What Engineers Know and How They Know It", Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.

There is a "real and important" difference between engineering and physics as similar to any science field has to do with technology. Physics is an exploratory science that seeks knowledge of principles while engineering uses knowledge for practical applications of principles. The former equates an understanding into a mathematical principle while the latter measures variables involved and creates technology. For technology, physics is an auxiliary and in a way technology is considered as applied physics. Though physics and engineering are interrelated, it does not mean that a physicist is trained to do an engineer's job. A physicist would typically require additional and relevant training. Physicists and engineers engage in different lines of work. But PhD physicists who specialize in sectors of engineering physics and applied physics are titled as Technology officer, R&D Engineers and System Engineers.

An example of this is the use of numerical approximations to the Navier–Stokes equations to describe aerodynamic flow over an aircraft, or the use of the Finite element method to calculate the stresses in complex components. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.

As stated by Fung "et al." in the revision to the classic engineering text "Foundations of Solid Mechanics":

Engineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress innovation and invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a complex system, device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what already exists. Since a design has to be realistic and functional, it must have its geometry, dimensions, and characteristics data defined. In the past engineers working on new designs found that they did not have all the required information to make design decisions. Most often, they were limited by insufficient scientific knowledge. Thus they studied mathematics, physics, chemistry, biology and mechanics. Often they had to add to the sciences relevant to their profession. Thus engineering sciences were born.

Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability, and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.

The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology.
Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers. The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.

Conversely, some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.

Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.

Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.

The heart for example functions much like a pump, the skeleton is like a linked structure with levers, the brain produces electrical signals etc. These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.

Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.

There are connections between engineering and art, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering).

The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design. Robert Maillart's bridge design is perceived by some to have been deliberately artistic. At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.

Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.

Business Engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management or "Management engineering" is a specialized field of management concerned with engineering practice or the engineering industry sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge, and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in-depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to engineering practice or the engineering sector. This work often deals with large scale complex business transformation or Business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical & electronics, power distribution & generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives.

In political science, the term "engineering" has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Financial engineering has similarly borrowed the term.








</doc>
<doc id="272955" url="https://en.wikipedia.org/wiki?curid=272955" title="Singing telegram">
Singing telegram

A singing telegram is a message that is delivered by an artist in a musical form. Singing telegrams are historically linked to normal telegrams, but tend to be humorous. Sometimes the artist is in costume or formal clothing. Singing telegrams are often given as a gift.

Western Union, the American telegraph company began offering singing telegram services in 1933. That July 28, a fan sent Hollywood singing star Rudy Vallee a birthday greeting by telegram. George P. Oslin (1899–1996), the Western Union public relations director, decided this would be a good opportunity to make telegrams, which had been associated with deaths and other tragic news, into something more popular. He asked a Western Union operator, Lucille Lipps, to sing the message over the telephone, and this became the first singing telegram. While Oslin created the singing telegram because he thought "that messages should be fun," he recalled that he "was angrily informed I was making a laughingstock of the company."

As relatively few telegram recipients had telephones, most telegrams, including singing telegrams, were first delivered in person. The popularization of the telephone in the 1960s reduced telegrams in general. By 1972, Western Union was receiving a small number of requests for singing telegrams and was seeking regulatory approval on a state-by-state basis to eliminate the offering. Western Union suspended its singing telegram service in 1974, but independent singing telegram companies, specializing in often costumed personal delivery of gift messages, have kept up the tradition.

A DIY singing telegram is a specially written song all about the person in karaoke DVD form, so that any singer anywhere in the world can deliver the fully personalised singing telegram.

A kissogram, also called kissagram (short for kissing telegram), is a message delivered along with a kiss, usually arranged as a fun surprise for a person for a special occasion. Message deliverers can be hired either by themselves, or as part of an agency. A kissogram is usually presented by a performer dressed in a costume or uniform such as a police officer, nurse, french maid or nun. This term was used in the TV program Doctor Who to describe the profession of The Doctor's companion Amy Pond.

A stripogram or strippergram is a form of message delivery in which a stripper will perform a striptease while singing or dancing. This type of entertainment became popular in the 1970s in the US and spread to the UK and Europe during the 1980s. Typically a Strip-O-Gram is most often a form of gift given for a birthday, anniversary or bachelor party. A common practice is for the stripper-gram to be dressed in an outfit of one kind or another and to act out some charade connected with this, before commencing their actfor example a police officer 'arresting' somebody, a lawyer pretending to serve papers, a jilted bride and so on (sometimes this charade will be relevant to something the intended 'victim' has experienced, such as a divorce, or brush with the law). Usually a Strip-o-gram entertainer is accompanied by a chaperone, who plays her music, holds on to the money, tips and secures her safety. Unlike an escort who comes alone and does more of a one on one with the celebrant and is not chaperoned. Now-a-days, some agencies or performers do both to secure a bigger pay out.




</doc>
<doc id="363786" url="https://en.wikipedia.org/wiki?curid=363786" title="Stereopticon">
Stereopticon

A stereopticon is a slide projector or relatively powerful "magic lantern", which has two lenses, usually one above the other, and has mainly been used to project photographic images.
These devices date back to the mid 19th century, and were a popular form of entertainment and education before the advent of moving pictures.

Magic lanterns originally used rather weak light sources, like candles or oil lamps, that produced projections that were just large and strong enough to entertain small groups of people. During the 19th century stronger light sources, like limelight, became available. 

For the "dissolving views" lantern shows that were popularized by Henry Langdon Childe since the late 1830s, lanternists needed to be able to project two aligned pictures in the same spot on a screen, gradually dimming a first picture while revealing a second one. This could be done with two lanterns, but soon biunial lanterns (with two objectives placed one above the other) became common.

William and Frederick Langenheim from Philadelphia introduced a photographic glass slide technology at the Crystal Palace Exhibition in London in 1851. For circa two centuries magic lanterns had been used to project painted images from glass slides, but the Langenheim brothers seem to have been the firsts to incorporate the relatively new medium of photography (introduced in 1839). To enjoy the details of photographic slides optimally, the stronger lanterns were needed.

By 1860 Massachusetts chemist and businessman John Fallon improved a large biunial lantern, imported from England, and named it ‘stereopticon’.

For a usual fee of ten cents, people could view realistic images of nature, history, and science themes. The two lenses are used to dissolve between images when projected. This "visual storytelling" with technology directly preceded the development of the first moving pictures. 

The term stereopticon has been widely misused to name a stereoscope. The stereopticon has not commonly been used for three-dimensional images. 


</doc>
<doc id="5669042" url="https://en.wikipedia.org/wiki?curid=5669042" title="Pleorama">
Pleorama

The best-known pleorama was a 19th-century moving panorama entertainment where the viewers sat in a rocking boat while panoramic views on painted canvas rolled past. The word has sometimes been used for other entertainments or innovations.

Architect Carl Ferdinand Langhans introduced a pleorama in Breslau in 1831 with scenes of the Bay of Naples on both sides of 24 "voyagers" sitting in a wooden boat floating in a pool of water. The illusion was enhanced by light and sound effects: the boatman singing, Vesuvius erupting. Writer/artist August Kopisch was involved in designing the hour-long show.

Carl Wilhelm Gropius, who had a diorama exhibit in Berlin, took over management of this pleorama in 1832, and there was also a pleorama of a journey along the river Rhine.

The Swiss writer Bernard Comment, among others, has pointed out the similarities between Langhans' pleorama and the ambitious mareorama at the 1900 Paris Exhibition.

A similar idea was used for a London "padorama" in 1834. Spectators were seated in railway carriages to watch a moving panorama of scenes visible from the Liverpool and Manchester Railway.

In 1850s Finland the name "pleorama" was given to shows which presented historic scenes and panoramic views using glass, but posters for these do not mention anything resembling Langhans' boat concept.

The name "pleorama" was coined from Greek elements. Like other 19th century novelties ending in "-orama" - diorama and cyclorama, for instance - the second half of the word has the sense of 'something seen'. The "pleo-" part here is understood to come from a Greek word meaning 'float' which applies to Langhans' boat in water idea. "Pleorama" is also the 21st century name of an innovative "floating house".




</doc>
<doc id="1017350" url="https://en.wikipedia.org/wiki?curid=1017350" title="Cosmorama">
Cosmorama

A cosmorama is an exhibition of perspective pictures of different places in the world, usually world landmarks. Careful use of illumination and lenses gives the images greater realism.

Cosmorama was also the name of an entertainment in 19th century London, at 207-209 Regent Street, at which the public could view scenes of distant lands and exotic subjects through optical devices that magnified the pictures. It was later converted into an exhibition of curiosities named the Prince of Wales Bazaar. Exhibits included a sea lion, a sea serpent and L. Bertolotto's Flea circus. 

Port-au-Prince Cosmorama was an exhibition of perspective pictures of different places and landmarks in the world, held on March 2, 1834 in Port-au-Prince, Haiti.

"Cosmoramic Views Exhibited"





</doc>
<doc id="542704" url="https://en.wikipedia.org/wiki?curid=542704" title="Spoiler (media)">
Spoiler (media)

A spoiler is an element of a disseminated summary or description of any piece of fiction that reveals any plot elements which threaten to give away important details. Typically, the details of the conclusion of the plot, including the climax and ending, are especially regarded as spoiler material. It can also be used to refer to any piece of information regarding any part of a given media that a potential consumer was not intended to know beforehand.

The term spoiler was introduced in the early days of the Internet, and came to prominence in newsgroup conversations. It is still common in internet articles and social media discussions. Early rules of netiquette insisted that spoilers could and should be normally avoided, but if the posting of "spoiling" information was unavoidable, it be preceded by a warning such as "SPOILER ALERT", or the spoiler itself has to be masked so that it can not be visible to any but those keen for details and not fazed at the thought of such potentially plot-revealing information.

Sometimes, these warnings are omitted, accidentally or deliberately (see below), and some unwitting readers have had literature, films, television programmes and other works that they were looking forward to experiencing "spoiled".

There is a common demand, especially among internet users, to have protection against accidentally seeing material considered to include "spoiler" information, even in the internet version of settings where such material has conventionally and historically appeared, such as discussion groups or literary reviews. As a result of this level of objection to spoilers, trolls may post them purely for their own pleasure, finding amusement in believing they are completely ruining a narrative experience for others. On reputable websites, these can be reported to moderators and such posts taken down, the posters blacklisted, but only after the fact. Most such websites provide a means of tagging certain threads as containing spoilers for those who wish to discuss a fictional work in depth, including the outcomes of events and the handling of the narrative resolution. Some have felt compelled to avoid participating on public websites altogether, set up "closed" websites to exclude those who are sensitive about spoilers, or decided they had to unilaterally blog at the expense of public exchange.

On Usenet, the common method for obscuring spoiler information is to precede it with many blank lines known as 'spoiler space' – traditionally enough to push the information in question on to the next screen of a 25-line terminal. A simple cipher called ROT13 is also used in newsgroups to obscure spoilers, but is rarely used for this purpose elsewhere.

Some producers actively seed bogus information in order to misdirect fans. The director of the film "Terminator Salvation" orchestrated a "disinformation campaign" where false spoilers were distributed about the film, to mask any true rumors about its plot.

Wikipedia discloses spoilers in its articles without giving advance warning, although it previously did give such warning before 2006. Matthew Prichard, the grandson of Agatha Christie, criticized Wikipedia for giving away spoilers in the play "The Mousetrap". Andrew Jarecki, the producer of "Catfish", a documentary, argued that Wikipedia should have spoiler alerts. The plot of "Catfish" had been posted on Wikipedia before its theatrical release because the film had been shown at the 2010 Sundance Film Festival. Jay Walsh, a Wikimedia Foundation spokesperson, said that Wikipedia is intended to be an exhaustive knowledge source, so it would have spoilers.

Some internet forums and reference sites, such as TV Tropes and the IMDb FAQ section, have optional spoiler tags covering major plot details. The information underneath may be revealed by highlighting the text or, in the case of IMDb, rolling over the spoiler tag.

In 2011, Nicholas Christenfeld and Jonathan Leavitt of UC San Diego did a psychological experiment testing whether spoilers diminish enjoyment of fiction. They gave subjects short stories with twist endings to read, giving some of the subject's information about the twist in advance. For nearly every story, subjects who had the story "spoiled" enjoyed the story more than the subjects who didn't know the ending in advance.

There are some applications that prevent users from reading spoilers, such as TVShow Time's Google Chrome extension, which, once set up, blocks posts on social media about episodes that the user has not seen.

The market campaigns for Marvel Studios' "" and its sequel "" extensively promoted the maintenance of secrecy regarding the films' plots, with the latter's social media campaign including a hashtag (#DontSpoilTheEndgame), a signed letter from the Russo brothers and a video featuring the film's ensemble cast demanding that earlier viewers of the film refrain from spoiling the plot.

One of the first print uses of the terms was in the April 1971 issue of "National Lampoon". An article entitled "Spoilers," by Doug Kenney, lists spoilers for famous films and movies.
In 2005, the "Chicago Sun-Times" film critic Roger Ebert wrote an article entitled "Critics have no right to play spoiler" which contained spoilers and spoilers warnings. Ebert wrote:

Ebert used two spoiler warnings in the article, saying "If you have not yet seen "Million Dollar Baby" and know nothing about the plot, read no further" and later said, "Now yet another spoiler warning, because I am going to become more explicit." Ebert discussed six films in the article and mentioned how many critics handled "The Crying Game" and also noted a detail about the film "The Year of Living Dangerously". Ebert also mentioned two films alongside "Million Dollar Baby".

Ebert additionally criticized two commentators, Rush Limbaugh and Michael Medved (the latter of whom had "for a long time been a political commentator, not a movie critic"), for deliberately revealing the ending of the movie due to a moral disagreement with the lead character's life decision. "[S]hould no movie be allowed to consider [the moral issue]?" Ebert asked. "The separation of church and state in America was wisely designed to prevent religions from dictating the personal choices of those who do not share the same beliefs."

In an interview about his "Dark Tower" series (appearing in issue #4 of the 2007 Marvel Comic adaptation "The Gunslinger Born"), Stephen King was asked if there are spoilers in the first few novels that would ruin someone's experience of the comic. "There are no spoilers!", King replied, "You might as well say 'I'm never gonna watch "Wizard of Oz" again because I know how it comes out'".

The executive producer of "Doctor Who", Steven Moffat, expressed disappointment with fans who revealed spoilers after a leaked copy of "The Impossible Astronaut" was made available online before the episode's first television broadcast.

In April 2015, the Under the Gun Theater created "Swarm of Spoilers", a parody show based on George R.R. Martin's "Game of Thrones" series. The comedic play recapped the previous four seasons of the HBO television show. Kevin Mullaney, who directed "Swarm of Spoilers", stated: "I'm somebody who's very sensitive about spoilers, so I wanted to make sure it was very clear from the title," though he went on to say, "There's actually this theory about spoilers that we think that they hurt the enjoyment of shows, and I definitely feel that way sometimes, but I think there's been studies that show the other side: that when we know the ending of a story that we haven't read before, it actually enhances the story, so I don't know if it would actually hurt anyone to come see it ["Swarm of Spoilers"]." The final production included 45 of the series' characters, and was played by an 18-person ensemble.

The spoiling of James Holzhauer's loss on "Jeopardy!", which was reported upon by both print and Internet sources hours before it aired on most of the show's stations, had a somewhat unexpectedly positive impact on that episode's ratings. Instead of ruining the outcome, the spoilers had teased just enough to encourage viewers to tune in to see how the previously dominant Holzhauer was beaten. "Jeopardy!" does not contractually require its members to remain silent in regard to spoilers; its audience members have generally honored the honor system in not leaking spoilers before episodes air.





</doc>
<doc id="1981875" url="https://en.wikipedia.org/wiki?curid=1981875" title="Showroom">
Showroom

A showroom is a large space used to display products or show entertainment.

A showroom is a large space used to display products for sale, such as automobiles, furniture, appliances, carpet or apparel. It is a retail store of a company in which products are on sale in a space created by their brand or company. A showroom can also be a space for wholesale buyers to view fashion merchandise for sale in their retail stores.

The world's most famous locations for a showroom, generally in the form of a cluster, are the Champs Elysees in Paris and Merchandise Mart in Chicago.

One of the world's largest showrooms is the BMW showroom in Abu Dhabi. The biggest collection of showrooms is a car showroom in Istanbul called Autopia Europia.

A showroom is a permanent enclosed space used to present a performance. Sometimes it is customized for a particular show; for example, the Las Vegas Hilton showroom used for the rock opera "Starlight Express" was customized by pouring concrete ramps onto its stage area and in the seating area.

Some showrooms are used daily, while others are only used when a performer is booked to perform. In some cases, a showroom is leased to a performer, who then retains all income rather than being paid by the showroom owner.

In fashion capitals such as New York City, Paris, Milan or London one can find temporary showrooms. 
These places can be rented on a daily or weekly basis. Some temporary showrooms are managed with the help of event management agencies. Temporary showrooms can also be pop-up stores, which are short-term sales spaces.

Automobile manufacturer Ferrari has recently introduced augmented reality technology into their showrooms, to provide customers with a more hands-on approach when purchasing a vehicle.



</doc>
<doc id="7566837" url="https://en.wikipedia.org/wiki?curid=7566837" title="Bioscope show">
Bioscope show

A Bioscope show was a music hall and fairground attraction consisting of a travelling cinema. The heyday of the Bioscope was from the late 1890s until World War I.

Bioscope shows were fronted by the largest fairground organs, and these formed the entire public face of the show. A stage was usually in front of the organ, and dancing girls would entertain the crowds between film shows. 

Films shown in the Bioscope were primitive, and the earliest of these were made by the showmen themselves. Later, films were commercially produced.

Bioscope shows were integrated, in Britain at least, into the Variety shows in the huge Music Halls which were built at the end of the nineteenth century.

After the Music Hall Strike of 1907 in London, bioscope operators set up a trade union to represent them. There were about seventy operators in London at this point.

In South Africa "Bioscope" or in Afrikaans "bioskoop" is an archaic word for the cinema and some people (especially older generations) still use it regularly.

In modern day Dutch, "bioscoop" is a wide-spread term, and the equivalent of the English "movie theater" or "cinema".

In Serbian language, "bioskop" is a modern term for movie theater.


</doc>
<doc id="181885" url="https://en.wikipedia.org/wiki?curid=181885" title="Ribaldry">
Ribaldry

Ribaldry, or blue comedy, is humorous entertainment that ranges from bordering on indelicacy to gross morality/indecency. It is also referred to as "bawdiness" or "bawdy".

Sex is presented in ribald material more for the purpose of poking fun at the foibles and weaknesses that manifest themselves in human sexuality, rather than to present sexual stimulation either excitingly or artistically. Also, ribaldry may use sex as a metaphor to illustrate some non-sexual concern, in which case ribaldry borders satire.

Like any humour, ribaldry may be read as conventional or subversive. Ribaldry typically depends on a shared background of sexual conventions and values, and its comedy generally depends on seeing those conventions broken.

The ritual taboo-breaking that is a usual counterpart of ribaldry underlies its controversial nature and explains why ribaldry is sometimes a subject of censorship. Ribaldry, whose usual aim is "not" "merely" to be sexually stimulating, often does address larger concerns than mere sexual appetite. However, being presented in the form of comedy, these larger concerns may be overlooked by censors.

Ribaldry differs from black comedy (or gallows humor) in that black comedy deals with topics which would normally be considered painful or frightening whereas ribaldry deals with topics that would only be considered offensive.

Ribaldry is present to some degree in every culture and has likely been around for all of human history. Works like "Lysistrata" by Aristophanes, "Menaechmi" by Plautus, "Cena Trimalchionis" by Petronius, and "The Golden Ass" of Apuleius are ribald classics from ancient Greece and Rome. Geoffrey Chaucer's "The Miller's Tale" from his "Canterbury Tales" and "The Crabfish", one of the oldest English traditional ballads, are classic examples. The Frenchman François Rabelais showed himself to be a master of ribaldry (technically called grotesque body) in his "Gargantua" and other works. "The Life and Opinions of Tristram Shandy, Gentleman" by Laurence Sterne and "The Lady's Dressing Room" by Jonathan Swift are also in this genre; as is Mark Twain's long-suppressed "1601".

Another example of ribaldry is "De Brevitate Vitae", a song which in many European-influenced universities is both a student beer-drinking song and an anthem sung by official university choirs at public graduation ceremonies. The private and public versions of the song contain vastly different words. More recent works like "Candy", "Barbarella", "L'Infermiera", the comedic works of Russ Meyer, "Little Annie Fanny" and John Barth's "The Sot-Weed Factor" are probably better classified as ribaldry than as either pornography or erotica.

A bawdy song is a humorous song that emphasises sexual themes and is often rich with innuendo. Historically these songs tend to be confined to groups of young males, either as students or in an environment where alcohol is flowing freely. An early collection was "Wit and Mirth, or Pills to Purge Melancholy", edited by Thomas D'Urfey and published between 1698 and 1720. Selected songs from "Wit and Mirth" have been recorded by the City Waites and other singers. Sailor's songs tend to be quite frank about the exploitative nature of the relationship between men and women. There are many examples of folk songs in which a man encounters a woman in the countryside. This is followed by a short conversation, and then sexual intercourse, e.g. "The Game of All Fours". Neither side demonstrates any shame or regret. If the woman becomes pregnant, the man will not be there anyway. Rugby songs are often bawdy. Examples of bawdy folk songs are: "Seventeen Come Sunday" and "The Ballad of Eskimo Nell". Robert Burns compiled "The Merry Muses of Caledonia" (the title is not Burns's), a collection of bawdy lyrics that were popular in the music halls of Scotland as late as the 20th century. In modern times Hash House Harriers have taken on the role of tradition-bearers for this kind of song. "The Unexpurgated Folk Songs of Men" (Arhoolie 4006) is a gramophone record containing a collection of American bawdy songs recorded in 1959.

Blue comedy is comedy that is off-color, risqué, indecent or profane, largely about sex. It often contains profanity or sexual imagery that may shock and offend some audience members.

"Working blue" refers to the act of using curse words and discussing things that people do not discuss in "polite society". A "blue comedian" or "blue comic" is a comedian who usually performs risqué routines layered with curse words.

There is a common belief that comedian Max Miller (1894–1963) coined the phrase, after his stage act which involved telling jokes from either a white book or a blue book, chosen by audience preference (the blue book contained ribald jokes). This is not so, as the "Oxford English Dictionary" contains earlier references to the use of blue to mean ribald: 1890 "Sporting Times" 25 Jan. 1/1 ""Shifter wondered whether the damsel knew any novel blue stories." and 1900 "Bulletin" (Sydney) 20 Oct. 12/4 "Let someone propose to celebrate Chaucer by publicly reading some of his bluest productions unexpurgated. The reader would probably be locked up.""

Private events at show business clubs such as the Bob Saget Club and The Masquers often showed this blue side of otherwise cleancut Bob Saget; a recording survives of one Masquers roast from the 1950s with Jack Benny, George Jessel, George Burns, and Art Linkletter all using highly risqué material and obscenities. Many comedians who are normally family-friendly might choose to work blue when off-camera or in an adult-oriented environment; Bob Saget exemplifies this dichotomy. Bill Cosby's 1969 record album "" records both his family-friendly evening standup comedy show, and his blue midnight show, which included a joke about impregnating his wife "right through the old midnight trampoline" (her diaphragm) and other sexual references.

Some comedians build their careers on blue comedy. Among the best known of these are Redd Foxx, Lawanda Page, and the team of Leroy and Skillet, all of whom later performed on the family-friendly television show "Sanford and Son". Page, Leroy, and Skillet specialized in a particular African American form of blue spoken word recitation called signifying or toasting. Dave Attell has also been described by his peers as one of the greatest modern-day blue comics.

On talk radio in the United States, many commentators use blue comedy in their political programs. Examples include Neal Boortz, Eric Von Haessler, Phil Hendrie and Steve Morrison.





</doc>
<doc id="2605599" url="https://en.wikipedia.org/wiki?curid=2605599" title="Calligraphic projection">
Calligraphic projection

Calligraphic projection is a system for displaying or projecting an image composed of a beam of light or electrons directly tracing the image, as opposed to sweeping in raster order over the entire display surface, as in a standard pixel-based display. Calligraphic projection is presently often used for laser lighting displays, whereby one or more laser beams draws an image on a screen by reflecting the laser beam from one or more mirrors attached to a deflecting mechanism.

Analog oscilloscopes have customarily employed this kind of vector graphics, as did a number of CRT-based vector monitor computer graphics terminals in the 1970s and 1980s, such as the Tektronix 4014 and the Evans & Sutherland Picture System.

Calligraphic projection is sometimes called Lissajous projection, after the mathematical figure (and mathematician).



</doc>
<doc id="6607106" url="https://en.wikipedia.org/wiki?curid=6607106" title="Party motivator">
Party motivator

A party motivator is a person paid to entertain attendants at a party. Typically, party motivators are attractive young men and women who dress fashionably and attempt to engage guests in socializing and dancing. Hiring party motivators is primarily a phenomenon within the American upper middle and upper classes; they are especially common at bar mitzvahs.




</doc>
<doc id="2955100" url="https://en.wikipedia.org/wiki?curid=2955100" title="Patter">
Patter

Patter is a prepared and practiced speech that is designed to produce a desired response from its audience. Examples of occupations with a patter might include the auctioneer, salesperson, dance caller, magician, or comedian.

The term may have been a colloquial shortening of "Pater Noster", or the Lord's Prayer, and may have referred to the practice of mouthing or mumbling prayers quickly and mechanically.

From this, it became a slang word for the secret and equally incomprehensible mutterings of a cant language used by beggars, thieves, gypsies, etc., and then the fluent plausible talk that a cheap-jack employs to pass off his goods. Many illusionists, e.g., card magicians, use patter both to enhance the show and to distract the attention of the spectators.

It is thus also used of any rapid manner of talking, and of a patter-song, in which a very large number of words have to be sung at high speed to fit the music. A western square dance caller may interpolate patter—in the form of metrical lines, often of nonsense—to fill in between commands to the dancers.

In some circumstances, the talk becomes a different sense of "patter": to make a series of rapid strokes or pats, as of raindrops. Here, it is a form of onomatopoeia.

In certain forms of entertainment, peep shows (in the historical meaning) and Russian "rayok", patter is an important component of a show. The radio DJ patter is among the roots of rapping.

In hypnotherapy, the hypnotist uses a 'patter' or script to deliver positive suggestions for change to the client.

In "London Labour and the London Poor", Henry Mayhew divides the street-sellers of his time into two groups: the "patterers", and everyone else.



</doc>
<doc id="16998086" url="https://en.wikipedia.org/wiki?curid=16998086" title="Seat filler">
Seat filler

A seat filler is a person who fills in an empty seat during an event. There are two types of seat fillers:





</doc>
<doc id="25445767" url="https://en.wikipedia.org/wiki?curid=25445767" title="Shadowgraphy (performing art)">
Shadowgraphy (performing art)

Shadowgraphy or ombromanie is the art of performing a story or show using images made by hand shadows. It can be called "cinema in silhouette". Performers are titled as a shadowgraphist or shadowgrapher.

The art has declined since the late 19th century when electricity became available to homes because light bulbs and electric lamps do not give off good shadows and because cinema and television were becoming a new form of entertainment. Shadows are greatly defined by candlelight; therefore hand shadows were common in earlier centuries.

The modern art of hand shadows was made popular by the French entertainer Félicien Trewey in the 19th century. He popularized the art by making silhouettes of famous personalities.

Since shadows have existed since the existence of objects obstructing light, it is hard to say when the art was first used by humans for entertainment. It could have been practiced by ancient or later humans, but it probably originated in the Far East. The French entertainer Félicien Trewey was interested in the art of Chinese shadow puppetry called Ombres Chinoises, which means "Chinese shadows". He popularized the art of hand shadows when he developed shadows of famous silhouettes. It then became popular in Europe in the 19th century.

Although the art is popular amongst different kinds of entertainers it seems prominent amongst magicians, because it was popularized by a magician who inspired many other magicians. Félicien Trewey perfected the widely known elephant, bird, and cat hand shadows and created some of his own such as The Volunteer, Robinson Crusoe, The Jockey, The Rope Dancer and more. In 1889, Trewey joined with Alexander Herrmann who most likely learned it from him. David Tobias Bamberg most likely learned it from Alexander who then passed it down to his son Okito (Tobias Leendert Bamberg) who then passed it down to "his" son Fu Manchu (David Theodore Bamberg). Fu Manchu passed his skill to Marcelo Contento, one of his apprentices, who became famous worldwide for it. Contento died before he could pass it on to his son.

Other magicians who used hand shadows in their act include David Devant, Edward Victor, and the duo Holden and Graham in which Holden was famous for his "Monkey in the Belfry" shadow.

The hands and fingers are exercised and different finger positions are practiced to help aid in forming shadows.

The light source to be used should be small and bright. The best shadows come from light proceeding from the smallest possible point. Albert Almoznino suggests a candle, a flashlight (with the lens and reflector removed) or any very small light. If a bulb is used, it should be clear. J. C. Cannell suggests in his book, "Modern Conjuring For Amateurs", that the best source of light is the electric arc, which Almoznino agrees to the small arc lamp, and the second best being the limelight (if used with a high-class jet). Trewey suggests the chalk for the limelight to be cut in a triangular form, or else it will produce a gray border around the shadow. Cannell states another favorite amongst shadowgraphists is the use of acetylene gas (i.e. acetylene gas lamp or carbide lamp). Nowadays it is possible to use a single lensless (for example, SMD) LED.

Albert Almoznino suggests to use a white or light-colored wall, sheet or table cloth for a small audience as in a private home. If a wall is dark-colored, the sheet or table cloth can be hung against it. If performing for a large audience such as in an auditorium or on a stage, he suggests a screen made of muslin or other thin cloth attached to a frame. In a nightclub, hall or small theater, he suggests a nylon screen on a pliable aluminum frame. It is a screen sometimes used for TV projection called a rear projection screen, but the light must be stronger like a small spotlight without the projector, lenses, or diffusers, or a motion-picture projector with the front lenses removed.

The performer sits or stands between the light source and the blank surface, while having the option to perform in front of the performance surface or behind it, with each having different advantages. The performer has another option to perform from the left or the right of the light source. The farther the hands are from the light, the smaller the shadows will be, while the closer the hands are to the light, the larger the shadows will be. Also, the closer the hands are to the blank surface, the sharper the shadows will be. Trewey suggests that the most convenient distance for the light from the hands is four feet while the hands from the performance surface should be about six feet. The performer should always watch their shadows instead of their hands.

Movement helps give the shadows character and brings them to life. Some shadows are performed with accessories attached to the hands or fingers to achieve movements or images not applicable to hands alone.





</doc>
<doc id="339806" url="https://en.wikipedia.org/wiki?curid=339806" title="Burlesque">
Burlesque

A burlesque is a literary, dramatic or musical work intended to cause laughter by caricaturing the manner or spirit of serious works, or by ludicrous treatment of their subjects. The word derives from the Italian ', which, in turn, is derived from the Italian ' – a joke, ridicule or mockery.

Burlesque overlaps in meaning with caricature, parody and travesty, and, in its theatrical sense, with extravaganza, as presented during the Victorian era. "Burlesque" has been used in English in this literary and theatrical sense since the late 17th century. It has been applied retrospectively to works of Chaucer and Shakespeare and to the Graeco-Roman classics. Contrasting examples of literary burlesque are Alexander Pope's "The Rape of the Lock" and Samuel Butler's "Hudibras". An example of musical burlesque is Richard Strauss's 1890 Burleske for piano and orchestra. Examples of theatrical burlesques include W. S. Gilbert's "Robert the Devil" and the A. C. Torr – Meyer Lutz shows, including "Ruy Blas and the Blasé Roué".

A later use of the term, particularly in the United States, refers to performances in a variety show format. These were popular from the 1860s to the 1940s, often in cabarets and clubs, as well as theatres, and featured bawdy comedy and female striptease. Some Hollywood films attempted to recreate the spirit of these performances from the 1930s to the 1960s, or included burlesque-style scenes within dramatic films, such as 1972's "Cabaret" and 1979's "All That Jazz", among others. There has been a resurgence of interest in this format since the 1990s.

The word first appears in a title in Francesco Berni's "Opere burlesche" of the early 16th century, works that had circulated widely in manuscript before they were printed. For a time, burlesque verses were known as "poesie bernesca" in his honour. 'Burlesque' as a literary term became widespread in 17th century Italy and France, and subsequently England, where it referred to a grotesque imitation of the dignified or pathetic. Shakespeare's Pyramus and Thisbe scene in "Midsummer Night's Dream" and the general mocking of romance in Beaumont and Fletcher's "The Knight of the Burning Pestle" were early examples of such imitation.

In 17th century Spain, playwright and poet Miguel de Cervantes ridiculed medieval romance in his many satirical works. Among Cervantes' works are "Exemplary Novels" and the "Eight Comedies and Eight New Interludes" published in 1615. The term burlesque has been applied retrospectively to works of Chaucer and Shakespeare and to the Graeco-Roman classics.

Burlesque was intentionally ridiculous in that it imitated several styles and combined imitations of certain authors and artists with absurd descriptions. In this, the term was often used interchangeably with "pastiche", "parody", and the 17th and 18th century genre of the "mock-heroic". Burlesque depended on the reader's (or listener's) knowledge of the subject to make its intended effect, and a high degree of literacy was taken for granted.

17th and 18th century burlesque was divided into two types: High burlesque refers to a burlesque imitation where a literary, elevated manner was applied to a commonplace or comically inappropriate subject matter as, for example, in the literary parody and the mock-heroic. One of the most commonly cited examples of high burlesque is Alexander Pope's "sly, knowing and courtly" "The Rape of the Lock". Low burlesque applied an irreverent, mocking style to a serious subject; an example is Samuel Butler's poem "Hudibras", which described the misadventures of a Puritan knight in satiric doggerel verse, using a colloquial idiom. Butler's addition to his comic poem of an ethical subtext made his caricatures into satire.

In more recent times, burlesque true to its literary origins is still performed in revues and sketches. Tom Stoppard's 1974 play "Travesties" is an example of a full-length play drawing on the burlesque tradition.

Beginning in the early 18th century, the term burlesque was used throughout Europe to describe musical works in which serious and comic elements were juxtaposed or combined to achieve a grotesque effect. As derived from literature and theatre, "burlesque" was used, and is still used, in music to indicate a bright or high-spirited mood, sometimes in contrast to seriousness.

In this sense of farce and exaggeration rather than parody, it appears frequently on the German-language stage between the middle of the 19th century and the 1920s. Burlesque operettas were written by Johann Strauss II ("Die lustigen Weiber von Wien", 1868), Ziehrer ("Mahomed's Paradies", 1866; "Das Orakel zu Delfi", 1872; "Cleopatra, oder Durch drei Jahrtausende", 1875; "In fünfzig Jahren", 1911) and Bruno Granichstaedten ("Casimirs Himmelfahrt", 1911). French references to burlesque are less common than German, though Grétry composed for a "drame burlesque" ("Matroco", 1777). Stravinsky called his 1916 one-act chamber opera-ballet "Renard" ("The Fox") a "Histoire burlesque chantée et jouée" ("burlesque tale sung and played") and his 1911 ballet "Petrushka" a "burlesque in four scenes". A later example is the 1927 burlesque operetta by Ernst Krenek entitled "Schwergewicht" ("Heavyweight") (1927).

Some orchestral and chamber works have also been designated as burlesques, of which two early examples are the Ouverture-Suite "Burlesque de Quixotte", TWV 55, by Telemann and the Sinfonia Burlesca by Leopold Mozart (1760). Another often-performed piece is Richard Strauss's 1890 Burleske for piano and orchestra. Other examples include the following:

Burlesque can be used to describe particular movements of instrumental musical compositions, often involving dance rhythms. Examples are the Burlesca, in Partita No. 3 for keyboard (BWV 827) by Bach, the "Rondo-Burleske" third movement of Symphony No. 9 by Mahler, and the "Burlesque" fourth movement of Shostakovich's Violin Concerto No. 1.

The use of burlesque has not been confined to classical music. Well known ragtime travesties include "The Russian Rag", by George L. Cobb, which is based on Rachmaninoff's Prelude in C-sharp minor, and Harry Alford's "Lucy's Sextette" based on the sextet, 'Chi mi frena in tal momento?', from "Lucia di Lammermoor" by Donizetti.

Victorian burlesque, sometimes known as "travesty" or "extravaganza", was popular in London theatres between the 1830s and the 1890s. It took the form of musical theatre parody in which a well-known opera, play or ballet was adapted into a broad comic play, usually a musical play, often risqué in style, mocking the theatrical and musical conventions and styles of the original work, and quoting or pastiching text or music from the original work. The comedy often stemmed from the incongruity and absurdity of the classical subjects, with realistic historical dress and settings, being juxtaposed with the modern activities portrayed by the actors. Madame Vestris produced burlesques at the Olympic Theatre beginning in 1831 with "Olympic Revels" by J. R. Planché. Other authors of burlesques included H. J. Byron, G. R. Sims, F. C. Burnand, W. S. Gilbert and Fred Leslie.

Victorian burlesque related to and in part derived from traditional English pantomime "with the addition of gags and 'turns'." In the early burlesques, following the example of ballad opera, the words of the songs were written to popular music; later burlesques mixed the music of opera, operetta, music hall and revue, and some of the more ambitious shows had original music composed for them. This English style of burlesque was successfully introduced to New York in the 1840s.
Some of the most frequent subjects for burlesque were the plays of Shakespeare and grand opera. The dialogue was generally written in rhyming couplets, liberally peppered with bad puns. A typical example from a burlesque of "Macbeth": Macbeth and Banquo enter under an umbrella, and the witches greet them with "Hail! hail! hail!" Macbeth asks Banquo, "What mean these salutations, noble thane?" and is told, "These showers of 'Hail' anticipate your 'reign'". A staple of burlesque was the display of attractive women in travesty roles, dressed in tights to show off their legs, but the plays themselves were seldom more than modestly risqué.
Burlesque became the speciality of certain London theatres, including the Gaiety and Royal Strand Theatre from the 1860s to the early 1890s. Until the 1870s, burlesques were often one-act pieces running less than an hour and using pastiches and parodies of popular songs, opera arias and other music that the audience would readily recognize. The house stars included Nellie Farren, John D'Auban, Edward Terry and Fred Leslie. From about 1880, Victorian burlesques grew longer, until they were a whole evening's entertainment rather than part of a double- or triple-bill. In the early 1890s, these burlesques went out of fashion in London, and the focus of the Gaiety and other burlesque theatres changed to the new more wholesome but less literary genre of Edwardian musical comedy.

American burlesque shows were originally an offshoot of Victorian burlesque. The English genre had been successfully staged in New York from the 1840s, and it was popularised by a visiting British burlesque troupe, Lydia Thompson and the "British Blondes", beginning in 1868. New York burlesque shows soon incorporated elements and the structure of the popular minstrel shows. They consisted of three parts: first, songs and ribald comic sketches by low comedians; second, assorted olios and male acts, such as acrobats, magicians and solo singers; and third, chorus numbers and sometimes a burlesque in the English style on politics or a current play. The entertainment was usually concluded by an exotic dancer or a wrestling or boxing match.

While burlesque went out of fashion in England towards the end of the 19th century, to be replaced by Edwardian musical comedy, the American style of burlesque flourished, but with increasing focus on female nudity. Exotic "cooch" dances were brought in, ostensibly Syrian in origin. The entertainments were given in clubs and cabarets, as well as music halls and theatres. By the early 20th century, there were two national circuits of burlesque shows competing with the vaudeville circuit, as well as resident companies in New York, such as Minsky's at the Winter Garden.
The transition from burlesque on the old lines to striptease was gradual. At first, soubrettes showed off their figures while singing and dancing; some were less active but compensated by appearing in elaborate stage costumes. The strippers gradually supplanted the singing and dancing soubrettes; by 1932 there were at least 150 strip principals in the US. Star strippers included Sally Rand, Gypsy Rose Lee, Tempest Storm, Lili St. Cyr, Blaze Starr, Ann Corio and Margie Hart, who was celebrated enough to be mentioned in song lyrics by Lorenz Hart and Cole Porter. By the late 1930s, burlesque shows would have up to six strippers supported by one or two comics and a master of ceremonies. Comics who appeared in burlesque early in their careers included Fanny Brice, Mae West, Eddie Cantor, Abbott and Costello, W. C. Fields, Jackie Gleason, Danny Thomas, Al Jolson, Bert Lahr, Phil Silvers, Sid Caesar, Danny Kaye, Red Skelton and Sophie Tucker.
The uninhibited atmosphere of burlesque establishments owed much to the free flow of alcoholic liquor, and the enforcement of Prohibition was a serious blow. In New York, Mayor Fiorello H. La Guardia clamped down on burlesque, effectively putting it out of business by the early 1940s. It lingered on elsewhere in the U.S., increasingly neglected, and by the 1970s, with nudity commonplace in theatres, reached "its final shabby demise." Both during its declining years and afterwards there have been films that sought to capture American burlesque, including "Lady of Burlesque" (1943), "Striporama" (1953), and "The Night They Raided Minsky's" (1968).
In recent decades, there has been a revival of burlesque, sometimes called Neo-Burlesque, on both sides of the Atlantic. A new generation, nostalgic for the spectacle and perceived glamour of the classic American burlesque, developed a cult following for the art in the early 1990s at Billie Madley's "Cinema" and later at the "Dutch Weismann's Follies" revues in New York City, "The Velvet Hammer" troupe in Los Angeles and The Shim-Shamettes in New Orleans. Ivan Kane's Royal Jelly Burlesque Nightclub at Revel Atlantic City opened in 2012. Notable Neo-burlesque performers include Dita Von Teese, and Julie Atlas Muz and Agitprop groups like Cabaret Red Light incorporated political satire and performance art into their burlesque shows. Annual conventions such as the Vancouver International Burlesque Festival and the Miss Exotic World Pageant are held.





</doc>
<doc id="31604229" url="https://en.wikipedia.org/wiki?curid=31604229" title="Ancillary market">
Ancillary market

Ancillary markets are non-theatrical markets for feature films, like home video, television, Pay Per View, VOD, Internet streaming, airlines and others.

Before television, studios played their films in theaters exclusively. However, in 1950 many studios began to sell all their pre-1948 features to television syndicators. The television syndicators then would use these films to fill in their programming schedules. Back in the 1960s, the first domestic ancillary market for feature films was created. NBC was the first to practice the market on September 23, 1961, by programming "NBC Saturday Night at the Movies." After such success, ABC became the second network ever to program a series of prime time features in 1962. One of the other networks, CBS, followed and added a prime time feature program in 1965.

Today, feature films opens in motion picture theaters to establish its box-office value. After that is established, it is then released to ancillary markets in a particular order as follows:


The sequence is to maximize the full economic potential of each market.

Home video recorders were made public when Sony introduced the half-inch Betamax cassette in 1975. Following Betamax, the company JVC introduced the Video Home System (VHS). Marketed by RCA and manufactured by Matsushita, VHS soon became known as the video-cassette recorders (VCRs). VCRs, which gave the consumer the option of recording programs from television, were a new form of competition in the demanding consumer market. VCRs revenue contributed to the development of the ancillary market of video and DVD. By the 1980s, five million households owned VCRs. Major studios had not yet adapted to the new video technologies that were being developed for consumers. There were no anticipations of new markets or other opportunities to expand until an entrepreneur, Andre Blay, opened Hollywood film companies’ eyes. Blay wanted the license to transfer and sell their films on tape. After he succeeded and his approach was beneficial, film companies all around became a part of the video distribution. The film companies could not deny the fact that this new distribution would lead to a new revenue stream.

As the VHS market saturated, multiple media executives and manufacturers liked the idea of utilizing other home video technologies. In 1993, the film industry upgraded their technology with the creation of several new formats including the DVD, or digital video disc. Many manufacturers such as the Japanese (Hitachi, JVC, Matsushita, Mitsubishi, Pioneer, Sony and Toshiba) and the European (Philips and Thomson) collaborated to facilitate development of the DVD Forum. In March 1997, the US launch of the DVD systems went smoothly due to Hollywood's solidarity. Manufacturers and film studios decided to avoid making the same mistake of the VHS format battles and agreed upon a universal standard of cooperation. When first introduced in 1997, DVDs sold at the low price of $20, for which they offered high-quality image and extra special features. Consumers liked the advantages of the DVDs and soon surpassed VHS sales.

The first premium rate television services were Phonevision, Telemeter, SubscriberVision, among others. None of them were successful, until the launch of Home Box Office (HBO) in 1972, considered the first successful premium-rate subscription television service. Other services were launched: Z Channel, Showtime, The Movie Channel, Cinemax, Spotlight and Home Theater Network. Only Showtime, The Movie Channel and Cinemax survived through the 1980s. These premium rate services air features unedited, uncut, and commercial-free, the same way they were shown on theaters and/or home video.

Out of the many ancillary markets out there, none were more effective and revenue rewarding than network television, and eventually syndication began. Amanda D. Lotz spoke on the radical change in her book titled "Television Will Be Revolutionized (2008)."
She mentions how time went on and as the post-network era developed, that the limited ways there were for medium to be distributed was eliminated. What was expanding, and evolving was network television as a whole.<Lotz 85> At first channels were few, and very limited. But soon came the multi-channel transition, then on with the creation of cable TV. As that later expended cable began to see the 'cash cow' in that and began selling shares to different networks in order to air their studio programs. Not only did Network Television open up ancillary markets for TV, but other markets as well. The VCR then became a hot commodity, because the consumer wanted the option of recording. That slowly transitioned to the DVD that took out the VCR, and finally DVR. Which seems to be just that, a VCR and a DVD combined. Network television took a gigantic step when it later allowed programs to have different showing dates, and even multiple air times. This offered different networks to still be able to ear revenue off of an older film that already has left the box office.

According to McDonald and Wasko, Hollywood's interest in emerging medium of television dates from the 1920s and includes experiments with the developing technology, an alternative model of television as home theater, applications for television frequencies, as well as investments in broadcasting companies that were exploring television (Anderson, 1994; Hilmes, 1990; Wasko, 1994). The evidence suggests that studios clearly wanted to control the development and implementation of television technology. Securing that control required the assistance of the Federal Communication Commission, which regulated television's development by articulating technical standards and operating rules and through the FCC's exclusive right to license the use of television frequencies for experimentation or broadcast (McDonald and Wasko, p. 107).

Video games are a rapidly growing ancillary market for feature films. Video game over-all gross income has consistently surpassed that of movie sales since 2004. Because of the increased interest in video games many major media and communication companies have begun to show interest in video games as a way to market, brand, and advertise for their product. Some movies that have benefited from this are the "Harry Potter", "Lord of the Rings", and "James Bond" series. Because the video game industry has been able to outplay and outgross many movies, video game manufacturers are beginning to look in the direction of producing their own movies as well. Microsoft's producers of "Halo" have decided to pass Hollywood studios and produce their own movie.

Hollywood has shown interest in the video game industry as an ancillary market almost as far as video games have existed. The film industry's goal within the video game industry is to either seek control and therefore ownership of the video game market or to license products to video game producers. Once the video game profits are high, that is when Hollywood seeks control. On the other hand, when profits are down, that is when Hollywood offers the licensing. The collaboration had always been problematic since the film industry had been uncertain about the development of video games. Hollywood has seen video games as another promotional scheme for a film.

Hollywood has had interactions with the recorded music industry that dates back as far as the studio system itself. The interactions between the two include three main periods; 1927-1957: recorded music as a form of promotion, 1957-1977: recorded music as cross-promotion and ancillary revenue, and 1977-1997: recorded music as cross-promotion, ancillary revenue stream, and means of spreading risk.
In addition to music's promotional value, studios realized that, "A chart success was an effective way of generating additional revenue for their companies, both in terms of publishing and performance royalties and, of course, in outright sales." Recorded music proved to be of great value as an ancillary market to the film industry.



</doc>
<doc id="31613084" url="https://en.wikipedia.org/wiki?curid=31613084" title="Script market">
Script market

A script market is the system in which a screenwriter and producer engage in the buying and selling of a script for the film and television industries. The process of selling a script may begin with the pitch, however since the end of the 1980s the ability to pitch a film to producers has greatly depended on the notoriety of the screenwriter. One reason attributed to this effect is that studios are looking for the next big hit, but scared to take a chance on a script that doesn’t meet a pre-established formula guaranteed to make money since no one knows what will work. The majority of scripts are read by studio interns and others, who give the scripts a “consider”, “pass”, or “recommend” status, with most scripts receiving a “pass” rating. However, an agent who's signed the Artists-Managers Agreement drawn up by the Writers Guild of America can submit scripts to producers directly. Agents try to create buzz in the script market using spec script. With everyone in the entertainment industry trying to pursue the million-dollar dream, and Hollywood so desperate for new material ideas, the script market functions and business practices have been pursued in the spec script manner.

Studio executives, producers, and agents don't have time to read every script, so readers or script analysts prepare script coverage for them. Spec scripts are written in hopes of being purchased by a producer or studio. A spec script can be passed around by an agent, which can create a bidding war. The spec script process is considered by some to be problematic, because the bidding process can attract inflated prices from the boosting of mediocre scripts.


</doc>
<doc id="490309" url="https://en.wikipedia.org/wiki?curid=490309" title="Peep show">
Peep show

A peep show or peepshow is an exhibition of pictures, objects or people viewed through a small hole or magnifying glass. Though historically a peep show was a form of entertainment provided by wandering showmen, nowadays it more commonly refers to a presentation of a sex show or pornographic film which is viewed through a viewing slot. Historically the peep hole was intended to control the point of view to allow an illusion of depth perception, while showmen would later use it to charge for access to the view. For sex shows it also became a means to view material that would be objected to if freely displayed.

Peep shows, also known as peep box or raree show ("rarity show") can be traced back to the early modern period (15th century in Europe) and are known in various cultures. 
Around 1437 Italian humanist author, artist, architect, poet, priest, linguist, philosopher and cryptographer Leon Battista Alberti is thought to have created the earliest impressive peep show boxes with painted pictures to be viewed through a small hole. He had two kinds: night scenes with the moon and stars, and day scenes. It is thought these pictures may have been transparent and lit from behind, possibly changing from day to night by changing the lighting. It has also been suggested that it may have been a predecessor of the magic lantern that could project images. 

In the 17th and 18th century peep shows were exhibited on streets and fairs across Europe by itinerant showmen, competing with other entertainment like dancing bears, jugglers, conjurers, et cetera. Their wooden cabinets could have several viewing holes and contain sets of pictures to be set into a viewing position by pulling a corresponding string. The show was accompanied by spoken recitation that explained or dramatized what was happening inside. The boxes were often decorated inside to resemble theatrical scenes. 

Peep shows were most popular in the 17th century in Holland. Some artists from the 17th-century Dutch Golden Age painting, like Pieter Janssens Elinga and Samuel Dirksz van Hoogstraten created a type of peep shows with an illusion of depth perception by manipulating the perspective of the view seen inside, usually the interior of a room.
From around 1700 many of these "perspective boxes" or "optica" had a bi-convex lens with a large diameter and small dioptre for an exaggerated perspective, giving a stronger illusion of depth. Most pictures showed architectural and topographical subjects with linear perspectives. From around 1745 similar perspective view prints became very popular for the zograscope, which used the same principle with the lens on a stand rather than in a box. 
Peep shows were further developed with translucent painting techniques, perforations and cut-out shapes that provided special effects when lit from behind by candles. Changing the light from the front to the back of the picture could change the scene from day to night, much like the dissolving views that would later become a popular type of magic lantern show.

In the early 18th century perspective boxes were very appreciated in Japan, where they were referred to as "Holland machines". The Dutch brought the first such device to Japan in the 1640s as a gift to the "shōgun", but the devices became popular only after the Chinese popularized them about 1758, after which they began to influence Japanese artists.

19th-century Chinese peep shows were known by many names including "la yang p'ien" (). Sometimes the showman would perform for a crowd with puppets or pictures outside the box and then charge people extra to look through the holes.
In Ottoman Syria a form of peep show called "sanduk al-ajayib" ("wonder box") existed, which the storyteller carried on his back. The box had six holes through which people could see scenes backlit by a central candle. "Sanduk al-ajayib" stories were about contemporary figures and events, or showed scenes of heaven and hell. 

Other common subjects in peep shows throughout the world have been exotic views and animals, scenes of classical drama or masques, court ceremonies, surprise transformations (e.g., of an angel into a devil) and of course, lewd pictures.
Raree shows were precursors of toy theatres, with movable scenes and paper figurines, popular in the 19th century. They can also be seen as predecessors to optical toys like Chinese fireworks, the diorama, the stereoscope and the magic lantern.

Some peep shows offer the only accurate representation of the stage design and scenery of the masques and pageants of their time.

Peep shows have been used for erotic and pornographic pictures, such as "What the Butler Saw", since before the turn of the twentieth century. 

In contemporary use, a peep show is a piecewise presentation of pornographic films or a live sex show which is viewed through a viewing slot, which shuts after the time paid for has expired. The viewing slots can be operated by a money box device, or paid for at a counter. 

Pornographic peep shows became popular in the 1970s as part of the developing pornography industry. Until home video became widespread, peep shows made up a major part of the way in which video pornography was accessed. In 1986 a US Presidential report into pornography said that peep shows were making significant earnings which were often undeclared or untaxed, and in some US locations peep shows were subsequently suppressed.

For live peep shows, booths can surround a stage upon which usually a female performer performs a striptease and sexually explicit poses. In Barcelona female performers at times also perform sexual intercourse with male performers on stage. In some cases, booths include paper towel dispensers, for customers who engage in masturbation. A customer and performer can mutually agree on a fee for a "private dance", which can take place in a peep show booth with a clear window and seating space for only one spectator.

Research on peep show establishments in California examined the hypothesis that neighborhoods surrounding sex businesses such as peep show establishments and X-rated movie stores have higher rates of crime. The researchers compared 911 calls in peep show and control neighborhoods in San Diego. Although peep show neighborhoods had approximately 16 percent more calls, the researchers concluded that the difference was not statistically significant. Other researchers reanalyzed the data and concluded that the difference was significant.

Regal Show World was an adult entertainment business on lower Market Street in San Francisco, California. The company's slogan was "Where you are king". The business had a peep show and an adult video arcade. The peep show had performers working in an enclosed round room with viewing booths surrounding it and sometimes had a "double in the bubble" show in which two performers worked simultaneously. During the winter of 1997 to 1998, the business had thirty-five performers. At this time, over 80% of performers there attempted to unionize and "signed union authorization cards for representation by the Service Employees International Union (SEIU), Local 790".

The business was owned by Bijou Group, Inc., a privately held company in San Francisco that was founded in 1990. Bijou Group owned similar businesses in San Francisco such as New Century Theater, Market Street Cinema, and the Campus Theater. Bijou Group, Inc. filed for Chapter 11 bankruptcy reorganization circa 1994. In November 1998, management of Regal Show World announced that the peep show would be closed on November 30th of that month due to "economic reasons". At the time, some performers in the industry stated that closure of the peep show was done as a retaliative measure against attempts for performers to unionize. The company declared bankruptcy after performers made a second attempt at unionization, whereby the performers "signed cards calling for a union election", and the theater was closed.

In Las Vegas in the early 1990s, city authorities began to move peep shows and other sexually-oriented businesses away from the city centre. The last peep show in Las Vegas closed in 2019.

The former Lusty Lady peep show in San Francisco, California, entered the news in 1997, when it became the first U.S. sex business to be unionized. In 2003 it was bought by the employees and became a worker cooperative.



Regal Show World



</doc>
<doc id="35694818" url="https://en.wikipedia.org/wiki?curid=35694818" title="Convenience technologies">
Convenience technologies

Convenience Technologies enable viewers and users of television, Internet, mobile devices, Digital Video Recorders (DVR), Video on Demand (VOD) and Digital Versatile Disc (DVD) to more easily seek out specific content and view it in individualized patterns. These technologies increase viewers’ ability to choose when they want to watch a program with the use of DVR, VOD and DVD, and where to watch a program with the use of DVD, iPOD, TiVo ToGo and mobile phones. These technological enhancers provide the most comprehensive and varied adjustments in the technological potential of the medium (Amanda D. Lotz, 2007, p. 59).

Convenience Technologies encourage active selection instead of generally watching what “comes on next” or “is on”. Because of this, consequently, viewers focused more on programs they wanted to watch than on the networks that supplied them (Lotz, 2007, p. 59)

The main problem for networks is that the DVR using audience appears, by basically every measure that’s vital to advertisers, more wanted than the non-DVR crowds. According to a Horizon Media Study, early adopters of technology are usually above the national average in income, in a “well-off” set. They are typically college graduates and white-collar workers (Lowry, 2010).


</doc>
<doc id="35970102" url="https://en.wikipedia.org/wiki?curid=35970102" title="Caption contest">
Caption contest

A caption contest or caption competition is a competition between multiple participants, who are required to give the best description for a certain image offered by the contest organizer. Rules and information about the competition process are also given by the competition organizer.

Caption contest participants have to describe a given image as accurately and originally as possible. Contests may vary by image type (photo or drawing), caption format (description of a situation or a dialogue), as well as judging criteria. The competition organizer is required to give information about caption submission rules and other information that is important for the contest.

Caption contests are mainly arranged as an amusement, and they can be organized under different conditions (between separate individuals, in public events, mass media, etc.), as well as using various means (drawn pictures on paper, digital image, printed image, press publication, etc.). Traditionally, the most popular and typical format has been publication of caption contests in newspapers or magazines, but with the development of modern technologies, it is also very popular to organize caption contests on the Internet.

Caption contests can be used not only as an entertainment but also for other purposes, such as newspaper promotion, bringing forward a certain problem, or product advertisement. An important aspect of caption competition is to look at an object, event or issue from unusual point of view.

The caption, unless competition rules do not restrict, can be in any form of literary expression - prose, poetry, as one word or a word play. Usually the submitted captions are judged by the originality and wit, but not always the decisive assessment is humour. It is generally assumed that the caption has to describe image as precisely as possible, so both humour and precision are equally important.

As caption writing is an art of words, it is rarely possible to evaluate the superiority of one caption over others accurately and objectively. Consequently, the judging of submitted captions is usually quite subjective. Sometimes competition rules limit caption size, but usually the caption is no longer than one sentence (10 - 20 words). As an exception, the caption can consist of several sentences. Usually it is not allowed to use profane or offensive language in submitted captions.

In several caption contests, winners are awarded with prizes.

It is considered that the origins of caption contests are related to the increase of popularity of cartoons and comics as well as widespread use of illustrations and the rise of photojournalism in printed media at the end of the 19th century. All these areas share the combination of words and images that could be multiplied and made available to a wide audience with the beginnings of mass printing and wide spread of newspapers and magazines.

From currently available information, the first caption contests in printed media appeared in the U.S. press of the late 19th century (such as "Life" magazine which was published from year 1883 to 1936). At that time these competitions were called "picture title" or "picture headline" contests, but their concept and meaning was identical to current caption contests. It must be considered, that the term "caption" in its present sense firstly appeared in English language only in the first half of the 20th century.

In the beginning of the 20th century, at least in the U.S. press (for example, "The San Francisco Call", "The Daily News", "The Bridgeport Telegram") caption contests were already quite popular. In these competitions drawn pictures and cartoons were mainly used. Photo caption contests appeared little later with the improvement of printing quality (towards the second quarter of 20th century).

During the course of the 20th century, caption contests were published in printed media in many countries of the world. For example, in Soviet Union one of the first caption contests was published in daily newspaper "Izvestia" Sunday supplement "Nedelya" ("The Week") at the beginning of the 1960s. Little later a newspaper "Literaturnaya gazeta" began to publish caption contests that became very popular and they are part of this newspaper up until now.

The spread of caption contests increased even more with the emergence of the Internet by the end of the 20th century, and at the beginning of the 21st century. Internet has made the organization of caption contests and participation in them possible for wide audiences across the world.

Although a considerable amount of caption contests are now on Internet, caption contests in printed media still exist and are quite popular. A very popular and prominent is a weekly caption contest published in American magazine "The New Yorker". The contest first appeared in 1998 and has been published regularly in each issue since 2005. Each week several thousand participants take part in this competition. On the Internet, a very popular is the weekly caption competition on "BBC" website ("Magazine Monitor" section), held since 2006.

Along the traditional caption contests, there have appeared different forms of this genre such as anti-caption contests for worst submitted caption and reverse caption contests where the most suitable picture must be submitted for a given caption. There are also caption contests devoted to particular themes such as history or football.

In 2006, the website caption.me (then captioncompetition.co.uk) was founded, featuring amusing photos loaded automatically from photo-sharing site Flickr. Visitors submit captions and vote for others. Caption.Me features a real-time collaborative mind-mapping feature called Maption

In 2009, there was released an online multiplayer caption writing game called "Caption That" where players write captions for a given image and then vote for their favourite from the list others have written.

In 2014, the iPhone app Captionit was released on the Apple App Store. Captionit is a free mobile app that allows people to play and create caption contests on their iPhone, iPad, and iPod Touch.

In 2016, mobile phone application TinnBin was released for both iPhone and Android. TinnBin users create contests called "Bins" by uploading a picture from their device and selecting how long they would like the contest to run. While the bin is open, users can add and vote on comments, called "Tinns". Once the bin is closed, the winning tinn is appended to the image in the form of a caption. TinnBin is different from most other caption contests as the winner is chosen strictly by users of the application, and not the contest creators, and all the contests are created by users of the application.

"The New Yorker" magazine, noting the popularity of its cartoon caption contest, has created a caption contest board game and issued a book with the most interesting cartoons and winning captions as well as comprehensive information covering this theme.

Also concerning the "New Yorker" caption contest, several scientific studies have been carried out: researchers from University of New Mexico, Department of Anthropology, in 2010 have compared humour ability in males and females in context of this competition; members of "The New Yorker" editorial board and University of Colorado Boulder Leeds School of Business in 2011 analysed several thousand entries of this contest to study the concept of humour.

The future of caption contests depends on the imagination and creativity of their organizers and participants, as well as future development of information and communication technologies.

In U.S. TV series "The Office", seventh-season episode "The Search", its character Pam starts a caption contest in the office, with her co-worker Gabe instituting rules that no one wants to follow.

In British sitcom "Green Wing", the second series, episode six, its character Alan Statham wins a caption competition on the "Consultant Radiologists International" website, an organisation of which Alan claims to be an "Esteemed member".

In a children's novel "Absolute Zero" by Helen Cresswell its character Uncle Parker has won a trip to the Caribbean in a caption contest. The rest of the family immediately enter similar competitions in an attempt to better his prize but, much of the time, beating the others to an entry form is a victory in itself.

In season two of the HBO series "Bored to Death", George (played by Ted Danson) struggles to caption a New Yorker cartoon, which portrays a police duck interacting with a suicidal bear.




</doc>
<doc id="39184350" url="https://en.wikipedia.org/wiki?curid=39184350" title="Military-entertainment complex">
Military-entertainment complex

The military-entertainment complex is the cooperation between the military and entertainment industries to their mutual benefit, especially in such fields as cinema, multimedia and virtual reality.

In Hollywood, many movie productions were directly supervised by the Department of Defense. Since 1989, the chief liaison between Hollywood and the DoD is Phil Strub. Directors looking to borrow Army material for their movies need to apply to the DoD, and submit their movies' scripts for vetting. Ultimately, the DoD has a say in virtually every US-made movie that use military resources in their production.

The movie "Top Gun", produced by Jerry Bruckheimer and in collaboration with the Pentagon, aimed at rebranding the US Navy's image post-Vietnam war, and attract new Navy recruits. "Top Gun" was the first full-blown collaboration between Hollywood and the US Navy. By the end of the 1980s and early 1990s, Hollywood producers were stressing script writers to create military-related plots to gain production power from the US Military.

Some US movies co-scripted with the DoD include:

The website "Spy Culture" compiled a list of 410 DoD-sponsored movies.

In 2011, Washington Post journalist David Sirota questioned if that strategy was not unconstitutional, since the DoD directly influences the outcome of movie scripts (abridging freedom of speech) and uses public material (the Army's gear paid by the tax-payers) to grow its influence in the movie industry. Cybersecurity book-author Daniel Miessler asserted that the Army is not legally authorized to recruit young people on education premises (schools, college campus, ...) because of the students' tender age and psychological vulnerability, thus underlining the Army's unethical stand when it promotes itself all over the movie industry.

Katy Perry's 2012 video clip "Part of Me", in which she signs up to join the Army, was shot at USMC Camp Pendleton in Oceanside, California, with the support of the Army.

On YouTube, a new music video genre appeared, the military music videos. Typically, these are video clips portraying singers in military gears and surrounded by military vehicles and weapons. This video genre is used by the Army across the globe (list of examples below)


The United States Air Force has an official rock band, "Max Impact", and released a punk version of its official anthem. In early 2019, the US Army released a promotional military hip hop video, "Giving all I got", with the explicit intent to get the attention of the younger crowd.

In February 2019, the armies of China and Taiwan made dueling propaganda videos, creating a music video battle. The Chinese video ("My War Eagles Are Flying Around The Treasured Island") showed Chinese jets flying over Taiwan, and Taiwan responded by showing muscles with the video clip "Freedom Isn't Free" glorifying the strength of the country's army.

In his book "From Sun Tzu to Xbox", Ed Halter wrote «The technologies that shape our culture have always been pushed forward by war». Video games «were not created directly for military purposes, [they] arose out of an intellectual environment whose existence was entirely predicated on defense research». The first known virtual military training equipment, a flight simulator made of wood, was created in the 1920s by Edward Link. Since the Second World War, the US Army and its sub-agencies played a major role in the development of digital computers. The DARPA, an agency of the DoD, contributed to the development of Advanced computing systems, computer graphics, the Internet, multiplayer networked systems, and the 3-D navigation of virtual environments.

Arguably the first video game (faux-military simulation), the PDP-1-powered "Spacewar!", was developed in 1962. The US Army's first video game created for training purposes, the board game Mech War, was implemented in the staff officer training curriculum in the 1970s at the Army War College. During the 1980s, Academic and military researchers led the development of distributed interactive simulations (DIS) that enable the creation of real-time, virtual theaters of war. The release by Atari of the game "Battlezone" was a revolution for the graphics perspective, introducing first-person shooter games for the first time. Donn A. Starry, head of the United States Army Training and Doctrine Command (TRADOC), said in a conference in 1981 : «[Today's soldiers have] learned to learn in a different world, [...] a world of television, electronic toys and games, computers, and a host of other electronic devices. They belong to a TV and technology generation... [so] how is it that our soldiers are still sitting in classrooms, still listening to lectures, still depending on books and other paper reading materials, when possibly new and better methods have been available for many years?» The Air Force captain Jack A. Thorpe developed SIMNET with DARPA, a real-time distributed networking to modernize virtual simulation capacities and enable soldiers to experience war situation in times of peace. The magazine "Wired" argued this was the real embryo of the Internet.

After the first-shooter hit "Doom" came out in 1993, the Marine Corps Modeling and Simulation Office (MCMSO) released the online Personal Computer Based Wargames Catalog where Army personnel published detailed reviews of the video games they investigated. "Doom" became the MCMSO's absolute preference, and in 1995, the game "Marine Doom" was released, and the alien-themed graphics of the game's first version was replaced by military-themed graphics.

Dave Anthony, a writer for "Call of Duty" left his job and became an "unknown conflict" adviser for the Department of Defense.

The video game "Homefront" was created by John Milius, who also wrote/directed the 1984 war film "Red Dawn" that gave its name to the Operation Red Dawn which led to the capture of Saddam Hussein.





</doc>
<doc id="39455287" url="https://en.wikipedia.org/wiki?curid=39455287" title="Naked butler">
Naked butler

A naked butler is a popular type of entertainment used at hen parties and for corporate entertainment. It was introduced into the UK in the early 2000s and is now popular across the USA, New Zealand, Australia and Canada.

The standard attire for a naked butler is only an apron or trunks, cuffs, detachable collar and bow tie. It is seen as a more conservative entertainment option to hiring a male stripper for a Bachelorette party as it can be suitable for a broader range of guests.

In this case the apron will be replaced with smart black trousers. This option may be more commonly used during corporate entertainment where the attire needs to be smarter and the exposing of a butlers naked bottom is not appropriate.

Naked butlers have become a popular gimmick on television shows in the UK and this is demonstrated by appearances on TV shows including "Deal or No Deal", "Come Dine with Me" and "the X Factor". In February 2005, the role and job of a naked butler was discussed in the documentary "Going to work Naked" on ITV. In 2018, naked butlers were part of an international news feature after being hired by a retirement home as part of the resident's wish program. 


</doc>
<doc id="3447151" url="https://en.wikipedia.org/wiki?curid=3447151" title="Scientific demonstration">
Scientific demonstration

A scientific demonstration is a procedure carried out for the purposes of demonstrating scientific principles, rather than for hypothesis testing or knowledge gathering (although they may originally have been carried out for these purposes).

Most scientific demonstrations are simple laboratory demonstrations intended to demonstrate physical principles, often in a surprising or entertaining way. They are carried out in schools and universities, and sometimes in public demonstrations in popular science lectures and TV programs aimed at the general public. Many scientific demonstrations are chosen for their combination of educational merit and entertainment value, which is often provided by dramatic phenomena such as explosions. 

Public scientific demonstrations were a common occurrence in the Age of Enlightenment, and have long been a feature of the British Royal Institution Christmas Lectures, which date back to 1825. In the television era, scientific demonstrations have featured in science-related entertainment shows such as "MythBusters" and "".

Some famous scientific demonstrations include:


Note: many scientific demonstrations are potentially dangerous, and should not be attempted without considerable laboratory experience and appropriate safety precautions. Many older well-known scientific demonstrations, once mainstays of science education, are now effectively impossible to demonstrate to an audience without breaking health and safety laws. Some older demonstrations, such as allowing the audience to play with liquid mercury, are sufficiently dangerous that they should not be attempted by anyone under any circumstances.



</doc>
<doc id="2042178" url="https://en.wikipedia.org/wiki?curid=2042178" title="Ball pit">
Ball pit

A ball pit (originally called "ball crawl" and also known as a ball pool or ball pond) is a padded box or pool filled with small colorful hollow plastic balls generally no larger than in diameter. Instead of balls, other spherical objects can be used, such as balloons. It is typically employed as a recreation and exercise for children.
Ball pits are often found at nurseries, carnivals, amusement parks, fun centers, fast-food restaurants, and large video arcades. Chuck E. Cheese's and (now defunct) Discovery Zone formerly had ball pits and they were frequently incorporated into larger play structures, such as mazes, slides, and jungle gyms.

Ball pits may be rented for parties, and smaller versions are sold for use in the home. While ball pits are traditionally intended for children, some are large enough to accommodate adults.

Ball pits may be used together with a trampoline, or combining the two by filling a closed trampoline with the balls.

In 1976 Eric McMillan created the first ball pit at SeaWorld Captain Kids World in San Diego as a result of his experience at Ontario Place.

Beginning in the late 1990s, a number of urban legends arose about children being severely injured or even killed due to ball-pit encounters with venomous snakes or hypodermic needles. Although there is no truth to these stories, workers have reported finding dirty diapers, half-eaten food and syringes in ball pits.

In China Miéville's short story "The Ball Room" (Looking for Jake)"," the ghost of a child who died in a ball pit haunts a local Ikea-like store.

In the "Rugrats" episode "Piggy's Pizza Palace", the Rugrats jump on a costumed pig named Piggy as an act of revenge to get Angelica's tickets back. It causes the ball pit structure to split right open and all of the balls fall out all over the restaurant.

In season 3 episode 14 ("The Einstein Approximation") of the TV series "The Big Bang Theory", Sheldon seeks inspiration in a ball pit at a mall, and then hides from Leonard, who spends a good amount of time and effort trying to retrieve Sheldon from the ball pit.

In 2014, a YouTube vlogger under the name Roman Atwood made a video of transforming the living room of his home into a massive ball pit, intended as a prank for his girlfriend who has returned from a trip. He later collaborates with another vlogger, Freddie Wong, to create a comedy video involving giant ball pit and "ball monster" prank.

In 2016, a pop-up "ball pit bar" opened in San Francisco.



</doc>
<doc id="2518059" url="https://en.wikipedia.org/wiki?curid=2518059" title="Season ticket">
Season ticket

A season ticket, or season pass, is a ticket that grants privileges over a defined period of time.

The "Oxford English Dictionary" has illustrative quotations which show the term "season ticket" used in the United States in 1820 for theatre tickets; and in the United Kingdom in 1836 for boat travel and 1862 for rail transport.

In sports, such as association football or American football, a season ticket grants the holder access to all regular-season home games for one season without additional charges. The ticket usually offers a discounted price over purchasing a ticket for each of the home games for a season individually. In some sports, season ticket holders are usually allowed to buy tickets for other home games (such the playoffs) earlier than other fans, and may be given priority when buying tickets for their team's allocation at a road game. Seats assigned to season tickets are generally the better ones in their seating section. Season ticket holders are frequently offered preferred seating at special events or extra games.

Season passes are commonly used for winter sports, but were originally in American ski resorts a privilege for club members and investors. However, in 2000 a discounting movement across the US triggered a price war and increased the widespread use of season passes.

A season ticket is also an option for many music venues (including Opera, Ballet, Symphony houses) and repertory theatre companies. The season subscription usually offers a discounted price over purchasing a ticket for each concert or play in a series or all concerts or plays in a season.

In public transport, a season ticket allows the user to travel by public transport an unlimited number of times within a period of time. The term "commuter pass" is used in some countries. Season tickets are typically sold for a week, month or year. The validity of season tickets varies. At one extreme it may only allow travel between two points (A to B) by only one operator and one route (if there are more than one competing). At the other extreme, it may allow unlimited travel within a geographic area, or even a whole country, allowing free choice of method of transportation (bus, tram, train, etc.) and free choice of operating company.

In television, a season ticket refers to the term invented by the manufacturers of the TiVo digital video recorder for the function which allows a user to program the device to record all the episodes of a season of a television show, even if their airings are rescheduled or pre-empted. Similar functions are available on competitive digital video recorder systems and software packages, such as ReplayTV and IceTV. With the advent of digital streaming services, such as Netflix and digital video stores like iTunes and Google Play Store, one can also purchase a season pass to gain the ability to watch every episode of a chosen season at one's leisure on a computer or mobile device.

Other examples of venues that often offer season passes include amusement parks, recreational sports venues (ski areas), and paid-admission parks (state and national parks). Some passes may also grant additional perks, such as free or prepaid parking, or coupons exclusive to pass-holders.



</doc>
<doc id="9955923" url="https://en.wikipedia.org/wiki?curid=9955923" title="Corporate entertainment">
Corporate entertainment

Corporate entertainment describes private events held by corporations or businesses for their staff, clients or stakeholders. These events can be for large audiences such as conventions and conferences, or smaller events such as retreats, holiday parties or even private concerts.

It is also commonly used to mean corporate hospitality, the process of entertaining guests at corporate events.

The companies that provides corporate entertainment are called corporate event planners or corporate booking agencies.

There are various types of corporate events that make use of entertainment. An opening general session may include entertainment that adds excitement and presents the overall theme of the meeting. Mixers or pre-dinner parties many times use entertainment meant to provide a backdrop for conversation, perhaps an acoustic ensemble or pre-recorded music. Awards or gala events, usually the last event in a series of meetings, can make use of many options, from celebrity entertainers to exciting bands providing dance music or other options that will leave the attendees with a feeling of excitement and looking forward to the next meeting. There are many different types of corporate entertainment.

Corporate entertainment can also include a day of team building activities. These activities include traditional camp activities like tug of war, scavenger hunts, and relay races. They could also include sports such as volleyball, soccer, or basketball. The goal of team building corporate entertainment is to have employees recognize how the challenges of the activities relate to the workplace. Team chemistry, identifying strengths and attributes, understanding how to work through solving problems as one, and reflecting makes for fruitful team building.

Awards or gala events are usually lavish events that celebrate accomplishment or milestones of a person or group of people in similar industries. Often these events serve as fundraisers for a specific cause. In addition to celebrating and recognizing achievements, it allows attendees to network with others with similar backgrounds or professions.

Holiday celebration events are ways for companies or departments to celebrate holidays and to show appreciation to employees. Entertainment at these events vary from raffles and door prizes, mystery dinners, music and an overall casual, social setting that can build social relationships. For Christmas celebrations, some companies have used the "A Christmas Story" theme.

Corporate seminars, workshops, symposiums, and conferences are more informative in nature and often focussed on educational purposes. A conference refers to a formal meeting where participants exchange their views on various topics. A seminar is a form of academic instruction, either at a university or offered by a commercial or professional organization. A workshop includes all the elements of the seminar, but with the largest portion emphasizing “hands-on-practice” or laboratory work. A symposium is a formal gathering in an academic setting where participants are experts in their fields. Entertainment for these events varies from kick-op brunches to start, special industry guest speakers, and mixers, dinners afterwards. There are also booths set up for trade shows to display a companies strengths and for better marketing.

Corporate charity events, whether concerts, golf tournaments, or anything else, play an important role in how businesses interact with the community. Corporate charity events unite people from all levels of the organization; such events are another form of team building which positively influence other aspects of work.


</doc>
<doc id="1151595" url="https://en.wikipedia.org/wiki?curid=1151595" title="Escapism">
Escapism

Escapism is mental diversion from unpleasant or boring aspects of daily life, typically through activities involving imagination or entertainment. Escapism may be used to occupy one's self away from persistent feelings of depression or general sadness.

Entire industries have sprung up to foster a growing tendency of people to remove themselves from the rigors of daily life – especially into the digital world. Many activities that are normal parts of a healthy existence (e.g., eating, sleeping, exercise, sexual activity) can also become avenues of escapism when taken to extremes or out of proper context; and as a result the word "escapism" often carries a negative connotation, suggesting that escapists are unhappy, with an inability or unwillingness to connect meaningfully with the world and to take necessary action. Indeed, the "Oxford English Dictionary" defined escapism as "The tendency to seek, or the practice of seeking, distraction from what normally has to be endured".

However, many challenge the idea that escapism is fundamentally and exclusively negative. C. S. Lewis was fond of humorously remarking that the usual enemies of escape were jailers; and considered that used in moderation escapism could serve both to refresh and to expand the imaginative powers. Similarly J. R. R. Tolkien argued for escapism in fantasy literature as the creative expression of reality within a secondary (imaginative) world, (but also emphasised that they required an element of horror in them, if they were not to be 'mere escapism'). Terry Pratchett considered that the twentieth century had seen the development over time of a more positive view of escapist literature. Apart from literature, music and video games have been seen and valued as an artistic mediums of escape, too.

Freud considers a quota of escapist fantasy a necessary element in the life of humans: "[T]hey cannot subsist on the scanty satisfaction they can extort from reality. 'We simply cannot do without auxiliary constructions', Theodor Fontane once said". His followers saw rest and wish fulfilment (in small measures) as useful tools in adjusting to traumatic upset; while later psychologists have highlighted the role of vicarious distractions in shifting unwanted moods, especially anger and sadness.

However, if permanent residence is taken up in some such psychic retreats, the results will often be negative and even pathological.Drugs cause some forms of escapism which can occur when certain mind-altering drugs are taken which make the participant forget the reality of where they are or what they are meant to be doing.

Some social critics warn of attempts by the powers that control society to provide means of escapism instead of bettering the condition of the people – what Juvenal called “bread and the games”. Escapist societies appear often in literature. "The Time Machine" depicts the Eloi, a lackadaisical, insouciant race of the future, and the horror of their happy lifestyle beliefs. The novel subtly criticizes capitalism, or at least classism, as a means of escape. Escapist societies are common in dystopian novels; for example, in the "Fahrenheit 451" society, television and "seashell radios" are used to escape a life with strict regulations and the threat of a forthcoming war. In science fiction media escapism is often depicted as an extension of social evolution, as society becomes detached from physical reality and processing into a virtual one, examples include the virtual world of Oz in the 2009 Japanese animated science fiction anime "Summer Wars" and the game "Society" in the 2009 American science fiction film "Gamer", a play on the real-life MMO game "Second Life". Other escapist societies in literature include "The Reality Bug" by D. J. McHale, where an entire civilization leaves their world in ruin while they 'jump' into their perfect realities. The aim of the anti-hero becomes a quest to make their realities seemingly less perfect to regain control over their dying planet.

Social philosopher Ernst Bloch wrote that utopias and images of fulfillment, however regressive they might be, also included an impetus for a radical social change. According to Bloch, social justice could not be realized without seeing things fundamentally differently. Something that is mere "daydreaming" or "escapism" from the viewpoint of a technological-rational society might be a seed for a new and more humane social order, as it can be seen as an "immature, but honest substitute for revolution".

The Norwegian psychologist Frode Stenseng has presented a dualistic model of escapism in relation to different types of activity engagements. He discusses the paradox that the flow state (Csikszentmihalyi) resembles psychological states obtainable through actions such as drug abuse, sexual masochism, and suicide ideation (Baumeister). Accordingly, he deduces that the state of escape can have both positive and negative meanings and outcomes. Stenseng argues that there exist two forms of escapism with different affective outcomes dependent on the motivational focus that lies behind the immersion in the activity. Escapism in the form of self-suppression stems from motives to run away from unpleasant thoughts, self-perceptions, and emotions, whereas self-expansion stems from motives to gain positive experiences through the activity and to discover new aspects of self. Stenseng has developed the "escape scale" to measure self-suppression and self-expansion in people´s favorite activities, such as sports, arts, and gaming. Empirical investigations of the model have shown that:

Alan Brinkley, author of "Culture and Politics in the Great Depression", presents how escapism became the new trend for dealing with the hardships created by the stock market crash in 1929: magazines, radio and movies, all were aimed to help people mentally escape from the mass poverty and economic downturn. "Life" magazine, which became hugely popular during the 1930s, was said to have pictures that give "no indication that there was such a thing as depression; most of the pictures are of bathing beauties and ship launchings and building projects and sports heroes – of almost anything but poverty and unemployment”. Famous director Preston Sturges aimed to validate this notion by creating a film called "Sullivan's Travels". The film ends with a group of poor destitute men in jail watching a comedic Mickey Mouse cartoon that ultimately lifts their spirits. Sturges aims to point out how "foolish and vain and self-indulgent" it would be to make a film about suffering. Therefore, movies of the time more often than not focused on comedic plot lines that distanced people emotionally from the horrors that were occurring all around them. These films "consciously, deliberately set out to divert people from their problems", but it also diverted them from the problems of those around them.




</doc>
<doc id="2164767" url="https://en.wikipedia.org/wiki?curid=2164767" title="Mathemagician">
Mathemagician

A mathemagician is a mathematician who is also a magician.

The name "mathemagician" was probably first applied to Martin Gardner, but has since been used to describe many mathematician/magicians, including Arthur T. Benjamin, Persi Diaconis, and Colm Mulcahy. Diaconis has suggested that the reason so many mathematicians are magicians is that "inventing a magic trick and inventing a theorem are very similar activities."

Mathemagician is a neologism, specifically a portmanteau, that combines mathematician and magician to suggest that mathematics is a kind of magic. A great number of self-working mentalism tricks rely on mathematical principles. Max Maven often utilizes this type of magic in his performance.

The Mathemagician is name of a character in the 1961 children's book "The Phantom Tollbooth". He is the ruler Digitopolis, the kingdom of mathematics.




</doc>
<doc id="2247943" url="https://en.wikipedia.org/wiki?curid=2247943" title="Home theater in a box">
Home theater in a box

A home theater in a box (HTIB) is an integrated home theater package which "bundles" together a combination DVD or Blu-ray player, a multi-channel amplifier (which includes a surround sound decoder, a radio tuner, and other features), speaker wires, connection cables, a remote control, a set of five or more surround sound speakers (or more rarely, just left and right speakers, a lower-price option known as "2.1") and a low-frequency subwoofer cabinet.
In 2016,
they are manufactured by most makers of consumer electronics. Budget HTIB's with generic or lower-price "house" brands (e.g., Best Buy's "Insignia" line) may be a "2.1" system. Many, however, are a full "5.1" system and some higher-end packages even have a 7.1 system. Some popular manufacturers of HTIB's are RCA, Philips, Panasonic, Sony, Yamaha, LG and Samsung, all of which make a variety of mid-price range packages. Bose and Onkyo make higher-end, higher-priced HTIB packages.

HTIBs are marketed as an "all-in-one" way for consumers to enjoy the surround sound experience of home cinema, even if they do not want to-or do not have the electronics "know-how" to pick out all of the components one-by-one and connect the cables. If a consumer were to buy all of the items individually, they would have to have a basic knowledge of electronics, so they could, for example, ensure that the speakers were of compatible impedance and power-handling for the amplifier. As well, the consumer would have to ensure that they purchased all of the different connection cables, which could include HDMI cables, optical connectors, speaker wire, and RCA connectors.

On the downside, most HTIBs lack the features and "tweakability" of home theater components which are sold separately. For example, while a standalone home theater amplifier may offer extensive equalization options, a HTIB amplifier may simply provide a few factory-set EQ presets. As well, while a standalone home theatre subwoofer may contain a range of sound-shaping circuitry, such as a crossover control, a phase inversion switch, and a parametric equalizer, a HTIB subwoofer system usually has its crossover point set at the factory, which means that the user cannot change it. In some cases, the factory preset crossover point on an HTIB subwoofer may cause it to sound too "boomy" in a room.

A typical HTIB generally consists of a central receiver unit which usually contains a DVD player (some systems separate the DVD player into a separate unit), a multi-channel power amplifier and a series of speakers for surround sound use, generally including a subwoofer. Some HTIB systems also have a radio tuner or an Internet-based streaming audio platform (e.g. Spotify). The least expensive systems usually have a passive subwoofer, which is amplified by the receiver unit. HTIB systems do not include a television set or monitor with which to display the visual material or a stand to place the receiver unit on. Beside auxiliary inputs, many of them are equipped today with HDMI with ARC, optical and SPDIF inputs. Some HTIB systems are also equipped with a phono input, to allow the connection of a turntable with magnetic cartridge. However such systems are not suitable for vinyl playing as they are mainly focussed on movies and rarely for high fidelity. Some home theaters are just stereo or 2.1, but even so, they are not intended as hi fi, this is just a marketing strategy.

There are systems in this class that are sold without a DVD player and are designed to integrate with existing video setups where there is already one, such as a DVD recorder or a DVD/VCR combo unit. The speaker cabinets supplied with most systems in this class are generally fairly small compared to typical stereo speakers, and are meant for wall- or shelf-mounting in tight spaces. There are some systems in this class that are supplied with slim freestanding speakers that stand on the floor. This may be typical of higher-priced systems that are equipped with more powerful amplifiers or most of the "receiver-only" packages that do not come with a DVD player.

Some HTIBs use proprietary connectors between components, sometimes even combining several different wires into one connector, to reduce cable clutter and increase the ease of installation. However, this can impede interoperability between different audio/visual devices and makes upgrading certain parts impossible. This may also be used by manufacturers to limit what a consumer can do with a low-end model and encourage them to upgrade should they want more autonomy.

A few manufacturers, notably Sony and Panasonic, have implemented wireless connection technology for the surround speakers in this class of equipment. This technology may be available as standard with some of the high-priced models or may be supplied as an aftermarket kit that only works with selected models in the manufacturer's range. It usually uses a line-level feed over a proprietary wireless link to a separate power amplifier used for the surround-sound channels. This link-receiver and power amplifier can be built into one of the surround speakers or housed in a black box that the surround speakers are connected to. Some higher-end HTIB models offer additional features such as 1080i or 4K ( mainly versions with Blu-ray) video resolution upscaling, a 5-disc platter, HDMI inputs, USB connectivity, Bluetooth support, Wi-fi support, Internet apps, DAB and DAB+, mirroring possibility, iPod dock and a hard disk for recording TV shows.

Some older HTIBs from the 1990s had a built-in VCR, besides a DVD, along with a TV tuner, and a hard disk for recording TV shows.


</doc>
<doc id="511640" url="https://en.wikipedia.org/wiki?curid=511640" title="5.1 surround sound">
5.1 surround sound

5.1 surround sound ("five-point one") is the common name for six-channel surround sound audio systems. 5.1 is the most commonly used layout in home theatre. It uses five full bandwidth channels and one low-frequency effects channel (the "point one"). Dolby Digital, Dolby Pro Logic II, DTS, SDDS, and THX are all common 5.1 systems. 5.1 is also the standard surround sound audio component of digital broadcast and music.

All 5.1 systems use the same speaker channels and configuration, having a front left and right, a center channel, two surround channels (left and right) and the low-frequency effects channel designed for a subwoofer.

A prototype for five-channel surround sound, then dubbed "quintaphonic sound", was used in the 1975 film "Tommy".

5.1 dates back to 1976 when Dolby Labs modified the track usage of the six analogue magnetic soundtracks on Todd-AO 70 mm film prints. The Dolby application of optical matrix encoding in 1976 (released on the film, "Logan's Run") did not use split surrounds, and thus was not 5.1. Dolby first used split surrounds with 70mm film, notably in 1979 with "Apocalypse Now". Instead of the five screen channels and one surround channel of the Todd-AO format, Dolby Stereo 70 mm Six Track provided three screen channels, two high-passed surround channels and a low-frequency surround channel monophonically blended with the two surround channels.

When digital sound was applied to 35 mm release prints, with "Batman Returns" in 1992, the 5.1 layout was adopted. The ability to provide 5.1 sound had been one of the key reasons for using 70 mm for prestige screenings. The provision of 5.1 digital sound on 35 mm significantly reduced the use of the very expensive 70 mm format. Digital sound and the 5.1 format were introduced in 1990, by KODAK and Optical Radiation Corporation, with releases of "Days of Thunder" and "The Doors" using the CDS (Cinema Digital Sound) format.

5.1 digital surround, in the forms of Dolby Digital AC3 and DTS, started appearing on several mid 90s Laserdisc releases, with among the earliest being "Clear and Present Danger" and "Jurassic Park" (the latter having both AC3 and DTS versions). Many DVD releases have Dolby Digital tracks up to 5.1 channels, due to the implementation of Dolby Digital in the development of the DVD format. In addition, some DVDs have DTS tracks with most being 5.1 channel mixes (a few releases, however, have 6.1 “matrixed” tracks). Blu-ray and digital cinema both have eight-channel capability which can be used to provide either 5.1 or 7.1 surround sound. 7.1 is an extension of 5.1 that uses four surround zones: two at the sides and two at the back.

A system of digital 5.1 surround sound has also been used in 1987 at the Parisian cabaret the Moulin Rouge, created by French engineer Dominique Bertrand. To achieve such a system in 1985 a dedicated mixing console had to be designed in cooperation with Solid State Logic, based on their 5000 series, and dedicated speakers in cooperation with APG. The console included ABCDEF channels. Respectively: A left, B right, C centre, D left rear, E right rear, F bass. The same engineer had already developed a similar 3.1 system in 1973, for use at the official International Summit of Francophone States in Dakar.

The order of channels in a 5.1 file is different across file formats. The order in WAV files is (not complete) Front Left, Front Right, Center, Low-frequency effects, Surround Left, Surround Right.

Regarding music, the main goal of 5.1 surround sound is a proper localization and equability of all acoustic sources for a centered positioned audience. Therefore, ideally five matched speakers should be used.

For play-back of 5.1 music recommendations of the International Telecommunication Union (ITU) have been released and propose the following configuration (ITU-R BS 775):



</doc>
<doc id="6451027" url="https://en.wikipedia.org/wiki?curid=6451027" title="Politainment">
Politainment

Politainment, a portmanteau word composed of politics and entertainment, describes tendencies in politics and mass media to liven up political reports and news coverage using elements from public relations to create a new kind of political communication. Politainment, while outwardly emphasizing the political aspects of the information communicated, nevertheless draws heavily upon techniques from pop culture and journalism to make complex information more accessible or convincing and distract public attention from politically unfavorable topics. The interdependencies of politicians and media are known as the politico-media complex.

Of doubtful virtue, declining amounts of content and substance can easily be compensated by giving news stories a sensationalistic twinge. Politainment thus ranges on the same level as edu- and infotainment.

Typical catchlines in politainment reports or media will at times bluntly argue "ad hominem" in a generalizing manner and try to emphasize virtues and charisma (""xyz" will make America great again") or vices and weaknesses (by denunciation: ""xyz" will wreck this country"). The latter example is also known as fear appeal. More moderate forms make extensive use of imprecise, metaphoric language (allegories, metonymy, periphrases, kennings etc.).

Politainment can be both a communication aspect of (1) politicians and spin doctors to their and their party's own advantage and the political adversary's disadvantage or (2) a strategy for news publishers, journalists, etc., to promote their medium and journalistic work.

Politainment may be a factor in party identification, mass-influencing voter's choices, it has thus become an indispensable tool in political campaigns and elections. As such it can also be one of the—seemingly innocuous—ingredients of crowd manipulation up to political psychological warfare.



</doc>
<doc id="166518" url="https://en.wikipedia.org/wiki?curid=166518" title="Pub quiz">
Pub quiz

A pub quiz is a quiz held in a pub or bar. These events are also called quiz nights, trivia nights, or bar trivia and may be held in other settings. Pub quizzes may attract customers to a pub who are not found there on other days. The pub quiz is a modern example of a pub game. Although different pub quizzes can cover a range of formats and topics, they have many features in common. The pub quiz was established in the UK in the 1970s by Burns and Porter and became part of British culture. The Great British Pub Quiz challenge is an annual event. In continental Europe, pub quizzes are a staple event at Irish pubs, where they are usually held in English.

In the U.S., trivia nights are sometimes used as fundraisers for nonprofit organizations, and these are very common in the Greater St. Louis area. 

The pub quiz was established in the UK in the 1970s, mainly by a company called Burns and Porter, to get people in to pubs on quieter nights. Popularity grew and grew over the next few years from just 30 teams to 10,000 playing each week in a Burns and Porter quiz. Redtooth runs an annual Great British Pub Quiz challenge, with more than 600 pubs taking part in 2012.

A 2009 study put the number of regular weekly pub quizzes in the UK at 22,445, and one website has counted approximately 2,000 regular weekly quizzes in the US.

On 4 May 2017 the first ever online pub quiz took place, run by Primordial Radio in the United Kingdom. It used existing technologies of a YouTube live stream and the answer sheet by survey monkey.

Pub quizzes (also known as live trivia, or table quizzes) are often weekly events and will have an advertised start time, most often in the evening.

While specific formats vary, most pub quizzes depend on answers being written in response to questions which may themselves be written or announced by a quizmaster.

One format for quizzing is called "infinite bounce". This format is generally used when the number of teams in the quiz is large – usually around 8–10. Every question is addressed to the team succeeding the team that answered the previous question. If no team answers the question, the next question is addressed to the team succeeding the team to whom the previous question was addressed.

Generally someone (either one of the bar staff or the person running the quiz) will come around with pens and quiz papers, which may contain questions or may just be blank sheets for writing the answers on. A mixture of both is common, in which case often only the blank sheet is to be handed in. Traditionally a member of the team hands the answers in for adjudication to the quiz master or to the next team along for marking when the answers are called.

It is up to the quizzers to form teams, which are generally based on tables, though if one table has a large group around it they may decide to split up. Some pubs insist on a maximum team size (usually between six and ten). The team members decide on a team name, often a supposedly humorous phrase or pun, which must be written on all papers handed in.

People often have to pay to participate – ranging from around 50p to £5 per person. This is often pooled to provide prize money. Many pub quizzes require no payment at all, as the event is simply a way to get paying customers into the venue, typically on less busy nights of the week.

The person asking the questions is known as the quizmaster. Quizmasters also mark and score answers submitted by teams, although formats exist where teams will mark each other's answer sheets.

The questions may be set by the bar staff or landlord, taken from a quiz book, bought from a specialist trivia company, or be set by volunteers from amongst the contestants. In the latter case, the quiz setter may be remunerated with drinks or a small amount of money.

Often questions may be drawn from the realm of 'everybody knows' trivia, therefore leading to controversies when the answers are false or unverifiable. In addition, as the quizzes are not formal affairs, slight errors in wording may lead to confusion and have led to a 2005 court case in the UK.

There may be between one and more than half a dozen rounds of questions, totalling anything from 10 to upwards of 80 questions. Rounds may include the following kinds (most common first):


In some quizzes teams are able to select one or two rounds as "jokers", in which their points will be doubled (or otherwise multiplied). Teams usually select their joker rounds before the start of the quiz, although some rounds may be excluded. Teams who consider themselves to be particularly strong on certain subjects can improve their chances with a good joker round, but risk wasting the joker if the questions are unexpectedly difficult. The idea of using a joker in a game may come from the BBC television programme "It's a Knockout".

Some quizzes include a bonus question, in which a single answer is required with one or more clues given each round making the answer progressively easier to solve. In some variants, the first team to hand in the correct answer wins either a spot prize or additional points to their total score. In others, the questions continue until all teams have the correct answer with each team been given progressively fewer additional points the longer it takes them to submit the correct answer.

Some quizzes add a small, separate round of questions to the end of a regular quiz, with the chance to win a jackpot. Each week an amount of money is added to the jackpot, and if no team answers the questions correctly, the money rolls over to the next quiz. The maximum amount of the jackpot may be limited by local gaming regulations.

Cash jackpots may be won by a variety of methods including one-off questions and dance-offs.

In some cases, the papers are marked by the bar staff. Alternatively, teams may have to mark their own answers and the handed-in papers are consulted only to check that prize claimants have not cheated by altering their answers. Another method is to have teams swap papers before marking, though this can be divisive.

One or two points are scored for each correct answer; some quizzes allow half marks for "nearly right" answers (such as a celebrity's surname when their full name was required). In some quizzes, certain questions score higher marks, particularly if they are unusually difficult.

With the mass use of mobile phones and mobile internet access, cheating has become a problem for some pub quizzes, with covert calls and texts made in the toilets, recent newspapers and magazines brought along especially for the event, ringers and so on. Though a maximum number of members set for teams may help to prevent large numbers of people collaborating, groups posing as several distinct teams are quite common. Some quizzes now ban the use of mobiles and nullify the score of any team found to be cheating. Though more prevalent where large sums of money are at stake, cheating can be observed even for relatively low stakes.

Some quizzes also now ban re-entry to the pub after the quiz has started, in order to prevent team members from using public internet stations, public telephones and mobile devices out of sight of the quizmaster. Generally, though, a pub runs its quiz alongside its normal operation, making such a measure impractical.

Prizes are awarded to the highest scoring team, and often to runners-up. Prizes are usually one of the following:


In a digital pub quiz wireless handsets replace the more usual pen and paper. A computer receives and records the answers from each team's handset and the results are exported to a spreadsheet at the end of the quiz. A time limit can be set for each question (e.g. 60 seconds) and it is possible to determine which team answers in the fastest time for spot prizes and tiebreaks.

As the pub quiz concept spread to the US in the 1990s, several companies formed to provide services to bars and restaurants organizing quizzes. Different from the quiz league in the UK, US commercial pub quizzes typically involve more than just two teams and can have as many as 25–35 teams playing in a single location, with up to 6 people per team. Quiz companies charge bars a fee for hosting the quiz, which may range from $80 per week to $175 or more depending on attendance. At least 20 different pub quiz companies currently exist in the US, with most operating events concentrated in major metropolitan areas.

A quiz league is an organisation that runs quizzes, normally in pubs, though such competitions are distinct from the standard pub quiz as they will normally involve two teams and often include a number of individual questions. No prizes are normally awarded at such a league match, but prizes and kudos may go to the quiz team winning a league or a competition. The National Trivia Association runs a nationwide contest involving various pub trivia games played around the US.

Teams from throughout a region, county, state or country meet annually for more prestigious competitions, with greater prizes. Representative teams may either be the best team from each pub, or a team selected from the best individuals.

Believe it or Not Quiz Events in New Zealand have held an annual Champion of Champions quiz in Auckland since 1999. Initially open to teams from pubs within the greater Auckland region, it is now open to teams from throughout New Zealand. In practice, travel costs prevent most teams from the lower North Island and the South Island participating, although Christchurch, Nelson and Wellington have all provided teams.

The Australasian Pub Quiz Championships takes place annually since 2018. Open to teams from pubs and clubs from across Australia and New Zealand, the 2018 Championships is simultaneously run in Sydney, Canberra and Wellington in late April.

In the United States National Trivia Association presents "The Riddle", a finals event open to eligible teams who play the official NTA "Quizzo!" live trivia game. Approximately a thousand players attended the 2008 event in Atlantic City, New Jersey.

Sporcle runs the Pub Champions Trivia League, which hosts regional, state, and national tournaments.

The largest pub quiz, according to the "Guinness Book of Records", was the "Quiz for Life", held at the Flanders Expo Halls in Ghent, Belgium, on 11 December 2010 with 2,280 participants. The annual "World famous pub quiz" in Birmingham counted approximately 2700 participants across 260 teams in August 2016.




</doc>
<doc id="3720589" url="https://en.wikipedia.org/wiki?curid=3720589" title="Entertainment technology">
Entertainment technology

Entertainment technology is the discipline of using manufactured or created components to enhance or make possible any sort of entertainment experience. Because entertainment categories are so broad, and because entertainment models the world in many ways, the types of implemented technology are derived from a variety of sources. Thus, in theatre, for example, entertainment technology practitioners must be able to design and construct scenery, install electrical systems, build clothing, use motors if there is scenery automation, provide plumbing (if functioning kitchen fixtures are required, or if "singing in the rain"), etc. In this way, the entertainment technology field intersects with most other types of technology.

Entertainment technology helps us relax and enjoy some free time. The latest technology has been revolutionized daily entertainment. Old ways such as recording on records, tapes, and CDs, have made music across the world. Movies are brought into living rooms through photography, film, and video. With the emerging of computer technology, ways of being entertained have optimized greatly. Many households are now having computers, consoles, or any other kinds of hand-holding computer game.The diversity and complexity of entertainment technology will bring endless joy and convenient to people's spare time. Traditionally, entertainment technology is derived from theatrical stagecraft, and stagecraft is an important subset of the discipline. However, the rise of new types and venues for entertainment, as well as rapidly advancing technological development, has increased the range and scope of its practice.

In animation and game design, the phrase "entertainment technology" refers to a very real world of entertainment experiences made possible by the advent of primarily computer-mediated digital technologies.

According to Sheau Ng, entertainment technology began with the invention of the phonograph by Thomas Edison, which was used to record and playback sound. This was followed by other media such as silent films, broadcast media, and different formats of pre-recorded music and other entertainment. This in turn impacted society, as this technology became a large part of everyday life and allowed people, governments, and organizations a way to communicate their ideas and creations with others.

Since the 19th century, the production, regulation, and dissemination of entertainment technology have been the core of controversies over the waft of information and cultural products. These technologies include video games, virtual worlds, and online role-playing games and recreational social networking technologies. In addition, there are two fundamental emphases in the scholarly cure of entertainment technologies. At the stage of audience consumption and participation, media outlets considered as entertainment applied sciences can be discussed as the capacity for acquiring statistics and cultivating attitudes and as a "space" for interaction. At the "macro" level of social family members and production, illustration can work to fortify modes of belonging, identity, and attitudes.

In the 1980s, consumers first adapt digital entertainment in the form of audio CDs, and then at the beginning of the 1990s, DVD format came to people’s lives, at the same time, the direct-to-home satellite had already started to provide customers with digital TV services. The satellite TV boxes that many households had at that time could be their earliest digital entertainment technology.

United States Analog television broadcast ended on June 12th, 2009. Television broadcast at most of the regions in the United States and Europe turned into digital with high-definition videos and digital sound. It was a big challenge at that time to switch to digital. As the approaching to millennium years, portable mobile devices were becoming popular among consumers. iPod published by Apple in 2001 could be a good example. Being one of the icons in the twenty-first century, it was a portable digital music player and started a revolution for mobile devices. iPod could be a very personal belonging, as time passes by, such personal digital devices would have the chance to replace the usage of the personal computers, TV, DVR, and including old mobile phones. Under some circumstances, consumers would prefer small-screen portable digital entertainment.

Video streaming is becoming a huge part of society in this day and age and it is only beginning to expand. Video streaming brought in a revenue of $30.29 billion in 2016 and based on projections conducted by Research and Markets, will reach $70.05 billion in the year 2021. Challenges for development in the media industry are how to maximize content, brands, and advertising. Consumers drive this field, companies are constantly running data about consumers preferences, relationships, habits, and locations.  

According to Ian Falles, CGI technology, which is known as Computer-generated Imagery, has been improved in recent years. For example, the actors who perform in 2016 Star Wars prequel died in 1977, and visual effects artists used motion-capture video of a stand-in reading his lines to reprised his role of Grand Moff Tarkin. Light on skins, hair, micro eye-darts, blood flow under skins are all elements to make face look real, which are all correlated with the re-creation of the similarities. The hologram technology will appears much more alive with the adapting of Epson projectors with "military -grade lasers'. In the future, the details of re-creation will be more focused, and artificial intelligence will be applied to CGI technology. What's more, computers will have algorithms embedded, and hours of footage will be recorded to generate human face movements.
In the future, gaming would be growing. Arlington, Texas was best known for its football stadium "Jerry World". While the opening of Esports Stadium Arlington, officers hope it could be the center of e-sports, and produce $1.7 billion revenue by 2021. The largest venue in North America has 100,000 square feet, with an 80-foot-wide stage, and 2000 gamers in the world sit around one by one, their movements are shown on an 85-foot LED screen. Several other esports venues include Esports Arena Las Vegas and Esports Arena Oakland. These venues are not only designed for championships but also can be a training center for gamers to get together so that they can communicate their skills there. Esports will attract more and more young people in the future. Many traditional sports owners, such as the owner of New England Patriots and the owner of New York Mets, have invested millions of dollars in gaming franchises. They believe that engaging in esports is the engagement with millennials in the future.

Customers will not be able to recognize the slight improvements in pictures and sound when they are using TVs or other screens. There are two new technologies that will change their minds. The first technology is called immersive sound, and it is mostly used in movie theatres. Dolby Atmos, which is a 3D sound format, and is different from the traditional surrounded sound. According to the “object-based” sound, speakers are facing toward the ceiling, so that Atmos will make people feel the sound flying over their head when there is a plane on the screen. The technology has been applied to the home theater since 2014, but with a very limited number of movies and shows, because most movies and shows hadn’t had those technologies embedded. The movie market finally caught up with the trend, atoms-embedded home theatre equipment is going to sell at a lower price; Apple’s 4k TV, Amazon Prime Video, Netflix all support 3D sound format streaming. Another technology is associated with the sharpest-ever picture. Many households are changing to 4K LCD TVs, while Samsung is developing new TVs which are much better than those televisions on sale, and it will change our imagination of TV. “The Wall”, which is the name of a giant TV, is 12 feet across with only a few inches thick, and it is powered by micro-LED panels, making images on TV look brighter and darker than other competing technologies. What’s more, those micro-LED panels are stitched together to perform, so “the Wall” can fulfill any requirements on sizes and shapes.

Schools that offer programs or degrees in entertainment technology include:


Currently, the only university offering a degree specifically in Entertainment Engineering and Design (EED) is the University of Nevada, Las Vegas (UNLV). Because UNLV's program is in its infancy, current entertainment technologists come from a wide variety of educational backgrounds, the most prevalent of which are theater and mechanical technology. The program provides a choice for students who want to get involved in the entertainment industry rather than pure engineering or technical theatre. The program can help students become competitive and successful in the incremental career. They will be proficient in engineering principles, new materials, and new technologies, and at the same time, they can still reach the artistic demand under the entertainment industry.

A bachelor's degree in these areas will typically have a difference of only a few specialized classes.

Traditionally, people interested in careers in this field either presented themselves as apprentices within craft unions or attended college programs in theatre technology. Although both are appropriate in limited ways, the growing world of entertainment technology encompasses many different types of performance and display environments than the theatre. To this end, newer opportunities have arisen that provide a wider educational base than these more traditional environments. An article "Rethinking Entertainment Technology Education" by John Huntington describes new teaching philosophies that resonate with the need for a richer and more flexible educational environment:



</doc>
<doc id="56337347" url="https://en.wikipedia.org/wiki?curid=56337347" title="Out-of-home entertainment">
Out-of-home entertainment

Out-of-Home Entertainment (also OOHE or OHE) is a term coined by the amusement industry to collectively refer to experiences at regional attractions like theme parks and waterparks with their thrill rides and slides, and smaller community-based entertainment venues such as family entertainment and cultural venues.

In the US alone, there are nearly 30,000 attractions—theme and amusement parks, attractions, water parks, family entertainment centers, zoos, aquariums, science centers, museums, and resorts, producing a total nationwide economic impact of $219 billion in 2011, according to leading international industry association, International Association of Amusement Parks and Attractions (IAAPA). The industry directly employs more than 1.3 million and indirectly generates 1 million jobs in the US, creating a total job impact of 2.3 million.

In recent years, the use of this term has gained acceptance with and been popularized by amusement industry players, industry associations, trade magazines and even securities analysts. This stems from the desire to distinguish between the social, competitive atmosphere and dedicated hardware found in location-based entertainment venues from at-home consumer-game entertainment, mobile entertainment or even augmented reality (AR) and virtual reality (VR). The reality is that the lines are increasingly blurred with today's sophisticated consumers and emerging technologies.

This term is not to be confused with out-of-home media advertising as used by the advertising industry, although the convergence of digital out-of-home advertising and the digital out-of-home entertainment is producing innovations in retail and hospitality, steeped in fundamentals of social gaming experiences defined by the video amusement industry during the 70’s.

Digital out-of-home entertainment (also DOE) is a sector that is understood by few but is a fast-growing technology sector with plenty of innovations transforming the sector. Its roots lie in the popularity of coin-operated arcade video games such as racing, fighting, Japanese imports, or pinball that Generation X will vividly recall with fond memories of countless hours of their youth spent in dimly-lit video-game rooms (popularly known as 'arcades').

When Generation Y came along, an audience well-versed in digital gaming favored game consoles over arcade machines. So while video amusement remains an integral part of the popular culture fabric today, its relevancy is diminished and even perceived as 'dead' partly due to the lack of coverage by consumer-game media even as the amusement industry transformed itself and research and development investments continue to pour into the sector, evolving and growing the out-of-home, pay-to-play experience.

In 2011, the non-profit Digital Out-of-Home Interactive Entertainment Network Association was established to help "define these amorphous groups that comprise this vibrant industry and illustrate how they all interact" with groups spanning from "family entertainment centers (FEC), location-based entertainment sites, visitor attractions, theme parks as well as retail, shopping malls and the hospitality sector – and not forgetting museums, heritage sites, schools".

Moviegoing is one of the most popular and affordable forms of out-of-home entertainment.
Other classic and expanded forms of OOHE making up the DOE sector include:

The traditional FECs is a classic form of OOHE that is easily understood by the public. FECs are essentially a converged outgrowth of theme restaurants and the winning formula of combining food and entertainment as a business model has been around for more than 30 years. The first Dave & Buster's was opened in 1982 in Dallas, Texas after discovering this winning formula and is a highly-successful FEC chain today with their "Eat, Drink, Play, Watch" offerings. Chuck E. Cheese first opened a store in 1977 and became the public embodiment of the typical children's party room combined with a pizza restaurant and arcade. Other restaurants started to come round to seeing the importance of amusement games and "anchor" attractions (bowling alleys, miniature golf, laser tags, batting cages, roller skating rinks, etc.) to encourage dwell time of 1–2 hours and stimulating repeat visits.

Probably known more by the blockbuster arcade video game titles they produced rather than by company names, these video game developers played a defining role in the birth of the video amusement industry. In 1972, Atari essentially created the first commercially successful video game "Pong," marking the beginning of the coin-operated video game industry. In 1978, the first blockbuster arcade video game, "Space Invaders" was produced by Taito Corporation and ushered in the golden age of arcade video games. Namco (of "Pac-Man"'s fame), Nintendo ("Donkey Kong"), Konami (), Capcom ("Street Fighters"), Sega AM2 ("Daytona") are among some of the most notable video game developers that remain active today in the video amusement scene.

Video game publishers are also making inroads into the OOHE market by licensing iconic IPs (intellectual property) to experienced arcade game developers and manufacturers, such as the recent collaboration between Ubisoft and LAI Games to produce "Virtual Rabbids: The Big Ride", an attendant-free VR attraction based on the popular "Rabbids" franchise.

A redemption game is an arcade amusement game involving skill that rewards the player (in gifts, tokens, etc.) proportionately to his or her score. (Merchandizers also fall in the redemption game category.) One of the most popular redemption games, "Skee Ball", has more than 100,000 Skee-Ball branded alley games in use worldwide by some estimates and continue to endure after more than a century. In 2016, BayTek Games (known for their hit game "Big Bass Wheel") bought the rights to "Skee-Ball" from Skee-Ball Amusement Games, Inc. Innovative Concepts in Entertainment (ICE), another reputable manufacturer, produced hit midway-style redemption games such as "Down The Clown" and "Gold Fishin". LAI Games (formerly part of the Leisure and Allied Industries Group which founded Timezone) produced hit games such as "Stacker" and "Speed of Light," the latter in which was embedded in popular culture with its appearance in Nickelodeon TV show Game Shakers.

Other notable players include Apple Industries, Inc., Coastal Amusements, Universal Space (UNIS), Adrenaline Amusements.

Another category of video amusement games are simulators. Raw Thrills, best known for developing arcade video games based on films such as "Jurassic Park Arcade" and "AMC's The" "Walking Dead" "Arcade", is a common name found in medium and larger-sized FECs. Other established companies in this category are Triotech, maker of "Typhoon" - a 3D arcade machine with 2 seats and delivers up to 2G Forces of acceleration, and CJ 4DPLEX with their "Mini Rider 3D" - a 2-seat simulator on an electric motion base with a choice of several 3D movies.


</doc>
<doc id="8957966" url="https://en.wikipedia.org/wiki?curid=8957966" title="Entertainment management">
Entertainment management

Entertainment management is a relatively new business management discipline that is increasingly being taught as a Bachelor of Arts degree. Entertainment management courses aim to provide graduates with appropriate knowledge and skills to progress into management careers within the entertainment sector, managing facilities such as sport events, theme parks, theaters, cinemas, live music venues, museums, art galleries, broadcast media companies and night clubs.

The Lubin School of Business at Pace University offers a BBA degree in management with a concentration in arts and entertainment.

A number of master's-level programs have emerged recently, including Carnegie Mellon University's Master of Entertainment Industry Management – which offers students with undergraduate degrees in the film and television the opportunity to refocus their education on the management dimension of the work, or Northwestern University's Master of Science in Leadership for Creative Enterprises program, which offers students with backgrounds in visual, performing, or interactive arts with management and entrepreneurial skills.

Growth in these courses has been linked with growth in both the creative and cultural industries. This growth is linked to increased consumer expenditure on recreation and entertainment activities. The result is a population assigning greater importance to the free time they have and a consequential willingness to spend more of their income on the 'experience' economy.


</doc>
<doc id="9988" url="https://en.wikipedia.org/wiki?curid=9988" title="Outline of entertainment">
Outline of entertainment

The following outline provides an overview of and topical guide to entertainment and the entertainment industry:

Entertainment is any activity which provides a diversion or permits people to amuse themselves in their leisure time, and may also provide fun, enjoyment, and laughter. People may create their own entertainment, such as when they spontaneously invent a game; participate actively in an activity they find entertaining, such as when they play sport as a hobby; or consume an entertainment product passively, such as when they attend a performance.

The entertainment industry (informally known as show business or show biz) is part of the tertiary sector of the economy and includes many sub-industries devoted to entertainment. However, the term is often used in the mass media to describe the mass media companies that control the distribution and manufacture of mass media entertainment. In the popular parlance, the term "show biz" in particular connotes the commercially popular performing arts, especially musical theatre, vaudeville, comedy, film, fun and music. It applies to every aspect of entertainment including cinema, television, radio, theatre and music.















</doc>
<doc id="13252351" url="https://en.wikipedia.org/wiki?curid=13252351" title="Retailtainment">
Retailtainment

Retailtainment is retail marketing as entertainment. In his book, "Enchanting a Disenchanted World: Revolutionizing the Means of Consumption" (1999), author George Ritzer describes "retailtainment" as the "use of ambience, emotion, sound and activity to get customers interested in the merchandise and in a mood to buy."

Sometimes called "inspirational retailing" or "entertailing," it has also been defined as "the modern trend of combining shopping and entertainment opportunities as an anchor for customers."

In 2001, Codeluppi described it as a way for marketers to "offer the consumer physical and emotional sensations during the shopping experience." And, in an article entitled "Using sonic branding in the retail environment" in the 2003 issue of the "Journal of Consumer Behavior", Fulberg described it as a way for retailers to entertain the consumer with a dramatization of their values."

According to Michael Morrison at the Australian Centre for Retail Studies:

“There is a move towards the concept of 'retailtainment.' This phenomenon, which brings together retailing, entertainment, music and leisure ... Retailers need to look further than the traditional retail store elements such as colour, lighting and visual merchandising to influence buying decisions. The specific atmosphere the retailer creates can, in some cases, be more influential in the decision-making process than the product itself. As goods and services become more of a commodity, it is what a shopper experiences and what atmosphere retailers create that really matters. Brand building is a combination of physical, functional, operational and psychological elements. Consumers will be willing to pay more for a brand if there is a perceived or actual added value from their experience of using the product or service.”
Shopper marketing expert Simon Temperley of Los Angeles agency The Marketing Arm, formerly U.S. Marketing & Promotions (Usmp), describes "retailtainment" as a "live brand experience" that frequently includes the use of "brand ambassadors" who "converse with the consumer."


</doc>
<doc id="57168807" url="https://en.wikipedia.org/wiki?curid=57168807" title="Social impact entertainment">
Social impact entertainment

Social Impact Entertainment (SIE) is a variety of mainstream entertainment forms, predominantly films and TV, that intend to have social impact next to great entertainment value.

The practitioners in this field are predominately producers and directors who want to add a "social surplus" to their entertainment, as contrasted to the field of Entertainment-Education which often starts out with a social change directive and then creates the entertainment around it.

A focus in the field is the study of actual effects (impact measurement), while the predominant interest is still to create entertaining, profitable content and work within the ecosystem of commercial film production and distribution. A common theme in the field of SIE is that mainstream film production companies employ so-called "Impact Producers" who specialize in impact-oriented distribution campaigns which can often include community screenings, screenings for legislators as well as impact measurement and evidence-building.

Unlike many other aspects of the mainstream entertainment industry, SIE tries to be scientific at times in its decision-making and frameworks; most notably building upon the work of Albert Bandura by utilizing social cognitive theory, and the work of TV producer Miguel Sabido, a contemporary of Bandura's, who pioneered large-scale television shows that used role modeling as "entertainment with a proven social benefit". SIE usually works with a "Theory of Change" on how the entertainment property will generate social impact and how said impact should be evaluated; this is usually preceded by extensive research on the issue, the stakeholders and past approaches for solving the issue at hand.

The most comprehensive overview over the field of Social Impact Entertainment, a report titled "The State of SIE", was published in March 2019 by the Skoll Center for SIE; it features sections such as "What is SIE", applications in narrative film, documentary film, theater, television, and "emerging forms". Similar to the "Cinema of Change Ecosystem", the State of SIE report features a "SIE Map", a mindmap-like structure that gives an overview of the field and its pioneers, participants and products. The report purposefully does not define the field of SIE.

Cinema of Change, a magazine and podcast dedicated to the field, reports on progress in the Social Impact Entertainment world since 2014. Currently, there are approximately 130 organizations, companies and film festivals operating in the Ecosystem, the most notable being Participant Media due to its $100 million+ funding and explicit mission in SIE.

Social impact entertainment is taught at a few universities and has taken foot with a handful of institutes, most notably University of Southern California's Norman Lear Center and UCLA TFT's Skoll Center for Social Impact Entertainment. The idea of this center came to Teri Schwartz in 2003, then a university dean; in 2007 she shared the idea with Jeffrey Skoll who was starting Participant Media at the time, and in 2014, once Schwartz had become the Dean of UCLA's film school, he donated $10 Million to create the center.

The Producers Guild of America launched its own initiative in the summer of 2018 - the PGA SIETF (Producer's Guild of America Social Impact Entertainment Task Force). The task force concentrates on educating the PGA membership in impact matters as well as organizing interdisciplinary events for producers working with impact-oriented projects in the fields of film, television, gaming and interactive media. During the PGA's flagship Produced By conference in 2019, the SIETF hosted a sold-out panel featuring Michael B. Jordan and impact producers, focusing exclusively on Social Impact Entertainment.

SIE has a different approach to generating impact when compared to Social and behavior change communication (SBCC), namely putting story and characters first, and shaping the impact around an existing intellectual property - rather than creating it from scratch for the sole purpose of impact.


</doc>
<doc id="9367761" url="https://en.wikipedia.org/wiki?curid=9367761" title="Entertainment in the 16th century">
Entertainment in the 16th century

British Entertainment in the 16th century included art, fencing, painting, the stocks and even executions.

While the 16th century and early 17th century squarely fall into the Renaissance period in Europe, that period was not only one of scientific and cultural advance, but also involved the development of changing forms of entertainment – both for the masses and for the elite.

Despite the great breadth of advancements in the arts during this time, the economic conditions of this period affected the types of entertainment available. There were three classes in society: A wealthy nobility, a merchant class, and the peasantry, who were typically poor.

The nobility could commission artisans to entertain them with works of art, music and theatre (Kareti, 1997). They would also enjoy or participate in the sports of fencing, falconry, horse riding and hunting; they enjoyed extravagant parties and dances, attended the opera house, and had the best seats in the theater. At that time Cricket was also a game associated with the nobility. In 1563, Lawrence Humphrey praised the five classic sports of Greece for the nobility – "whirling, leaping, casting the darte, wrestling, running" and derided "dauncing, fayninge to instrumentes, playe at dise, chesse, or tennes."

The middle class of merchants, wrights, inn keepers and the like, would occasionally enjoy the fine arts, for example the theater. Blood sports were popular – including bear baiting, bull baiting, dog fighting and cockfighting. Travelling troupes of actors entertained the masses. Enterprising bards would settle and build theaters – such as William Shakespeare’s Globe Theater (The Old Globe Theater History, 2005) in London.

The poor could rarely afford the theater, generally having to stand as groundlings. Executions were seen as a form of entertainment (Alchin, 2005), as was attending public humiliations in the stocks. Another popular public event with spectators, was the witch trial by use of water trial.(Lambert, 2007).



</doc>
<doc id="217777" url="https://en.wikipedia.org/wiki?curid=217777" title="Educational entertainment">
Educational entertainment

Educational entertainment (also referred to by the portmanteau edutainment) is media designed to educate through entertainment and a term used as early as 1954 by Walt Disney. Most often it includes content intended to teach but has incidental entertainment value. It has been used by academia, corporations, governments, and other entities in various countries to disseminate information in classrooms and/or via television, radio, and other media to influence viewers' opinions and behaviors.

Interest in combining education with entertainment, especially in order to make learning more enjoyable, has existed for hundreds of years, with the Renaissance and Enlightenment being movements in which this combination was presented to students. Komenský in particular is affiliated with the “school as play” concept, which proposes pedagogy with dramatic or delightful elements.

"Poor Richard’s Almanack" demonstrates early implementation of edutainment, with Benjamin Franklin combining entertaining and educational content, such as puzzles and rules of conduct, into an instructional entity for colonists.

Later development of the concept of edutainment can be tied to Walt Disney, with his first educational short film, "Tommy Tucker’s Tooth", being commissioned and shot in 1922 for the Deneer Dental Institute. The entry of the U.S. into World War II also had a major impact on the popularity of educational entertainment, as a relationship between Disney and the U.S. government formed; Disney was able to experiment with educational and nonfiction films in a way that continued even after the war, with series such as "True-Life Adventures" and "Disneyland". In the transcript of an interview with Alexander P. de Seversky from The Walt Disney Archives, of which its date and interviewer is unknown, the following quotation is found:Since the 1970s, various groups in the United States, the United Kingdom, and Latin America have used edutainment to address health and social issues such as substance abuse, immunization, teenage pregnancy, HIV/AIDS, and cancer. Initiatives in major universities, such as Johns Hopkins Center for Communication Programs and the University of Wisconsin–Madison, NGOs such as PCI-Media Impact, and government agencies such as the U.S. Centers for Disease Control (CDC) have produced edutainment content.

Modern forms of edutainment include television productions, film, museum exhibits, and computer software which use entertainment to attract and maintain an audience, while incorporating deliberate educational content or messages. It is also apparent that educational elements are becoming implemented into traditionally recreational realms, such as vacations and games.

The term edutainment was used as early as 1954 by Walt Disney to describe the "True Life Adventures" series. The noun "edutainment" is a neologistic portmanteau used by Robert Heyman in 1973 while producing documentaries for the National Geographic Society. It was used by Dr. Chris Daniels in 1975 to encapsulate the theme of his Millennium Project. This project later became known as The Elysian World Project. The terms “edutainment” (and “busitainment”) were used in 2001 to explain how the CRUMPET project, on context-aware and personalised Tourism, refers to people travelling for adventure yet who also travel for education and business and who do not perceive themselves as classical “tourists” . The offshoot word "Edutainer" has been used by Craig Sim Webb since before the turn of the millennium to describe an individual who offers edutainment presentations and performances.

"Schoolhouse Rock", "Wishbone", "Sesame Street", and "Bill Nye the Science Guy" are examples of shows that use music and video to teach topics like math, science, and history. Using music to aid memory dates back to the passing of ancient oral traditions, including the "Iliad" and the "Odyssey". Much of what edutainment can offer through audio and video especially, is accessible over the internet on platforms such as YouTube, with such channels as "Vsauce", "CGP Grey", "MinutePhysics", "Meet Arnold, "Veritasium", and "Crash Course".

Public Service Broadcasting is a band that incorporates audio and footage from the British Film Institute into their music and performances, and this partnership helps the British Film Institute showcase its material; their album "Inform—Educate—Entertain", which covers topics such as the climbing of Mount Everest and highway safety, demonstrates the connection between the concept of edutainment and their music.

Motion pictures with educational content appeared as early as 1943, such as "Private Snafu", and can still be seen in films such as "An Inconvenient Truth". After World War II, educational entertainment shifted towards television. Television programs can be divided into three main categories: those with primarily educational intentions, those with a high degree of both education and entertainment, and entertainment shows with incidental or occasional educational value.

Mexican TV producer Miguel Sabido pioneered in the 1970s a form of edutainment via telenovelas, "soap operas for social change". The "Sabido method" has been adopted in many other countries subsequently, including India, Peru, Kenya, and China. In Mexico, the government in the 1970s successfully used a telenovela to promote family planning to curb the country's high birthrate.

The third season of the television show "MTV Shuga" was analyzed by researchers and then published online in 2017 in terms of its effects related to its goal of educating African youth about sexual health and HIV, and secondarily, gender-based violence. In the randomized control trials, those in the treatment group who watched the show for six months were “almost twice as likely” to get tested at HIV testing centers; mixed results were found in regards to the show's effects on gender-based violence.

According to the article "" Walt Disney believed in education through the entertainment of film and television. He is known as the master of communications who brought both entertainment and education into the world in distinctive ways. His creation of mickey mouse and his adventures taught his audience that life is not always so simple, there will be obstacles that will stand in your way. Mickey mouse's is unpolished and imperfect which make him so realistic to his audience. In many of his stories we see him battle and adventure which he overcomes teach a value in life for viewers, keep going in life. Disney achieved and educational characteristic in his work through the picture series of live animals in their natural habitat by which some film techniques added to the drama of the series. This series was one that satisfied Walt Disney the most as it obtained both the qualities of entertainment and education, which was the characteristics of this series.

Educating the public about health issues is a focus for many stakeholders, including the pharmaceutical industry. In recent years, several initiatives have used educational entertainment principles to highlight specific conditions or wider healthcare issues. Examples include In Memory about dementia, Millefeuille about psoriasis and This is Axiom about the challenges facing the UK's National Health Service.

Games fulfill a number of educational purposes. Some games may be explicitly designed with education in mind, while others may have incidental or secondary educational value. All types of games, including board, card, and video games, may be used in an educational environment. Educational games are designed to teach people about certain subjects, expand concepts, reinforce development, understand an historical event or culture, or assist them in learning a skill as they play.

According to Paraskeva (2010), at least 68% of American households play video games. Many recent research articles postulate education and gaming can be joined to provide academic benefits.

According to Van Eck (2006), there are three reasons why games are considered learning tools: 1. Ongoing research that has included the last 20 years of educational findings have proven that digital games can be educational; 2. The new generation of today wants "multiple streams of information" (p. 1), which includes quick and frequent interaction that allows inductive reasoning; and 3. The mere popularity of games has created a billion-dollar industry. The idea of playing a game assumes the person is engaging in that activity by choice. The activity should have some value of "fun". This does not mean that the person is engaging in the activity only for leisure pursuits; it can also include the desire to learn a skill, connect with other gamers (social community), and spend time in a chosen activity. The activity needs to remain one of choice for the gamer.

Kim (2008) supports the use of off-the-shelf games with meta-cognitive strategies to provide an increase in students' cognitive performance.

Radio can serve as an effective vehicle for educational entertainment. The British radio soap opera "The Archers" has for decades been systematically educating its audience on agricultural matters; likewise, the Tanzanian radio soap opera "Twende na Wakati" ("Let's Go With the Times") was written primarily to promote family planning.

Likewise, podcasts have begun to exemplify the concept of edutainment, with some radio programs also becoming available in this digital format. Not only are there are series with educational elements that are listened to recreationally, but there are also podcasts used as an educational tool. Lessons based on podcasts have increased in popularity, with "TeachersPayTeachers" finding that lesson plans relating to podcasts rose in downloads by 21 percent and 650 percent in 2014 and 2015, respectively, which corresponded with the release of "Serial".

Other successful radio programs and/or podcasts that have fused entertainment and education include:

Toys are perhaps the earliest "edutainment" objects a person encounters, as many toys have also an educational aspect beside their aesthetic appeal. They can teach children literacy, numerical, conceptual or motor skills. Many toys (e.g., a miniature piano) are simply colorful, scaled-down versions of more complex objects, and thus can base children in skills and benefits associated with the latter. It is up to grown-ups to guide children to the toy's proper use in order to make the most out of it.

Toys are often employed in the context of mimicry and roleplay to partially experience personalities or situations not otherwise possible, very akin to simulation in video games. They can be used as primitive means to nurture an instinct or a character trait in children. Often, toys work simultaneously the other way, providing children with the means to "express" those things: a doll may be used by a girl to mimic her mother or express motherhood as much as to explore it.

Even for toys that don't possess explicit educational value, a thoughtful parent or teacher can turn a static figurine, for example, into an object of interest, by pointing out its features or costumes, or referring to its history or science (e.g., a figurine of a Native American may be a starting point for exploring American history; a Santa Claus may be used to explore the roots of Christmas; a toy astronaut to explore science...), which can be done in conjunction with a more-explicitly "edutaining" object, such as a picture book. Most children are naturally inquisitive (possibly why they sometimes break their toys; simply to know what is inside or how it moves or what produces that sound), and caregivers should not waste this opportunity.

Even "grown-ups" can learn through toys about children: what are their talents or interests; if they are more extrovert or introvert; indeed if they dislike toys and prefer social activities or sport, and thus capitalize on the children's abilities and correct what is wrong or lacking.

Some toys are of considerable appeal and benefit to both children and adults, such as Lego or Rubik's Cube, as their design and implementation can range from the simple to the sophisticated.

The term "edutainment" was first made popular amongst the hip hop community by KRS-One (a.k.a. The Teacha) thanks to his Boogie Down Productions album by the same name.

The term has since been borrowed to describe hip hop culture's innate pedagogical aesthetics. Examples of this include how Hip Hop uniquely combines both thought and action (see Paulo Freire's use of praxis in "Pedagogy of the Oppressed") and values both Eurocentric values of rational thought and Afrocentric epistemology of kinetic, affective and emotional ways of knowing. Another example is in the transmission of technical and historical knowledge, the manner in which Hip Hop practitioners learn and exchange by sharing moments of performance together through "building" or "building sessions."

The concept of educational entertainment is being used for building learning programs for organizations. High technology is used to make the programs entertaining and educational. As an example, PowerPoint presentations may become more entertaining with the addition of flashy animations or graphics. An article in a satirical newspaper, "The Onion", poked fun at the concept of embellishing boring presentations with attention-catching effects. A fictional marketing executive in the article noted the previous lack of excitement in the presentation, saying "When we first finished the PowerPoint, the content was all there, but it still lacked that certain something."

Theme parks are a specific kind of setting in which the combination of entertaining and educational elements can be prevalent. Epcot at Walt Disney World, which is owned by The Walt Disney Company, is highly based on edutainment; the park features attractions that teach about the past, conservation, imagination, future technologies, and the world. The park's dedication plaque, written by Marty Sklar and Erwin Okun, states the following: “May EPCOT Center entertain, inform and inspire. And above all, may it instill a new sense of belief and pride in man’s ability to shape a world that offers hope to people everywhere."

The offerings and promotions at SeaWorld associated with the educational topics of marine biology, conservation, and animal rescue efforts are increasing at their current locations and in plans for the upcoming Abu Dhabi theme park, which is likely due to activism for animal rights.

Other notable theme parks that incorporate educational elements and topics are Disney's Animal Kingdom, Holy Land Experience, Dinosaur World, Busch Gardens, and Puy du Fou.

Edutainment is also a growing paradigm within the science center and children's museum community in the United States, as well as in many other locations such as the zoo or a botanical garden. Educational locations such as these are constantly looking for new and innovative ways to reach the surrounding public and get them interested in areas such as the fine arts, science, literature, and history. Additionally, field trip visits to these educational places provide participants with interactional stimulus to learn about the content offered there. Since people are used to flashy, polished entertainment venues like movie theaters and theme parks, they demand similar experiences at science centers or museums. Consequently, interactive experiences, such as games and mobile apps, are implemented in museums in order to more effectively help people learn about what they are seeing. Museums are also embracing the ability to use storytelling to engage people, especially those who are young, in hopes to increase attendance; all the while, though, it is possible for the focus and purpose of museums to be diluted. Thus, a museum or a zoo can be seen as just another business competing for entertainment dollars from the public, rather than as an institution that serves the public welfare through education or historical preservation.
Much research has been done in association with edutainment and its effectiveness in its various forms. Particularly, the effects of the implementation of the concept of edutainment in the classroom setting have been studied on multiple occasions. The concept of flipped classrooms, in association with edutainment, was studied by Retta Guy and Gerald Marquis (2016), in which students were assigned video lessons and podcasts as opposed to projects prior to class; it was found that these students outperformed those in traditional classrooms, found the actual time in class to prompt more interaction, and thought the class to be more enjoyable, although there was a noticeable period of adaption.

In a study conducted by Ruby Lynch-Arroyo and Joyce Asing-Cashman (2016), "Numb3rs", which is an example of edutainment in the form of a television show, was integrated into the education of preservice mathematics teachers. The subsequent results exemplified the potential for edutainment to promote critical thinking, increased engagement, and growth mindsets.

Similarly, Craig D. Cox, "et al". (2017) conducted a study in which a mini-series that combined educational and entertaining elements was developed and presented to pharmacy preceptors; it was effective in increasing the confidence of the participants and was an honorable mention for the American Association of Colleges of Pharmacy Innovations in Teaching Award in 2015.

A system that incorporated the concept of edutainment through the use of games has also been studied in association with disabled students by Amal Dandashi, "et al". (2015), and it was found that the system had a positive impact in terms of scores, coordination, communication, and memorization skills; after replaying, higher scores were often achieved as well.

As for podcasts and narration specifically, according to a study conducted by Trish L. Varao Sousa, Jonathan S. A. Carriere, and Daniel Smilek (2013) with student participants, these forms may not be particularly helpful, as their use can result in both less information actually internalized and less engagement overall when compared to other "reading encounters."

Debate regarding the value of educational entertainment and its implementation has been had for decades. The negative feedback received by Walt Disney for his "True-Life Adventures" series represents the views of some skeptics of the concept of edutainment. For example, the use of music along with the footage of animals, such as the circumstance in which "The courtship of tarantulas was set to a tango, while the movements of two scorpions were showcased with square dance music in the background" was criticized at the time; the purpose of the music was to enhance the footage, but some people took issue with this humanization. Additionally, without approval, some of the film crew of "White Wilderness" prompted unnatural behavior in lemmings that would be filmed, which then generated a negative response.

"Sesame Street", a television show that demonstrates the concept of edutainment, has also specifically been subject to criticism. For instance, in an article published in "The Atlantic" in May 1971, John Holt criticizes the promotion of "Right Answers" in the television show without actual action being taken by the children, and also argues that it is nonsensical and perplexing to have adults convey to children that everything that is to be discovered is logical and easy to understand.

The argument that the concept of edutainment hurts the actual value of education has been prevalent as well. The book entitled "Amusing Ourselves to Death" by theorist Neil Postman demonstrates this notion, as it is claimed that areas of study have been “transformed into congenial adjuncts of show business.” Museum professionals especially have faced this dilemma, as in study conducted by Pierre Balloffet, François H. Courvoisier, and Joëlle Lagier (2014), museum professionals did not have severe negative opinions of the incorporation of educational entertainment, but individuals nevertheless had varying viewpoints on "the appropriateness or potential risks of edutainment."


</doc>
<doc id="62647022" url="https://en.wikipedia.org/wiki?curid=62647022" title="Phou-oibi, the rice goddess">
Phou-oibi, the rice goddess

" Phouoibi, the rice goddess " is a 2013 Manipuri language ballad opera, performed by the Laihui Ensemble from Manipur, India, as a part Tapestry Sacred Music 2013 programme held at the Esplanade in Singapore.

The ballad opera "Phou-oibi the rice goddess" tells the divine story of various Goddesses, of fish, land, metal, water, wealth and after all of rice, Phouoibi, who are sent down to earth by the supreme God to prosper the human civilization. On her way, Phouoibi meets Akongjamba and both fall in love with each other.



</doc>
<doc id="9262" url="https://en.wikipedia.org/wiki?curid=9262" title="Entertainment">
Entertainment

Entertainment is a form of activity that holds the attention and interest of an audience or gives pleasure and delight. It can be an idea or a task, but is more likely to be one of the activities or events that have developed over thousands of years specifically for the purpose of keeping an audience's attention. Although people's attention is held by different things, because individuals have different preferences in entertainment, most forms are recognisable and familiar. Storytelling, music, drama, dance, and different kinds of performance exist in all cultures, were supported in royal courts, developed into sophisticated forms and over time became available to all citizens. The process has been accelerated in modern times by an entertainment industry that records and sells entertainment products. Entertainment evolves and can be adapted to suit any scale, ranging from an individual who chooses a private entertainment from a now enormous array of pre-recorded products; to a banquet adapted for two; to any size or type of party, with appropriate music and dance; to performances intended for thousands; and even for a global audience.

The experience of being entertained has come to be strongly associated with amusement, so that one common understanding of the idea is fun and laughter, although many entertainments have a serious purpose. This may be the case in the various forms of ceremony, celebration, religious festival, or satire for example. Hence, there is the possibility that what appears as entertainment may also be a means of achieving insight or intellectual growth.

An important aspect of entertainment is the audience, which turns a private recreation or leisure activity into entertainment. The audience may have a passive role, as in the case of persons watching a play, opera, television show, or film; or the audience role may be active, as in the case of games, where the participant/audience roles may be routinely reversed. Entertainment can be public or private, involving formal, scripted performance, as in the case of theatre or concerts; or unscripted and spontaneous, as in the case of children's games. Most forms of entertainment have persisted over many centuries, evolving due to changes in culture, technology, and fashion for example with stage magic. Films and video games, for example, although they use newer media, continue to tell stories, present drama, and play music. Festivals devoted to music, film, or dance allow audiences to be entertained over a number of consecutive days.

Some entertainment, such as public executions, are now illegal in most countries. Activities such as fencing or archery, once used in hunting or war, have become spectator sports. In the same way, other activities, such as cooking, have developed into performances among professionals, staged as global competitions and then broadcast for entertainment. What is entertainment for one group or individual may be regarded as work or an act of cruelty by another.

The familiar forms of entertainment have the capacity to cross over different media and have demonstrated a seemingly unlimited potential for creative remix. This has ensured the continuity and longevity of many themes, images, and structures.

Entertainment can be distinguished from other activities such as education and marketing even though they have learned how to use the appeal of entertainment to achieve their different goals. Sometimes entertainment can be a mixture for both. The importance and impact of entertainment is recognised by scholars and its increasing sophistication has influenced practices in other fields such as museology.

Psychologists say the function of media entertainment is "the attainment of gratification". No other results or measurable benefit are usually expected from it (except perhaps the final score in a sporting entertainment). This is in contrast to education (which is designed with the purpose of developing understanding or helping people to learn) and marketing (which aims to encourage people to purchase commercial products). However, the distinctions become blurred when education seeks to be more "entertaining" and entertainment or marketing seek to be more "educational". Such mixtures are often known by the neologisms "edutainment" or "infotainment". The psychology of entertainment as well as of learning has been applied to all these fields. Some education-entertainment is a serious attempt to combine the best features of the two. Some people are entertained by others' pain or the idea of their unhappiness (schadenfreude).

An entertainment might go beyond gratification and produce some insight in its audience. Entertainment may skilfully consider universal philosophical questions such as: "What does it mean to be human?"; "What is the right thing to do?"; or "How do I know what I know?". "The Meaning of Life", for example, is the subject in a wide range of entertainment forms, including film, music and literature. Questions such as these drive many narratives and dramas, whether they are presented in the form of a story, film, play, poem, book, dance, comic, or game. Dramatic examples include Shakespeare's influential play "Hamlet", whose hero articulates these concerns in poetry; and films, such as "The Matrix", which explores the nature of knowledge and was released worldwide. Novels give great scope for investigating these themes while they entertain their readers. An example of a creative work that considers philosophical questions so entertainingly that it has been presented in a very wide range of forms is "The Hitchhiker's Guide to the Galaxy". Originally a radio comedy, this story became so popular that it has also appeared as a novel, film, television series, stage show, comic, audiobook, LP record, adventure game and online game, its ideas became popular references (see Phrases from The Hitchhiker's Guide to the Galaxy) and has been translated into many languages. Its themes encompass the meaning of life, as well as "the ethics of entertainment, artificial intelligence, multiple worlds, God, and philosophical method".

The "ancient craft of communicating events and experiences, using words, images, sounds and gestures" by telling a story is not only the means by which people passed on their cultural values and traditions and history from one generation to another, it has been an important part of most forms of entertainment ever since the earliest times. Stories are still told in the early forms, for example, around a fire while camping, or when listening to the stories of another culture as a tourist. "The earliest storytelling sequences we possess, now of course, committed to writing, were undoubtedly originally a speaking from mouth to ear and their force as entertainment derived from the very same elements we today enjoy in films and novels." Storytelling is an activity that has evolved and developed "toward variety". Many entertainments, including storytelling but especially music and drama, remain familiar but have developed into a wide variety of form to suit a very wide range of personal preferences and cultural expression. Many types are blended or supported by other forms. For example, drama, stories and banqueting (or dining) are commonly enhanced by music; sport and games are incorporated into other activities to increase appeal. Some may have evolved from serious or necessary activities (such as running and jumping) into competition and then become entertainment. It is said, for example, that pole vaulting "may have originated in the Netherlands, where people used long poles to vault over wide canals rather than wear out their clogs walking miles to the nearest bridge. Others maintain that pole vaulting was used in warfare to vault over fortress walls during battle." The equipment for such sports has become increasingly sophisticated. Vaulting poles, for example, were originally made from woods such as ash, hickory or hazel; in the 19th century bamboo was used and in the 21st century poles can be made of carbon fibre. Other activities, such as walking on stilts, are still seen in circus performances in the 21st century. Gladiatorial combats, also known as "gladiatorial games", popular during Roman times, provide a good example of an activity that is a combination of sport, punishment, and entertainment.

Changes to what is regarded as entertainment can occur in response to cultural or historical shifts. Hunting wild animals, for example, was introduced into the Roman Empire from Carthage and became a popular public entertainment and spectacle, supporting an international trade in wild animals.

Entertainment also evolved into different forms and expressions as a result of social upheavals such as wars and revolutions. During the Chinese Cultural Revolution, for example, Revolutionary opera was sanctioned by the Communist party and World War I, the Great Depression and the Russian revolution all affected entertainment.

Relatively minor changes to the form and venue of an entertainment continue to come and go as they are affected by the period, fashion, culture, technology, and economics. For example, a story told in dramatic form can be presented in an open-air theatre, a music hall, a movie theatre, a multiplex, or as technological possibilities advanced, via a personal electronic device such as a tablet computer. Entertainment is provided for mass audiences in purpose-built structures such as a theatre, auditorium, or stadium. One of the most famous venues in the Western world, the Colosseum, "dedicated AD 80 with a hundred days of games, held fifty thousand spectators," and in it audiences "enjoyed blood sport with the trappings of stage shows". Spectacles, competitions, races, and sports were once presented in this purpose-built arena as public entertainment. New stadia continue to be built to suit the ever more sophisticated requirements of global audiences.

Imperial and royal courts have provided training grounds and support for professional entertainers, with different cultures using palaces, castles and forts in different ways. In the Maya city states, for example, "spectacles often took place in large plazas in front of palaces; the crowds gathered either there or in designated places from which they could watch at a distance." Court entertainments also crossed cultures. For example, the durbar was introduced to India by the Mughals, and passed onto the British Empire, which then followed Indian tradition: "institutions, titles, customs, ceremonies by which a Maharaja or Nawab were installed ... the exchange of official presents ... the order of precedence", for example, were "all inherited from ... the Emperors of Delhi". In Korea, the "court entertainment dance" was "originally performed in the palace for entertainment at court banquets."

Court entertainment often moved from being associated with the court to more general use among commoners. This was the case with "masked dance-dramas" in Korea, which "originated in conjunction with village shaman rituals and eventually became largely an entertainment form for commoners". Nautch dancers in the Mughal Empire performed in Indian courts and palaces. Another evolution, similar to that from courtly entertainment to common practice, was the transition from religious ritual to secular entertainment, such as happened during the Goryeo dynasty with the Narye festival. Originally "solely religious or ritualistic, a secular component was added at the conclusion". Former courtly entertainments, such as jousting, often also survived in children's games.

In some courts, such as those during the Byzantine Empire, the genders were segregated among the upper classes, so that "at least before the period of the Komnenoi" (1081–1185) men were separated from women at ceremonies where there was entertainment such as receptions and banquets.

Court ceremonies, palace banquets and the spectacles associated with them, have been used not only to entertain but also to demonstrate wealth and power. Such events reinforce the relationship between ruler and ruled; between those with power and those without, serving to "dramatise the differences between ordinary families and that of the ruler". This is the case as much as for traditional courts as it is for contemporary ceremonials, such as the Hong Kong handover ceremony in 1997, at which an array of entertainments (including a banquet, a parade, fireworks, a festival performance and an art spectacle) were put to the service of highlighting a change in political power. Court entertainments were typically performed for royalty and courtiers as well as "for the pleasure of local and visiting dignitaries". Royal courts, such as the Korean one, also supported traditional dances. In Sudan, musical instruments such as the so-called "slit" or "talking" drums, once "part of the court orchestra of a powerful chief", had multiple purposes: they were used to make music; "speak" at ceremonies; mark community events; send long-distance messages; and call men to hunt or war.

Courtly entertainments also demonstrate the complex relationship between entertainer and spectator: individuals may be either an entertainer or part of the audience, or they may swap roles even during the course of one entertainment. In the court at the Palace of Versailles, "thousands of courtiers, including men and women who inhabited its apartments, acted as both performers and spectators in daily rituals that reinforced the status hierarchy".

Like court entertainment, royal occasions such as coronations and weddings provided opportunities to entertain both the aristocracy and the people. For example, the splendid 1595 Accession Day celebrations of Queen Elizabeth I offered tournaments and jousting and other events performed "not only before the assembled court, in all their finery, but also before thousands of Londoners eager for a good day's entertainment. Entry for the day's events at the Tiltyard in Whitehall was set at 12d".

Although most forms of entertainment have evolved and continued over time, some once-popular forms are no longer as acceptable. For example, during earlier centuries in Europe, watching or participating in the punishment of criminals or social outcasts was an accepted and popular form of entertainment. Many forms of public humiliation also offered local entertainment in the past. Even capital punishment such as hanging and beheading, offered to the public as a warning, were also regarded partly as entertainment. Capital punishments that lasted longer, such as stoning and drawing and quartering, afforded a greater public spectacle. "A hanging was a carnival that diverted not merely the unemployed but the unemployable. Good bourgeois or curious aristocrats who could afford it watched it from a carriage or rented a room." Public punishment as entertainment lasted until the 19th century by which time "the awesome event of a public hanging aroused the[ir] loathing of writers and philosophers". Both Dickens and Thackeray wrote about a hanging in Newgate Prison in 1840, and "taught an even wider public that executions are obscene entertainments".

Children's entertainment is centred on play and is significant for their growth. Entertainment is also provided to children or taught to them by adults and many activities that appeal to them such as puppets, clowns, pantomimes and cartoons are also enjoyed by adults.

Children have always played games. It is accepted that as well as being entertaining, playing games helps children's development. One of the most famous visual accounts of children's games is a painting by Pieter Bruegel the Elder called "Children's Games", painted in 1560. It depicts children playing a range of games that presumably were typical of the time. Many of these games, such as marbles, hide-and-seek, blowing soap bubbles and piggyback riding continue to be played.
Most forms of entertainment can be or are modified to suit children's needs and interests. During the 20th century, starting with the often criticised but nonetheless important work of G. Stanley Hall, who "promoted the link between the study of development and the 'new' laboratory psychology", and especially with the work of Jean Piaget, who "saw cognitive development as being analogous to biological development", it became understood that the psychological development of children occurs in stages and that their capacities differ from adults. Hence, stories and activities, whether in books, film, or video games were developed specifically for child audiences. Countries have responded to the special needs of children and the rise of digital entertainment by developing systems such as television content rating systems, to guide the public and the entertainment industry.

In the 21st century, as with adult products, much entertainment is available for children on the internet for private use. This constitutes a significant change from earlier times. The amount of time expended by children indoors on screen-based entertainment and the "remarkable collapse of children's engagement with nature" has drawn criticism for its negative effects on imagination, adult cognition and psychological well-being.<ref name="http://www.guardian.co.uk/commentisfree/2012/nov/19/children-lose-contact-with-nature"></ref>

Banquets have been a venue for entertainment since ancient times, continuing until the 21st century, when they are still being used for many of their original purposes to impress visitors, especially important ones (4, 6, 9); to show hospitality (2, 4, 8); as an occasion to showcase supporting entertainments such as music or dancing, or both (2, 3). They were an integral part of court entertainments (3, 4) and helped entertainers develop their skills (2, 3). They are also important components of celebrations such as coronations (9), weddings (7), birthdays (10) civic or political achievements (5), military engagements or victories (6) as well as religious obligations (1). In modern times, banquets are commercially available, for example, in restaurants (10) and combined with a performance in dinner theatres. Cooking by professional chefs has also become a form of entertainment as part of global competitions such as the Bocuse d'Or.
Music is a supporting component of many kinds of entertainment and most kinds of performance. For example, it is used to enhance storytelling, it is indispensable in dance (1, 4) and opera, and is usually incorporated into dramatic film or theatre productions.

Music is also a universal and popular type of entertainment on its own, constituting an entire performance such as when concerts are given (2, 4, 5, 6, 7, 8, 9 ). Depending on the rhythm, instrument, performance and style, music is divided into many genres, such as classical, jazz, folk, (4, 5, 8), rock, pop music (6, 9) or traditional (1, 3). Since the 20th century, performed music, once available only to those who could pay for the performers, has been available cheaply to individuals by the entertainment industry, which broadcasts it or pre-records it for sale.

The wide variety of musical performances, whether or not they are artificially amplified (6, 7, 9, 10), all provide entertainment irrespective of whether the performance is from soloists (6), choral (2) or orchestral groups (5, 8), or ensemble (3). Live performances use specialised venues, which might be small or large; indoors or outdoors; free or expensive. The audiences have different expectations of the performers as well as of their own role in the performance. For example, some audiences expect to listen silently and are entertained by the excellence of the music, its rendition or its interpretation (5, 8). Other audiences of live performances are entertained by the ambience and the chance to participate (7, 9). Even more listeners are entertained by pre-recorded music and listen privately (10).

The instruments used in musical entertainment are either solely the human voice (2, 6) or solely instrumental (1, 3) or some combination of the two (4, 5, 7, 8). Whether the performance is given by vocalists or instrumentalists, the performers may be soloists or part of a small or large group, in turn entertaining an audience that might be individual (10), passing by (3), small (1, 2) or large (6, 7, 8, 9). Singing is generally accompanied by instruments although some forms, notably a cappella and overtone singing, are unaccompanied. Modern concerts often use various special effects and other theatrics to accompany performances of singing and dancing (7).

Games are played for entertainment—sometimes purely for entertainment, sometimes for achievement or reward as well. They can be played alone, in teams, or online; by amateurs or by professionals. The players may have an audience of non-players, such as when people are entertained by watching a chess championship. On the other hand, players in a game may constitute their own audience as they take their turn to play. Often, part of the entertainment for children playing a game is deciding who is part of their audience and who is a player.

Equipment varies with the game. Board games, such as Go, "Monopoly" or backgammon need a board and markers. One of the oldest known board games is Senet, a game played in Ancient Egypt, enjoyed by the pharaoh Tutankhamun. Card games, such as whist, poker and Bridge have long been played as evening entertainment among friends. For these games, all that is needed is a deck of playing cards. Other games, such as bingo, played with numerous strangers, have been organised to involve the participation of non-players via gambling. Many are geared for children, and can be played outdoors, including hopscotch, hide and seek, or Blind man's bluff. The list of ball games is quite extensive. It includes, for example, croquet, lawn bowling and paintball as well as many sports using various forms of balls. The options cater to a wide range of skill and fitness levels. Physical games can develop agility and competence in motor skills. Number games such as Sudoku and puzzle games like the Rubik's cube can develop mental prowess.

Video games are played using a controller to create results on a screen. They can also be played online with participants joining in remotely. In the second half of the 20th century and in the 21st century the number of such games increased enormously, providing a wide variety of entertainment to players around the world. Video games are popular across the world.

Reading has been a source of entertainment for a very long time, especially when other forms, such as performance entertainments, were (or are) either unavailable or too costly. Even when the primary purpose of the writing is to inform or instruct, reading is well known for its capacity to distract from everyday worries. Both stories and information have been passed on through the tradition of orality and oral traditions survive in the form of performance poetry for example. However, they have drastically declined. "Once literacy had arrived in strength, there was no return to the oral prerogative." The advent of printing, the reduction in costs of books and an increasing literacy all served to enhance the mass appeal of reading. Furthermore, as fonts were standardised and texts became clearer, "reading ceased being a painful process of decipherment and became an act of pure pleasure". By the 16th century in Europe, the appeal of reading for entertainment was well established.

Among literature's many genres are some designed, in whole or in part, purely for entertainment. Limericks, for example, use verse in a strict, predictable rhyme and rhythm to create humour and to amuse an audience of listeners or readers. Interactive books such as "choose your own adventure" can make literary entertainment more participatory.
Comics and cartoons are literary genres that use drawings or graphics, usually in combination with text, to convey an entertaining narrative. Many contemporary comics have elements of fantasy and are produced by companies that are part of the entertainment industry. Others have unique authors who offer a more personal, philosophical view of the world and the problems people face. Comics about superheroes such as Superman are of the first type. Examples of the second sort include the individual work over 50 years of Charles M. Schulz who produced a popular comic called "Peanuts" about the relationships among a cast of child characters; and Michael Leunig who entertains by producing whimsical cartoons that also incorporate social criticism. The Japanese Manga style differs from the western approach in that it encompasses a wide range of genres and themes for a readership of all ages. Caricature uses a kind of graphic entertainment for purposes ranging from merely putting a smile on the viewer's face, to raising social awareness, to highlighting the moral characteristics of a person being caricatured.

Comedy is both a genre of entertainment and a component of it, providing laughter and amusement, whether the comedy is the sole purpose or used as a form of contrast in an otherwise serious piece. It is a valued contributor to many forms of entertainment, including in literature, theatre, opera, film and games. In royal courts, such as in the Byzantine court, and presumably, also in its wealthy households, "mimes were the focus of orchestrated humour, expected or obliged to make fun of all at court, not even excepting the emperor and members of the imperial family. This highly structured role of jester consisted of verbal humour, including teasing, jests, insult, ridicule, and obscenity and non-verbal humour such as slapstick and horseplay in the presence of an audience." In medieval times, all comic types the buffoon, jester, hunchback, dwarf, jokester, were all "considered to be essentially of one comic type: the fool", who while not necessarily funny, represented "the shortcomings of the individual".

Shakespeare wrote seventeen comedies that incorporate many techniques still used by performers and writers of comedy—such as jokes, puns, parody, wit, observational humor, or the unexpected effect of irony. One-liner jokes and satire are also used to comedic effect in literature. In farce, the comedy is a primary purpose.

The meaning of the word "comedy" and the audience's expectations of it have changed over time and vary according to culture. Simple physical comedy such as slapstick is entertaining to a broad range of people of all ages. However, as cultures become more sophisticated, national nuances appear in the style and references so that what is amusing in one culture may be unintelligible in another.

Live performances before an audience constitute a major form of entertainment, especially before the invention of audio and video recording. Performance takes a wide range of forms, including theatre, music and drama. In the 16th and 17th centuries, European royal courts presented masques that were complex theatrical entertainments involving dancing, singing and acting. Opera is a similarly demanding performance style that remains popular. It also encompass all three forms, demanding a high level of musical and dramatic skill, collaboration and like the masque, production expertise as well.
Audiences generally show their appreciation of an entertaining performance with applause. However, all performers run the risk of failing to hold their audience's attention and thus, failing to entertain. Audience dissatisfaction is often brutally honest and direct.

Storytelling is an ancient form of entertainment that has influenced almost all other forms. It is "not only entertainment, it is also thinking through human conflicts and contradictions". Hence, although stories may be delivered directly to a small listening audience, they are also presented as entertainment and used as a component of any piece that relies on a narrative, such as film, drama, ballet, and opera. Written stories have been enhanced by illustrations, often to a very high artistic standard, for example, on illuminated manuscripts and on ancient scrolls such as Japanese ones. Stories remain a common way of entertaining a group that is on a journey. Showing how stories are used to pass the time and entertain an audience of travellers, Chaucer used pilgrims in his literary work "The Canterbury Tales" in the 14th century, as did Wu Cheng'en in the 16th century in "Journey to the West". Even though journeys can now be completed much faster, stories are still told to passengers en route in cars and aeroplanes either orally or delivered by some form of technology.

The power of stories to entertain is evident in one of the most famous ones—Scheherazade—a story in the Persian professional storytelling tradition, of a woman who saves her own life by telling stories. The connections between the different types of entertainment are shown by the way that stories like this inspire a retelling in another medium, such as music, film or games. For example, composers Rimsky-Korsakov, Ravel and Szymanowski have each been inspired by the Scheherazade story and turned it into an orchestral work; director Pasolini made a film adaptation; and there is an innovative video game based on the tale. Stories may be told wordlessly, in music, dance or puppetry for example, such as in the Javanese tradition of wayang, in which the performance is accompanied by a gamelan orchestra or the similarly traditional Punch and Judy show.

Epic narratives, poems, sagas and allegories from all cultures tell such gripping tales that they have inspired countless other stories in all forms of entertainment. Examples include the Hindu "Ramayana" and "Mahabharata"; Homer's "Odyssey" and "Iliad"; the first Arabic novel "Hayy ibn Yaqdhan"; the Persian epic "Shahnameh"; the Sagas of Icelanders and the celebrated "Tale of the Genji". Collections of stories, such as "Grimms' Fairy Tales" or those by Hans Christian Andersen, have been similarly influential. Originally published in the early 19th century, this collection of folk stories significantly influence modern popular culture, which subsequently used its themes, images, symbols, and structural elements to create new entertainment forms.

Some of the most powerful and long-lasting stories are the foundation stories, also called origin or creation myths such as the Dreamtime myths of the Australian aborigines, the Mesopotamian "Epic of Gilgamesh", or the Hawaiian stories of the origin of the world. These too are developed into books, films, music and games in a way that increases their longevity and enhances their entertainment value.

Theatre performances, typically dramatic or musical, are presented on a stage for an audience and have a history that goes back to Hellenistic times when "leading musicians and actors" performed widely at "poetical competitions", for example at "Delphi, Delos, Ephesus". Aristotle and his teacher Plato both wrote on the theory and purpose of theatre. Aristotle posed questions such as "What is the function of the arts in shaping character? Should a member of the ruling class merely watch performances or be a participant and perform? What kind of entertainment should be provided for those who do not belong to the elite?" The "Ptolemys in Egypt, the Seleucids in Pergamum" also had a strong theatrical tradition and later, wealthy patrons in Rome staged "far more lavish productions".

Expectations about the performance and their engagement with it have changed over time (1). For example, in England during the 18th century, "the prejudice against actresses had faded" and in Europe generally, going to the theatre, once a socially dubious activity, became "a more respectable middle-class pastime" in the late 19th and early 20th centuries, when the variety of popular entertainments increased. Operetta and music halls became available, and new drama theatres such as the Moscow Art Theatre and the Suvorin Theatre in Russia opened. At the same time, commercial newspapers "began to carry theatre columns and reviews" that helped make theatre "a legitimate subject of intellectual debate" in general discussions about art and culture. Audiences began to gather to "appreciate creative achievement, to marvel at, and be entertained by, the prominent 'stars'." Vaudeville and music halls, popular at this time in the United States, England, Canada, Australia and New Zealand, were themselves eventually superseded.

Plays, musicals, monologues, pantomimes, and performance poetry are part of the very long history of theatre, which is also the venue for the type of performance known as stand-up comedy. In the 20th century, radio and television, often broadcast live, extended the theatrical tradition that continued to exist alongside the new forms.

The stage and the spaces set out in front of it for an audience create a theatre. All types of stage are used with all types of seating for the audience, including the impromptu or improvised (2, 3, 6); the temporary (2); the elaborate (9); or the traditional and permanent (5, 7). They are erected indoors (3, 5, 9) or outdoors (2, 4, 6). The skill of managing, organising and preparing the stage for a performance is known as stagecraft (10). The audience's experience of the entertainment is affected by their expectations, the stagecraft, the type of stage, and the type and standard of seating provided.

Films are a major form of entertainment, although not all films have entertainment as their primary purpose: documentary film, for example, aims to create a record or inform, although the two purposes often work together. The medium was a global business from the beginning: "The Lumière brothers were the first to send cameramen throughout the world, instructing them to film everything which could be of interest for the public." In 1908, Pathé launched and distributed newsreels and by World War I, films were meeting an enormous need for mass entertainment. "In the first decade of the [20th] century cinematic programmes combined, at random, fictions and newsfilms." The Americans first "contrived a way of producing an illusion of motion through successive images," but "the French were able to transform a scientific principle into a commercially lucrative spectacle". Film therefore became a part of the entertainment industry from its early days. Increasingly sophisticated techniques have been used in the film medium to delight and entertain audiences. Animation, for example, which involves the display of rapid movement in an art work, is one of these techniques that particularly appeals to younger audiences. The advent of computer-generated imagery (CGI) in the 21st century made it "possible to do spectacle" more cheaply and "on a scale never dreamed of" by Cecil B. DeMille. From the 1930s to 1950s, movies and radio were the "only mass entertainment" but by the second decade of the 21st century, technological changes, economic decisions, risk aversion and globalisation reduced both the quality and range of films being produced. Sophisticated visual effects and CGI techniques, for example, rather than humans, were used not only to create realistic images of people, landscapes and events (both real and fantastic) but also to animate non-living items such as Lego normally used as entertainment as a game in physical form. Creators of "The Lego Movie" "wanted the audience to believe they were looking at actual Lego bricks on a tabletop that were shot with a real camera, not what we actually did, which was create vast environments with digital bricks inside the computer." The convergence of computers and film has allowed entertainment to be presented in a new way and the technology has also allowed for those with the personal resources to screen films in a home theatre, recreating in a private venue the quality and experience of a public theatre. This is similar to the way that the nobility in earlier times could stage private musical performances or the use of domestic theatres in large homes to perform private plays in earlier centuries.

Films also re-imagine entertainment from other forms, turning stories, books and plays, for example, into new entertainments. "", a documentary about the history of film, gives a survey of global achievements and innovations in the medium, as well as changes in the conception of film-making. It demonstrates that while some films, particularly those in the Hollywood tradition that combines "realism and melodramatic romanticism", are intended as a form of escapism, others require a deeper engagement or more thoughtful response from their audiences. For example, the award-winning Senegalese film "Xala" takes government corruption as its theme. Charlie Chaplin's film "The Great Dictator" was a brave and innovative parody, also on a political theme. Stories that are thousands of years old, such as "Noah", have been re-interpreted in film, applying familiar literary devices such as allegory and personification with new techniques such as CGI to explore big themes such as "human folly", good and evil, courage and despair, love, faith, and death themes that have been a main-stay of entertainment across all its forms.

As in other media, excellence and achievement in films is recognised through a range of awards, including ones from the American Academy of Motion Picture Arts and Sciences, the British Academy of Film and Television Arts, the Cannes International Film Festival in France and the Asia Pacific Screen Awards.

The many forms of dance provide entertainment for all age groups and cultures. Dance can be serious in tone, such as when it is used to express a culture's history or important stories; it may be provocative; or it may put in the service of comedy. Since it combines many forms of entertainment music, movement, storytelling, theatre it provides a good example of the various ways that these forms can be combined to create entertainment for different purposes and audiences.

Dance is "a form of cultural representation" that involves not just dancers, but "choreographers, audience members, patrons and impresarios ... coming from all over the globe and from vastly varied time periods." Whether from Africa, Asia or Europe, dance is constantly negotiating the realms of political, social, spiritual and artistic influence." Even though dance traditions may be limited to one cultural group, they all develop. For example, in Africa, there are "Dahomean dances, Hausa dances, Masai dances and so forth." Ballet is an example of a highly developed Western form of dance that moved to the theatres from the French court during the time of Louis XIV, the dancers becoming professional theatrical performers. Some dances, such as the quadrille, a square dance that "emerged during the Napoleonic years in France" and other country dances were once popular at social gatherings like balls, but are now rarely performed. On the other hand, many folk dances (such as Scottish Highland dancing and Irish dancing), have evolved into competitions, which by adding to their audiences, has increased their entertainment value. "Irish dance theatre, which sometimes features traditional Irish steps and music, has developed into a major dance form with an international reputation."

Since dance is often "associated with the female body and women's experiences", female dancers, who dance to entertain, have in some cases been regarded as distinct from "decent" women because they "use their bodies to make a living instead of hiding them as much as possible". Society's attitudes to female dancers depend on the culture, its history and the entertainment industry itself. For example, while some cultures regard any dancing by women as "the most shameful form of entertainment", other cultures have established venues such as strip clubs where deliberately erotic or sexually provocative dances such as striptease are performed in public by professional women dancers for mostly male audiences.

Various political regimes have sought to control or ban dancing or specific types of dancing, sometimes because of disapproval of the music or clothes associated with it. Nationalism, authoritarianism and racism have played a part in banning dances or dancing. For example, during the Nazi regime, American dances such as swing, regarded as "completely un-German", had "become a public offense and needed to be banned". Similarly, in Shanghai, China, in the 1930s, "dancing and nightclubs had come to symbolise the excess that plagued Chinese society" and officials wondered if "other forms of entertainment such as brothels" should also be banned. Banning had the effect of making "the dance craze" even greater. In Ireland, the Public Dance Hall Act of 1935 "banned but did not stop dancing at the crossroads and other popular dance forms such as house and barn dances." In the US, various dances were once banned, either because like burlesque, they were suggestive, or because, like the Twist, they were associated with African Americans. "African American dancers were typically banned from performing in minstrel shows until after the Civil War."

Dances can be performed solo (1, 4); in pairs, (2, 3); in groups, (5, 6, 7); or by massed performers (10). They might be improvised (4, 8) or highly choreographed (1, 2, 5, 10); spontaneous for personal entertainment, (such as when children begin dancing for themselves); a private audience, (4); a paying audience (2); a world audience (10); or an audience interested in a particular dance genre (3, 5). They might be a part of a celebration, such as a wedding or New Year (6, 8); or a cultural ritual with a specific purpose, such as a dance by warriors like a haka (7). Some dances, such as traditional dance in 1 and ballet in 2, need a very high level of skill and training; others, such as the can-can, require a very high level of energy and physical fitness. Entertaining the audience is a normal part of dance but its physicality often also produces joy for the dancers themselves (9).

Animals have been used for the purposes of entertainment for millennia. They have been hunted for entertainment (as opposed to hunted for food); displayed while they hunt for prey; watched when they compete with each other; and watched while they perform a trained routine for human amusement. The Romans, for example, were entertained both by competitions involving wild animals and acts performed by trained animals. They watched as "lions and bears danced to the music of pipes and cymbals; horses were trained to kneel, bow, dance and prance ... acrobats turning handsprings over wild lions and vaulting over wild leopards." There were "violent confrontations with wild beasts" and "performances over time became more brutal and bloodier".

Animals that perform trained routines or "acts" for human entertainment include fleas in flea circuses, dolphins in dolphinaria, and monkeys doing tricks for an audience on behalf of the player of a street organ. Animals kept in zoos in ancient times were often kept there for later use in the arena as entertainment or for their entertainment value as exotica.

Many contests between animals are now regarded as sports for example, horse racing is regarded as both a sport and an important source of entertainment. Its economic impact means that it is also considered a global industry, one in which horses are carefully transported around the world to compete in races. In Australia, the horse race run on Melbourne Cup Day is a public holiday and the public regards the race as an important annual event. Like horse racing, camel racing requires human riders, while greyhound racing does not. People find it entertaining to watch animals race competitively, whether they are trained, like horses, camels or dogs, or untrained, like cockroaches.

The use of animals for entertainment is often controversial, especially the hunting of wild animals. Some contests between animals, once popular entertainment for the public, have become illegal because of the cruelty involved. Among these are blood sports such as bear-baiting, dog fighting and cockfighting. Other contests involving animals remain controversial and have both supporters and detractors. For example, the conflict between opponents of pigeon shooting who view it as "a cruel and moronic exercise in marksmanship, and proponents, who view it as entertainment" has been tested in a court of law. Fox hunting, which involves the use of horses as well as hounds, and bullfighting, which has a strong theatrical component, are two entertainments that have a long and significant cultural history. They both involve animals and are variously regarded as sport, entertainment or cultural tradition. Among the organisations set up to advocate for the rights of animals are some whose concerns include the use of animals for entertainment. However, "in many cases of animal advocacy groups versus organisations accused of animal abuse, both sides have cultural claims."

A circus, described as "one of the most brazen of entertainment forms", is a special type of theatrical performance, involving a variety of physical skills such as acrobatics and juggling and sometimes performing animals. Usually thought of as a travelling show performed in a big top, circus was first performed in permanent venues. Philip Astley is regarded as the founder of the modern circus in the second half of the 18th century and Jules Léotard is the French performer credited with developing the art of the trapeze, considered synonymous with circuses. Astley brought together performances that were generally familiar in traditional British fairs "at least since the beginning of the 17th century": "tumbling, rope-dancing, juggling, animal tricks and so on". It has been claimed that "there is no direct link between the Roman circus and the circus of modern times. ... Between the demise of the Roman 'circus' and the foundation of Astley's Amphitheatre in London some 1300 years later, the nearest thing to a circus ring was the rough circle formed by the curious onlookers who gathered around the itinerant tumbler or juggler on a village green."

The form of entertainment known as stage magic or conjuring and recognisable as performance, is based on traditions and texts of magical rites and dogmas that have been a part of most cultural traditions since ancient times. (References to magic, for example, can be found in the Bible, in Hermeticism, in Zoroastrianism, in the Kabbalistic tradition, in mysticism and in the sources of Freemasonry.)

Stage magic is performed for an audience in a variety of media and locations: on stage, on television, in the street, and live at parties or events. It is often combined with other forms of entertainment, such as comedy or music and showmanship is often an essential part of magic performances. Performance magic relies on deception, psychological manipulation, sleight of hand and other forms of trickery to give an audience the illusion that a performer can achieve the impossible. Audiences amazed at the stunt performances and escape acts of Harry Houdini, for example, regarded him as a magician.

Fantasy magicians have held an important place in literature for centuries, offering entertainment to millions of readers. Famous wizards such as Merlin in the Arthurian legends have been written about since the 5th and 6th centuries, while in the 21st century, the young wizard Harry Potter became a global entertainment phenomenon when the book series about him sold about 450 million copies (as at June 2011), making it the best-selling book series in history.

Street entertainment, street performance or "busking" are forms of performance that have been meeting the public's need for entertainment for centuries. It was "an integral aspect of London's life", for example, when the city in the early 19th century was "filled with spectacle and diversion". Minstrels or troubadours are part of the tradition. The art and practice of busking is still celebrated at annual busking festivals.

There are three basic forms of contemporary street performance. The first form is the "circle show". It tends to gather a crowd, usually has a distinct beginning and end, and is done in conjunction with street theatre, puppeteering, magicians, comedians, acrobats, jugglers and sometimes musicians. This type has the potential to be the most lucrative for the performer because there are likely to be more donations from larger audiences if they are entertained by the act. Good buskers control the crowd so patrons do not obstruct foot traffic. The second form, the "walk-by act", has no distinct beginning or end. Typically, the busker provides an entertaining ambience, often with an unusual instrument, and the audience may not stop to watch or form a crowd. Sometimes a walk-by act spontaneously turns into a circle show. The third form, "café busking", is performed mostly in restaurants, pubs, bars and cafés. This type of act occasionally uses public transport as a venue.

Parades are held for a range of purposes, often more than one. Whether their mood is sombre or festive, being public events that are designed to attract attention and activities that necessarily divert normal traffic, parades have a clear entertainment value to their audiences. Cavalcades and the modern variant, the motorcade, are examples of public processions. Some people watching the parade or procession may have made a special effort to attend, while others become part of the audience by happenstance. Whatever their mood or primary purpose, parades attract and entertain people who watch them pass by. Occasionally, a parade takes place in an improvised theatre space (such as the Trooping the Colour in 8) and tickets are sold to the physical audience while the global audience participates via broadcast.

One of the earliest forms of parade were "triumphs" grand and sensational displays of foreign treasures and spoils, given by triumphant Roman generals to celebrate their victories. They presented conquered peoples and nations that exalted the prestige of the victor. "In the summer of 46 BCE Julius Caesar chose to celebrate four triumphs held on different days extending for about one month." In Europe from the Middle Ages to the Baroque the Royal Entry celebrated the formal visit of the monarch to the city with a parade through elaborately decorated streets, passing various shows and displays. The annual Lord Mayor's Show in London is an example of a civic parade that has survived since medieval times.

Many religious festivals (especially those that incorporate processions, such as Holy Week processions or the Indian festival of Holi) have some entertainment appeal in addition to their serious purpose. Sometimes, religious rituals have been adapted or evolved into secular entertainments, or like the Festa del Redentore in Venice, have managed to grow in popularity while holding both secular and sacred purposes in balance. However, pilgrimages, such as the Roman Catholic pilgrimage of the Way of St. James, the Muslim Hajj and the Hindu Kumbh Mela, which may appear to the outsider as an entertaining parade or procession, are not intended as entertainment: they are instead about an individual's spiritual journey. Hence, the relationship between spectator and participant, unlike entertainments proper, is different. The manner in which the Kumbh Mela, for example, "is divorced from its cultural context and repackaged for Western consumption renders the presence of voyeurs deeply problematic."

Parades generally impress and delight often by including unusual, colourful costumes (7, 10). Sometimes they also commemorate (5, 8) or celebrate (1, 4, 6, 8, 9). Sometimes they have a serious purpose, such as when the context is military (1, 2, 5), when the intention is sometimes to intimidate; or religious, when the audience might participate or have a role to play (6, 7, 10). Even if a parade uses new technology and is some distance away (9), it is likely to have a strong appeal, draw the attention of onlookers and entertain them.

Fireworks are a part of many public entertainments and have retained an enduring popularity since they became a "crowning feature of elaborate celebrations" in the 17th century. First used in China, classical antiquity and Europe for military purposes, fireworks were most popular in the 18th century and high prices were paid for pyrotechnists, especially the skilled Italian ones, who were summoned to other countries to organise displays. Fire and water were important aspects of court spectacles because the displays "inspired by means of fire, sudden noise, smoke and general magnificence the sentiments thought fitting for the subject to entertain of his sovereign: awe fear and a vicarious sense of glory in his might. Birthdays, name-days, weddings and anniversaries provided the occasion for celebration." One of the most famous courtly uses of fireworks was one used to celebrate the end of the War of the Austrian Succession and while the fireworks themselves caused a fire, the accompanying Music for the Royal Fireworks written by Handel has been popular ever since. Aside from their contribution to entertainments related to military successes, courtly displays and personal celebrations, fireworks are also used as part of religious ceremony. For example, during the Indian Dashavatara Kala of Gomantaka "the temple deity is taken around in a procession with a lot of singing, dancing and display of fireworks".

The "fire, sudden noise and smoke" of fireworks is still a significant part of public celebration and entertainment. For example, fireworks were one of the primary forms of display chosen to celebrate the turn of the millennium around the world. As the clock struck midnight and 1999 became 2000, firework displays and open-air parties greeted the New Year as the time zones changed over to the next century. Fireworks, carefully planned and choreographed, were let off against the backdrop of many of the world's most famous buildings, including the Sydney Harbour Bridge, the Pyramids of Giza in Egypt, the Acropolis in Athens, Red Square in Moscow, Vatican City in Rome, the Brandenburg Gate in Berlin, the Eiffel Tower in Paris, and Elizabeth Tower in London.

Sporting competitions have always provided entertainment for crowds. To distinguish the players from the audience, the latter are often known as spectators. Developments in stadium and auditorium design, as well as in recording and broadcast technology, have allowed off-site spectators to watch sport, with the result that the size of the audience has grown ever larger and spectator sport has become increasingly popular. Two of the most popular sports with global appeal are association football and cricket. Their ultimate international competitions, the World Cup and test cricket, are broadcast around the world. Beyond the very large numbers involved in playing these sports, they are notable for being a major source of entertainment for many millions of non-players worldwide. A comparable multi-stage, long-form sport with global appeal is the Tour de France, unusual in that it takes place outside of special stadia, being run instead in the countryside.

Aside from sports that have worldwide appeal and competitions, such as the Olympic Games, the entertainment value of a sport depends on the culture and country where people play it. For example, in the United States, baseball and basketball games are popular forms of entertainment; in Bhutan, the national sport is archery; in New Zealand, it is rugby union; in Iran, it is freestyle wrestling. Japan's unique sumo wrestling contains ritual elements that derive from its long history. In some cases, such as the international running group Hash House Harriers, participants create a blend of sport and entertainment for themselves, largely independent of spectator involvement, where the social component is more important than the competitive.

The evolution of an activity into a sport and then an entertainment is also affected by the local climate and conditions. For example, the modern sport of surfing is associated with Hawaii and that of snow skiing probably evolved in Scandinavia. While these sports and the entertainment they offer to spectators have spread around the world, people in the two originating countries remain well known for their prowess. Sometimes the climate offers a chance to adapt another sport such as in the case of ice hockey—an important entertainment in Canada.

Fairs and exhibitions have existed since ancient and medieval times, displaying wealth, innovations and objects for trade and offering specific entertainments as well as being places of entertainment in themselves. Whether in a medieval market or a small shop, "shopping always offered forms of exhilaration that took one away from the everyday". However, in the modern world, "merchandising has become entertainment: spinning signs, flashing signs, thumping music ... video screens, interactive computer kiosks, day care .. cafés".

By the 19th century, "expos" that encouraged arts, manufactures and commerce had become international. They were not only hugely popular but affected international ideas. For example, the 1878 Paris Exposition facilitated international cooperation about ideas, innovations and standards. From London 1851 to Paris 1900, "in excess of 200 million visitors had entered the turnstiles in London, Paris, Vienna, Philadelphia, Chicago and a myriad of smaller shows around the world." Since World War II "well over 500 million visits have been recorded through world expo turnstiles". As a form of spectacle and entertainment, expositions influenced "everything from architecture, to patterns of globalisation, to fundamental matters of human identity" and in the process established the close relationship between "fairs, the rise of department stores and art museums", the modern world of mass consumption and the entertainment industry.

Some entertainments, such as at large festivals (whether religious or secular), concerts, clubs, parties and celebrations, involve big crowds. From earliest times, crowds at an entertainment have associated hazards and dangers, especially when combined with the recreational consumption of intoxicants such as alcohol. The Ancient Greeks had Dionysian Mysteries, for example, and the Romans had Saturnalia. The consequence of excess and crowds can produce breaches of social norms of behaviour, sometimes causing injury or even death, such as for example, at the Altamont Free Concert, an outdoor rock festival. The list of serious incidents at nightclubs includes those caused by stampede; overcrowding; terrorism, such as the 2002 Bali bombings that targeted a nightclub; and especially fire. Investigations, such as that carried out in the US after The Station nightclub fire often demonstrate that lessons learned "regarding fire safety in nightclubs" from earlier events such as the Cocoanut Grove fire do "not necessarily result in lasting effective change". Efforts to prevent such incidents include appointing special officers, such as the medieval Lord of Misrule or, in modern times, security officers who control access; and also ongoing improvement of relevant standards such as those for building safety. The tourism industry now regards safety and security at entertainment venues as an important management task.

Although kings, rulers and powerful people have always been able to pay for entertainment to be provided for them and in many cases have paid for public entertainment, people generally have made their own entertainment or when possible, attended a live performance. Technological developments in the 20th century meant that entertainment could be produced independently of the audience, packaged and sold on a commercial basis by an entertainment industry. Sometimes referred to as show business, the industry relies on business models to produce, market, broadcast or otherwise distribute many of its traditional forms, including performances of all types. The industry became so sophisticated that its economics became a separate area of academic study.

The film industry is a part of the entertainment industry. Components of it include the Hollywood and Bollywood film industries, as well as the cinema of the United Kingdom and all the cinemas of Europe, including France, Germany, Spain, Italy and others. The sex industry is another component of the entertainment industry, applying the same forms and media (for example, film, books, dance and other performances) to the development, marketing and sale of sex products on a commercial basis.

Amusement parks entertain paying guests with rides, such as roller coasters, ridable miniature railways, water rides, and dark rides, as well as other events and associated attractions. The parks are built on a large area subdivided into themed areas named "lands". Sometimes the whole amusement park is based on one theme, such as the various SeaWorld parks that focus on the theme of sea life.

One of the consequences of the development of the entertainment industry has been the creation of new types of employment. While jobs such as writer, musician and composer exist as they always have, people doing this work are likely to be employed by a company rather than a patron as they once would have been. New jobs have appeared, such as gaffer or special effects supervisor in the film industry, and attendants in an amusement park.

Prestigious awards are given by the industry for excellence in the various types of entertainment. For example, there are awards for Music, Games (including video games), Comics, Comedy, Theatre, Television, Film, Dance and Magic. Sporting awards are made for the results and skill, rather than for the entertainment value.

Purpose-built structures as venues for entertainment that accommodate audiences have produced many famous and innovative buildings, among the most recognisable of which are theatre structures. For the ancient Greeks, "the architectural importance of the theatre is a reflection of their importance to the community, made apparent in their monumentality, in the effort put into their design, and in the care put into their detail." The Romans subsequently developed the stadium in an oval form known as a circus. In modern times, some of the grandest buildings for entertainment have brought fame to their cities as well as their designers. The Sydney Opera House, for example, is a World Heritage Site and The O₂ in London is an entertainment precinct that contains an indoor arena, a music club, a cinema and exhibition space. The Bayreuth Festspielhaus in Germany is a theatre designed and built for performances of one specific musical composition.

Two of the chief architectural concerns for the design of venues for mass audiences are speed of egress and safety. The speed at which the venue empty is important both for amenity and safety, because large crowds take a long time to disperse from a badly designed venue, which creates a safety risk. The Hillsborough disaster is an example of how poor aspects of building design can contribute to audience deaths. Sightlines and acoustics are also important design considerations in most theatrical venues.

In the 21st century, entertainment venues, especially stadia, are "likely to figure among the leading architectural genres". However, they require "a whole new approach" to design, because they need to be "sophisticated entertainment centres, multi-experience venues, capable of being enjoyed in many diverse ways". Hence, architects now have to design "with two distinct functions in mind, as sports and entertainment centres playing host to live audiences, and as sports and entertainment studios serving the viewing and listening requirements of the remote audience".

Architects who push the boundaries of design or construction sometimes create buildings that are entertaining because they exceed the expectations of the public and the client and are aesthetically outstanding. Buildings such as Guggenheim Museum Bilbao, designed by Frank Gehry, are of this type, becoming a tourist attraction as well as a significant international museum. Other apparently usable buildings are really follies, deliberately constructed for a decorative purpose and never intended to be practical.

On the other hand, sometimes architecture is entertainment, while pretending to be functional. The tourism industry, for example, creates or renovates buildings as "attractions" that have either never been used or can never be used for their ostensible purpose. They are instead re-purposed to entertain visitors often by simulating cultural experiences. Buildings, history and sacred spaces are thus made into commodities for purchase. Such intentional tourist attractions divorce buildings from the past so that "the difference between historical authenticity and contemporary entertainment venues/theme parks becomes hard to define". Examples include "the preservation of the Alcázar of Toledo, with its grim Civil War History, the conversion of slave dungeons into tourist attractions in Ghana, [such as, for example, Cape Coast Castle] and the presentation of indigenous culture in Libya". The specially constructed buildings in amusement parks represent the park's theme and are usually neither authentic nor completely functional.

By the second half of the 20th century, developments in electronic media made possible the delivery of entertainment products to mass audiences across the globe. The technology enabled people to see, hear and participate in all the familiar forms stories, theatre, music, dance wherever they live. The rapid development of entertainment technology was assisted by improvements in data storage devices such as cassette tapes or compact discs, along with increasing miniaturisation. Computerisation and the development of barcodes also made ticketing easier, faster and global.

In the 1940s, radio was the electronic medium for family entertainment and information. In the 1950s, it was television that was the new medium and it rapidly became global, bringing visual entertainment, first in black and white, then in colour, to the world. By the 1970s, games could be played electronically, then hand-held devices provided mobile entertainment, and by the last decade of the 20th century, via networked play. In combination with products from the entertainment industry, all the traditional forms of entertainment became available personally. People could not only select an entertainment product such as a piece of music, film or game, they could choose the time and place to use it. The "proliferation of portable media players and the emphasis on the computer as a site for film consumption" together have significantly changed how audiences encounter films. One of the most notable consequences of the rise of electronic entertainment has been the rapid obsolescence of the various recording and storage methods. As an example of speed of change driven by electronic media, over the course of one generation, television as a medium for receiving standardised entertainment products went from unknown, to novel, to ubiquitous and finally to superseded. One estimate was that by 2011 over 30 percent of households in the US would own a Wii console, "about the same percentage that owned a television in 1953". Some expected that halfway through the second decade of the 21st century, online entertainment would have completely replaced television—which didn't happen. The so-called "digital revolution" has produced an increasingly transnational marketplace that has caused difficulties for governments, business, industries, and individuals, as they all try to keep up. Even the sports stadium of the future will increasingly compete with television viewing "...in terms of comfort, safety and the constant flow of audio-visual information and entertainment available." Other flow on effects of the shift are likely to include those on public architecture such as hospitals and nursing homes, where television, regarded as an essential entertainment service for patients and residents, will need to be replaced by access to the internet. At the same time, the ongoing need for entertainers as "professional engagers" shows the continuity of traditional entertainment.

By the second decade of the 21st century, analogue recording was being replaced by digital recording and all forms of electronic entertainment began to converge. For example, convergence is challenging standard practices in the film industry: whereas "success or failure used to be determined by the first weekend of its run. Today, ... a series of exhibition 'windows', such as DVD, pay-per-view, and fibre-optic video-on-demand are used to maximise profits." Part of the industry's adjustment is its release of new commercial product directly via video hosting services. Media convergence is said to be more than technological: the convergence is cultural as well. It is also "the result of a deliberate effort to protect the interests of business entities, policy institutions and other groups". Globalisation and cultural imperialism are two of the cultural consequences of convergence. Others include fandom and interactive storytelling as well as the way that single franchises are distributed through and affect a range of delivery methods. The "greater diversity in the ways that signals may be received and packaged for the viewer, via terrestrial, satellite or cable television, and of course, via the Internet" also affects entertainment venues, such as sports stadia, which now need to be designed so that both live and remote audiences can interact in increasingly sophisticated ways for example, audiences can "watch highlights, call up statistics", "order tickets and merchandise" and generally "tap into the stadium's resources at any time of the day or night".

The introduction of television altered the availability, cost, variety and quality of entertainment products for the public and the convergence of online entertainment is having a similar effect. For example, the possibility and popularity of user-generated content, as distinct from commercial product, creates a "networked audience model [that] makes programming obsolete". Individuals and corporations use video hosting services to broadcast content that is equally accepted by the public as legitimate entertainment.

While technology increases demand for entertainment products and offers increased speed of delivery, the forms that make up the content are in themselves, relatively stable. Storytelling, music, theatre, dance and games are recognisably the same as in earlier centuries.



</doc>
<doc id="2062017" url="https://en.wikipedia.org/wiki?curid=2062017" title="Induction puzzles">
Induction puzzles

Induction puzzles are logic puzzles, which are examples of multi-agent reasoning, where the solution evolves along with the principle of induction.

A puzzle's scenario always involves multiple players with the same reasoning capability, who go through the same reasoning steps. According to principle of induction, a solution to the simplest case makes the solution of the next complicated case obvious. Once the simplest case of the induction puzzle is solved, the whole puzzle is solved subsequently. 

Typical tell-tale features of these puzzles include any puzzle in which each participant has a given piece of information about all other participants but not themselves. Also, usually, some kind of hint is given to suggest that the participants can trust each other's intelligence — they are capable of theory of mind.

Muddy children puzzle is the most frequently appearing induction puzzle in scientific literature on epistemic logic. In February 2020, there have been 437 hits on Google scholar mentioning muddy children puzzle. Muddy children puzzle is a variant of the well known wise men or cheating wives/husbands puzzles.

Hat puzzles are induction puzzle variations that date back to as early as 1961. In many variations, hat puzzles are described in the context of prisoners. In other cases, hat puzzles are described in the context of wise men.

There is a set of attentive children. They think perfectly logical. The children consider it possible to have a muddy face. None of the children can determine the state of his or her face him- or herself. But, every child knows the state of all other children's faces. A custodian tells the children that at least one of them has a muddy face. Hereafter, the custodian starts to count and after every stroke, every muddy child has an opportunity to step forward.

Let's assume that there just 2 children: Alice and Bob! If only Alice is dirty, she will step forward at the first stroke, because she does not see any other dirty faces. The same is for Bob. If Alice sees Bob not stepping forward at the first stroke, she must conclude that he certainly sees other muddy child and they will step forward simultaneously at the second stroke.

Let's assume that there just 3 children: Alice, Bob, and Charly! If there are less than 3 muddy children, the puzzle evolves like in the case with 2 children. If Charly sees that Alice and Bob are muddy and not stepping forward at the second stoke, they all together will step forward at the third stoke.

It can be proven that formula_1 muddy children will step forward after formula_1 strokes.

Muddy children puzzle can also be solved using the means of game theory. Muddy children puzzle can be represented as a extensive form game of imperfect information. Every player has two actions — stay back and step forwards. There is a move by nature at the start of the game, which determines the childen with and without muddy faces. Children do not communicate as in non-cooperative games. Every stroke is a simultaneous move by children. It is a sequential game of unlimited length. The game-theoretic solution needs some additional assumptions:

If only Alice is muddy, the last assumption makes it irrational for her to hesitate. If Alice and Bob are muddy, Alice knows that Bob's only reason of staying back after the first stroke is the apprehension to receive the big penalty of stepping forward without a muddy face. In the case with formula_1 muddy children, receiving formula_1 times the minor penalty is still better than the big penalty.

The King called the three wisest men in the country to his court to decide who would become his new advisor. He placed a hat on each of their heads, such that each wise man could see all of the other hats, but none of them could see their own. Each hat was either white or blue. The king gave his word to the wise men that at least one of them was wearing a blue hat; in other words, there could be one, two, or three blue hats, but not zero. The king also announced that the contest would be fair to all three men. The wise men were also forbidden to speak to each other. The king declared that whichever man stood up first and correctly announced the colour of his own hat would become his new advisor. The wise men sat for a very long time before one stood up and correctly announced the answer. What did he say, and how did he work it out?

The King's Wise Men is one of the simplest induction puzzles and one of the clearest indicators to the method used.


Since there must be three blue hats, the first man to figure that out will stand up and say blue.

Alternative solution: This does not require the rule that the contest be fair to each. Rather it relies on the fact that they are all wise men, and that it takes some time before they arrive at a solution.
There can only be 3 scenarios, one blue hat, two blue hats or 3 blue hats. If there was only one blue hat, then the wearer of that hat would see two white hats, and quickly know that he has to have a blue hat, so he would stand up and announce this straight away. Since this hasn't happened, then there must be at least two blue hats. If there were two blue hats, than either one of those wearing a blue hat would look across and see one blue hat and one white hat, but not know the colour of their own hat. If the first wearer of the blue hat assumed he had a white hat, he would know that the other wearer of the blue hat would be seeing two white hats, and thus the 2nd wearer of the blue hat would have already stood up and announced he was wearing a blue hat. Thus, since this hasn't happened, the first wearer of the blue hat would know he was wearing a blue hat, and could stand up and announce this. Since either one or two blue hats is so easy to solve, and that no one has stood up quickly, then they must all be wearing blue hats.

In Josephine's Kingdom every woman has to pass a logic exam before being allowed to marry. Every married woman knows about the fidelity of every man in the Kingdom "except" for her own husband, and etiquette demands that no woman should be told about the fidelity of her husband. Also, a gunshot fired in any house in the Kingdom will be heard in any other house. Queen Josephine announced that at least one unfaithful man had been discovered in the Kingdom, and that any woman knowing her husband to be unfaithful was required to shoot him at midnight following the day after she discovered his infidelity. How did the wives manage this?

Josephine's Problem is another good example of a general case.


This problem is also known as the Cheating Husbands Problem, the Unfaithful Wives Problem, the Muddy Children Problem. It is logically identical to the Blue Eyes Problem.

This problem also appears as a problem involving black hats and white hats in C. L. Liu's classic textbook 'Elements of Discrete Mathematics'.

At the Secret Convention of Logicians, the Master Logician placed a band on each attendee's head, such that everyone else could see it but the person themselves could not. There were many different colours of band. The Logicians all sat in a circle, and the Master instructed them that a bell was to be rung in the forest at regular intervals: at the moment when a Logician knew the colour on his own forehead, he was to leave at the next bell. They were instructed not to speak, nor to use a mirror or camera or otherwise avoid using logic to determine their band colour. In case any impostors had infiltrated the convention, anyone failing to leave on time would be gruffly removed at the correct time. Similarly, anyone trying to leave early would be gruffly held in place and removed at the correct time. The Master reassured the group by stating that the puzzle would not be impossible for any True Logician present. How did they do it?

Alice at the convention of Logicians is general induction plus a leap of logic.


A number of players are each wearing a hat, which may be of various specified colours. Players can see the colours of at least some other players' hats, but not that of their own. With highly restricted communication or none, some of the players must guess the colour of their hat. The problem is to find a strategy for the players to determine the colours of their hats based on the hats they see and what the other players do. In some versions, they compete to be the first to guess correctly; in others, they can work out a strategy beforehand to cooperate and maximize the probability of correct guesses.

One variation received some new publicity as a result of Todd Ebert's 1998 Ph.D. thesis at the University of California, Santa Barbara. It is a strategy question about a cooperative game, which has connections to algebraic coding theory.

Three players are told that each of them will receive either a red hat or a blue hat. They are to raise their hands if they see a red hat on another player as they stand in a circle facing each other. The first to guess the colour of his or her hat correctly wins.

All three players raise their hands. After the players have seen each other for a few minutes without guessing, one player announces "Red", and wins. How did the winner do it, and what is the color of everyone's hats?

First, if two people had blue hats, not everyone's hand would have been raised. Next, if player 1 had seen a blue hat on player 2 & a red hat on player 3, then player 1 would have known immediately that his own hat must be red. Thus any player who sees a blue hat can guess at once. Finally, the winner realizes that since no one guesses at once, there must be no blue hats, so every hat must be red.

In the case where every player has to make a guess, but they are free to choose when to guess, there is a cooperative strategy that allows every player to guess correctly unless all the hats are the same colour. Each player should act as follows:


Suppose that in total there are "B" black hats and "W" white hats. There are three cases.

If "B" = "W" then those players wearing black hats see "B−1" black hats and "B" white hats, so wait "B"−1 seconds then correctly guess that they are wearing a black hat. Similarly, those players wearing a white hat will wait "W"−1 seconds before guessing correctly that they are wearing a white hat. So all players make a correct guess at the same time.

If "B" < "W" then those wearing a black hat will see "B"−1 black hats and "W" white hats, whilst those wearing a white hat will see "B" black hats and "W"−1 white hats. Since "B"−1 < "B" ≤ "W"−1, those players wearing a black hat will be the first to speak, guessing correctly that their hat is black. The other players then guess correctly that their hat is white.

The case where "W" < "B" is similar.

According to the story, four prisoners are arrested for a crime, but the jail is full and the jailer has nowhere to put them. He eventually comes up with the solution of giving them a puzzle so if they succeed they can go free but if they fail they are executed.

The jailer seats three of the men into a line. B faces the wall, C faces B, and D faces C and B. The fourth man, A, is put behind a screen (or in a separate room). The jailer gives all four men party hats. He explains that there are two black hats and two white hats, that each prisoner is wearing one of the hats, and that each of the prisoners see only the hats in front of him but neither on himself nor behind him. The fourth man behind the screen can't see or be seen by any other prisoner. No communication among the prisoners is allowed.

If any prisoner can figure out what color hat he has on his own head with 100% certainty (without guessing) he must then announce it, "and all four prisoners go free". If any prisoner suggests an incorrect answer, all four prisoners are executed. The puzzle is to find how the prisoners can escape.

The prisoners know that there are only two hats of each color. So if D observes that B and C have hats of the same color, D would deduce that his own hat is the opposite color. However, if B and C have hats of different colors, then D can say nothing. The key is that prisoner C, after allowing an appropriate interval, and knowing what D would do, can deduce that if D says nothing the hats on B and C must be different; able to see B's hat, he can deduce his own hat color.

In common with many puzzles of this type, the solution relies upon the assumption that all participants are totally rational and intelligent enough to make the appropriate deductions.

After solving this puzzle, some insight into the nature of communication can be gained by pondering whether the meaningful silence of prisoner D violates the "No communication" rule (given that communication is usually defined as the "transfer of information").

In this variant there are 3 prisoners and 3 hats. Each prisoner is assigned a random hat, either red or blue. In all, there are three red hats and two blue. Each person can see the hats of two others, but not their own. On a cue, they each have to guess their own hat color or pass. They win release if at least one person guessed correctly and none guessed incorrectly (passing is neither correct nor incorrect).

This puzzle doesn't have a 100% winning strategy, so the question is: What is the best strategy? Which strategy has the highest probability of winning?

If you think of colors of hats as bits, this problem has some important applications in coding theory.

The solution and the discussion of this puzzle can be found here (also a solution to the analogous 7-hat puzzle) and other 3 variants are available on this Logic Puzzles page (they are called Masters of Logic I-IV).

In a variant of this puzzle, the prisoners know that there are 3 hats of one color and only 1 hat of another (e.g. 3 black and 1 white), and the 3 prisoners can see each other i.e. D sees B & C, B sees D & C, and C sees D & B. (A again cannot be seen and is only there to wear the last hat.)

There are two cases: in the trivial case, one of the three prisoners wears the single off-color hat. Each of the other two prisoners can see that one prisoner is wearing the off-color hat.
In the non-trivial case, the three prisoners wear hats of the same color, while A wears the off-color hat.
After a while, all three prisoners should be able to deduce that, because neither of the others was able to state the color of his own hat, A must wear the off-color hat.

In another variant, only three prisoners and five hats (supposedly two black and three white) are involved. 
The three prisoners are ordered to stand in a straight line facing the front, with A in front and C at the back. They are told that there will be two black hats and three white hats. One hat is then put on each prisoner's head; each prisoner can only see the hats of the people in front of him and not on his own. The first prisoner that is able to announce the color of his hat correctly will be released. No communication between the prisoners is allowed.

Assume that A wears a black hat: 
So if A wears a black hat there will be a fairly quick response from B or C.

Assume that A wears a white hat: 
In this case A, B and C would remain silent for some time, until A finally deduces that he must have a white hat because C and B have remained silent for some time.

As mentioned, there are three white hats and two black hats in total, and the three prisoners know this. In this riddle, you can assume that all three prisoners are very clever and very smart. If C could not guess the color of his own hat that is because he saw either two white hats or one of each color. If he saw two black hats, he could have deduced that he was wearing a white hat.

In this variant there are 10 prisoners and 10 hats. Each prisoner is assigned a random hat, either red or blue, but the number of each color hat is not known to the prisoners. The prisoners will be lined up single file where each can see the hats in front of him but not behind. Starting with the prisoner in the back of the line and moving forward, they must each, in turn, say only one word which must be "red" or "blue". If the word matches their hat color they are released, if not, they are killed on the spot. A sympathetic guard warns them of this test one hour beforehand and tells them that they can formulate a plan where by following the stated rules, 9 of the 10 prisoners will definitely survive, and 1 has a 50/50 chance of survival. What is the plan to achieve the goal?

The prisoners agree that if the first prisoner sees an odd number of red hats, he will say "red". This way, the nine other prisoners will know their own hat color after the prisoner behind them responds.

As before, there are 10 prisoners and 10 hats. Each prisoner is assigned a random hat, either red or blue, but the number of each color hat is not known to the prisoners. The prisoners are distributed in the room such that they can see the hats of the others but not their own. Now, they must each, simultaneously, say only one word which must be "red" or "blue". If the word matches their hat color they are released, and if enough prisoners resume their liberty they can rescue the others. A sympathetic guard warns them of this test one hour beforehand. If they can formulate a plan following the stated rules, 5 of the 10 prisoners will definitely be released and be able to rescue the others. What is the plan to achieve the goal?

The prisoners pair off. In a pair (A, B) of the prisoners A says the color he can see on the head of B, who says the opposite color he sees on the head of A. Then, if both wear hats with the same color, A is released (and B is not), if the colors are different, B is released (and A is not). In total, 5 prisoners answer correctly and 5 do not. This assumes the pair can communicate who is A and who is B, which may not be allowed.

Alternatively, the prisoners build two groups of 5. One group assumes that the number of red hats is even, the other assumes that there is an odd number of red hats. Similar to the variant with hearing, they can deduce their hat color out of this assumption. Exactly one group will be right, so 5 prisoners answer correctly and 5 do not.

Note that the prisoners cannot find a strategy guaranteeing the release of more than 5 prisoners. Indeed, for a single prisoner, there are as many distributions of hat colors where he says the correct answer than there are where he does not. Hence, there are as many distributions of hat colors where 6 or more prisoners say the correct answer than there are where 4 or fewer do so.

In this variant, a countably infinite number of prisoners, each with an unknown and randomly assigned red or blue hat line up single file line. Each prisoner faces away from the beginning of the line, and each prisoner can see all the hats in front of him, and none of the hats behind. Starting from the beginning of the line, each prisoner must correctly identify the color of his hat or he is killed on the spot. As before, the prisoners have a chance to meet beforehand, but unlike before, once in line, no prisoner can hear what the other prisoners say. The question is, is there a way to ensure that only finitely many prisoners are killed?

If one accepts the axiom of choice, and assumes the prisoners each have the (unrealistic) ability to memorize an uncountably infinite amount of information and perform computations with uncountably infinite computational complexity, the answer is yes. In fact, even if we allow an uncountable number of different colors for the hats and an uncountable number of prisoners, the axiom of choice provides a solution that guarantees that only finitely many prisoners must die provided that each prisoner can see the hats of every other prisoner (not just those ahead of them in a line), or at least that each prisoner can see all but finitely many of the other hats. The solution for the two color case is as follows, and the solution for the uncountably infinite color case is essentially the same:

The prisoners standing in line form a sequence of 0s and 1s, where 0 is taken to represent blue, and 1 is taken to represent red. Before they are put into the line, the prisoners define the following equivalence relation over all possible sequences that they might be put into: Two sequences are equivalent if they are identical after a finite number of entries. From this equivalence relation, the prisoners get a collection of equivalence classes. Assuming the axiom of choice, there exists a set of representative sequences—one from each equivalence class. (Almost every specific value is impossible to compute, but the axiom of choice implies that "some" set of values exists, so we assume that the prisoners have access to an oracle.)

When they are put into their line, each prisoner can see all but a finite number of hats, and can therefore see which equivalence class the "actual" sequence of hats belongs to. (This assumes that each prisoner can perform an "uncountably infinite" number of comparisons to find a match, with each class comparison requiring a "countably infinite" number of individual hat-comparisons). They then proceed guessing their hat color as if they were in the "representative" sequence from the appropriate equivalence class. Because the actual sequence and the representative sequence are in the same equivalence class, their entries are the same after some finite number "N" of prisoners. All prisoners after these first "N" prisoners are saved.

Because the prisoners have no information about the color of their own hat and would make the same guess whichever color it has, each prisoner has a 50% chance of being killed. It may seem paradoxical that an infinite number of prisoners each have an even chance of being killed and yet it is certain that only a finite number are killed. The solution to this paradox lies in the fact that the function employed to determine each prisoner's guess is not Measurable function.

To see this, consider the case of zero prisoners being killed. This happens if and only if the actual sequence is one of the selected representative sequences. If the sequences of 0s and 1s are viewed as binary representations of a real number between 0 and 1, the representative sequences form a non-measurable set. (This set is similar to a Vitali set, the only difference being that equivalence classes are formed with respect to numbers with finite binary representations rather than all rational numbers.) Hence no probability can be assigned to the event of zero prisoners being killed. The argument is similar for other finite numbers of prisoners being killed, corresponding to a finite number of variations of each representative.

This variant is the same as the last one except that prisoners can hear the colors called out by other prisoners. The question is, what is the optimal strategy for the prisoners such that the fewest of them die in the worst case?

It turns out that, if one allows the prisoners to hear the colors called out by the other prisoners, it is possible to guarantee the life of every prisoner except the first, who dies with a 50% probability.

To do this, we define the same equivalence relation as above and again select a representative sequence from each equivalence class. Now, we label every sequence in each class with either a 0 or a 1. First, we label the representative sequence with a 0. Then, we label any sequence which differs from the representative sequence in an even number of places with a 0, and any sequence which differs from the representative sequence in an odd number of places with a 1. In this manner, we have labeled every possible infinite sequence with a 0 or a 1 with the important property that any two sequences which differ by only one digit have opposite labels.

Now, when the warden asks the first person to say a color, or in our new interpretation, a 0 or a 1, he simply calls out the label of the sequence he sees. Given this information, everyone after him can determine exactly what his own hat color is. The second person sees all but the first digit of the sequence that the first person sees. Thus, as far as he knows, there are two possible sequences the first person could have been labeling: one starting with a 0, and one starting with a 1. Because of our labeling scheme, these two sequences would receive opposite labels, so based on what the first person says, the second person can determine which of the two possible strings the first person saw, and thus he can determine his own hat color. Similarly, every later person in the line knows every digit of the sequence except the one corresponding to his own hat color. He knows those before him because they were called out, and those after him because he can see them. With this information, he can use the label called out by the first person to determine his own hat color. Thus, everyone except the first person always guesses correctly.

Ebert's version of the problem states that all players who guess must guess at the same predetermined time, but that not all players are required to guess. Now not all players can guess correctly, so the players win if at least one player guesses and all of those who guess do so correctly. How can the players maximise their chance of winning?

One strategy for solving this version of the hat problem employs Hamming codes, which are commonly used to detect and correct errors in data transmission. The probability for winning will be much higher than 50%, depending on the number of players in the puzzle configuration: for example, a winning probability of 87.5% for 7 players.

Similar strategies can be applied to team sizes of "N" = 2−1 and achieve a win rate (2-1)/2. Thus the Hamming code strategy yields greater win rates for larger values of "N".

In this version of the problem, any individual guess has a 50% chance of being right. However, the Hamming code approach works by concentrating wrong guesses together onto certain distributions of hats. For some cases, all the players will guess incorrectly; whereas for the other cases, only one player will guess, but correctly. While half of all guesses are still incorrect, this results in the players winning more than 50% of the time.

A simple example of this type of solution with three players is instructive. With three players, there are eight possibilities; in two of them all players have the same colour hat, and in the other six, two players have one colour and the other player has the other colour.

The players can guarantee that they win in the latter cases (75% of the time) with the following strategy:

In the two cases when all three players have the same hat colour, they will all guess incorrectly. But in the other six cases, only one player will guess, and correctly, that his hat is the opposite of his fellow players'.

Sneetches are creatures from Dr. Seuss's famous story "The Sneetches". There are two types of Sneetches, star-bellied and plain-bellied. All Sneetches must pass a logic test to live in Sneetchville, which has a limited number of homes and has a strict housing law that each home must contain no more than one star-bellied Sneetch and one plain-bellied Sneetch. No Sneetch is able to see its own belly, but can still see all other Sneetches' bellies. To prevent further conflict among the Sneetches, there is a law that forbids Sneetches to discuss their bellies. Each Sneetch cannot skip a home until it is sure that it cannot move in. If a Sneetch breaks the law, it is executed. How do the Sneetches choose their homes?



</doc>
<doc id="1149904" url="https://en.wikipedia.org/wiki?curid=1149904" title="Geoarchaeology">
Geoarchaeology

Geoarchaeology is a multi-disciplinary approach which uses the techniques and subject matter of geography, geology, geophysics and other Earth sciences to examine topics which inform archaeological knowledge and thought. Geoarchaeologists study the natural physical processes that affect archaeological sites such as geomorphology, the formation of sites through geological processes and the effects on buried sites and artifacts post-deposition. Geoarchaeologists' work frequently involves studying soil and sediments as well as other geographical concepts to contribute an archaeological study. Geoarchaeologists may also use computer cartography, geographic information systems (GIS) and digital elevation models (DEM) in combination with disciplines from human and social sciences and earth sciences. Geoarchaeology is important to society because it informs archaeologists about the geomorphology of the soil, sediments and the rocks on the buried sites and artifacts they're researching on. By doing this we are able to locate ancient cities and artifacts and estimate by the quality of soil how "prehistoric" they really are.

Column sampling is a technique of collecting samples from a section for analyzing and detecting the buried processes down the profile of the section. Narrow metal tins are hammered into the section in a series to collect the complete profile for study. If more than one tin is needed they are arranged offset and overlapping to one side so the complete profile can be rebuilt offsite in laboratory conditions.

Loss on ignition testing for soil organic content – a technique of measuring organic content in soil samples. Samples taken from a known place in the profile collected by column sampling are weighed then placed in a fierce oven which burns off the organic content. The resulting cooked sample is weighed again and the resulting loss in weight is an indicator of organic content in the profile at a certain depth. These readings are often used to detect buried soil horizons. A buried soil's horizons may not be visible in section and this horizon is an indicator of possible occupation levels. Ancient land surfaces especially from the prehistoric era can be difficult to discern so this technique is useful for evaluating an area's potential for prehistoric surfaces and archaeological evidence. Comparative measurements down the profile are made and a sudden rise in organic content at some point in the profile combined with other indicators is strong evidence for buried surfaces.

Geophysical archaeological prospection methods are used to non-destructively explore and investigate possible structures of archaeological interest buried in the subsurface. Commonly used methods are:


Less commonly used geophysical archaeological prospection methods are:


The magnetic susceptibility of a material is a measure of its ability to become magnetised by an external magnetic field (Dearing, 1999). The magnetic susceptibility of a soil reflects the presence of magnetic iron-oxide minerals such as maghaematite; just because a soil contains a lot of iron does not mean that it will have high magnetic susceptibility. Magnetic forms of iron can be formed by burning and microbial activity such as occurs in top soils and some anaerobic deposits. Magnetic iron compounds can also be found in igneous and metamorphic rocks.

The relationship between iron and burning means that magnetic susceptibility is often used for:

The relationship between soil formation and magnetic susceptibility means that it can also be used to:

Phosphate in man-made soils derives from people, their animals, rubbish and bones. 100 people excrete about 62 kg of phosphate annually, with about the same from their rubbish. Their animals excrete even more. A human body contains about 650 g of (500 g–80% in the skeleton), which results in elevated levels in burial sites. Most is quickly immobilised on the clay of the soil and 'fixed', where it can persist for thousands of years. For a 1 ha site this corresponds to about 150 kg ha-1yr-1 about 0.5% to 10% of that already present in most soils. Therefore, it doesn't take long for human occupation to make orders of magnitude differences to the phosphate concentration in soil. Phosphorus exist in different 'pools' in the soil 1) organic (available), 2) occluded (adsorbed), 3) bound (chemically bound). Each of these pools can be extracted using progressively more aggressive chemicals. Some workers (Eidt especially), think that the ratios between these pools can give information about past land use, and perhaps even dating.

Whatever the method of getting the phosphorus from the soil into solution, the method of detecting it is usually the same. This uses the 'molybdate blue' reaction, where the depth of the colour is proportional to phosphorus concentration. In the lab, this is measured using a colorimeter, where light shining through a standard cell produces an electric current proportional to the light attenuation. In the field, the same reaction is used on detector sticks, which are compared to a colour chart.

Phosphate concentrations can be plotted on archaeological plans to show former activity areas, and is also used to prospect for sites in the wider landscape.

The particle size distribution of a soil sample may indicate the conditions under which the strata or sediment were deposited. Particle sizes are generally separated by means of dry or wet sieving (coarse samples such as till, gravel and sands, sometimes coarser silts) or by measuring the changes of the density of a dispersed solution (in sodium pyrophosphate, for example))of the sample (finer silts, clays). A rotating clock-glass with a very fine-grained dispersed sample under a heat lamp is useful in separating particles.

The results are plotted on curves which can be analyzed with statistical methods for particle distribution and other parameters.

The fractions received can be further investigated for cultural indicators, macro- and microfossils and other interesting features, so particle size analysis is in fact the first thing to do when handling these samples.

Trace element geochemistry is the study of the abundances of elements in geological materials that do not occur in a large quantity in these materials. Because these trace elements' concentrations are determined by a large number of particular situations under which a certain geological material is formed, they are usually unique between two locations which contain the same type of rock or other geological material.

Geoarchaeologists use this uniqueness in trace element geochemistry to trace ancient patterns of resource-acquisition and trade. For example, researchers can look at the trace element composition of obsidian artifacts in order to "fingerprint" those artifacts. They can then study the trace element composition of obsidian outcrops in order to determine the original source of the raw material used to make the artifact.

Geoarchaeologists study the mineralogical characteristics of pots through macroscopic and microscopic analyses. They can use these characteristics to understand the various manufacturing techniques used to make the pots, and through this, to know which production centers likely made these pots. They can also use the mineralogy to trace the raw materials used to make the pots to specific clay deposits.

Naturally occurring Ostracods in freshwater bodies are impacted by changes in salinity and pH due to human activities. Analysis of Ostracod shells in sediment columns show the changes brought about by farming and habitation activities. This record can be correlated with age dating techniques to help identify changes in human habitation patterns and population migrations.

Archaeological geology is a term coined by Werner Kasig in 1980. It is a sub-field of geology which emphasises the value of earth constituents for human life.





</doc>
<doc id="6786225" url="https://en.wikipedia.org/wiki?curid=6786225" title="Distance decay">
Distance decay

Distance decay is a geographical term which describes the effect of distance on cultural or spatial interactions. The distance decay effect states that the interaction between two locales declines as the distance between them increases. Once the distance is outside of the two locales' activity space, their interactions begin to decrease.

With the advent of faster travel, distance has less effect than it did in the past, except where places previously connected by now-abandoned railways, for example, have fallen off the beaten path. Advances in communications technology, such as telegraphs, telephones, broadcasting, and internet, have further decreased the effects of distance.

Distance decay is graphically represented by a curving line that swoops concavely downward as distance along the x-axis increases. Distance decay can be mathematically represented as an Inverse-square law by the expression

formula_1 
or
formula_2,

where I is interaction and d is distance. It can take other forms such as negative exponential, i.e.

formula_3

Distance decay is evident in town/city centres. It can refer to various things which decline with greater distance from the center of the Central Business District (CBD):

Distance decay weighs into the decision to migrate, leading many migrants to move less far.

Related terms include "friction of distance", which describes the force that creates distance decay and Waldo R. Tobler's "First law of geography", an informal statement that "All things are related, but near things are more related than far things." 
"Loss of Strength Gradient" holds that the amount of a nation's military power that could be brought to bear in any part of the world depends on geographic distance.




</doc>
<doc id="6974596" url="https://en.wikipedia.org/wiki?curid=6974596" title="Land cover">
Land cover

Land cover is the physical material at the surface of the earth. Land covers include grass, asphalt, trees, bare ground, water, etc. Earth cover is the expression used by ecologist Frederick Edward Clements that has its closest modern equivalent being vegetation. The expression continues to be used by the United States Bureau of Land Management.

There are two primary methods for capturing information on land cover: field survey and analysis of remotely sensed imagery. Land change models can be built from these types of data to assess future shifts in land cover 

One of the major land cover issues (as with all natural resource inventories) is that every survey defines similarly named categories in different ways. For instance, there are many definitions of "forest"—sometimes within the same organisation—that may or may not incorporate a number of different forest features (e.g., stand height, canopy cover, strip width, inclusion of grasses, and rates of growth for timber production). Areas without trees may be classified as forest cover "if the intention is to re-plant" (UK and Ireland), while areas with many trees may not be labelled as forest "if the trees are not growing fast enough" (Norway and Finland).

"Land cover" is distinct from "land use", despite the two terms often being used interchangeably. Land use is a description of how people "utilize" the land and of socio-economic activity. Urban and agricultural land uses are two of the most commonly known land use classes. At any one point or place, there may be multiple and alternate land uses, the specification of which may have a political dimension. The origins of the "land cover/land use" couplet and the implications of their confusion are discussed in Fisher et al. (2005).

Following table is Land Cover statistics by Food and Agriculture Organization (FAO) with 14 classes.




</doc>
<doc id="3439019" url="https://en.wikipedia.org/wiki?curid=3439019" title="Landlocked developing countries">
Landlocked developing countries

Landlocked developing countries (LLDC) are developing countries that are landlocked. The economic and other disadvantages experienced by such countries makes the majority of landlocked countries Least Developed Countries (LDCs), with inhabitants of these countries occupying the bottom billion tier of the world's population in terms of poverty. Apart from Europe, there is not a single successful highly developed landlocked country as measured by the Human Development Index (HDI), and nine of the twelve countries with the lowest HDI scores are landlocked. Landlocked European countries are exceptions in terms of development outcomes due to their close integration with the regional European market. Landlocked countries that rely on transoceanic trade usually suffer a cost of trade that is double that of their maritime neighbours. Landlocked countries experience economic growth 6% less than their non-landlocked countries, holding other variables constant.

About 442.8 million people live in current LLDCs, as of 2012.

The United Nations has an Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States (UN-OHRLLS). It mainly holds the view that high transport costs due to distance and terrain result in the erosion of competitive edge for exports from landlocked countries. In addition, it recognizes the constraints on landlocked countries to be mainly physical, including lack of direct access to the sea, isolation from world markets and high transit costs due to physical distance. It also attributes geographic remoteness as one of the most significant reasons why developing landlocked nations cannot alleviate themselves, while European landlocked cases are mostly developed because of short distances to the sea through well-developed countries. One other commonly cited factor is the administrative burdens associated with border crossings as there is a heavy load of bureaucratic procedures, paperwork, custom charges, and most importantly, traffic delay due to border wait times, which affect delivery contracts. Delays and inefficiency compound geographically, where a 2 to 3 week wait due to border customs between Uganda and Kenya makes it impossible to book ships ahead of time in Mombasa, furthering delivery contract delays. Despite these explanations, it is also important to consider the transit countries that neighbour LLDCs, from whose ports the goods of LLDCs are exported.

Although Adam Smith and traditional thought hold that geography and transportation are the culprits for keeping LLDCs from realizing development gains, Faye, Sachs and Snow hold the argument that no matter the advancement of infrastructure or lack of geographic distance to a port, landlocked nations are still dependent on their neighbouring transit nations. Outlying this specific relationship of dependency, Faye et al. insist that though LLDCs vary across the board in terms of HDI index scores, LLDCs almost uniformly straddle at the bottom of HDI rankings in terms of region, suggesting a correlated dependency relationship of development for landlocked countries with their respective regions. In fact, HDI levels decrease as one moves inland along the major transit route that runs from the coast of Kenya, across the country before going through Uganda, Rwanda and then finally Burundi. Just recently, it has been economically modeled that if the economic size of a transit country is increased by just 1%, a subsequent increase of at least 2% is experienced by the landlocked country, which shows that there is hope for LLDCs if the conditions of their transit neighbours are addressed. In fact, some LLDCs are seeing the brighter side of such a relationship, with the Central Asian nations geographic location between three BRIC nations (China, Russia and India) hungry for the region's oil and mineral wealth serving to boost economic development. The three major factors that LLDCs are dependent on their transit neighbours are dependence on transit infrastructure, dependence on political relations with neighbours, and dependence on internal peace and stability within transit neighbours.

Burundi has relatively good internal road networks, but it cannot export its goods using the most direct route to the sea since the inland infrastructure of Tanzania is poorly connected to the port of Dar es Salaam. Thus Burundi relies on Kenya's port of Mombasa for export; but this route was severed briefly in the 1990s when political relations with Kenya deteriorated. Further, Burundi's exports could not pass through Mozambique around the same time due to violent civil conflict. Thus, Burundi had to export its goods using a 4500 km route, crossing several borders and changing transport modes, to reach the port of Durban in South Africa.


The mineral resource-rich countries of Central Asia and Mongolia offer a unique set of landlocked cases to explore in more depth, as these are nations where economic growth has grown exceptionally in recent years. In Central Asia, oil and coal deposits have influenced development: Kazakhstan’s GDI per capita in purchasing power parity was five times greater than Kyrgyzstan's in 2009. Despite substantial development growth, these nations are not on a stable and destined path to being well developed, as the exploitation of their natural resources translates into an overall low average income and disparity of income, and because their limited deposits of resources allow growth only in the short term, and most importantly because dependence on unprocessed materials increases the risk of shocks due to variations in market prices. And though it is widely conceived that free trade can permit faster economic growth, Mongolia is now subjected to a new geopolitical game about the traffic on its railway lines between China and Russia. Russian Railways now effectively owns 50% of Mongolia's rail infrastructure, which could mean more efficient modernization and the laying of new rail lines, but in reality also translates into powerful leverage to pressure the government of Mongolia to concede unfair terms for license grants of coal, copper, and gold mines. Thus, it can be argued that these nations with extraordinary mineral wealth should pursue economic diversification. All of these nations possess education qualifications, as they are inheritors of the Soviet Union's social education system. This implies that it is due to poor economic policies that more than 40% of the labour force is bogged down in the agricultural sector instead of being diverted into secondary or tertiary economic activity. Yet, it cannot be ignored that Mongolia benefits exceptionally from its proximity to two giant BRIC nations, resulting in a rapid development of railway ports along its borders, especially along the Chinese border, as the Chinese seek to direct coking coal from Mongolia to China's northwestern industrial core as well as for transportation to Japan and South Korea, resulting in revenue generation through the port of Tianjin.

Nepal is another landlocked country with extreme dependency on its transit neighbour India. India does not have poor relations with Nepal, nor does it lack relevant transport infrastructure or internal stability. However, there have been two cases of economic blockades imposed by the government of India on Nepal - the official 1989 blockade and the unofficial 2015 blockade - both of which left the nation in severe economic crisis. In the 1970s, Nepal suffered from large commodity concentration and a high geographic centralization in its export trade: over 98% of its exports were to India, and 90% of its imports came from India. As a result of all this, Nepal had a poor trade bargaining position. In the 1950s, Nepal was forced to comply with India's external tariffs as well as the prices of India's exports. This was problematic since the two countries have different levels of development, resulting in greater gains for India which was larger, more advanced and with more resources. It was feared that a parasitic relationship might emerge, since India had a head start in industrialization, and dominated Nepal in manufacturing, which could reduce Nepal to being just a supplier of raw materials. Because of these problems, and Nepal's inability to develop its own infant industries (as it could not compete with Indian manufactures) treaties were drafted in 1960 and 1971, with amendments to the equal tariffs conditions, and terms of trade have since progressed.

In August, 2003, the International Ministerial Conference of Landlocked and Transit Developing Countries and Donor Countries on Transit Transport Cooperation (Almaty Ministerial Conference) was held in Almaty, Kazakhstan, setting the necessities of LLDCs in a universal document whereas there were no coordinated efforts on the global scale to serve the unique needs of LLDCs in the past. Other than acknowledging the main forms of dependency that must be addressed, it also acknowledged the additional dependency issue where neighbouring transit countries are often observed to export the same products as their landlocked neighbours. One result of the conference was a direct call for donor countries to step in to direct aid into setting up suitable infrastructure of transit countries to alleviate the burden of supporting LLDCs in regions of poor development in general. The general objectives of the Almaty Program of Action is as follows:









</doc>
<doc id="13387896" url="https://en.wikipedia.org/wiki?curid=13387896" title="Geographic targeting">
Geographic targeting

Geographic targeting is a viable way for resource allocation, especially to alleviate poverty in a country. In this context, public expenditure and policy interventions can be deployed to reach the neediest people in the poorest areas.

Geographical targeting for poverty alleviation employs a variety of techniques, such as database, and geographic information systems to construct poverty maps.


</doc>
<doc id="3439212" url="https://en.wikipedia.org/wiki?curid=3439212" title="Small Island Developing States">
Small Island Developing States

Small Island Developing States (SIDS) are a group of small island countries that tend to share similar sustainable development challenges, including small but growing populations, limited resources, remoteness, susceptibility to natural disasters, vulnerability to external shocks, excessive dependence on international trade, and fragile environments. Their growth and development is also held back by high communication, energy and transportation costs, irregular international transport volumes, disproportionately expensive public administration and infrastructure due to their small size, and little to no opportunity to create economies of scale.

The SIDS were first recognized as a distinct group of developing countries at the United Nations Conference on Environment and Development in June 1992. The Barbados Programme of Action was produced in 1994 to assist the SIDS in their sustainable development efforts. The United Nations Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States (UN-OHRLLS) represents this group of states.

Many SIDS now recognise the need to move towards low-carbon, climate resilient economies, as set out in the Caribbean Community (CARICOM) implementation plan for climate change-resilient development. SIDS often rely heavily on imported fossil fuels, spending an ever-larger proportion of their GDP on energy imports. Renewable technologies have the advantage of providing energy at a lower cost than fossil fuels and making SIDS more sustainable. Barbados has been successful in adopting the use of solar water heaters (SWHs). A 2012 report published by the Climate & Development Knowledge Network showed that its SWH industry now boasts over 50,000 installations. These have saved consumers as much as US$137 million since the early 1970s. The report suggested that Barbados's experience could be easily replicated in other SIDS with high fossil fuel imports and abundant sunshine.

Currently, the United Nations Department of Economic and Social Affairs lists 57 small island developing states. These are broken down into three geographic regions: the Caribbean; the Pacific; and Africa, Indian Ocean, Mediterranean and South China Sea (AIMS)., including Associate Members of the Regional Commissions. Each of these regions has a regional cooperation body: the Caribbean Community, the Pacific Islands Forum and the Indian Ocean Commission respectively, which many SIDS are members or associate members of. In addition, most (but not all) SIDS are members of the Alliance of Small Island States (AOSIS), which performs lobbying and negotiating functions for the SIDS within the United Nations system. The UNCTAD website states that "the UN never established criteria to determine an official list of SIDS" but it maintains a shorter, unofficial list on its website for analytical purposes.


 


</doc>
<doc id="10063629" url="https://en.wikipedia.org/wiki?curid=10063629" title="Rank-size distribution">
Rank-size distribution

Rank-size distribution is the distribution of size by rank, in decreasing order of size. For example, if a data set consists of items of sizes 5, 100, 5, and 8, the rank-size distribution is 100, 8, 5, 5 (ranks 1 through 4). This is also known as the rank-frequency distribution, when the source data are from a frequency distribution. These are particularly of interest when the data vary significantly in scale, such as city size or word frequency. These distributions frequently follow a power law distribution, or less well-known ones such as a stretched exponential function or parabolic fractal distribution, at least approximately for certain ranges of ranks; see below.

A rank-size distribution is not a probability distribution or cumulative distribution function. Rather, it is a discrete form of a quantile function (inverse cumulative distribution) in reverse order, giving the size of the element at a given rank.

In the case of city populations, the resulting distribution in a country, a region, or the world will be characterized by its largest city, with other cities decreasing in size respective to it, initially at a rapid rate and then more slowly. This results in a few large cities and a much larger number of cities orders of magnitude smaller. For example, a rank 3 city would have one-third the population of a country's largest city, a rank 4 city would have one-fourth the population of the largest city, and so on.

When any log-linear factor is ranked, the ranks follow the Lucas numbers, which consist of the sequentially additive numbers 1, 3, 4, 7, 11, 18, 29, 47, 76, 123, 199, etc. Like the more famous Fibonacci sequence, each number is approximately 1.618 (the Golden ratio) times the preceding number. For example, the third term in the sequence above, 4, is approximately 1.618, or 4.236; the fourth term, 7, is approximately 1.618, or 6.854; the eighth term, 47, is approximately 1.618, or 46.979. With higher values, the figures converge. An equiangular spiral is sometimes used to visualize such sequences.

A rank-size (or rank-frequency) distribution is often segmented into ranges. This is frequently done somewhat arbitrarily or due to external factors, particularly for market segmentation, but can also be due to distinct behavior as rank varies.

Most simply and commonly, a distribution may be split in two, termed the head and tail. If a distribution is broken into three pieces, the third (middle) piece has several terms, generically middle, also belly, torso, and body. These frequently have some adjectives added, most significantly "long tail", also "fat belly", "chunky middle", etc. In more traditional terms, these may be called "top-tier", "mid-tier", and "bottom-tier".

The relative sizes and weights of these segments (how many ranks in each segment, and what proportion of the total population is in a given segment) qualitatively characterizes a distribution, analogously to the skewness or kurtosis of a probability distribution. Namely: is it dominated by a few top members (head-heavy, like profits in the recorded music industry), or is it dominated by many small members (tail-heavy, like internet search queries), or distributed in some other way? Practically, this determines strategy: where should attention be focused?

These distinctions may be made for various reasons. For example, they may arise from differing properties of the population, as in the 90–9–1 principle, which posits that in an internet community, 90% of the participants of a community only view content, 9% of the participants edit content, and 1% of the participants actively create new content. As another example, in marketing one may pragmatically consider the head as all members that receive personalized attention, such as personal phone calls; while the tail is everything else, which does not receive personalized attention, for example receiving form letters; and the line is simply as far as resources allow, or where it makes business sense to stop.

Purely quantitatively, a conventional way of splitting a distribution into head and tail is to consider the head to be the first "p" portion of ranks, which account for formula_1 of the overall population, as in the 80:20 Pareto principle, where the top 20% (head) comprises 80% of the overall population. The exact cutoff depends on the distribution – each distribution has a single such cutoff point—and for power laws can be computed from the Pareto index.

Segments may arise naturally due to actual changes in behavior of the distribution as rank varies. Most common is the king effect, where behavior of the top handful of items does not fit the pattern of the rest, as illustrated at top for country populations, and above for most common words in English Wikipedia. For higher ranks, behavior may change at some point, and be well-modeled by different relations in different regions; on the whole by a piecewise function. For example, if two different power laws fit better in different regions, one can use a broken power law for the overall relation; the word frequency in English Wikipedia (above) also demonstrates this.

The Yule–Simon distribution that results from preferential attachment (intuitively, "the rich get richer" and "success breeds success") simulates a broken power law and has been shown to "very well capture" word frequency versus rank distributions. It originated from trying to explain the population verses rank in different species. It has also been show to fit city population versus rank better.

The rank-size rule (or law), describes the remarkable regularity in many phenomena, including the distribution of city sizes, the sizes of businesses, the sizes of particles (such as sand), the lengths of rivers, the frequencies of word usage, and wealth among individuals.

All are real-world observations that follow power laws, such as Zipf's law, the Yule distribution, or the Pareto distribution. If one ranks the population size of cities in a given country or in the entire world and calculates the natural logarithm of the rank and of the city population, the resulting graph will show a log-linear pattern. This is the rank-size distribution.

One study claims that the rank size rule "works" because it is a "shadow" or coincidental measure of the true phenomenon. The true value of rank size is thus not as an accurate mathematical measure (since other power-law formulas are more accurate, especially at ranks lower than 10) but rather as a handy measure or "rule of thumb" to spot power laws. When presented with a ranking of data, is the third-ranked variable approximately one-third the value of the highest-ranked one? Or, conversely, is the highest-ranked variable approximately ten times the value of the tenth-ranked one? If so, the rank size rule has possibly helped spot another power law relationship.

While Zipf's law works well in many cases, it tends to not fit the largest cities in many countries; one type of deviation is known as the King effect. A 2002 study found that Zipf's law was rejected for 53 of 73 countries, far more than would be expected based on random chance. The study also found that variations of the Pareto exponent are better explained by political variables than by economic geography variables like proxies for economies of scale or transportation costs. A 2004 study showed that Zipf's law did not work well for the five largest cities in six countries. In the richer countries, the distribution was flatter than predicted. For instance, in the United States, although its largest city, New York City, has more than twice the population of second-place Los Angeles, the two cities' metropolitan areas (also the two largest in the country) are much closer in population. In metropolitan-area population, New York City is only 1.3 times larger than Los Angeles. In other countries, the largest city would dominate much more than expected. For instance, in the Democratic Republic of the Congo, the capital, Kinshasa, is more than eight times larger than the second-largest city, Lubumbashi. When considering the entire distribution of cities, including the smallest ones, the rank-size rule does not hold. Instead, the distribution is log-normal. This follows from Gibrat's law of proportionate growth.

Because exceptions are so easy to find, the function of the rule for analyzing cities today is to compare the city-systems in different countries. The rank-size rule is a common standard by which urban primacy is established. A distribution such as that in the United States or China does not exhibit a pattern of primacy, but countries with a dominant "primate city" clearly vary from the rank-size rule in the opposite manner. Therefore, the rule helps to classify national (or regional) city-systems according to the degree of dominance exhibited by the largest city. Countries with a primate city, for example, have typically had a colonial history that accounts for that city pattern. If a normal city distribution pattern is expected to follow the rank-size rule (i.e. if the rank-size principle correlates with central place theory), then it suggests that those countries or regions with distributions that do not follow the rule have experienced some conditions that have altered the normal distribution pattern. For example—the presence of multiple regions within large nations such as China and the United States tends to favor a pattern in which more large cities appear than would be predicted by the rule. By contrast, small countries that had been connected (e.g. colonially/economically) to much larger areas will exhibit a distribution in which the largest city is much larger than would fit the rule, compared with the other cities—the excessive size of the city theoretically stems from its connection with a larger system rather than the natural hierarchy that central place theory would predict within that one country or region alone.



</doc>
<doc id="15516115" url="https://en.wikipedia.org/wiki?curid=15516115" title="Geo-replication">
Geo-replication

Geo-replication systems are designed to improve the distribution of data across geographically distributed data networks. This is intended to improve the response time for applications such as web portals. Geo-replication can be achieved using software, hardware or a combination of the two. 

Geo-replication software is a network performance-enhancing technology that is designed to provide improved access to portal or intranet content for uses at the most remote parts of large organizations. It is based on the principle of storing complete replicas of portal content on local servers, and then keeping the content on those servers up-to-date using heavily compressed data updates.

Geo-replication technologies are used to provide replication of the content of portals, intranets, web applications, content and data between servers, across wide area networks WAN to allow users at remote sites to access central content at LAN speeds. 

Geo-replication software can improve the performance of data networks that suffer limited bandwidth, latency and periodic disconnection. Terabytes of data can be replicated over a wide area network, giving remote sites faster access to web applications.

Geo-replication software uses a combination of data compression and content caching technologies. differencing technologies can also be employed to reduce the volume of data that has to be transmitted to keep portal content accurate across all servers. This update compression can reduce the load that portal traffic place on networks, and improve the response time of a portal.

Remote users of web portals and collaboration environments will frequently experience network bandwidth and latency problems which will slow down their experience of opening and closing files, and otherwise interacting with the portal. Geo-replication technology is deployed to accelerate the remote end user portal performance to be equivalent to that experienced by users locally accessing the portal in the central office.

To deliver this reduction in the size of the required data updates across a portal, geo-replication systems often use differencing engine technologies. These systems are able to difference the content of each portal server right down to the byte level. This knowledge of the content that is already on each server enables the system to rebuild any changes to the content on one server, across each of the other servers in the deployment from content already hosted on those other servers. This type of differencing system ensures that no content, at the byte level, is ever sent to a server twice.

Geo-replication systems are often extended to deliver local replication beyond the server and down to the laptop used by a single user. Server to laptop replication enables mobile users to have access to a local replica of their business portal on a standard laptop. This technology may be employed to provide in the field access to portal content by, for example, sales forces and combat forces.




</doc>
<doc id="286960" url="https://en.wikipedia.org/wiki?curid=286960" title="Mainland">
Mainland

Mainland is defined as "relating to or forming the main part of a country or continent, not including the islands around it [regardless of status under territorial jurisdiction by an entity]." The term is often politically, economically and/or demographically more significant than politically associated remote territories, such as exclaves or oceanic islands situated outside the continental shelf. 

In geography, "mainland" can denote the continental (i.e. non-insular) part of any polity or the main island within an island nation. In geopolitics, "mainland" is sometimes used interchangeably with terms like Metropole as an antonym to overseas territories. In the sense of "heartland", mainland is the opposite of periphery.

The term is relative - in Tasmania, continental Australia is the mainland, while to residents of Flinders Island, the main island of Tasmania is also "the mainland", although the geological Australian continent includes all the former plus the island of New Guinea and all the smaller islands (e.g. the Torres Strait Islands) in between.

"This list denotes prominent usages of the term "mainland" to distinguish the islands of a continent from the mainland of a continent through a geopolitical lens."


"This list denotes prominent usages of the term "mainland" to distinguish between distinct regions within a single country based on an "islands-to-mainland" relationship. Note that the "mainland" can sometimes consist of a large island rather than a continental landmass."


"This list denotes prominent internal usages of the term "mainland" that are disputed."


"This list denotes prominent usages of the term "mainland" to distinguish between distinct regions within an irredentist region."




</doc>
<doc id="404571" url="https://en.wikipedia.org/wiki?curid=404571" title="Spatial mismatch">
Spatial mismatch

Spatial mismatch is the mismatch between where low-income households reside and suitable job opportunities. In its original formulation (see below) and in subsequent research, it has mostly been understood as a phenomenon affecting African-Americans, as a result of residential segregation, economic restructuring, and the suburbanization of employment.

Spatial mismatch was first proposed by John F. Kain in a seminal 1968 article, "Housing Segregation, Negro Employment, and Metropolitan Decentralization". That article did not specifically use the term "spatial mismatch", and Kain disclaimed credit. 

In 1987, William Julius Wilson was an important exponent, elaborating the role of economic restructuring, as well as the departure of the black middle-class, in the development of a ghetto underclass in the United States.

After World War I, many wealthy Americans started decentralizing out of the cities and into the suburbs. During the second half of the 20th century, department stores followed the trend of moving into the suburbs. In 1968, Kain formulated the “Spatial Mismatch Hypothesis”, but he did not refer to it by this term. His hypothesis was that black workers reside in segregated zones that are distant and poorly connected to major centers of growth. The phenomenon has many implications for inner-city residents dependent on low-level entry jobs. For example, distance from work centers can lead to increasing unemployment rates and further dampen poverty outcomes for the region at large. Since its conceptualization in the late 1960s, the spatial mismatch hypothesis has been widely cited to explain the economic problems encountered by inner-city minorities.

In 2007, Laurent Gobillon, Harris Selod, and Yves Zenou suggested that there are seven different factors that support the spatial mismatch phenomenon. Four factors are attributed to potential workers accessibility and initiatives. The remaining three factors stress employers' reluctance to divert away from the negative stigma of city people and in particular minorities when hiring.


Growth of ghost cities in China, mostly from not yet agglomerated areas between or adjacent metropolitan areas or coal mining towns, as in the case of the most famous example, Kangbashi New Area of Ordos, are an example of spatial mismatch. In the case of places near metropolitan areas, it represents less of a risk going forward than in mining areas.



</doc>
<doc id="23668992" url="https://en.wikipedia.org/wiki?curid=23668992" title="Geocriticism">
Geocriticism

Geocriticism is a method of literary analysis and literary theory that incorporates the study of geographic space. The term designates a number of different critical practices. In France, Bertrand Westphal has elaborated the concept of "géocritique" in several works. In the United States, Robert Tally has argued for a geocriticism as a critical practice suited to the analysis of what he has termed "literary cartography".

Some of the first expressly "geocritical" writings emerged from symposia organized by Westphal at the University of Limoges. Westphal's foundational essay, "Pour une approche géocritique des textes" constitutes a manifesto for geocriticism. Westphal's theory is elaborated in greater detail in his "Geocriticism: Real and Fictional Spaces", translated by Tally, who also provides a brief introduction. But there are also many works addressing similar themes and using similar methods that might be considered geocritical, even if the term "geocriticism" is not used.

In Westphal's theory, geocriticism is based on three theoretical concepts: spatio-temporality, transgressivity, and referentiality. 

The idea that space and time form a continuum (space-time) is a tenet of modern physics. In the field of literary theory, geocriticism is an interdisciplinary method of literary analysis that focuses not only on such temporal data as relations between the life and times of the author (as in biographical criticism), the history of the text (as in textual criticism), or the story (as studied by narratology), but also on spatial data. Geocriticism therefore has affinities with geography, architecture, urban studies, and so on; it also correlates to philosophical concepts such as deterritorialization.
Following the work of Michel Foucault, Gilles Deleuze, Henri Lefebvre and Mikhail Bakhtin, among others, a geocritical approach to literature recognizes that representations of space are often transgressive, crossing the boundaries of established norms while also reestablishing new relations among people, places, and things. Cartography is no longer seen as the exclusive province of the state or the government; rather, various agents or groups may be responsible for representing the geographic spaces at the same time and with different effects. In practice, therefore, geocriticism is multifocal, examining a variety of topics at once, thus differentiating itself from practices that focus on the singular point of view of the traveler or protagonist. 

Geocriticism also assumes a literary referentiality between world and text, or, in other words, between the referent and its representation. By questioning the relations between a given space's nature and its actually existing condition, the geocritical approach allows for a study of fiction that points also to the theory of possible worlds, such as may be seen in the work on third space by the American geographer Edward Soja ("Thirdspace"). Tally's book "Spatiality", an introduction to spatiality studies in literature and critical theory, includes a chapter on geocriticism.

Geocriticism frequently involves the study of places described in the literature by various authors, but it can also study the effects of literary representations of a given space. An example of the range of geocritical practices can be found in Tally's collection "Geocritical Explorations: Space, Place, and Mapping in Literary and Cultural Studies".

Geocriticism derives some of its practices from precursors whose theoretical work helped establish space as a valid topic for literary analysis. For example, in "The Poetics of Space" and elsewhere, Gaston Bachelard studied literary works to develop a typology of places according to their connotations. Maurice Blanchot's writings have legitimized the idea of literary space, an imaginary place for the creation of the work of literature. One might also look at the developments of cultural studies and especially postcolonial studies, such as Raymond Williams's "The Country and the City" or Edward Said's "Culture and Imperialism", which employ what Said has called a "geographical inquiry into historical experience." Fredric Jameson's concept of cognitive mapping and his theoretical engagement with the postmodern condition also highlight the importance of spatial representation and aesthetic productions, including literature, film, architecture, and design. In "The Atlas of European Novel, 1800-1900", Franco Moretti has examined the diffusion of literary spaces in Europe, focusing on the complex relationship between the text and space. Moretti has also promulgated a theory of literary history, or literary geography, that would use maps to bring to light new connections between the texts studied and their social spaces. And, in his study of Herman Melville's literary cartography, Robert Tally has offered a geocritical approach to certain texts. 

Geocriticism has intellectual and methodological affiliations with such fields as Literature and the Environment or ecocriticism, regional literature, urban studies, sociological and philosophical approaches to literature, and utopian studies.

Notes
Further reading


</doc>
<doc id="19021764" url="https://en.wikipedia.org/wiki?curid=19021764" title="Spatial justice">
Spatial justice

Spatial justice links together social justice and space, most notably in the works of geographers David Harvey and Edward W. Soja. The organization of space is a crucial dimension of human societies and reflects social facts and influences social relations (Henri Lefebvre, 1968, 1972). Consequently, both justice and injustice become visible in space. Therefore, the analysis of the interactions between space and society is necessary to understand social injustices and to formulate territorial policies aiming at tackling them. It is at this junction that the concept of spatial justice has been developed.

According to this political theory, space being a fundamental dimension of human societies, social justice is embedded in it. So the understanding of interactions between space and societies is essential to the understanding of social injustices and to a reflection on planning policies that aims at reducing them. This reflection can be guided by the concept of spatial justice, which ties Social Justice with space.
Spatial justice is a crucial challenge because it is the ultimate goal of many planning policies. However, the diversity of definitions of "Justice" (and of the possible "social contracts" that legitimate them), is high and the political objectives of regional planning or urban planning can be quite different and even contradictory.

Therefore, it is important to analyze the concept of spatial justice, which is still rarely questioned (particularly since the work of Anglo-American radical geographers in the 1970s–1980s) to the extent that it has been taken for granted. These past few years, several events and publications have demonstrated the rising interest of human and social sciences for the concept of spatial justice.

The concept of spatial justice opens up several perspectives for social sciences. Building on the work of several famous Justice philosophers (John Rawls, 1971; Iris Marion Young, 1990, 2000), two contrasting approaches of justice have polarized the debate: one focuses on redistribution issues, the other concentrates on decision-making processes. 
A first set of approaches consists in asking questions about spatial or socio-spatial distributions and working to achieve an equal geographical distribution of society's wants and needs, such as job opportunities, access to health care, good air quality, et cetera. This is of particular concern in regions where the population has difficulty moving to a more spatially just location due to poverty, discrimination, or political restrictions (such as apartheid pass laws). Even in free, developed nations, access to many places are limited. Geographer Don Mitchell points to the mass privatization of once-public land as a common example of spatial injustice. In this distributive justice perspective, the access to material and immaterial goods, or to social positions indicates whether the situation is fair or not. At the scale of urban space, questions of accessibility, walkability and transport equity can also be seen as matters of distribution of spatial resources.

Another way of tackling the concept of spatial justice is to focus on decision-making procedures: this approach also raises issues of representations of space, of territorial or other identities and of social practices. For instance, focusing on minorities allows to explore their spatial practices but also to investigate how these are experienced and managed by various agents: this may lead to reveal forms of oppression or discrimination that a universalist approach might disregard otherwise.
In sum, depending on the chosen approach, either questions are asked about spatial distributions because justice is evaluated from "results", or questions are asked about space representations, (spatial or not) identities and experiences because justice is defined as a process. Spatial justice stands as a unifying concept for the social sciences: its coherence stems from a reflection on the modalities of the political decision-making and on the policies implemented in order to improve spatial distributions.

The emergence of the concept of sustainable development has also fostered a debate on environmental equity. It questions our ontological relationship to the world, and the possibility of a fair policy addressing the needs of mankind, present and future, local and global, and of new forms of governance. The notion of "Environmental Justice" was created in the 1970s–1980s in North American cities to denounce the spatial overlapping between forms of racial discrimination and social-economic exclusion, industrial pollutions and vulnerability to natural hazards.





</doc>
