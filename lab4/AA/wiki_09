<doc id="10772350" url="https://en.wikipedia.org/wiki?curid=10772350" title="History">
History

History (from Greek , "historia", meaning 'inquiry; knowledge acquired by investigation') is the past as it is described in written documents, and the study thereof. Events occurring before written records are considered prehistory. "History" is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Scholars who write about history are called historians.

History also includes the academic discipline which uses a narrative to examine and analyse a sequence of past events, and objectively determine the patterns of cause and effect that determine them. Historians sometimes debate the nature of history and its usefulness by discussing the study of the discipline as an end in itself and as a way of providing "perspective" on the problems of the present.

Stories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends, because they do not show the "disinterested investigation" required of the discipline of history. Herodotus, a 5th-century BC Greek historian is often considered within the Western tradition to be the "father of history", or by some the "father of lies", and, along with his contemporary Thucydides, helped form the foundations for the modern study of human history. Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In East Asia, a state chronicle, the Spring and Autumn Annals, was known to be compiled from as early as 722 BC although only 2nd-century BC texts have survived.

Ancient influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and the study of certain topical or thematical elements of historical investigation. Often history is taught as part of primary and secondary education, and the academic study of history is a major discipline in university studies.

The word "history" comes from the Ancient Greek ἱστορία ("historía"), meaning 'inquiry', 'knowledge from inquiry', or 'judge'. It was in that sense that Aristotle used the word in his "History of Animals." The ancestor word is attested early on in Homeric Hymns, Heraclitus, the Athenian ephebes' oath, and in Boiotic inscriptions (in a legal sense, either 'judge' or 'witness', or similar). The Greek word was borrowed into Classical Latin as "historia", meaning "investigation, inquiry, research, account, description, written account of past events, writing of history, historical narrative, recorded knowledge of past events, story, narrative". "History" was borrowed from Latin (possibly via Old Irish or Old Welsh) into Old English as "stær" ('history, narrative, story'), but this word fell out of use in the late Old English period. Meanwhile, as Latin became Old French (and Anglo-Norman), "historia" developed into forms such as "istorie", "estoire", and "historie", with new developments in the meaning: "account of the events of a person's life (beginning of the 12th century), chronicle, account of events as relevant to a group of people or people in general (1155), dramatic or pictorial representation of historical events (c. 1240), body of knowledge relative to human evolution, science (c. 1265), narrative of real or imaginary events, story (c. 1462)".

It was from Anglo-Norman that "history" was borrowed into Middle English, and this time the loan stuck. It appears in the 13th-century "Ancrene Wisse", but seems to have become a common word in the late 14th century, with an early attestation appearing in John Gower's "Confessio Amantis" of the 1390s (VI.1383): "I finde in a bok compiled | To this matiere an old histoire, | The which comth nou to mi memoire". In Middle English, the meaning of "history" was "story" in general. The restriction to the meaning "the branch of knowledge that deals with past events; the formal record or study of past events, esp. human affairs" arose in the mid-15th century. With the Renaissance, older senses of the word were revived, and it was in the Greek sense that Francis Bacon used the term in the late 16th century, when he wrote about "Natural History". For him, "historia" was "the knowledge of objects determined by space and time", that sort of knowledge provided by memory (while science was provided by reason, and poetry was provided by fantasy).

In an expression of the linguistic synthetic vs. analytic/isolating dichotomy, English like Chinese (史 vs. 诌) now designates separate words for human history and storytelling in general. In modern German, French, and most Germanic and Romance languages, which are solidly synthetic and highly inflected, the same word is still used to mean both 'history' and 'story'. "Historian" in the sense of a "researcher of history" is attested from 1531. In all European languages, the substantive "history" is still used to mean both "what happened with men", and "the scholarly study of the happened", the latter sense sometimes distinguished with a capital letter, or the word "historiography". The adjective "historical" is attested from 1661, and "historic" from 1669.

Historians write in the context of their own time, and with due regard to the current dominant ideas of how to interpret the past, and sometimes write to provide lessons for their own society. In the words of Benedetto Croce, "All history is contemporary history". History is facilitated by the formation of a "true discourse of past" through the production of narrative and analysis of past events relating to the human race. The modern discipline of history is dedicated to the institutional production of this discourse.

All events that are remembered and preserved in some authentic form constitute the historical record. The task of historical discourse is to identify the sources which can most usefully contribute to the production of accurate accounts of past. Therefore, the constitution of the historian's archive is a result of circumscribing a more general archive by invalidating the usage of certain texts and documents (by falsifying their claims to represent the "true past").

The study of history has sometimes been classified as part of the humanities and at other times as part of the social sciences. It can also be seen as a bridge between those two broad areas, incorporating methodologies from both. Some individual historians strongly support one or the other classification. In the 20th century, French historian Fernand Braudel revolutionized the study of history, by using such outside disciplines as economics, anthropology, and geography in the study of global history.

Traditionally, historians have recorded events of the past, either in writing or by passing on an oral tradition, and have attempted to answer historical questions through the study of written documents and oral accounts. From the beginning, historians have also used such sources as monuments, inscriptions, and pictures. In general, the sources of historical knowledge can be separated into three categories: what is written, what is said, and what is physically preserved, and historians often consult all three. But writing is the marker that separates history from what comes before.

Archaeology is a discipline that is especially helpful in dealing with buried sites and objects, which, once unearthed, contribute to the study of history. But archaeology rarely stands alone. It uses narrative sources to complement its discoveries. However, archaeology is constituted by a range of methodologies and approaches which are independent from history; that is to say, archaeology does not "fill the gaps" within textual sources. Indeed, "historical archaeology" is a specific branch of archaeology, often contrasting its conclusions against those of contemporary textual sources. For example, Mark Leone, the excavator and interpreter of historical Annapolis, Maryland, USA; has sought to understand the contradiction between textual documents and the material record, demonstrating the possession of slaves and the inequalities of wealth apparent via the study of the total historical environment, despite the ideology of "liberty" inherent in written documents at this time.

There are varieties of ways in which history can be organized, including chronologically, culturally, territorially, and thematically. These divisions are not mutually exclusive, and significant overlaps are often present, as in "The International Women's Movement in an Age of Transition, 1830–1975." It is possible for historians to concern themselves with both the very specific and the very general, although the modern trend has been toward specialization. The area called Big History resists this specialization, and searches for universal patterns or trends. History has often been studied with some practical or theoretical aim, but also may be studied out of simple intellectual curiosity.

The history of the world is the memory of the past experience of "Homo sapiens sapiens" around the world, as that experience has been preserved, largely in written records. By "prehistory", historians mean the recovery of knowledge of the past in an area where no written records exist, or where the writing of a culture is not understood. By studying painting, drawings, carvings, and other artifacts, some information can be recovered even in the absence of a written record. Since the 20th century, the study of prehistory is considered essential to avoid history's implicit exclusion of certain civilizations, such as those of Sub-Saharan Africa and pre-Columbian America. Historians in the West have been criticized for focusing disproportionately on the Western world. In 1961, British historian E. H. Carr wrote:

This definition includes within the scope of history the strong interests of peoples, such as Indigenous Australians and New Zealand Māori in the past, and the oral records maintained and transmitted to succeeding generations, even before their contact with European civilization.

Historiography has a number of related meanings. Firstly, it can refer to how history has been produced: the story of the development of methodology and practices (for example, the move from short-term biographical narrative towards long-term thematic analysis). Secondly, it can refer to what has been produced: a specific body of historical writing (for example, "medieval historiography during the 1960s" means "Works of medieval history written during the 1960s"). Thirdly, it may refer to why history is produced: the Philosophy of history. As a meta-level analysis of descriptions of the past, this third conception can relate to the first two in that the analysis usually focuses on the narratives, interpretations, world view, use of evidence, or method of presentation of other historians. Professional historians also debate the question of whether history can be taught as a single coherent narrative or a series of competing narratives.

The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history.

Herodotus of Halicarnassus (484 BC – ca.425 BC) has generally been acclaimed as the "father of history". However, his contemporary Thucydides (c. 460 BC – c. 400 BC) is credited with having first approached history with a well-developed historical method in his work the "History of the Peloponnesian War". Thucydides, unlike Herodotus, regarded history as being the product of the choices and actions of human beings, and looked at cause and effect, rather than as the result of divine intervention (though Herodotus was not wholly committed to this idea himself). In his historical method, Thucydides emphasized chronology, a nominally neutral point of view, and that the human world was the result of the actions of human beings. Greek historians also viewed history as cyclical, with events regularly recurring.

There were historical traditions and sophisticated use of historical method in ancient and medieval China. The groundwork for professional historiography in East Asia was established by the Han dynasty court historian known as Sima Qian (145–90 BC), author of the "Records of the Grand Historian" ("Shiji"). For the quality of his written work, Sima Qian is posthumously known as the Father of Chinese historiography. Chinese historians of subsequent dynastic periods in China used his "Shiji" as the official format for historical texts, as well as for biographical literature.

Saint Augustine was influential in Christian and Western thought at the beginning of the medieval period. Through the Medieval and Renaissance periods, history was often studied through a sacred or religious perspective. Around 1800, German philosopher and historian Georg Wilhelm Friedrich Hegel brought philosophy and a more secular approach in historical study.

In the preface to his book, the "Muqaddimah" (1377), the Arab historian and early sociologist, Ibn Khaldun, warned of seven mistakes that he thought that historians regularly committed. In this criticism, he approached the past as strange and in need of interpretation. The originality of Ibn Khaldun was to claim that the cultural difference of another age must govern the evaluation of relevant historical material, to distinguish the principles according to which it might be possible to attempt the evaluation, and lastly, to feel the need for experience, in addition to rational principles, in order to assess a culture of the past. Ibn Khaldun often criticized "idle superstition and uncritical acceptance of historical data." As a result, he introduced a scientific method to the study of history, and he often referred to it as his "new science". His historical method also laid the groundwork for the observation of the role of state, communication, propaganda and systematic bias in history, and he is thus considered to be the "father of historiography" or the "father of the philosophy of history".

In the West, historians developed modern methods of historiography in the 17th and 18th centuries, especially in France and Germany. In 1851, Herbert Spencer summarized these methods: 

By the "rich ore" Spencer meant scientific theory of history. Meanwhile, Henry Thomas Buckle expressed a dream of history becoming one day science: 

Contrary to Buckle's dream, the 19th-century historian with greatest influence on methods became Leopold von Ranke in Germany. He limited history to “what really happened” and by this directed the field further away from science. For Ranke, historical data should be collected carefully, examined objectively and put together with critical rigor. But these procedures “are merely the prerequisites and preliminaries of science. The heart of science is searching out order and regularity in the data being examined and in formulating generalizations or laws about them.”

In the 20th century, academic historians focused less on epic nationalistic narratives, which often tended to glorify the nation or great men, to more objective and complex analyses of social and intellectual forces. A major trend of historical methodology in the 20th century was a tendency to treat history more as a social science rather than as an art, which traditionally had been the case. Some of the leading advocates of history as a social science were a diverse collection of scholars which included Fernand Braudel, E. H. Carr, Fritz Fischer, Emmanuel Le Roy Ladurie, Hans-Ulrich Wehler, Bruce Trigger, Marc Bloch, Karl Dietrich Bracher, Peter Gay, Robert Fogel, Lucien Febvre and Lawrence Stone. Many of the advocates of history as a social science were or are noted for their multi-disciplinary approach. Braudel combined history with geography, Bracher history with political science, Fogel history with economics, Gay history with psychology, Trigger history with archaeology while Wehler, Bloch, Fischer, Stone, Febvre and Le Roy Ladurie have in varying and differing ways amalgamated history with sociology, geography, anthropology, and economics. Nevertheless, these multidisciplinary approaches failed to produce a theory of history. So far only one theory of history came from the pen of a professional Historian. Whatever other theories of history we have, they were written by experts from other fields (for example, Marxian theory of history). More recently, the field of digital history has begun to address ways of using computer technology to pose new questions to historical data and generate digital scholarship.

In sincere opposition to the claims of history as a social science, historians such as Hugh Trevor-Roper, John Lukacs, Donald Creighton, Gertrude Himmelfarb and Gerhard Ritter argued that the key to the historians' work was the power of the imagination, and hence contended that history should be understood as an art. French historians associated with the Annales School introduced quantitative history, using raw data to track the lives of typical individuals, and were prominent in the establishment of cultural history (cf. "histoire des mentalités"). Intellectual historians such as Herbert Butterfield, Ernst Nolte and George Mosse have argued for the significance of ideas in history. American historians, motivated by the civil rights era, focused on formerly overlooked ethnic, racial, and socio-economic groups. Another genre of social history to emerge in the post-WWII era was "Alltagsgeschichte" (History of Everyday Life). Scholars such as Martin Broszat, Ian Kershaw and Detlev Peukert sought to examine what everyday life was like for ordinary people in 20th-century Germany, especially in the Nazi period.

Marxist historians such as Eric Hobsbawm, E. P. Thompson, Rodney Hilton, Georges Lefebvre, Eugene Genovese, Isaac Deutscher, C. L. R. James, Timothy Mason, Herbert Aptheker, Arno J. Mayer and Christopher Hill have sought to validate Karl Marx's theories by analyzing history from a Marxist perspective. In response to the Marxist interpretation of history, historians such as François Furet, Richard Pipes, J. C. D. Clark, Roland Mousnier, Henry Ashby Turner and Robert Conquest have offered anti-Marxist interpretations of history. Feminist historians such as Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese, and Lynn Hunt have argued for the importance of studying the experience of women in the past. In recent years, postmodernists have challenged the validity and need for the study of history on the basis that all history is based on the personal interpretation of sources. In his 1997 book "In Defence of History", Richard J. Evans defended the worth of history. Another defence of history from post-modernist criticism was the Australian historian Keith Windschuttle's 1994 book, "The Killing of History".

The Marxist theory of historical materialism theorises that society is fundamentally determined by the "material conditions" at any given time – in other words, the relationships which people have with each other in order to fulfill basic needs such as feeding, clothing and housing themselves and their families. Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe. Marxist historiography was once orthodoxy in the Soviet Union, but since the collapse of communism there in 1991, Mikhail Krom says it has been reduced to the margins of scholarship.

Historical study often focuses on events and developments that occur in particular blocks of time. Historians give these periods of time names in order to allow "organising ideas and classificatory generalisations" to be used by historians. The names given to a period can vary with geographical location, as can the dates of the beginning and end of a particular period. Centuries and decades are commonly used periods and the time they represent depends on the dating system used. Most periods are constructed retrospectively and so reflect value judgments made about the past. The way periods are constructed and the names given to them can affect the way they are viewed and studied.

The field of history generally leaves prehistory to the archaeologists, who have entirely different sets of tools and theories. The usual method for periodisation of the distant prehistoric past, in archaeology is to rely on changes in material culture and technology, such as the Stone Age, Bronze Age and Iron Age and their sub-divisions also based on different styles of material remains. Here prehistory is divided into a series of "chapters" so that periods in history could unfold not only in a relative chronology but also narrative chronology. This narrative content could be in the form of functional-economic interpretation. There are periodisation, however, that do not have this narrative aspect, relying largely on relative chronology and, thus, devoid of any specific meaning.

Despite the development over recent decades of the ability through radiocarbon dating and other scientific methods to give actual dates for many sites or artefacts, these long-established schemes seem likely to remain in use. In many cases neighbouring cultures with writing have left some history of cultures without it, which may be used. Periodisation, however, is not viewed as a perfect framework with one account explaining that "cultural changes do not conveniently start and stop (combinedly) at periodisation boundaries" and that different trajectories of change are also needed to be studied in their own right before they get intertwined with cultural phenomena.

Particular geographical locations can form the basis of historical study, for example, continents, countries, and cities. Understanding why historic events took place is important. To do this, historians often turn to geography. According to Jules Michelet in his book "Histoire de France" (1833), "without geographical basis, the people, the makers of history, seem to be walking on air." Weather patterns, the water supply, and the landscape of a place all affect the lives of the people who live there. For example, to explain why the ancient Egyptians developed a successful civilization, studying the geography of Egypt is essential. Egyptian civilization was built on the banks of the Nile River, which flooded each year, depositing soil on its banks. The rich soil could help farmers grow enough crops to feed the people in the cities. That meant everyone did not have to farm, so some people could perform other jobs that helped develop the civilization. There is also the case of climate, which historians like Ellsworth Huntington and Allen Semple, cited as a crucial influence on the course of history and racial temperament.


Military history concerns warfare, strategies, battles, weapons, and the psychology of combat. The "new military history" since the 1970s has been concerned with soldiers more than generals, with psychology more than tactics, and with the broader impact of warfare on society and culture.

The history of religion has been a main theme for both secular and religious historians for centuries, and continues to be taught in seminaries and academe. Leading journals include "Church History", "The Catholic Historical Review", and "History of Religions". Topics range widely from political and cultural and artistic dimensions, to theology and liturgy. This subject studies religions from all regions and areas of the world where humans have lived.

"Social history", sometimes called the "new social history", is the field that includes history of ordinary people and their strategies and institutions for coping with life. In its "golden age" it was a major growth field in the 1960s and 1970s among scholars, and still is well represented in history departments. In two decades from 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%. In the history departments of British universities in 2007, of the 5723 faculty members, 1644 (29%) identified themselves with social history while political history came next with 1425 (25%).
The "old" social history before the 1960s was a hodgepodge of topics without a central theme, and it often included political movements, like Populism, that were "social" in the sense of being outside the elite system. Social history was contrasted with political history, intellectual history and the history of great men. English historian G. M. Trevelyan saw it as the bridging point between economic and political history, reflecting that, "Without social history, economic history is barren and political history unintelligible." While the field has often been viewed negatively as history with the politics left out, it has also been defended as "history with the people put back in."

The chief subfields of social history include:
Smaller specialties include:

Cultural history replaced social history as the dominant form in the 1980s and 1990s. It typically combines the approaches of anthropology and history to look at language, popular cultural traditions and cultural interpretations of historical experience. It examines the records and narrative descriptions of past knowledge, customs, and arts of a group of people. How peoples constructed their memory of the past is a major topic.
Cultural history includes the study of art in society as well is the study of images and human visual production (iconography).

Diplomatic history focuses on the relationships between nations, primarily regarding diplomacy and the causes of wars. More recently it looks at the causes of peace and human rights. It typically presents the viewpoints of the foreign office, and long-term strategic values, as the driving force of continuity and change in history. This type of "political history" is the study of the conduct of international relations between states or across state boundaries over time. Historian Muriel Chamberlain notes that after the First World War, "diplomatic history replaced constitutional history as the flagship of historical investigation, at once the most important, most exact and most sophisticated of historical studies." She adds that after 1945, the trend reversed, allowing social history to replace it.

Although economic history has been well established since the late 19th century, in recent years academic studies have shifted more and more toward economics departments and away from traditional history departments. Business history deals with the history of individual business organizations, business methods, government regulation, labour relations, and impact on society. It also includes biographies of individual companies, executives, and entrepreneurs. It is related to economic history; Business history is most often taught in business schools.

Environmental history is a new field that emerged in the 1980s to look at the history of the environment, especially in the long run, and the impact of human activities upon it.

World history is the study of major civilizations over the last 3000 years or so. World history is primarily a teaching field, rather than a research field. It gained popularity in the United States, Japan and other countries after the 1980s with the realization that students need a broader exposure to the world as globalization proceeds.

It has led to highly controversial interpretations by Oswald Spengler and Arnold J. Toynbee, among others.

The World History Association publishes the "Journal of World History" every quarter since 1990. The H-World discussion list serves as a network of communication among practitioners of world history, with discussions among scholars, announcements, syllabi, bibliographies and book reviews.

A people's history is a type of historical work which attempts to account for historical events from the perspective of common people. A people's history is the history of the world that is the story of mass movements and of the outsiders. Individuals or groups not included in the past in other type of writing about history are the primary focus, which includes the disenfranchised, the oppressed, the poor, the nonconformists, and the otherwise forgotten people. The authors are typically on the left and have a socialist model in mind, as in the approach of the History Workshop movement in Britain in the 1960s.

Intellectual history and the history of ideas emerged in the mid-20th century, with the focus on the intellectuals and their books on the one hand, and on the other the study of ideas as disembodied objects with a career of their own.

Gender history is a sub-field of History and Gender studies, which looks at the past from the perspective of gender. It is in many ways, an outgrowth of women's history. Despite its relatively short life, Gender History (and its forerunner Women's History) has had a rather significant effect on the general study of history. Since the 1960s, when the initially small field first achieved a measure of acceptance, it has gone through a number of different phases, each with its own challenges and outcomes. Although some of the changes to the study of history have been quite obvious, such as increased numbers of books on famous women or simply the admission of greater numbers of women into the historical profession, other influences are more subtle.

Public history describes the broad range of activities undertaken by people with some training in the discipline of history who are generally working outside of specialized academic settings. Public history practice has quite deep roots in the areas of historic preservation, archival science, oral history, museum curatorship, and other related fields. The term itself began to be used in the U.S. and Canada in the late 1970s, and the field has become increasingly professionalized since that time. Some of the most common settings for public history are museums, historic homes and historic sites, parks, battlefields, archives, film and television companies, and all levels of government.

Professional and amateur historians discover, collect, organize, and present information about past events. They discover this information through archaeological evidence, written primary sources from the past and other various means such as place names. In lists of historians, historians can be grouped by order of the historical period in which they were writing, which is not necessarily the same as the period in which they specialized. Chroniclers and annalists, though they are not historians in the true sense, are also frequently included.

Since the 20th century, Western historians have disavowed the aspiration to provide the "judgement of history." The goals of historical judgements or interpretations are separate to those of legal judgements, that need to be formulated quickly after the events and be final. A related issue to that of the judgement of history is that of collective memory.

Pseudohistory is a term applied to texts which purport to be historical in nature but which depart from standard historiographical conventions in a way which undermines their conclusions.
It is closely related to deceptive historical revisionism. Works which draw controversial conclusions from new, speculative, or disputed historical evidence, particularly in the fields of national, political, military, and religious affairs, are often rejected as pseudohistory.

A major intellectual battle took place in Britain in the early twentieth century regarding the place of history teaching in the universities. At Oxford and Cambridge, scholarship was downplayed. Professor Charles Harding Firth, Oxford's Regius Professor of history in 1904 ridiculed the system as best suited to produce superficial journalists. The Oxford tutors, who had more votes than the professors, fought back in defence of their system saying that it successfully produced Britain's outstanding statesmen, administrators, prelates, and diplomats, and that mission was as valuable as training scholars. The tutors dominated the debate until after the Second World War. It forced aspiring young scholars to teach at outlying schools, such as Manchester University, where Thomas Frederick Tout was professionalizing the History undergraduate programme by introducing the study of original sources and requiring the writing of a thesis.

In the United States, scholarship was concentrated at the major PhD-producing universities, while the large number of other colleges and universities focused on undergraduate teaching. A tendency in the 21st century was for the latter schools to increasingly demand scholarly productivity of their younger tenure-track faculty. Furthermore, universities have increasingly relied on inexpensive part-time adjuncts to do most of the classroom teaching.

From the origins of national school systems in the 19th century, the teaching of history to promote national sentiment has been a high priority. In the United States after World War I, a strong movement emerged at the university level to teach courses in Western Civilization, so as to give students a common heritage with Europe. In the U.S. after 1980, attention increasingly moved toward teaching world history or requiring students to take courses in non-western cultures, to prepare students for life in a globalized economy.

At the university level, historians debate the question of whether history belongs more to social science or to the humanities. Many view the field from both perspectives.

The teaching of history in French schools was influenced by the "Nouvelle histoire" as disseminated after the 1960s by "Cahiers pédagogiques and Enseignement" and other journals for teachers. Also influential was the Institut national de recherche et de documentation pédagogique, (INRDP). Joseph Leif, the Inspector-general of teacher training, said pupils children should learn about historians' approaches as well as facts and dates. Louis François, Dean of the History/Geography group in the Inspectorate of National Education advised that teachers should provide historic documents and promote "active methods" which would give pupils "the immense happiness of discovery." Proponents said it was a reaction against the memorization of names and dates that characterized teaching and left the students bored. Traditionalists protested loudly it was a postmodern innovation that threatened to leave the youth ignorant of French patriotism and national identity.

In several countries history textbooks are tools to foster nationalism and patriotism, and give students the official narrative about national enemies.

In many countries, history textbooks are sponsored by the national government and are written to put the national heritage in the most favourable light. For example, in Japan, mention of the Nanking Massacre has been removed from textbooks and the entire Second World War is given cursory treatment. Other countries have complained. It was standard policy in communist countries to present only a rigid Marxist historiography.

In the United States, the history of the Southern states, slavery and the American Civil War are controversial topics. McGraw-Hill Education for example, was criticised for describing Africans brought to American plantations as "workers" instead of slaves in a textbook.

Academic historians have often fought against the politicization of the textbooks, sometimes with success.

In 21st-century Germany, the history curriculum is controlled by the 16 states, and is characterized not by superpatriotism but rather by an "almost pacifistic and deliberately unpatriotic undertone" and reflects "principles formulated by international organizations such as UNESCO or the Council of Europe, thus oriented towards human rights, democracy and peace." The result is that "German textbooks usually downplay national pride and ambitions and aim to develop an understanding of citizenship centered on democracy, progress, human rights, peace, tolerance and Europeanness."






</doc>
<doc id="26791" url="https://en.wikipedia.org/wiki?curid=26791" title="Satire">
Satire

In fiction and less frequently in non-fiction, satire is a genre of literature and performing arts, in which vices, follies, abuses and shortcomings are held up to ridicule, ideally with the intent of shaming individuals, corporations, government, or society itself into improvement. Although satire is usually meant to be humorous, its greater purpose is often constructive social criticism, using wit to draw attention to both particular and wider issues in society.

A feature of satire is strong irony or sarcasm —"in satire, irony is militant", according to literary critic Northrup Frye— but parody, burlesque, exaggeration, juxtaposition, comparison, analogy, and double entendre are all frequently used in satirical speech and writing. This "militant" irony or sarcasm often professes to approve of (or at least accept as natural) the very things the satirist wishes to question.

Satire is nowadays found in many artistic forms of expression, including internet memes, literature, plays, commentary, television shows, and media such as lyrics.

The word satire comes from the Latin word "satur" and the subsequent phrase "lanx satura." "Satur" meant "full" but the juxtaposition with "lanx" shifted the meaning to "miscellany or medley": the expression "lanx satura" literally means "a full dish of various kinds of fruits".

The word "satura" as used by Quintilian, however, was used to denote only Roman verse satire, a strict genre that imposed hexameter form, a narrower genre than what would be later intended as "satire". Quintilian famously said that "satura," that is a satire in hexameter verses, was a literary genre of wholly Roman origin ("satura tota nostra est"). He was aware of and commented on Greek satire, but at the time did not label it as such, although today the origin of satire is considered to be Aristophanes' Old Comedy. The first critic to use the term "satire" in the modern broader sense was Apuleius.

To Quintilian, the satire was a strict literary form, but the term soon escaped from the original narrow definition. Robert Elliott writes:
The word "satire" derives from "satura", and its origin was not influenced by the Greek mythological figure of the "satyr". In the 17th century, philologist Isaac Casaubon was the first to dispute the etymology of satire from satyr, contrary to the belief up to that time.

Laughter is not an essential component of satire; in fact there are types of satire that are not meant to be "funny" at all. Conversely, not all humour, even on such topics as politics, religion or art is necessarily "satirical", even when it uses the satirical tools of irony, parody, and burlesque.

Even light-hearted satire has a serious "after-taste": the organizers of the Ig Nobel Prize describe this as "first make people laugh, and then make them think".

Satire and irony in some cases have been regarded as the most effective source to understand a society, the oldest form of social study. They provide the keenest insights into a group's collective psyche, reveal its deepest values and tastes, and the society's structures of power. Some authors have regarded satire as superior to non-comic and non-artistic disciplines like history or anthropology. In a prominent example from ancient Greece, philosopher Plato, when asked by a friend for a book to understand Athenian society, referred him to the plays of Aristophanes.

Historically, satire has satisfied the popular need to debunk and ridicule the leading figures in politics, economy, religion and other prominent realms of power. Satire confronts public discourse and the collective imaginary, playing as a public opinion counterweight to power (be it political, economic, religious, symbolic, or otherwise), by challenging leaders and authorities. For instance, it forces administrations to clarify, amend or establish their policies. Satire's job is to expose problems and contradictions, and it's not obligated to solve them. Karl Kraus set in the history of satire a prominent example of a satirist role as confronting public discourse.

For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. The satiric impulse, and its ritualized expressions, carry out the function of resolving social tension. Institutions like the ritual clowns, by giving expression to the antisocial tendencies, represent a safety valve which re-establishes equilibrium and health in the collective imaginary, which are jeopardized by the repressive aspects of society.

The state of political satire in a given society reflects the tolerance or intolerance that characterizes it, and the state of civil liberties and human rights. Under totalitarian regimes any criticism of a political system, and especially satire, is suppressed. A typical example is the Soviet Union where the dissidents, such as Aleksandr Solzhenitsyn and Andrei Sakharov were under strong pressure from the government. While satire of everyday life in the USSR was allowed, the most prominent satirist being Arkady Raikin, political satire existed in the form of anecdotes that made fun of Soviet political leaders, especially Brezhnev, famous for his narrow-mindedness and love for awards and decorations.

Satire is a diverse genre which is complex to classify and define, with a wide range of satiric "modes".

Satirical literature can commonly be categorized as either Horatian, Juvenalian, or Menippean.

Horatian satire, named for the Roman satirist Horace (65–8 BCE), playfully criticizes some social vice through gentle, mild, and light-hearted humour. Horace (Quintus Horatius Flaccus) wrote Satires to gently ridicule the dominant opinions and "philosophical beliefs of ancient Rome and Greece" (Rankin). Rather than writing in harsh or accusing tones, he addressed issues with humor and clever mockery. Horatian satire follows this same pattern of "gently [ridiculing] the absurdities and follies of human beings" (Drury).

It directs wit, exaggeration, and self-deprecating humour toward what it identifies as folly, rather than evil. Horatian satire's sympathetic tone is common in modern society.

A Horatian satirist's goal is to heal the situation with smiles, rather than by anger. Horatian satire is a gentle reminder to take life less seriously and evokes a wry smile. A Horatian satirist makes fun of general human folly rather than engaging in specific or personal attacks. Shamekia Thomas suggests, "In a work using Horatian satire, readers often laugh at the characters in the story who are the subject of mockery as well as themselves and society for behaving in those ways." Alexander Pope has been established as an author whose satire "heals with morals what it hurts with wit" (Green). Alexander Pope—and Horatian satire—attempt to teach.

Examples of Horatian satire:


Juvenalian satire, named for the writings of the Roman satirist Juvenal (late first century – early second century AD), is more contemptuous and abrasive than the Horatian. Juvenal disagreed with the opinions of the public figures and institutions of the Republic and actively attacked them through his literature. "He utilized the satirical tools of exaggeration and parody to make his targets appear monstrous and incompetent" (Podzemny). Juvenal's satire follows this same pattern of abrasively ridiculing societal structures. Juvenal also, unlike Horace, attacked public officials and governmental organizations through his satires, regarding their opinions as not just wrong, but evil.

Following in this tradition, Juvenalian satire addresses perceived social evil through scorn, outrage, and savage ridicule. This form is often pessimistic, characterized by the use of irony, sarcasm, moral indignation and personal invective, with less emphasis on humor. Strongly polarized political satire can often be classified as Juvenalian.

A Juvenal satirist's goal is generally to provoke some sort of political or societal change because he sees his opponent or object as evil or harmful. A Juvenal satirist mocks "societal structure, power, and civilization" (Thomas) by exaggerating the words or position of his opponent in order to jeopardize their opponent's reputation and/or power. Jonathan Swift has been established as an author who "borrowed heavily from Juvenal's techniques in [his critique] of contemporary English society" (Podzemny).

Examples of Juvenalian satire:

See Menippean satire.

In the history of theatre there has always been a conflict between engagement and disengagement on politics and relevant issue, between satire and grotesque on one side, and jest with teasing on the other. Max Eastman defined the spectrum of satire in terms of "degrees of biting", as ranging from satire proper at the hot-end, and "kidding" at the violet-end; Eastman adopted the term kidding to denote what is just satirical in form, but is not really firing at the target. Nobel laureate satirical playwright Dario Fo pointed out the difference between satire and teasing ("sfottò"). Teasing is the reactionary side of the comic; it limits itself to a shallow parody of physical appearance. The side-effect of teasing is that it humanizes and draws sympathy for the powerful individual towards which it is directed. Satire instead uses the comic to go against power and its oppressions, has a subversive character, and a moral dimension which draws judgement against its targets. Fo formulated an operational criterion to tell real satire from "sfottò", saying that real satire arouses an outraged and violent reaction, and that the more they try to stop you, the better is the job you are doing. Fo contends that, historically, people in positions of power have welcomed and encouraged good-humoured buffoonery, while modern day people in positions of power have tried to censor, ostracize and repress satire.

Teasing ("sfottò") is an ancient form of simple buffoonery, a form of comedy without satire's subversive edge. Teasing includes light and affectionate parody, good-humoured mockery, simple one-dimensional poking fun, and benign spoofs. Teasing typically consists of an impersonation of someone monkeying around with his exterior attributes, tics, physical blemishes, voice and mannerisms, quirks, way of dressing and walking, and/or the phrases he typically repeats. By contrast, teasing never touches on the core issue, never makes a serious criticism judging the target with irony; it never harms the target's conduct, ideology and position of power; it never undermines the perception of his morality and cultural dimension. "Sfottò" directed towards a powerful individual makes him appear more human and draws sympathy towards him. Hermann Göring propagated jests and jokes against himself, with the aim of humanizing his image.

Types of satire can also be classified according to the topics it deals with. From the earliest times, at least since the plays of Aristophanes, the primary topics of literary satire have been politics, religion and sex. This is partly because these are the most pressing problems that affect anybody living in a society, and partly because these topics are usually taboo. Among these, politics in the broader sense is considered the pre-eminent topic of satire. Satire which targets the clergy is a type of political satire, while religious satire is that which targets religious beliefs. Satire on sex may overlap with blue comedy, off-color humor and dick jokes.

Scatology has a long literary association with satire, as it is a classical mode of the grotesque, the grotesque body and the satiric grotesque. Shit plays a fundamental role in satire because it symbolizes death, the turd being "the ultimate dead object". The satirical comparison of individuals or institutions with human excrement, exposes their "inherent inertness, corruption and dead-likeness". The ritual clowns of clown societies, like among the Pueblo Indians, have ceremonies with filth-eating. In other cultures, sin-eating is an apotropaic rite in which the sin-eater (also called filth-eater), by ingesting the food provided, takes "upon himself the sins of the departed". Satire about death overlaps with black humor and gallows humor.

Another classification by topics is the distinction between political satire, religious satire and satire of manners. Political satire is sometimes called topical satire, satire of manners is sometimes called satire of everyday life, and religious satire is sometimes called philosophical satire. Comedy of manners, sometimes also called satire of manners, criticizes mode of life of common people; political satire aims at behavior, manners of politicians, and vices of political systems. Historically, comedy of manners, which first appeared in British theater in 1620, has uncritically accepted the social code of the upper classes. Comedy in general accepts the rules of the social game, while satire subverts them.

Another analysis of satire is the spectrum of his possible tones: wit, ridicule, irony, sarcasm, cynicism, the sardonic and invective. 

The type of humour that deals with creating laughter at the expense of the person telling the joke is called reflexive humour.Reflexive humour can take place at dual levels of directing humour at self or at the larger community the self identifies with. The audience's understanding of the context of reflexive humour is important for its receptivity and success . Satire is found not only in written literary forms. In preliterate cultures it manifests itself in ritual and folk forms, as well as in trickster tales and oral poetry. 

It appears also in graphic arts, music, sculpture, dance, cartoon strips, and graffiti. Examples are Dada sculptures, Pop Art works, music of Gilbert and Sullivan and Erik Satie, punk and rock music. In modern media culture, stand-up comedy is an enclave in which satire can be introduced into mass media, challenging mainstream discourse. Comedy roasts, mock festivals, and stand-up comedians in nightclubs and concerts are the modern forms of ancient satiric rituals.

One of the earliest examples of what we might call satire, The Satire of the Trades, is in Egyptian writing from the beginning of the 2nd millennium BC. The text's apparent readers are students, tired of studying. It argues that their lot as scribes is not only useful, but far superior to that of the ordinary man. Scholars such as Helck think that the context was meant to be serious.

The Papyrus Anastasi I (late 2nd millennium BC) contains a satirical letter which first praises the virtues of its recipient, but then mocks the reader's meagre knowledge and achievements.

The Greeks had no word for what later would be called "satire", although the terms cynicism and parody were used. Modern critics call the Greek playwright Aristophanes one of the best known early satirists: his plays are known for their critical political and societal commentary, particularly for the political satire by which he criticized the powerful Cleon (as in "The Knights"). He is also notable for the persecution he underwent. Aristophanes' plays turned upon images of filth and disease. His bawdy style was adopted by Greek dramatist-comedian Menander. His early play "Drunkenness" contains an attack on the politician Callimedon.

The oldest form of satire still in use is the Menippean satire by Menippus of Gadara. His own writings are lost. Examples from his admirers and imitators mix seriousness and mockery in dialogues and present parodies before a background of diatribe. As in the case of Aristophanes plays, menippean satire turned upon images of filth and disease.

The first Roman to discuss satire critically was Quintilian, who invented the term to describe the writings of Gaius Lucilius. The two most prominent and influential ancient Roman satirists are Horace and Juvenal, who wrote during the early days of the Roman Empire. Other important satirists in ancient Latin are Gaius Lucilius and Persius. "Satire" in their work is much wider than in the modern sense of the word, including fantastic and highly coloured humorous writing with little or no real mocking intent. When Horace criticized Augustus, he used veiled ironic terms. In contrast, Pliny reports that the 6th-century-BC poet Hipponax wrote "satirae" that were so cruel that the offended hanged themselves.

In the 2nd century AD, Lucian wrote "True History", a book satirizing the clearly unrealistic travelogues/adventures written by Ctesias, Iambulus, and Homer. He states that he was surprised they expected people to believe their lies, and stating that he, like them, has no actual knowledge or experience, but shall now tell lies as if he did. He goes on to describe a far more obviously extreme and unrealistic tale, involving interplanetary exploration, war among alien life forms, and life inside a 200 mile long whale back in the terrestrial ocean, all intended to make obvious the fallacies of books like "Indica" and "The Odyssey".

Medieval Arabic poetry included the satiric genre "hija". Satire was introduced into Arabic prose literature by the author Al-Jahiz in the 9th century. While dealing with serious topics in what are now known as anthropology, sociology and psychology, he introduced a satirical approach, "based on the premise that, however serious the subject under review, it could be made more interesting and thus achieve greater effect, if only one leavened the lump of solemnity by the insertion of a few amusing anecdotes or by the throwing out of some witty or paradoxical observations. He was well aware that, in treating of new themes in his prose works, he would have to employ a vocabulary of a nature more familiar in "hija", satirical poetry." For example, in one of his zoological works, he satirized the preference for longer human penis size, writing: "If the length of the penis were a sign of honor, then the mule would belong to the (honorable tribe of) Quraysh". Another satirical story based on this preference was an "Arabian Nights" tale called "Ali with the Large Member".

In the 10th century, the writer Tha'alibi recorded satirical poetry written by the Arabic poets As-Salami and Abu Dulaf, with As-Salami praising Abu Dulaf's wide breadth of knowledge and then mocking his ability in all these subjects, and with Abu Dulaf responding back and satirizing As-Salami in return. An example of Arabic political satire included another 10th-century poet Jarir satirizing Farazdaq as "a transgressor of the Sharia" and later Arabic poets in turn using the term "Farazdaq-like" as a form of political satire.

The terms "comedy" and "satire" became synonymous after Aristotle's "Poetics" was translated into Arabic in the medieval Islamic world, where it was elaborated upon by Islamic philosophers and writers, such as Abu Bischr, his pupil Al-Farabi, Avicenna, and Averroes. Due to cultural differences, they disassociated comedy from Greek dramatic representation and instead identified it with Arabic poetic themes and forms, such as "hija" (satirical poetry). They viewed comedy as simply the "art of reprehension", and made no reference to light and cheerful events, or troubled beginnings and happy endings, associated with classical Greek comedy. After the Latin translations of the 12th century, the term "comedy" thus gained a new semantic meaning in Medieval literature.

Ubayd Zakani introduced satire in Persian literature during the 14th century. His work is noted for its satire and obscene verses, often political or bawdy, and often cited in debates involving homosexual practices. He wrote the "Resaleh-ye Delgosha", as well as "Akhlaq al-Ashraf" ("Ethics of the Aristocracy") and the famous humorous fable "Masnavi Mush-O-Gorbeh" (Mouse and Cat), which was a political satire. His non-satirical serious classical verses have also been regarded as very well written, in league with the other great works of Persian literature. Between 1905 and 1911, Bibi Khatoon Astarabadi and other Iranian writers wrote notable satires.

In the Early Middle Ages, examples of satire were the songs by Goliards or vagants now best known as an anthology called Carmina Burana and made famous as texts of a composition by the 20th-century composer Carl Orff. Satirical poetry is believed to have been popular, although little has survived. With the advent of the High Middle Ages and the birth of modern vernacular literature in the 12th century, it began to be used again, most notably by Chaucer. The disrespectful manner was considered "unchristian" and ignored, except for the moral satire, which mocked misbehaviour in Christian terms. Examples are "Livre des Manières" by (~1178), and some of Chaucer's "Canterbury Tales". Sometimes epic poetry (epos) was mocked, and even feudal society, but there was hardly a general interest in the genre.

Direct social commentary via satire returned with a vengeance in the 16th century, when farcical texts such as the works of François Rabelais tackled more serious issues (and incurred the wrath of the crown as a result).

Two major satirists of Europe in the Renaissance were Giovanni Boccaccio and François Rabelais. Other examples of Renaissance satire include "Till Eulenspiegel", "Reynard the Fox", Sebastian Brant's "Narrenschiff" (1494), Erasmus's "Moriae Encomium" (1509), Thomas More's "Utopia" (1516), and "Carajicomedia" (1519).

The Elizabethan (i.e. 16th-century English) writers thought of satire as related to the notoriously rude, coarse and sharp satyr play. Elizabethan "satire" (typically in pamphlet form) therefore contains more straightforward abuse than subtle irony. The French Huguenot Isaac Casaubon pointed out in 1605 that satire in the Roman fashion was something altogether more civilised. Casaubon discovered and published Quintilian's writing and presented the original meaning of the term (satira, not satyr), and the sense of wittiness (reflecting the "dishfull of fruits") became more important again. Seventeenth-century English satire once again aimed at the "amendment of vices" (Dryden).

In the 1590s a new wave of verse satire broke with the publication of Hall's "Virgidemiarum", six books of verse satires targeting everything from literary fads to corrupt noblemen. Although Donne had already circulated satires in manuscript, Hall's was the first real attempt in English at verse satire on the Juvenalian model. The success of his work combined with a national mood of disillusion in the last years of Elizabeth's reign triggered an avalanche of satire—much of it less conscious of classical models than Hall's — until the fashion was brought to an abrupt stop by censorship.

Satire ("Kataksh" or "Vyang") has played a prominent role in Indian and Hindi literature, and is counted as one of the "ras" of literature in ancient books. With the commencement of printing of books in local language in the nineteenth century and especially after India's freedom, this grew. Many of the works of Tulsi Das, Kabir, Munshi Premchand, village ministrels, Hari katha singers, poets, Dalit singers and current day stand up Indian comedians incorporate satire, usually ridiculing authoritarians, fundamentalists and incompetent people in power. In India, it has usually been used as a means of expression and an outlet for common people to express their anger against authoritarian entities. A popular custom in Northern India of "Bura na mano Holi hai" continues, in which comedians on the stage roast local people of importance (who are usually brought in as special guests).

The Age of Enlightenment, an intellectual movement in the 17th and 18th centuries advocating rationality, produced a great revival of satire in Britain. This was fuelled by the rise of partisan politics, with the formalisation of the Tory and Whig parties—and also, in 1714, by the formation of the Scriblerus Club, which included Alexander Pope, Jonathan Swift, John Gay, John Arbuthnot, Robert Harley, Thomas Parnell, and Henry St John, 1st Viscount Bolingbroke. This club included several of the notable satirists of early-18th-century Britain. They focused their attention on Martinus Scriblerus, "an invented learned fool... whose work they attributed all that was tedious, narrow-minded, and pedantic in contemporary scholarship". In their hands astute and biting satire of institutions and individuals became a popular weapon. The turn to the 18th century was characterized by a switch from Horatian, soft, pseudo-satire, to biting "juvenal" satire.

Jonathan Swift was one of the greatest of Anglo-Irish satirists, and one of the first to practise modern journalistic satire. For instance, In his "A Modest Proposal" Swift suggests that Irish peasants be encouraged to sell their own children as food for the rich, as a solution to the "problem" of poverty. His purpose is of course to attack indifference to the plight of the desperately poor. In his book "Gulliver's Travels" he writes about the flaws in human society in general and English society in particular. John Dryden wrote an influential essay entitled "A Discourse Concerning the Original and Progress of Satire" that helped fix the definition of satire in the literary world. His satirical "Mac Flecknoe" was written in response to a rivalry with Thomas Shadwell and eventually inspired Alexander Pope to write his satirical "The Rape of the Lock". Other satirical works by Pope include the "Epistle to Dr Arbuthnot".

Alexander Pope (b. May 21, 1688) was a satirist known for his Horatian satirist style and translation of the "Iliad". Famous throughout and after the long 18th century, Pope died in 1744. Pope, in his "The Rape of the Lock", is delicately chiding society in a sly but polished voice by holding up a mirror to the follies and vanities of the upper class. Pope does not actively attack the self-important pomp of the British aristocracy, but rather presents it in such a way that gives the reader a new perspective from which to easily view the actions in the story as foolish and ridiculous. A mockery of the upper class, more delicate and lyrical than brutal, Pope nonetheless is able to effectively illuminate the moral degradation of society to the public. "The Rape of the Lock" assimilates the masterful qualities of a heroic epic, such as the "Iliad", which Pope was translating at the time of writing "The Rape of the Lock". However, Pope applied these qualities satirically to a seemingly petty egotistical elitist quarrel to prove his point wryly.

Daniel Defoe pursued a more journalistic type of satire, being famous for his "The True-Born Englishman" which mocks xenophobic patriotism, and "The Shortest-Way with the Dissenters"—advocating religious toleration by means of an ironical exaggeration of the highly intolerant attitudes of his time.

The pictorial satire of William Hogarth is a precursor to the development of political cartoons in 18th-century England. The medium developed under the direction of its greatest exponent, James Gillray from London. With his satirical works calling the king (George III), prime ministers and generals (especially Napoleon) to account, Gillray's wit and keen sense of the ridiculous made him the pre-eminent cartoonist of the era.

Ebenezer Cooke (1665–1732), author of "The Sot-Weed Factor" (1708), was among the first American colonialists to write literary satire. Benjamin Franklin (1706–1790) and others followed, using satire to shape an emerging nation's culture through its sense of the ridiculous.

Several satiric papers competed for the public's attention in the Victorian era (1837–1901) and Edwardian period, such as "Punch" (1841) and "Fun" (1861).

Perhaps the most enduring examples of Victorian satire, however, are to be found in the Savoy Operas of Gilbert and Sullivan. In fact, in "The Yeomen of the Guard", a jester is given lines that paint a very neat picture of the method and purpose of the satirist, and might almost be taken as a statement of Gilbert's own intent:

Novelists such as Charles Dickens (1812-1870) often used passages of satiric writing in their treatment of social issues.

Continuing the tradition of Swiftian journalistic satire, Sidney Godolphin Osborne (1808-1889) was the most prominent writer of scathing "Letters to the Editor" of the London Times. Famous in his day, he is now all but forgotten. His maternal grandfather William Eden, 1st Baron Auckland was considered to be a possible candidate for the authorship of the Junius letters. If this were true, we can read Osborne as following in his grandfather's satiric "Letters to the Editor" path. Osborne's satire was so bitter and biting that at one point he received a public censure from Parliament's then Home Secretary Sir James Graham. Osborne wrote mostly in the Juvenalian mode over a wide range of topics mostly centered on British government's and landlords' mistreatment of poor farm workers and field laborers. He bitterly opposed the New Poor Laws and was passionate on the subject of Great Britain's botched response to the Irish Famine and its mistreatment of soldiers during the Crimean War.

Later in the nineteenth century, in the United States, Mark Twain (1835–1910) grew to become American's greatest satirist: his novel "Huckleberry Finn" (1884) is set in the antebellum South, where the moral values Twain wishes to promote are completely turned on their heads. His hero, Huck, is a rather simple but goodhearted lad who is ashamed of the "sinful temptation" that leads him to help a runaway slave. In fact his conscience, warped by the distorted moral world he has grown up in, often bothers him most when he is at his best. He is prepared to do good, believing it to be wrong.

Twain's younger contemporary Ambrose Bierce (1842–1913) gained notoriety as a cynic, pessimist and black humorist with his dark, bitterly ironic stories, many set during the American Civil War, which satirized the limitations of human perception and reason. Bierce's most famous work of satire is probably "The Devil's Dictionary" (1906), in which the definitions mock cant, hypocrisy and received wisdom.

Karl Kraus is considered the first major European satirist since Jonathan Swift. In 20th-century literature, satire was used by English authors such as Aldous Huxley (1930s) and George Orwell (1940s), which under the inspiration of Zamyatin's Russian 1921 novel "We", made serious and even frightening commentaries on the dangers of the sweeping social changes taking place throughout Europe. Anatoly Lunacharsky wrote ‘Satire attains its greatest significance when a newly evolving class creates an ideology considerably more advanced than that of the ruling class, but has not yet developed to the point where it can conquer it. Herein lies its truly great ability to triumph, its scorn for its adversary and its hidden fear of it. Herein lies its venom, its amazing energy of hate, and quite frequently, its grief, like a black frame around glittering images. Herein lie its contradictions, and its power.’ Many social critics of this same time in the United States, such as Dorothy Parker and H. L. Mencken, used satire as their main weapon, and Mencken in particular is noted for having said that "one horse-laugh is worth ten thousand syllogisms" in the persuasion of the public to accept a criticism. Novelist Sinclair Lewis was known for his satirical stories such as "Main Street" (1920), "Babbitt" (1922), "Elmer Gantry" (1927; dedicated by Lewis to H. L. Menchen), and "It Can't Happen Here" (1935), and his books often explored and satirized contemporary American values. The film "The Great Dictator" (1940) by Charlie Chaplin is itself a parody of Adolf Hitler; Chaplin later declared that he would have not made the film if he had known about the concentration camps.
In the United States 1950s, satire was introduced into American stand-up comedy most prominently by Lenny Bruce and Mort Sahl. As they challenged the taboos and conventional wisdom of the time, were ostracized by the mass media establishment as "sick comedians". In the same period, Paul Krassner's magazine "The Realist" began publication, to become immensely popular during the 1960s and early 1970s among people in the counterculture; it had articles and cartoons that were savage, biting satires of politicians such as Lyndon Johnson and Richard Nixon, the Vietnam War, the Cold War and the War on Drugs. This baton was also carried by the original National Lampoon magazine, edited by Doug Kenney and Henry Beard and featuring blistering satire written by Michael O'Donoghue, P.J. O'Rourke, and Tony Hendra, among others. Prominent satiric stand-up comedian George Carlin acknowledged the influence "The Realist" had in his 1970s conversion to a satiric comedian.

A more humorous brand of satire enjoyed a renaissance in the UK in the early 1960s with the satire boom, led by such luminaries as Peter Cook, Alan Bennett, Jonathan Miller, and Dudley Moore, whose stage show "Beyond the Fringe" was a hit not only in Britain, but also in the United States. Other significant influences in 1960s British satire include David Frost, Eleanor Bron and the television program "That Was The Week That Was".

Joseph Heller's most famous work, "Catch-22" (1961), satirizes bureaucracy and the military, and is frequently cited as one of the greatest literary works of the twentieth century. Departing from traditional Hollywood farce and screwball, director and comedian Jerry Lewis used satire in his self-directed films "The Bellboy" (1960), "The Errand Boy" (1961) and "The Patsy" (1964) to comment on celebrity and the star-making machinery of Hollywood.
The film "Dr. Strangelove" (1964) starring Peter Sellers was a popular satire on the Cold War.

Contemporary popular usage of the term "satire" is often very imprecise. While satire often uses caricature and parody, by no means all uses of these or other humorous devices are satiric. Refer to the careful definition of satire that heads this article.
Satire is used on many UK television programmes, particularly popular panel shows and quiz shows such as "Mock the Week" (2005–ongoing) and "Have I Got News for You" (1990–ongoing). It is found on radio quiz shows such as "The News Quiz" (1977–ongoing) and "The Now Show" (1998–ongoing). One of the most watched UK television shows of the 1980s and early 1990s, the puppet show "Spitting Image" was a satire of the royal family, politics, entertainment, sport and British culture of the era. Court Flunkey from "Spitting Image" is a caricature of James Gillray, intended as a homage to the father of political cartooning. Created by DMA Design in 1997, satire features prominently in the British video game series "Grand Theft Auto".

Trey Parker and Matt Stone's "South Park" (1997–ongoing) relies almost exclusively on satire to address issues in American culture, with episodes addressing racism, anti-Semitism, militant atheism, homophobia, sexism, environmentalism, corporate culture, political correctness and anti-Catholicism, among many other issues.

Australian Chris Lilley produces comedy art in the style of mockumentaries ("", "Summer Heights High", "Angry Boys") and his work is often described as complex social satire.
Stephen Colbert's television program, "The Colbert Report" (2005–14), is instructive in the methods of contemporary American satire. Colbert's character is an opinionated and self-righteous commentator who, in his TV interviews, interrupts people, points and wags his finger at them, and "unwittingly" uses a number of logical fallacies. In doing so, he demonstrates the principle of modern American political satire: the ridicule of the actions of politicians and other public figures by taking all their statements and purported beliefs to their furthest (supposedly) logical conclusion, thus revealing their perceived hypocrisy or absurdity.

The American sketch comedy television show "Saturday Night Live" is also known for its satirical impressions and parodies of prominent persons and politicians, among some of the most notable, their parodies of U.S. political figures Hillary Clinton and of Sarah Palin.

Other political satire includes various political causes in the past, including the relatively successful Polish Beer-Lovers' Party and the joke political candidates Molly the Dog and Brian Miner.

In the United Kingdom, a popular modern satirist was the late Sir Terry Pratchett, author of the internationally best-selling "Discworld" book series. One of the most well-known and controversial British satirists is Chris Morris, co-writer and director of "Four Lions".

In Canada, satire has become an important part of the comedy scene. Stephen Leacock was one of the best known early Canadian satirists, and in the early 20th century, he achieved fame by targeting the attitudes of small town life. In more recent years, Canada has had several prominent satirical television series and radio shows. Some, including "CODCO", "The Royal Canadian Air Farce", "This Is That", and "This Hour Has 22 Minutes" deal directly with current news stories and political figures, while others, like "History Bites" present contemporary social satire in the context of events and figures in history. The Beaverton is a Canadian news satire site similar to The Onion. Canadian songwriter Nancy White uses music as the vehicle for her satire, and her comic folk songs are regularly played on CBC Radio.

Cartoonists often use satire as well as straight humour. Al Capp's satirical comic strip "Li'l Abner" was censored in September 1947. The controversy, as reported in "Time", centred on Capp's portrayal of the US Senate. Said Edward Leech of Scripps-Howard, "We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables." Walt Kelly's "Pogo" was likewise censored in 1952 over his overt satire of Senator Joe McCarthy, caricatured in his comic strip as "Simple J. Malarky". Garry Trudeau, whose comic strip "Doonesbury" focuses on satire of the political system, and provides a trademark cynical view on national events. Trudeau exemplifies humour mixed with criticism. For example, the character Mark Slackmeyer lamented that because he was not legally married to his partner, he was deprived of the "exquisite agony" of experiencing a nasty and painful divorce like heterosexuals. This, of course, satirized the claim that gay unions would denigrate the sanctity of heterosexual marriage.
Like some literary predecessors, many recent television satires contain strong elements of parody and caricature; for instance, the popular animated series "The Simpsons" and "South Park" both parody modern family and social life by taking their assumptions to the extreme; both have led to the creation of similar series. As well as the purely humorous effect of this sort of thing, they often strongly criticise various phenomena in politics, economic life, religion and many other aspects of society, and thus qualify as satirical. Due to their animated nature, these shows can easily use images of public figures and generally have greater freedom to do so than conventional shows using live actors.

News satire is also a very popular form of contemporary satire, appearing in as wide an array of formats as the news media itself: print (e.g. "The Onion", "Waterford Whispers News", "Private Eye"), radio (e.g. "On the Hour"), television (e.g. "The Day Today", "The Daily Show", "Brass Eye") and the web (e.g. "Faking News", "El Koshary Today", "Babylon Bee", "The Beaverton", "The Daily Bonnet" and "The Onion"). Other satires are on the list of satirists and satires. Another internet-driven form of satire is to lampoon bad internet performers. An example of this is the Internet meme character Miranda Sings.

In an interview with "Wikinews", Sean Mills, President of "The Onion", said angry letters about their news parody always carried the same message. "It's whatever affects that person", said Mills. "So it's like, 'I love it when you make a joke about murder or rape, but if you talk about cancer, well my brother has cancer and that's not funny to me.' Or someone else can say, 'Cancer's "hilarious", but don't talk about rape because my cousin got raped.' Those are rather extreme examples, but if it affects somebody personally, they tend to be more sensitive about it."

Zhou Libo, a comedian from Shanghai, is the most popular satirist in China. His humour has interested middle-class people and has sold-out shows ever since his rise to fame.

Paweł Kuczyński is a polish artist that has been awarded many prices for his works on political satyre. His topics range from the absurdity of war to the absorbing power of today's technology.

Literary satire is usually written out of earlier satiric works, reprising previous conventions, commonplaces, stance, situations and tones of voice. Exaggeration is one of the most common satirical techniques. Contrarily diminution is also a satirical technique.

For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. In Germany and Italy satire is protected by the constitution.

Since satire belongs to the realm of art and artistic expression, it benefits from broader lawfulness limits than mere freedom of information of journalistic kind. In some countries a specific "right to satire" is recognized and its limits go beyond the "right to report" of journalism and even the "right to criticize". Satire benefits not only of the protection to freedom of speech, but also to that to culture, and that to scientific and artistic production.

In September 2017 The Juice Media received an e-mail from the Australian National Symbols Officer requesting that the use of a satirical logo, called the "Coat of Harms" based on the Australian Coat of Arms, no longer be used as they had received complaints from the members of the public. Coincidentally 5 days later a Bill was proposed to Australian parliament to amend the Criminal Code Act 1995. If successfully passed those found to be in breach of the new amendment can face 2–5 years imprisonment.

As of June 2018, the Criminal Code Amendment (Impersonating a Commonwealth Body) Bill 2017 was before the Australian Senate with the third reading moved May 10, 2018.

Descriptions of satire's biting effect on its target include 'venomous', 'cutting', 'stinging', vitriol. Because satire often combines anger and humor, as well as the fact that it addresses and calls into question many controversial issues, it can be profoundly disturbing.

Because it is essentially ironic or sarcastic, satire is often misunderstood. A typical misunderstanding is to confuse the satirist with his persona.

Common uncomprehending responses to satire include revulsion (accusations of poor taste, or that "it's just not funny" for instance) and the idea that the satirist actually does support the ideas, policies, or people he is attacking. For instance, at the time of its publication, many people misunderstood Swift's purpose in "A Modest Proposal", assuming it to be a serious recommendation of economically motivated cannibalism.

Some critics of Mark Twain see "Huckleberry Finn" as racist and offensive, missing the point that its author clearly intended it to be satire (racism being in fact only one of a number of Mark Twain's known concerns attacked in "Huckleberry Finn"). This same misconception was suffered by the main character of the 1960s British television comedy satire "Till Death Us Do Part". The character of Alf Garnett (played by Warren Mitchell) was created to poke fun at the kind of narrow-minded, racist, little Englander that Garnett represented. Instead, his character became a sort of anti-hero to people who actually agreed with his views. (The same situation occurred with Archie Bunker in American TV show "All in the Family", a character derived directly from Garnett.)

The Australian satirical television comedy show "The Chaser's War on Everything" has suffered repeated attacks based on various perceived interpretations of the "target" of its attacks. The "Make a Realistic Wish Foundation" sketch (June 2009), which attacked in classical satiric fashion the heartlessness of people who are reluctant to donate to charities, was widely interpreted as an attack on the Make a Wish Foundation, or even the terminally ill children helped by that organisation. Prime Minister of the time Kevin Rudd stated that The Chaser team "should hang their heads in shame". He went on to say that "I didn't see that but it's been described to me. ...But having a go at kids with a terminal illness is really beyond the pale, absolutely beyond the pale." Television station management suspended the show for two weeks and reduced the third season to eight episodes.

The romantic prejudice against satire is the belief spread by the romantic movement that satire is something unworthy of serious attention; this prejudice has held considerable influence to this day. Such prejudice extends to humour and everything that arouses laughter, which are often underestimated as frivolous and unworthy of serious study. For instance, humor is generally neglected as a topic of anthropological research and teaching.

Because satire criticises in an ironic, essentially indirect way, it frequently escapes censorship in a way more direct criticism might not. Periodically, however, it runs into serious opposition, and people in power who perceive themselves as attacked attempt to censor it or prosecute its practitioners. In a classic example, Aristophanes was persecuted by the demagogue Cleon.

In 1599, the Archbishop of Canterbury John Whitgift and the Bishop of London Richard Bancroft, whose offices had the function of licensing books for publication in England, issued a decree banning verse satire. The decree, now known as the Bishops' Ban of 1599, ordered the burning of certain volumes of satire by John Marston, Thomas Middleton, Joseph Hall, and others; it also required histories and plays to be specially approved by a member of the Queen's Privy Council, and it prohibited the future printing of satire in verse.

The motives for the ban are obscure, particularly since some of the books banned had been licensed by the same authorities less than a year earlier. Various scholars have argued that the target was obscenity, libel, or sedition. It seems likely that lingering anxiety about the Martin Marprelate controversy, in which the bishops themselves had employed satirists, played a role; both Thomas Nashe and Gabriel Harvey, two of the key figures in that controversy, suffered a complete ban on all their works. In the event, though, the ban was little enforced, even by the licensing authority itself.

In 2005, the Jyllands-Posten Muhammad cartoons controversy caused global protests by offended Muslims and violent attacks with many fatalities in the Near East. It was not the first case of Muslim protests against criticism in the form of satire, but the Western world was surprised by the hostility of the reaction: Any country's flag in which a newspaper chose to publish the parodies was being burnt in a Near East country, then embassies were attacked, killing 139 people in mainly four countries; politicians throughout Europe agreed that satire was an aspect of the freedom of speech, and therefore to be a protected means of dialogue. Iran threatened to start an International Holocaust Cartoon Competition, which was immediately responded to by Jews with an Israeli Anti-Semitic Cartoons Contest.

In 2006 British comedian Sacha Baron Cohen released "Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan", a "mockumentary" that satirized everyone, from high society to frat boys. The film was criticized by many. Although Baron Cohen is Jewish, some complained that it was antisemitic, and the government of Kazakhstan boycotted the film. The film itself had been a reaction to a longer quarrel between the government and the comedian.

In 2008, popular South African cartoonist and satirist Jonathan Shapiro (who is published under the pen name Zapiro) came under fire for depicting then-president of the ANC Jacob Zuma in the act of undressing in preparation for the implied rape of 'Lady Justice' which is held down by Zuma loyalists. The cartoon was drawn in response to Zuma's efforts to duck corruption charges, and the controversy was heightened by the fact that Zuma was himself acquitted of rape in May 2006. In February 2009, the South African Broadcasting Corporation, viewed by some opposition parties as the mouthpiece of the governing ANC, shelved a satirical TV show created by Shapiro, and in May 2009 the broadcaster pulled a documentary about political satire (featuring Shapiro among others) for the second time, hours before scheduled broadcast. Apartheid South Africa also had a long history of censorship.

On December 29, 2009, Samsung sued Mike Breen, and the "Korea Times" for $1 million, claiming criminal defamation over a satirical column published on Christmas Day, 2009.

On April 29, 2015, the UK Independence Party (UKIP) requested Kent Police investigate the BBC, claiming that comments made about Party leader Nigel Farage by a panelist on the comedy show "Have I Got News For You" might hinder his chances of success in the general election (which would take place a week later), and claimed the BBC breached the Representation of the People Act. Kent Police rebuffed the request to open an investigation, and the BBC released a statement, "Britain has a proud tradition of satire, and everyone knows that the contributors on "Have I Got News for You" regularly make jokes at the expense of politicians of all parties."

Satire is occasionally prophetic: the jokes precede actual events. Among the eminent examples are:

In June 2019, Punocracy, "Nigeria"'s foremost satire platform organised a nationwide writing competition for youth in the country with the objective to make satire a widely accepted and understood tool of socio-political commentary. Some of the entries addressed issues like "gender violence", "political corruption", religious hypocrisy, "internet fraud", educational decay and so on. The group also declared November 9 as World Satire Day with the idea of "trying to fight against the ills in the society not by ammunition but by humour, sarcasm etcetera". 






</doc>
<doc id="1271927" url="https://en.wikipedia.org/wiki?curid=1271927" title="Gender equality">
Gender equality

Gender equality, also known as sexual equality or equality of the sexes, is the state of equal ease of access to resources and opportunities regardless of gender, including economic participation and decision-making; and the state of valuing different behaviors, aspirations and needs equally, regardless of gender.

Gender equality is the goal, while gender neutrality and gender equity are practices and ways of thinking that help in achieving the goal. Gender parity, which is used to measure gender balance in a given situation, can aid in achieving gender equality but is not the goal in and of itself. Gender equality is more than equal representation, it is strongly tied to women's rights, and often requires policy changes. As of 2017, the global movement for gender equality has not incorporated the proposition of genders besides women and men, or gender identities outside of the gender binary.

UNICEF says gender equality "means that women and men, and girls and boys, enjoy the same rights, resources, opportunities and protections. It does not require that girls and boys, or women and men, be the same, or that they be treated exactly alike."

On a global scale, achieving gender equality also requires eliminating harmful practices against women and girls, including sex trafficking, femicide, wartime sexual violence, and other oppression tactics. UNFPA stated that, "despite many international agreements affirming their human rights, women are still much more likely than men to be poor and illiterate. They have less access to property ownership, credit, training and employment. They are far less likely than men to be politically active and far more likely to be victims of domestic violence."

As of 2017, gender equality is the fifth of seventeen sustainable development goals of the United Nations. Gender inequality is measured annually by the United Nations Development Programme's Human Development Reports.

Christine de Pizan, an early advocate for gender equality, states in her 1405 book "The Book of the City of Ladies" that the oppression of women is founded on irrational prejudice, pointing out numerous advances in society probably created by women.

The Shakers, an evangelical group, which practiced segregation of the sexes and strict celibacy, were early practitioners of gender equality. They branched off from a Quaker community in the north-west of England before emigrating to America in 1774. In America, the head of the Shakers' central ministry in 1788, Joseph Meacham, had a revelation that the sexes should be equal. He then brought Lucy Wright into the ministry as his female counterpart, and together they restructured the society to balance the rights of the sexes. Meacham and Wright established leadership teams where each elder, who dealt with the men's spiritual welfare, was partnered with an eldress, who did the same for women. Each deacon was partnered with a deaconess. Men had oversight of men; women had oversight of women. Women lived with women; men lived with men. In Shaker society, a woman did not have to be controlled or owned by any man. After Meacham's death in 1796, Wright became the head of the Shaker ministry until her death in 1821.

Shakers maintained the same pattern of gender-balanced leadership for more than 200 years. They also promoted equality by working together with other women's rights advocates. In 1859, Shaker Elder Frederick Evans stated their beliefs forcefully, writing that Shakers were "the first to disenthrall woman from the condition of vassalage to which all other religious systems (more or less) consign her, and to secure to her those just and equal rights with man that, by her similarity to him in organization and faculties, both God and nature would seem to demand". Evans and his counterpart, Eldress Antoinette Doolittle, joined women's rights advocates on speakers' platforms throughout the northeastern U.S. in the 1870s. A visitor to the Shakers wrote in 1875:
The Shakers were more than a radical religious sect on the fringes of American society; they put equality of the sexes into practice. It has been argued that they demonstrated that gender equality was achievable and how to achieve it.

In wider society, the movement towards gender equality began with the suffrage movement in Western cultures in the late-19th century, which sought to allow women to vote and hold elected office. This period also witnessed significant changes to women's property rights, particularly in relation to their marital status. (See for example, Married Women's Property Act 1882.)

Since World War II, the women's liberation movement and feminism have created a general movement towards recognition of women's rights. The United Nations and other international agencies have adopted several conventions which promote gender equality. These conventions have not been uniformly adopted by all countries, and include:

Such legislation and affirmative action policies have been critical to bringing changes in societal attitudes. A 2015 Pew Research Center survey of citizens in 38 countries found that majorities in 37 of those 38 countries said that gender equality is at least "somewhat important," and a global median of 65% believe it is "very important" that women have the same rights as men. Most occupations are now equally available to men and women, in many countries.

Similarly, men are increasingly working in occupations which in previous generations had been considered women's work, such as nursing, cleaning and child care. In domestic situations, the role of Parenting or child rearing is more commonly shared or not as widely considered to be an exclusively female role, so that women may be free to pursue a career after childbirth. For further information, see Shared earning/shared parenting marriage.

Another manifestation of the change in social attitudes is the non-automatic taking by a woman of her husband's surname on marriage.

A highly contentious issue relating to gender equality is the role of women in religiously orientated societies. Some Christians or Muslims believe in Complementarianism, a view that holds that men and women have different but complementing roles. This view may be in opposition to the views and goals of gender equality.

In addition, there are also non-Western countries of low religiosity where the contention surrounding gender equality remains. In China, a cultural preference for a male child has resulted in a shortfall of women in the population. The feminist movement in Japan has made many strides which resulted in the Gender Equality Bureau, but Japan still remains low in gender equality compared to other industrialized nations.

The notion of gender equality, and of its degree of achievement in a certain country, is very complex because there are countries that have a history of a high level of gender equality in certain areas of life but not in other areas. Indeed, there is a need for caution when categorizing countries by the level of gender equality that they have achieved. According to Mala Htun and S. Laurel Weldon "gender policy is not one issue but many" and:
Not all beliefs relating to gender equality have been popularly adopted. For example, topfreedom, the right to be bare breasted in public, frequently applies only to males and has remained a marginal issue. Breastfeeding in public is now more commonly tolerated, especially in semi-private places such as restaurants.

It is the vision that men and women should be treated equally in social, economic and all other aspects of society, and to not be discriminated against on the basis of their gender. Gender equality is one of the objectives of the United Nations Universal Declaration of Human Rights. World bodies have defined gender equality in terms of human rights, especially women's rights, and economic development. The United Nation's Millennium Development Goals Report states that their goal is to "achieve gender equality and the empowerment of women".Despite economic struggles in developing countries, the United Nations is still trying to promote gender equality, as well as help create a sustainable living environment is all its nations.Their goals also include giving women who work certain full-time jobs equal pay to the men with the same job.

There has been criticism from some feminists towards the political discourse and policies employed in order to achieve the above items of "progress" in gender equality, with critics arguing that these gender equality strategies are superficial, in that they do not seek to challenge social structures of male domination, and only aim at improving the situation of women within the societal framework of subordination of women to men, and that official public policies (such as state policies or international bodies policies) are questionable, as they are applied in a patriarchal context, and are directly or indirectly controlled by agents of a system which is for the most part male. One of the criticisms of the gender equality policies, in particular, those of the European Union, is that they disproportionately focus on policies integrating women in public life, but do not seek to genuinely address the deep private sphere oppression.
A further criticism is that a focus on the situation of women in non-Western countries, while often ignoring the issues that exist in the West, is a form of imperialism and of reinforcing Western moral superiority; and a way of "othering" of domestic violence, by presenting it as something specific to outsiders - the "violent others" - and not to the allegedly progressive Western cultures. These critics point out that women in Western countries often face similar problems, such as domestic violence and rape, as in other parts of the world. They also cite the fact that women faced de jure legal discrimination until just a few decades ago; for instance, in some Western countries such as Switzerland, Greece, Spain, and France, women obtained equal rights in family law in the 1980s. Another criticism is that there is a selective public discourse with regard to different types of oppression of women, with some forms of violence such as honor killings (most common in certain geographic regions such as parts of Asia and North Africa) being frequently the object of public debate, while other forms of violence, such as the lenient punishment for crimes of passion across Latin America, do not receive the same attention in the West. It is also argued that the criticism of particular laws of many developing countries ignores the influence of colonialism on those legal systems. There has been controversy surrounding the concepts of Westernization and Europeanisation, due to their reminder of past colonialism, and also due to the fact that some Western countries, such as Switzerland, have been themselves been very slow to give women legal rights. There have also been objections to the way Western media presents women from various cultures creating stereotypes, such as that of 'submissive' Asian or Eastern European women, a stereotype closely connected to the mail order brides industry. Such stereotypes are often blatantly untrue: for instance women in many Eastern European countries occupy a high professional status. Feminists in many developing countries have been strongly opposed to the idea that women in those countries need to be 'saved' by the West. There are questions on how exactly should gender equality be measured, and whether the West is indeed "best" at it: a study in 2010 found that among the top 20 countries on female graduates in the science fields at university level most countries were countries that were considered internationally to score very low on the position of women's rights, with the top 3 being Iran, Saudi Arabia and Oman, and only 5 European countries made it to that top: Romania, Bulgaria, Italy, Georgia and Greece.

Controversy regarding Western cultural influence in the world is not new; in the late 1940s, when the Universal Declaration of Human Rights was being drafted, the American Anthropological Association warned that the document would be defining universal rights from a Western perspective which could be detrimental to non-Western countries, and further argued that the West's history of colonialism and forceful interference with other societies made them a problematic moral representative for universal global standards.

There has been criticism that international law, international courts, and universal gender neutral concepts of human rights are at best silent on many of the issues important to women and at worst male centered; considering the male person to be the default. Excessive gender neutrality can worsen the situation of women, because the law "assumes" women are in the same position as men, ignoring the biological fact that in the process of reproduction and pregnancy there is no 'equality', and that apart from physical differences there are socially constructed limitations which assign a socially and culturally inferior position to women - a situation which requires a specific approach to women's rights, not merely a gender neutral one. In a 1975 interview, Simone de Beauvoir talked about the negative reactions towards women's rights from the left that was supposed to be progressive and support social change, and also expressed skepticism about mainstream international organizations.

In 2010, the European Union opened the European Institute for Gender Equality (EIGE) in Vilnius, Lithuania to promote gender equality and to fight sex discrimination. In 2015 the EU published the Gender Action Plan 2016–2020.

Gender equality is part of the national curriculum in Great Britain and many other European countries. By presidential decree, the Republic of Kazakhstan created a Strategy for Gender Equality 2006–2016 to chart the subsequent decade of gender equality efforts. Personal, Social and Health Education, religious studies and Language acquisition curricula tend to address gender equality issues as a very serious topic for discussion and analysis of its effect in society.

A large and growing body of research has shown how gender inequality undermines health and development. To overcome gender inequality the United Nations Population Fund states that, "Women's empowerment and gender equality requires strategic interventions at all levels of programming and policy-making. These levels include reproductive health, economic empowerment, educational empowerment and political empowerment."

UNFPA says that "research has also demonstrated how working with men and boys as well as women and girls to promote gender equality contributes to achieving health and development outcomes."

Social constructs of gender (that is, cultural ideals of socially acceptable masculinity and femininity) often have a negative effect on health. The World Health Organization cites the example of women not being allowed to travel alone outside the home (to go to the hospital), and women being prevented by cultural norms to ask their husbands to use a condom, in cultures which simultaneously encourage male promiscuity, as social norms that harm women's health. Teenage boys suffering accidents due to social expectations of impressing their peers through risk taking, and men dying at much higher rate from lung cancer due to smoking, in cultures which link smoking to masculinity, are cited by the WHO as examples of gender norms negatively affecting men's health. The World Health Organization has also stated that there is a strong connection between gender socialization and transmission and lack of adequate management of HIV/AIDS.

Certain cultural practices, such as female genital mutilation (FGM), negatively affect women's health. Female genital mutilation is the ritual cutting or removal of some or all of the external female genitalia. It is rooted in inequality between the sexes, and constitutes a form of discrimination against women. The practice is found in Africa, Asia and the Middle East, and among immigrant communities from countries in which FGM is common. UNICEF estimated in 2016 that 200 million women have undergone the procedure.

According to the World Health Organization, gender equality can improve men's health. The study shows that traditional notions of masculinity have a big impact on men's health. Among European men, non-communicable diseases, such as cancer, cardiovascular diseases, respiratory illnesses, and diabetes, account for the vast majority of deaths of men aged 30–59 in Europe which are often linked to unhealthy diets, stress, substance abuse, and other habits, which the report connects to behaviors often stereotypically seen as masculine behaviors like heavy drinking and smoking. Traditional gender stereotypes that keep men in the role of breadwinner and systematic discrimination preventing women from equally contributing to their households and participating in the workforce can put additional stress on men, increasing their risk of health issues and men bolstered by cultural norms, tend to take more risks and engage in interpersonal violence more often than women, which could result in fatal injuries.

Violence against women is a technical term used to collectively refer to violent acts that are primarily or exclusively committed against women. This type of violence is gender-based, meaning that the acts of violence are committed against women expressly "because" they are women, or as a result of patriarchal gender constructs. Violence and mistreatment of women in marriage has come to international attention during the past decades. This includes both violence committed inside marriage (domestic violence) as well as violence related to marriage customs and traditions (such as dowry, bride price, forced marriage and child marriage).

According to some theories, violence against women is often caused by the acceptance of violence by various cultural groups as a means of conflict resolution within intimate relationships. Studies on Intimate partner violence victimization among ethnic minorities in the United Studies have consistently revealed that immigrants are a high-risk group for intimate violence.

In countries where gang murders, armed kidnappings, civil unrest, and other similar acts are rare, the vast majority of murdered women are killed by partners/ex-partners. By contrast, in countries with a high level of organized criminal activity and gang violence, murders of women are more likely to occur in a public sphere, often in a general climate of indifference and impunity. In addition, many countries do not have adequate comprehensive data collection on such murders, aggravating the problem.

In some parts of the world, various forms of violence against women are tolerated and accepted as parts of everyday life.

In most countries, it is only in more recent decades that domestic violence against women has received significant legal attention. The Istanbul Convention acknowledges the long tradition of European countries of ignoring this form of violence.

In some cultures, acts of violence against women are seen as crimes against the male 'owners' of the woman, such as husband, father or male relatives, rather the woman herself. This leads to practices where men inflict violence upon women in order to get revenge on male members of the women's family. Such practices include payback rape, a form of rape specific to certain cultures, particularly the Pacific Islands, which consists of the rape of a female, usually by a group of several males, as revenge for acts committed by members of her family, such as her father or brothers, with the rape being meant to humiliate the father or brothers, as punishment for their prior behavior towards the perpetrators.

Richard A. Posner writes that "Traditionally, rape was the offense of depriving a father or husband of a valuable asset — his wife's chastity or his daughter's virginity". Historically, rape was seen in many cultures (and is still seen today in some societies) as a crime against the honor of the family, rather than against the self-determination of the woman. As a result, victims of rape may face violence, in extreme cases even honor killings, at the hands of their family members. Catharine MacKinnon argues that in male dominated societies, sexual intercourse is imposed on women in a coercive and unequal way, creating a continuum of victimization, where women have few positive sexual experiences. Socialization within rigid gender constructs often creates an environment where sexual violence is common. One of the challenges of dealing with sexual violence is that in many societies women are perceived as being readily available for sex, and men are seen as entitled to their bodies, until and unless women object.

In 2009, United States data showed that transgender people are likely to experience a broad range of violence in the entirety of their lifetime. Violence against trans women in Puerto Rico started to make headlines after being treated as "An Invisible Problem" decades before. It was reported at the 58th Convention of the Puerto Rican Association that many transgender women face institutional, emotional, and structural obstacles. Most trans women don't have access to health care for STD prevention and are not educated on violence prevention, mental health, and social services that could benefit them.

Trans women in the United States have encountered the subject of anti-trans stigma, which includes criminalization, dehumanization, and violence against those who identify as transgender. From a societal stand point, a trans person can be victim to the stigma due to lack of family support, issues with health care and social services, police brutality, discrimination in the work place, cultural marginalisation, poverty, sexual assault, assault, bullying, and mental trauma. The Human Rights Campaign tracked over 128 cases that ended in fatality against transgender people in the US from 2013–2018, of which eighty percent included a trans woman of color. In the US, high rates of Intimate Partner violence impact trans women differently because they are facing discrimination from police and health providers, and alienation from family. In 2018, it was reported that 77 percent of transgender people who were linked to sex work and 72 percent of transgender people who were homeless, were victims of intimate partner violence.

The importance of women having the right and possibility to have control over their body, reproduction decisions, and sexuality, and the need for gender equality in order to achieve these goals are recognized as crucial by the Fourth World Conference on Women in Beijing and the UN International Conference on Population and Development Program of Action. The World Health Organization (WHO) has stated that promotion of gender equality is crucial in the fight against HIV/AIDS.

Maternal mortality is a major problem in many parts of the world. UNFPA states that countries have an obligation to protect women's right to health, but many countries do not do that. Maternal mortality is considered today not just an issue of development but also an issue of human rights.

The right to reproductive and sexual autonomy is denied to women in many parts of the world, through practices such as forced sterilization, forced/coerced sexual partnering (e.g. forced marriage, child marriage), criminalization of consensual sexual acts (such as sex outside marriage), lack of criminalization of marital rape, violence in regard to the choice of partner (honor killings as punishment for 'inappropriate' relations). The sexual health of women is often poor in societies where a woman's right to control her sexuality is not recognized.

Adolescent girls have the highest risk of sexual coercion, sexual ill health, and negative reproductive outcomes. The risks they face are higher than those of boys and men; this increased risk is partly due to gender inequity (different socialization of boys and girls, gender based violence, child marriage) and partly due to biological factors.

Family planning is the practice of freely deciding the number of children one has and the intervals between their births, particularly by means of contraception or voluntary sterilization. Abortion is the induced termination of pregnancy. Abortion laws vary significantly by country. The availability of contraception, sterilization and abortion is dependent on laws, as well as social, cultural and religious norms. Some countries have liberal laws regarding these issues, but in practice it is very difficult to access such services due to doctors, pharmacists and other social and medical workers being conscientious objectors. Family planning is particularly important from a women's rights perspective, as having very many pregnancies, especially in areas where malnutrition is present, can seriously endanger women's health. UNFA writes that "Family planning is central to gender equality and women’s empowerment, and it is a key factor in reducing poverty".

Family planning is often opposed by governments who have strong natalist policies. During the 20th century, such examples have included the aggressive natalist policies from communist Romania and communist Albania. State mandated forced marriage was also practiced by some authoritarian governments as a way to meet population targets: the Khmer Rouge regime in Cambodia systematically forced people into marriages, in order to increase the population and continue the revolution. By contrast, the one child policy of China (1979–2015) included punishments for families with more than one child and forced abortions. Some governments have sought to prevent certain ethnic or social groups from reproduction. Such policies were carried out against ethnic minorities in Europe and North America in the 20th century, and more recently in Latin America against the Indigenous population in the 1990s; in Peru, President Alberto Fujimori (in office from 1990 to 2000) has been accused of genocide and crimes against humanity as a result of a sterilization program put in place by his administration targeting indigenous people (mainly the Quechuas and the Aymaras).

Human rights organizations have expressed concern about the legal impunity of perpetrators of crimes against women, with such crimes being often ignored by authorities. This is especially the case with murders of women in Latin America. In particular, there is impunity in regard to domestic violence.

Women are often, in law or in practice, unable to access legal institutions. UN Women has said that: "Too often, justice institutions, including the police and the courts, deny women justice". Often, women are denied legal recourse because the state institutions themselves are structured and operate in ways incompatible with genuine justice for women who experience violence.

"Harmful traditional practices" refer to forms of violence which are committed in certain communities often enough to become cultural practice, and accepted for that reason. Young women are the main victims of such acts, although men can be affected. They occur in an environment where women and girls have unequal rights and opportunities.
These practices include, according to the Office of the United Nations High Commissioner for Human Rights:

Son preference refers to a cultural preference for sons over daughters, and manifests itself through practices such as sex selective abortion; female infanticide; or abandonment, neglect or abuse of girl-children.

Abuses regarding nutrition are taboos in regard to certain foods, which result in poor nutrition of women, and may endanger their health, especially if pregnant.

The caste system in India which leads to untouchability (the practice of ostracizing a group by segregating them from the mainstream society) often interacts with gender discrimination, leading to a double discrimination faced by Dalit women. In a 2014 survey, 27% of Indians admitted to practicing untouchability.

Traditional customs regarding birth sometimes endanger the mothers. Births in parts of Africa are often attended by traditional birth attendants (TBAs), who sometimes perform rituals that are dangerous to the health of the mother. In many societies, a difficult labour is believed to be a divine punishment for marital infidelity, and such women face abuse and are pressured to "confess" to the infidelity.

Tribal traditions can be harmful to males; for instance, the Satere-Mawe tribe use bullet ants as an initiation rite. Men must wear gloves with hundreds of bullet ants woven in for ten minutes: the ants' stings cause severe pain and paralysis. This experience must be completed twenty times for boys to be considered "warriors".

Other harmful traditional practices include marriage by abduction, ritualized sexual slavery (Devadasi, Trokosi), breast ironing and widow inheritance.

UNFPA and UNICEF regard the practice of female genital mutilation as "a manifestation of deeply entrenched gender inequality. It persists for many reasons. In some societies, for example, it is considered a rite of passage. In others, it is seen as a prerequisite for marriage. In some communities – whether Christian, Jewish, Muslim – the practice may even be attributed to religious beliefs." 

An estimated 125 million women and girls living today have undergone FGM in the 29 countries where data exist. Of these, about half live in Egypt and Ethiopia. It is most commonly carried out on girls between infancy and 15 years old.

Early marriage, child marriage or forced marriage is prevalent in parts of Asia and Africa. The majority of victims seeking advice are female and aged between 18 and 23. Such marriages can have harmful effects on a girl's education and development, and may expose girls to social isolation or abuse.

The 2013 UN Resolution on Child, Early and Forced Marriage calls for an end to the practice, and states that "Recognizing that child, early and forced marriage is a harmful practice that violates abuses, or impairs human rights and is linked to and perpetuates other harmful practices and human rights violations, that these violations have a disproportionately negative impact on women and girls [...]". Despite a near-universal commitment by governments to end child marriage, "one in three girls in developing countries (excluding China) will probably be married before they are 18." UNFPA states that, "over 67 million women 20–24 year old in 2010 had been married as girls. Half were in Asia, one-fifth in Africa. In the next decade 14.2 million girls under 18 will be married every year; this translates into 39,000 girls married each day. This will rise to an average of 15.1 million girls a year, starting in 2021 until 2030, if present trends continue."

Bride price (also called bridewealth or bride token) is money, property, or other form of wealth paid by a groom or his family to the parents of the bride. This custom often leads to women having reduced ability to control their fertility. For instance, in northern Ghana, the payment of bride price signifies a woman's requirement to bear children, and women using birth control face threats, violence and reprisals. The custom of bride price has been criticized as contributing to the mistreatment of women in marriage, and preventing them from leaving abusive marriages. UN Women recommended its abolition, and stated that: "Legislation should [...] State that divorce shall not be contingent upon the return of bride price but such provisions shall not be interpreted to limit women’s right to divorce; State that a perpetrator of domestic violence, including marital rape, cannot use the fact that he paid bride price as a defence to a domestic violence charge."

The custom of bride price can also curtail the free movement of women: if a wife wants to leave her husband, he may demand back the bride price that he had paid to the woman's family; and the woman's family often cannot or does not want to pay it back, making it difficult for women to move out of violent husbands' homes.

Promoting gender equality is seen as an encouragement to greater economic prosperity. Female economic activity is a common measure of gender equality in an economy.

Gender discrimination often results in women obtaining low-wage jobs and being disproportionately affected by poverty, discrimination and exploitation. A growing body of research documents what works to economically empower women, from providing access to formal financial services to training on agricultural and business management practices, though more research is needed across a variety of contexts to confirm the effectiveness of these interventions.

Gender biases also exist in product and service provision. The term "Women's Tax", also known as "Pink Tax", refers to gendered pricing in which products or services marketed to women are more expensive than similar products marketed to men. Gender-based price discrimination involves companies selling almost identical units of the same product or service at comparatively different prices, as determined by the target market. Studies have found that women pay about $1,400 a year more than men due to gendered discriminatory pricing. Although the "pink tax" of different goods and services is not uniform, overall women pay more for commodities that result in visual evidence of feminine body image.

Since the 1950s, social scientists as well as feminists have increasingly criticized gendered arrangements of work and care and the male breadwinner role. Policies are increasingly targeting men as fathers as a tool of changing gender relations.
Shared earning/shared parenting marriage, that is, a relationship where the partners collaborate at sharing their responsibilities inside and outside of the home, is often encouraged in Western countries.

Western countries with a strong emphasis on women fulfilling the role of homemakers, rather than a professional role, include parts of German speaking Europe (i.e. parts of Germany, Austria and Switzerland); as well as the Netherlands and Ireland. In the computer technology world of Silicon Valley in the United States, "New York Times" reporter Nellie Bowles has covered harassment and bias against women as well as a backlash against female equality.

A key issue towards insuring gender equality in the workplace is the respecting of maternity rights and reproductive rights of women. Different countries have different rules regarding maternity leave, paternity leave and parental leave. Another important issue is ensuring that employed women are not "de jure" or "de facto" prevented from having a child. In some countries, employers ask women to sign formal or informal documents stipulating that they will not get pregnant or face legal punishment. Women often face severe violations of their reproductive rights at the hands of their employers; and the International Labour Organization classifies forced abortion coerced by the employer as labour exploitation. Other abuses include routine virginity tests of unmarried employed women.

The degree to which women can participate (in law and in practice) in public life varies by culture and socioeconomic characteristics. Seclusion of women within the home was a common practice among the upper classes of many societies, and this still remains the case today in some societies. Before the 20th century it was also common in parts of Southern Europe, such as much of Spain.

Women's freedom of movement continues to be legally restricted in some parts of the world. This restriction is often due to marriage laws. In some countries, women must legally be accompanied by their male guardians (such as the husband or male relative) when they leave home.

The Convention on the Elimination of all Forms of Discrimination Against Women (CEDAW) states at Article 15 (4) that:
In addition to laws, women's freedom of movement is also restricted by social and religious norms. Restrictions on freedom of movement also exist due to traditional practices such as baad, swara, or vani.

In many parts of the world, girls' access to education is very restricted. In developing parts of the world women are often denied opportunities for education as girls and women face many obstacles. These include: early and forced marriages; early pregnancy; prejudice based on gender stereotypes at home, at school and in the community; violence on the way to school, or in and around schools; long distances to schools; vulnerability to the HIV epidemic; school fees, which often lead to parents sending only their sons to school; lack of gender sensitive approaches and materials in classrooms. According to OHCHR, there have been multiple attacks on schools worldwide during the period 2009–2014 with "a number of these attacks being specifically directed at girls, parents and teachers advocating for gender equality in education". The United Nations Population Fund says:

Women are underrepresented in most countries' National Parliaments. The 2011 UN General Assembly resolution on women's political participation called for female participation in politics, and expressed concern about the fact that "women in every part of the world continue to be largely marginalized from the political sphere". Only 22 percent of parliamentarians globally are women and therefore, men continue to occupy most positions of political and legal authority. As of November 2014, women accounted for 28% of members of the single or lower houses of parliaments in the European Union member states.

In some Western countries women have only recently obtained the right to vote.

In 2015, 61.3% of Rwanda's Lower House of Parliament were women, the highest proportion anywhere in the world, but worldwide that was one of only two such bodies where women were in the majority, the other being Bolivia's Lower House of Parliament. (See also Gender equality in Rwanda).

Equal rights for women in marriage, divorce, and property/land ownership and inheritance are essential for gender equality. The Convention on the Elimination of all Forms of Discrimination Against Women (CEDAW) has called for the end of discriminatory family laws. In 2013, UN Women stated that "While at least 115 countries recognize equal land rights for women and men, effective implementation remains a major challenge".

The legal and social treatment of married women has been often discussed as a political issue from the 19th century onwards. Until the 1970s, legal subordination of married women was common across European countries, through marriage laws giving legal authority to the husband, as well as through marriage bars. In 1978, the Council of Europe passed the "Resolution (78) 37 on equality of spouses in civil law". Switzerland was one of the last countries in Europe to establish gender equality in marriage, in this country married women's rights were severely restricted until 1988, when legal reforms providing for gender equality in marriage, abolishing the legal authority of the husband, come into force (these reforms had been approved in 1985 by voters in a referendum, who narrowly voted in favor with 54.7% of voters approving). In the Netherlands, it was only in 1984 that full legal equality between husband and wife was achieved: prior to 1984 the law stipulated that the husband's opinion prevailed over the wife's regarding issues such as decisions on children's education and the domicile of the family.

In the United States, a wife's legal subordination to her husband was fully ended by the case of "Kirchberg v. Feenstra", , a United States Supreme Court case in which the Court held a Louisiana Head and Master law, which gave sole control of marital property to the husband, unconstitutional.

There have been and sometimes continue to be unequal treatment of married women in various aspects of everyday life. For example, in Australia, until 1983 a husband had to authorize an application for an Australian passport for a married woman. Other practices have included, and in many countries continue to include, a requirement for a husband's consent for an application for bank loans and credit cards by a married woman, as well as restrictions on the wife's reproductive rights, such as a requirement that the husband consents to the wife's acquiring contraception or having an abortion. In some places, although the law itself no longer requires the consent of the husband for various actions taken by the wife, the practice continues "de facto", with the authorization of the husband being asked in practice.

Although dowry is today mainly associated with South Asia, the practice has been common until the mid-20th century in parts of Southeast Europe.

Laws regulating marriage and divorce continue to discriminate against women in many countries. In Iraq husbands have a legal right to "punish" their wives, with paragraph 41 of the criminal code stating that there is no crime if an act is committed while exercising a legal right. In the 1990s and the 21st century there has been progress in many countries in Africa: for instance in Namibia the marital power of the husband was abolished in 1996 by the "Married Persons Equality Act"; in Botswana it was abolished in 2004 by the "Abolition of Marital Power Act"; and in Lesotho it was abolished in 2006 by the "Married Persons Equality Act". Violence against a wife continues to be seen as legally acceptable in some countries; for instance in 2010, the United Arab Emirates Supreme Court ruled that a man has the right to physically discipline his wife and children as long as he does not leave physical marks. The criminalization of adultery has been criticized as being a prohibition, which, in law or in practice, is used primarily against women; and incites violence against women (crimes of passion, honor killings).

Two recent movements in countries with large Kurdish populations have implemented political gender equality. One has been the Kurdish movement in southeastern Turkey led by the Democratic Regions Party (DBP) and the Peoples' Democratic Party (HDP), from 2006 or before.
The mayorships of 2 metropolitan areas and 97 towns are led jointly by a man and a woman, both called co-mayors. Party offices are also led by a man and a woman. Local councils were formed, which also had to be co-presided over by a man and a woman together. However, in November 2016 the Turkish government cracked down on the HDP, jailing ten of its members of Parliament, including the party's male and female co-leaders.

A movement in northern Syria, also Kurdish, has been led by the Democratic Union Party (PYD). In northern Syria all villages, towns and cities governed by the PYD were co-governed by a man and a woman. Local councils were formed where each sex had to have 40% representation, and minorities also had to be represented.

Gender stereotypes arise from the socially approved roles of women and men in the private or public sphere, at home or in the workplace. In the household, women are typically seen as mother figures, which usually places them into a typical classification of being "supportive" or "nurturing". Women are expected to want to take on the role of a mother and take on primary responsibility for household needs. Their male counterparts are seen as being "assertive" or "ambitious" as men are usually seen in the workplace or as the primary breadwinner for his family. Due to these views and expectations, women often face discrimination in the public sphere, such as the workplace. Women are stereotyped to be less productive at work because they are believed to focus more on family when they get married or have children.
A gender role is a set of societal norms dictating the types of behaviors which are generally considered acceptable, appropriate, or desirable for people based on their sex. Gender roles are usually centered on conceptions of femininity and masculinity, although there are exceptions and variations.

The way women are represented in the media has been criticized as perpetuating negative gender stereotypes. The exploitation of women in mass media refers to the criticisms that are levied against the use or objectification of women in the mass media, when such use or portrayal aims at increasing the appeal of media or a product, to the detriment of, or without regard to, the interests of the women portrayed, or women in general. Concerns include the fact that all forms of media have the power to shape the population's perceptions and portray images of unrealistic stereotypical perceptions by portraying women either as submissive housewives or as sex objects. The media emphasizes traditional domestic or sexual roles that normalize violence against women. The vast array of studies that have been conducted on the issue of the portrayal of women in the media have shown that women are often portrayed as irrational, fragile, not intelligent, submissive and
subservient to men. Research has shown that stereotyped images such as these have been shown to negatively impact on the mental health of many female viewers who feel bound by these roles, causing amongst other problems, self-esteem issues, depression and anxiety.

According to a study, the way women are often portrayed by the media can lead to: "Women of average or normal appearance feeling inadequate or less beautiful in comparison to the overwhelming use of extraordinarily attractive women"; "Increase in the likelihood and acceptance of sexual violence"; "Unrealistic expectations by men of how women should look or behave"; "Psychological disorders such as body dysmorphic disorder, anorexia, bulimia and so on"; "The importance of physical appearance is emphasized and reinforced early in most girls' development." Studies have found that nearly half of females ages 6–8 have stated they want to be slimmer. (Striegel-Moore & Franko, 2002)".


While in many countries, the problem lies in the lack of adequate legislation, in others the principal problem is not as much the lack of a legal framework, but the fact is that most women do not know their legal rights. This is especially the case as many of the laws dealing with women's rights are of recent date. This lack of knowledge enables to abusers to lead the victims (explicitly or implicitly) to believe that their abuse is within their rights. This may apply to a wide range of abuses, ranging from domestic violence to employment discrimination. The United Nations Development Programme states that, in order to advance gender justice, "Women must know their rights and be able to access legal systems".

The 1993 UN Declaration on the Elimination of Violence Against Women states at Art. 4 (d) [...] "States should also inform women of their rights in seeking redress through such mechanisms". Enacting protective legislation against violence has little effect, if women do not know how to use it: for example a study of Bedouin women in Israel found that 60% did not know what a restraining order was; or if they don't know what acts are illegal: a report by Amnesty International showed in Hungary, in a public opinion poll of nearly 1,200 people in 2006, a total of 62% did not know that marital rape was an illegal (it was outlawed in 1997) and therefore the crime was rarely reported. Ensuring women have a minim understanding of health issues is also important: lack of access to reliable medical information and available medical procedures to which they are entitled hurts women's health.

Gender mainstreaming is described as the public policy of assessing the different implications for women and men of any planned policy action, including legislation and programmes, in all areas and levels, with the aim of achieving gender equality. The concept of gender mainstreaming was first proposed at the 1985 Third World Conference on Women in Nairobi, Kenya. The idea has been developed in the United Nations development community. Gender mainstreaming "involves ensuring that gender perspectives and attention to the goal of gender equality are central to all activities".

According to the Council of Europe definition: "Gender mainstreaming is the (re)organization, improvement, development and evaluation of policy processes, so that a gender equality perspective is incorporated in all policies at all levels and at all stages, by the actors normally involved in policy-making."

An integrated gender mainstreaming approach is "the attempt to form alliances and common platforms that bring together the power of faith and gender-equality aspirations to advance human rights." For example, "in Azerbaijan, UNFPA conducted a study on gender equality by comparing the text of the Convention on the Elimination of All Forms of Discrimination against Women with some widely recognized Islamic references and resources. The results reflect the parallels between the Convention and many tenets of Islamic scripture and practice. The study showcased specific issues, including VAW, child marriage, respect for the dignity of women, and equality in the economic and political participation of women. The study was later used to produce training materials geared towards sensitizing religious leaders."









</doc>
<doc id="57993" url="https://en.wikipedia.org/wiki?curid=57993" title="Tragedy">
Tragedy

Tragedy (from the , "tragōidia") is a form of drama based on human suffering that invokes an accompanying catharsis or pleasure in audiences. While many cultures have developed forms that provoke this paradoxical response, the term "tragedy" often refers to a specific tradition of drama that has played a unique and important role historically in the self-definition of Western civilization. That tradition has been multiple and discontinuous, yet the term has often been used to invoke a powerful effect of cultural identity and historical continuity—"the Greeks and the Elizabethans, in one cultural form; Hellenes and Christians, in a common activity," as Raymond Williams puts it.

From its origins in the theatre of ancient Greece 2500 years ago, from which there survives only a fraction of the work of Aeschylus, Sophocles and Euripides, as well as many fragments from other poets; through its singular articulations in the works of Shakespeare, Lope de Vega, Jean Racine, and Friedrich Schiller to the more recent naturalistic tragedy of Henrik Ibsen and August Strindberg; Samuel Beckett's modernist meditations on death, loss and suffering; Müller's postmodernist reworkings of the tragic canon, tragedy has remained an important site of cultural experimentation, negotiation, struggle, and change. A long line of philosophers—which includes Plato, Aristotle, Saint Augustine, Voltaire, Hume, Diderot, Hegel, Schopenhauer, Kierkegaard, Nietzsche, Freud, Benjamin, Camus, Lacan, and Deleuze—have analysed, speculated upon, and criticised the genre.

In the wake of Aristotle's "Poetics" (335 BCE), tragedy has been used to make genre distinctions, whether at the scale of poetry in general (where the tragic divides against epic and lyric) or at the scale of the drama (where tragedy is opposed to comedy). In the modern era, tragedy has also been defined against drama, melodrama, the tragicomic, and epic theatre. Drama, in the narrow sense, cuts across the traditional division between comedy and tragedy in an anti- or a-generic deterritorialisation from the mid-19th century onwards. Both Bertolt Brecht and Augusto Boal define their epic theatre projects (non-Aristotelian drama and Theatre of the Oppressed, respectively) against models of tragedy. Taxidou, however, reads epic theatre as an incorporation of tragic functions and its treatments of mourning and speculation.

The word "tragedy" appears to have been used to describe different phenomena at different times. It derives from Classical Greek , contracted from "trag(o)-aoidiā" = "goat song", which comes from "tragos" = "he-goat" and "aeidein" = "to sing" ("cf." "ode"). Scholars suspect this may be traced to a time when a goat was either the prize in a competition of choral dancing or was that around which a chorus danced prior to the animal's ritual sacrifice. In another view on the etymology, Athenaeus of Naucratis (2nd–3rd century CE) says that the original form of the word was "trygodia" from "trygos" (grape harvest) and "ode" (song), because those events were first introduced during grape harvest.

Writing in 335 BCE (long after the Golden Age of 5th-century Athenian tragedy), Aristotle provides the earliest-surviving explanation for the origin of the dramatic art form in his "Poetics", in which he argues that tragedy developed from the improvisations of the leader of choral dithyrambs (hymns sung and danced in praise of Dionysos, the god of wine and fertility):
In the same work, Aristotle attempts to provide a scholastic definition of what tragedy is:
There is some dissent to the dithyrambic origins of tragedy, mostly based on the differences between the shapes of their choruses and styles of dancing. A common descent from pre-Hellenic fertility and burial rites has been suggested. Friedrich Nietzsche discussed the origins of Greek tragedy in his early book "The Birth of Tragedy" (1872). Here, he suggests the name originates in the use of a chorus of goat-like satyrs in the original dithyrambs from which the tragic genre developed.

Scott Scullion writes: 

Athenian tragedy—the oldest surviving form of tragedy—is a type of dance-drama that formed an important part of the theatrical culture of the city-state. Having emerged sometime during the 6th century BCE, it flowered during the 5th century BCE (from the end of which it began to spread throughout the Greek world), and continued to be popular until the beginning of the Hellenistic period. No tragedies from the 6th century and only 32 of the more than a thousand that were performed in the 5th century have survived. We have complete texts extant by Aeschylus, Sophocles, and Euripides.

Athenian tragedies were performed in late March/early April at an annual state religious festival in honor of Dionysus. The presentations took the form of a contest between three playwrights, who presented their works on three successive days. Each playwright offered a tetralogy consisting of three tragedies and a concluding comic piece called a satyr play. The four plays sometimes featured linked stories. Only one complete trilogy of tragedies has survived, the "Oresteia" of Aeschylus. The Greek theatre was in the open air, on the side of a hill, and performances of a trilogy and satyr play probably lasted most of the day. Performances were apparently open to all citizens, including women, but evidence is scant. The theatre of Dionysus at Athens probably held around 12,000 people.

All of the choral parts were sung (to the accompaniment of an "aulos") and some of the actors' answers to the chorus were sung as well. The play as a whole was composed in various verse metres. All actors were male and wore masks. A Greek chorus danced as well as sang, though no one knows exactly what sorts of steps the chorus performed as it sang. Choral songs in tragedy are often divided into three sections: strophe ("turning, circling"), antistrophe ("counter-turning, counter-circling") and epode ("after-song").

Many ancient Greek tragedians employed the "ekkyklêma" as a theatrical device, which was a platform hidden behind the scene that could be rolled out to display the aftermath of some event which had happened out of sight of the audience. This event was frequently a brutal murder of some sort, an act of violence which could not be effectively portrayed visually, but an action of which the other characters must see the effects in order for it to have meaning and emotional resonance. A prime example of the use of the "ekkyklêma" is after the murder of Agamemnon in the first play of Aeschylus' "Oresteia", when the king's butchered body is wheeled out in a grand display for all to see. Variations on the "ekkyklêma" are used in tragedies and other forms to this day, as writers still find it a useful and often powerful device for showing the consequences of extreme human actions. Another such device was a crane, the mechane, which served to hoist a god or goddess on stage when they were supposed to arrive flying. This device gave origin to the phrase "deus ex machina" ("god out of a machine"), that is, the surprise intervention of an unforeseen external factor that changes the outcome of an event.

Following the expansion of the Roman Republic (509–27 BCE) into several Greek territories between 270–240 BCE, Rome encountered Greek tragedy. From the later years of the republic and by means of the Roman Empire (27 BCE-476 CE), theatre spread west across Europe, around the Mediterranean and even reached England. While Greek tragedy continued to be performed throughout the Roman period, the year 240 BCE marks the beginning of regular Roman drama. Livius Andronicus began to write Roman tragedies, thus creating some of the first important works of Roman literature. Five years later, Gnaeus Naevius also began to write tragedies (though he was more appreciated for his comedies). No complete early Roman tragedy survives, though it was highly regarded in its day; historians know of three other early tragic playwrights—Quintus Ennius, Marcus Pacuvius and Lucius Accius.

From the time of the empire, the tragedies of two playwrights survive—one is an unknown author, while the other is the Stoic philosopher Seneca. Nine of Seneca's tragedies survive, all of which are "fabula crepidata" (tragedies adapted from Greek originals); his "Phaedra", for example, was based on Euripides' "Hippolytus". Historians do not know who wrote the only extant example of the "fabula praetexta" (tragedies based on Roman subjects), "Octavia", but in former times it was mistakenly attributed to Seneca due to his appearance as a character in the tragedy.

Seneca's tragedies rework those of all three of the Athenian tragic playwrights whose work has survived. Probably meant to be recited at elite gatherings, they differ from the Greek versions in their long declamatory, narrative accounts of action, their obtrusive moralising, and their bombastic rhetoric. They dwell on detailed accounts of horrible deeds and contain long reflective soliloquies. Though the gods rarely appear in these plays, ghosts and witches abound. Senecan tragedies explore ideas of revenge, the occult, the supernatural, suicide, blood and gore. The Renaissance scholar Julius Caesar Scaliger (1484–1558), who knew both Latin and Greek, preferred Seneca to Euripides.

Classical Greek drama was largely forgotten in Western Europe from the Middle Ages to the beginning of the 16th century. Medieval theatre was dominated by mystery plays, morality plays, farces and miracle plays. In Italy, the models for tragedy in the later Middle Ages were Roman, particularly the works of Seneca, interest in which was reawakened by the Paduan Lovato de' Lovati (1241–1309). His pupil Albertino Mussato (1261–1329), also of Padua, in 1315 wrote the Latin verse tragedy "Eccerinis", which uses the story of the tyrant Ezzelino III da Romano to highlight the danger to Padua posed by Cangrande della Scala of Verona. It was the first secular tragedy written since Roman times, and may be considered the first Italian tragedy identifiable as a Renaissance work. The earliest tragedies to employ purely classical themes are the "Achilles" written before 1390 by Antonio Loschi of Vicenza (c.1365–1441) and the "Progne" of the Venetian Gregorio Correr (1409–1464) which dates from 1428–29.

In 1515 Gian Giorgio Trissino (1478–1550) of Vicenza wrote his tragedy "Sophonisba" in the vernacular that would later be called Italian. Drawn from Livy's account of Sophonisba, the Carthaginian princess who drank poison to avoid being taken by the Romans, it adheres closely to classical rules. It was soon followed by the "Oreste" and "Rosmunda" of Trissino's friend, the Florentine Giovanni di Bernardo Rucellai (1475–1525). Both were completed by early 1516 and are based on classical Greek models, "Rosmunda" on the "Hecuba" of Euripides, and "Oreste" on the "Iphigenia in Tauris" of the same author; like "Sophonisba", they are in Italian and in blank (unrhymed) hendecasyllables. Another of the first of all modern tragedies is "A Castro", by Portuguese poet and playwright António Ferreira, written around 1550 (but only published in 1587) in polymetric verse (most of it being blank hendecasyllables), dealing with the murder of Inês de Castro, one of the most dramatic episodes in Portuguese history. Although these three Italian plays are often cited, separately or together, as being the first regular tragedies in modern times, as well as the earliest substantial works to be written in blank hendecasyllables, they were apparently preceded by two other works in the vernacular: "Pamfila" or "Filostrato e Panfila" written in 1498 or 1508 by Antonio Cammelli (Antonio da Pistoia); and a "Sophonisba" by Galeotto del Carretto of 1502.

From about 1500 printed copies, in the original languages, of the works of Sophocles, Seneca, and Euripides, as well as comedic writers such as Aristophanes, Terence and Plautus, were available in Europe and the next forty years saw humanists and poets translating and adapting their tragedies. In the 1540s, the European university setting (and especially, from 1553 on, the Jesuit colleges) became host to a Neo-Latin theatre (in Latin) written by scholars. The influence of Seneca was particularly strong in its humanist tragedy. His plays, with their ghosts, lyrical passages and rhetorical oratory, brought a concentration on rhetoric and language over dramatic action to many humanist tragedies.

The most important sources for French tragic theatre in the Renaissance were the example of Seneca and the precepts of Horace and Aristotle (and contemporary commentaries by Julius Caesar Scaliger and Lodovico Castelvetro), although plots were taken from classical authors such as Plutarch, Suetonius, etc., from the Bible, from contemporary events and from short story collections (Italian, French and Spanish). The Greek tragic authors (Sophocles and Euripides) would become increasingly important as models by the middle of the 17th century. Important models were also supplied by the Spanish Golden Age playwrights Pedro Calderón de la Barca, Tirso de Molina and Lope de Vega, many of whose works were translated and adapted for the French stage.

The common forms are the:

In English, the most famous and most successful tragedies are those of William Shakespeare and his Elizabethan contemporaries. Shakespeare's tragedies include:

A contemporary of Shakespeare, Christopher Marlowe, also wrote examples of tragedy in English, notably:

John Webster (1580?–1635?), also wrote famous plays of the genre:

Contemporary with Shakespeare, an entirely different approach to facilitating the rebirth of tragedy was taken in Italy. Jacopo Peri, in the preface to his "Euridice" refers to "the ancient Greeks and Romans (who in the opinion of many sang their staged tragedies throughout in representing them on stage)." The attempts of Peri and his contemporaries to recreate ancient tragedy gave rise to the new Italian musical genre of opera. In France, tragic operatic works from the time of Lully to about that of Gluck were not called opera, but "tragédie en musique" ("tragedy in music") or some similar name; the "tragédie en musique" is regarded as a distinct musical genre. Some later operatic composers have also shared Peri's aims: Richard Wagner's concept of "Gesamtkunstwerk" ("integrated work of art"), for example, was intended as a return to the ideal of Greek tragedy in which all the arts were blended in service of the drama. Nietzsche, in his "The Birth of Tragedy" (1872) was to support Wagner in his claims to be a successor of the ancient dramatists.

For much of the 17th century, Pierre Corneille, who made his mark on the world of tragedy with plays like "Medée" (1635) and "Le Cid" (1636), was the most successful writer of French tragedies. Corneille's tragedies were strangely un-tragic (his first version of "Le Cid" was even listed as a tragicomedy), for they had happy endings. In his theoretical works on theatre, Corneille redefined both comedy and tragedy around the following suppositions:

Corneille continued to write plays through 1674 (mainly tragedies, but also something he called "heroic comedies") and many continued to be successes, although the "irregularities" of his theatrical methods were increasingly criticised (notably by François Hédelin, abbé d'Aubignac) and the success of Jean Racine from the late 1660s signalled the end of his preeminence.

Jean Racine's tragedies—inspired by Greek myths, Euripides, Sophocles and Seneca—condensed their plot into a tight set of passionate and duty-bound conflicts between a small group of noble characters, and concentrated on these characters' double-binds and the geometry of their unfulfilled desires and hatreds. Racine's poetic skill was in the representation of pathos and amorous passion (like Phèdre's love for her stepson) and his impact was such that emotional crisis would be the dominant mode of tragedy to the end of the century. Racine's two late plays ("Esther" and "Athalie") opened new doors to biblical subject matter and to the use of theatre in the education of young women. Racine also faced criticism for his irregularities: when his play, "Bérénice", was criticised for not containing any deaths, Racine disputed the conventional view of tragedy.

For more on French tragedy of the 16th and 17th centuries, see French Renaissance literature and French literature of the 17th century.

Bourgeois tragedy (German: Bürgerliches Trauerspiel) is a form that developed in 18th-century Europe. It was a fruit of the Enlightenment and the emergence of the bourgeois class and its ideals. It is characterised by the fact that its protagonists are ordinary citizens. The first true bourgeois tragedy was an English play, George Lillo's "The London Merchant; or, the History of George Barnwell", which was first performed in 1731. Usually, Gotthold Ephraim Lessing's play "Miss Sara Sampson", which was first produced in 1755, is said to be the earliest "Bürgerliches Trauerspiel" in Germany.

In modernist literature, the definition of tragedy has become less precise. The most fundamental change has been the rejection of Aristotle's dictum that true tragedy can only depict those with power and high status. Arthur Miller's essay "Tragedy and the Common Man" (1949) argues that tragedy may also depict ordinary people in domestic surroundings thus defining Domestic tragedies. British playwright Howard Barker has argued strenuously for the rebirth of tragedy in the contemporary theatre, most notably in his volume "Arguments for a Theatre". "You emerge from tragedy equipped against lies. After the musical, you're anybody's fool," he insists.

Critics such as George Steiner have even been prepared to argue that tragedy may no longer exist in comparison with its former manifestations in classical antiquity. In "The Death of Tragedy" (1961) George Steiner outlined the characteristics of Greek tragedy and the traditions that developed from that period. In the Foreword (1980) to a new edition of his book Steiner concluded that ‘the dramas of Shakespeare are not a renascence of or a humanistic variant of the absolute tragic model. They are, rather, a rejection of this model in the light of tragi-comic and “realistic” criteria.’ In part, this feature of Shakespeare’s mind is explained by his bent of mind or imagination which was ‘so encompassing, so receptive to the plurality of diverse orders of experience.’ When compared to the drama of Greek antiquity and French classicism Shakespeare's forms are ‘richer but hybrid'.

Numerous books and plays continue to be written in the tradition of tragedy to this day examples include "Froth on the Daydream", "The Road", "The Fault in Our Stars", "Fat City", "Rabbit Hole", "Requiem for a Dream", "The Handmaid's Tale".

Defining tragedy is no simple matter, and there are many definitions, some of which are incompatible with each other. Oscar Mandel, in "A Definition of Tragedy" (1961), contrasted two essentially different means of arriving at a definition. First is what he calls the "derivative" way, in which the tragedy is thought to be an expression of an ordering of the world; "instead of asking what tragedy expresses, the derivative definition tends to ask what expresses itself through tragedy". The second is the "substantive" way of defining tragedy, which starts with the work of art which is assumed to "contain" the ordering of the world. Substantive critics "are interested in the constituent elements of art, rather than its ontological sources". He recognizes four subclasses: a. "definition by formal elements" (for instance the supposed "three unities"); b. "definition by situation" (where one defines tragedy for instance as "exhibiting the fall of a good man"); c. "definition by ethical direction" (where the critic is concerned with the meaning, with the "intellectual and moral effect); and d. "definition by emotional effect" (and he cites Aristotle's "requirement of pity and fear").

Aristotle wrote in his work "Poetics" that
tragedy is characterised by seriousness and involves a great person who experiences a reversal of fortune ("Peripeteia"). Aristotle's definition can include a change of fortune from bad to good as in the "Eumenides", but he says that the change from good to bad as in "Oedipus Rex" is preferable because this induces pity and fear within the spectators. Tragedy results in a catharsis (emotional cleansing) or healing for the audience through their experience of these emotions in response to the suffering of the characters in the drama.

According to Aristotle, "the structure of the best tragedy should not be simple but complex and one that represents incidents arousing fear and pity—for that is peculiar to this form of art." This reversal of fortune must be caused by the tragic hero's "hamartia", which is often translated as either a character flaw, or as a mistake (since the original Greek etymology traces back to "hamartanein", a sporting term that refers to an archer or spear-thrower missing his target). According to Aristotle, "The misfortune is brought about not by [general] vice or depravity, but by some [particular] error or frailty." The reversal is the inevitable but unforeseen result of some action taken by the hero. It is also a misconception that this reversal can be brought about by a higher power (e.g. the law, the gods, fate, or society), but if a character's downfall is brought about by an external cause, Aristotle describes this as a misadventure and not a tragedy.

In addition, the tragic hero may achieve some revelation or recognition (anagnorisis--"knowing again" or "knowing back" or "knowing throughout") about human fate, destiny, and the will of the gods. Aristotle terms this sort of recognition "a change from ignorance to awareness of a bond of love or hate."

In "Poetics", Aristotle gave the following definition in ancient Greek of the word "tragedy" (τραγῳδία):

Common usage of tragedy refers to any story with a sad ending, whereas to be an Aristotelian tragedy the story must fit the set of requirements as laid out by "Poetics". By this definition social drama cannot be tragic because the hero in it is a victim of circumstance and incidents that depend upon the society in which he lives and not upon the inner compulsions—psychological or religious—which determine his progress towards self-knowledge and death. Exactly what constitutes a "tragedy", however, is a frequently debated matter.

According to Aristotle, there are four species of tragedy:

1. Complex, which involves Peripety and Discovery

2. Suffering, tragedies of such nature can be seen in the Greek mythological stories of Ajaxes and Ixions

3. Character, a tragedy of moral or ethical character. Tragedies of this nature can be found in Phthiotides and Peleus

4. Spectacle, that of a horror-like theme. Examples of this nature are Phorcides and Prometheus

G.W.F. Hegel, the German philosopher most famous for his dialectical approach to epistemology and history, also applied such a methodology to his theory of tragedy. In his essay "Hegel's Theory of Tragedy," A.C. Bradley first introduced the English-speaking world to Hegel's theory, which Bradley called the "tragic collision", and contrasted against the Aristotelian notions of the "tragic hero" and his or her "hamartia" in subsequent analyses of the Aeschylus' Oresteia trilogy and of Sophocles' Antigone. Hegel himself, however, in his seminal "The Phenomenology of Spirit" argues for a more complicated theory of tragedy, with two complementary branches which, though driven by a single dialectical principle, differentiate Greek tragedy from that which follows Shakespeare. His later lectures formulate such a theory of tragedy as a conflict of ethical forces, represented by characters, in ancient Greek tragedy, but in Shakespearean tragedy the conflict is rendered as one of subject and object, of individual personality which must manifest self-destructive passions because only such passions are strong enough to defend the individual from a hostile and capricious external world:
Hegel's comments on a particular play may better elucidate his theory: "Viewed externally, Hamlet's death may be seen to have been brought about accidentally... but in Hamlet's soul, we understand that death has lurked from the beginning: the sandbank of finitude cannot suffice his sorrow and tenderness, such grief and nausea at all conditions of life... we feel he is a man whom inner disgust has almost consumed well before death comes upon him from outside."

The writer Bharata Muni, in his work on dramatic theory "A Treatise on Theatre" (Sanskrit: "Nātyaśāstra", नाट्य शास्त्र, c. 200 BCE – 200 CE), identified several "rasas" (such as pity, anger, disgust and terror) in the emotional responses of audiences for the Sanskrit drama of ancient India. The text also suggests the notion of musical modes or jatis which are the origin of the notion of the modern melodic structures known as ragas. Their role in invoking emotions are emphasised; thus compositions emphasising the notes gandhara or rishabha are said to provoke "sadness" or "pathos" ("karuna rasa") whereas rishabha evokes heroism ("vira rasa"). Jatis are elaborated in greater detail in the text "Dattilam", composed around the same time as the "Treatise".

The celebrated ancient Indian epic, "Mahabharata", can also be related to tragedy in some ways. According to Hermann Oldenberg, the original epic once carried an immense "tragic force". It was common in Sanskrit drama to adapt episodes from the "Mahabharata" into dramatic form.





</doc>
<doc id="21207536" url="https://en.wikipedia.org/wiki?curid=21207536" title="Art history">
Art history

Art history is the study of objects of art in their historical development and stylistic contexts; that is genre, design, format, and style. The study includes painting, sculpture, architecture, ceramics, furniture, and other decorative objects.

Art history is the history of different groups of people and their culture represented throughout their artwork. Art historians compare different time periods in art history. Such as a comparison to Medieval Art to Renaissance Art. This history of cultures is shown in their art work in different forms. Art can be shown by attire, architecture, religion, sports. Or more visual pieces of art such as paintings, drawings, sculptures.

As a term, art history (its product being history of art) encompasses several methods of studying the visual arts; in common usage referring to works of art and architecture. Aspects of the discipline overlap. As the art historian Ernst Gombrich once observed, "the field of art history [is] much like Caesar's Gaul, divided in three parts inhabited by three different, though not necessarily hostile tribes: (i) the connoisseurs, (ii) the critics, and (iii) the academic art historians".

As a discipline, art history is distinguished from art criticism, which is concerned with establishing a relative artistic value upon individual works with respect to others of comparable style, or sanctioning an entire style or movement; and art theory or "philosophy of art", which is concerned with the fundamental nature of art. One branch of this area of study is aesthetics, which includes investigating the enigma of the sublime and determining the essence of beauty. Technically, art history is not these things, because the art historian uses historical method to answer the questions: How did the artist come to create the work?, Who were the patrons?, Who were his or her teachers?, Who was the audience?, Who were his or her disciples?, What historical forces shaped the artist's oeuvre, and how did he or she and the creation, in turn, affect the course of artistic, political, and social events? It is, however, questionable whether many questions of this kind can be answered satisfactorily without also considering basic questions about the nature of art. The current disciplinary gap between art history and the philosophy of art (aesthetics) often hinders this inquiry.

Art history is not only a biographical endeavor. Art historians often root their studies in the scrutiny of individual objects. They thus attempt to answer in historically specific ways, questions such as: What are key features of this style?, What meaning did this object convey?, How does it function visually?, Did the artist meet their goals well?, What symbols are involved?, and Does it function discursively?

The historical backbone of the discipline is a celebratory chronology of creations commissioned by public or religious bodies or wealthy individuals in western Europe. Such a "canon" remains prominent, as indicated by the selection of objects present in art history textbooks. Nonetheless, since the 20th century there has been an effort to re-define the discipline to be more inclusive of non-Western art, art made by women, and vernacular creativity.

Art history as we know it in the 21st century began in the 19th century but has precedents that date to the ancient world. Like the analysis of historical trends in politics, literature, and the sciences, the discipline benefits from the clarity and portability of the written word, but art historians also rely on formal analysis, semiotics, psychoanalysis and iconography. Advances in photographic reproduction and printing techniques after World War II increased the ability of reproductions of artworks. Such technologies have helped to advance the discipline in profound ways, as they have enabled easy comparisons of objects. The study of visual art thus described, can be a practice that involves understanding context, form, and social significance.

Art historians employ a number of methods in their research into the ontology and history of objects.

Art historians often examine work in the context of its time. At best, this is done in a manner which respects its creator's motivations and imperatives; with consideration of the desires and prejudices of its patrons and sponsors; with a comparative analysis of themes and approaches of the creator's colleagues and teachers; and with consideration of iconography and symbolism. In short, this approach examines the work of art in the context of the world within which it was created.

Art historians also often examine work through an analysis of form; that is, the creator's use of line, shape, color, texture, and composition. This approach examines how the artist uses a two-dimensional picture plane or the three dimensions of sculptural or architectural space to create his or her art. The way these individual elements are employed results in representational or non-representational art. Is the artist imitating an object or image found in nature? If so, it is representational. The closer the art hews to perfect imitation, the more the art is "realistic". Is the artist not imitating, but instead relying on symbolism, or in an important way striving to capture nature's essence, rather than copy it directly? If so the art is non-representational—also called abstract. Realism and abstraction exist on a continuum. Impressionism is an example of a representational style that was not directly imitative, but strove to create an "impression" of nature. If the work is not representational and is an expression of the artist's feelings, longings and aspirations, or is a search for ideals of beauty and form, the work is non-representational or a work of expressionism.

An iconographical analysis is one which focuses on particular design elements of an object. Through a close reading of such elements, it is possible to trace their lineage, and with it draw conclusions regarding the origins and trajectory of these motifs. In turn, it is possible to make any number of observations regarding the social, cultural, economic, and aesthetic values of those responsible for producing the object.

Many art historians use critical theory to frame their inquiries into objects. Theory is most often used when dealing with more recent objects, those from the late 19th century onward. Critical theory in art history is often borrowed from literary scholars, and it involves the application of a non-artistic analytical framework to the study of art objects. Feminist, Marxist, critical race, queer, and postcolonial theories are all well established in the discipline. As in literary studies, there is an interest among scholars in nature and the environment, but the direction that this will take in the discipline has yet to be determined.

More recently, media and digital technology introduced possibilities of visual, spatial and experiential analyses. The relevant forms vary from movies, to interactive forms, including virtual environments, augmented environments, situated media, networked media, etc. The methods enabled by such techniques are in active development and promise to include qualitative approaches that can emphasize narrative, dramatic, emotional and ludic characteristics of history and art.

The earliest surviving writing on art that can be classified as art history are the passages in Pliny the Elder's "Natural History" (c. AD 77-79), concerning the development of Greek sculpture and painting. From them it is possible to trace the ideas of Xenokrates of Sicyon (c. 280 BC), a Greek sculptor who was perhaps the first art historian. Pliny's work, while mainly an encyclopaedia of the sciences, has thus been influential from the Renaissance onwards. (Passages about techniques used by the painter Apelles c. (332-329 BC), have been especially well-known.) Similar, though independent, developments occurred in the 6th century China, where a canon of worthy artists was established by writers in the scholar-official class. These writers, being necessarily proficient in calligraphy, were artists themselves. The artists are described in the "Six Principles of Painting" formulated by Xie He.

While personal reminiscences of art and artists have long been written and read (see Lorenzo Ghiberti "Commentarii," for the best early example), it was Giorgio Vasari, the Tuscan painter, sculptor and author of the "Lives of the Most Excellent Painters, Sculptors, and Architects", who wrote the first true "history" of art. He emphasized art's progression and development, which was a milestone in this field. His was a personal and a historical account, featuring biographies of individual Italian artists, many of whom were his contemporaries and personal acquaintances. The most renowned of these was Michelangelo, and Vasari's account is enlightening, though biased in places.

Vasari's ideas about art were enormously influential, and served as a model for many, including in the north of Europe Karel van Mander's "Schilder-boeck" and Joachim von Sandrart's "Teutsche Akademie". Vasari's approach held sway until the 18th century, when criticism was leveled at his biographical account of history.

Scholars such as Johann Joachim Winckelmann (1717–1768), criticised Vasari's "cult" of artistic personality, and they argued that the real emphasis in the study of art should be the views of the learned beholder and not the unique viewpoint of the charismatic artist. Winckelmann's writings thus were the beginnings of art criticism. His two most notable works that introduced the concept of art criticism were "Gedanken über die Nachahmung der griechischen Werke in der Malerei und Bildhauerkunst", published in 1755, shortly before he left for Rome (Fuseli published an English translation in 1765 under the title "Reflections on the Painting and Sculpture of the Greeks"), and "Geschichte der Kunst des Altertums" ("History of Art in Antiquity"), published in 1764 (this is the first occurrence of the phrase ‘history of art’ in the title of a book)". Winckelmann critiqued the artistic excesses of Baroque and Rococo forms, and was instrumental in reforming taste in favor of the more sober Neoclassicism. Jacob Burckhardt (1818–1897), one of the founders of art history, noted that Winckelmann was 'the first to distinguish between the periods of ancient art and to link the history of style with world history'. From Winckelmann until the mid-20th century, the field of art history was dominated by German-speaking academics. Winckelmann's work thus marked the entry of art history into the high-philosophical discourse of German culture.

Winckelmann was read avidly by Johann Wolfgang Goethe and Friedrich Schiller, both of whom began to write on the history of art, and his account of the Laocoön group occasioned a response by Lessing. The emergence of art as a major subject of philosophical speculation was solidified by the appearance of Immanuel Kant's "Critique of Judgment" in 1790, and was furthered by Hegel's "Lectures on Aesthetics". Hegel's philosophy served as the direct inspiration for Karl Schnaase's work. Schnaase's "Niederländische Briefe" established the theoretical foundations for art history as an autonomous discipline, and his "Geschichte der bildenden Künste", one of the first historical surveys of the history of art from antiquity to the Renaissance, facilitated the teaching of art history in German-speaking universities. Schnaase's survey was published contemporaneously with a similar work by Franz Theodor Kugler.

Heinrich Wölfflin (1864–1945), who studied under Burckhardt in Basel, is the "father" of modern art history. Wölfflin taught at the universities of Berlin, Basel, Munich, and Zurich. A number of students went on to distinguished careers in art history, including Jakob Rosenberg and Frida Schottmuller. He introduced a scientific approach to the history of art, focusing on three concepts. Firstly, he attempted to study art using psychology, particularly by applying the work of Wilhelm Wundt. He argued, among other things, that art and architecture are good if they resemble the human body. For example, houses were good if their façades looked like faces. Secondly, he introduced the idea of studying art through comparison. By comparing individual paintings to each other, he was able to make distinctions of style. His book "Renaissance and Baroque" developed this idea, and was the first to show how these stylistic periods differed from one another. In contrast to Giorgio Vasari, Wölfflin was uninterested in the biographies of artists. In fact he proposed the creation of an "art history without names." Finally, he studied art based on ideas of nationhood. He was particularly interested in whether there was an inherently "Italian" and an inherently "German" style. This last interest was most fully articulated in his monograph on the German artist Albrecht Dürer.

Contemporaneous with Wölfflin's career, a major school of art-historical thought developed at the University of Vienna. The first generation of the Vienna School was dominated by Alois Riegl and Franz Wickhoff, both students of Moritz Thausing, and was characterized by a tendency to reassess neglected or disparaged periods in the history of art. Riegl and Wickhoff both wrote extensively on the art of late antiquity, which before them had been considered as a period of decline from the classical ideal. Riegl also contributed to the revaluation of the Baroque.

The next generation of professors at Vienna included Max Dvořák, Julius von Schlosser, Hans Tietze, Karl Maria Swoboda, and Josef Strzygowski. A number of the most important twentieth-century art historians, including Ernst Gombrich, received their degrees at Vienna at this time. The term "Second Vienna School" (or "New Vienna School") usually refers to the following generation of Viennese scholars, including Hans Sedlmayr, Otto Pächt, and Guido Kaschnitz von Weinberg. These scholars began in the 1930s to return to the work of the first generation, particularly to Riegl and his concept of "Kunstwollen", and attempted to develop it into a full-blown art-historical methodology. Sedlmayr, in particular, rejected the minute study of iconography, patronage, and other approaches grounded in historical context, preferring instead to concentrate on the aesthetic qualities of a work of art. As a result, the Second Vienna School gained a reputation for unrestrained and irresponsible formalism, and was furthermore colored by Sedlmayr's overt racism and membership in the Nazi party. This latter tendency was, however, by no means shared by all members of the school; Pächt, for example, was himself Jewish, and was forced to leave Vienna in the 1930s.

Our 21st-century understanding of the symbolic content of art comes from a group of scholars who gathered in Hamburg in the 1920s. The most prominent among them were Erwin Panofsky, Aby Warburg, and Fritz Saxl. Together they developed much of the vocabulary that continues to be used in the 21st century by art historians. "Iconography"—with roots meaning "symbols from writing" refers to subject matter of art derived from written sources—especially scripture and mythology. "Iconology" is a broader term that referred to all symbolism, whether derived from a specific text or not. Today art historians sometimes use these terms interchangeably.

Panofsky, in his early work, also developed the theories of Riegl, but became eventually more preoccupied with iconography, and in particular with the transmission of themes related to classical antiquity in the Middle Ages and Renaissance. In this respect his interests coincided with those of Warburg, the son of a wealthy family who had assembled an impressive library in Hamburg devoted to the study of the classical tradition in later art and culture. Under Saxl's auspices, this library was developed into a research institute, affiliated with the University of Hamburg, where Panofsky taught.

Warburg died in 1929, and in the 1930s Saxl and Panofsky, both Jewish, were forced to leave Hamburg. Saxl settled in London, bringing Warburg's library with him and establishing the Warburg Institute. Panofsky settled in Princeton at the Institute for Advanced Study. In this respect they were part of an extraordinary influx of German art historians into the English-speaking academy in the 1930s. These scholars were largely responsible for establishing art history as a legitimate field of study in the English-speaking world, and the influence of Panofsky's methodology, in particular, determined the course of American art history for a generation.

Heinrich Wölfflin was not the only scholar to invoke psychological theories in the study of art. Psychoanalyst Sigmund Freud wrote a book on the artist Leonardo da Vinci, in which he used Leonardo's paintings to interrogate the artist's psyche and sexual orientation. Freud inferred from his analysis that Leonardo was probably homosexual.

Though the use of posthumous material to perform psychoanalysis is controversial among art historians, especially since the sexual mores of Leonardo's time and Freud's are different, it is often attempted. One of the best-known psychoanalytic scholars is Laurie Schneider Adams, who wrote a popular textbook, "Art Across Time", and a book "Art and Psychoanalysis".

An unsuspecting turn for the history of art criticism came in 1914 when Sigmund Freud published a psychoanalytical interpretation of Michelangelo's Moses titled Der Moses des Michelangelo as one of the first psychology based analyses on a work of art. Freud first published this work shortly after reading Vasari's "Lives". For unknown purposes, Freud originally published the article anonymously.

Carl Jung also applied psychoanalytic theory to art. C.G. Jung was a Swiss psychiatrist, an influential thinker, and founder of analytical psychology. Jung's approach to psychology emphasized understanding the psyche through exploring the worlds of dreams, art, mythology, world religion and philosophy. Much of his life's work was spent exploring Eastern and Western philosophy, alchemy, astrology, sociology, as well as literature and the arts. His most notable contributions include his concept of the psychological archetype, the collective unconscious, and his theory of synchronicity. Jung believed that many experiences perceived as coincidence were not merely due to chance but, instead, suggested the manifestation of parallel events or circumstances reflecting this governing dynamic. He argued that a collective unconscious and archetypal imagery were detectable in art. His ideas were particularly popular among American Abstract expressionists in the 1940s and 1950s. His work inspired the surrealist concept of drawing imagery from dreams and the unconscious.

Jung emphasized the importance of balance and harmony. He cautioned that modern humans rely too heavily on science and logic and would benefit from integrating spirituality and appreciation of the unconscious realm. His work not only triggered analytical work by art historians, but it became an integral part of art-making. Jackson Pollock, for example, famously created a series of drawings to accompany his psychoanalytic sessions with his Jungian psychoanalyst, Dr. Joseph Henderson. Henderson who later published the drawings in a text devoted to Pollock's sessions realized how powerful the drawings were as a therapeutic tool.

The legacy of psychoanalysis in art history has been profound, and extends beyond Freud and Jung. The prominent feminist art historian Griselda Pollock, for example, draws upon psychoanalysis both in her reading into contemporary art and in her rereading of modernist art. With Griselda Pollock's reading of French feminist psychoanalysis and in particular the writings of Julia Kristeva and Bracha L. Ettinger, as with Rosalind Krauss readings of Jacques Lacan and Jean-François Lyotard and Catherine de Zegher's curatorial rereading of art, Feminist theory written in the fields of French feminism and Psychoanalysis has strongly informed the reframing of both men and women artists in art history.

During the mid-20th century, art historians embraced social history by using critical approaches. The goal was to show how art interacts with power structures in society. One critical approach that art historians used was Marxism. Marxist art history attempted to show how art was tied to specific classes, how images contain information about the economy, and how images can make the status quo seem natural (ideology).

Marcel Duchamp and Dada Movement jump started the Anti-art style. Various artist did not want to create artwork that everyone was conforming to at the time. These two movements helped other artist to create pieces that were not viewed as traditional art. Some examples of styles that branched off the anti-art movement would be Neo-Dadaism, Surrealism, and Constructivism. These styles and artist did not want to surrender to traditional ways of art. This way of thinking provoked political movements such as the Russian Revolution and the communist ideals.

Artist Isaak Brodsky work of art 'Shock-worker from Dneprstroi' in 1932 shows his political involvement within art. This piece of art can be analysed to show the internal troubles Soviet Russia was experiencing at the time.
Perhaps the best-known Marxist was Clement Greenberg, who came to prominence during the late 1930s with his essay "Avant-Garde and Kitsch". In the essay Greenberg claimed that the avant-garde arose in order to defend aesthetic standards from the decline of taste involved in consumer society, and seeing kitsch and art as opposites. Greenberg further claimed that avant-garde and Modernist art was a means to resist the leveling of culture produced by capitalist propaganda. Greenberg appropriated the German word 'kitsch' to describe this consumerism, although its connotations have since changed to a more affirmative notion of leftover materials of capitalist culture. Greenberg later became well known for examining the formal properties of modern art.

Meyer Schapiro is one of the best-remembered Marxist art historians of the mid-20th century. Although he wrote about numerous time periods and themes in art, he is best remembered for his commentary on sculpture from the late Middle Ages and early Renaissance, at which time he saw evidence of capitalism emerging and feudalism declining.

Arnold Hauser wrote the first Marxist survey of Western Art, entitled "The Social History of Art". He attempted to show how class consciousness was reflected in major art periods. The book was controversial when published during the 1950s since it makes generalizations about entire eras, a strategy now called "vulgar Marxism".

Marxist Art History was refined in the department of Art History at UCLA with scholars such as T.J. Clark, O.K. Werckmeister, David Kunzle, Theodor W. Adorno, and Max Horkheimer. T.J. Clark was the first art historian writing from a Marxist perspective to abandon vulgar Marxism. He wrote Marxist art histories of several impressionist and realist artists, including Gustave Courbet and Édouard Manet. These books focused closely on the political and economic climates in which the art was created.

Linda Nochlin's essay "Why Have There Been No Great Women Artists?" helped to ignite feminist art history during the 1970s and remains one of the most widely read essays about female artists. This was then followed by a 1972 College Art Association Panel, chaired by Nochlin, entitled "Eroticism and the Image of Woman in Nineteenth-Century Art". Within a decade, scores of papers, articles, and essays sustained a growing momentum, fueled by the Second-wave feminist movement, of critical discourse surrounding women's interactions with the arts as both artists and subjects. In her pioneering essay, Nochlin applies a feminist critical framework to show systematic exclusion of women from art training, arguing that exclusion from practicing art as well as the canonical history of art was the consequence of cultural conditions which curtailed and restricted women from art producing fields. The few who did succeed were treated as anomalies and did not provide a model for subsequent success. Griselda Pollock is another prominent feminist art historian, whose use of psychoanalytic theory is described above.

While feminist art history can focus on any time period and location, much attention has been given to the Modern era. Some of this scholarship centers on the feminist art movement, which referred specifically to the experience of women. Often, feminist art history offers a critical "re-reading" of the Western art canon, such as Carol Duncan's re-interpretation of Les Demoiselles d'Avignon. Two pioneers of the field are Mary Garrard and Norma Broude. Their anthologies "Feminism and Art History: Questioning the Litany", "The Expanding Discourse: Feminism and Art History", and "Reclaiming Feminist Agency: Feminist Art History After Postmodernism" are substantial efforts to bring feminist perspectives into the discourse of art history. The pair also co-founded the Feminist Art History Conference.

As opposed to iconography which seeks to identify meaning, semiotics is concerned with how meaning is created. Roland Barthes’s connoted and denoted meanings are paramount to this examination. In any particular work of art, an interpretation depends on the identification of denoted meaning—the recognition of a visual sign, and the connoted meaning—the instant cultural associations that come with recognition. The main concern of the semiotic art historian is to come up with ways to navigate and interpret connoted meaning.

Semiotic art history seeks to uncover the codified meaning or meanings in an aesthetic object by examining its connectedness to a collective consciousness. Art historians do not commonly commit to any one particular brand of semiotics but rather construct an amalgamated version which they incorporate into their collection of analytical tools. For example, Meyer Schapiro borrowed Saussure’s differential meaning in effort to read signs as they exist within a system. According to Schapiro, to understand the meaning of frontality in a specific pictorial context, it must be differentiated from, or viewed in relation to, alternate possibilities such as a profile, or a three-quarter view. Schapiro combined this method with the work of Charles Sanders Peirce whose object, sign, and interpretant provided a structure for his approach. Alex Potts demonstrates the application of Peirce’s concepts to visual representation by examining them in relation to the "Mona Lisa". By seeing the "Mona Lisa", for example, as something beyond its materiality is to identify it as a sign. It is then recognized as referring to an object outside of itself, a woman, or "Mona Lisa". The image does not seem to denote religious meaning and can therefore be assumed to be a portrait. This interpretation leads to a chain of possible interpretations: who was the sitter in relation to Leonardo da Vinci? What significance did she have to him? Or, maybe she is an icon for all of womankind. This chain of interpretation, or “unlimited semiosis” is endless; the art historian's job is to place boundaries on possible interpretations as much as it is to reveal new possibilities.

Semiotics operates under the theory that an image can only be understood from the viewer's perspective. The artist is supplanted by the viewer as the purveyor of meaning, even to the extent that an interpretation is still valid regardless of whether the creator had intended it. Rosalind Krauss espoused this concept in her essay “In the Name of Picasso.” She denounced the artist's monopoly on meaning and insisted that meaning can only be derived after the work has been removed from its historical and social context. Mieke Bal argued similarly that meaning does not even exist until the image is observed by the viewer. It is only after acknowledging this that meaning can become opened up to other possibilities such as feminism or psychoanalysis.

Aspects of the subject which have come to the fore in recent decades include interest in the patronage and consumption of art, including the economics of the art market, the role of collectors, the intentions and aspirations of those commissioning works, and the reactions of contemporary and later viewers and owners. Museum studies, including the history of museum collecting and display, is now a specialized field of study, as is the history of collecting.

Scientific advances have made possible much more accurate investigation of the materials and techniques used to create works, especially infra-red and x-ray photographic techniques which have allowed many underdrawings of paintings to be seen again. Proper analysis of pigments used in paint is now possible, which has upset many attributions. Dendrochronology for panel paintings and radio-carbon dating for old objects in organic materials have allowed scientific methods of dating objects to confirm or upset dates derived from stylistic analysis or documentary evidence. The development of good colour photography, now held digitally and available on the internet or by other means, has transformed the study of many types of art, especially those covering objects existing in large numbers which are widely dispersed among collections, such as illuminated manuscripts and Persian miniatures, and many types of archaeological artworks.

Concurrent to those technological advances, art historians have shown increasing interest in new theoretical approaches to the nature of artworks as objects. Thing theory, actor–network theory, and object-oriented ontology have played an increasing role in art historical literature.

The making of art, the academic history of art, and the history of art museums are closely intertwined with the rise of nationalism. Art created in the modern era, in fact, has often been an attempt to generate feelings of national superiority or love of one's country. Russian art is an especially good example of this, as the Russian avant-garde and later Soviet art were attempts to define that country's identity.

Most art historians working today identify their specialty as the art of a particular culture and time period, and often such cultures are also nations. For example, someone might specialize in the 19th-century German or contemporary Chinese art history. A focus on nationhood has deep roots in the discipline. Indeed, Vasari's "Lives of the Most Excellent Painters, Sculptors, and Architects" is an attempt to show the superiority of Florentine artistic culture, and Heinrich Wölfflin's writings (especially his monograph on Albrecht Dürer) attempt to distinguish Italian from German styles of art.

Many of the largest and most well-funded art museums of the world, such as the Louvre, the Victoria and Albert Museum, and the National Gallery of Art in Washington are state-owned. Most countries, indeed, have a national gallery, with an explicit mission of preserving the cultural patrimony owned by the government—regardless of what cultures created the art—and an often implicit mission to bolster that country's own cultural heritage. The National Gallery of Art thus showcases art made in the United States, but also owns objects from across the world.

The field of Art History is traditionally divided into specializations or concentrations based on eras and regions, with further sub-division based on media. Thus, someone might specialize in "19th-century German architecture" or in "16th-century Tuscan sculpture." Sub-fields are often included under a specialization. For example, the Ancient Near East, Greece, Rome, and Egypt are all typically considered special concentrations of Ancient art. In some cases, these specializations may be closely allied (as Greece and Rome, for example), while in others such alliances are far less natural (Indian art versus Korean art, for example).

Non-Western art is a relative newcomer to the Art Historical canon. Recent revisions of the semantic division between art and artifact have recast objects created in non-Western cultures in more aesthetic terms. Relative to those studying Ancient Rome or the Italian Renaissance, scholars specializing in Africa, the Ancient Americas and Asia are a growing minority.

"Contemporary art history" refers to research into the period from the 1960s until today reflecting the break from the assumptions of modernism brought by artists of the neo-avant-garde and a continuity in contemporary art in terms of practice based on conceptualist and post-conceptualist practices.

In the United States, the most important art history organization is the College Art Association. It organizes an annual conference and publishes the "Art Bulletin" and "Art Journal". Similar organizations exist in other parts of the world, as well as for specializations, such as architectural history and Renaissance art history. In the UK, for example, the Association of Art Historians is the premiere organization, and it publishes a journal titled "Art History".





</doc>
<doc id="60162699" url="https://en.wikipedia.org/wiki?curid=60162699" title="Change and continuity">
Change and continuity

Change and continuity is a classic dichotomy within the fields of history, historical sociology, and the social sciences more broadly. The dichotomy is used to discuss and evaluate the extent to which a historical development or event represents a decisive historical change or whether a situation remains largely unchanged. The question of change and continuity is considered a classic discussion in the study of historical developments. A good example of this discussion is the question of "how much" the Peace of Westphalia in 1648 represents an important "change" in European history. In a similar vein, historian Richard Kirkendall questioned whether FDR's New Deal represented "a radical innovation or a continuation of earlier themes in American life?" and posed the question of whether "historical interpretations of the New Deal [should] stress change or emphasize continuity?"

The dichotomy lends itself to constructing and evaluating historical periodizations. In terms of creating and discussing periodizations (e.g. the Enlightenment or the Victorian Era), the dichotomy can be used to assess when a period can be said to start and end, thus making the dichotomy important in relation to understanding historical chronology.
Economic historian Alexander Gerschenkron argues that continuity "appears to mean no more than absence of change, i.e. stability." German historian Reinhart Koselleck, however, has been said to challenge this dichotomy. 


</doc>
<doc id="30531158" url="https://en.wikipedia.org/wiki?curid=30531158" title="Authenticity in art">
Authenticity in art

Authenticity in art is the different ways in which a work of art or an artistic performance may be considered authentic.
Denis Dutton distinguishes between "nominal authenticity" and "expressive authenticity".
The first refers to the correct identification of the author of a work of art, to how closely a performance of a play or piece of music conforms to the author's intention, or to how closely a work of art conforms to an artistic tradition.
The second sense refers to how much the work possesses original or inherent authority, how much sincerity, genuineness of expression, and moral passion the artist or performer puts into the work.

A quite different concern is the authenticity of the experience, which may be impossible to achieve. A modern visitor to a museum may not only see an object in a very different context from that which the artist intended, but may be unable to understand important aspects of the work. The authentic experience may be impossible to recapture.

Authenticity is a requirement for inscription upon the UNESCO World Heritage List. According to the "Nara Document on Authenticity", it can be expressed through 'form and design; materials and substance; use and function; traditions and techniques; location and setting; spirit and feeling; and other internal and external factors.'

Authenticity of provenance means that the origin or authorship of a work of art has been correctly identified.
As Lionel Trilling points out in his 1972 book "Sincerity and Authenticity", the question of authenticity of provenance has acquired a profoundly moral dimension. Regardless of the appearance of the object or the quality of workmanship, there is great importance in knowing whether a vase is a genuine Ming vase or just a clever forgery.
This intense interest in authenticity is relatively recent and is largely confined to the western world. In the medieval period, and in countries such as modern Thailand, there was or is little interest in the identity of the artist.

The case of Han van Meegeren is well known. After failing to succeed as an artist in his own right, he turned to creating fake Vermeer paintings. These were accepted as genuine by experts and acclaimed as masterpieces. After being arrested for selling national treasures to the Germans, he caused a sensation when he publicly demonstrated that he was the artist.
To guard against forgeries like this, a certificate of authenticity may be used to prove that a work of art is authentic, but there is a sizable market in fake certificates.
Furthermore a combination of art historical, conservational and technical evidence can be used to authenticate a work of art. The financial importance of authenticity may bias collectors to acquiring recent works of art where provenance can more easily be proven, perhaps even by a statement from the artist.
For older works, an increasingly sophisticated array of forensic techniques may be deployed to establish authenticity of provenance.

The philosopher Nelson Goodman discusses at length the question raised by Aline B. Saarinen: "If a fake is so expert that even after the most thorough and trustworthy examination its authenticity is still open to doubt, is it or is it not as satisfactory a work of art as if it were unequivocally genuine?" Goodman concludes that the question is academic, since there must be some way to distinguish a forgery from the original, and once the forgery is known for what it is, that knowledge alters the perception of value.
However, Arthur Koestler in "The Act of Creation" answers that if a forgery fits into an artist's body of work and produces the same kind of aesthetic pleasure as other works by that artist, there is no reason to exclude it from a museum.

The question of the value of a forgery may be irrelevant to a curator, since they are concerned only with the provenance of the work and not with its artistic merit.
Even for the curator, in many cases provenance is a matter of probabilities rather than a certainty - absolute proof is not possible.
But once a forgery has been exposed, no matter how highly the work was praised when it was thought to be "authentic" there is rarely any interest in evaluating the work on its own merit.
Reproduction is inherent to some forms of art. In Medieval Europe, an artist might create a drawing which was used by another craftsman to create a woodcut block. The drawing was usually destroyed in the block-cutting process, and the block was thrown away when it became worn out. The copies printed from the block are all that remain of the work.
In a 1936 essay, Walter Benjamin discussed the new media of photography and film, in which the work of art can be reproduced many times with no one version being the authentic "original". He linked this shift from authentic objects to broadly accessible mass media with a transformation in the function of art from ritual to politics.
Modern art may raise new issues of authenticity of provenance. For example, the artist Duane Hanson instructed the conservators of his 1971 sculpture "Sunbather" to feel free to replace elements such as the bathing cap or swimsuit if they became faded.
As Julian H. Scaff points out, the computer and the internet further confuse the issue of authenticity of provenance, since a digital work of art may exist in thousands or millions of identical versions, and in variants where there is no way to determine the original version or even the author.

Authenticity of provenance is concerned with identifying the person who made the work, or at least pinning down the place and time in which the work was made as closely as possible. Cultural authenticity, or authenticity of style or tradition, is concerned with whether a work is a genuine expression of an artistic tradition, even when the author may be anonymous. Interest in this form of authenticity may be associated with a romantic sense of the value of the pure, unadulterated tradition, often linked to nationalistic and possibly racist beliefs.

A work of art may be considered an authentic example of a traditional culture or genre when it conforms to the style, materials, process of creation and other essential attributes of that genre.
Many traditions are thought to be "owned" by an ethnic group, and work in that genre is only considered authentic if it is created by a member of that group. Thus Inuit art can only be considered authentic if created by an Inuit. This may help to protect the originators of an art tradition from cultural appropriation, but there is a racist aspect to the view as described by Joel Rudinow in his essay "Race, Ethnicity, Expressive Authenticity: Can White People Sing the Blues?"

The market for "primitive art" developed in the western world towards the end of the 19th century as explorers or colonialists came into contact with formerly unknown cultural groups in Africa, Asia and the Pacific. These people quickly learned how to incorporate new materials supplied by traders into their art, such as cloth and glass beads, but found that they could not sell these "inauthentic" objects. However, they learned how to manufacture works from local materials that would be considered authentic for sale to the westerners.
This process of creating art that will be considered authentic by western buyers continues to this day. The objects may be designed or modified to give the impression of having popular attributes and provenance, including religious or ritual uses, antiquity and association with royalty.
Similarly, in the 1940s Haitian professional artists began to create imitations of images provided by foreign entrepreneurs. The images represented the foreigners' view of the essence of Haitian Vodou art. The Haitian works were later claimed to be authentic.
To distinguish from crude objects made for the tourist trade, many collectors consider that a work is only an authentic example of a traditional genre if it meets certain standards of quality and was made for the original purpose. Dutton gives the example of the Igorot of northern Luzon who have long created figurines ("bulul") for use in traditional ceremonies, but today produce them primarily for the tourist trade. An Igorot family may purchase a roughly carved "bulul" from a tourist booth and use it for traditional ceremonies, thus giving authenticity to the work that would not, perhaps, be present otherwise.
Although collectors place greater value on "tribal" masks or sculptures that have been used in an active ritual, it may be impossible to prove whether this is the case. Even if a video shows the mask being worn in a ritual dance, the dance may have been staged for tourists. Yet if the provenance of the mask is proven, if the mask was made by a member of the society using traditional designs and techniques, it is presumably an authentic example of the style or tradition.

It is not always clear what constitutes a style. For example, production of Zimbabwean stone sculptures is relatively recent, dating to the 1950s. It does not draw on any earlier tradition. However, the sculpture plays an important role in establishing the existence of a uniquely Zimbabwean culture, and the authenticity of this style is strongly emphasized by the government of Zimbabwe despite the difficulty of defining its characteristics.
Navajo sand paintings raise a different issue. The traditional paintings must be destroyed on completion of the ritual in which they are used. However, Navajo artists create sand paintings for sale with slightly modified designs. Can these paintings be considered authentic examples of Navajo art?

Traditions change. In an exploration of the evolution of the art of the Maroon people of French Guiana, Sally Price shows that contemporary styles have developed through a complex interaction between artists and buyers. The Maroons have a long tradition of artwork, primarily in the form of decoration of everyday objects such as paddles or shutters. This art was purely aesthetic in purpose, with no symbolic meaning. However, European collectors needed to assign symbolism to "native art". Over time, the Maroon artists have come to accept the European semiotic vocabulary and to assign symbolism to their work, which younger artists may believe to be based on ancestral traditions. The artists have also moved into new media and new designs. Their art may still be considered authentic examples of Maroon art, but the art form and the meaning associated with it is new.

With performance arts such as music and theater, both the composer or playwright and the performers are involved in creating an instance of the work. There are some who consider that a performance is only truly authentic if it approximates as closely as possible what the original author would have expected to see and hear. In a historically informed performance, the actors or musicians will make every effort to achieve this effect by using replicas of historical instruments, studying historical guides to acting and so on. They would consider, for example, that a performance of one of Mozart's piano concertos would be "inauthentic" if played on a modern concert grand piano, an instrument that would have been unknown to the composer.

Others would not take such a rigorous view. For example, they would accept a performance of a play by Shakespeare as authentic even if the female parts were played by women rather than boys, and if the words were spoken with modern pronunciation rather than with the pronunciation of the Elizabethan era, which would be difficult for a modern audience to understand.

Dutton's concept of expressive authenticity is based on the "Oxford English Dictionary" alternative definition of "possessing original or inherent authority". In this sense, authenticity is a measure of the degree to which the artist's work is a committed, personal expression rather than derived from other work. It includes concepts of originality, honesty and integrity.
In the case of a musical performance, authenticity of expression may conflict with authenticity of performance. The player is true to their personal musical sense and does not imitate someone else's method of playing. Their performance may thus differ significantly from that of a player attempting to follow the style common at the time the musical work was composed.
Expressive authenticity is related to the technical term "authenticity" as used in existential philosophy. It has always been thought right to know oneself and to act accordingly, and in existential psychology this form of authenticity is seen as central to mental health.
Prominent artists such as the Abstract Expressionists Jackson Pollock, Arshile Gorky, and Willem de Kooning have been understood in existentialist terms, as have filmmakers such as Jean-Luc Godard and Ingmar Bergman.
The greater popularity of performer-based music as opposed to composition-based music is relatively recent. It seems to reflect a growing interest in expressive authenticity, and thus in musicians who have a unique and charismatic style.

The question of whether an artistic work is an authentic expression depends on the artist's background, beliefs and ideals. Andrew Potter cites the example of Avril Lavigne, a teenage singer from Napanee, Ontario who released her debut album in 2002. She claimed to be a small-town skateboarder, with her background providing the subjects of her songs, and said these songs were her own compositions. These claims of authenticity of expression and of provenance were both challenged. However, her work could have been authentic in expression even if Lavigne had not written it, or authentic in provenance if she had written it but not authentic in expression if the carefully cultivated skater-girl image were false.
Authenticity of expression may thus be linked with authenticity of style or tradition. Many feel it is not permissible for someone to speak in the voice of another culture or racial background, and that such an expression cannot be authentic. For example, hip hop was originally an art form through which underprivileged minorities in the United States protested against their condition. As it has become less of an underground culture, there is debate over whether the spirit of hip hop can survive in a marketable integrated version.
In "Authenticity Within Hip Hop and Other Cultures Threatened with Assimilation," Kembrew McLeod argues that hip hop culture is threatened with assimilation by a larger, mainstream culture, and that authenticity of expression in this genre is being lost.

A quite different concern is the authenticity of the experience, which may be impossible to achieve. A modern visitor to a museum may not only see an object in a very different context from that which the artist intended, but also may be unable to understand important aspects of the work. The authentic experience may be impossible to recapture. A curator may accept this, perhaps attempting to present the works of art in their authentic condition, but accepting that the artificial setting and lighting are legitimate in providing a contemporary experience of the artwork, even if this experience cannot be "authentic".

Dutton discusses the importance of the audience, giving a hypothetical example based on "La Scala", the famous Milan opera house. He imagines that the natural audience, informed aficionados of the opera, lose interest and cease to attend, but the performances continue to be given to tourists who have no understanding of the work they are experiencing. In another example, he quotes a Pacific Island dancer saying "Culture? That's what we do for the tourists." In both cases, although the performances may be authentic in the sense of being true to the original, the authenticity of the experience is open to debate.



</doc>
<doc id="13692155" url="https://en.wikipedia.org/wiki?curid=13692155" title="Philosophy">
Philosophy

Philosophy (from Greek , "philosophia", literally "love of wisdom") is the study of general and fundamental questions about existence, knowledge, values, reason, mind, and language. Such questions are often posed as problems to be studied or resolved. The term was probably coined by Pythagoras (c. 570 – 495 BCE). Philosophical methods include questioning, critical discussion, rational argument, and systematic presentation. Classic philosophical questions include: Is it possible to know anything and to prove it? What is most real? Philosophers also pose more practical and concrete questions such as: Is there a best way to live? Is it better to be just or unjust (if one can get away with it)? Do humans have free will?

Historically, "philosophy" encompassed any body of knowledge. From the time of Ancient Greek philosopher Aristotle to the 19th century, "natural philosophy" encompassed astronomy, medicine, and physics. For example, Newton's 1687 "Mathematical Principles of Natural Philosophy" later became classified as a book of physics. In the 19th century, the growth of modern research universities led academic philosophy and other disciplines to professionalize and specialize. In the modern era, some investigations that were traditionally part of philosophy became separate academic disciplines, including psychology, sociology, linguistics, and economics.

Other investigations closely related to art, science, politics, or other pursuits remained part of philosophy. For example, is beauty objective or subjective? Are there many scientific methods or just one? Is political utopia a hopeful dream or hopeless fantasy? Major sub-fields of academic philosophy include metaphysics ("concerned with the fundamental nature of reality and being"), epistemology (about the "nature and grounds of knowledge [and]...its limits and validity"), ethics, aesthetics, political philosophy, logic and philosophy of science.

Traditionally, the term "philosophy" referred to any body of knowledge. In this sense, philosophy is closely related to religion, mathematics, natural science, education and politics. Newton's 1687 "Mathematical Principles of Natural Philosophy" is classified in the 2000s as a book of physics; he used the term "natural philosophy" because it used to encompass disciplines that later became associated with sciences such as astronomy, medicine and physics.

In the first part of the first book of his "Academics", Cicero introduced the division of philosophy into logic, physics, and ethics. He was copying Epicurus' division of his doctrine into canon, physics, and ethics. In section thirteen of the first book of his "Lives and Opinions of the Eminent Philosophers", the 3rd-century Diogenes Laërtius, the first historian of philosophy, established the traditional division of philosophical inquiry into three parts:

This division is not obsolete but has changed. Natural philosophy has split into the various natural sciences, especially astronomy, physics, chemistry, biology, and cosmology. Moral philosophy has birthed the social sciences, but still includes value theory (including aesthetics, ethics, political philosophy, etc.). Metaphysical philosophy has birthed formal sciences such as logic, mathematics and philosophy of science, but still includes epistemology, cosmology and others.

Many philosophical debates that began in ancient times are still debated today. Colin McGinn and others claim that no philosophical progress has occurred during that interval. Chalmers and others, by contrast, see progress in philosophy similar to that in science, while Talbot Brewer argued that "progress" is the wrong standard by which to judge philosophical activity.

In one general sense, philosophy is associated with wisdom, intellectual culture and a search for knowledge. In that sense, all cultures and literate societies ask philosophical questions such as "how are we to live" and "what is the nature of reality". A broad and impartial conception of philosophy then, finds a reasoned inquiry into such matters as reality, morality and life in all world civilizations.

Western philosophy is the philosophical tradition of the Western world and dates to Pre-Socratic thinkers who were active in Ancient Greece in the 6th century BCE such as Thales (c. 624–546 BCE) and Pythagoras (c. 570–495 BCE) who practiced a "love of wisdom" ("philosophia") and were also termed "physiologoi" (students of "physis", or nature). Socrates was a very influential philosopher, who insisted that he possessed no "wisdom" but was a "pursuer of" wisdom. Western philosophy can be divided into three eras: ancient (Greco-Roman), medieval philosophy (Christian European), and modern philosophy.

The ancient era was dominated by Greek philosophical schools which arose out of the various pupils of Socrates, such as Plato, who founded the Platonic Academy and his student Aristotle, founding the Peripatetic school, who were both extremely influential in Western tradition. Other traditions include Cynicism, Stoicism, Skepticism and Epicureanism. Important topics covered by the Greeks included metaphysics (with competing theories such as atomism and monism), cosmology, the nature of the well-lived life ("eudaimonia"), the possibility of knowledge and the nature of reason (logos). With the rise of the Roman empire, Greek philosophy was also increasingly discussed in Latin by Romans such as Cicero and Seneca (see Roman philosophy).

Medieval philosophy (5th–16th centuries) is the period following the fall of the Western Roman Empire and was dominated by the rise of Christianity and hence reflects Judeo-Christian theological concerns as well as retaining a continuity with Greco-Roman thought. Problems such as the existence and nature of God, the nature of faith and reason, metaphysics, the problem of evil were discussed in this period. Some key Medieval thinkers include St. Augustine, Thomas Aquinas, Boethius, Anselm and Roger Bacon. Philosophy for these thinkers was viewed as an aid to Theology ("ancilla theologiae") and hence they sought to align their philosophy with their interpretation of sacred scripture. This period saw the development of Scholasticism, a text critical method developed in medieval universities based on close reading and disputation on key texts. The Renaissance period saw increasing focus on classic Greco-Roman thought and on a robust Humanism.

Early modern philosophy in the Western world begins with thinkers such as Thomas Hobbes and René Descartes (1596–1650). Following the rise of natural science, modern philosophy was concerned with developing a secular and rational foundation for knowledge and moved away from traditional structures of authority such as religion, scholastic thought and the Church. Major modern philosophers include Spinoza, Leibniz, Locke, Berkeley, Hume, and Kant. 19th-century philosophy (late modern philosophy) is influenced by the wider movement termed the Enlightenment, and includes figures such as Hegel a key figure in German idealism, Kierkegaard who developed the foundations for existentialism, Nietzsche a famed anti-Christian, John Stuart Mill who promoted utilitarianism, Karl Marx who developed the foundations for communism and the American William James. The 20th century saw the split between analytic philosophy and continental philosophy, as well as philosophical trends such as phenomenology, existentialism, logical positivism, pragmatism and the linguistic turn (see Contemporary philosophy).

The regions of the fertile Crescent, Iran and Arabia are home to the earliest known philosophical Wisdom literature and is today mostly dominated by Islamic culture. Early wisdom literature from the fertile crescent was a genre which sought to instruct people on ethical action, practical living and virtue through stories and proverbs. In Ancient Egypt, these texts were known as sebayt ('teachings') and they are central to our understandings of Ancient Egyptian philosophy. Babylonian astronomy also included much philosophical speculations about cosmology which may have influenced the Ancient Greeks. Jewish philosophy and Christian philosophy are religio-philosophical traditions that developed both in the Middle East and in Europe, which both share certain early Judaic texts (mainly the Tanakh) and monotheistic beliefs. Jewish thinkers such as the Geonim of the Talmudic Academies in Babylonia and Maimonides engaged with Greek and Islamic philosophy. Later Jewish philosophy came under strong Western intellectual influences and includes the works of Moses Mendelssohn who ushered in the Haskalah (the Jewish Enlightenment), Jewish existentialism and Reform Judaism.

Pre-Islamic Iranian philosophy begins with the work of Zoroaster, one of the first promoters of monotheism and of the dualism between good and evil. This dualistic cosmogony influenced later Iranian developments such as Manichaeism, Mazdakism, and Zurvanism.

After the Muslim conquests, Early Islamic philosophy developed the Greek philosophical traditions in new innovative directions. This Islamic Golden Age influenced European intellectual developments. The two main currents of early Islamic thought are Kalam which focuses on Islamic theology and Falsafa which was based on Aristotelianism and Neoplatonism. The work of Aristotle was very influential among the falsafa such as al-Kindi (9th century), Avicenna (980 – June 1037) and Averroes (12th century). Others such as Al-Ghazali were highly critical of the methods of the Aristotelian falsafa. Islamic thinkers also developed a scientific method, experimental medicine, a theory of optics and a legal philosophy. Ibn Khaldun was an influential thinker in philosophy of history.

In Iran several schools of Islamic philosophy continued to flourish after the Golden Age and include currents such as Illuminationist philosophy, Sufi philosophy, and Transcendent theosophy. The 19th- and 20th-century Arab world saw the Nahda (awakening or renaissance) movement which influenced contemporary Islamic philosophy.

Indian philosophy (; 'world views', 'teachings') refers to the diverse philosophical traditions that emerged since the ancient times on the Indian subcontinent. Jainism and Buddhism originated at the end of the Vedic period, while Hinduism emerged as a fusion of diverse traditions, starting after the end of the Vedic period.

Hindus generally classify these traditions as either orthodox or heterodox – āstika or nāstika – depending on whether they accept the authority of the Vedas and the theories of Brahman and Atman (soul, self) therein. The orthodox schools include the Hindu traditions of thought, while the heterodox schools include the Buddhist and the Jain traditions. Other schools include the Ajñana, Ajivika and Cārvāka which became extinct over their history.

Important Indian philosophical concepts shared by the Indian philosophies include dharma, karma, artha, kama, dukkha (suffering), anitya (anicca, impermanence), dhyana (jhana, meditation), renunciation (with or without monasticism or asceticism), various samsara with cycles of rebirth, moksha (nirvana, kaivalya, liberation from rebirth), and virtues such as ahimsa.

Jain philosophy accepts the concept of a permanent soul (jiva) as one of the five "astikayas", or eternal infinite categories that make up the substance of existence. The other four being "dharma", "adharma", "akasha" (space) and "pudgala" (matter). The Jain thought separates matter from the soul completely. It has two major subtraditions: Digambara (sky dressed, naked) and Svetambara (white dressed), along with several more minor traditions such as Terapanthis. Asceticism is a major monastic virtue in Jainism. Jain texts such as the "Tattvartha Sutra" state that right faith, right knowledge and right conduct is the path to liberation. The Jain thought holds that all existence is cyclic, eternal and uncreated. The "Tattvartha Sutra" is the earliest known, most comprehensive and authoritative compilation of Jain philosophy.

Buddhist philosophy begins with the thought of Gautama Buddha (fl. between sixth and fourth centuries BCE) and is preserved in the early Buddhist texts. It originated in India and later spread to East Asia, Tibet, Central Asia, and Southeast Asia, developing various traditions in these regions. Mahayana forms are the dominant Buddhist philosophical traditions in East Asian regions such as China, Korea and Japan. The Theravada forms are dominant in Southeast Asian countries, such as Sri Lanka, Burma and Thailand.

Because ignorance to the true nature of things is considered one of the roots of suffering ("dukkha"), Buddhist philosophy is concerned with epistemology, metaphysics, ethics and psychology. Buddhist philosophical texts must also be understood within the context of meditative practices which are supposed to bring about certain cognitive shifts. Key innovative concepts include the four noble truths as an analysis of suffering, anicca (impermanence) and anatta (not-self).

After the death of the Buddha, various groups began to systematize his main teachings, eventually developing comprehensive philosophical systems termed 'Abhidharma'. Following the Abhidharma schools, Mahayana philosophers such as Nagarjuna and Vasubandhu developed the theories of "shunyata" (emptiness of all phenomena) and "vijñapti-matra" (appearance only), a form of phenomenology or transcendental idealism. The Dignāga school of "pramāṇa" (lit. means of knowledge) promoted a sophisticated form of Buddhist logico-epistemology.

There were numerous schools, sub-schools and traditions of Buddhist philosophy in India. According to Oxford professor of Buddhist philosophy Jan Westerhoff, the major Indian schools from 300 BCE to 1000 CE were:


After the disappearance of Buddhism from India, some of these philosophical traditions continued to develop in the Tibetan Buddhist, East Asian Buddhist and Theravada Buddhist traditions.

The Vedas-based orthodox schools are a part of the Hindu traditions and they are traditionally classified into six "darsanas": Nyaya, Vaisheshika, Samkhya, Yoga, Mīmāṃsā and Vedanta. The Vedas as a knowledge source were interpreted differently by these six schools of Hindu philosophy, with varying degrees of overlap. They represent a "collection of philosophical views that share a textual connection", according to Chadha. They also reflect a tolerance for a diversity of philosophical interpretations within Hinduism while sharing the same foundation.

Some of the earliest surviving Hindu mystical and philosophical texts are the Upanishads of the later Vedic period (1000–500 BCE). Hindu philosophers of the six schools developed systems of epistemology (pramana) and investigated topics such as metaphysics, ethics, psychology ("guna"), hermeneutics and soteriology within the framework of the Vedic knowledge, while presenting a diverse collection of interpretations. These schools of philosophy accepted the Vedas and the Vedic concept of "Atman" and "Brahman", differed from the following Indian religions that rejected the authority of the Vedas:

The commonly named six orthodox schools over time led to what has been called the "Hindu synthesis" as exemplified by its scripture the "Bhagavad Gita".

East Asian philosophical thought began in Ancient China, and Chinese philosophy begins during the Western Zhou Dynasty and the following periods after its fall when the "Hundred Schools of Thought" flourished (6th century to 221 BCE). This period was characterized by significant intellectual and cultural developments and saw the rise of the major philosophical schools of China, Confucianism, Legalism, and Daoism as well as numerous other less influential schools. These philosophical traditions developed metaphysical, political and ethical theories such Tao, Yin and yang, Ren and Li which, along with Chinese Buddhism, directly influenced Korean philosophy, Vietnamese philosophy and Japanese philosophy (which also includes the native Shinto tradition). Buddhism began arriving in China during the Han Dynasty (206 BCE – 220 CE), through a gradual Silk road transmission and through native influences developed distinct Chinese forms (such as Chan/Zen) which spread throughout the East Asian cultural sphere. During later Chinese dynasties like the Ming Dynasty (1368–1644) as well as in the Korean Joseon dynasty (1392–1897) a resurgent Neo-Confucianism led by thinkers such as Wang Yangming (1472–1529) became the dominant school of thought, and was promoted by the imperial state.

In the Modern era, Chinese thinkers incorporated ideas from Western philosophy. Chinese Marxist philosophy developed under the influence of Mao Zedong, while a Chinese pragmatism under Hu Shih and New Confucianism's rise was influenced by Xiong Shili. Modern Japanese thought meanwhile developed under strong Western influences such as the study of Western Sciences (Rangaku) and the modernist Meirokusha intellectual society which drew from European enlightenment thought. The 20th century saw the rise of State Shinto and also Japanese nationalism. The Kyoto School, an influential and unique Japanese philosophical school developed from Western phenomenology and Medieval Japanese Buddhist philosophy such as that of Dogen.

African philosophy is philosophy produced by African people, philosophy that presents African worldviews, ideas and themes, or philosophy that uses distinct African philosophical methods. Modern African thought has been occupied with Ethnophilosophy, with defining the very meaning of African philosophy and its unique characteristics and what it means to be African. During the 17th century, Ethiopian philosophy developed a robust literary tradition as exemplified by Zera Yacob. Another early African philosopher was Anton Wilhelm Amo (c. 1703–1759) who became a respected philosopher in Germany. Distinct African philosophical ideas include Ujamaa, the Bantu idea of 'Force', Négritude, Pan-Africanism and Ubuntu. Contemporary African thought has also seen the development of Professional philosophy and of Africana philosophy, the philosophical literature of the African diaspora which includes currents such as black existentialism by African-Americans. Some modern African thinkers have been influenced by Marxism, African-American literature, Critical theory, Critical race theory, Postcolonialism and Feminism.

Indigenous American philosophy is the philosophy of the Indigenous people of the Americas. There is a wide variety of beliefs and traditions among these different American cultures. Among some of the Native Americans in the United States there is a belief in a metaphysical principle called the "Great Mystery" (Siouan: Wakan Tanka, Algonquian: Gitche Manitou). Another widely shared concept was that of Orenda or "spiritual power". According to Peter M. Whiteley, for the Native Americans, "Mind is critically informed by transcendental experience (dreams, visions and so on) as well as by reason." The practices to access these transcendental experiences are termed Shamanism. Another feature of the indigenous American worldviews was their extension of ethics to non-human animals and plants.

In Mesoamerica, Aztec philosophy was an intellectual tradition developed by individuals called Tlamatini ('those who know something') and its ideas are preserved in various Aztec codices. The Aztec worldview posited the concept of an ultimate universal energy or force called Ometeotl which can be translated as "Dual Cosmic Energy" and sought a way to live in balance with a constantly changing, "slippery" world. The theory of Teotl can be seen as a form of Pantheism. Aztec philosophers developed theories of metaphysics, epistemology, values, and aesthetics. Aztec ethics was focused on seeking "tlamatiliztli" (knowledge, wisdom) which was based on moderation and balance in all actions as in the Nahua proverb "the middle good is necessary".

The Inca civilization also had an elite class of philosopher-scholars termed the Amawtakuna who were important in the Inca education system as teachers of religion, tradition, history and ethics. Key concepts of Andean thought are Yanantin and Masintin which involve a theory of “complementary opposites” that sees polarities (such as male/female, dark/light) as interdependent parts of a harmonious whole.

Philosophical questions can be grouped into categories. These groupings allow philosophers to focus on a set of similar topics and interact with other thinkers who are interested in the same questions. The groupings also make philosophy easier for students to approach. Students can learn the basic principles involved in one aspect of the field without being overwhelmed with the entire set of philosophical theories.

Various sources present different categorical schemes. The categories adopted in this article aim for breadth and simplicity.

These five major branches can be separated into sub-branches and each sub-branch contains many specific fields of study.

These divisions are neither exhaustive, nor mutually exclusive. (A philosopher might specialize in Kantian epistemology, or Platonic aesthetics, or modern political philosophy). Furthermore, these philosophical inquiries sometimes overlap with each other and with other inquiries such as science, religion or mathematics.

Metaphysics is the study of the most general features of reality, such as existence, time, objects and their properties, wholes and their parts, events, processes and causation and the relationship between mind and body. Metaphysics includes cosmology, the study of the world in its entirety and ontology, the study of being.

A major point of debate is between realism, which holds that there are entities that exist independently of their mental perception and idealism, which holds that reality is mentally constructed or otherwise immaterial. Metaphysics deals with the topic of identity. Essence is the set of attributes that make an object what it fundamentally is and without which it loses its identity while accident is a property that the object has, without which the object can still retain its identity. Particulars are objects that are said to exist in space and time, as opposed to abstract objects, such as numbers, and universals, which are properties held by multiple particulars, such as redness or a gender. The type of existence, if any, of universals and abstract objects is an issue of debate.

Epistemology is the study of knowledge (Greek episteme). Epistemologists study the putative sources of knowledge, including intuition, a priori reason, memory, perceptual knowledge, self-knowledge and testimony. They also ask: What is truth? Is knowledge justified true belief? Are any beliefs justified? Putative knowledge includes propositional knowledge (knowledge that something is the case), know-how (knowledge of how to do something) and acquaintance (familiarity with someone or something). Epistemologists examine these and ask whether knowledge is really possible.

Skepticism is the position which doubts claims to knowledge. The regress argument, a fundamental problem in epistemology, occurs when, in order to completely prove any statement, its justification itself needs to be supported by another justification. This chain can go on forever, called infinitism, it can eventually rely on basic beliefs that are left unproven, called foundationalism, or it can go in a circle so that a statement is included in its own chain of justification, called coherentism.

Rationalism is the emphasis on reasoning as a source of knowledge. It is associated with a priori knowledge, which is independent of experience, such as math and logical deduction. Empiricism is the emphasis on observational evidence via sensory experience as the source of knowledge.

Among the numerous topics within metaphysics and epistemology, broadly construed are:

Value theory (or axiology) is the major branch of philosophy that addresses topics such as goodness, beauty and justice. Value theory includes ethics, aesthetics, political philosophy, feminist philosophy, philosophy of law and more.

Ethics, or "moral philosophy", studies and considers what is good and bad conduct, right and wrong values, and good and evil. Its primary investigations include how to live a good life and identifying standards of morality. It also includes meta-investigations about whether a best way to live or related standards exists. The main branches of ethics are normative ethics, meta-ethics and applied ethics.

A major area of debate involves consequentialism, in which actions are judged by the potential results of the act, such as to maximize happiness, called utilitarianism, and deontology, in which actions are judged by how they adhere to principles, irrespective of negative ends.

Aesthetics is the "critical reflection on art, culture and nature." It addresses the nature of art, beauty and taste, enjoyment, emotional values, perception and with the creation and appreciation of beauty. It is more precisely defined as the study of sensory or sensori-emotional values, sometimes called judgments of sentiment and taste. Its major divisions are art theory, literary theory, film theory and music theory. An example from art theory is to discern the set of principles underlying the work of a particular artist or artistic movement such as the Cubist aesthetic. The philosophy of film analyzes films and filmmakers for their philosophical content and explores film (images, cinema, etc.) as a medium for philosophical reflection and expression.

Political philosophy is the study of government and the relationship of individuals (or families and clans) to communities including the state. It includes questions about justice, law, property and the rights and obligations of the citizen. Politics and ethics are traditionally linked subjects, as both discuss the question of how people should live together.

Other branches of value theory:

Many academic disciplines generated philosophical inquiry. The relationship between "X" and the "philosophy of X" is debated. Richard Feynman argued that the philosophy of a topic is irrelevant to its primary study, saying that "philosophy of science is as useful to scientists as ornithology is to birds." Curtis White, by contrast, argued that philosophical tools are essential to humanities, sciences and social sciences.

The topics of philosophy of science are numbers, symbols and the formal methods of reasoning as employed in the social sciences and natural sciences.

Logic is the study of reasoning and argument. An argument is ""a" "connected series of statements intended to establish a proposition"." The connected series of statements are "premises" and the proposition is the conclusion. For example:
Deductive reasoning is when, given certain premises, conclusions are unavoidably implied. Rules of inference are used to infer conclusions such as, modus ponens, where given “A” and “If A then B”, then “B” must be concluded.

Because sound reasoning is an essential element of all sciences, social sciences and humanities disciplines, logic became a formal science. Sub-fields include mathematical logic, philosophical logic, Modal logic, computational logic and non-classical logics. A major question in the philosophy of mathematics is whether mathematical entities are objective and discovered, called mathematical realism, or invented, called mathematical antirealism.

This branch explores the foundations, methods, history, implications and purpose of science. Many of its sub-divisions correspond to a specific branch of science. For example, philosophy of biology deals specifically with the metaphysical, epistemological and ethical issues in the biomedical and life sciences. The philosophy of mathematics studies the philosophical assumptions, foundations and implications of mathematics.

Some philosophers specialize in one or more historical periods. The history of philosophy (study of a specific period, individual or school) is related to but not the same as the philosophy of history (the theoretical aspect of history, which deals with questions such as the nature of historical evidence and the possibility of objectivity).

Hegel's "Lectures on the Philosophy of History" influenced many philosophers to interpret truth in light of history, a view called historicism.

Philosophy of religion deals with questions that involve religion and religious ideas from a philosophically neutral perspective (as opposed to theology which begins from religious convictions). Traditionally, religious questions were not seen as a separate field from philosophy proper, the idea of a separate field only arose in the 19th century.

Issues include the existence of God, the relationship between reason and faith, questions of religious epistemology, the relationship between religion and science, how to interpret religious experiences, questions about the possibility of an afterlife, the problem of religious language and the existence of souls and responses to religious pluralism and diversity.

Some philosophers specialize in one or more of the major philosophical schools, such as Continental philosophy, Analytical philosophy, Thomism, Asian philosophy or African philosophy.

A variety of other academic and non-academic approaches have been explored.

The ideas conceived by a society have profound repercussions on what actions the society performs. Weaver argued that ideas have consequences. Philosophy yields applications such as those in ethics – applied ethics in particular – and political philosophy. The political and economic philosophies of Confucius, Sun Tzu, Chanakya, Ibn Khaldun, Ibn Rushd, Ibn Taymiyyah, Machiavelli, Leibniz, Hobbes, Locke, Rousseau, Adam Smith, John Stuart Mill, Marx, Tolstoy, Gandhi and Martin Luther King Jr. have been used to shape and justify governments and their actions. Progressive education as championed by Dewey had a profound impact on 20th-century US educational practices. Descendants of this movement include efforts in philosophy for children, which are part of philosophy education. Clausewitz's political philosophy of war has had a profound effect on statecraft, international politics and military strategy in the 20th century, especially around World War II. Logic is important in mathematics, linguistics, psychology, computer science and computer engineering.

Other important applications can be found in epistemology, which aid in understanding the requisites for knowledge, sound evidence and justified belief (important in law, economics, decision theory and a number of other disciplines). The philosophy of science discusses the underpinnings of the scientific method and has affected the nature of scientific investigation and argumentation. Philosophy thus has fundamental implications for science as a whole. For example, the strictly empirical approach of B.F. Skinner's behaviorism affected for decades the approach of the American psychological establishment. Deep ecology and animal rights examine the moral situation of humans as occupants of a world that has non-human occupants to consider also. Aesthetics can help to interpret discussions of music, literature, the plastic arts and the whole artistic dimension of life. In general, the various philosophies strive to provide practical activities with a deeper understanding of the theoretical or conceptual underpinnings of their fields.

Some of those who study philosophy become professional philosophers, typically by working as professors who teach, research and write in academic institutions. However, most students of academic philosophy later contribute to law, journalism, religion, sciences, politics, business, or various arts. For example, public figures who have degrees in philosophy include comedians Steve Martin and Ricky Gervais, filmmaker Terrence Malick, Pope John Paul II, Wikipedia co-founder Larry Sanger, technology entrepreneur Peter Thiel, Supreme Court Justice Stephen Bryer and vice presidential candidate Carly Fiorina.

Recent efforts to avail the general public to the work and relevance of philosophers include the million-dollar Berggruen Prize, first awarded to Charles Taylor in 2016.

Germany was the first country to professionalize philosophy. The doctorate of philosophy (PhD) developed in Germany as the terminal Teacher's credential in the mid 17th century. At the end of 1817, Georg Wilhelm Friedrich Hegel was the first philosopher to be appointed Professor by the State, namely by the Prussian Minister of Education, as an effect of Napoleonic reform in Prussia. In the United States, the professionalization grew out of reforms to the American higher-education system largely based on the German model.
Within the last century, philosophy has increasingly become a professional discipline practiced within universities, like other academic disciplines. Accordingly, it has become less general and more specialized. In the view of one prominent recent historian: "Philosophy has become a highly organized discipline, done by specialists primarily for other specialists. The number of philosophers has exploded, the volume of publication has swelled, and the subfields of serious philosophical investigation have multiplied. Not only is the broad field of philosophy today far too vast to be embraced by one mind, something similar is true even of many highly specialized subfields." Some philosophers argue that this professionalization has negatively affected the discipline.

The end result of professionalization for philosophy has meant that work being done in the field is now almost exclusively done by university professors holding a doctorate in the field publishing in highly technical, peer-reviewed journals. While it remains common among the population at large for a person to have a set of religious, political or philosophical views that they consider their "philosophy", these views are rarely informed by or connected to the work being done in professional philosophy today. Furthermore, unlike many of the sciences for which there has come to be a healthy industry of books, magazines, and television shows meant to popularize science and communicate the technical results of a scientific field to the general populace, works by professional philosophers directed at an audience outside the profession remain rare. Philosopher Michael Sandel's book "Justice: What's the Right Thing to Do?" and Harry Frankfurt's "On Bullshit" are examples of works that hold the uncommon distinction of having been written by professional philosophers but directed at and ultimately popular among a broader audience of non-philosophers. Both works became "New York Times" best sellers.

Many inquiries outside of academia are philosophical in the broad sense. Novelists, playwrights, filmmakers, and musicians, as well as scientists and others engage in recognizably philosophical activity.












</doc>
<doc id="27900272" url="https://en.wikipedia.org/wiki?curid=27900272" title="Health humanities">
Health humanities

Health humanities is an interdisciplinary field of study that draws on aspects of the arts and humanities in its approach to health care, health and well-being. It involves the application of the creative or fine arts (including visual arts, music, performing arts) and humanities disciplines (including literary studies, languages, law, history, philosophy, religion, etc.) to questions of human health and well-being. This applied capacity of the humanities is not itself a novel idea; however, the construct of the health humanities only began to emerge in the first decade of the 21st century. Historically, the roots informing the health humanities can be traced back to, and can now be considered to include, such multidisciplinary areas as the medical humanities and the expressive therapies/creative arts therapies.

In the health humanities, health (and the promotion of health) is understood according to the constructivist (and other non-positivist) principles indigenous to the humanities, as opposed to the positivism of science. The health humanities are rooted in dialogical (negotiated, intersubjective voices of multiple truths), versus monological (a singular, authoritative voice of "the" truth) perspectives on health. As such, evidence upon which health practices are based is generally considered axiological (based in meanings, values, and aesthetics), versus epistemological (based in factual knowledge), in orientation. The health humanities are not an alternative to the health sciences, but rather offer a contrasting paradigm and pragmatic approach with respect to health and its promotion, and can function in a manner that is complementary to the health sciences.

In January 2009, Paul Crawford became the world's first Professor of Health Humanities at the University of Nottingham, and led with Victoria Tischler, Charley Baker, Brian Brown, Lisa Mooney-Smith and Ronald Carter the development of the Arts and Humanities Research Council-funded International Health Humanities Network. Baccalaureate and Masters programs in health humanities have been developed in the US, Canada and UK. In the UK, a Health Humanities Centre was established in 2015 at University College London, dedicated to research and teaching in the health humanities, including a Master of Arts degree in health humanities. In 2020, a Master of Science by Research in Health Humanities and Arts started at The University of Edinburgh.

Textbooks on the health humanities include "Health Humanities Reader", "Health Humanities", "Research Methods in Health Humanities", and "The Routledge Companion to Health Humanities".



</doc>
<doc id="42736966" url="https://en.wikipedia.org/wiki?curid=42736966" title="Critical theory">
Critical theory

Critical theory is the reflective assessment and critique of society and culture by applying knowledge from the social sciences and the humanities to reveal and challenge power structures. It argues that social problems are influenced and created more by societal structures and cultural assumptions than by individual and psychological factors. Critical theory has origins in sociology and also in literary criticism. The sociologist Max Horkheimer described a theory as critical insofar as it seeks "to liberate human beings from the circumstances that enslave them".

In sociology and political philosophy, the term "Critical Theory" describes the Western Marxist philosophy of the Frankfurt School, which was developed in Germany in the 1930s. This use of the term requires proper noun capitalization, whereas "a critical theory" or "a critical social theory" may have similar elements of thought, but does not stress the intellectual lineage specific to the Frankfurt School. Frankfurt School critical theorists drew on the critical methods of Karl Marx and Sigmund Freud. Critical theory maintains that ideology is the principal obstacle to human liberation. Critical theory was established as a school of thought primarily by the Frankfurt School theoreticians Herbert Marcuse, Theodor Adorno, Max Horkheimer, Walter Benjamin, and Erich Fromm. Modern critical theory has additionally been influenced by György Lukács and Antonio Gramsci, as well as the second generation Frankfurt School scholars, notably Jürgen Habermas. In Habermas's work, critical theory transcended its theoretical roots in German idealism and progressed closer to American pragmatism. Concern for social "base and superstructure" is one of the remaining Marxist philosophical concepts in much of contemporary critical theory.

Postmodern critical theory analyzes the fragmentation of cultural identities in order to challenge modernist era constructs such as metanarratives, rationality and universal truths, while politicizing social problems "by situating them in historical and cultural contexts, to implicate themselves in the process of collecting and analyzing data, and to relativize their findings".

Critical theory () was first defined by Max Horkheimer of the Frankfurt School of sociology in his 1937 essay "Traditional and Critical Theory": Critical theory is a social theory oriented toward critiquing and changing society as a whole, in contrast to traditional theory oriented only to understanding or explaining it. Horkheimer wanted to distinguish critical theory as a radical, emancipatory form of Marxian theory, critiquing both the model of science put forward by logical positivism and what he and his colleagues saw as the covert positivism and authoritarianism of orthodox Marxism and Communism. He described a theory as critical insofar as it seeks "to liberate human beings from the circumstances that enslave them". Critical theory involves a normative dimension, either through criticizing society from some general theory of values, norms, or "oughts", or through criticizing it in terms of its own espoused values.

The core concepts of critical theory are as follows:

This version of "critical" theory derives from Kant's (18th-century) and Marx's (19th-century) use of the term "critique", as in Kant's "Critique of Pure Reason" and Marx's concept that his work "Das Kapital" ("Capital") forms a "critique of political economy". For Kant's transcendental idealism, "critique" means examining and establishing the limits of the validity of a faculty, type, or body of knowledge, especially through accounting for the limitations imposed by the fundamental, irreducible concepts in use in that knowledge system.

Kant's notion of critique has been associated with the overturning of false, unprovable, or dogmatic philosophical, social, and political beliefs, because Kant's critique of reason involved the critique of dogmatic theological and metaphysical ideas and was intertwined with the enhancement of ethical autonomy and the Enlightenment critique of superstition and irrational authority. Ignored by many in "critical realist" circles, however, is that Kant's immediate impetus for writing his "Critique of Pure Reason" was to address problems raised by David Hume's skeptical empiricism which, in attacking metaphysics, employed reason and logic to argue against the knowability of the world and common notions of causation. Kant, by contrast, pushed the employment of a priori metaphysical claims as requisite, for if anything is to be said to be knowable, it would have to be established upon abstractions distinct from perceivable phenomena.

Marx explicitly developed the notion of critique into the critique of ideology and linked it with the practice of social revolution, as stated in the famous 11th of his "Theses on Feuerbach": "The philosophers have only interpreted the world, in various ways; the point is to change it."

One of the distinguishing characteristics of critical theory, as Adorno and Horkheimer elaborated in their "Dialectic of Enlightenment" (1947), is a certain ambivalence concerning the ultimate source or foundation of social domination, an ambivalence which gave rise to the "pessimism" of the new critical theory over the possibility of human emancipation and freedom. This ambivalence was rooted, of course, in the historical circumstances in which the work was originally produced, in particular, the rise of National Socialism, state capitalism, and culture industry as entirely new forms of social domination that could not be adequately explained within the terms of traditional Marxist sociology.

For Adorno and Horkheimer, state intervention in economy had effectively abolished the tension between the "relations of production" and "material productive forces of society", a tension which, according to traditional critical theory, constituted the primary contradiction within capitalism. The market (as an "unconscious" mechanism for the distribution of goods) had been replaced by centralized planning.

Yet, contrary to Marx's famous prediction in the "Preface to a Contribution to the Critique of Political Economy", this shift did not lead to "an era of social revolution", but rather to fascism and totalitarianism. As such, critical theory was left, in Jürgen Habermas' words, without "anything in reserve to which it might appeal, and when the forces of production enter into a baneful symbiosis with the relations of production that they were supposed to blow wide open, there is no longer any dynamism upon which critique could base its hope". For Adorno and Horkheimer, this posed the problem of how to account for the apparent persistence of domination in the absence of the very contradiction that, according to traditional critical theory, was the source of domination itself.

In the 1960s, Jürgen Habermas, a proponent of critical social theory, raised the epistemological discussion to a new level in his "Knowledge and Human Interests", by identifying critical knowledge as based on principles that differentiated it either from the natural sciences or the humanities, through its orientation to self-reflection and emancipation. Although unsatisfied with Adorno and Horkeimer's thought presented in "Dialectic of Enlightenment", Habermas shares the view that, in the form of instrumental rationality, the era of modernity marks a move away from the liberation of enlightenment and toward a new form of enslavement. In Habermas's work, critical theory transcended its theoretical roots in German idealism, and progressed closer to American pragmatism.

Habermas is now influencing the philosophy of law in many countries—for example the creation of the social philosophy of law in Brazil, and his theory also has the potential to make the discourse of law one important institution of the modern world as a heritage of the Enlightenment.

His ideas regarding the relationship between modernity and rationalization are in this sense strongly influenced by Max Weber. Habermas dissolved further the elements of critical theory derived from Hegelian German Idealism, although his thought remains broadly Marxist in its epistemological approach. Perhaps his two most influential ideas are the concepts of the public sphere and communicative action; the latter arriving partly as a reaction to new post-structural or so-called "postmodern" challenges to the discourse of modernity. Habermas engaged in regular correspondence with Richard Rorty and a strong sense of philosophical pragmatism may be felt in his theory; thought which frequently traverses the boundaries between sociology and philosophy.

While modernist critical theory (as described above) concerns itself with "forms of authority and injustice that accompanied the evolution of industrial and corporate capitalism as a political-economic system," postmodern critical theory politicizes social problems "by situating them in historical and cultural contexts, to implicate themselves in the process of collecting and analyzing data, and to relativize their findings." Meaning itself is seen as unstable due to the rapid transformation in social structures. As a result, the focus of research is centered on local manifestations, rather than broad generalizations.

Postmodern critical research is also characterized by the "crisis of representation", which rejects the idea that a researcher's work is an "objective depiction of a stable other". Instead, many postmodern scholars have adopted "alternatives that encourage reflection about the 'politics and poetics' of their work. In these accounts, the embodied, collaborative, dialogic, and improvisational aspects of qualitative research are clarified".

The term "critical theory" is often appropriated when an author works within sociological terms, yet attacks the social or human sciences (thus attempting to remain "outside" those frames of inquiry). Michel Foucault is one of these authors.

Jean Baudrillard has also been described as a critical theorist to the extent that he was an unconventional and critical sociologist; this appropriation is similarly casual, holding little or no relation to the Frankfurt School.

Jürgen Habermas of The Frankfurt School is one of the key critics of postmodernism.

Critical theory is focused on language, symbolism, communication, and social construction. Critical theory has been applied within the social sciences as a critique of social construction and postmodern society. 

From the 1960s and 1970s onward, language, symbolism, text, and meaning came to be seen as the theoretical foundation for the humanities, through the influence of Ludwig Wittgenstein, Ferdinand de Saussure, George Herbert Mead, Noam Chomsky, Hans-Georg Gadamer, Roland Barthes, Jacques Derrida and other thinkers in linguistic and analytic philosophy, structural linguistics, symbolic interactionism, hermeneutics, semiology, linguistically oriented psychoanalysis (Jacques Lacan, Alfred Lorenzer), and deconstruction.

When, in the 1970s and 1980s, Jürgen Habermas redefined critical social theory as a study of communication, i.e. communicative competence and communicative rationality on the one hand, distorted communication on the other, the two versions of critical theory began to overlap to a much greater degree than before.

Critical theorists have widely credited Paulo Freire for the first applications of critical theory towards education/pedagogy. They consider his best-known work, "Pedagogy of the Oppressed", a seminal text in what is now known as the philosophy and social movement of critical pedagogy. For a history of the emergence of critical theory in the field of education, see Isaac Gottesman (2016), "The Critical Turn in Education: From Marxist Critique to Postructuralist Feminism to Critical Theories of Race" (New York: Routledge).

While critical theorists have been frequently defined as Marxist intellectuals, their tendency to denounce some Marxist concepts and to combine Marxian analysis with other sociological and philosophical traditions has resulted in accusations of revisionism by classical, orthodox, and analytical Marxists, and by Marxist–Leninist philosophers. Martin Jay has stated that the first generation of critical theory is best understood as not promoting a specific philosophical agenda or a specific ideology, but as "a gadfly of other systems".

Critical theory has been criticized for not offering any clear road map to political action (praxis) following critique, often explicitly repudiating any solutions (such as with Herbert Marcuse's concept of "the Great Refusal", which promoted abstaining from engaging in active political change).








</doc>
<doc id="61233314" url="https://en.wikipedia.org/wiki?curid=61233314" title="Parergon">
Parergon

Parergon (, plural: parerga) is an ancient Greek philosophical concept defined as a supplementary issue. Parergon is also referred to as "embellishment" or extra. 

The literal meaning of the ancient Greek term is "beside, or additional to the work". According to Jacques Derrida, it is "summoned and assembled like a supplement because of the lack - a certain 'internal indetermination - in the very thing it enframes". It is added to a system to augment something lacking such as in the case of ergon (function, task or work), with parergon constituting an internal structural link that makes its unity possible. 
Parergon is viewed negatively, particularly within Greek classical thought, since it is against ergon or the true matter. Socrates used parergon to refer to the violation of the Athenian rule of "one man, one job", criticizing supplementary occupations that keep the citizens from specialization and works they are naturally fitted. His criticism also stemmed from the accusation that philosophy is a type of parergon. In the "Republic", he explained that "paideia" – the rearing and education of the ideal member of the polis or state – must not be considered equivalent to parergon. The emphasis stems from the perception that both connote a relationship to the essential function, expressed in a rigorous definition. The Socratic dialogue established that "paideia" does not indicate the possibility of changing the main issue and does not indicate marginality but these were identified as characteristics of parergon. 

Plato considered parergon as something that is secondary and that his philosophical discourse is often against it, explaining how it is against and beyond the ergon, conceptualized as the work accomplished. Parergon was included in Plato's work "Laws", where it was compared with the concepts of "paideia" and "deuteron". These two terms were both cited as superior in importance in the sense that they must be taken more seriously than parergon.

In Greek philosophy, however, parergon is not considered incidental. 

Immanuel Kant also used parergon in his philosophy. In his works, he associated it with ergon, which in his view is the "work" of one's field (e.g. work of art, work of literature, and work of music, etc.). According to Kant, parergon is what is beyond ergon. It is what columns are to buildings or the frame to a painting. He provided three examples of parergon: 1) clothing on a statue; 2) columns on a building; and, 3) the frame of a painting. He likened it to an ornament, one that primarily appeals to the senses. Kant's conceptualization influenced Derrida's usage of the term, particularly how it served as an agent of deconstruction using Kant's conceptualization of the painting's frame.

Parergon is also described as separate - that it is detached not only from the thing it enframes but also from the outside (the wall where a painting is hung or the space in which the object stands). This conceptualization underscores the significance of parergon for thinkers such as Derrida and Heidegger as it makes the split in the duality of intellect/senses. It plays an important rule in aesthetic judgment if it augments the pleasure of taste. It diminishes in value if it is not formally beautiful, lapsing as a simple adornment. According to Kant, this case is like a gilt frame of a painting, a mere attachment to gain approval through its charm and could even detract from the genuine beauty of the art.

Derrida cited parergon in his wider theory of deconstruction, using it with the term "supplement" to denote the relationship between the core and the periphery and reverse the order of priority so that it becomes possible for the supplement – the outside, secondary and inessential – to be the core or the centerpiece. In "The Truth in Painting", the philosopher likened parergon with the frame, borders, and marks of boundaries, which are capable of "unfixing" any stability so that conceptual oppositions are dismantled. It is, for the philosopher, "neither work (ergon) nor outside work", disconcerting any opposition while not remaining indeterminate. For Derrida, parergon is also fundamental, particularly to the ergon since, without it, it "cannot distinguish itself from itself". 

In artistic works, parergon is viewed as separate from an artwork it frames but merges with the milieu, which allows it to merge with the work of art. 

In a book, parergon can be the liminal devices that mediate it to its reader such as title, foreword, epigraph, preface, etc. It can also be a short literary piece added to the main volume such as the case of James Beattie's "The Castle of Scepticism." This is an allegory written as a parergon and was included in the philosopher's main work called Essay on truth, which criticized David Hume, Voltaire, and Thomas Hobbes. 


</doc>
<doc id="26700" url="https://en.wikipedia.org/wiki?curid=26700" title="Science">
Science

Science (from the Latin word "scientia", meaning "knowledge") is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.

The earliest roots of science can be traced to Ancient Egypt and Mesopotamia in around 3500 to 3000 BCE. Their contributions to mathematics, astronomy, and medicine entered and shaped Greek natural philosophy of classical antiquity, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes. After the fall of the Western Roman Empire, knowledge of Greek conceptions of the world deteriorated in Western Europe during the early centuries (400 to 1000 CE) of the Middle Ages but was preserved in the Muslim world during the Islamic Golden Age. The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th century revived "natural philosophy", which was later transformed by the Scientific Revolution that began in the 16th century as new ideas and discoveries departed from previous Greek conceptions and traditions. The scientific method soon played a greater role in knowledge creation and it was not until the 19th century that many of the institutional and professional features of science began to take shape; along with the changing of "natural philosophy" to "natural science."

Modern science is typically divided into three major branches that consist of the natural sciences (e.g., biology, chemistry, and physics), which study nature in the broadest sense; the social sciences (e.g., economics, psychology, and sociology), which study individuals and societies; and the formal sciences (e.g., logic, mathematics, and theoretical computer science), which study abstract concepts. There is disagreement, however, on whether the formal sciences actually constitute a science as they do not rely on empirical evidence. Disciplines that use existing scientific knowledge for practical purposes, such as engineering and medicine, are described as applied sciences.

Science is based on research, which is commonly conducted in academic and research institutions as well as in government agencies and companies. The practical impact of scientific research has led to the emergence of science policies that seek to influence the scientific enterprise by prioritizing the development of commercial products, armaments, health care, and environmental protection.

Science in a broad sense existed before the modern era and in many historical civilizations. Modern science is distinct in its approach and successful in its results, so it now defines what science is in the strictest sense of the term. Science in its original sense was a word for a type of knowledge, rather than a specialized word for the pursuit of such knowledge. In particular, it was the type of knowledge which people can communicate to each other and share. For example, knowledge about the working of natural things was gathered long before recorded history and led to the development of complex abstract thought. This is shown by the construction of complex calendars, techniques for making poisonous plants edible, public works at national scale, such as those which harnessed the floodplain of the Yangtse with reservoirs, dams, and dikes, and buildings such as the Pyramids. However, no consistent conscious distinction was made between knowledge of such things, which are true in every community, and other types of communal knowledge, such as mythologies and legal systems. Metallurgy was known in prehistory, and the Vinča culture was the earliest known producer of bronze-like alloys. It is thought that early experimentation with heating and mixing of substances over time developed into alchemy.

Neither the words nor the concepts "science" and "nature" were part of the conceptual landscape in the ancient near east. The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing; they also studied animal physiology, anatomy, and behavior for divinatory purposes and made extensive records of the movements of astronomical objects for their study of astrology. The Mesopotamians had intense interest in medicine and the earliest medical prescriptions appear in Sumerian during the Third Dynasty of Ur ( 2112 BCE – 2004 BCE). Nonetheless, the Mesopotamians seem to have had little interest in gathering information about the natural world for the mere sake of gathering information and mainly only studied scientific subjects which had obvious practical applications or immediate relevance to their religious system.

In classical antiquity, there is no real ancient analog of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time. Before the invention or discovery of the concept of "nature" (ancient Greek "phusis") by the Pre-Socratic philosophers, the same words tend to be used to describe the "natural" "way" in which a plant grows, and the "way" in which, for example, one tribe worships a particular god. For this reason, it is claimed these men were the first philosophers in the strict sense, and also the first people to clearly distinguish "nature" and "convention." Natural philosophy, the precursor of natural science, was thereby distinguished as the knowledge of nature and things which are true for every community, and the name of the specialized pursuit of such knowledge was "philosophy" – the realm of the first philosopher-physicists. They were mainly speculators or theorists, particularly interested in astronomy. In contrast, trying to use knowledge of nature to imitate nature (artifice or technology, Greek "technē") was seen by classical scientists as a more appropriate interest for artisans of lower social class.
The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural. The Pythagoreans developed a complex number philosophy and contributed significantly to the development of mathematical science. The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus. The Greek doctor Hippocrates established the tradition of systematic medical science and is known as "The Father of Medicine".

A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. This was a reaction to the Sophist emphasis on rhetoric. The Socratic method searches for general, commonly held truths that shape beliefs and scrutinizes them to determine their consistency with other beliefs. Socrates criticized the older type of study of physics as too purely speculative and lacking in self-criticism. Socrates was later, in the words of his "Apology", accused of corrupting the youth of Athens because he did "not believe in the gods the state believes in, but in other new spiritual beings". Socrates refuted these claims, but was sentenced to death.

Aristotle later created a systematic programme of teleological philosophy: Motion and change is described as the actualization of potentials already in things, according to what types of things they are. In his physics, the Sun goes around the Earth, and many things have it as part of their nature that they are for humans. Each thing has a formal cause, a final cause, and a role in a cosmic order with an unmoved mover. The Socratics also insisted that philosophy should be used to consider the practical question of the best way to live for a human being (a study Aristotle divided into ethics and political philosophy). Aristotle maintained that man knows a thing scientifically "when he possesses a conviction arrived at in a certain way, and when the first principles on which that conviction rests are known to him with certainty".

The Greek astronomer Aristarchus of Samos (310–230 BCE) was the first to propose a heliocentric model of the universe, with the Sun at the center and all the planets orbiting it. Aristarchus's model was widely rejected because it was believed to violate the laws of physics. The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus and has sometimes been credited as its inventor, although his proto-calculus lacked several defining features. Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopedia "Natural History", dealing with history, geography, medicine, astronomy, earth science, botany, and zoology.
Other scientists or proto-scientists in Antiquity were Theophrastus, Euclid, Herophilos, Hipparchus, Ptolemy, and Galen.

Because of the collapse of the Western Roman Empire due to the Migration Period an intellectual decline took place in the western part of Europe in the 400s. In contrast, the Byzantine Empire resisted the attacks from invaders, and preserved and improved upon the learning. John Philoponus, a Byzantine scholar in the 500s, questioned Aristotle's teaching of physics and to note its flaws. John Philoponus' criticism of Aristotelian principles of physics served as an inspiration to medieval scholars as well as to Galileo Galilei who ten centuries later, during the Scientific Revolution, extensively cited Philoponus in his works while making the case for why Aristotelian physics was flawed.

During late antiquity and the early Middle Ages, the Aristotelian approach to inquiries on natural phenomena was used. Aristotle's four causes prescribed that four "why" questions should be answered in order to explain things scientifically. Some ancient knowledge was lost, or in some cases kept in obscurity, during the fall of the Western Roman Empire and periodic political struggles. However, the general fields of science (or "natural philosophy" as it was called) and much of the general knowledge from the ancient world remained preserved through the works of the early Latin encyclopedists like Isidore of Seville. However, Aristotle's original texts were eventually lost in Western Europe, and only one text by Plato was widely known, the "Timaeus", which was the only Platonic dialogue, and one of the few original works of classical natural philosophy, available to Latin readers in the early Middle Ages. Another original work that gained influence in this period was Ptolemy's "Almagest", which contains a geocentric description of the solar system.

During late antiquity, in the Byzantine empire many Greek classical texts were preserved. Many Syriac translations were done by groups such as the Nestorians and Monophysites. They played a role when they translated Greek classical texts into Arabic under the Caliphate, during which many types of classical learning were preserved and in some cases improved upon. In addition, the neighboring Sassanid Empire established the medical Academy of Gondeshapur where Greek, Syriac and Persian physicians established the most important medical center of the ancient world during the 6th and 7th centuries.

The House of Wisdom was established in Abbasid-era Baghdad, Iraq,
where the Islamic study of Aristotelianism flourished. Al-Kindi (801–873) was the first of the Muslim Peripatetic philosophers, and is known for his efforts to introduce Greek and Hellenistic philosophy to the Arab world. The Islamic Golden Age flourished from this time until the Mongol invasions of the 13th century. Ibn al-Haytham (Alhazen), as well as his predecessor Ibn Sahl, was familiar with Ptolemy's "Optics", and used experiments as a means to gain knowledge. Alhazen disproved Ptolemy's theory of vision, but did not make any corresponding changes to Aristotle's metaphysics. Furthermore, doctors and alchemists such as the Persians Avicenna and Al-Razi also greatly developed the science of Medicine with the former writing the Canon of Medicine, a medical encyclopedia used until the 18th century and the latter discovering multiple compounds like alcohol. Avicenna's canon is considered to be one of the most important publications in medicine and they both contributed significantly to the practice of experimental medicine, using clinical trials and experiments to back their claims.

In Classical antiquity, Greek and Roman taboos had meant that dissection was usually banned in ancient times, but in Middle Ages it changed: medical teachers and students at Bologna began to open human bodies, and Mondino de Luzzi (c. 1275–1326) produced the ﬁrst known anatomy textbook based on human dissection.

By the eleventh century most of Europe had become Christian; stronger monarchies emerged; borders were restored; technological developments and agricultural innovations were made which increased the food supply and population. In addition, classical Greek texts started to be translated from Arabic and Greek into Latin, giving a higher level of scientific discussion in Western Europe.

By 1088, the first university in Europe (the University of Bologna) had emerged from its clerical beginnings. Demand for Latin translations grew (for example, from the Toledo School of Translators); western Europeans began collecting texts written not only in Latin, but also Latin translations from Greek, Arabic, and Hebrew. Manuscript copies of Alhazen's "Book of Optics" also propagated across Europe before 1240, as evidenced by its incorporation into Vitello's "Perspectiva". Avicenna's Canon was translated into Latin. In particular, the texts of Aristotle, Ptolemy, and Euclid, preserved in the Houses of Wisdom and also in the Byzantine Empire, were sought amongst Catholic scholars. The influx of ancient texts caused the Renaissance of the 12th century and the flourishing of a synthesis of Catholicism and Aristotelianism known as Scholasticism in western Europe, which became a new geographic center of science. An "experiment" in this period would be understood as a careful process of observing, describing, and classifying. One prominent scientist in this era was Roger Bacon. Scholasticism had a strong focus on revelation and dialectic reasoning, and gradually fell out of favour over the next centuries, as alchemy's focus on experiments that include direct observation and meticulous documentation slowly increased in importance.

New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. Before what we now know as the Renaissance started, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle. A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final.

In the sixteenth century, Copernicus formulated a heliocentric model of the solar system unlike the geocentric model of Ptolemy's "Almagest". This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model.

Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light. Kepler modelled the eye as a water-filled glass sphere with an aperture in front of it to model the entrance pupil. He found that all the light from a single point of the scene was imaged at a single point at the back of the glass sphere. The optical chain ends on the retina at the back of the eye. Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics, and described his work as a search for the Harmony of the Spheres.
Galileo made innovative use of experiment and mathematics. However, he became persecuted after Pope Urban VIII blessed Galileo to write about the Copernican system. Galileo had used arguments from the Pope and put them in the voice of the simpleton in the work "Dialogue Concerning the Two Chief World Systems", which greatly offended Urban VIII.

In Northern Europe, the new technology of the printing press was widely used to publish many arguments, including some that disagreed widely with contemporary ideas of nature. René Descartes and Francis Bacon published philosophical arguments in favor of a new type of non-Aristotelian science. Descartes emphasized individual thought and argued that mathematics rather than geometry should be used in order to study nature. Bacon emphasized the importance of experiment over contemplation. Bacon further questioned the Aristotelian concepts of formal cause and final cause, and promoted the idea that science should study the laws of "simple" natures, such as heat, rather than assuming that there is any specific nature, or "formal cause", of each complex type of thing. This new science began to see itself as describing "laws of nature". This updated approach to studies in nature was seen as mechanistic. Bacon also argued that science should aim for the first time at practical inventions for the improvement of all human life.

As a precursor to the Age of Enlightenment, Isaac Newton and Gottfried Wilhelm Leibniz succeeded in developing a new physics, now referred to as classical mechanics, which could be confirmed by experiment and explained using mathematics (Newton (1687), "Philosophiæ Naturalis Principia Mathematica"). Leibniz also incorporated terms from Aristotelian physics, but now being used in a new non-teleological way, for example, "energy" and "potential" (modern versions of Aristotelian ""energeia" and "potentia""). This implied a shift in the view of objects: Where Aristotle had noted that objects have certain innate goals that can be actualized, objects were now regarded as devoid of innate goals. In the style of Francis Bacon, Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes for each type of thing. It is during this period that the word "science" gradually became more commonly used to refer to a "type of pursuit" of a type of knowledge, especially knowledge of nature – coming close in meaning to the old term "natural philosophy."

During this time, the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, "the real and legitimate goal of sciences is the endowment of human life with new inventions and riches", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond "the fume of subtle, sublime, or pleasing speculation".

Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were also the backbone of the maturation of the scientific profession. Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the "Encyclopédie" and the popularization of Newtonianism by Voltaire as well as by Émilie du Châtelet, the French translator of Newton's "Principia".

Some historians have marked the 18th century as a drab period in the history of science; however, the century saw significant advancements in the practice of medicine, mathematics, and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry.

Enlightenment philosophers chose a short history of scientific predecessors – Galileo, Boyle, and Newton principally – as the guides and guarantors of their applications of the singular concept of nature and natural law to every physical and social field of the day. In this respect, the lessons of history and the social structures built upon it could be discarded.

The nineteenth century is a particularly important period in the history of science since during this era many distinguishing characteristics of contemporary modern science began to take shape such as: transformation of the life and physical sciences, frequent use of precision instruments, emergence of terms like "biologist", "physicist", "scientist"; slowly moving away from antiquated labels like "natural philosophy" and "natural history", increased professionalization of those studying nature lead to reduction in amateur naturalists, scientists gained cultural authority over many dimensions of society, economic expansion and industrialization of numerous countries, thriving of popular science writings and emergence of science journals.

Early in the 19th century, John Dalton suggested the modern atomic theory, based on Democritus's original idea of individible particles called "atoms".

Both John Herschel and William Whewell systematized methodology: the latter coined the term scientist. When Charles Darwin published "On the Origin of Species" he established evolution as the prevailing explanation of biological complexity. His theory of natural selection provided a natural explanation of how species originated, but this only gained wide acceptance a century later.

The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. With the advent of the steam engine and the industrial revolution, there was, however, an increased understanding that all forms of energy as defined in physics were not equally useful： they did not have the same energy quality. This realization led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time.

The electromagnetic theory was also established in the 19th century, and raised new questions which could not easily be answered using Newton's framework. The phenomena that would allow the deconstruction of the atom were discovered in the last decade of the 19th century: the discovery of X-rays inspired the discovery of radioactivity. In the next year came the discovery of the first subatomic particle, the electron.

Einstein's theory of relativity and the development of quantum mechanics led to the replacement of classical mechanics with a new physics which contains two parts that describe different types of events in nature.

In the first half of the century, the development of antibiotics and artificial fertilizer made global human population growth possible. At the same time, the structure of the atom and its nucleus was discovered, leading to the release of "atomic energy" (nuclear power). In addition, the extensive use of technological innovation stimulated by the wars of this century led to revolutions in transportation (automobiles and aircraft), the development of ICBMs, a space race, and a nuclear arms race.

The molecular structure of DNA was discovered in 1953. The discovery of the cosmic microwave background radiation in 1964 led to a rejection of the Steady State theory of the universe in favour of the Big Bang theory of Georges Lemaître.

The development of spaceflight in the second half of the century allowed the first astronomical measurements done on or near other objects in space, including manned landings on the Moon. Space telescopes lead to numerous discoveries in astronomy and cosmology.

Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematization of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling, which are partly based on the Aristotelian paradigm.

Harmful environmental issues such as ozone depletion, acidification, eutrophication and climate change came to the public's attention in the same period, and caused the onset of environmental science and environmental technology.

The Human Genome Project was completed in 2003, determining the sequence of nucleotide base pairs that make up human DNA, and identifying and mapping all of the genes of the human genome. Induced pluripotent stem cells were developed in 2006, a technology allowing adult cells to be transformed into stem cells capable of giving rise to any cell type found in the body, potentially of huge importance to the field of regenerative medicine.

With the discovery of the Higgs boson in 2012, the last particle predicted by the Standard Model of particle physics was found. In 2015, gravitational waves, predicted by general relativity a century before, were first observed.

Modern science is commonly divided into three major branches that consist of the natural sciences, social sciences, and formal sciences. Each of these branches comprise various specialized yet overlapping scientific disciplines that often possess their own nomenclature and expertise. Both natural and social sciences are empirical sciences as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions.

There are also closely related disciplines that use science, such as engineering and medicine, which are sometimes described as applied sciences. The relationships between the branches of science are summarized by the following table.
Natural science is concerned with the description, prediction, and understanding of natural phenomena based on empirical evidence from observation and experimentation. It can be divided into two main branches: life science (or biological science) and physical science. Physical science is subdivided into branches, including physics, chemistry, astronomy and earth science. These two branches may be further divided into more specialized disciplines. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, "natural history" suggests observational descriptions aimed at popular audiences.

Social science is concerned with society and the relationships among individuals within a society. It has many branches that include, but are not limited to, anthropology, archaeology, communication studies, economics, history, human geography, jurisprudence, linguistics, political science, psychology, public health, and sociology. Social scientists may adopt various philosophical theories to study individuals and society. For example, positivist social scientists use methods resembling those of the natural sciences as tools for understanding society, and so define science in its stricter modern sense. Interpretivist social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense. In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining both quantitative and qualitative research). The term "social research" has also acquired a degree of autonomy as practitioners from various disciplines share in its aims and methods.

Formal science is involved in the study of formal systems. It includes mathematics, systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts. The formal sciences are therefore "a priori" disciplines and because of this, there is disagreement on whether they actually constitute a science. Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics. Natural and social sciences that rely heavily on mathematical applications include mathematical physics, mathematical chemistry, mathematical biology, mathematical finance, and mathematical economics.

Scientific research can be labeled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Although some scientific research is applied research into specific problems, a great deal of our understanding comes from the curiosity-driven undertaking of basic research. This leads to options for technological advance that were not planned or sometimes even imaginable. This point was made by Michael Faraday when allegedly in response to the question "what is the "use" of basic research?" he responded: "Sir, what is the use of a new-born child?". For example, research into the effects of red light on the human eye's rod cells did not seem to have any practical purpose; eventually, the discovery that our night vision is not troubled by red light would lead search and rescue teams (among others) to adopt red light in the cockpits of jets and helicopters. Finally, even basic research can take unexpected turns, and there is some sense in which the scientific method is built to harness luck.

Scientific research involves using the scientific method, which seeks to objectively explain the events of nature in a reproducible way. An explanatory thought experiment or hypothesis is put forward as explanation using principles such as parsimony (also known as "Occam's Razor") and are generally expected to seek consilience – fitting well with other accepted facts related to the phenomena. This new explanation is used to make falsifiable predictions that are testable by experiment or observation. The predictions are to be posted before a confirming experiment or observation is sought, as proof that no tampering has occurred. Disproof of a prediction is evidence of progress. This is done partly through observation of natural phenomena, but also through experimentation that tries to simulate natural events under controlled conditions as appropriate to the discipline (in the observational sciences, such as astronomy or geology, a predicted observation might take the place of a controlled experiment). Experimentation is especially important in science to help establish causal relationships (to avoid the correlation fallacy).

When a hypothesis proves unsatisfactory, it is either modified or discarded. If the hypothesis survived testing, it may become adopted into the framework of a scientific theory, a logically reasoned, self-consistent model or framework for describing the behavior of certain natural phenomena. A theory typically describes the behavior of much broader sets of phenomena than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. In addition to testing hypotheses, scientists may also generate a model, an attempt to describe or depict the phenomenon in terms of a logical, physical or mathematical representation and to generate new hypotheses that can be tested, based on observable phenomena.

While performing experiments to test hypotheses, scientists may have a preference for one outcome over another, and so it is important to ensure that science as a whole can eliminate this bias. This can be achieved by careful experimental design, transparency, and a thorough peer review process of the experimental results as well as any conclusions. After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be. Taken in its entirety, the scientific method allows for highly creative problem solving while minimizing any effects of subjective bias on the part of its users (especially the confirmation bias).

John Ziman points out that intersubjective verifiability is fundamental to the creation of all scientific knowledge. Ziman shows how scientists can identify patterns to each other across centuries; he refers to this ability as "perceptual consensibility." He then makes consensibility, leading to consensus, the touchstone of reliable knowledge.

Mathematics is essential in the formation of hypotheses, theories, and laws in the natural and social sciences. For example, it is used in quantitative scientific modeling, which can generate new hypotheses and predictions to be tested. It is also used extensively in observing and collecting measurements. Statistics, a branch of mathematics, is used to summarize and analyze data, which allow scientists to assess the reliability and variability of their experimental results.

Computational science applies computing power to simulate real-world situations, enabling a better understanding of scientific problems than formal mathematics alone can achieve. According to the Society for Industrial and Applied Mathematics, computation is now as important as theory and experiment in advancing scientific knowledge.

Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: (1) that there is an objective reality shared by all rational observers; (2) that this objective reality is governed by natural laws; (3) that these laws can be discovered by means of systematic observation and experimentation. Philosophy of science seeks a deep understanding of what these underlying assumptions mean and whether they are valid.

The belief that scientific theories should and do represent metaphysical reality is known as realism. It can be contrasted with anti-realism, the view that the success of science does not depend on it being accurate about unobservable entities such as electrons. One form of anti-realism is idealism, the belief that the mind or consciousness is the most basic essence, and that each mind generates its own reality. In an idealistic world view, what is true for one mind need not be true for other minds.

There are different schools of thought in philosophy of science. The most popular position is empiricism, which holds that knowledge is created by a process involving observation and that scientific theories are the result of generalizations from such observations. Empiricism generally encompasses inductivism, a position that tries to explain the way general theories can be justified by the finite number of observations humans can make and hence the finite amount of empirical evidence available to confirm scientific theories. This is necessary because the number of predictions those theories make is infinite, which means that they cannot be known from the finite amount of evidence using deductive logic only. Many versions of empiricism exist, with the predominant ones being Bayesianism and the hypothetico-deductive method.
Empiricism has stood in contrast to rationalism, the position originally associated with Descartes, which holds that knowledge is created by the human intellect, not by observation. Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper. Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories and that the only way a theory can be affected by observation is when it comes in conflict with it. Popper proposed replacing verifiability with falsifiability as the landmark of scientific theories and replacing induction with falsification as the empirical method. Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error. It covers all products of the human mind, including science, mathematics, philosophy, and art.

Another approach, instrumentalism, colloquially termed "shut up and multiply," emphasizes the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be something that should simply be ignored and that scientists shouldn't make a fuss about (see interpretations of quantum mechanics). Close to instrumentalism is constructive empiricism, according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true.

Thomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent "portrait" of the world that is consistent with observations made from its framing. He characterized "normal science" as the process of observation and "puzzle solving" which takes place within a paradigm, whereas "revolutionary science" occurs when one paradigm overtakes another in a paradigm shift. Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more "portraits" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of relativism.

Finally, another approach often cited in debates of scientific skepticism against controversial movements like "creation science" is methodological naturalism. Its main point is that a difference between natural and supernatural explanations should be made and that science should be restricted methodologically to natural explanations. That the restriction is merely methodological (rather than ontological) means that science should not consider supernatural explanations itself, but should not claim them to be wrong either. Instead, supernatural explanations should be left a matter of personal belief outside the scope of science. Methodological naturalism maintains that proper science requires strict adherence to empirical study and independent verification as a process for properly developing and evaluating explanations for observable phenomena. The absence of these standards, arguments from authority, biased observational studies and other common fallacies are frequently cited by supporters of methodological naturalism as characteristic of the non-science they criticize.

A scientific theory is empirical and is always open to falsification if new evidence is presented. That is, no theory is ever considered strictly certain as science accepts the concept of fallibilism. The philosopher of science Karl Popper sharply distinguished truth from certainty. He wrote that scientific knowledge "consists in the search for truth," but it "is not the search for certainty ... All human knowledge is fallible and therefore uncertain.

New scientific knowledge rarely results in vast changes in our understanding. According to psychologist Keith Stanovich, it may be the media's overuse of words like "breakthrough" that leads the public to imagine that science is constantly proving everything it thought was true to be false. While there are such famous cases as the theory of relativity that required a complete reconceptualization, these are extreme exceptions. Knowledge in science is gained by a gradual synthesis of information from different experiments by various researchers across different branches of science; it is more like a climb than a leap. Theories vary in the extent to which they have been tested and verified, as well as their acceptance in the scientific community. For example, heliocentric theory, the theory of evolution, relativity theory, and germ theory still bear the name "theory" even though, in practice, they are considered factual.
Philosopher Barry Stroud adds that, although the best definition for "knowledge" is contested, being skeptical and entertaining the "possibility" that one is incorrect is compatible with being correct. Therefore, scientists adhering to proper scientific approaches will doubt themselves even once they possess the truth. The fallibilist C. S. Peirce argued that inquiry is the struggle to resolve actual doubt and that merely quarrelsome, verbal, or hyperbolic doubt is fruitless – but also that the inquirer should try to attain genuine doubt rather than resting uncritically on common sense. He held that the successful sciences trust not to any single chain of inference (no stronger than its weakest link) but to the cable of multiple and various arguments intimately connected.

Stanovich also asserts that science avoids searching for a "magic bullet"; it avoids the single-cause fallacy. This means a scientist would not ask merely "What is "the" cause of ...", but rather "What "are" the most significant "causes" of ...". This is especially the case in the more macroscopic fields of science (e.g. psychology, physical cosmology). Research often analyzes few factors at once, but these are always added to the long list of factors that are most important to consider. For example, knowing the details of only a person's genetics, or their history and upbringing, or the current situation may not explain a behavior, but a deep understanding of all these variables combined can be very predictive.

Scientific research is published in an enormous range of scientific literature. Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, "Journal des Sçavans" followed by the "Philosophical Transactions", began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500. The United States National Library of Medicine currently indexes 5,516 journals that contain articles on topics related to the life sciences. Although the journals are in 39 languages, 91 percent of the indexed articles are published in English.

Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper. Science has become so pervasive in modern societies that it is generally considered necessary to communicate the achievements, news, and ambitions of scientists to a wider populace.

Science magazines such as "New Scientist", "Science & Vie", and "Scientific American" cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research. Science books engage the interest of many more people. Tangentially, the science fiction genre, primarily fantastic in nature, engages the public imagination and transmits the ideas, if not the methods, of science.

Recent efforts to intensify or develop links between science and non-scientific disciplines such as literature or more specifically, poetry, include the "Creative Writing Science" resource developed through the Royal Literary Fund.

Discoveries in fundamental science can be world-changing. For example:

The replication crisis is an ongoing methodological crisis primarily affecting parts of the social and life sciences in which scholars have found that the results of many scientific studies are difficult or impossible to replicate or reproduce on subsequent investigation, either by independent researchers or by the original researchers themselves. The crisis has long-standing roots; the phrase was coined in the early 2010s as part of a growing awareness of the problem. The replication crisis represents an important body of research in metascience, which aims to improve the quality of all scientific research while reducing waste.

An area of study or speculation that masquerades as science in an attempt to claim a legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience, fringe science, or junk science. Physicist Richard Feynman coined the term "cargo cult science" for cases in which researchers believe they are doing science because their activities have the outward appearance of science but actually lack the "kind of utter honesty" that allows their results to be rigorously evaluated. Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as "the most important tool" for separating valid claims from invalid ones.

There can also be an element of political or ideological bias on all sides of scientific debates. Sometimes, research may be characterized as "bad science," research that may be well-intended but is actually incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term "scientific misconduct" refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person.

The scientific community is a group of all interacting scientists, along with their respective societies and institutions.

Scientists are individuals who conduct scientific research to advance knowledge in an area of interest. The term "scientist" was coined by William Whewell in 1833. In modern times, many professional scientists are trained in an academic setting and upon completion, attain an academic degree, with the highest degree being a doctorate such as a Doctor of Philosophy (PhD). Many scientists pursue careers in various sectors of the economy such as academia, industry, government, and nonprofit organizations.

Scientists exhibit a strong curiosity about reality, with some scientists having a desire to apply scientific knowledge for the benefit of health, nations, environment, or industries. Other motivations include recognition by their peers and prestige. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, chemistry, and economics.

Science has historically been a male-dominated field, with some notable exceptions. Women faced considerable discrimination in science, much as they did in other areas of male-dominated societies, such as frequently being passed over for job opportunities and denied credit for their work. For example, Christine Ladd (1847–1930) was able to enter a PhD program as "C. Ladd"; Christine "Kitty" Ladd completed the requirements in 1882, but was awarded her degree only in 1926, after a career which spanned the algebra of logic (see truth table), color vision, and psychology. Her work preceded notable researchers like Ludwig Wittgenstein and Charles Sanders Peirce. The achievements of women in science have been attributed to their defiance of their traditional role as laborers within the domestic sphere.

In the late 20th century, active recruitment of women and elimination of institutional discrimination on the basis of sex greatly increased the number of women scientists, but large gender disparities remain in some fields; in the early 21st century over half of new biologists were female, while 80% of PhDs in physics are given to men. In the early part of the 21st century, women in the United States earned 50.3% of bachelor's degrees, 45.6% of master's degrees, and 40.7% of PhDs in science and engineering fields. They earned more than half of the degrees in psychology (about 70%), social sciences (about 50%), and biology (about 50-60%) but earned less than half the degrees in the physical sciences, earth sciences, mathematics, engineering, and computer science. Lifestyle choice also plays a major role in female engagement in science; women with young children are 28% less likely to take tenure-track positions due to work-life balance issues, and female graduate students' interest in careers in research declines dramatically over the course of graduate school, whereas that of their male colleagues remains unchanged.

Learned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance. Many scientists belong to a learned society that promotes their respective scientific discipline, profession, or group of related disciplines. Membership may be open to all, may require possession of some scientific credentials, or may be an honor conferred by election. Most scientific societies are non-profit organizations, and many are professional associations. Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some also act as professional bodies, regulating the activities of their members in the public interest or the collective interest of the membership. Scholars in the sociology of science argue that learned societies are of key importance and their formation assists in the emergence and development of new disciplines or professions.

The professionalization of science, begun in the 19th century, was partly enabled by the creation of distinguished academy of sciences in a number of countries such as the Italian in 1603, the British Royal Society in 1660, the French in 1666, the American National Academy of Sciences in 1863, the German Kaiser Wilhelm Institute in 1911, and the Chinese Academy of Sciences in 1928. International scientific organizations, such as the International Council for Science, have since been formed to promote cooperation between the scientific communities of different nations.

Science policy is an area of public policy concerned with the policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care and environmental monitoring. Science policy also refers to the act of applying scientific knowledge and consensus to the development of public policies. Science policy thus deals with the entire domain of issues that involve the natural sciences. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public.

State policy has influenced the funding of public works and science for thousands of years, particularly within civilizations with highly organized governments such as imperial China and the Roman Empire. Prominent historical examples include the Great Wall of China, completed over the course of two millennia through the state support of several dynasties, and the Grand Canal of the Yangtze River, an immense feat of hydraulic engineering begun by Sunshu Ao (孫叔敖 7th c. BCE), Ximen Bao (西門豹 5th c.BCE), and Shi Chi (4th c. BCE). This construction dates from the 6th century BCE under the Sui Dynasty and is still in use today. In China, such state-supported infrastructure and scientific research projects date at least from the time of the Mohists, who inspired the study of logic during the period of the Hundred Schools of Thought and the study of defensive fortifications like the Great Wall of China during the Warring States period.

Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organizations that fund research. Vannevar Bush, director of the Office of Scientific Research and Development for the United States government, the forerunner of the National Science Foundation, wrote in July 1945 that "Science is a proper concern of government."

Scientific research is often funded through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP. In the OECD, around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10% respectively by universities and government. The government funding proportion in certain industries is higher, and it dominates research in social science and humanities. Similarly, with some exceptions (e.g. biotechnology) government provides the bulk of the funds for basic scientific research. Many governments have dedicated agencies to support scientific research. Prominent scientific organizations include the National Science Foundation in the United States, the National Scientific and Technical Research Council in Argentina, Commonwealth Scientific and Industrial Research Organisation (CSIRO) in Australia, in France, the Max Planck Society and in Germany, and CSIC in Spain. In commercial research and development, all but the most research-oriented corporations focus more heavily on near-term commercialisation possibilities rather than "blue-sky" ideas or technologies (such as nuclear fusion).

The public awareness of science relates to the attitudes, behaviors, opinions, and activities that make up the relations between science and the general public. it integrates various themes and activities such as science communication, science museums, science festivals, science fairs, citizen science, and science in popular culture. Social scientists have devised various metrics to measure the public understanding of science such as factual knowledge, self-reported knowledge, and structural knowledge.

The mass media face a number of pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter. Few journalists have real scientific knowledge, and even beat reporters who know a great deal about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover.

Politicization of science occurs when government, business, or advocacy groups use legal or economic pressure to influence the findings of scientific research or the way it is disseminated, reported, or interpreted. Many factors can act as facets of the politicization of science such as populist anti-intellectualism, perceived threats to religious beliefs, postmodernist subjectivism, and fear for business interests. Politicization of science is usually accomplished when scientific information is presented in a way that emphasizes the uncertainty associated with the scientific evidence. Tactics such as shifting conversation, failing to acknowledge facts, and capitalizing on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence. Examples of issues that have involved the politicization of science include the global warming controversy, health effects of pesticides, and health effects of tobacco.


Publications

Resources


</doc>
<doc id="9145213" url="https://en.wikipedia.org/wiki?curid=9145213" title="Outline of science">
Outline of science

The following outline is provided as a topical overview of science:

Science is both the systematic effort of acquiring knowledge through observation, experimentation and reasoning, and the body of knowledge thus acquired. The word "science" comes from the Latin word "scientia" meaning knowledge. A practitioner of science is called a "scientist". Modern science respects objective logical reasoning, and follows a set of core procedures or rules in order to determine the nature and underlying natural laws of the universe and everything in it. Some scientists do not know of the rules themselves, but follow them through research policies. These procedures are known as the scientific method.


Scientific method   (outline) – body of techniques for investigating phenomena and acquiring new knowledge, as well as for correcting and integrating previous knowledge. It is based on observable, empirical, measurable evidence, and subject to laws of reasoning, both deductive and inductive.

Branches of science – divisions within science with respect to the entity or system concerned, which typically embodies its own terminology and nomenclature.

Formal science – branches of knowledge that are concerned with formal systems, such as those under the branches of: logic, mathematics, computer science, statistics, and some aspects of linguistics. Unlike other sciences, the formal sciences are not concerned with the validity of theories based on observations in the real world, but instead with the properties of formal systems based on definitions and rules.

Natural science   (outline) – major branch of science, that tries to explain and predict nature's phenomena, based on empirical evidence. In natural science, hypotheses must be verified scientifically to be regarded as scientific theory. Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: biology, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Social science – study of the social world constructed between humans. The social sciences usually limit themselves to an anthropomorphically centric view of these interactions with minimal emphasis on the inadvertent impact of social human behavior on the external environment (physical, biological, ecological, etc.). 'Social' is the concept of exchange/influence of ideas, thoughts, and relationship interactions (resulting in harmony, peace, self enrichment, favoritism, maliciousness, justice seeking, etc.) between humans. The scientific method is utilized in many social sciences, albeit adapted to the needs of the social construct being studied.

Applied science – branch of science that applies existing scientific knowledge to develop more practical applications, including inventions and other technological advancements.







See – 






The scientific fields mentioned below are generally described by the science they study.




Science education




</doc>
<doc id="44312" url="https://en.wikipedia.org/wiki?curid=44312" title="Invention">
Invention

An invention is a unique or novel device, method, composition or process. The invention process is a process within an overall engineering and product development process. It may be an improvement upon a machine or product or a new process for creating an object or a result. An invention that achieves a completely unique function or result may be a radical breakthrough. Such works are novel and not obvious to others skilled in the same field. An inventor may be taking a big step toward success or failure.

Some inventions can be patented. A patent legally protects the intellectual property rights of the inventor and legally recognizes that a claimed invention is actually an invention. The rules and requirements for patenting an invention vary by country and the process of obtaining a patent is often expensive.

Another meaning of invention is cultural invention, which is an innovative set of useful social behaviours adopted by people and passed on to others. The Institute for Social Inventions collected many such ideas in magazines and books. Invention is also an important component of artistic and design creativity. Inventions often extend the boundaries of human knowledge, experience or capability.

Inventions are of three kinds: scientific-technological (including medicine), sociopolitical (including economics and law), and humanistic, or cultural.

Scientific-technological inventions include railroads, aviation, vaccination, hybridization, antibiotics, astronautics, holography, the atomic bomb, computing, the Internet, and the smartphone.

Sociopolitical inventions comprise new laws, institutions, and procedures that change modes of social behavior and establish new forms of human interaction and organization. Examples include the British Parliament, the US Constitution, the Manchester (UK) General Union of Trades, the Boy Scouts, the Red Cross, the Olympic Games, the United Nations, the European Union, and the Universal Declaration of Human Rights, as well as movements such as socialism, Zionism, suffragism, feminism, and animal-rights veganism.

Humanistic inventions encompass culture in its entirety and are as transformative and important as any in the sciences, although people tend to take them for granted. In the domain of linguistics, for example, many alphabets have been inventions, as are all neologisms (Shakespeare invented about 1,700 words). Literary inventions include the epic, tragedy, comedy, the novel, the sonnet, the Renaissance, neoclassicism, Romanticism, Symbolism, Aestheticism, Socialist Realism, Surrealism, postmodernism, and (according to Freud) psychoanalysis. Among the inventions of artists and musicians are oil painting, printmaking, photography, cinema, musical tonality, atonality, jazz, rock, opera, and the symphony orchestra. Philosophers have invented logic (several times), dialectics, idealism, materialism, utopia, anarchism, semiotics, phenomenology, behaviorism, positivism, pragmatism, and deconstruction. Religious thinkers are responsible for such inventions as monotheism, pantheism, Methodism, Mormonism, iconoclasm, puritanism, deism, secularism, ecumenism, and Baha’i. Some of these disciplines, genres, and trends may seem to have existed eternally or to have emerged spontaneously of their own accord, but most of them have had inventors.
Ideas for an invention may be developed on paper or on a computer, by writing or drawing, by trial and error, by making models, by experimenting, by testing and/or by making the invention in its whole form. Brainstorming also can spark new ideas for an invention. Collaborative creative processes are frequently used by engineers, designers, architects and scientists. Co-inventors are frequently named on patents.

In addition, many inventors keep records of their working process - notebooks, photos, etc., including Leonardo da Vinci, Galileo Galilei, Evangelista Torricelli, Thomas Jefferson and Albert Einstein.

In the process of developing an invention, the initial idea may change. The invention may become simpler, more practical, it may expand, or it may even "morph" into something totally different. Working on one invention can lead to others too.

History shows that turning the concept of an invention into a working device is not always swift or direct. Inventions may also become more useful after time passes and other changes occur. For example, the parachute became more useful once powered flight was a reality.

Invention is often a creative process. An open and curious mind allows an inventor to see beyond what is known. Seeing a new possibility, connection or relationship can spark an invention. Inventive thinking frequently involves combining concepts or elements from different realms that would not normally be put together. Sometimes inventors disregard the boundaries between distinctly separate territories or fields. Several concepts may be considered when thinking about invention.

Play may lead to invention. Childhood curiosity, experimentation, and imagination can develop one's play instinct. Inventors feel the need to play with things that interest them, and to explore, and this internal drive brings about novel creations.

Sometimes inventions and ideas may seem to arise spontaneously while daydreaming, especially when the mind is free from its usual concerns. For example, both J. K. Rowling (the creator of Harry Potter) and Frank Hornby (the inventor of Meccano) first had their ideas while on train journeys.

In contrast, the successful aerospace engineer Max Munk advocated "aimful thinking".

To invent is to see anew. Inventors often envision a new idea, seeing it in their mind's eye. New ideas can arise when the conscious mind turns away from the subject or problem when the inventor's focus is on something else, or while relaxing or sleeping. A novel idea may come in a flash—a Eureka! moment. For example, after years of working to figure out the general theory of relativity, the solution came to Einstein suddenly in a dream "like a giant die making an indelible impress, a huge map of the universe outlined itself in one clear vision". Inventions can also be accidental, such as in the case of polytetrafluoroethylene (Teflon).

Insight can also be a vital element of invention. Such inventive insight may begin with questions, doubt or a hunch. It may begin by recognizing that something unusual or accidental may be useful or that it could open a new avenue for exploration. For example, the odd metallic color of plastic made by accidentally adding a thousand times too much catalyst led scientists to explore its metal-like properties, inventing electrically conductive plastic and light emitting plastic-—an invention that won the Nobel Prize in 2000 and has led to innovative lighting, display screens, wallpaper and much more (see conductive polymer, and organic light-emitting diode or OLED).

Invention is often an exploratory process with an uncertain or unknown outcome. There are failures as well as successes. Inspiration can start the process, but no matter how complete the initial idea, inventions typically must be developed.

Inventors may, for example, try to improve something by making it more effective, healthier, faster, more efficient, easier to use, serve more purposes, longer lasting, cheaper, more ecologically friendly, or aesthetically different, lighter weight, more ergonomic, structurally different, with new light or color properties, etc.

In economic theory, inventions are one of the chief examples of "positive externalities", a beneficial side effect that falls on those outside a transaction or activity. One of the central concepts of economics is that externalities should be internalized—unless some of the benefits of this positive externality can be captured by the parties, the parties are under-rewarded for their inventions, and systematic under-rewarding leads to under-investment in activities that lead to inventions. The patent system captures those positive externalities for the inventor or other patent owner so that the economy as a whole invests an optimum amount of resources in the invention process.

In the social sciences, an innovation is something that is new and better, and has been adopted and proven to create positive value. This is a key distinction from an invention which may not create positive value but furthers progress in a given area of development. The theory for adoption of an innovation, called "diffusion of innovations", considers the likelihood that an innovation is adopted and the taxonomy of persons likely to adopt it or spur its adoption. This theory was first put forth by Everett Rogers. Gabriel Tarde also dealt with the adoption of innovations in his "Laws of Imitation".

An invention can serve many purposes, and does not necessarily create positive value. These purposes might differ significantly and may change over time. An invention or its development may serve purposes never envisioned by its inventors. Plastic is a good example.

The term "invention" is also an important legal concept and central to patent law systems worldwide. As is often the case for legal concepts, its legal meaning is slightly different from common usage of the word. Additionally, the "legal" concept of invention is quite different in American and European patent law.

In Europe, the first test a patent application must pass is, "Is this an invention?" If it is, subsequent questions are whether it is new and sufficiently inventive. The implication—counter-intuitively—is that a legal invention is not inherently novel. Whether a patent application relates to an invention is governed by Article 52 of the European Patent Convention, that excludes, e.g., discoveries "as such" and software "as such". The EPO Boards of Appeal decided that the technical character of an application is decisive for it to represent an invention, following an age-old Italian and German tradition. British courts don't agree with this interpretation. Following a 1959 Australian decision ("NRDC"), they believe that it is not possible to grasp the invention concept in a single rule. A British court once stated that the technical character test implies a "restatement of the problem in more imprecise terminology."

In the United States, all patent applications are considered inventions. The statute explicitly says that the American invention concept includes discoveries (35 USC § 100(a)), contrary to the European invention concept. The European invention concept corresponds to the American "patentable subject matter" concept: the first test a patent application is submitted to. While the statute (35 USC § 101) virtually poses no limits to patenting whatsoever, courts have decided in binding precedents that abstract ideas, natural phenomena and laws of nature are not patentable. Various attempts have been made to substantiate the "abstract idea" test, which suffers from abstractness itself, but none have succeeded. The last attempt so far was the "machine or transformation" test, but the U.S. Supreme Court decided in 2010 that it is merely an indication at best.

In India, invention means a new product or process that involves an inventive step, and capable of being made or used in an industry. Whereas, "new invention" means any invention that has not been anticipated in any prior art or used in the country or any where in the world.

Invention has a long and important history in the arts. Inventive thinking has always played a vital role in the creative process. While some inventions in the arts are patentable, others are not because they cannot fulfill the strict requirements governments have established for granting them. (see patent).

Some inventions in art include the:


Likewise, Jackson Pollock invented an entirely new form of painting and a new kind of abstraction by dripping, pouring, splashing and splattering paint onto un-stretched canvas lying on the floor.

Inventive tools of the artist's trade also produced advances in creativity. Impressionist painting became possible because of newly invented collapsible, resealable metal paint tubes that facilitated spontaneous painting outdoors. Inventions originally created in the form of artwork can also develop other uses, e.g. Alexander Calder's mobile, which is now commonly used over babies' cribs. Funds generated from patents on inventions in art, design and architecture can support the realization of the invention or other creative work. Frédéric Auguste Bartholdi's 1879 design patent on the Statue of Liberty helped fund the famous statue because it covered small replicas, including those sold as souvenirs.

The timeline for invention in the arts lists the most notable artistic inventors.




</doc>
<doc id="11983318" url="https://en.wikipedia.org/wiki?curid=11983318" title="Branches of science">
Branches of science

The branches of science, also referred to as sciences, "scientific fields", or "scientific disciplines," are commonly divided into three major groups:


Natural and social sciences are empirical sciences, meaning that the knowledge must be based on observable phenomena and must be capable of being verified by other researchers working under the same conditions. This verifiability may well vary even "within" a scientific discipline

Natural, social, and formal science make up the fundamental sciences, which form the basis of interdisciplinary and applied sciences such as engineering and medicine. Specialized scientific disciplines that exist in multiple categories may include parts of other scientific disciplines but often possess their own terminologies and expertises.

The "formal sciences" are the branches of science that are concerned with formal systems, such as logic, mathematics, theoretical computer science, information theory, systems theory, decision theory, statistics, and theoretical linguistics.

Unlike other branches, the formal sciences are not concerned with the validity of theories based on observations in the real world (empirical knowledge), but rather with the properties of formal systems based on definitions and rules. Methods of the formal sciences are, however, essential to the construction and testing of scientific models dealing with observable reality, and major advances in formal sciences have often enabled major advances in the empirical sciences.

"Decision theory" in economics, psychology, philosophy, mathematics, and statistics is concerned with identifying the values, uncertainties and other issues relevant in a given decision, its rationality, and the resulting optimal decision. It is very closely related to the field of game theory.

"Logic" (from the Greek "λογική" logikē) is the formal systematic study of the principles of valid inference and correct reasoning. Logic is used in most intellectual activities, but is studied primarily in the disciplines of philosophy, mathematics, semantics, and computer science. Logic examines general forms which arguments may take, which forms are valid, and which are fallacies. In philosophy, the study of logic figures in most major areas: epistemology, ethics, metaphysics. In mathematics and computer science, it is the study of valid inferences within some formal language.
Logic is also studied in argumentation theory.

"Mathematics", first of all, known as The Science of numbers which are classified in Arithmetic and Algebra, is classified as a formal science, has both similarities and differences with the empirical sciences (the natural and social sciences). It is similar to empirical sciences in that it involves an objective, careful and systematic study of an area of knowledge; it is different because of its method of verifying its knowledge, using "a priori" rather than empirical methods.

"Statistics" is the study of the collection, organization, and interpretation of data. It deals with all aspects of this, including the planning of data collection in terms of the design of surveys and experiments.

A statistician is someone who is particularly well versed in the ways of thinking necessary for the successful application of statistical analysis. Such people have often gained this experience through working in any of a wide number of fields. There is also a discipline called "mathematical statistics", which is concerned with the theoretical basis of the subject.

The word "statistics", when referring to the scientific discipline, is singular, as in "Statistics is an art." This should not be confused with the word "statistic", referring to a quantity (such as mean or median) calculated from a set of data, whose plural is "statistics" ("this statistic seems wrong" or "these statistics are misleading").

"Systems theory" is the transdisciplinary study of systems in general, to elucidate principles that can be applied to all types of systems in all fields of research. The term does not yet have a well-established, precise meaning, but systems theory can reasonably be considered a specialization of systems thinking and a generalization of systems science. The term originates from Bertalanffy's General System Theory (GST) and is used in later efforts in other fields, such as the action theory of Talcott Parsons and the system-theory of Niklas Luhmann.

In this context the word "systems" is used to refer specifically to self-regulating systems, i.e. that are self-correcting through feedback. Self-regulating systems are found in nature, including the physiological systems of our body, in local and global ecosystems, and climate.

"Theoretical computer science" (TCS) is a division or subset of general computer science and focuses on more abstract or mathematical aspects of computing.

These divisions and subsets include analysis of algorithms and formal semantics of programming languages. Technically, there are hundreds of divisions and subsets besides these two. Each of the multiple parts has its leaders (of popularity) and there are many associations and professional social groups and publications of distinction.

Natural science is a branch of science that seeks to elucidate the rules that govern the natural world by applying an empirical and scientific method to the study of the universe. The term natural sciences is used to distinguish it from the social sciences, which apply the scientific method to study human behavior and social patterns; the humanities, which use a critical, or analytical approach to the study of the human condition; and the formal sciences.

"Physical science" is an encompassing term for the branches of natural science and science that study non-living systems, in contrast to the life sciences. However, the term "physical" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena. There is a difference between physical science and physics.

"Physics" (from ) is a natural science that involves the study of matter and its motion through spacetime, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.

Physics is one of the oldest academic disciplines, perhaps the oldest through its inclusion of astronomy. Over the last two millennia, physics was a part of natural philosophy along with chemistry, certain branches of mathematics, and biology, but during the Scientific Revolution in the 16th century, the natural sciences emerged as unique research programs in their own right. Certain research areas are interdisciplinary, such as biophysics and quantum chemistry, which means that the boundaries of physics are not rigidly defined. In the nineteenth and twentieth centuries physicalism emerged as a major unifying feature of the philosophy of science as physics provides fundamental explanations for every observed natural phenomenon. New ideas in physics often explain the fundamental mechanisms of other sciences, while opening to new research areas in mathematics and philosophy.

"Chemistry" (the etymology of the word has been much disputed) is the science of matter and the changes it undergoes. The science of matter is also addressed by physics, but while physics takes a more general and fundamental approach, chemistry is more specialized, being concerned by the composition, behavior (or reaction), structure, and properties of matter, as well as the changes it undergoes during chemical reactions. It is a physical science which studies various substances, atoms, molecules, and matter (especially carbon based); biochemistry, the study of substances found in biological organisms; physical chemistry, the study of chemical processes using physical concepts such as thermodynamics and quantum mechanics; and analytical chemistry, the analysis of material samples to gain an understanding of their chemical composition and structure. Many more specialized disciplines have emerged in recent years, e.g. neurochemistry the chemical study of the nervous system.

"Earth science" (also known as "geoscience", "the geosciences" or "the Earth sciences") is an all-embracing term for the sciences related to the planet Earth. It is arguably a special case in planetary science, the Earth being the only known life-bearing planet. There are both reductionist and holistic approaches to Earth sciences. The formal discipline of Earth sciences may include the study of the atmosphere, hydrosphere, oceans, and biosphere, as well as the solid earth. Typically Earth scientists will use tools from physics, chemistry, biology, geography, chronology and mathematics to build a quantitative understanding of how the Earth system works, and how it evolved to its current state.

Ecology (from Greek: οἶκος, "house"; -λογία, "study of") is the scientific study of the relationships that living organisms have with each other and with their abiotic environment. Topics of interest to ecologists include the composition, distribution, amount (biomass), number, and changing states of organisms within and among ecosystems.

Oceanography, or marine biology, is the branch of Earth science that studies the ocean. It covers a wide range of topics, including marine organisms and ecosystem dynamics; ocean currents, waves, and geophysical fluid dynamics; plate tectonics and the geology of the seafloor; and fluxes of various chemical substances and physical properties within the ocean and across its boundaries. These diverse topics reflect multiple disciplines that oceanographers blend to further knowledge of the world ocean and understanding of processes within it: biology, chemistry, geology, meteorology, and physics as well as geography.

Geology (from the Greek γῆ, gê, "earth" and λόγος, logos, "study") is the science comprising the study of solid Earth, the rocks of which it is composed, and the processes by which they change.

Meteorology is the interdisciplinary scientific study of the atmosphere. Studies in the field stretch back millennia, though significant progress in meteorology did not occur until the 17th century. The 19th century saw breakthroughs occur after observing networks developed across several countries. After the development of the computer in the latter half of the 20th century, breakthroughs in weather forecasting were achieved.

Space science or Astronomy is the study of everything in outer space. This has sometimes been called astronomy, but recently astronomy has come to be regarded as a division of broader space science, which has grown to include other related fields, such as studying issues related to space travel and space exploration (including space medicine), space archaeology and science performed in outer space (see space research).

The science of living things comprises the branches of science that involve the scientific study of living organisms, like plants, animals, and human beings. However, the study of behavior of organisms, such as practiced in ethology and psychology, is only included in as much as it involves a biological aspect. While biology remains the centerpiece of the science of living things, technological advances in molecular biology and biotechnology have led to a burgeoning of specializations and new, often interdisciplinary, fields.

"Biology" is the branch of natural science concerned with the study of life and living organisms, including their structure, function, growth, origin, evolution, distribution, and taxonomy. Biology is a vast subject containing many subdivisions, topics, and disciplines.

Zoology, occasionally spelled zoölogy, is the branch of science that relates to the animal kingdom, including the structure, embryology, evolution, classification, habits, and distribution of all animals, both living and extinct. The term is derived from Ancient Greek ζῷον (zōon, "animal") + λόγος (logos, "knowledge"). Some branches of zoology include: anthrozoology, arachnology, archaeozoology, cetology, embryology, entomology, helminthology, herpetology, histology, ichthyology, malacology, mammalogy, morphology, nematology, ornithology, palaeozoology, pathology, primatology, protozoology, taxonomy, and zoogeography.

Human biology is an interdisciplinary academic field of biology, biological anthropology, nutrition and medicine which focuses on humans; it is closely related to primate biology, and a number of other fields.
Botany, plant science, or plant biology is a branch of biology that involves the scientific study of plant life. Botany covers a wide range of scientific disciplines including structure, growth, reproduction, metabolism, development, diseases, chemical properties, and evolutionary relationships among taxonomic groups. Botany began with early human efforts to identify edible, medicinal and poisonous plants, making it one of the oldest sciences. Today botanists study over 550,000 species of living organisms.
The term "botany" comes from Greek βοτάνη, meaning "pasture, grass, fodder", perhaps via the idea of a livestock keeper needing to know which plants are safe for livestock to eat.

Mycology, the study of fungi, is a branch of biology that focuses on fungi.

The "social sciences" are the fields of scholarship that study society. "Social science" is commonly used as an umbrella term for empirical fields outside of the natural sciences. These include: anthropology, archaeology, criminology, economics, linguistics, international relations, political science (aka government), public health, sociology, some branches of psychology (results of which can not be replicated or validated easily - e.g. social psychology), and certain aspects of business administration, communication, education, geography, history, and law.

"Applied science" is the application of scientific knowledge transferred into a physical environment. Examples include testing a theoretical model through the use of formal science or solving a practical problem through the use of natural science.

Applied science differs from fundamental science, which seeks to describe the most basic objects and forces, having less emphasis on practical applications. Applied science can be like biological science and physical science.

Example fields of applied science include

Fields of engineering are closely related to applied sciences. Applied science is important for technology development. Its use in industrial settings is usually referred to as research and development (R&D).




</doc>
<doc id="54738" url="https://en.wikipedia.org/wiki?curid=54738" title="Interpretations of quantum mechanics">
Interpretations of quantum mechanics

An interpretation of quantum mechanics is an attempt to explain how the mathematical theory of quantum mechanics "corresponds" to reality. Although quantum mechanics has held up to rigorous and extremely precise tests in an extraordinarily broad range of experiments (not one prediction from quantum mechanics is found to be contradicted by experiments), there exist a number of contending schools of thought over their interpretation. These views on interpretation differ on such fundamental questions as whether quantum mechanics is deterministic or stochastic, which elements of quantum mechanics can be considered "real", and what is the nature of measurement, among other matters.

Despite nearly a century of debate and experiment, no consensus has been reached among physicists and philosophers of physics concerning which interpretation best "represents" reality.

The definition of quantum theorists' terms, such as "wave functions" and "matrix mechanics", progressed through many stages. For instance, Erwin Schrödinger originally viewed the electron's wave function as its charge density smeared across space, whereas Max Born reinterpreted the absolute square value of the wave function as the electron's probability density distributed across space.

The views of several early pioneers of quantum mechanics, such as Niels Bohr and Werner Heisenberg, are often grouped together as the "Copenhagen interpretation", though physicists and historians of physics have argued that this terminology obscures differences between the views so designated. While Copenhagen-type ideas were never universally embraced, challenges to a perceived Copenhagen orthodoxy gained increasing attention in the 1950s with the pilot-wave interpretation of David Bohm and the many-worlds interpretation of Hugh Everett III.

Moreover, the strictly formalist position, shunning interpretation, has been challenged by proposals for falsifiable experiments that might one day distinguish among interpretations, as by measuring an AI consciousness or via quantum computing.

The physicist N. David Mermin once quipped, "New interpretations appear every year. None ever disappear." As a rough guide to development of the mainstream view during the 1990s to 2000s, consider the "snapshot" of opinions collected in a poll by Schlosshauer et al. at the "Quantum Physics and the Nature of Reality" conference of July 2011.
The authors reference a similarly informal poll carried out by Max Tegmark at the "Fundamental Problems in Quantum Theory" conference in August 1997. The main conclusion of the authors is that "the Copenhagen interpretation still reigns supreme", receiving the most votes in their poll (42%), besides the rise to mainstream notability of the many-worlds interpretations:

More or less, all interpretations of quantum mechanics share two qualities:
Two qualities vary among interpretations:

In philosophy of science, the distinction of knowledge versus reality is termed "epistemic" versus "ontic". A general law is a "regularity" of outcomes (epistemic), whereas a causal mechanism may "regulate" the outcomes (ontic). A phenomenon can receive interpretation either ontic or epistemic. For instance, indeterminism may be attributed to limitations of human observation and perception (epistemic), or may be explained as a real existing "maybe" encoded in the universe (ontic). Confusing the epistemic with the ontic, like if one were to presume that a general law actually "governs" outcomes—and that the statement of a regularity has the role of a causal mechanism—is a category mistake.

In a broad sense, scientific theory can be viewed as offering scientific realism—approximately true description or explanation of the natural world—or might be perceived with antirealism. A realist stance seeks the epistemic and the ontic, whereas an antirealist stance seeks epistemic but not the ontic. In the 20th century's first half, antirealism was mainly logical positivism, which sought to exclude unobservable aspects of reality from scientific theory.

Since the 1950s, antirealism is more modest, usually instrumentalism, permitting talk of unobservable aspects, but ultimately discarding the very question of realism and posing scientific theory as a tool to help humans make predictions, not to attain metaphysical understanding of the world. The instrumentalist view is carried by the famous quote of David Mermin, "Shut up and calculate", often misattributed to Richard Feynman.

Other approaches to resolve conceptual problems introduce new mathematical formalism, and so propose alternative theories with their interpretations. An example is Bohmian mechanics, whose empirical equivalence with the three standard formalisms—Schrödinger's wave mechanics, Heisenberg's matrix mechanics, and Feynman's path integral formalism—has been demonstrated.


As well as the mainstream interpretations discussed below, a number of other interpretations have been proposed which have not made a significant scientific impact for whatever reason. These range from proposals by mainstream physicists to the more occult ideas of quantum mysticism.

The current usage of realism and completeness originated in the 1935 paper in which Einstein and others proposed the EPR paradox. In that paper the authors proposed the concepts "element of reality" and the "completeness of a physical theory". They characterised element of reality as a quantity whose value can be predicted with certainty before measuring or otherwise disturbing it, and defined a complete physical theory as one in which every element of physical reality is accounted for by the theory. In a semantic view of interpretation, an interpretation is complete if every element of the interpreting structure is present in the mathematics. Realism is also a property of each of the elements of the maths; an element is real if it corresponds to something in the interpreting structure. For example, in some interpretations of quantum mechanics (such as the many-worlds interpretation) the ket vector associated to the system state is said to correspond to an element of physical reality, while in other interpretations it is not.

Determinism is a property characterizing state changes due to the passage of time, namely that the state at a future instant is a function of the state in the present (see time evolution). It may not always be clear whether a particular interpretation is deterministic or not, as there may not be a clear choice of a time parameter. Moreover, a given theory may have two interpretations, one of which is deterministic and the other not.

Local realism has two aspects:

A precise formulation of local realism in terms of a local hidden-variable theory was proposed by John Bell.

Bell's theorem, combined with experimental testing, restricts the kinds of properties a quantum theory can have, the primary implication being that quantum mechanics cannot satisfy both the principle of locality and counterfactual definiteness.

Regardless of Einstein's concerns about interpretation issues, Dirac and other quantum notables embraced the technical advances of the new theory while devoting little or no attention to interpretational aspects.

The Copenhagen interpretation is the "standard" interpretation of quantum mechanics formulated by Niels Bohr and Werner Heisenberg while collaborating in Copenhagen around 1927. Bohr and Heisenberg extended the probabilistic interpretation of the wavefunction proposed originally by Max Born. The Copenhagen interpretation rejects questions like "where was the particle before I measured its position?" as meaningless. The measurement process randomly picks out exactly one of the many possibilities allowed for by the state's wave function in a manner consistent with the well-defined probabilities that are assigned to each possible state. According to the interpretation, the interaction of an observer or apparatus that is external to the quantum system is the cause of wave function collapse, thus according to Paul Davies, "reality is in the observations, not in the electron". In general, after a measurement (click of a Geiger counter or a trajectory in a spark or bubble chamber) it ceases to be relevant unless subsequent experimental observations can be performed.

Quantum informational approaches have attracted growing support. They subdivide into two kinds.

The state is not an objective property of an individual system but is that information, obtained from a knowledge of how a system was prepared, which can be used for making predictions about future measurements.
...A quantum mechanical state being a summary of the observer's information about an individual physical system changes both by dynamical laws, and whenever the observer acquires new information about the system through the process of measurement. The existence of two laws for the evolution of the state vector...becomes problematical only if it is believed that the state vector is an objective property of the system...The "reduction of the wavepacket" does take place in the consciousness of the observer, not because of any unique physical process which takes place there, but only because the state is a construct of the observer and not an objective property of the physical system.

The essential idea behind relational quantum mechanics, following the precedent of special relativity, is that different observers may give different accounts of the same series of events: for example, to one observer at a given point in time, a system may be in a single, "collapsed" eigenstate, while to another observer at the same time, it may be in a superposition of two or more states. Consequently, if quantum mechanics is to be a complete theory, relational quantum mechanics argues that the notion of "state" describes not the observed system itself, but the relationship, or correlation, between the system and its observer(s). The state vector of conventional quantum mechanics becomes a description of the correlation of some "degrees of freedom" in the observer, with respect to the observed system. However, it is held by relational quantum mechanics that this applies to all physical objects, whether or not they are conscious or macroscopic. Any "measurement event" is seen simply as an ordinary physical interaction, an establishment of the sort of correlation discussed above. Thus the physical content of the theory has to do not with objects themselves, but the relations between them.

An independent relational approach to quantum mechanics was developed in analogy with David Bohm's elucidation of special relativity, in which a detection event is regarded as establishing a relationship between the quantized field and the detector. The inherent ambiguity associated with applying Heisenberg's uncertainty principle is subsequently avoided.

Quantum Bayesianism (also called QBism) is an interpretation of quantum mechanics that takes an agent's actions and experiences as the central concerns of the theory. This interpretation is distinguished by its use of a subjective Bayesian account of probabilities to understand the quantum mechanical Born rule as a normative addition to good decision-making. QBism draws from the fields of quantum information and Bayesian probability and aims to eliminate the interpretational conundrums that have beset quantum theory.

QBism deals with common questions in the interpretation of quantum theory about the nature of wavefunction superposition, quantum measurement, and entanglement. According to QBism, many, but not all, aspects of the quantum formalism are subjective in nature. For example, in this interpretation, a quantum state is not an element of reality—instead it represents the degrees of belief an agent has about the possible outcomes of measurements. For this reason, some philosophers of science have deemed QBism a form of anti-realism. The originators of the interpretation disagree with this characterization, proposing instead that the theory more properly aligns with a kind of realism they call "participatory realism", wherein reality consists of "more" than can be captured by any putative third-person account of it.

The many-worlds interpretation is an interpretation of quantum mechanics in which a universal wavefunction obeys the same deterministic, reversible laws at all times; in particular there is no (indeterministic and irreversible) wavefunction collapse associated with measurement. The phenomena associated with measurement are claimed to be explained by decoherence, which occurs when states interact with the environment producing entanglement, repeatedly "splitting" the universe into mutually unobservable alternate histories—effectively distinct universes within a greater multiverse.

The consistent histories interpretation generalizes the conventional Copenhagen interpretation and attempts to provide a natural interpretation of quantum cosmology. The theory is based on a consistency criterion that allows the history of a system to be described so that the probabilities for each history obey the additive rules of classical probability. It is claimed to be consistent with the Schrödinger equation.

According to this interpretation, the purpose of a quantum-mechanical theory is to predict the relative probabilities of various alternative histories (for example, of a particle).

The ensemble interpretation, also called the statistical interpretation, can be viewed as a minimalist interpretation. That is, it claims to make the fewest assumptions associated with the standard mathematics. It takes the statistical interpretation of Born to the fullest extent. The interpretation states that the wave function does not apply to an individual systemfor example, a single particlebut is an abstract statistical quantity that only applies to an ensemble (a vast multitude) of similarly prepared systems or particles. In the words of Einstein:

The most prominent current advocate of the ensemble interpretation is Leslie E. Ballentine, professor at Simon Fraser University, author of the text book "Quantum Mechanics, A Modern Development". An experiment illustrating the ensemble interpretation is provided in Akira Tonomura's Video clip 1. 
The de Broglie–Bohm theory of quantum mechanics (also known as the pilot wave theory) is a theory by Louis de Broglie and extended later by David Bohm to include measurements. Particles, which always have positions, are guided by the wavefunction. The wavefunction evolves according to the Schrödinger wave equation, and the wavefunction never collapses. The theory takes place in a single space-time, is non-local, and is deterministic. The simultaneous determination of a particle's position and velocity is subject to the usual uncertainty principle constraint. The theory is considered to be a hidden-variable theory, and by embracing non-locality it satisfies Bell's inequality. The measurement problem is resolved, since the particles have definite positions at all times. Collapse is explained as phenomenological.

Quantum Darwinism is a theory meant to explain the emergence of the classical world from the quantum world as due to a process of Darwinian natural selection induced by the environment interacting with the quantum system; where the many possible quantum states are selected against in favor of a stable pointer state. It was proposed in 2003 by Wojciech Zurek and a group of collaborators including Ollivier, Poulin, Paz and Blume-Kohout. The development of the theory is due to the integration of a number of Zurek’s research topics pursued over the course of twenty-five years including: pointer states, einselection and decoherence.

The transactional interpretation of quantum mechanics (TIQM) by John G. Cramer is an interpretation of quantum mechanics inspired by the Wheeler–Feynman absorber theory. It describes the collapse of the wave function as resulting from a time-symmetric transaction between a possibility wave from the source to the receiver (the wave function) and a possibility wave from the receiver to source (the complex conjugate of the wave function). This interpretation of quantum mechanics is unique in that it not only views the wave function as a real entity, but the complex conjugate of the wave function, which appears in the Born rule for calculating the expected value for an observable, as also real.

An entirely classical derivation and interpretation of Schrödinger's wave equation by analogy with Brownian motion was suggested by Princeton University professor Edward Nelson in 1966. Similar considerations had previously been published, for example by R. Fürth (1933), I. Fényes (1952), and Walter Weizel (1953), and are referenced in Nelson's paper. More recent work on the stochastic interpretation has been done by M. Pavon. An alternative stochastic interpretation was developed by Roumen Tsekov.

Objective collapse theories differ from the Copenhagen interpretation by regarding both the wave function and the process of collapse as ontologically objective (meaning these exist and occur independent of the observer). In objective theories, collapse occurs either randomly ("spontaneous localization") or when some physical threshold is reached, with observers having no special role. Thus, objective-collapse theories are realistic, indeterministic, no-hidden-variables theories. Standard quantum mechanics does not specify any mechanism of collapse; QM would need to be extended if objective collapse is correct. The requirement for an extension to QM means that objective collapse is more of a theory than an interpretation. Examples include 

In his treatise "The Mathematical Foundations of Quantum Mechanics", John von Neumann deeply analyzed the so-called measurement problem. He concluded that the entire physical universe could be made subject to the Schrödinger equation (the universal wave function). He also described how measurement could cause a collapse of the wave function. This point of view was prominently expanded on by Eugene Wigner, who argued that human experimenter consciousness (or maybe even dog consciousness) was critical for the collapse, but he later abandoned this interpretation.

Variations of the consciousness causes collapse interpretation include:

Other physicists have elaborated their own variations of the consciousness causes collapse interpretation; including:

Quantum logic can be regarded as a kind of propositional logic suitable for understanding the apparent anomalies regarding quantum measurement, most notably those concerning composition of measurement operations of complementary variables. This research area and its name originated in the 1936 paper by Garrett Birkhoff and John von Neumann, who attempted to reconcile some of the apparent inconsistencies of classical boolean logic with the facts related to measurement and observation in quantum mechanics.

Modal interpretations of quantum mechanics were first conceived of in 1972 by B. van Fraassen, in his paper "A formal approach to the philosophy of science." However, this term now is used to describe a larger set of models that grew out of this approach. The Stanford Encyclopedia of Philosophy describes several versions:

Several theories have been proposed which modify the equations of quantum mechanics to be symmetric with respect to time reversal. (E.g. see Wheeler-Feynman time-symmetric theory). This creates retrocausality: events in the future can affect ones in the past, exactly as events in the past can affect ones in the future. In these theories, a single measurement cannot fully determine the state of a system (making them a type of hidden-variables theory), but given two measurements performed at different times, it is possible to calculate the exact state of the system at all intermediate times. The collapse of the wavefunction is therefore not a physical change to the system, just a change in our knowledge of it due to the second measurement. Similarly, they explain entanglement as not being a true physical state but just an illusion created by ignoring retrocausality. The point where two particles appear to "become entangled" is simply a point where each particle is being influenced by events that occur to the other particle in the future.

Not all advocates of time-symmetric causality favour modifying the unitary dynamics of standard quantum mechanics. Thus a leading exponent of the two-state vector formalism, Lev Vaidman, states that the two-state vector formalism dovetails well with Hugh Everett's many-worlds interpretation.

BST theories resemble the many worlds interpretation; however, "the main difference is that the BST interpretation takes the branching of history to be a feature of the topology of the set of events with their causal relationships... rather than a consequence of the separate evolution of different components of a state vector." In MWI, it is the wave functions that branches, whereas in BST, the space-time topology itself branches.
BST has applications to Bell's theorem, quantum computation and quantum gravity. It also has some resemblance to hidden-variable theories and the ensemble interpretation: particles in BST have multiple well defined trajectories at the microscopic level. These can only be treated stochastically at a coarse grained level, in line with the ensemble interpretation.

The most common interpretations are summarized in the table below. The values shown in the cells of the table are not without controversy, for the precise meanings of some of the concepts involved are unclear and, in fact, are themselves at the center of the controversy surrounding the given interpretation. For another table comparing interpretations of quantum theory, see reference.

No experimental evidence exists that distinguishes among these interpretations. To that extent, the physical theory stands, and is consistent with itself and with reality; difficulties arise only when one attempts to "interpret" the theory. Nevertheless, designing experiments which would test the various interpretations is the subject of active research.

Most of these interpretations have variants. For example, it is difficult to get a precise definition of the Copenhagen interpretation as it was developed and argued about by many people.


Although interpretational opinions are openly and widely discussed today, that was not always the case. A notable exponent of a tendency of silence was Paul Dirac who once wrote: "The interpretation of quantum mechanics has been dealt with by many authors, and I do not want to discuss it here. I want to deal with more fundamental things." This position is not uncommon among practitioners of quantum mechanics. Others, like Nico van Kampen and Willis Lamb, have openly criticized non-orthodox interpretations of quantum mechanics.


Almost all authors below are professional physicists.



</doc>
<doc id="26997" url="https://en.wikipedia.org/wiki?curid=26997" title="Scientist">
Scientist

A scientist is someone who conducts scientific research to advance knowledge in an area of interest.

In classical antiquity, there was no real ancient analog of a modern scientist. Instead, philosophers engaged in the philosophical study of nature called natural philosophy, a precursor of natural science. It was not until the 19th century that the term "scientist" came into regular use after it was coined by the theologian, philosopher, and historian of science William Whewell in 1833.

In modern times, many scientists have advanced degrees in an area of science and pursue careers in various sectors of the economy such as academia, industry, government, and nonprofit environments.""

The roles of "scientists", and their predecessors before the emergence of modern scientific disciplines, have evolved considerably over time. Scientists of different eras (and before them, natural philosophers, mathematicians, natural historians, natural theologians, engineers, and others who contributed to the development of science) have had widely different places in society, and the social norms, ethical values, and epistemic virtues associated with scientists—and expected of them—have changed over time as well. Accordingly, many different historical figures can be identified as early scientists, depending on which characteristics of modern science are taken to be essential.

Some historians point to the Scientific Revolution that began in 16th century as the period when science in a recognizably modern form developed. It wasn't until the 19th century that sufficient socioeconomic changes occurred for scientists to emerge as a major profession.

Knowledge about nature in classical antiquity was pursued by many kinds of scholars. Greek contributions to science—including works of geometry and mathematical astronomy, early accounts of biological processes and catalogs of plants and animals, and theories of knowledge and learning—were produced by philosophers and physicians, as well as practitioners of various trades. These roles, and their associations with scientific knowledge, spread with the Roman Empire and, with the spread of Christianity, became closely linked to religious institutions in most of European countries. Astrology and astronomy became an important area of knowledge, and the role of astronomer/astrologer developed with the support of political and religious patronage. By the time of the medieval university system, knowledge was divided into the "trivium"—philosophy, including natural philosophy—and the "quadrivium"—mathematics, including astronomy. Hence, the medieval analogs of scientists were often either philosophers or mathematicians. Knowledge of plants and animals was broadly the province of physicians.

Science in medieval Islam generated some new modes of developing natural knowledge, although still within the bounds of existing social roles such as philosopher and mathematician. Many proto-scientists from the Islamic Golden Age are considered polymaths, in part because of the lack of anything corresponding to modern scientific disciplines. Many of these early polymaths were also religious priests and theologians: for example, Alhazen and al-Biruni were mutakallimiin; the physician Avicenna was a hafiz; the physician Ibn al-Nafis was a hafiz, muhaddith and ulema; the botanist Otto Brunfels was a theologian and historian of Protestantism; the astronomer and physician Nicolaus Copernicus was a priest. During the Italian Renaissance scientists like Leonardo Da Vinci, Michelangelo, Galileo Galilei and Gerolamo Cardano have been considered as the most recognizable polymaths.

During the Renaissance, Italians made substantial contributions in science. Leonardo Da Vinci made significant discoveries in paleontology and anatomy. The Father of modern Science,
Galileo Galilei, made key improvements on the thermometer and telescope which allowed him to observe and clearly describe the solar system. Descartes was not only a pioneer of analytic geometry but formulated a theory of mechanics and advanced ideas about the origins of animal movement and perception. Vision interested the physicists Young and Helmholtz, who also studied optics, hearing and music. Newton extended Descartes' mathematics by inventing calculus (at the same time as Leibniz). He provided a comprehensive formulation of classical mechanics and investigated light and optics. Fourier founded a new branch of mathematics — infinite, periodic series — studied heat flow and infrared radiation, and discovered the greenhouse effect. Girolamo Cardano, Blaise Pascal Pierre de Fermat, Von Neumann, Turing, Khinchin, Markov and Wiener, all mathematicians, made major contributions to science and probability theory, including the ideas behind computers, and some of the foundations of statistical mechanics and quantum mechanics. Many mathematically inclined scientists, including Galileo, were also musicians.

There are many compelling stories in medicine and biology, such as the development of ideas about the circulation of blood from Galen to Harvey.

During the age of Enlightenment, Luigi Galvani, the pioneer of the bioelectromagnetics, discovered the animal electricity. He discovered that a charge applied to the spinal cord of a frog could generate muscular spasms throughout its body. Charges could make frog legs jump even if the legs were no longer attached to a frog. While cutting a frog leg, Galvani's steel scalpel touched a brass hook that was holding the leg in place. The leg twitched. Further experiments confirmed this effect, and Galvani was convinced that he was seeing the effects of what he called animal electricity, the life force within the muscles of the frog. At the University of Pavia, Galvani's colleague Alessandro Volta was able to reproduce the results, but was sceptical of Galvani's explanation.

Lazzaro Spallanzani is one of the most influential figures in experimental physiology and the natural sciences. His investigations have exerted a lasting influence on the medical sciences. He made important contributions to the experimental study of bodily functions and animal reproduction.

Francesco Redi discovered that microorganisms can cause disease. 

Until the late 19th or early 20th century, scientists were still referred to as "natural philosophers" or "men of science".

English philosopher and historian of science William Whewell coined the term "scientist" in 1833, and it first appeared in print in Whewell's anonymous 1834 review of Mary Somerville's "On the Connexion of the Physical Sciences" published in the "Quarterly Review". Whewell's suggestion of the term was partly satirical, a response to changing conceptions of science itself in which natural knowledge was increasingly seen as distinct from other forms of knowledge. Whewell wrote of "an increasing proclivity of separation and dismemberment" in the sciences; while highly specific terms proliferated—chemist, mathematician, naturalist—the broad term "philosopher" was no longer satisfactory to group together those who pursued science, without the caveats of "natural" or "experimental" philosopher. Members of the British Association for the Advancement of Science had been complaining about the lack of a good term at recent meetings, Whewell reported in his review; alluding to himself, he noted that "some ingenious gentleman proposed that, by analogy with "artist", they might form [the word] "scientist", and added that there could be no scruple in making free with this term since we already have such words as "economist", and "atheist"—but this was not generally palatable".

Whewell proposed the word again more seriously (and not anonymously) in his 1840 "The Philosophy of the Inductive Sciences":

He also proposed the term "physicist" at the same time, as a counterpart to the French word "physicien". Neither term gained wide acceptance until decades later; "scientist" became a common term in the late 19th century in the United States and around the turn of the 20th century in Great Britain. By the twentieth century, the modern notion of science as a special brand of information about the world, practiced by a distinct group and pursued through a unique method, was essentially in place.

Ramón y Cajal won the Nobel Prize in 1906 for his remarkable observations in neuroanatomy.

Marie Curie became the first female to win the Nobel Prize and the first person to win it twice. Her efforts led to the development of nuclear energy and Radiotherapy for the treatment of cancer. In 1922, she was appointed a member of the International Commission on Intellectual Co-operation by the Council of the League of Nations. She campaigned for scientist's right to patent their discoveries and inventions. She also campaigned for free access to international scientific literature and for internationally recognized scientific symbols.

As a profession, the scientist of today is widely recognized. 

In modern times, many professional scientists are trained in an academic setting (e.g., universities and research institutes), mostly at the level of graduate schools. Upon completion, they would normally attain an academic degree, with the highest degree being a doctorate such as a Doctor of Philosophy (PhD). Although graduate education for scientists varies among institutions and countries, some common training requirements include specializing in an area of interest, publishing research findings in peer-reviewed scientific journals and presenting them at scientific conferences, giving lectures or teaching, and defending a thesis (or dissertation) during an oral examination. To aid them in this endeavor, graduate students often work under the guidance of a mentor, usually a senior scientist, which may continue after the completion of their doctorates whereby they work as postdoctoral researchers.

After the completion of their training, many scientists pursue careers in a variety of work settings and conditions. In 2017, the British scientific journal "Nature" published the results of a large-scale survey of more than 5,700 doctoral students worldwide, asking them which sectors of the economy that would like to work in. A little over half of the respondents wanted to pursue a career in academia, with smaller proportions hoping to work in industry, government, and nonprofit environments.

Scientists are motivated to work in several ways. Many have a desire to understand why the world is as we see it and how it came to be. They exhibit a strong curiosity about reality. Other motivations are recognition by their peers and prestige. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, chemistry, and economics. 

Some scientists have a desire to apply scientific knowledge for the benefit of people's health, the nations, the world, nature, or industries (academic scientist and industrial scientist). Scientists tend to be less motivated by direct financial reward for their work than other careers. As a result, scientific researchers often accept lower average salaries when compared with many other professions which require a similar amount of training and qualification.

Although there have been exceptions, most scientists tend to do their best research when they are relatively young, in their 30s.

Scientists include experimentalists who mainly perform experiments to test hypotheses, and theoreticians who mainly develop models to explain existing data and predict new results. There is a continuum between two activities and the division between them is not clear-cut, with many scientists performing both tasks.

Those considering science as a career often look to the frontiers. These include cosmology and biology, especially molecular biology and the human genome project. Other areas of active research include the exploration of matter at the scale of elementary particles as described by high-energy physics, and materials science, which seeks to discover and design new materials. Although there have been remarkable discoveries with regard to brain function and neurotransmitters, the nature of the mind and human thought still remains unknown.


The number of scientists is vastly different from country to country. For instance, there are only four full-time scientists per 10,000 workers in India while this number is 79 for the United Kingdom and the United States.
According to the United States National Science Foundation 4.7 million people with science degrees worked in the United States in 2015, across all disciplines and employment sectors. The figure included twice as many men as women. Of that total, 17% worked in academia, that is, at universities and undergraduate institutions, and men held 53% of those positions. 5% of scientists worked for the federal government and about 3.5% were self-employed. Of the latter two groups, two-thirds were men. 59% of US scientists were employed in industry or business, and another 6% worked in non-profit positions.

Scientist and engineering statistics are usually intertwined, but they indicate that women enter the field far less than men, though this gap is narrowing. The number of science and engineering doctorates awarded to women rose from a mere 7 percent in 1970 to 34 percent in 1985 and in engineering alone the numbers of bachelor's degrees awarded to women rose from only 385 in 1975 to more than 11000 in 1985. 







</doc>
<doc id="40272" url="https://en.wikipedia.org/wiki?curid=40272" title="Industrial sociology">
Industrial sociology

Industrial sociology, until recently a crucial research area within the field of sociology of work, examines
One branch of industrial sociology is labour process theory (LPT). In 1974, Harry Braverman wrote "Labor and Monopoly Capital", which provided a critical analysis of scientific management. This book analysed capitalist productive relations from a Marxist perspective. Following Marx, Braverman argued that work within capitalist organizations was exploitative and alienating, and therefore workers had to be coerced into servitude. For Braverman the pursuit of capitalist interests over time ultimately leads to deskilling and routinization of the worker. The Taylorist work design is the ultimate embodiment of this tendency.

Braverman demonstrated several mechanisms of control in both the factory blue-collar and clerical white-collar labour force.
His key contribution is his "deskilling" thesis. Braverman argued that capitalist owners and managers were incessantly driven to deskill the labour force to lower production costs and ensure higher productivity. Deskilled labour is cheap and above all easy to control due to the workers' lack of direct engagement in the production process. In turn work becomes intellectually or emotionally unfulfilling; the lack of capitalist reliance on human skill reduces the need of employers to reward workers in anything but a minimal economic way.

Braverman's contribution to the sociology of work and industry (i.e., industrial sociology) has been important and his theories of the labour process continue to inform teaching and research. Braverman's thesis has, however, been contested, by Andrew Freidman in his work "Industry and Labour" (1977). In it, Freidman suggests that whilst the direct control of labour is beneficial for the capitalist under certain circumstances, a degree of "responsible autonomy" can be granted to unionized or "core" workers, in order to harness their skill under controlled conditions. Also, Richard Edwards showed in 1979 that although hierarchy in organizations has remained constant, additional forms of control (such as technical control via email monitoring, call monitoring; bureaucratic control via procedures for leave, sickness etc.) has been added to gain the interests of the capitalist class versus the workers. Duncan Gallie has shown how important it is to approach the question of skill from a social class perspective. In his study, the majority of non-manual, intermediate and skilled manual workers believed that their work had come to demand a higher level of skill, but the majority of manual worker felt that the responsibility and skill needed in their work had either remained constant or declined. This means that Braverman's claims can't be applied to all social classes.

The notion the particular type of technology workers were exposed to shapes their experience was most forcefully argued in a classic study by Robert Blauner. He argued that some work is alienating more than other types because of the different technologies workers use. Alienation, to Blauner, has four dimensions: powerlessness, meaninglessness, isolation, and self-estrangement. Individuals are powerless when they can't control their own actions or conditions of work; work is meaningless when it gives employees little or no sense of value, interest or worth; work is isolating when workers cannot identify with their workplace; and work is self-estranging when, at the subjective level, the worker has no sense of involvement in the job.

Blauner's claims however fail to recognize that the same technology can be experienced in a variety of ways. Studies have shown that cultural differences with regard to management–union relations, levels of hierarchical control, and reward and performance appraisal policies mean that the experience of the same kind of work can vary considerably between countries and firms. The individualization of work and the need for workers to have more flexible skills in order to respond to technological changes means that Blauner's characterization of work experience is no longer valid. Additionally, workers today may work in teams to alleviate workers' sense of alienation, since they are involved in the entire process, rather than just a small part of it. In conclusion, automative technologies and computerized work systems have typically enhanced workers' job satisfaction and skill deployment in the better-paid, secure public and private sector jobs. But, in more non-skilled manual work, they have just perpetuated job dissatisfaction, especially for the many women involved in this type of work.



</doc>
<doc id="7369020" url="https://en.wikipedia.org/wiki?curid=7369020" title="International Standard Industrial Classification">
International Standard Industrial Classification

The International Standard Industrial Classification of All Economic Activities (ISIC) is a United Nations industry classification system. Wide use has been made of ISIC in classifying data according to kind of economic activity in the fields of employment and health data.

It is maintained by the United Nations Statistics Division.

ISIC classifies entities by activity. The most detailed categories are defined by combinations of activities described in statistical units, considering the relative importance of the activities included in these classes.

ISIC Rev.4 continues to use criteria such as input, output and use of the products produced, but places additional emphasis on production processes.

The United Nations Statistics Division has published the following revisions of the ISIC standard:





</doc>
<doc id="8264048" url="https://en.wikipedia.org/wiki?curid=8264048" title="Group technology">
Group technology

Group technology or GT is a manufacturing technique in which parts having similarities in geometry, manufacturing process and/or functions are manufactured in one location using a small number of machines or processes. Group technology is based on a general principle that many problems are similar and by grouping similar problems, a single solution can be found to a set of problems, thus saving time and effort.

The group of similar parts is known as part family and the group of machineries used to process an individual part family is known as machine cell. It is not necessary for each part of a part family to be processed by every machine of corresponding machine cell. This type of manufacturing in which a part family is produced by a machine cell is known as cellular manufacturing. 
The manufacturing efficiencies are generally increased by employing GT because the required operations may be confined to only a small cell and thus avoiding the need for transportation of in-process parts.

Group technology is an approach in which similar parts are identified and grouped together in order to take advantage of the similarities in design and production. Similarities among parts permit them to be classified into part families.

The advantage of GT can be divided into three groups:

Disadvantages of GT Manufacturing :


</doc>
<doc id="173366" url="https://en.wikipedia.org/wiki?curid=173366" title="Mechanization">
Mechanization

Mechanization is the process of changing from working largely or exclusively by hand or with animals to doing that work with machinery. In an early engineering text a machine is defined as follows:

In some fields, mechanization includes the use of hand tools. In modern usage, such as in engineering or economics, mechanization implies machinery more complex than hand tools and would not include simple devices such as an ungeared horse or donkey mill. Devices that cause speed changes or changes to or from reciprocating to rotary motion, using means such as gears, pulleys or sheaves and belts, shafts, cams and cranks, usually are considered machines. After electrification, when most small machinery was no longer hand powered, mechanization was synonymous with motorized machines. Extension of mechanization of the production process is termed as automation and it is controlled by a closed loop system in which feedback is provided by the sensors. It controls the operations of different machines automatically.

Water wheels date to the Roman period and were used to grind grain and lift irrigation water. Water powered bellows were in use on blast furnaces in China in 31 AD. By the 13th century, water wheels powered sawmills and trip hammers, to full cloth and pound flax and later cotton rags into pulp for making paper. Trip hammers are shown crushing ore in "De re Metallica" (1555).

Clocks were some of the most complex early mechanical devices. Clock makers were important developers of machine tools including gear and screw cutting machines, and were also involved in the mathematical development of gear designs. Clocks were some of the earliest mass-produced items, beginning around 1830.

Water powered bellows for blast furnaces, used in China in ancient times, were in use in Europe by the 15th century. "De re Metallica" contains drawings related to bellows for blast furnaces including a fabrication drawing.

Improved gear designs decreased wear and increased efficiency. Mathematical gear designs were developed in the mid 17th century. French mathematician and engineer Desargues designed and constructed the first mill with epicycloidal teeth ca. 1650. In the 18th century involute gears, another mathematical derived design, came into use. Involute gears are better for meshing gears of different sizes than epicycloidal. Gear cutting machines came into use in the 18th century.

The Newcomen steam engine was first used, to pump water from a mine, in 1712. John Smeaton introduced metal gears and axles to water wheels in the mid to last half of the 18th century. The Industrial Revolution started mainly with textile machinery, such as the spinning jenny (1764) and water frame (1768).

Demand for metal parts used in textile machinery led to the invention of many machine tools in the late 1700s until the mid-1800s. After the early decades of the 19th century, iron increasingly replaced wood in gearing and shafts in textile machinery. In the 1840s "self acting" machine tools were developed. Machinery was developed to make nails ca. 1810. The Fourdrinier paper machine paper machine for continuous production of paper was patented in 1801, displacing the centuries-old hand method of making individual sheets of paper.

One of the first mechanical devices used in agriculture was the seed drill invented by Jethro Tull around 1700. The seed drill allowed more uniform spacing of seed and planting depth than hand methods, increasing yields and saving valuable seed. In 1817, the first bicycle was invented and used in Germany. Mechanized agriculture greatly increased in the late eighteenth and early nineteenth centuries with horse drawn reapers and horse powered threshing machines. By the late nineteenth century steam power was applied to threshing and steam tractors appeared. Internal combustion began being used for tractors in the early twentieth century. Threshing and harvesting was originally done with attachments for tractors, but in the 1930s independently powered combine harvesters were in use.

In the mid to late 19th century, hydraulic and pneumatic devices were able to power various mechanical actions, such as positioning tools or work pieces. Pile drivers and steam hammers are examples for heavy work. In food processing, pneumatic or hydraulic devices could start and stop filling of cans or bottles on a conveyor. Power steering for automobiles uses hydraulic mechanisms, as does practically all earth moving equipment and other construction equipment and many attachments to tractors. Pneumatic (usually compressed air) power is widely used to operate industrial valves.

By the early 20th century machines developed the ability to perform more complex operations that had previously been done by skilled craftsmen. An example is the glass bottle making machine developed 1905. It replaced highly paid glass blowers and child labor helpers and led to the mass production of glass bottles.

After 1900 factories were electrified, and electric motors and controls were used to perform more complicated mechanical operations. This resulted in mechanized processes to manufacture almost all goods.

In manufacturing, mechanization replaced hand methods of making goods. Prime movers are devices that convert thermal, potential or kinetic energy into mechanical work. Prime movers include internal combustion engines, combustion turbines (jet engines), water wheels and turbines, windmills and wind turbines and steam engines and turbines. Powered transportation equipment such as locomotives, automobiles and trucks and airplanes, is a classification of machinery which includes sub classes by engine type, such as internal combustion, combustion turbine and steam. Inside factories, warehouses, lumber yards and other manufacturing and distribution operations, material handling equipment replaced manual carrying or hand trucks and carts.

Mechanized agriculture

In mining and excavation, power shovels replaced picks and shovels. Rock and ore crushing had been done for centuries by water powered trip hammers, but trip hammers have been replaced by modern ore crushers and ball mills.

Bulk material handling systems and equipment are used for a variety of materials including coal, ores, grains, sand, gravel and wood products.

Construction equipment includes cranes, concrete mixers, concrete pumps, cherry pickers and an assortment of power tools.

Powered machinery today usually means either by electric motor or internal combustion engine. Before the first decade of the 20th century powered usually meant by steam engine, water or wind.

Many of the early machines and machine tools were hand powered, but most changed over to water or steam power by the early 19th century.

Before electrification, mill and factory power was usually transmitted using a line shaft. Electrification allowed individual machines to each be powered by a separate motor in what is called "unit drive". Unit drive allowed factories to be better arranged and allowed different machines to run at different speeds. Unit drive also allowed much higher speeds, which was especially important for machine tools.

A step beyond mechanization is automation. Early production machinery, such as the glass bottle blowing machine (ca. 1890s), required a lot of operator involvement. By the 1920s fully automatic machines, which required much less operator attention, were being used.

See: Mass production

The term is also used in the military to refer to the use of tracked armoured vehicles, particularly armoured personnel carriers, to move troops that would otherwise have marched or ridden trucks into combat. In military terminology, "mechanized" refers to ground units that can fight from vehicles, while "motorized" refers to units that go to battle in vehicles but then dismount and fight without them. Thus, a towed artillery unit is considered motorized while a self-propelled one is mechanized.

When we compare the efficiency of a labourer, we see that he has an efficiency of about 1%-5.5% (depending on whether he uses arms, or a combination of arms and legs). Internal combustion engines mostly have an efficiency of about 20%, although large diesel engines, such as those used to power ships, may have efficiencies of nearly 50%. Industrial electric motors have efficiencies up to the low 90% range, before correcting for the conversion efficiency of fuel to electricity of about 35%.

When we compare the costs of using an internal combustion engine to a worker to perform work, we notice that an engine can perform more work at a comparative cost. 1 liter of fossil fuel burnt with an IC engine equals about 50 hands of workers operating for 24 hours or 275 arms and legs for 24 hours.

In addition, the combined work capability of a human is also much lower than that of a machine. An average human worker can provide work good for around 0,9 hp (2.3 MJ per hour) while a machine (depending on the type and size) can provide for far greater amounts of work. For example, it takes more than one and a half hour of hard labour to deliver only one kWh - which a small engine could deliver in less than one hour while burning less than one litre of petroleum fuel. This implies that a gang of 20 to 40 men will require a financial compensation for their work at least equal to the required expended food calories (which is at least 4 to 20 times higher). In most situations, the worker will also want compensation for the lost time, which is easily 96 times greater per day. Even if we assume the real wage cost for the human labour to be at US $1.00/day, an energy cost is generated of about $4.00/kWh. Despite this being a low wage for hard labour, even in some of the countries with the lowest wages, it represents an energy cost that is significantly more expensive than even exotic power sources such as solar photovoltaic panels (and thus even more expensive when compared to wind energy harvesters or luminescent solar concentrators).

For simplification, one can study mechanization as a series of steps. Many students refer to this series as indicating basic-to-advanced forms of mechanical society.





</doc>
<doc id="5267764" url="https://en.wikipedia.org/wiki?curid=5267764" title="Economic importance of bacteria">
Economic importance of bacteria

Bacteria are economically important as these microorganisms are used by humans for many purposes. The beneficial uses of bacteria include the production of traditional foods such as yogurt, cheese, and vinegar. Microbes are also important in agriculture for the compost and fertilizer production.

Sourdough bread is made to rise by fermentation, with a leaven that consists of bacteria, often combined with wild yeast enzymes. The milk-souring bacterial genus "Lactobacillus" is used to make yogurt and cheese. Bacteria are also used to form organic acids in pickles and vinegar.

Biotechnology involves the use of microorganisms including bacteria and fungi in the manufacturing and services industries. These include chemical manufacturing such as ethanol, acetone, organic acid, enzymes, and perfumes. Bacteria are important in the production of many dietary supplements and pharmaceuticals. For example, "Escherichia coli" is used for commercial preparation of riboflavin and vitamin K. "E. coli" is also used to produce D-amino acids such as D-p-hydroxyphenylglycine, an important intermediate for synthesis of the antibiotic amoxicillin.

Genetic engineering is the manipulation of genes. It is also called recombinant DNA technology. In genetic engineering, pieces of DNA (genes) are introduced into a host by a variety of techniques, one of the earliest being the use of a virus vector. The foreign DNA becomes a permanent feature of the host, and is replicated and passed on to daughter cells along with the rest of its DNA. Bacterial cells are transformed and used in production of commercially important products. Examples include production of human insulin (used to treat diabetes) and human growth hormone (somatotrophin used to treat pituitary dwarfism).

Bacteria such as "Clostridium butyricum" are used to separate fibres of jute, hemp and flax in the process of retting. The plants are immersed in water and when they swell, inoculated with bacteria which hydrolyze pectic substances of the cell walls and separate the fibres. Alternatively, the plants are spread out on the ground, where they become wetted by dew and ret naturally. These separated fibres are used to make ropes, sacks etc.

Bacteria can also be used in the place of pesticides in biological pest control. This commonly uses "Bacillus thuringiensis" (also called BT), a Gram-positive, soil-dwelling bacterium. This bacterium is used as a Lepidopteran-specific insecticide under trade names such as Dipel and Thuricide. Because of their specificity, these pesticides are regarded as environmentally friendly, with little effect on humans, wildlife, pollinators, or other beneficial insects.

Bacteria can be used to remove pollutants from contaminated water, soil and subsurface material. During the "Mega Borg" Oil Spill, for example, 100 pounds of bacteria were sprayed over an acre of the oil slick to break down the hydrocarbons present into more benign by-products.

Bacteria living in the gut of cattle, horses and other herbivores, for example "Ruminococcus" spp., help digest cellulose by secreting the enzyme cellulase. This is how herbivores are able get the energy they need from grass and other plants.

Also, "Escherichia coli", part of the intestinal microbiota of humans and other herbivorous animals, converts consumed food into vitamin K. This is absorbed in the colon and, in animal models, is sufficient to meet their daily requirement of the vitamin.

Bacteria helps purify animal hides to make them fit for use.

Bacteria are used to create multiple antibiotics such as Streptomycin from the bacteria streptococcus. Bacteria can also be used to create vaccines to prevent several diseases.

Some bacteria are harmful and act either as disease-causing agents (pathogens) both in plants and animals, or may play a role in food spoilage.

Bacteria cause a wide range of diseases in humans and other animals. These include superficial infections (eg. impetigo), systemic infections (eg. typhoid fever), acute infections (eg. cholera) and chronic infections (eg. tuberculosis).

Plant diseases caused by bacteria are commercially important worldwide for agriculture. Besides bacterial pathogens that are already established in many areas, there are many instances of pathogens moving to new geographic areas or even the emergence of new pathogen variants. In addition, bacterial plant pathogens are difficult to control because of the shortage of chemical control agents for bacteria.

Saprotrophic bacteria attack and decompose organic matter. This characteristic has posed a problem to mankind as food such as stored grains, meat, fish, vegetable and fruits are attacked by saprotrophic bacteria and spoiled. Similarly milk and products are easily contaminated by bacteria and spoiled.


</doc>
<doc id="68040" url="https://en.wikipedia.org/wiki?curid=68040" title="Standard Industrial Classification">
Standard Industrial Classification

The Standard Industrial Classification (SIC) is a system for classifying industries by a four-digit code. Established in the United States in 1937, it is used by government agencies to classify industry areas. The SIC system is also used by agencies in other countries, e.g., by the United Kingdom's Companies House.

In the United States, the SIC code has been replaced by the North American Industry Classification System (NAICS code), which was released in 1997. Some U.S. government departments and agencies, such as the U.S. Securities and Exchange Commission (SEC), continued to use SIC codes through at least 2019.

The SIC code for an establishment, that is, a workplace with a U.S. address, was determined by the industry appropriate for the overall largest product lines of the company or organization of which the establishment was a part. The later NAICS classification system has a different concept, assigning establishments into categories based on each one's output.

In the early 1900s, each branch of a United States government agency would conduct business analysis using its own methods and metrics, unknown and meaningless to other branches. In the 1930s, the government needed standardized and meaningful ways in which to measure, analyze and share data across its various agencies. Thus, the Standard Industrial Classification system was born. SIC codes are four-digit numerical representations of major businesses and industries. SIC codes are assigned based on common characteristics shared in the products, services, production and delivery system of a business.

SIC codes have a hierarchical, top-down structure that begins with general characteristics and narrows down to the specifics. The first two digits of the code represent the major industry sector to which a business belongs. The third and fourth digits describe the sub-classification of the business group and specialization, respectively. For example, "36" refers to a business that deals in "Electronic and Other Equipment." Adding "7" as a third digit to get "367" indicates that the business operates in "Electronic, Component and Accessories." The fourth digit distinguishes the specific industry sector, so a code of "3672" indicates that the business is concerned with "Printed Circuit Boards."

The U.S. Census Bureau, Bureau of Labor Statistics, Internal Revenue Service and Social Security Administration utilize SIC codes in their reporting, although SIC codes are also used in academic and business sectors. The Bureau of Labor Statistics updates the codes every three years and uses SIC to report on work force, wages and pricing issues. The Social Security Administration assigns SIC codes to businesses based on the descriptions provided by employers under the primary business activity entry on employer ID applications.

Over the years, the U.S. Census has identified three major limitations to using the SIC system. The first limitation surrounds its definition and mistaken classification of employee groups. For example, administrative assistants in the automotive industry support all levels of the business, yet the SIC defines these employees as part of the "Basic Sector" of manufacturing jobs when they should be reported as "Non-Basic." Secondly, SIC codes were developed for traditional industries prior to 1970. Business has changed considerably since then from manufacturing-based to mostly service-based. As a result, and thirdly the SIC has been slow to recognize new and emerging industries, such as those in the computer, software, and information technology sectors.

The Office of Management and Budget, or OMB, was tasked with revising the SIC system to reflect changing economic conditions. The OMB established the Economic Classification Policy Committee in 1992 to develop a new system representative of the current industrial climate. The result was the North American Industry Classification System, or NAICS, a collaborative effort between Canada, the U.S. and Mexico. NAICS expanded the four-digit SIC code to a six-digit code, and it provided more flexibility in handling emerging industries. The new code was implemented in Canada and the United States in 1997 and in Mexico one year later.

NAICS classified establishments (workplace) by their main output, instead of classifying them with the larger firm or organization of which the establishment was a part. This gives more precise information on establishment and worker activities than the SIC system, but changed the meaning of the classifications somewhat, making some time series of data hard to sustain accurately. Fort and Klimek (2016) found using longitudinal data on establishments that the switch from SIC to NAICS reclassified large numbers of workers differently by industry/sector than NAICS does, notably by reclassifying some from the Manufacturing sector into Services.

The SIC codes can be grouped into progressively broader industry classifications: industry group, major group, and division. The first 3 digits of the SIC code indicate the industry group, and the first two digits indicate the major group. Each division encompasses a range of SIC codes: 

To look at a particular example of the hierarchy, SIC code 2024 (ice cream and frozen desserts) belongs to industry group 202 (dairy products), which is part of major group 20 (food and kindred products), which belongs to the division of manufacturing.

The following table is from the SEC's website, which allows searching for companies by SIC code in its database of filings. The acronym NEC stands for "not elsewhere classified".




</doc>
<doc id="10162367" url="https://en.wikipedia.org/wiki?curid=10162367" title="Sunset industry">
Sunset industry

A sunset industry is an industry in decline, one that has passed its peak or boom periods. As one example, analogue recording technologies for audio or video have been supplanted by digital equivalents; although analogue equipment is still offered, sales have declined dramatically and are not expected to recover, so this segment of the market has been branded a 'sunset industry'. Many countries try to protect domestic sunset industries as they still provide important employment. They use protectionism policies to slow down the decline whilst sunrise industries develop.



</doc>
<doc id="20335379" url="https://en.wikipedia.org/wiki?curid=20335379" title="Pareto priority index">
Pareto priority index

The Pareto priority index (PPI), so named because of its connection with the Pareto principle, which is in turn named after the economist Vilfredo Pareto, can be used to prioritize several (quality improvement) projects. It is especially used in the surroundings of six sigma projects. It has first been established by AT&T.

The PPI is calculated as follows:

A high PPI suggests a high project priority.


</doc>
<doc id="23671111" url="https://en.wikipedia.org/wiki?curid=23671111" title="Reindustrialization">
Reindustrialization

Reindustrialization is the economic, social, and political process of organizing national resources for the purpose of re-establishing industries. The process proceeds as a result of a need to reinvigorate national economies.

China, India and South-East Asia were industrial powerhouse for major parts of human history. These countries and regions suffered great loss of industrial production due to colonization during 18th-20th centuries. After many decades of their independence, these countries have started reindustrializing themselves. In last three decades, share of these countries in global industrial output has increased manifold.
In context of declining share of OECD in world GDP and outsourcing of manufacturing and services, reindustrialization is considered as a contrasts to deindustrialization, the process under which industry, especially manufacturing, is relocated outside of a country's borders, and seeks to reverse that trend.

No longer the preserve of BRICs or South-East Asian countries, the notion of reindustrialization seems to be making inroads in the political discourse of populist policy makers in the developed economies of Western Europe and North America: notably France and the United States - where the rise of Trumponomics may potentially challenge some of the free trade tenets of the neoliberal "Washington Consensus".





</doc>
<doc id="23947154" url="https://en.wikipedia.org/wiki?curid=23947154" title="Thomson Reuters Business Classification">
Thomson Reuters Business Classification

The Thomson Reuters Business Classification (TRBC) is an industry classification of global companies; it is owned and operated by Thomson Reuters and is also the basis for Thomson Reuters Indices.

TRBC covers over 70,000 public companies from 130 countries and provides over 10 years of classification history. The classification consists of five levels of hierarchical structure. Each company is allocated an Industry, which falls under an Industry group, then Business Sector, which is then part of an overall economic sector. TRBC consists of 10 economic sectors, 28 business sectors, 54 industry groups, 143 industries and 837 Activities.

The top four levels are as follows:
TRBC is a market-based classification schema, similar to the GICS and ICB systems. These classify companies on the basis of degree of impact on markets, rather than establishment-based classification systems such as the North American Industry Classification System(NAICS).

TRBC is used primarily in the Financial Investment and Advisory space, where investors identify and select groups of comparable companies and look at trends. More specifically, this could be Investment Managers allocating funds and benchmarking their portfolios; Investment Bankers highlighting acquisition targets and opportunities for financial restructuring, or more generally corporates performing competitive analysis of their peers in the marketplace.





</doc>
<doc id="24734729" url="https://en.wikipedia.org/wiki?curid=24734729" title="Boilery">
Boilery

A boilery or boiling house is a place of boiling, much as a bakery is a place of baking. Boilery can also mean the process and equipment for boiling. Although they are now generally confined to factories, and usually boil industrial products rather than food, historically they were more common in daily life. Boileries are typically for boiling large quantities of fluid.

In the 17th to 19th centuries, boileries were used to convert sugarcane juice into raw sugar. These boileries were usually sturdy places, built from stone, and contained several copper kettles, each with a furnace beneath it., Sugarcane juice was treated with lime in large clarifying vats, before it was heated in copper kettles over individual furnaces. Due to their importance, many Western sugar plantations had their own boileries on site. 

Soap would also be made in a boiling house. 

Another use for a boilery is to make salt through the evaporation of brine water.,


</doc>
<doc id="1300528" url="https://en.wikipedia.org/wiki?curid=1300528" title="Industrial district">
Industrial district

Industrial district concept was initially used by Alfred Marshall to describe some aspects of the industrial organisation of nations. Industrial district (ID) is a place where workers and firms, specialised in a main industry and auxiliary industries, live and work. At the end of the 1990s the industrial districts in developed or developing countries had gained a recognised attention in international debates on industrialisation and policies of regional development.

The term was used the first time by Alfred Marshall in "The Principles of Economics" (1890, 1922). and in "Industry and Trade". Marshall talks of a... "thickly peopled industrial district".

The term was also used in political struggle. The 1917 handbook of the Industrial Workers of the World states:-

The term also appears in English literature. For instance, in a short story of 1920 by D. H. Lawrence, "You Touched Me" (aka 'Hadrian'):-

The strong specialisation of the workers and an appropriate support of public goods and institutions are supported by an "Industrial Atmosphere" related to a locally developing division of labour. Competences and knowledge are shared in informal way with processes of learning by doing and learning by using, and this promotes innovation over time. Local firms, families and civic organisations are connected by way of both market mechanisms and non-market mechanisms, like trust within bilateral or team exchanges, and collective action supporting the availability of local industrial, social and environmental infrastructure. Also, the notion that firms located in geographical proximity benefit from agglomeration effects in having a common or collective infrastructure is frequently mentioned as one of the main bases in the industrial district literature.

Within the study of economics, the term has evolved. Giacomo Becattini rediscovered the concept to describe the Italian industrial configuration of the middle of the 20th century. Since the 1980s, the dynamic industrial development in NEC (North, East and Centre) of Italy, where after the Second World War geographical concentration of specialised small and medium-sized enterprises (SME) raised up, led to an increasing attention to the Marshall' seminal works. A growing literature with an accompanying cloud of definitions of what is meant as an industrial district characterised the international dabate, e.g. Cluster. Industrial districts in Italy have a coherent location and a narrow specialisation profile, e.g. Prato in woollen fabric, Sassuolo in ceramic tiles or Brenta in ladies' footwear.

The success of SME-based Italian districts in the last century and the alternate fortunes of the current ones led to investigate more thoroughly some related aspects. The general characteristics of the ID are consistent with gradual change supported by processes of innovation from below, or decentralized industrial creativity. However, the globalisation processes asked non-gradual changes to the historical IDs and technical and organisational difficulties could hit them.

In the Industry 4.0 era, the specialised capabilities of these areas seem to have the possibility to encourage the emergence of the New Artisans, Maker in the context of adapted models like the "ID mark 3.0".



</doc>
<doc id="171855" url="https://en.wikipedia.org/wiki?curid=171855" title="Industrial policy">
Industrial policy

An industrial policy (IP) or industrial strategy of a country is its official strategic effort to encourage the development and growth of all or part of the economy, often focused on all or part of the manufacturing sector. The government takes measures "aimed at improving the competitiveness and capabilities of domestic firms and promoting structural transformation." A country's infrastructure (including transportation, telecommunications and energy industry) is a major enabler of the wider economy and so often has a key role in IP.

Industrial policies are interventionist measures typical of mixed economy countries. Many types of industrial policies contain common elements with other types of interventionist practices such as trade policy. Industrial policy is usually seen as separate from broader macroeconomic policies, such as tightening credit and taxing capital gains. Traditional examples of industrial policy include subsidizing export industries and import-substitution-industrialization (ISI), where trade barriers are temporarily imposed on some key sectors, such as manufacturing. By selectively protecting certain industries, these industries are given time to learn (learning by doing) and upgrade. Once competitive enough, these restrictions are lifted to expose the selected industries to the international market. More contemporary industrial policies include measures such as support for linkages between firms and support for upstream technologies.

The traditional arguments for industrial policies go back as far as the 18th century. Prominent early arguments in favor of selective protection of industries were contained in the 1791 "Report on the Subject of Manufactures" of US economist and politician Alexander Hamilton, as well as the work of German economist Friedrich List. List's views on free trade were in explicit contradiction to those of Adam Smith, who, in "The Wealth of Nations", said that "the most advantageous method in which a landed nation can raise up artificers, manufacturers, and merchants of its own is to grant the most perfect freedom of trade to the artificers, manufacturers, and merchants of all other nations." The arguments of List and others were subsequently picked up by scholars of early development economics such as Albert Hirschman and Alexander Gerschenkron, who called for the selective promotion of "key sectors" in overcoming economic backwardness.

The relationship between government and industry in the United States has never been a simple one, and the labels used in categorizing these relationships at different times are often misleading if not false. In the early nineteenth century, for example, "it is quite clear that the laissez faire label is an inappropriate one." In the US, an industrial policy was explicitly presented for the first time by the Jimmy Carter administration in August 1980, but it was subsequently dismantled with the election of Ronald Reagan the following year.

Historically, there is a growing consensus that most developed countries, including United Kingdom, United States, Germany, and France, have intervened actively in their domestic economy through industrial policies. These early examples are followed by interventionist ISI strategies pursued in Latin American countries such as Brazil, Mexico or Argentina. More recently, the rapid growth of East Asian economies, or the newly industrialized countries (NICs), has also been associated with active industrial policies that selectively promoted manufacturing and facilitated technology transfer and industrial upgrading. The success of these state-directed industrialization strategies are often attributed to developmental states and strong bureaucracies such as the Japanese MITI. According to Princeton's Atul Kohli, the reason Japanese colonies such as South Korea developed so rapidly and successfully was down to Japan exporting to its colonies the same centralised state development that it had used to develop itself. Precisely speaking, South Korea's development can be explained by the fact that it followed the similar industrial policies that UK, US and Germany implemented, and South Korea adopted Export-Oriented Industrialization (EOI) policy from 1964 based on its own decision contrary to the Import Substitution Industrialization (ISI) policy touted by international aid organizations and experts at that time. Many of these domestic policy choices, however, are now seen as detrimental to free trade and are hence limited by various international agreements such as WTO TRIMs or TRIPS. Instead, the recent focus for industrial policy has shifted towards the promotion of local business clusters and the integration into global value chains.

During the Reagan administration, an economic development initiative called Project Socrates was initiated to address US decline in ability to compete in world markets. Project Socrates, directed by Michael Sekora, resulted in a computer-based competitive strategy system that was made available to private industry and all other public and private institutions that impact economic growth, competitiveness and trade policy. A key objective of Socrates was to utilize advanced technology to enable US private institutions and public agencies to cooperate in the development and execution of competitive strategies without violating existing laws or compromising the spirit of "free market". President Reagan was satisfied that this objective was fulfilled in the Socrates system. Through the advances of innovation age technology, Socrates would provide "voluntary" but "systematic" coordination of resources across multiple "economic system" institutions including industry clusters, financial service organizations, university research facilities and government economic planning agencies. While the view of one US President and the Socrates team was that technology made it virtually possible for both to exist simultaneously, the industrial policy vs. free market debate continued as later under the George H. W. Bush administration, Socrates was labeled as industrial policy and de-funded.

Following the Financial Crisis of 2007–08, many countries around the world – including the US, UK, Australia, Japan and most countries of the European Union – have adopted industry policies. However contemporary industry policy generally accepts globalisation as a given, and focuses less on the decline of older industries, and more on the growth of emergent industries. It often involves government working collaboratively with industry to respond to challenges and opportunities. China is a prominent case where the central and subnational governments intervene in nearly all economic sectors and processes. Even though market mechanisms have gained in importance, the state control prevails. In order to catch-up and even overtake industrialised countries technologically, China's "state activities even extend to efforts to prevent the dominance of foreign investors and technologies in areas considered to be of key significance such as the strategic industries and the new technologies" including robotics and new energy vehicles.

The main criticism against industrial policy arises from the concept of government failure. Industrial policy is seen as harmful as governments lack the required information, capabilities and incentives to successfully determine whether the benefits of promoting certain sectors above others exceeds the costs and in turn implement the policies. While the East Asian Tigers provided successful examples of heterodox interventions and protectionist industrial policies, industrial policies such as import-substitution-industrialization (ISI) have failed in many other regions such as Latin America and Sub-Saharan Africa. Governments, in making decisions with regard to electoral or personal incentives, can be captured by vested interests, leading to industrial policies supporting local rent-seeking political elites while distorting the efficient allocation of resources by market forces.

Despite criticism, there is a consensus in recent development theory that says state interventions may be necessary when market failures occur. Market failures often exist in the form of externalities and natural monopolies. Such market failures may hinder the emergence of a well-functioning market and corrective industrial policies are required to ensure the allocative efficiency of a free market. Even relatively sceptical economists now recognise that public action can boost certain development factors "beyond what market forces on their own would generate." In practice, these interventions are often aimed at regulating networks, public infrastructure, R&D or correcting information asymmetries. While the current debate has shifted away from dismissing industrial policies overall, the best ways of promoting industrial policy are still widely debated.

One question is which kinds of industrial policy are most effective in promoting economic development. For example, economists debate whether developing countries should focus on their comparative advantage by promoting mostly resource- and labour-intensive products and services, or invest in higher-productivity industries, which may only become competitive in the longer term.

Debate also surrounds the issue whether government failures are more pervasive and severe than market failures. Some argue that the lower the government accountability and capabilities, the higher the risk of political capture of industrial policies, which may be economically more harmful than existing market failures.

Of particular relevance for developing countries are the conditions under which industrial policies may also contribute to poverty reduction, such as a focus on specific industries or the promotion of linkages between larger companies and smaller local enterprises.





</doc>
<doc id="153392" url="https://en.wikipedia.org/wiki?curid=153392" title="Industrial archaeology">
Industrial archaeology

Industrial archaeology (IA) is the systematic study of material evidence associated with the industrial past. This evidence, collectively referred to as industrial heritage, includes buildings, machinery, artifacts, sites, infrastructure, documents and other items associated with the production, manufacture, extraction, transport or construction of a product or range of products. The field of industrial archaeology incorporates a range of disciplines including archaeology, architecture, construction, engineering, historic preservation, museology, technology, urban planning and other specialties, in order to piece together the history of past industrial activities. The scientific interpretation of material evidence is often necessary, as the written record of many industrial techniques is often incomplete or nonexistent. Industrial archaeology includes both the examination of standing structures and sites that must be studied by an excavation.

The field of industrial archaeology developed during the 1950s in Great Britain, at a time when many historic industrial sites and artifacts were being lost throughout that country, including the notable case of Euston Arch in London. In the 1960s and 1970s, with the rise of national cultural heritage movements, industrial archaeology grew as a distinct form of archaeology, with a strong emphasis on preservation, first in Great Britain, and later in the United States and other parts of the world. During this period, the first organized national industrial heritage inventories were begun, including the Industrial Monuments Survey in England and the Historic American Engineering Record in the United States. Additionally, a number of regional and national IA organizations were established, including the North American-based Society for Industrial Archeology in 1971, and the British-based Association for Industrial Archaeology in 1973. That same year, the "First International Conference on the Conservation of Industrial Monuments" was held at Ironbridge in Shropshire. This conference led, in 1978, to the formal establishment of The International Committee for the Conservation of the Industrial Heritage (commonly known as "TICCIH") as a worldwide organization for the promotion of industrial heritage. The members of these and other IA groups are generally a diverse mix of professionals and amateurs who share a common interest in promoting the study, appreciation and preservation of industrial heritage resources.

Industrial archaeology covers a wide range of topics, from early ironworks and water-powered mills to large modern factories, as well as ancillary sites and structures such as worker housing, warehouses and infrastructure.

IA topics generally fall into one of four categories: 

Additionally, the topic of power generation (water, wind, steam, electric, etc.), while applicable to each of the four major IA categories, is sometimes considered its own category.
"See also:" List of industrial archaeology topics

The work of industrial archaeologists has led to greater public awareness of industrial heritage, including the creation of industry museums and the inclusion of sites on national and international historic cultural registers in many parts of the world. Notable examples include the Ironbridge Gorge Museums, Engelsberg Ironworks and Lowell National Historical Park, among many others.

"See also:" List of notable industrial heritage sites

One of the earliest forerunners of the mid-20th-century IA-movement was the Sheffield Trades Technical Societies, established in 1918 at the University of Sheffield to preserve elements of that city's industrial history. In 1920, the Newcomen Society was founded in Great Britain to foster the study of the history of engineering and technology, including many relics of the industrial revolution, such as steam engines, canals, iron bridges, machinery, and other historical artifacts. The Newcomen Society also established the "Journal of Industrial Archaeology" in 1964, first national IA-publication in the UK. Another early development was the formation of the Cornish Engines Preservation Committee (CEPC) in 1935, to rescue the Levant Mine and Beam Engine in Cornwall.

During the early 20th century, the historic preservation movement in the United States was still in its infancy. Most of the historic sites that received any attention were related to presidents and political figures, or the early colonial period. However, in 1925, one of the first industrial museums in the United States opened at Old Slater Mill, in Pawtucket, Rhode Island, at the site of the first successful textile mill in the country, built in 1793. The museum was founded by a group of business leaders with ties to the New England textile industry, during a period of decline due to Southern competition. The Old Slater Mill Association had the foresight to restore the old mill to its early 19th-century appearance, and fill it with a representative collection of textile machinery. In 1966, Old Slater Mill was declared a National Historic Landmark. In the early 1970s, Paul E. Rivard, then the director of the Old Slater Mill museum was one of the key figures in the founding of the Society for Industrial Archeology.

Another notable example of an early industrial archaeology site (one that predates the widespread IA-movement), is the Saugus Iron Works National Historic Site in Saugus, Massachusetts. It is the site of the first integrated iron works in North America, and was reconstructed in the 1950s after extensive archaeological excavations that began in the late 1940s by Roland W. Robbins.

The term “industrial archaeology” was popularised in Great Britain in 1955 by Michael Rix of Birmingham University, who wrote an article in "The Amateur Historian", about the need for greater study and preservation of 18th and 19th century industrial sites and relics of the British industrial revolution. In 1959, Council for British Archaeology (CBA) established an industrial archaeology research committee. The CBA soon developed a standardized record card for industrial monuments, which it distributed to volunteer groups around the UK. In 1965, the National Record of Industrial Monuments (NRIM) was created as a central archive for the record cards that had been collected by Angus Buchanan at the University of Bath. By the late 1960s, a number of local industrial archaeology groups had been formed in the UK, including the Gloucestershire Society for Industrial Archaeology in 1963, the Bristol Industrial Archaeological Society in 1967, and the Greater London Industrial Archaeology Society in 1968, among others. The primary mission of these local IA groups during this period was recording the remaining relics of industrial history, especially those deemed to be most at risk from urban redevelopment schemes. Depending on the condition of the site or artifact, recording typically consists of compiling a brief summary of the site's history through available records, including old maps or photographs, followed by detailed onsite measurements, drawings and photographs of the existing conditions of the site. Generally, a report is prepared and copies are filed in a public archive for the benefit of future generations. Most recording trips are intended to obtain a general overview of existing conditions, and are not meant to be an exhaustive study.

One of the first areas to be the subject of a systematic study of industrial archaeology was the Ironbridge Gorge in Shropshire, United Kingdom. This landscape developed from the 17th century as one of the first industrial landscapes in the world, and by the 18th century had a range of extractive industries as well as extensive iron making, ceramic manufacturing, and a series of early railways. The Ironbridge Gorge Museum Trust was established in 1967, and the significance of the Ironbridge Gorge was recognized in 1986 with its designation as a UNESCO World Heritage Site.

In 1963, British journalist Kenneth Hudson published the first IA text, titled "Industrial archaeology: an introduction". Four years later in April 1967, Hudson spoke at a seminar at the Smithsonian Institution in Washington, D.C., at what is considered the birth of the IA-movement in the United States. The seminar, which was attended by an audience of historic preservationists, museum professionals and others, focused on what was being done to promote the study of industrial archaeology in Great Britain and in Europe, and what needed to be done in the United States. By this time, a number of select historic industrial sites had been recorded by the Historic American Buildings Survey (HABS), which until then had mainly concentrated its efforts on architecturally significant sites. In 1967, the notable "New England Textile Mills Survey" (NETMS) was performed under the HABS umbrella, led by Robert M. Vogel, curator of the Division of Mechanical and Civil at the Smithsonian Museum of History and Technology. The NETMS was the first large-scale, industrial recording project by HABS. It was followed by the "New England Textile Mill Survey II" in 1968. The full reports from the 1967 and 1968 textile mill surveys are now available for public viewing on the Library of Congress website, including the Amoskeag Millyard in Manchester, New Hampshire, which was drastically altered soon after the survey was completed.

The success of the 1967 and 1968 mill surveys led to the formation of the Historic American Engineering Record (HAER) in 1969, in conjunction with the American Society of Civil Engineers. Since then, thousands of industrial / engineering sites and structures throughout the United States have been recorded by HAER, and are on record at the Library of Congress for public benefit.

By the early 1970s, industrial archaeology was, for the most part, being practiced in a few select countries by amateurs and professionals with different backgrounds and objectives. While much had been accomplished during the preceding decade, the "new" field of industrial archaeology was still struggling to gain acceptance as a true scholarly pursuit. In October 1971, a group of representatives from various museums, universities, and government organizations in the United States and Canada met in Washington, D.C. to establish a means to improve the exchange of ideas and information. The result was the first national-level, IA-related academic society in the world; the Society for Industrial Archeology (SIA). It was decided that the name of the Society would take on the US-Government's spelling of "archeology", instead of "archaeology". The first SIA newsletter was published in January 1972, with Robert M. Vogel as editor. In April of that same year the new group held its first annual conference in New York City. In 1975, the SIA introduced its academic journal, IA, The Journal of the Society for Industrial Archeology, with Emory Kemp as editor.

In 1973, the Association for Industrial Archaeology (AIA) was founded in Great Britain. It brought together the numerous local IA-groups that had been formed throughout the country. The AIA publishes a newsletter, "Industrial Archaeology News", along with its academic journal, "Industrial Archaeology Review", introduced in 1976. Many AIA members have been active in promoting the mission of IA throughout Europe and the rest of the world.

With the rapid decline of many established industries in North America and Europe during the 1970s, industrial archaeologists began to take on a new role of recording and preserving recently closed sites, as opposed to antique relics from earlier periods. Among the notable projects during this decade was the successful transformation of Sloss Furnaces in Birmingham, Alabama after it shut down in 1971 into an open air industrial museum. Sloss Furnaces was declared a NHL in 1981. The museum opened in 1983 and offers a variety of educational and civic programs.

In 1982, I.A.Recordings was founded by a small group of volunteers in the UK, to record past and present industries on film and video, as a resource for future generations.

During the 1980s, the scope of the field of industrial archaeology in Great Britain shifted away from what was taken place in North America, where the theories of social archaeology that were developed in the historical archaeology field began to be applied to the study of industrial sites. British industrial archaeologists meanwhile mainly focused on the recording of the technical aspects of sites and artifacts. One key development during this period was the shift toward thematic studies of monuments by type, including three initial textile mill surveys in Greater Manchester, Yorkshire and eastern Cheshire led by Keith Falconer.

Since 1990, there has been an ever-increasing awareness of the importance of industrial heritage, confirmed most prominently by the addition of numerous industrial sites to the UNESCO World Heritage List. Many preserved industrial sites have become a vital part of heritage tourism, including the European Route of Industrial Heritage (ERIH), established in 1999. Based on the success of the "Route der Industriekultur" in Ruhr, Germany, the ERIH has expanded to consist of sixteen routes in seven countries, with plans for new routes in additional countries. The number of industrial sites that have been preserved and converted to other uses such as apartments, public spaces or museums instead of being demolished is also a testament to the efforts of industrial archaeologists.

Industrial archaeology has gradually gained acceptance in the academic arena. In the UK, where the field developed largely from the efforts of volunteer researchers, the emergence of developer-funded projects in the past two decades has led to an increased presence of professional practitioners, with the application of theoretical archaeology methods such as landscape archaeology to the industrial setting. However, while many university archaeology departments now include the industrial period in their degree courses, industrial archaeology remains a fairly limited field of study, with few dedicated industrial archaeology programs, such as those offered at Michigan Technological University and the Ironbridge Institute.

Widespread appreciation of the importance of industrial heritage by the general public is still lacking in many areas, as the subject often maintains the perception of being "not old enough" to truly be considered archaeology. Additionally, there are often negative associations with neglected or abandoned industrial sites, including the social, economic and environmental consequences ("brownfield" sites). As with other history-based fields, one of the continuing challenges of industrial archaeologists throughout the world is the competition for ever-decreasing public funding for their research, educational and preservation projects. The sheer number of historic industrial sites and limited funding often means that many are still being lost to neglect, fire and demolition.

In 2003, the "Nizhny Tagil Charter" was adopted by TICCIH at its XII Congress in Nizhny Tagil, Russia. It is the international standard for the study, documentation, conservation and interpretation of the industrial heritage.

There are national industrial archaeology societies in many countries. They bring together people interested in researching, recording, preserving and presenting industrial heritage. Industrial architecture, mineral extraction, heritage-based tourism, power technology, adaptive reuse, and transport history are just some of the themes that are investigated by society members. Most groups publish periodic newsletters and host a variety of conferences, seminars and tours of IA-sites and still-active industries (known as process tours). IA organizations may also be involved in advising on historic conservation matters, or advising government units on revision or demolition of significant sites or buildings.




"Great Britain"

"United States"

The Society for Industrial Archeology has the following local chapters:




</doc>
<doc id="20182719" url="https://en.wikipedia.org/wiki?curid=20182719" title="Cartoning machine">
Cartoning machine

A cartoning machine or cartoner, is a packaging machine that forms cartons: erect, close, folded, side seamed and sealed cartons.

Packaging machines which form a carton board blank into a carton filled with a product or bag of products or number of products say into single carton, after the filling, the machine engages its tabs / slots to apply adhesive and close both the ends of carton completely sealing the carton.

Cartoning machines can be divided into two types:

A cartoning machine which picks a single piece from stack of folded carton and erects it, fills with a product or bag of products or number of products horizontally through an open end and closes by tucking the end flaps of the carton or applying glue or adhesive. The product might be pushed in the carton either through the mechanical sleeve or by pressurized air. For many applications however, the products are inserted into the carton manually. This type of Cartoning machine is widely used for packaging foodstuffs, confectionery, medicine, cosmetics, sundry goods, etc.

A cartoning machine which erects a folded carton, fills with a product or number of products vertically through an open end and closes by either tucking the end flaps of the carton or applying glue or adhesive, is called an end load cartoning machine. Cartoning machines are widely used for packaging bottled foodstuffs, confectionery, medicine, cosmetics, etc., and can vary based on the scale of business.




</doc>
<doc id="17856623" url="https://en.wikipedia.org/wiki?curid=17856623" title="Fabric structure">
Fabric structure

In architecture, fabric structures are forms of constructed fibers that provide end users a variety of aesthetic free-form building designs. Custom-made fabric structures are engineered and fabricated to meet worldwide structural, flame retardant, weather-resistant, and natural force requirements.
Fabric structures are considered a sub-category of tensile structure.

A fabric structure's material selection, proper design, engineering, fabrication, and installation are integral components to ensuring a sound structure.

Most fabric structures are composed of actual fabric rather than meshes or films. Typically, the fabric is coated and laminated with synthetic materials for increased strength, durability, and environmental resistance. Among the most widely used materials are polyesters laminated or coated with polyvinyl chloride (PVC), and woven fiberglass coated with polytetrafluoroethylene (PTFE).

The traditional fabric for fabric structures is light cotton twill, light canvas, or heavy proofed canvas.

Strength, durability, cost, and stretch make polyester material the most widely used in fabric structures. Polyesters that are laminated or coated with PVC films are usually the least expensive option for longer-term fabrications.
Laminates generally consist of vinyl films over woven or knitted polyester meshes (called scrims or substrates), while vinyl-coated polyesters usually have a high-count, high-tensile base fabric coated with a bondable substance that provides extra strength. Precontraint fabric is made by placing the polyester fabric under tension both before and during the coating process. This results in a weave that has increased dimensional stability.

A laminated fabric usually is composed of a reinforcing polyester scrim pressed between two layers of unsupported PVC film. For most fabric structure uses, however, it refers to two or more layers of fabric or film joined by heat, pressure, and an adhesive to form a single ply.

With an open-weave or mesh polyester scrim, the exterior vinyl films bond to themselves through the openings in the fabric. Heavier fabric scrims, however, are too tightly woven to allow the same bonding. In this case, an adhesive is used to bond the exterior films to the base fabric.

A good chemical bond is critical to both prevention of delamination and development of seam strengths. The seam is created when vinyl-coated fabrics are welded together. The adhesive enables the seam to meet shear forces and load requirements for a structure at all temperatures. The adhesive prevents wicking of moisture into the scrim’s fibers, which also prevents fungal growth or freezing that could affect the exterior coating's adhesion to the scrim. Adhesives are water-based to comply with EPA regulations.

Open-weave scrims generally make the fabric more economical, although this can also depend on the number and type of features that you require in the vinyl. Almost any color, UV resistance vinyl coated polyester, and colorfastness may be incorporated into the vinyl. However, the more features added, the higher the cost of the fabric.

Vinyl coated polyester is the most frequently used material for flexible fabric structures. It is made up of a polyester scrim, a bonding or adhesive agent, and exterior PVC coatings. The scrim supports the coating (which is initially applied in liquid form) and provides the tensile strength, elongation, tear strength, and dimensional stability of the resulting fabric. Vinyl-coated polyester is manufactured in large panels by heat-sealing an over-lap seam with either a radio-frequency welder or a hot-air sealer. A proper seam will be able to carry the load requirements for the structure. The seam area should be stronger than the original coated fabric when testing for tensile strength.

The base fabric's tensile strength is determined by the size (denier) and strength (tenacity) of the yarns and the number of yarns per linear inch or meter. The larger the yarn and the more yarns per inch, the greater the finished product's tensile strength.

The adhesive agent acts as a chemical bond between the polyester fibers and the exterior coating and also prevents wicking, or fibers absorbing water, which could result in freeze-thaw damage in the fabric.

The PVC coating liquid (vinyl Organisol or Plastisol) contains chemicals to achieve the desired properties of color, water and mildew resistance, and flame retardancy. Fabric can also be manufactured that contains high levels of light transmission or can be made completely opaque. After the coating has been applied to the scrim, the fabric is put through a heating chamber that dries the liquid coating. PVC coatings are available in a range of colors, although non-standard colors can be pricey. Colors may be subject to minimum order runs that allow the coating machine to clear out traces of any previous color.

Woven fiberglass coated with PTFE (Teflon or silicone) is also a widely used base material. Glass fibers are drawn into continuous filaments, which are then bundled into yarns. The yarns are woven to form a substrate. The fiberglass carries a high ultimate tensile strength, behaves elastically, and does not suffer from significant stress relaxation or creep. The PTFE coating is chemically inert, can withstand temperatures from 100 °F upwards to 450 °F+. It is also immune to radiation and can be cleaned with water. PTFE fiberglass is additionally Energy Star and Cool Roof Rating Council certified. During scientific tests of its solar properties, it was discovered that PTFE fiberglass membranes reflect as much as 73 percent of the sun’s energy while holding just seven percent on its exterior surface. Certain grades of PTFE fiberglass can absorb 14 percent of the sun’s energy while allowing 13 percent of natural daylight and seven percent of re-radiated energy (solar heat) to transmit through.

Because of its energy efficiency, high melting temperature and lack of creep, fiberglass-based fabrics have been the material of choice for stadium domes and other permanent structures, particularly in the United States. However, when properly constructed, polyester structures may be equally durable.

A number of polymers consisting mainly of polyethylene, polypropylene or combinations of the two are available for fabric structures.

PVDF woven fabric are available for fabric structures.

ePTFE woven fabric are available for fabric structures..

Blackout material, also known as blockout material, is an opaque fabric. Blackout fabric consists of a laminate that sandwiches an opaque layer between two white exterior layers. Heating and lighting of a structure may be controlled because the fabric does not allow light to permeate the top or walls. The opaque quality also prevents stains, dirt, repairs, or slightly mismatched panels on the structure's exterior from being noticed from the inside.

Most fabrics used for fabric structures have some form of topcoating applied to the exterior or coating to make cleaning easier. Topcoating provides a hard surface on the outside of the material, forming a barrier that aids in preventing dirt from sticking to the material, while allowing the fabric to be cleaned with water. As the material ages, the topcoating will eventually erode, exposing the fabric to dirt and making it more difficult to clean. The thicker the topcoating, the longer it will last. However, coatings that are too thick will embrittle and crack when folded.

There are several commonly used topcoatings:


When discussing fabric properties for use on a structure, there are several terms that are commonly used:

When deciding on a fabric it is imperative to keep certain fabric properties in mind. These include stress versus strain (unit load versus unit elongation), expected service life, the mechanisms of joining the material together (welding, gluing, etc.), and the fabric’s behavior in or around fire.

Stress versus strain data should be obtained in both uniaxial and biaxial forms. This information characterizes the fabric in terms of stiffness, elasticity, and plasticity. This is essential information when determining the material's response under load in a load-carrying application. Shear strength, shear strain, and Poisson's ratios, though difficult to obtain, are fundamental when analyzing a fabric as a structural material.

There can be multiple advantages to fabric structures over traditional buildings in certain scenarios. In some cases, no lighting is required as the fabric used is generally translucent, which makes it an energy efficient solution. Mobility: You can move them, either on wheels or relocate them completely. Savings: They cost about half of what a traditional structure costs.

Fabric properties: When discussing fabric properties for use on a structure, there are several terms that are commonly used:

Tensile strength is a basic indicator of relative strength. It is fundamental for architectural fabrics that function primarily in tension.
Tear Strength is important in that if a fabric ruptures in place, it generally will do so by tearing. This can occur when a local stress concentration or local damage results in the failure of one yarn, which thereby increases the stress on remaining yarns.

Adhesion strength is a measure of the strength of the bond between the base material and coating or film laminate that protects it. It is useful for evaluating the strength of welded joints for connecting strips of fabric into fabricated assembly.
Flame retardancy does not have the same meaning as flameproofing. Fabric that contains a flame-retardant coating can withstand even a very hot point source. However, it can still burn if a large ignition source is present.

Of course, other properties must be factored in when determining a material's suitability for a structure. To fully understand a fabric's value and usefulness, consider the following:



</doc>
<doc id="23838445" url="https://en.wikipedia.org/wiki?curid=23838445" title="Industrial safety system">
Industrial safety system

An industrial safety system is a countermeasure crucial in any hazardous plants such as oil and gas plants and nuclear plants. They are used to protect human, industrial plant, and the environment in case of the process going beyond the allowed control margins.

As the name suggests, these systems are not intended for controlling the process itself but rather protection. Process control is performed by means of process control systems (PCS) and is interlocked by the safety systems so that immediate actions are taken should the process control systems fail.

Process control and safety systems are usually merged under one system, called "Integrated Control and Safety System" (ICSS).
Industrial safety systems typically use dedicated systems that are SIL 2 certified at minimum; whereas control systems can start with SIL 1. SIL applies to both hardware and software requirements such as cards, processors redundancy and voting functions.

There are 2 main types of industrial safety systems in process industry:
A third system also exists which acts as a barrier and contains the spray out of hot oil & gases from flanges, valves & pipe joints. These systems are popularly known as safety spray shields and flange guards. The use of spray guards is mandated by OSHA.

These systems may also be redefined in terms of ESD/EDP levels as:

The safety shutdown system (SSS) shall shut down the facilities to a safe state in case of an emergency situation, thus protecting personnel, the environment and the asset. The safety shutdown system shall manage all inputs and outputs relative to emergency shutdown (ESD) functions (environment and personnel protection). This system might also be fed by signals from the main fire and gas system.

The main objectives of the fire and gas system are to protect personnel, environment, and plant (including equipment and structures).
The FGS shall achieve these objectives by:

Due to closing ESD valves in a process, there may be some trapped flammable fluids, and these must be released in order to avoid any undesired consequences (such as pressure increase in vessels and piping). For this, emergency depressurization (EDP) systems are used in conjunction with the ESD systems to release (to a safe location and in a safe manner) such trapped fluids.

Pressure safety valves or PSVs are usually used as a final safety solution when all previous systems fail to prevent any further pressure accumulation and protect vessels from rupture due to overpressure by their designed action.



</doc>
<doc id="31472958" url="https://en.wikipedia.org/wiki?curid=31472958" title="Modelling of particle breakage">
Modelling of particle breakage

Modelling of particle breakage is a process used in grinding.

Grinding is an important unit operation used in many industries, such as ceramics, composites, foods, minerals, paints, inks and pharmaceuticals. Current technology, however, is inefficient and power-intensive. It is, therefore, important that grinding processes are properly designed and grinding devices are operated at optimum operating conditions. 

There are two methods to model particle breakage: population balance model and discrete element method.

Population balance model (PBM) is often used to predict grinding performance, requiring the knowledge of selection and breakage function which are related to the energy distribution inside mills.

Grinding performance is a complex process depending strongly on the interactions between particles. The knowledge of energy distribution inside mills is critical to the determination of parameters in PBM model. While such information is difficult to obtain from experiments, numerical modelling based on discrete element method (DEM) can readily determine the energy distribution based on the well established contact mechanics.


</doc>
<doc id="16659679" url="https://en.wikipedia.org/wiki?curid=16659679" title="Kaldor's growth laws">
Kaldor's growth laws

Kaldor's growth laws are a series of three "laws" relating to the causation of economic growth.

Looking at the countries of the world now and through time Nicholas Kaldor noted a high correlation between living standards and the share of resources devoted to industrial activity, at least up to some level of income. Only New Zealand, Australia and Canada have become rich whilst relying mainly on agriculture. He proposed three laws on these empirical regularities:

Thirlwall (2003, p123–124) also reports Kaldor's highlighting of three subsidiary propositions which are also important to take into account. They are:


</doc>
<doc id="14551" url="https://en.wikipedia.org/wiki?curid=14551" title="Tertiary sector of the economy">
Tertiary sector of the economy

The service sector is the third of the three economic sectors of the three-sector theory. The others are the secondary sector (approximately the same as manufacturing), and the primary sector (raw materials).

The service sector consists of the production of services instead of end products. Services (also known as "intangible goods") include attention, advice, access, experience, and affective labor. The production of information has long been regarded as a service, but some economists now attribute it to a fourth sector, the quaternary sector.

The tertiary sector of industry involves the provision of services to other businesses as well as final consumers. Services may involve the transport, distribution and sale of goods from producer to a consumer, as may happen in wholesaling and retailing, pest control or entertainment. The goods may be transformed in the process of providing the service, as happens in the restaurant industry. However, the focus is on people interacting with people and serving the customer rather than transforming physical goods

It is sometimes hard to define whether a given company is part and parcel of the secondary or tertiary sector. And it is not only companies that have been classified as part of that sector in some schemes; government and its services such as police or military, and non-profit organizations such as charities or research associations can also be seen as part of that sector.

In order to classify a business as a service, one can use classification systems such as the United Nations' International Standard Industrial Classification standard, the United States' Standard Industrial Classification (SIC) code system and its new replacement, the North American Industrial Classification System (NAICS), the Statistical Classification of Economic Activities in the European Community (NACE) in the EU and similar systems elsewhere. These governmental classification systems have a first-level hierarchy that reflects whether the economic goods are tangible or intangible.

For purposes of finance and market research, market-based classification systems such as the Global Industry Classification Standard and the Industry Classification Benchmark are used to classify businesses that participate in the service sector. Unlike governmental classification systems, the first level of market-based classification systems divides the economy into functionally related markets or industries. The second or third level of these hierarchies then reflects whether goods or services are produced.

For the last 100 years, there has been a substantial shift from the primary and secondary sectors to the tertiary sector in industrialized countries. This shift is called tertiarisation. The tertiary sector is now the largest sector of the economy in the Western world, and is also the fastest-growing sector.
In examining the growth of the service sector in the early Nineties, the globalist Kenichi Ohmae noted that:

Economies tend to follow a developmental progression that takes them from a heavy reliance on agriculture and mining, toward the development of manufacturing (e.g. automobiles, textiles, shipbuilding, steel) and finally toward a more service-based structure. The first economy to follow this path in the modern world was the United Kingdom. The speed at which other economies have made the transition to service-based (or "post-industrial") economies has increased over time.

Historically, manufacturing tended to be more open to international trade and competition than services. However, with dramatic cost reduction and speed and reliability improvements in the transportation of people and the communication of information, the service sector now includes some of the most intensive international competition, despite residual protectionism.

Service providers face obstacles selling services that goods-sellers rarely face. Services are intangible, making it difficult for potential customers to understand what they will receive and what value it will hold for them. Indeed, some, such as consultants and providers of investment services, offer no guarantees of the value for price paid.

Since the quality of most services depends largely on the quality of the individuals providing the services, "people costs" are usually a high fraction of service costs. Whereas a manufacturer may use technology, simplification, and other techniques to lower the cost of goods sold, the service provider often faces an unrelenting pattern of increasing costs.

Product differentiation is often difficult. For example, how does one choose one investment adviser over another, since they are often seen to provide identical services? Charging a premium for services is usually an option only for the most established firms, who charge extra based upon brand recognition.

Examples of tertiary industries may include:


Below is a list of countries by service output at market exchange rates in 2016:



</doc>
<doc id="17065840" url="https://en.wikipedia.org/wiki?curid=17065840" title="Consciousness Industry">
Consciousness Industry

The Consciousness Industry is a term coined by author and theorist Hans Magnus Enzensberger, which identifies the mechanisms through which the human mind is reproduced as a social product. Foremost among these mechanisms are the institutions of mass media and education. According to Enzensberger, the mind industry does not produce anything specific; rather, its main business is to perpetuate the existing order of man's domination over man.

Hans Haacke elaborates on the consciousness industry as it applies to the arts in a wider system of production, distribution, and consumption. Haacke specifically implicates museums as manufacturers of aesthetic perception that fail to acknowledge their intellectual, political, and moral authority: "rather than sponsoring intelligent, critical awareness, museums thus tend to foster appeasement."



Museums: Managers of Consciousness 


</doc>
<doc id="14553" url="https://en.wikipedia.org/wiki?curid=14553" title="Secondary sector of the economy">
Secondary sector of the economy

The secondary sector of the economy including industries that produce a finished, usable product or are involved in construction.

This sector generally takes the output of the primary sector and manufactures finished goods or where they are suitable for use by other businesses, for export, or sale to domestic consumers. This sector is often divided into light industry and heavy industry. Many of these industries consume large quantities of energy and require factories and machinery to convert raw materials into goods and products. They also produce waste materials and waste heat that may cause environmental problems or cause pollution. The secondary sector supports both the primary and tertiary sector.

Some economists contrast wealth-producing sectors in an economy such as manufacturing with the service sector which tends to be wealth-consuming.[1] Examples of service may include retail, insurance, and government. These economists contend that an economy begins to decline as its wealth-producing sector shrinks.[2] Manufacturing is an important activity to promote economic growth and development. Nations that export manufactured products tend to generate higher marginal GDP growth which supports higher incomes and marginal tax revenue needed to fund the quality of life initiatives such as health care and infrastructure in the economy. The field is an important source for engineering job opportunities. Among developed countries, it is an important source of well-paying jobs for the middle class to facilitate greater social mobility for successive generations on the economy.


</doc>
<doc id="41722640" url="https://en.wikipedia.org/wiki?curid=41722640" title="Low carbon leakage">
Low carbon leakage

Low carbon leakage refers to the phenomenon of a country or a region losing its low carbon industries to another country or region. The underlying low carbon leakage trend can also be identified by looking into clean energy patent distribution around the World. The threat of low carbon leakage to the European Union has been repeatedly expressed by a number of European Politicians such as climate Commissioner Connie Hedegaard, UK Energy Secretary Edward Davey and others.

The low carbon leakage increases its relevance for the industrial competitiveness as the low carbon economy grows and has reached in 2013 $4 trillion and continues to grow at 4% per year. Not taking relevant part in this growth opportunity is also considered as low carbon leakage.

In October 2014 E&Y published a report "European Low Carbon Industries. A Health Check." specifically examining the state of the European low carbon sectors. The report lists a wide variety of cases in which "low carbon leakage" occurs or could occur.

Low carbon leakage could lead to a significant loss of competitiveness for Europe. According to former German federal minister Trittin: “In reality there is no carbon leakage. The danger of low carbon leakage is much more real." I typical example is the solar panel manufacturing that has developed rapidly in China and shrunk in Europe.


</doc>
<doc id="1417710" url="https://en.wikipedia.org/wiki?curid=1417710" title="Energy policy">
Energy policy

Energy policy is the manner in which a given entity (often governmental) has decided to address issues of energy development including energy production, distribution and consumption. The attributes of energy policy may include legislation, international treaties, incentives to investment, guidelines for energy conservation, taxation and other public policy techniques. Energy is a core component of modern economies. A functioning economy requires not only labor and capital but also energy, for manufacturing processes, transportation, communication, agriculture, and more. 

Concerning the term of energy policy, the importance of implementation of an eco-energy-oriented policy at a global level to address the issues of global warming and climate changes should be accentuated.

Although research is ongoing, the "human dimensions" of energy use are of increasing interest to business, utilities, and policymakers. Using the social sciences to gain insights into energy consumer behavior can empower policymakers to make better decisions about broad-based climate and energy options. This could facilitate more efficient energy use, renewable energy commercialization, and carbon emission reductions. Access to energy is also critical for basic social needs, such as lighting, heating, cooking, and health care. As a result, the price of energy has a direct effect on jobs, economic productivity and business competitiveness, and the cost of goods and services.

A national energy policy comprises a set of measures involving that country's laws, treaties and agency directives. The energy policy of a sovereign nation may include one or more of the following measures:


Frequently the dominant issue of energy policy is the risk of supply-demand mismatch (see: energy crisis). Current energy policies also address environmental issues (see: climate change), particularly challenging because of the need to reconcile global objectives and international rules with domestic needs and laws. Some governments state explicit energy policy, but, declared or not, each government practices some type of energy policy. Economic and energy modelling can be used by governmental or inter-governmental bodies as an advisory and analysis tool (see: economic model, POLES).

There are a number of elements that are naturally contained in a national energy policy, regardless of which of the above measures was used to arrive at the resultant policy. The chief elements intrinsic to an energy policy are:


Even within a state it is proper to talk about energy policies in plural. Influential entities, such as municipal or regional governments and energy industries, will each exercise policy. Policy measures available to these entities are lesser in sovereignty, but may be equally important to national measures. In fact, there are certain activities vital to energy policy which realistically cannot be administered at the national level, such as monitoring energy conservation practices in the process of building construction, which is normally controlled by state-regional and municipal building codes (although can appear basic federal legislation).

Brazil is the 10th largest energy consumer in the world and the largest in South America. At the same time, it is an important oil and gas producer in the region and the world's second largest ethanol fuel producer. The governmental agencies responsible for energy policy are the Ministry of Mines and Energy (MME), the National Council for Energy Policy (CNPE, in the Portuguese-language acronym), the National Agency of Petroleum, Natural Gas and Biofuels (ANP) and the National Agency of Electricity (ANEEL). State-owned companies Petrobras and Eletrobrás are the major players in Brazil's energy sector.

Currently, the major issues in U.S. energy policy revolve around the rapidly growing production of domestic and other North American energy resources. The U.S. drive toward energy independence and less reliance on oil and coal is fraught with partisan conflict because these issues revolve around how best to balance both competing values, such as environmental protection and economic growth, and the demands of rival organized interests, such as those of the fossil fuel industry and of the newer renewable energy businesses. 

Although the European Union has legislated, set targets, and negotiated internationally in the area of energy policy for many years, and evolved out of the European Coal and Steel Community, the concept of introducing a mandatory common European Union energy policy was only approved at the meeting of the European Council on October 27, 2005 in London. Following this the first policy proposals, "Energy for a Changing World", were published by the European Commission, on January 10, 2007. The most well known energy policy objectives in the EU are 20/20/20 objectives, binding for all EU Member States. The EU is planning to increase the share of renewable energy in its final energy use to 20%, reduce greenhouse gases by 20% and increase energy efficiency by 20%.

In September 2010, the German government adopted a set of ambitious goals to transform their national energy system and to reduce national greenhouse gas emissions by 80 to 95% by 2050 (relative to 1990).
This transformation become known as the "Energiewende". Subsequently, the government decided to the phase-out the nation's fleet of nuclear reactors, to be complete by 2022.
As of 2014, the country is making steady progress on this transition.

The energy policy of the United Kingdom has achieved success in reducing energy intensity (but still really high), reducing energy poverty, and maintaining energy supply reliability to date. The United Kingdom has an ambitious goal to reduce carbon dioxide emissions for future years, but it is unclear whether the programs in place are sufficient to achieve this objective (the way to be so efficient as France is still hard). Regarding energy self sufficiency, the United Kingdom policy does not address this issue, other than to concede historic energy self sufficiency is currently ceasing to exist (due to the decline of the North Sea oil production). With regard to transport, the United Kingdom historically has a good policy record encouraging public transport links with cities, despite encountering problems with high speed trains, which have the potential to reduce dramatically domestic and short-haul European flights. The policy does not, however, significantly encourage hybrid vehicle use or ethanol fuel use, options which represent viable short term means to moderate rising transport fuel consumption. Regarding renewable energy, the United Kingdom has goals for wind and tidal energy. The White Paper on Energy, 2007, set the target that 20% of the UK's energy must come from renewable sources by 2020.

The Soviet Union was the largest energy provider in the world until the late 1980s. Russia, one of the world's energy superpowers, is rich in natural energy resources, the world’s leading net energy exporter, and a major supplier to the European Union. The main document defining the energy policy of Russia is the Energy Strategy, which initially set out policy for the period up to 2020, later was reviewed, amended and prolonged up to 2030. While Russia has also signed and ratified the Kyoto Protocol. Numerous scholars note that Russia uses its energy exports as a foreign policy instrument towards other countries. 

In September 2016, both chambers of the Swiss Parliament voted for the "Energiestrategie 2050", a set of measures to replace electrical energy produced by atomic reactors with renewable energy, reduce the use of fossil fuel and increase the efficiency of energy consumption. This decision was challenged by a national Referendum.
In May 2017, the Swiss people voted against the Referendum, thereby confirming the decision taken by the parliament.

The energy policy of India is characterized by trades between four major drivers:

In recent years, these challenges have led to a major set of continuing reforms, restructuring and a focus on energy conservation.

The energy policy of Thailand is characterized by 1) increasing energy consumption efficiency, 2) increasing domestic energy production, 3) increasing the private sector's role in the energy sector, 4) increasing the role of market mechanisms in setting energy prices. These policies have been consistent since the 1990s, despite various changes in governments. The pace and form of industry liberalization and privatization has been highly controversial.

The first National Energy Policy (NEP) of Bangladesh was formulated in 1996 by the Ministry of Power, Energy and Mineral resources to ensure proper exploration, production, distribution and rational use of energy resources to meet the growing energy demands of different zones, consuming sectors and consumers groups on a sustainable basis.[1] With rapid change of global as well as domestic situation, the policy was updated in 2004. The updated policy included additional objectives namely to ensure environmentally sound sustainable energy development programmes causing minimum damage to environment, to encourage public and private sector participation in the development and management of energy sector and to bring the entire country under electrification by the year 2020.[2]

Australia's energy policy features a combination of coal power stations and hydro electricity plants. The Australian government has decided not to build nuclear power plants, although it is one of the world's largest producers of uranium.





</doc>
<doc id="12073594" url="https://en.wikipedia.org/wiki?curid=12073594" title="Sisyphism">
Sisyphism

Sisyphism is a term used by French classical liberal theorist, political economist, and member of the French assembly, Frédéric Bastiat to ridicule those that think that greater productivity causes poverty by increasing unemployment. The term derives from Sisyphus, the mythological king of Ephyra, punished for chronic deceitfulness by being compelled to roll an immense boulder up a hill, only to watch it roll back down, and to repeat this action forever.


</doc>
<doc id="39314328" url="https://en.wikipedia.org/wiki?curid=39314328" title="Eco-industrial development">
Eco-industrial development

Eco-industrial development (EID) is a framework for industry to develop while reducing its impact on the environment. It uses a closed loop production cycle to tackle a broad set of environmental challenges such as soil and water pollution, desertification, species preservation, energy management, by-product synergy, resource efficiency, air quality, etc.

Mutually beneficial connections among industry, natural systems, energy, material and local communities become central factors in designing industrial production processes.

The approach itself is largely voluntary and market-driven but often pressed ahead by favorable government treatment or efforts of development co-operation.

Since the early 1990s the idea of EID evolved from biological symbiosis. This concept was adapted by industrial ecologists in the search for innovative approaches to solve problems of waste, energy shortage and degradation of the environment. A continuous approach towards improving both environmental and economic outcomes is used.

In 1992, the international community officially connected development co-operation to sustainable environmental protection for the first time. At the United Nations Conference on Environment and Development (UNCED) in Rio de Janeiro, Brazil nearly 180 states signed the conference's Rio Declaration. Although non-binding, the Rio Declaration on Environment and Development laid out 27 principles that shall guide the increasing inter-connectedness of development cooperation and sustainability. Moreover, the documents drafting was accompanied by a presentation describing the idea of eco-industrial development for the first time.

In the following years, EID became popular throughout the United States. The recently elected Clinton administration installed a summit of business, labor, government and environmental protection representatives to further develop the approach. This summit established the idea of eco-industrial parks but soon realized that at first a more efficient management of raw materials, energy and waste has to be achieved.

Since then, the broad goals and application principles of EID have hardly changed and only became adapted to the rapid technological progress.

In 2012 the IGEP Foundation, for the promotion of trade, published a report called "Pathway to Eco Industrial Development in India – Concepts and Cases".

The field is researched by the Nation Centre for Eco-Industrial Development, a joint project by the University of Southern California and Cornell University.

The primary goal of eco-industrial development is a significant and continuous improvement in both business and environmental performance. Herein, the notion of industry not only relates to private-sector manufacturing but also covers state-owned enterprise, the service sector as well as transportation. EID's twin guideline is reflected specifically in the "eco" of eco-industrial as it resembles ecology (decrease in pollution and waste) and economy (increase in commercial success) at the same time. In order to build a framework of defining an enterprise's sustainable performance at the micro level, resource use optimization, minimization of waste, cleaner technologies and pollution limits are used in achieving a broad range of goals in EID: 


Eco-industrial development hence explores the possibility of improvement at the local level. In unique case-to-case analyses, particular geography, human potential or business climate are investigated. In contrast to the widespread race for top-down governmental support such as tax cuts, EID emphasizes locally achievable success and rooms for improvement. As a result, purposeful enforcements of action plans can make a large difference by optimizing the interaction of business, community and ecological systems.

Eco-industrial development includes and employs four major conceptual instruments. Each of the approaches intends to combine the seemingly antithetic processes of industrial development and bolstering sustainability.





</doc>
<doc id="42713269" url="https://en.wikipedia.org/wiki?curid=42713269" title="Wedge-based mechanical exfoliation">
Wedge-based mechanical exfoliation

Wedge-based mechanical exfoliation is a method that involves the use of an ultra-sharp single crystal diamond wedge to penetrate inside a material and cleave a thin layer of material. It was proposed to produce few layers of graphene from a bulk highly ordered pyrolytic graphite (HOPG).

Molecular dynamics simulations studies have been performed to understand how and under what conditions graphene layers separate, fold and shear during the wedge-based mechanical exfoliation machining technique. Molecular simulations of initial wedge engagement show that the entry location of the wedge tip, vis-a-vis the nearest graphene layer, plays a key role in determining whether layers separate or fold and which layers and how many of them fold.


</doc>
<doc id="3683130" url="https://en.wikipedia.org/wiki?curid=3683130" title="Machine shop">
Machine shop

A machine shop is a room, building, or company where machining, a form of subtractive manufacturing, is done. In a machine shop, machinists use machine tools and cutting tools to make parts, usually of metal or plastic (but sometimes of other materials such as glass or wood). A machine shop can be a small business (such as a job shop) or a portion of a factory, whether a toolroom or a production area for manufacturing. The building construction and the layout of the place and equipment vary, and are specific to the shop; for instance, the flooring in one shop may be concrete, or even compacted dirt, and another shop may have asphalt floors. A shop may be air-conditioned or not; but in other shops it may be necessary to maintain a controlled climate. Each shop has its own tools and machinery which differ from other shops in quantity, capability and focus of expertise.

The parts produced can be the end product of the factory, to be sold to customers in the machine industry, the car industry, the aircraft industry, or others. It may encompass the frequent machining of customized components. In other cases, companies in those fields have their own machine shops.

The production can consist of cutting, shaping, drilling, finishing, and other processes, frequently those related to metalworking. The machine tools typically include metal lathes, milling machines, machining centers, multitasking machines, drill presses, or grinding machines, many controlled with computer numerical control (CNC). Other processes, such as heat treating, electroplating, or painting of the parts before or after machining, are often done in a separate facility.

A machine shop can contain some raw materials (such as bar stock for machining) and an inventory of finished parts. These items are often stored in a warehouse. The control and traceability of the materials usually depend on the company's management and the industries that are served, standard certification of the establishment, and stewardship.

A machine shop can be a capital intensive business, because the purchase of equipment can require large investments. A machine shop can also be labour-intensive, especially if it is specialized in repairing machinery on a job production basis, but production machining (both batch production and mass production) is much more automated than it was before the development of CNC, programmable logic control (PLC), microcomputers, and robotics. It no longer requires masses of workers, although the jobs that remain tend to require high talent and skill. Training and experience in a machine shop can both be scarce and valuable.

Methodology, such as the practice of 5S, the level of compliance over safety practices and the use of personal protective equipment by the personnel, as well as the frequency of maintenance to the machines and how stringent housekeeping is performed in a shop, may vary widely from one place to another business.

The first machine shops started to appear in the 19th century when the Industrial Revolution was already long underway. Before the industrial revolution parts and tools were produced in workshops in local villages and cities on small-scale often for a local market. The first machinery that made possible the Industrial Revolution were also developed in similar workshops.

The production machines in the first factories were built on site, where every part was still individually made to fit. After some time those factories started their own workshops, where parts of the existing machinery were repaired or modified. In those days textiles were the dominant industry, and these industries started to further develop their own machine tools.

Further development early in the 19th century in England, Germany and Scotland of machine tools and cheaper methods for production of steel, such as the Bessemer steel, triggered the Second Industrial Revolution, which culminated in early factory electrification, mass production and the production line. The machine shop emerged as Burghardt called, a "place in which metal parts are cut to the size required and put together to form mechanical units or machines, the machines so made to be used directly or indirectly in the production of the necessities and luxuries of civilization."

The rise of machine shops and their specific manufacturing and organizational problems triggered the early job shop management pioneers, whose theories became known with as scientific management. One of the earliest publications in this field was Horace Lucian Arnold, who in 1896 wrote a first series of articles about "Modern Machine-Shop Economics." This work stretched out from production technology, production methods and factory lay out to time studies, production planning, and machine shop management. A series of publications on these topic would follow. In 1899 Joshua Rose published the book "Modern machine-shop practice," about the operation, construction, and principles of shop machinery, steam engines, and electrical machinery.

In 1903 the "Cyclopedia of Modern Shop Practice" was published with Howard Monroe Raymond as Editor-in-Chief, and in the same year Frederick Winslow Taylor published his "Shop management; a paper read before the American society of mechanical engineers. New York." Taylor had started his workmanship as a machine-shop laborer at Midvale Steel Works in 1878, and worked his way up to machine shop foreman, research director, and finally chief engineer of the works. As independent consulting engineer one of his first major assignments was in 1898 at Bethlehem Steel was to solve an expensive machine-shop capacity problem.

In 1906 Oscar E. Perrigo published the popular book "Modern machine shop," construction the equipment and management of machines shops. The first part of "Modern machine shop," Perrigo (1906) focussed on the physical construction of the building, and presented a model machine shop. With this model machine shop Perrigo explored the way the space in factories could be organized. This was not uncommon in his days. Many industrial engineers, like Alexander Hamilton Church, J. Slater Lewis, Hugo Diemer etc., published plans for some new industrial complex.

These works among others cumulated in the scientific management movement on which Taylor in 1911 wrote his famous "The Principles of Scientific Management," a seminal text of modern organization and decision theory, with a significant part dedicated to the organization of machine shops. The introduction of new cutting materials as high-speed steel, and better organization of the production by implementing new scientific management methods such as planning boards (see image), significantly improved machine shop productivity and efficiency of machine shops. In the course of the 20th century these further increased with the further development of technology.

In the early 20th century, the power for the machine tools was still supplied by a mechanical belt, which was powered by a central steam engine. In the course of the 20th century electric motors took over the power supply of the machine tools.

As materials and chemical substances, including cutting oil, become more sophisticated, the awareness of the impact to the environment slowly grew. In parallel to the acknowledgement of the ever-present reality of accidents and potential occupational injury, the sorting of scrap materials for recycling and the disposal of refuse evolved in an area related to the environment, safety, and health. In regulated machine shops this would turn into a constant practice supported by what would be a discipline known as EHS (for environment, health and safety), or of a similar name, such as HQSE that would include quality assurance.

In the second part of the 20th century, automation started with numerical control (NC) automation, and computer numerical control (CNC).

Digital instruments for quality control and inspection become widely available, and the utilization of lasers for precision measurements became more common for the larger shops that can afford the equipment. 

Further integration of information technology into machine tools lead to beginning of computer-integrated manufacturing. Production design and production became integrated into CAD/CAM, and production control became integrated in enterprise resource planning.

The introduction of industrial robots in the second part of the 20th century further increased factory automation. Typical applications of robots include welding, painting, assembly, pick and place (such as packaging, palletizing and SMT), product inspection, and testing. As a result of this introduction the machine shop also "has been modernized to the extent that robotics and electronic controls have been introduced into the operation and control of machines. For small machine shops, though, having robots is more of an exception.

A machine is a tool containing one or more parts that uses energy to perform an intended action. Machines are usually powered by mechanical, chemical, thermal, or electrical means, and are often motorized. Historically, a power tool also required moving parts to classify as a machine. However, the advent of electronics has led to the development of power tools without moving parts that are considered machines.

Machining is any of various processes in which a piece of raw material is cut into a desired final shape and size by a controlled material-removal process. The many processes that have this common theme, controlled material removal, are today collectively known as subtractive manufacturing, in distinction from processes of controlled material addition, which are known as additive manufacturing. Exactly what the "controlled" part of the definition implies can vary, but it almost always implies the use of machine tools (in addition to just power tools and hand tools).

A machine tool is a machine for shaping or machining metal or other rigid materials, usually by cutting, boring, grinding, shearing, or other forms of deformation. Machine tools employ some sort of tool that does the cutting or shaping. All machine tools have some means of constraining the workpiece and provide a guided movement of the parts of the machine. Thus the relative movement between the workpiece and the cutting tool is controlled or constrained by the machine to at least some extent, rather than being entirely "offhand" or "freehand".

Some machine shops are better organized than other, and some places are keep cleaner than other establishments. In some instances, the shop is swept minutes before the end of every shift, and in other cases, there’s no schedule or routine, or the cycle for sweeping and cleaning is more relaxed.

When it comes to machines, in some places the care and maintenance of the equipment is paramount, and the swarf (commonly known as chips) produced after parts have been machined, are removed daily, and then the machine is air-blown and wiped clean; while in other machine shops, the chips are left in the machines until is an absolute necessity to remove them; the second instance is not advisable.

Quality assurance, quality control and inspection, are terms commonly used interchangeably. The accuracy and precision to be attain depend on several determining factors. Since not all machines have the same level of reliability and capability to execute predictable finished results within certain tolerances, nor all manufacturing processes achieve the same range of exactness, the machine shop is then limited to its own dependability in delivering the desire outcomes. Subsequently, subject to the rigor declared by the customer, the machine shop may be required to undergo a verification and validation even prior to the issuance and acknowledgment of an order.

The machine shop may have a specific area established for measuring and inspecting the parts in order to confirm compliance, while other shops only rely on the inspections performed by the machinists and fabricators. For instance, in some shops a granite, calibrated, surface plate may be shared by different departments, and in other shops, the lathes, the mills, etc, may have their own, or may not have one at all.

The location, and orientation of the machines is important. Preferably, some prior thought has been given in the positioning of the equipment; likely not as meticulously as in a plant layout study, the closeness of the machines, the types of machines, were the raw material are received and kept, as well as other factors, including ventilation, are taken in account to establish the initial layout of the machine shop. A routing diagram and daily operations may dictate the need to rearrange.




</doc>
<doc id="44362658" url="https://en.wikipedia.org/wiki?curid=44362658" title="Machine industry">
Machine industry

The machine industry or machinery industry is a subsector of the industry, that produces and maintains machines for consumers, the industry, and most other companies in the economy.

This machine industry traditionally belongs to the heavy industry. Nowadays, many smaller companies in this branch are considered part of the light industry. Most manufacturers in the machinery industry are called machine factories.

The machine industry is a subsector of the industry that produces a range of products from power tools, different types of machines, and domestic technology to factory equipment etc. On the one hand the machine industry provides:
These means of production are called capital goods, because a certain amount of capital is invested. Much of those production machines require regular maintenance, which becomes supplied specialized companies in the machine industry.

On the other end the machinery industry supplies consumer goods, including kitchen appliances, refrigerators, washers, dryers and a like. Production of radio and television, however, is generally considered belonging to the electrical equipment industry. The machinery industry itself is a major customer of the steel industry.

The production of the machinery industry varies widely from single-unit production and series production to mass production. Single-unit production is about constructing unique products, which are specified in specific customer requirements. Due to modular design such devices and machines can often be manufactured in small series, which significantly reduces the costs. From a certain stage in the production, the specific customer requirements are build in, and the unique product is created.

The machinery industry came into existence during the Industrial Revolution. Companies in this emerging field grew out of iron foundries, shipyards, forges and repair shops. Often companies were a combination of machine factory and shipyard. Early in the 20th century several motorcycle and automobile manufacturers began their own machine factories.

Prior to the industrial revolution a variety of machines existed such as clocks, weapons and running gear for mills (watermill, windmill, horse mill etc.) Production of these machines were on much smaller scale in artisan workshops mostly for the local or regional market. With the advent of the industrial revolution manufacturing began of composite tools with more complex construction, such as steam engines and steam generators for the evolving industry and transport. In addition, the emerging machine factories started making machines for production machines as textile machinery, compressors, agricultural machinery, and engines for ships.

During the first decades of the industrial revolution in England, from 1750, there was a concentration of labor usually in not yet mechanized factories. There were all kinds of new machines invented, which were initially made by the inventors themselves. Early in the 18th century, the first steam engines, the Newcomen engine, came into use throughout Britain and Europe, principally to pump water out of mines.

In the 1770s James Watt significantly improved this design. He introduced a steam engine easy employable to supply a large amounts of energy, which set the mechanization of factories underway. In England certain cities concentrated on making specific products, such as specific types of textiles or pottery. Around these cities specialized machinery industry arose in order to enable the mechanization of the plants. Hereby late in the 18th century arose the first machinery industry in the UK and also in Germany and Belgium.

The Industrial Revolution received a further boost with the upcoming railways. These arose at the beginning of the 19th century in England as innovation in the mining industry. The work in coal mines was hard and dangerous, and so there was a great need for tools to ease this work. In 1804, Richard Trevithick placed the first steam engine on rails, and was in 1825 the Stockton and Darlington Railway was opened, intended to transport coals from the mine to the port. In 1835 the first train drove in continental Europe between Mechelen and Brussels, and in the Netherlands in 1839 the first train drove between Amsterdam and Haarlem. For the machinery industry this brought all sorts of new work with new machinery for metallurgy, machine tool for metalworking, production of steam engines for trains with all its necessities etc.

In time the market for the machine industry became wider, specialized products were manufactured for a greater national and often international market. For example, it was not uncommon in the second half of the 19th century that American steelmakers ordered their production in England, where new steelmaking techniques were more advanced. In the far east Japan would import these product until the early 1930s, the creation of an own machinery industry got underway. .

The term "machinery industry" came into existence later in the 19th century. One of the first times this branch of industry was recognized as such, and was investigated, was in a production statistics of 1907 created by the British Ministry of Trade and Industry. In this statistic the output of the engineering industry, was divided into forty different categories, including for example, agricultural machinery, machinery for the textile industry and equipment, and parts for train and tram.

The inventions of new propulsion techniques based on electric motors, internal combustion engines and gas turbines brought a new generation of machines in the 20th century from cars to household appliances. Not only the product range of the machinery industry increased considerably, but especially smaller machines could also deliver products in much greater numbers fabricated in mass production. With the rise of mass production in other parts of the industry, there was also a high demand for manufacturing and production systems, to increase the entire production.

Shortage of labor in agriculture and industry at the beginning of the second half of the 20th century, raised the need for further mechanization of production, which required for more specific machines. The rise of the computer made further automation of production possible, which in turn set new demands on the machinery industry.

The machinery industry produces different kind of products, for example, engines, pumps, logistics equipment; for different kind of markets from the agriculture industry, food & beverage industry, manufacturing industry, health industry, and amusement industry till different branches of the consumer market. As such companies in the machine industry can be classified by product of market.
In the world of today, all kinds of Industry classifications exists. Some classifications recognize the machine industry as a specific class, and offer a subdivision for this field. For example, the Dutch Standard Industrial Classification of 1993, developed by the Statistics Netherlands, give the following breakdown of the machinery industry:

This composition of the machinery industry has been significantly altered with the latest revision of the Dutch Standard Industrial Classification of 1993. The Standard Industrial Classification of 1974 broke down the machinery industry into nine sectors:
It may be clear that classification is by markets, and the more recent classification is by product.

The machine industry makes a very diverse range of products. A selection:

In Germany, in 2011 about 900,000 people were employed in the machine industry and an estimated of 300,000 abroad. The combined turnover of the sector was €130 billion, of which 60% came from export. There were about 6,600 active companies, and 95% of those companies employed less than 500 people. Each employee generated an average of 148,000 Euro. Some of the largest companies in Germany are DMG Mori Seiki AG, GEA Group, Siemens AG, and ThyssenKrupp.

In the French machinery industry in 2009 about 650,000 people were employed, and the sector generated a turnover of 98 billion euros. Because of the crisis, the turnover of the sector had fallen by 15 percent. Due to stronger consumer spending and continuing demand from the energy sector and transport sector, the damage of the crisis was still limited.
Alternatively, some companies decided to focus their request on used industrial equipment. This guarantee attractive prices and better time delivery. 

In the Netherlands in 1996, a total of some 93,000 workers were employed in the machinery industry, with approximately 2,500 companies present. In 1000 of these companies there were working 20 or more employees. In the Netherlands, according to the Chamber of Commerce in this subsector of the industry in 2011 some 15,000 companies were active. Some of the largest companies in the Netherlands are Lely (company), Philips and Stork B.V..

U.S. machinery industries had total domestic and foreign sales of $413.7 billion in 2011. The United States is the world’s largest market for machinery, as well as the third-largest supplier. American manufacturers held a 58.5 percent share of the U.S. domestic market.




</doc>
<doc id="251540" url="https://en.wikipedia.org/wiki?curid=251540" title="Private sector">
Private sector

The private sector is the part of the economy, sometimes referred to as the citizen sector, which is owned by private individuals or groups, usually as a means of enterprise for profit, rather than being owned by the State.

The private sector employs most of the workforce in some countries. In private sector, activities are guided by the motive to earn money.

A 2013 study by the International Finance Corporation (part of the World Bank Group) identified that 90 percent of jobs in developing countries are in the private sector.

In free enterprise countries, such as the United States of America, the private sector is wider, and the state places fewer constraints on firms. In countries with more government authority, such as China, the public sector makes up most of the economy.

States legally regulate the private sector. Businesses operating within a country must comply with the laws in that country.

In some cases, usually involving multinational corporations that can pick and choose their suppliers and locations based on their perception of the regulatory environment, local state regulations have resulted in uneven practices within one company. For example, workers in one country may benefit from strong labour unions, while workers in another country have very weak laws supporting labour unions, even though they work for the same employer. In some cases, industries and individual businesses choose to self-regulate by applying higher standards for dealing with their workers, customers, or the environment than the minimum that is legally required of them.

There can be negative effects from the private sector. In the early 1980s, the Corrections Corporation of America pioneered the idea of running prisons for a profit. Today, corporate-run prisons hold eight percent of America's inmates. Since it is from the private sector, their main priority isn't rehabilitation, but profit. This has resulted in many human rights violations across the United States.



</doc>
<doc id="659359" url="https://en.wikipedia.org/wiki?curid=659359" title="Industrial processes">
Industrial processes

Industrial processes are procedures involving chemical, physical, electrical or mechanical steps to aid in the manufacturing of an item or items, usually carried out on a very large scale. Industrial processes are the key components of heavy industry.


The availability of electricity and its effect on materials gave rise to several processes for plating or separating metals.



The physical shaping of materials by forming their liquid form using a mould.

Many materials exist in an impure form, purification, or separation provides a usable product.

Distillation is the purification of volatile substances by evaporation and condensation

In additive manufacturing, material is progressively added to the piece until the desired shape and size are obtained.

The nature of an organic molecule means it can be transformed at the molecular level to create a range of products.

Organized by product:


A list by process:



</doc>
<doc id="46536012" url="https://en.wikipedia.org/wiki?curid=46536012" title="Transmittal document">
Transmittal document

A transmittal document is a kind of "packing slip" for a document or collection of documents that are transferred from one company to another. The transmittal might be just the front page in an extensive document. But more often it is a separate document file that contains details of the documents that are sent. The transmittal also contains specific (company or project-related) details to help further processing of the documents for the recipient.

The content of the transmittal document depends on the situation. Some typical content in a transmittal can be:


Transmittals are used in engineering and construction companies as a necessary tool in projects where a large number of documents are involved. Several document handling systems have functions for generating transmittal document along with packages of document for transfer.



</doc>
<doc id="9675622" url="https://en.wikipedia.org/wiki?curid=9675622" title="Cleaner">
Cleaner

A cleaner or a cleaning operative is a type of industrial or domestic worker who cleans homes or commercial premises for payment. Cleaning operatives may specialise in cleaning particular things or places, such as window cleaners. Cleaning operatives often work when the people who otherwise occupy the space are not around. They may clean offices at night or houses during the workday. 

The 2000 film "Bread and Roses" by British director Ken Loach depicted the struggle of cleaners in Los Angeles, California, for better pay and working conditions and for the right to join a union. In an interview with the BBC in 2001, Loach stated that thousands of cleaners from around 30 countries have since contacted him with tales similar to the one told in the film.

The cleaning industry is quite big as different types of cleaning are required for different objects and different properties. For example, cleaning an office space requires the services of a commercial cleaner whereas cleaning a house requires domestic cleaning services. Depending on the task, even these categories can be subdivided into, for example, end-of-lease cleaning, carpet cleaning, upholstery cleaning, window cleaning, car cleaning services etc. Cleaners specialize in a specific cleaning sector or even a specific task in a cleaning sector, and one cannot expect a window cleaner to be able or willing to clean a carpet. Some types of cleaners are mentioned below.

These cleaning operatives are quite easy to find due to their large numbers, and they are usually denoted as maid service providers, janitors, and domestic cleaning operatives. These type of operatives specialize in house cleaning services such as spring cleaning, end-of-lease cleaning etc. Most house cleaning operatives are known to pay good attention to details, but due to their large numbers, there is no guarantee that one will find one who is trustworthy and works effectively. The cost of this type of service depends on the demand for the service and the number of providers. In Australia, one can hire a household cleaner from anywhere between $20 to $30 an hour. In countries like India, China, the Philippines, etc., where the labor rates are quite low, people can afford a full-time maid or domestic cleaner.

The following are some items used by cleaning staff:




</doc>
<doc id="50825029" url="https://en.wikipedia.org/wiki?curid=50825029" title="Industrial augmented reality">
Industrial augmented reality

Industrial augmented reality (IAR) is related to the application of augmented reality (AR) to support an industrial process. The use of IAR dates back to the 1990s with the work of Thomas Caudell and David Mizell about the application of AR at Boeing. Since then several applications of this technique over the years have been proposed showing its potential in supporting some industrial processes. Although there have been several advances in technology, IAR is still considered to be at an infant developmental stage.

Some challenging factors of IAR development are related to the necessary interdisciplinarity knowledge in areas such as object recognition, computer graphics, artificial intelligence and human-computer-interaction. Where a partial context understanding is required for the adaptation to unexpected conditions and understand the user's actions and intentions. Additionally user intuitive interfaces still remain a challenge likewise hardware improvements such as sensors and displays.

Further, some controversy prevails about the boundaries that define IAR and its potential benefits for some activities with the currently available technology.

Although the origins of Augmented Reality dates from the 1960s, when Ivan Sutherland created the first head-mounted display it did not gain strength until the early 1990s, when the David Mizell and Thomas Caudell developed the first industrial AR at Boeing. They used a head-mounted display (HMD) to superimpose a computer-generated diagram of the manufacturing process with a real-time world registration and the user's head position calculation. They coined the name Augmented Reality to this technology.
Contemporary several prototypes were proposed to demonstrate AR's application to manufacturing: a laser-printer maintenance application was proposed in 1993 by Steven K. Feiner and coauthors by introducing the concept of knowledge-based AR for maintenance assistance (KARMA). Whitaker Ross et al. proposed a system to display the name of the part pointed by the user in an engine.

By the 2000s, the interest in AR had grown considerably. Some important groups were funded: the largest consortium for IAR backed by the German's Federal Ministry of Education and Research (ARVIKA), with the aim of researching and implementing AR in relevant German industries,; the European Community founded several projects including Service and Training through (STAR), which is a collaboration between institutes and companies from Europe and the US, and Advanced Augmented Reality Technologies for Industrial Service Applications (ARTESAS) derived from ARVIKA, focused on the development of AR for automotive and aerospace maintenance. Likewise, from other countries such as Sweden, Australia and Japan with the aim of encourage the IAR development.

From the beginning of 2010 until today, advances in hardware devices such as the wearable Google Glass, the reduced cost of mobile devices, and the increasing user familiarity with this technology. Besides the increasing product-development complexity where products are becoming more versatile and intricate, with multiple variations and mass customization. Opened new scenarios for this technology.

One of the most promising fields of AR application is industrial manufacturing, where it can be used to support some activities in product development and manufacturing through providing information available to reduce and simplify the user's decisions. The general issues of the development of an AR system can still be classified into:

There are technologies needed to build AR systems. Some of them are directly related with the performance of the software and hardware that enable the deployment of AR, such as displays, sensors, processors, recognition, tracking, registration among others. Thus AR uses different approaches to integrate the virtual and real worlds, where several technologies influence the usability and applicability.

Some common unsolved issues concern tracking systems suited for industrial scenarios which mean: poorly textured objects with smooth surfaces and strong light variation; object recognition using natural features when it is not possible to use markers; the improvement of accuracy and latency of registration, and 3D context scene capture to allow context awareness.

The limited understanding of human factors is likely to be obstructing the spreading of IAR beyond laboratories prototypes. Their study is challenged to overcome technological issues (deficiencies in resolution, field of view, brightness, contrast, tracking systems, among others) in order to separate the AR performance from interface factors and technological issues.

It was suggested by that for an IAR application to be successful in a commercial environment, it has to be "user friendly", meaning that it needs to be easy and safe to setup, learn, use and customize and the user should feel free to move with an AR system. As well as the use of natural interfaces in order to control AR by using natural movements of the body have also motivated a good deal of research. The reason for this is that the usability not only depends on the system's stability but also on the control interface's quality.

Further, the user interface should avoid overloading the user with information and prevent over relying on it in order to avoid losing important cues from the environment. Other issues are also related with improving multiple user collaboration

It is the final challenge, given an ideal AR system (hardware, software and an intuitive interface) to be accepted and become a part of a user's daily life.
Consequently, one of the most important factors related with the adoption of any new technology is the perception of usefulness, and AR needs to show a clear cost-benefit relation. Some studies suggest that, in order for AR to be perceived as useful the task should be high enough to require its use.

Other non-addressed but important issues for technology acceptance are related with fashion, ethics, and privacy.

Assembling is the process of putting together several separate components in order to create a functional one. It can be performed in different stages of the product's life. Even though nowadays many assembly operations are automated, some of them still require human assistance as, in many cases, their bits of information are detached from the equipment. Thus it is necessary to alternate their attention which leads to decreasing productivity and increasing of errors and injuries

The use of AR is encouraged by the premise that instructions might be easier to understand if instead of being available as manuals they are super imposed upon the actual equipment. Some of the uses of AR in the support of the assembly can be categorized into:

Similarly, by using AR it is possible to simulate the user's motion during assembly to acquire an accurate and realistic movement of virtual parts.

On the other hand, some of the critical issues of the support assembly task are related with the dynamic reconfiguration of the state diagram, which allows to automatically identify the step of assembly and also adapt to unexpected actions or errors of the user. Thus, defining 'what', 'where', and 'when' to display information becomes a challenge since it requires a minimum understanding of the surrounding scene.

Like the assembly task, maintenance serves as a natural application for AR because it is a tasks that requires keeping the user's attention on a specific area, and also synthesizing additional information such as complex sequences, component identification, and textual data.
The maintenance activity can be supported by displaying related information about an unfamiliar piece of equipment to a technician instead of searching it in a repair manual. Similarly, AR can support maintenance tasks by acting as an "x-ray" like vision, or providing information from sensors directly to the user.

It can also be employed in repairing tasks. For instance, in the diagnosis of modern cars whose status information can be loaded via a plug-like connector. AR can be used to immediately display the diagnosis on the engine

Many industries are required to perform complex activities that needs previous training. Therefore, for learning a new skill, technicians need to be trained in sensorimotive and cognitive of sub-skills that can be challenging. This kind of training can be supported by AR.

Additionally, the possibility to use AR to motivate both trainees and students through enhancing the realism of practices has been suggested. By providing instructions with an AR, the immediate capability to perform the task can be also accomplished.

Other advantages of using AR for training are that students can interact with real objects and, at the same time, have access to guidance information, and the existence of tactile feedback provided by the interaction with real objects.

By displaying information about the components of manufacturing in real time. For instance, Volkswagen has used it to verify parts by analyzing their interfering edges and variance. AR has also been used in automobile development to display and verify car components and ergonomic test in reality. By superimposing the original 3D model over the real surface, deviations can easily be identified, and therefore sources of error can be corrected.

The use of AR have been proposed to interact with scientific data in shared environments, where it allows a 3D interaction with real objects compared to virtual reality, while allowing the user to move freely in the real world. Similar systems for allowing multiple user's with HMD can interact with dynamic visual simulations of engineering processes

In the same way, AR simulation of working machinery can be checked from mobile devices as well as other information such as temperature and, time of use, which can reduce worker's movements and stress.

The benefits of implementing AR into some industrial activities have been of a high interest. However, there is still a debate as the current level of the technology hides all of its potential.

It has been reported that, in maintenance and repair, the use of AR can reduce time to locate a task, head and neck movements, and also other disadvantages related to bulky, low resolution display hardware
In addition, in training it aims to improve the performance up to 30% and, at the same time reduce costs by 25%.

Similar benefits have been reported by Juha Sääski et al. in the comparative use of AR versus paper instructions to support the assembly operation of the parts of a tractor's accessory power unit which showed a time and error reduction (six time less).

But, on the other hand, the long-term use has been reported to cause stress and strain to the user. Johannes Tümler et al. compared the stress and strain produced by picking parts from a rack using AR versus using paper as a reference, which showed as result a change of the strain that could be assumed by the non-optimal system.

Additionally, it has been suggested that one potential danger in use AR for training is the user dependability of this technology. As a consequence the user will not be able to perform the task without it. In order to overcome this situation, the information available during training needs to be controlled.

The boundaries that define IAR are still not clear. For instance, one of the most accepted definitions of AR implies that the virtual elements need to be registered. But in industrial field, performance is a main goal, and therefore has been an extensive research about the presentation of virtual components in AR regarding the type of the task. This research has shown that the optimal visual aid type may variate depending on the difficulty of the task.

Finally, it has been suggested that in order to have commercial IAR solutions, they must be:


</doc>
<doc id="273856" url="https://en.wikipedia.org/wiki?curid=273856" title="Industrial society">
Industrial society

In sociology, industrial society is a society driven by the use of technology to enable mass production, supporting a large population with a high capacity for division of labour. Such a structure developed in the Western world in the period of time following the Industrial Revolution, and replaced the agrarian societies of the pre-modern, pre-industrial age. Industrial societies are generally mass societies, and may be succeeded by an information society. They are often contrasted with traditional societies.

Industrial societies use external energy sources, such as fossil fuels, to increase the rate and scale of production. The production of food is shifted to large commercial farms where the products of industry, such as combine harvesters and fossil fuel-based fertilizers, are used to decrease required human labor while increasing production. No longer needed for the production of food, excess labor is moved into these factories where mechanization is utilized to further increase efficiency. As populations grow, and mechanization is further refined, often to the level of automation, many workers shift to expanding service industries.

Industrial society makes urbanization desirable, in part so that workers can be closer to centers of production, and the service industry can provide labor to workers and those that benefit financially from them, in exchange for a piece of production profits with which they can buy goods. This leads to the rise of very large cities and surrounding suburb areas with a high rate of economic activity.

These urban centers require the input of external energy sources in order to overcome the diminishing returns of agricultural consolidation, due partially to the lack of nearby arable land, associated transportation and storage costs, and are otherwise unsustainable. This makes the reliable availability of the needed energy resources high priority in industrial government policies.

Some theoreticians (namely Ulrich Beck, Anthony Giddens, and Manuel Castells) argue that we are located in the middle of a transformation or transition from industrial societies to post-industrial societies. The triggering technology for the change from an agricultural to an industrial organization was steam power, allowing mass production and reducing the agricultural work necessary. Thus, many industrial cities have been built around rivers. Identified as catalyst or trigger for the transition to post-modern or informational society is global information technology.

Some, such as Theodore Kaczynski, have argued that an industrialized society leads to psychological pain and that citizens must actively work to return to a more primitive society. His essay, "Industrial Society and Its Future", describes different political factions and laments the direction of technology and the modern world.




</doc>
<doc id="14543" url="https://en.wikipedia.org/wiki?curid=14543" title="Industry">
Industry

An industry is a sector that produces goods or related services within an economy. The major source of revenue of a group or company is an indicator of what industry it should be classified in. When a large corporate group has multiple sources of revenue generation, it is considered to be working in different industries. The manufacturing industry became a key sector of production and labour in European and North American countries during the Industrial Revolution, upsetting previous mercantile and feudal economies. This came through many successive rapid advances in technology, such as the development of steam power and the production of steel and coal.

Following the Industrial Revolution, possibly a third of the economic output came from manufacturing industries. Many developed countries and many developing/semi-developed countries (China, India etc.) depend significantly on manufacturing industry.

Slavery, the practice of utilizing forced labor to produce goods and services, has occurred since antiquity throughout the world as a means of low-cost production. It typically produces goods for which profit depends on economies of scale, especially those for which labor was simple and easy to supervise. International law has declared slavery illegal.

Guilds, associations of artisans and merchants, oversee the production and distribution of a particular good. Guilds have their roots in the Roman Empire as "collegia" (singular: "collegium") Membership in these early guilds was voluntary. The Roman "collegia" did not survive the fall of Rome. In the early middle ages, guilds once again began to emerge in Europe, reaching a degree of maturity by the beginning of the 14th century. While few guilds remain , some modern labor structures resemble those of traditional guilds. Other guilds, such as the SAG-AFTRA act as trade unions rather than as classical guilds. Professor Sheilagh Ogilvie claims that guilds negatively affected quality, skills, and innovation in areas that they were present.

The industrial revolution (from the mid-18th century to the mid-19th century) saw the development and popularization of mechanized means of production as a replacement for hand production. The industrial revolution played a role in the abolition of slavery in Europe and in North America.

In a process dubbed "tertiarization", the economic preponderance of primary and secondary industries has declined in recent centuries relative to the rising importance of tertiary industry,
resulting in the post-industrial economy. Specialization in industry
and in the classification of industry has also occurred. Thus (for example) a record producer might claim to speak on behalf of the Japanese rock industry, the recording industry, the music industry or the entertainment industry - and any formulation will sound grandiose and weighty.

The Industrial Revolution led to the development of factories for large-scale production with consequent changes in society. Originally the factories were steam-powered, but later transitioned to electricity once an electrical grid was developed. The mechanized assembly line was introduced to assemble parts in a repeatable fashion, with individual workers performing specific steps during the process. This led to significant increases in efficiency, lowering the cost of the end process. Later automation was increasingly used to replace human operators. This process has accelerated with the development of the computer and the robot.

Historically certain manufacturing industries have gone into a decline due to various economic factors, including the development of replacement technology or the loss of competitive advantage. An example of the former is the decline in carriage manufacturing when the automobile was mass-produced.

A recent trend has been the migration of prosperous, industrialized nations towards a post-industrial society. This is manifested by an increase in the service sector at the expense of manufacturing, and the development of an information-based economy, the so-called informational revolution. In a post-industrial society, manufacturers relocate to more profitable locations through a process of off-shoring.

Measurements of manufacturing industries outputs and economic effect are not historically stable. Traditionally, success has been measured in the number of jobs created. The reduced number of employees in the manufacturing sector has been assumed to result from a decline in the competitiveness of the sector, or the introduction of the lean manufacturing process.

Related to this change is the upgrading of the quality of the product being manufactured. While it is possible to produce a low-technology product with low-skill labour, the ability to manufacture high-technology products well is dependent on a highly skilled staff.

An industrial society is a society driven by the use of technology to enable mass production, supporting a large population with a high capacity for division of labour. Today, industry is an important part of most societies and nations. A government must have some kind of industrial policy, regulating industrial placement, industrial pollution, financing and industrial labour.

In an industrial society, industry employs a major part of the population. This occurs typically in the manufacturing sector. A labour union is an organization of workers who have banded together to achieve common goals in key areas such as wages, hours, and other working conditions. The trade union, through its leadership, bargains with the employer on behalf of union members (rank and file members) and negotiates labour contracts with employers. This movement first rose among industrial workers.

The Industrial Revolution changed warfare, with mass-produced weaponry and supplies, machine-powered transportation, mobilization, the total war concept and weapons of mass destruction. Early instances of industrial warfare were the Crimean War and the American Civil War, but its full potential showed during the world wars. See also military-industrial complex, arms industries, military industry and modern warfare.





</doc>
<doc id="3320853" url="https://en.wikipedia.org/wiki?curid=3320853" title="Chemical process">
Chemical process

In a scientific sense, a chemical process is a method or means of somehow changing one or more chemicals or chemical compounds. Such a chemical process can occur by itself or be caused by an outside force, and involves a chemical reaction of some sort. In an "engineering" sense, a chemical process is a method intended to be used in manufacturing or on an industrial scale (see Industrial process) to change the composition of chemical(s) or material(s), usually using technology similar or related to that used in chemical plants or the chemical industry. 

Neither of these definitions are exact in the sense that one can always tell definitively what is a chemical process and what is not; they are practical definitions. There is also significant overlap in these two definition variations. Because of the inexactness of the definition, chemists and other scientists use the term "chemical process" only in a general sense or in the engineering sense. However, in the "process (engineering)" sense, the term "chemical process" is used extensively. The rest of the article will cover the engineering type of chemical processes. 

Although this type of chemical process may sometimes involve only one step, often multiple steps, referred to as unit operations, are involved. In a plant, each of the unit operations commonly occur in individual vessels or sections of the plant called units. Often, one or more chemical reactions are involved, but other ways of changing chemical (or material) composition may be used, such as mixing or separation processes. The process steps may be sequential in time or sequential in space along a stream of flowing or moving material; see Chemical plant. For a given amount of a feed (input) material or product (output) material, an expected amount of material can be determined at key steps in the process from empirical data and material balance calculations. These amounts can be scaled up or down to suit the desired capacity or operation of a particular chemical plant built for such a process. More than one chemical plant may use the same chemical process, each plant perhaps at differently scaled capacities. 
Chemical processes like distillation and crystallization go back to alchemy in Alexandria, Egypt.

Such chemical processes can be illustrated generally as block flow diagrams or in more detail as process flow diagrams. Block flow diagrams show the units as blocks and the streams flowing between them as connecting lines with arrowheads to show direction of flow. 

In addition to chemical plants for producing chemicals, chemical processes with similar technology and equipment are also used in oil refining and other refineries, natural gas processing, polymer and pharmaceutical manufacturing, food processing, and water and wastewater treatment.

Unit processing is the basic processing in chemical engineering. Together with unit operations it forms the main principle of the varied chemical industries. Each genre of unit processing follows the same chemical law much as each genre of unit operations follows the same physical law.

Chemical engineering unit processing consists of the following important processes:




</doc>
<doc id="55424384" url="https://en.wikipedia.org/wiki?curid=55424384" title="French Fab">
French Fab

French Fab is an accreditation award created 2 October 2017 in order to federate French companies and to promote French industries throughout the world.

The French government has been inspired by the French Tech created in 2013 to promote startups and French IT.

The award has been created 2 October 2017 by Bruno Le Maire, Minister of Finance.



</doc>
<doc id="2168581" url="https://en.wikipedia.org/wiki?curid=2168581" title="Sunrise industry">
Sunrise industry

A sunrise industry is one that is new or relatively new, is growing fast and is expected to become important in the future. Examples of sunrise industries include hydrogen fuel production, petrochemical industry, food processing industry, space tourism, and online encyclopedias.



</doc>
<doc id="25557935" url="https://en.wikipedia.org/wiki?curid=25557935" title="EMO (trade show)">
EMO (trade show)

EMO is a European trade show for the manufacturing industries. It occurs every odd-numbered year, with a cycle that finds it at the Hanover Fairground in Hanover, Germany for 2 shows, then the Fiera Milano exhibition center in Milan, Italy for 1 show.

The name "EMO" came from the name Exposition Mondiale de la Machine-Outil (Machine Tool World Exposition), and the scope of the content still reflects the machine tool heritage, although it now also extends beyond it. The show covers the spectrum of metalworking technologies, such as machine tools for milling, turning, and forming; manufacturing systems; precision measuring tools; automated materials handling; computer technology; industrial electronics; and accessories. 

EMO is run by CECIMO, the European Association of the Machine Tool Industries (Comité européen de coopération des industries de la machine-outil) (www.cecimo.eu). The Verein Deutscher Werkzeugmaschinenfabriken (German Machine Tool Builders’ Association), or VDW, is responsible for the organization of the trade show when in Hanover, while UCIMU, the Association of Italian Manufacturers of Machine Tools, Robots, Automation Systems and ancillary products (NC, tools, components, accessories) manages the Milan show.

An agreement between the Association for Manufacturing Technology (AMT), which organizes the US-based International Manufacturing Technology Show (IMTS) and CECIMO coordinates the IMTS and the EMO such that every even-numbered year the IMTS is held in Chicago, and every odd-numbered year the EMO is held in Europe.

EMO began as the EEMO (Exposition européenne de machines-outils, European machine tools exhibition). The first EEMO was held in 1951. The name changed in 1975 to EMO (Exposition Mondiale de la Machine-Outil). Nowadays its scope extends beyond machine tools, and the acronym expansion is not used by CECIMO anymore. 



</doc>
<doc id="4954764" url="https://en.wikipedia.org/wiki?curid=4954764" title="The Year in Industry">
The Year in Industry

The Year in Industry (YinI) is a UK scheme, which organises gap year placements for pre-university and undergraduate students. Each year The Year in Industry places around 750 students in engineering, science, IT, and business. The Year in Industry is run by the not for shareholder profit Engineering Development Trust and is accredited by the Learning Grid.

Students submit a Curriculum vitae to The Year in Industry detailing what field they are interested in finding a placement. The Year in Industry then send individual students 'company CV's' in that field. Students can then elect to be put forward for that placement, and may be selected by the company for interview and ultimately the placement. Placements usually last around 12 months, during which in between two and four on-site visits are made by YinI to check up on the student.

The Year in Industry was set up in 1987 in the Bristol area and was originally called Pre-Formation of Undergraduate Engineers (PFUE). It has placed over 8500 students to date, in 2007 over 250 UK companies were involved in the scheme. The scheme has received a lot of praise from both universities and industry.

Placements usually last around 12 months and start between July and September, depending on the company and students requirements. Applications are free as The Year in Industry no longer charge an administrative fee to students. Students apply for placements through regional offices with the following process:


The Year in Industry is run by the non-profit organization Engineering Development Trust,
an independent education charity whose aim is to involve young people in engineering, science and technology. The scheme is also part of the Royal Academy of Engineering's "BEST" programme, likewise this is aimed at encouraging young people to undertake careers in engineering.

The organisation is split up into twelve regional offices:


Each year the Year in Industry runs a Contribution to Business award. This is aimed at providing students with the opportunity to demonstrate how they have made a difference to their company through their work placement. The award is open to all placement students, who submit a written application detailing the contributions they have made to their regional YinI office. This is supported by a statement written about the student by their line manager.
Around eight people in each region are short-listed and are then required to give a short presentation to industry figures at the regional open day. The winner from each region then attends a national final to find an overall winner. In 2007 the winner of each regional final won a prize of £500. The overall winner will receive an award of £1000, plus three additional prizes are to be awarded of £500 each for innovation, communication skills and environment. The event in 2007 was held on Thursday, 6 September at The Royal College of Physicians, Regent's Park, London.

Students are expected to attend and ideally complete a Chartered Management Institute course or equivalent during the year; different regions undertake different courses due to differences in funding. Most regions undertake an 'Introduction to Management' certificate and a 'Certificate in Management', the latter being accredited to NVQ level 3 standard. Teaching comes in the form of residential and one-day-a-month workshops, usually taught by local colleges, totalling around 14 days of tuition. Other regions undertake a smaller Institute of Leadership & Management Level 3 Certificate in Leadership. This involves around 60 hours of tuition spread over five one-day-a-month workshops.
The course is organised by the Year in Industry and is paid for by the companies employing YinI students. Enrollment on this course gives YinI students membership to the NUS.

At the end of the year students attend a regional open day. Here they exhibit their work and get the opportunity to talk to industry professionals about their experiences. The judging of the regional Contribution to Business Award also takes places at the open day.

In 2007/8 academic year the Year in Industry launched its 'YinI Combo' placements which allowed students to work for part of the year before travelling.

A voluntary maths course is offered by Loughborough University Best Maths Team. This course has been specially designed for engineering based students working in industry and aims to improve, and maintain students' maths skills over the course of the placement. The course is completed by the student through a series of distance learning modules and on-line tests.

At this moment in time the Year in Industry placements do not legally entitle participants to student council tax exemptions. Undergraduate students are entitled to the exemption since they remain members of their respective universities for the duration of the gap year, however, pre-university students with deferred entry to university do not qualify as students by council tax law. In the past some appeals with support from the Year in Industry have been successful.




</doc>
<doc id="8340480" url="https://en.wikipedia.org/wiki?curid=8340480" title="Industrial production">
Industrial production

Industrial production is a measure of output of the industrial sector of the economy. The industrial sector includes manufacturing, mining, and utilities. Although these sectors contribute only a small portion of gross domestic product (GDP), they are highly sensitive to interest rates and consumer demand. This makes industrial production an important tool for forecasting future GDP and economic performance. Industrial production figures are also used by central banks to measure inflation, as high levels of industrial production can lead to uncontrolled levels of consumption and rapid inflation.The Industrial productions came about during the Industrial revolution.



</doc>
<doc id="33746051" url="https://en.wikipedia.org/wiki?curid=33746051" title="Industrial sickness">
Industrial sickness

Industrial sickness is defined all over the world as "an industrial company (being a company registered for not less than five years) which has, at the end of any financial year, accumulated losses equal to, or exceeding, its entire net worth and has also suffered cash losses in such financial year and the financial year immediately preceding such financial year".

Industrial sickness is an term applied to various things associated with industry that make people ill and cause them to miss work. The solutions will have to be tailored to the specific industry, and only in that way can any real effect be made on improving the health and productivity of the industrial workforce.

The key is an aggressive work-up on the health issues for a given segment of the industrial workforce, and usually broken down by type of work (which makes sense). Even as coal miners face overpowering respiratory threats, and foundry and mill workers have to confront major physical threats from large (heavy) quantities of extremely hot materials, each facet of industrial production has its hot-button health issues.

Industrial health managers need training and experience identifying and remediating conditions that present major health threats to their respective workforces. Then they can train the rest of management and can teach the workers themselves about the best way to carry out their jobs with minimum threats to their health.

According to Companies (Second Amendment) Act, 2002

"'Sick Industrial Company' means an industrial company which has

i) The Accumulated losses in any financial year equal to 50 per cent or more of its average net worth during four years immediately preceding such financial year; or

ii) Failed to repay its debts within any three consecutive quarters on demand made in writing for its repayment by a creditor or creditors of such company."

Industrial sickness specially in small-scale Industry has been always a demerit for the Indian economy, because more and more industries like – cotton, Jute, Sugar, Textiles small steel and engineering industries are being affected by this sickness problem.

As per an estimate 300 units in the medium and large scale sector were either closed or were on the stage of closing in the year 1976. About 10% of 4 lakhs unit were also reported to be ailing. And this position also remain same in the next decades. At the end of year 1986, the member of sick units in the portfolio of scheduled commercial banks stood at 1,47,740 involving an out standing bank credit of Rs. 4874 crores.

The different types of industrial sickness in Small Scale Industry (SSI) fall under two important categories. They are as follows:

We can say pertaining to the factors which are within the control of management. This sickness arises due to internal disorder in the areas justified as following:

a) Lack of Finance: This including weak equity base, poor utilization of assets, inefficient working capital management, absence of costing & pricing, absence of planning and budgeting and inappropriate utilization or diversion of funds.

b) Bad Production Policies : Another very important reason for sickness is wrong selection of site which is related to production, inappropriate plant & machinery, bad maintenance of Plant & Machinery, lack of quality control, lack of standard research & development and so on.

c) Marketing and Sickness : This is another part which always affects the health of any sector as well as SSI. This including wrong demand forecasting, selection of inappropriate product mix, absence of product planning, wrong market research methods, and bad sales promotions.

d) Inappropriate Personnel Management: Another internal reason for the sickness of SSIs is inappropriate personnel management policies which includes bad wages and salary administration, bad labour relations, lack of behavioural approach causes dissatisfaction among the employees and workers.

e) Ineffective Corporate Management: Another reason for the sickness of SSIs is ineffective or bad corporate management which includes improper corporate planning, lack of integrity in top management, lack of coordination and control etc.

a) Personnel Constraint: The first for most important reason for the sickness of small scale industries are non availability of skilled labour or manpower wages disparity in similar industry and general labour invested in the area.

b) Marketing Constraints: The second cause for the sickness is related to marketing. The sickness arrives due to liberal licensing policies, restrain of purchase by bulk purchasers, changes in global marketing scenario, excessive tax policies by govt. and market recession.

c) Production Constraints: This is another reason for the sickness which comes under external cause of sickness. This arises due to shortage of raw material, shortage of power, fuel and high prices, import-export restrictions.

d) Finance Constraints: Another external cause for the sickness of SSIs is lack of finance. This arises due to credit restrains policy, delay in disbursement of loan by govt., unfavorable investments, fear of nationalization.

e)credit squeeze initiated by the government policies.





</doc>
<doc id="6587138" url="https://en.wikipedia.org/wiki?curid=6587138" title="Outline of industry">
Outline of industry

The following outline is provided as an overview of and topical guide to industry:

Industry – refers to the production of an economic good or service within an economy.



1. Agriculture "(see also Agribusiness)"

2. Manufacturing

The manufacturing industry compromises a wide variety of production of goods, ranging from low tech and low labor skills for the process. There are many areas that requires intense and stunning technology to be achieve; examples include aeronautics, electronics, pharmaceutical, robotics. 


3. Service industries







</doc>
<doc id="8055028" url="https://en.wikipedia.org/wiki?curid=8055028" title="Industrial slave">
Industrial slave

An industrial slave is a type of slave who typically worked in an industrial setting. These slaves often had work that was more dangerous than agricultural slaves.

In the antebellum southern United States, industrial slaves were often the property of a company instead of an individual. These companies spanned various industries including sawmills, cotton gins and mills, fishing, steamboats, sugar refineries, coal and gold mining, and railroads.

Industrial slaves were exposed to many dangerous jobs in factories. Most of the machinery and tools were very new and the simplest mistake could mean the loss of a hand, foot, or even death. Industrial slaves worked twelve hours per day, six days per week. The only breaks they received were for a short lunch during the day, and Sunday or the occasional holiday during the week. Not many of the slaves had to endure working every day the whole year around, however.

Industrial textile mills in the old south that used slave labor "Usually earned annual profits on capital between 10 and 65 percent and averaging about 16 percent." The use of industrial slaves sometimes allowed a bankrupt company to be resurrected: "The Woodville mill, which went bankrupt with free labor, annually paid 10 to 15 per cent dividends after switching to slave labor".


</doc>
<doc id="210545" url="https://en.wikipedia.org/wiki?curid=210545" title="Industrialisation">
Industrialisation

Industrialisation (or industrialization) is the period of social and economic change that transforms a human group from an agrarian society into an industrial society. This involves an extensive re-organisation of an economy for the purpose of manufacturing.

As industrial workers' incomes rise, markets for consumer goods and services of all kinds tend to expand and provide a further stimulus to industrial investment and economic growth.

After the last stage of the Proto-industrialization, the first transformation from an agricultural to an industrial economy is known as the Industrial Revolution and took place from the mid-18th to early 19th century in certain areas in Europe and North America; starting in Great Britain, followed by Belgium, Switzerland, Germany, and France. Characteristics of this early industrialisation were technological progress, a shift from rural work to industrial labor, financial investments in new industrial structure, and early developments in class consciousness and theories related to this. Later commentators have called this the First Industrial Revolution.

The "Second Industrial Revolution" labels the later changes that came about in the mid-19th century after the refinement of the steam engine, the invention of the internal combustion engine, the harnessing of electricity and the construction of canals, railways and electric-power lines. The invention of the assembly line gave this phase a boost.
Coal mines, steelworks, and textile factories replaced homes as the place of work.
By the end of the 20th century, East Asia had become one of the most recently industrialised regions of the world. The BRICS states (Brazil, Russia, India, China and South Africa) are undergoing the process of industrialisation.

There is considerable literature on the factors facilitating industrial modernisation and enterprise development.

As the Industrial Revolution was a shift from the agrarian society, people migrated from villages in search of jobs to places where factories were established. This shifting of rural people led to urbanisation and increase in the population of towns. The concentration of labour in factories has increased urbanisation and the size of settlements, to serve and house the factory workers.

Family structure changes with industrialisation. Sociologist Talcott Parsons noted that in pre-industrial societies there is an extended family structure spanning many generations who probably remained in the same location for generations. In industrialised societies the nuclear family, consisting of only parents and their growing children, predominates. Families and children reaching adulthood are more mobile and tend to relocate to where jobs exist. Extended family bonds become more tenuous.

 the "international development community" (World Bank, Organisation for Economic Co-Operation and Development (OECD), many United Nations departments, and some other organisations) endorses development policies like water purification or primary education and co-operation amongst third world communities. Some members of the economic communities do not consider contemporary industrialisation policies as being adequate to the global south (Third World countries) or beneficial in the longer term, with the perception that they may only create inefficient local industries unable to compete in the free-trade dominated political order which industrialisation has fostered. Environmentalism and Green politics may represent more visceral reactions to industrial growth. Nevertheless, repeated examples in history of apparently successful industrialisation (Britain, Soviet Union, South Korea, China, etc.) may make conventional industrialisation seem like an attractive or even natural path forward, especially as populations grow, consumerist expectations rise and agricultural opportunities diminish.

The relationships among economic growth, employment, and poverty reduction are complex. Higher productivity, it is argued, may lead to lower employment (see jobless recovery).
There are differences across sectors, whereby manufacturing is less able than the tertiary sector to accommodate both increased productivity and employment opportunities; more than 40% of the world's employees are "working poor", whose incomes fail to keep themselves and their families above the $2-a-day poverty line. There is also a phenomenon of deindustrialisation, as in the former USSR countries' transition to market economies, and the agriculture sector is often the key sector in absorbing the resultant unemployment.



</doc>
<doc id="25493759" url="https://en.wikipedia.org/wiki?curid=25493759" title="Industry classification">
Industry classification

Industry classification or industry taxonomy is a type of economic taxonomy that organizes companies into industrial groupings based on similar production processes, similar products, or similar behavior in financial markets.

Many are used by national and international statistical agencies to summarize economic conditions. They are also used by securities analysts to understand common forces acting on groups of companies, to compare companies' performance to their peers', and to construct either specialized or diversified portfolios.

Industries can be classified in a variety of ways. At the top level, industry is often classified according to the three-sector theory into sectors: primary (extraction and agriculture), secondary (manufacturing), and tertiary (services). Some authors add quaternary (knowledge) or even quinary (culture and research) sectors. Over time, the fraction of a society's industry within each sector changes.

Below the economic sectors are more detailed classifications. They commonly divide industries according to similar functions and markets and identify businesses producing related products.

Industries can also be identified by product, such as: construction industry, chemical industry, petroleum industry, automotive industry, electronic industry, power engineering and power manufacturing (such as gas or wind turbines), meatpacking industry, hospitality industry, food industry, fish industry, software industry, paper industry, entertainment industry, semiconductor industry, cultural industry, and poverty industry.

Market-based classification systems such as the Global Industry Classification Standard and the Industry Classification Benchmark are used in finance and market research.

A wide variety of taxonomies is in use, sponsored by different organizations and based on different criteria.

The NAICS Index File lists 19745 rubrics beyond the 6 digits which are not assigned codes.




</doc>
<doc id="310864" url="https://en.wikipedia.org/wiki?curid=310864" title="Intertwingularity">
Intertwingularity

Intertwingularity is a term coined by Ted Nelson to express the complexity of interrelations in human knowledge.

Nelson wrote in "Computer Lib/Dream Machines" : "EVERYTHING IS DEEPLY INTERTWINGLED. In an important sense there are no "subjects" at all; there is only all knowledge, since the cross-connections among the myriad topics of this world simply cannot be divided up neatly."

He added the following comment in the revised edition : "Hierarchical and sequential structures, especially popular since Gutenberg, are usually forced and artificial. Intertwingularity is not generally acknowledged—people keep pretending they can make things hierarchical, categorizable and sequential when they can't."

Intertwingularity is related to Nelson's coining of the term hypertext, partially inspired by "As We May Think" (1945) by Vannevar Bush.

Peter Morville, an influential figure in information architecture, discusses intertwingularity in some of his books. In "Ambient Findability: What We Find Changes Who We Become" (2005), Morville uses the concept of intertwingularity to describe the experience of using hypertext on the web and starting to use computers embedded in everyday objects termed--also known as ubiquitous computing. In 2014, he published a book called "Intertwingled: Information Changes Everything" about the intertwingularity of the universe, crediting Nelson with the word. 

David Weinberger wrote about intertwingularity in "Everything Is Miscellaneous: The Power of the New Digital Disorder" in 2008, explaining that providing unique identifiers for items helps enable intertwingularity.

The concept of intertwingularity was celebrated at the "Intertwingled: The Work and Influence of Ted Nelson" conference on April 14, 2014 at Chapman University. The organizers published a book called "Intertwingled: The Work and Influence of Ted Nelson" in 2015, with articles about Nelson's work and legacy. One of the organizers of the conference and editors of the book, Douglas Dechow, said, "In the 1960s, he saw a world of networked, interlinked – intertwingled, if you will – documents where all of the world’s knowledge is able to interact and intermingle...He was the first, or among the first, people to have that idea."




</doc>
<doc id="4911388" url="https://en.wikipedia.org/wiki?curid=4911388" title="Knowledge-based theory of the firm">
Knowledge-based theory of the firm

The knowledge-based theory of the firm considers knowledge as the most strategically significant resource of a firm. Its proponents argue that because knowledge-based resources are usually difficult to imitate and socially complex, heterogeneous knowledge bases and capabilities among firms are the major determinants of sustained competitive advantage and superior corporate performance.

This knowledge is embedded and carried through multiple entities including organizational culture and identity, policies, routines, documents, systems, and employees. Originating from the strategic management literature, this perspective builds upon and extends the resource-based view of the firm (RBV) initially promoted by Penrose (1959) and later expanded by others (Wernerfelt 1984, Barney 1991, Conner 1991).

Although the resource-based view of the firm recognizes the important role of knowledge in firms that achieve a competitive advantage, proponents of the knowledge-based view argue that the resource-based perspective does not go far enough. Specifically, the RBV treats knowledge as a generic resource, rather than having special characteristics. It therefore does not distinguish between different types of knowledge-based capabilities. Information technologies can play an important role in the knowledge-based view of the firm in that information systems can be used to synthesize, enhance, and expedite large-scale intra- and inter-firm knowledge management (Alavi and Leidner 2001).

Whether or not the Knowledge-based theory of the firm actually constitutes a theory has been the subject of considerable debate. See for example, Foss (1996) and Phelan & Lewin (2000). According to one notable proponent of the knowledge-based view of the firm (KBV), "The emerging knowledge-based view of the firm is not a theory of the firm in any formal sense" (Grant, 2002, p. 135).




</doc>
<doc id="3013461" url="https://en.wikipedia.org/wiki?curid=3013461" title="Metaknowledge">
Metaknowledge

Metaknowledge or meta-knowledge is knowledge about a preselected knowledge.

For the reason of different definitions of knowledge in the subject matter literature, meta-information may or may not be included in meta-knowledge. Detailed cognitive, systemic and epistemic study of human knowledge requires a distinguishing of these concepts.

Meta-knowledge is a fundamental conceptual instrument in such research and scientific domains as, knowledge engineering, knowledge management, and others dealing with study and operations on knowledge, seen as a unified object/entities, abstracted from local conceptualizations and terminologies.
Examples of the first-level individual meta-knowledge are methods of planning, modeling, tagging, learning and every modification of a domain knowledge. 
Indeed, universal meta-knowledge frameworks have to be valid for the organization of meta-levels of individual meta-knowledge.

Metaknowledge may be automatically harvested from electronic publication archives, to reveal patterns in research, relationships between researchers and institutions and to identify contradictory results.




</doc>
<doc id="2158184" url="https://en.wikipedia.org/wiki?curid=2158184" title="Credential">
Credential

A credential is an attestation of qualification, competence, or authority issued to an individual by a third party with a relevant" or "de facto" authority or assumed competence to do so.

Examples of credentials include academic diplomas, academic degrees, certifications, security clearances, identification documents, badges, passwords, user names, keys, powers of attorney, and so on. Sometimes publications, such as scientific papers or books, may be viewed as similar to credentials by some people, especially if the publication was peer reviewed or made in a well-known journal or reputable publisher.

A person holding a credential is usually given documentation or secret knowledge ("e.g.," a password or key) as proof of the credential. Sometimes this proof (or a copy of it) is held by a third, trusted party. While in some cases a credential may be as simple as a paper membership card, in other cases, such as diplomas, it involves the presentation of letters directly from the issuer of the credential its faith in the person representing them in a negotiation or meeting.

Counterfeiting of credentials is a constant and serious problem, irrespective of the type of credential. A great deal of effort goes into finding methods to reduce or prevent counterfeiting. In general, the greater the perceived value of the credential, the greater the problem with counterfeiting and the greater the lengths to which the issuer of the credential must go to prevent fraud.

In diplomacy, credentials, also known as a letter of credence, are documents that ambassadors, diplomatic ministers, plenipotentiary, and chargés d'affaires provide to the government to which they are accredited, for the purpose, chiefly, of communicating to the latter the envoy's diplomatic rank. It also contains a request that full credence be accorded to his official statements. Until his credentials have been presented and found in proper order, an envoy receives no official recognition. 

The credentials of an ambassador or minister plenipotentiary are signed by the head of state, those of a chargé d'affaires by the foreign minister. Diplomatic credentials are granted and withdrawn at the pleasure of the issuing authority, based on widely varying criteria. A receiving government may reject a diplomat’s credentials by declining to receive them, but in practice this rarely happens.

In medicine, the process of credentialing is a detailed review of all permissions granted a medical doctor, physician assistant or nurse practitioner at every institution at which he or she has worked in the past, to determine a risk profile for them at a new institution. It vets the practitioner for both receiving practice insurance and the ability to bill to insurance for patient care. As well, it certifies legal and administrative body requirements, such as the Joint Commission. 

Medical practitioners must also have credentials in the form of licenses issued by the government of the jurisdictions in which they practice, which they obtain after suitable education, training, and/or practical experience. Most medical credentials are granted for a practice specific group. They may also be withdrawn in the event of fraud or malpractice by their holders. Typically they require continuing education validation and renewal to continue practice.

Information systems commonly use credentials to control access to information or other resources. The classic combination of a user's account number or name and a secret password is a widely used example of IT credentials. An increasing number of information systems use other forms of documentation of credentials, such as biometrics (fingerprints, voice recognition, retinal scans), X.509, public key certificates, and so on.

Operators of vehicles such as automobiles, boats, and aircraft must have credentials in the form of government-issued licenses in many jurisdictions. Often the documentation of the license consists of a simple card or certificate that the operator keeps on his person while operating the vehicle, backed up by an archival record of the license at some central location. Licenses are granted to operators after a period of successful training and/or examination.

This type of credential often requires certification of good health and may also require psychological evaluations and screening for substance abuse.

Operator licenses often expire periodically and must be renewed at intervals. Renewal may simply be a formality, or it may require a new round of examinations and training.

Credentials in cryptography establish the identity of a party to communication. Usually they take the form of machine-readable cryptographic keys and/or passwords. Cryptographic credentials may be self-issued, or issued by a trusted third party; in many cases the only criterion for issuance is unambiguous association of the credential with a specific, real individual or other entity. Cryptographic credentials are often designed to expire after a certain period, although this is not mandatory. An x.509 certificate is an example of a cryptographic credential.

In military and government organizations, and some private organizations, a system of compartmenting information exists to prevent the uncontrolled dissemination of information considered to be sensitive or confidential. Persons with a legitimate need to have access to such information are issued "security clearances," which can be tracked and verified to ensure that no unauthorized persons gain access to protected information.

Security clearances are among the most carefully guarded credentials. Often they are granted to individuals only after a lengthy investigation and only after their need to have access to protected information has been adequately justified to the issuing authority. The most elaborate security-clearance systems are found in the world's military organizations. Some credentials of this type are considered so sensitive that their holders are not even permitted to acknowledge that they have them (except to authorized parties). Documentation of security clearances usually consists of records keep at a secure facility and verifiable on demand from authorized parties.

Breaches of security involving security clearances are often punished by specific statutory law, particularly if they occur in the context of deliberate espionage, whereas most other counterfeiting and misuse of credentials is punished by law only when used with deliberate intent to defraud in specific contexts. Security clearances are regularly withdrawn when they are no longer justified, or when the person holding them is determined to be too great a security risk.

In many democratic nations, press credentials are not required at the national or federal level for any publication of any kind. However, individual corporations, and certain government or military entities require press credentials, such as a press pass, as a formal invitation to members of the press which grants them rights to photographs or videos, press conferences, or interviews. Press credentials indicate that a person has been verified as working for a known publication, and holding a press pass typically allows that person special treatment or access rights.

Some governments impose restrictions on who may work as a journalist, requiring anyone working for the press to carry government-issued credentials. Restricting press credentials can be problematic because of its limitations on freedom of the press, particularly if government leaders selectively grant, withhold, or withdraw press credentials to disallow critique of government policy. Any press coverage published under governments that restrict journalism in this way is often treated with skepticism by others, and may not be considered any more truthful or informative than propaganda.

Some trades and professions in some jurisdictions require special credentials of anyone practicing the trade or profession. These credentials may or may not be associated with specific competencies or skills. In some cases, they exist mainly to control the number of people who are allowed to exercise a trade or profession, in order to control salaries and wages.

Persons acting as merchants, freelancers, etc., may require special credentials in some jurisdictions as well. Here again, the purpose is mainly to control the number of people working in this way, and sometimes also to track them for tax-reporting or other purposes like people evaluation.

The academic and professional world makes very extensive use of credentials, such as diplomas, degrees, certificates, and certifications, in order to attest to the completion of specific training or education programs by students, to attest to their successful completion of tests and exams, and to provide independent validation of an individual's possession of the knowledge, skills, and ability necessary to practice a particular occupation competently.

Documentation of academic and professional credentials usually consists of a printed, formal document. The issuing institution often maintains a record of the credential as well. Academic credentials are normally valid for the lifetime of the person to whom they are issued. Professional certifications are normally valid for a limited number of years, based on the pace of change in the certified profession, and require periodic recertification through reexamination (to demonstrate continuing competency as occupational standards of practice evolve) or continuing professional development (to demonstrate continually enhanced competency). 

Acquisition of these credentials often leads to increased economic mobility and work opportunity, especially for low-income people.

Titles are credentials that identify a person as belonging to a specific group, such as nobility or aristocracy, or a specific command grade in the military, or in other largely symbolic ways. They may or may not be associated with specific authority, and they do not usually attest to any specific competence or skill (although they may be associated with other credentials that do). A partial list of such titles includes



Dynamics:


</doc>
<doc id="5493220" url="https://en.wikipedia.org/wiki?curid=5493220" title="Interactional expertise">
Interactional expertise

Interactional expertise is part of a more complex classification of expertise developed by Harry Collins and Robert Evans (both based at Cardiff University). In this initial formulation interactional expertise was part of a threefold classification of substantive expertise that also included ‘no expertise’ and ‘contributory expertise’, by which they meant the expertise needed to contribute fully to all aspects of a domain of activity.

The distinction between these three different types of expertise can be illustrated by imagining the experience of a social science researcher approaching a topic for the first time. It is easy to see that, whether the research project is to be about plumbing or physics, most researchers will start from a position of ‘no expertise’ in that area. As the research project proceeds and the social interactions between the researcher and the plumbers or physicists continue, the social researcher will become increasingly knowledgeable about that topic. For example, they will find that they can talk more interestingly about plumbing or physics and ask more pertinent questions about how it works. Eventually, the researcher may even get to the point where they can answer questions about plumbing or physics as though they were a plumber or physicist even though they can’t do plumbing or physics. It is this kind of expertise that Collins and Evans call interactional expertise.

The important thing to note about interactional expertise is that the only thing the social researcher can’t do that a practicing plumber or physicist can do is the practical work of actually installing central heating or conducting experiments. It is this difference – the difference between being able to talk like a plumber/physicist and actually do plumbing/physics – that is the difference between interactional expertise (what the researcher has) and contributory expertise (what the plumbers and physicists have). Of course, plumbers and physicists who can talk fluently about their work will have both kinds of expertise.

In identifying this separate and distinctive kind of linguistic expertise, the idea of interactional expertise makes a clear break with other theories of expertise, particularly those developed in Science and Technology Studies, which tend to see expertise as a social status granted by others rather than a property of the individual. As discussed in more detail below, the idea of interactional expertise also differs from more traditional phenomenological theories of expertise, in which the embodied expertise of the contributory expert is well-recognised but the distinctively linguistic expertise of the interactional expert appears to have been overlooked. In this context, it must be emphasised that interactional expertise is a tacit knowledge-laden ability and thus similar in kind to the more embodied contributory expertise. This means that, like contributory expertise, interactional expertise cannot be acquired from books alone and it cannot be encoded in computerised expert systems. It is a specialised natural language and, as such; it can only be acquired by linguistic interaction with experts. The difference between interactional and contributory expertise is that, in the case of interactional expertise, the tacit knowledge pertains to the language of the domain but not its practice. In the case of contributory expertise, tacit knowledge relating to both the language and practice must be acquired.

The concept of interactional expertise 

In standard philosophy of knowledge the key distinction is between knowledge that is embodied and knowledge that is formally and explicitly articulated. In this dichotomous formulation, knowledge exists either as codified rules and facts or as some intangible property of the body that performs the task. This distinction forms the basis of the key debate about Artificial Intelligence research in which Hubert Dreyfus, starting from Heidegger argued that because computers don’t have bodies they can’t do what humans do and will not, therefore, succeed in becoming intelligent, no matter how sophisticated and detailed the knowledge base and rules with which they are programmed (see Dreyfus 1972).

In 1990, Harry Collins developed an alternative critique of AI which, although similar to Dreyfus’s in that it suggested fundamental limits to what AI could achieve, grounded this explanation in an understanding of socialisation rather than embodiment. Collins’s argument was that because computers are asocial objects that cannot be socialised into the life of a community, then they cannot be intelligent. In this sense, Collins is taking the alternative to the 'thinking machine' first proposed by Alan Turing in 1950 (and now known as the Turing Test) in which so-called intelligence in a machine is defined as the ability to hold a conversation. In the Turing Test, the conversation is conducted via keyboards and the challenge for the AI community is to produce a computer that can give answers that are indistinguishable from those produced by a real human. Given that such interactions are by their very nature open-ended and context-dependent Collins argues that only a fully socialised intelligence will be able to respond appropriately to any of the new and potentially unknown sentences directed to it.

Although the argument was not made in these terms at the time, the concept of interactional expertise is important here. In the original critique of AI research, Collins distinguished between behaviour-specific action (capable of being encoded and reproduced by machines) and natural action (what humans do the rest of the time and which machines cannot reproduce). In a later work with Martin Kusch, this same distinction was re-cast as the distinction as mimeomorphic action (action performed in the same way each time and thus amenable to mechanical reproduction) and polimorphic action (actions that depend on context and local convention for their correct interpretation and continuation and thus not reproducible by machines, however sophisticated).

The link between these arguments, the embodiment debate and the idea of interactional expertise is the importance of natural language. If interactional expertise exists then it suggests that people who cannot perform a particular task or skill – and who therefore cannot have the embodied expertise associated with it – can still talk about that skill as if they did possess the embodied skills. Interactional expertise thus raises a key question about the “amount” of embodiment that is needed for expertise to be transferred. For proponents of the embodiment thesis, quite a lot of embodiment is needed as the expertise resides in the relative position, movement and feel of the body. From the perspective of interactional expertise much less embodiment is needed and, taken to its logical minimum, perhaps only the ability to hear and speak are needed.

The idea of interactional expertise also has many practical applications and accounts for many everyday practices and activities. The implication of interactional expertise is to legitimate commentary and opinion from individuals outside a group of contributory expertise without necessarily saying that all opinions and views are equally valid. Examples of circumstances in which some degree of interactional expertise would be important include:

Scientific papers and research are subject to peer review but, in most cases, the reviewers will be drawn from cognate or related fields. This is particularly common in research funding decisions, where the likelihood of an application being reviewed by non-specialists increases with the amount of money involved. Even in the case of smaller awards, and of peer reviewed papers, it is still often the case that reviewers will have contributory expertise in a different narrowly defined specialism to that of the author being reviewed. If interactional expertise did not exist, then justifying peer review would be difficult. If, however, reviewers can have expertise by virtue of their interaction with a range of cognate scientists, then the process of peer review seems reasonable.

Whilst there may be some skills that are more or less generic in the management of large organisations – presumably the kinds of skills that are taught on MBA schemes around the world – we can also ask if managers do better if they understand particularities of the business they are in charge of. Intuitively it seems reasonable to suggest that the manager of a newspaper should know something about how a journalist works or that a manager of a car factory should know something about how a Production-line works. Whilst this kind of thinking is formally included in many training schemes, the idea of interactional expertise allows us to ask about the kind of experience that is needed in order for managers who lack the embodied experience of writing copy or working on a production line to understand what this is like for those that do fulfil these roles. One implication of interactional expertise is that direct experience – working one’s way up through the ranks – may be less important than previously thought even though lots of interaction with those who do these tasks could still be important. Certainly in the management of large science projects, managers will work hard to acquire interactional expertise quickly.

In art, design, science, technology, medicine and public policy many activities are undertaken by interdisciplinary teams. In science and technology, these take the form of scientists and engineers from many different disciplines working together on a single project. It is this situation, in which different groups of specialists with different, mutually incompatible and sometimes incomprehensible ideas nonetheless manage to find a way of communicating with each other and working together that inspired Peter Galison to develop the original trading zones metaphor. Similar teams are often found in public health settings, where cases are decided by multi-disciplinary teams comprising social workers, psychologists, psychiatrists, lawyers and so on. In the trading zone case, these teams work by developing a new composite language, called a pidgin or creole language, which the group shares and uses to communicate. The idea of trading zones has been developed by Mike Gorman, who has identified different types of trading zones, and examined their operation in a range of settings, including nanotechnology. Interactional expertise offers an alternative to this approach. Instead of a new language emerging, some members of the group learn the language of the others and shift back and forth between the two worlds. This is more akin to translation between two cultures rather than the creation of a new, shared, culture.

Most journalists cover many different topics in their career but some focus on a specific area, becoming specialist journalists, covering particular beats like politics, medicine, science, environment, security and so on. In the case of science, but in other areas too, the job of journalists is to render the specialist expertise of some esoteric group intelligible and relevant to ordinary folk. In doing so, they interpret events and place them in a broader context. In many cases, journalists do this by presenting ‘both sides of the argument’ in order to provide a balanced story and prevent accusations of bias. This is fine in principle but difficult in practice, particularly for science, as it requires the journalist to make a judgement about how credible a scientific claim is and thus how it should be reported. For example, should it be reported in a balanced way, in which two more or less equal sides get to make contrasting claims, as a story about a fringe, maverick or otherwise highly uncertain claim being made but not widely supported, or simply ignored as nonsense and not reported at all? In the UK, the reporting of the MMR controversy arguably adopted the ‘balanced approach’ for too long, thus lending greater credibility to the claims that MMR was dangerous than they deserved according to most members of the scientific community. In the latter stages of the debate these stories were often produced by the general news journalists and not the specialist health or science journalists who, by virtue of what we might call their interactional expertise, no longer saw the claims as credible.





</doc>
<doc id="1519168" url="https://en.wikipedia.org/wiki?curid=1519168" title="Organizing Knowledge Cognitively">
Organizing Knowledge Cognitively

People sort and store knowledge in many different ways. The main storage types are: Concepts, Schemes and Scripts, and Personal Theories.

A concept is a system of grouping and categorizing our brain uses to sort and store information. Concepts change and adapt as the amount of knowledge about a particular subject changes and grows. For example, as a child we were told that dogs and cats are animals. The concept of an animal might have been something furry with four legs. As school progressed and we learned more about animals the concept changed to incorporate everything from mammals to amphibians to fish. 

Limited concepts can lead to two things: 

The theorist view states that creating a concept includes defining the distinct features and characteristics that are present in all examples of a concept. If an object or event shares defining features with a concept, that object or event can be said to be part of that concept. For example, an animal must eat food, a plant must grow, and a vertebrate must have a spine. Every example of an animal must have the defining feature or eating food, every plant must grow, and every vertebrate must have a spine to be included in the concept.

Most people have a mental prototype, or mental example of a concept. For example, when referring to the concept of "transportation" you might think of a car, bus, truck, or train, but not typically of a skateboard or a pogo stick. Once the prototype for a concept is found, compare new objects and experiences with that prototype. Objects or events similar to the prototype are readily accepted as instances of the concept. Objects and events that are different are often rejected as instances of the concept when, in fact, they are. 

Exemplars are similar to the prototype except your concept was formed by a mixture of different examples. This helps to limit undergeneralization, a common problem with using the prototype alone. For example, when developing the concept of birds, not only learn about sparrows and pigeons but penguins and ostriches. By learning from a variety of examples, the concept is more complete and less susceptible to error.

A scheme is simply an organized set of knowledge about specific items and events. Schemes give a general or common understanding of how things are. Schemes are not only a way to organize information but also influence our perception and interpretation of new things or experiences. A script is a scheme with a particular order or sequence. For example, if we hear a story about how a man left his house, got into his car, and went for a drive at night, we would assume that he had turned on his lights between getting in his car and going for a drive even though we are not told so. The general information about driving a car would be the scheme and the sequence of events in driving the car would be the script.

From birth, individuals form personal theories about their environment to explain the events and objects in their environment. Although these theories are based on observation and fact, they are always correct or complete. Theories grow and change the same way as concepts and schemes. Personal theories influence defining features of concepts, and thus influence the concepts themselves. This is an individual process; personal theories are formed without any outside help. This can often lead to misconceptions or false beliefs. The most correct personal theories are the ones based on the most correct concepts and schemes, the building blocks of theories.

Source: Educational Psychology- Developing Learners 4th Edition, Jeanne Ellis Ormrod


</doc>
<doc id="9511103" url="https://en.wikipedia.org/wiki?curid=9511103" title="Inert knowledge">
Inert knowledge

Inert knowledge is information which one can express but not use. The process of understanding by learners does not happen to that extent where the knowledge can be used for effective problem-solving in realistic situations.

The phenomenon of inert knowledge was first described in 1929 by Alfred North Whitehead:

An example for inert knowledge is vocabulary of a foreign language which is available during an exam but not in a real situation of communication.

An explanation for the problem of inert knowledge is that people often encode knowledge to a specific situation, so that later remindings occur only for highly similar situations.

In contrast so called conditionalized knowledge is knowledge about something which includes also knowledge as to the contexts in which that certain knowledge will be useful.


</doc>
<doc id="3468466" url="https://en.wikipedia.org/wiki?curid=3468466" title="Wise fool">
Wise fool

The wise fool, or the wisdom of the fool, is a form of literary paradox in which through a narrative a character recognized as a fool comes to be seen as a beholder of wisdom. A recognizable trope found in stories and artworks from antiquity to the twenty-first century, the wisdom of the fool often captures what reason fails to illuminate of a thing's meaning or significance; thus, the wise fool is often associated with the wisdom found through blind faith, reckless desire, hopeless romance, and wild abandon.

In turn, the wise fool is often opposed to learned or elite knowledge. While examples of the paradox can be found in a wide range of early world literature, from Greco-Roman works to the oral traditions of folk culture, the paradox received unprecedented attention from authors and artists during the Renaissance. More than Shakespeare for his range of clownish wise men or Cervantes for his lunatic genius Don Quijote, sixteenth century scholar Erasmus is often credited for creating the definitive wise fool and most famous paradox in western literature through his portrayal of Stultitia, the goddess of folly. Influential to all later fools, she shows the foolish ways of the wise and the wisdom of fools through delivering her own eulogy, "The Praise of Folly."

In his article, "The Wisdom of The Fool", Walter Kaiser illustrates that the varied names and words people have attributed to real fools in different societies when put altogether reveal the general characteristics of the wise fool as a literary construct: "empty-headed ("μάταιος", "inanis," fool), dull-witted ("μῶρος", "stultus," dolt, clown), feebleminded ("imbécile," dotard), and lacks understanding (ἄνοοσ, ἄφρων "in-sipiens"); that he is different from normal men (idiot); that he is either inarticulate ("Tor") or babbles incoherently ("fatuus") and is given to boisterous merrymaking ("buffone"); that he does not recognize the codes of propriety ("ineptus") and loves to mock others ("Narr"); that he acts like a child (νήπιος); and that he has a natural simplicity and innocence of heart (εὐήθης, natural, simpleton).

While society reprimands violent maniacs, destined to be locked away in jails or asylums, the harmless fool often receives kindnesses and benefits from the social elite. Seemingly guided by nothing other than natural instinct, the fool is not expected to grasp social conventions and thus is left to enjoy relative freedom, particularly in his or her freedom of speech. This unusual power dynamic is famously demonstrated through the fool in Shakespeare's "King Lear," who works in the royal court and remains the only character who Lear does not severely punish for speaking his mind about the king and his precarious situations. This ability to be reckless, honest, and free with language has greatly contributed to the wise fool's popularity in the literary imagination.

"To call a man a fool is not necessarily an insult, for the authentic life has frequently been pictured under the metaphor of the fool. In figures such as Socrates, Christ, and the Idiot of Dostoyevsky we see that foolishness and wisdom are not always what they seem to be." - Sam Keen, Apology for Wonder

The employment and occupation of the fool played a significant role in the ancient world. The Ancient Greek authors Xenophon and Athenaeus wrote of normal men hired to behave as insane fools and clowns while the Roman authors Lucian and Plautus left records of powerful Romans who housed deformed buffoons famous for their insolence and brazen madness. Plato, through the guise of Socrates, provides an early example of the wisdom of the fool in "The Republic" through the figure of an escaped prisoner in The Allegory of the Cave. The escaped prisoner, part of a group imprisoned from birth, returns to free his fellow inmates but is regarded as a madman in his attempts to convince his shackled friends of a greater world beyond the cave.

Numerous scholars have long regarded Socrates as the paramount wise fool of classical antiquity. Through what would come be to branded as Socratic irony, the philosopher was known to make fools of people who claimed to be wise by pretending to be an ignorant fool himself. His name also bears a strong association with the Socratic Paradox, "I know that I know nothing," a statement that has come to frame him in the oxymoron of the ignorant knower. In Plato's "Apology", this self admission of ignorance ultimately leads the oracle at Delphi to claim there is no man with greater wisdom than Socrates.

The wise fool manifested most commonly throughout the Middle Ages as a religious figure in stories and poetry. During the Islamic Golden Age (approx. 750 - 1280 CE), an entire literary genre formed around reports about the "intelligent insane." One book in particular, "Kitab Ugala al-majanin", by an-Naysaburi, a muslim author from the Abbasid Period, recounts the lives of numerous men and women recognized during their lifetimes as 'wise fools.' Folkloric variations of madmen, lost between wisdom and folly, also appear throughout the period's most enduring classic, "The Thousand and One Nights". Buhlil the Madman, also known as the Lunatic of Kufa and Wise Buhlil, is often credited as the prototype for the wise fool across the Middle East.

The fool for God's sake was a figure that appeared in both the Muslim and Christian world. Often wearing little to no clothes, this variant of the holy fool would forego all social customs and conventions and feign madness in order to be possessed with their creator's spirit. By the twelfth century in France, such feigning led to the Fête des Fous (Feast of Fools), a celebration in which clergy were allowed to behave as fools without inhibition or restraint. During the Crusades, Christ was recognized as a 'wise fool' figure through his childlike teachings that yet confounded the powerful and intellectual elite. Numerous other writers during this period would explore this theological paradox of the wise fool in Christ, sustaining the trope into the Renaissance.

The wise fool received tremendous popularity in the literary imagination during the Italian and English Renaissances. In Italian scholar Erasmus' "Moriae encomium," written in 1509 and first published in 1511, the author portrays Stultitia, the goddess of folly, and a wise fool herself, who asks what it means to be a fool and puts forth a brazen argument praising folly and claiming that all people are fools of one kind or another. According to scholar Walter Kaiser, Stultitia is "the foolish creation of the most learned man of his time, she is the literal embodiment of the word "oxymoron," and in her idiotic wisdom she represents the finest flowering of that fusion of Italian humanistic thought and northern piety which has been called Christian Humanism."

At the same time, Shakespeare greatly helped popularize the wise fool in the English theater through incorporating the trope in a variety of characters throughout many of his plays. While Shakespeare's early plays largely portray the wise fool in comic terms as a buffoon, the later plays characterize the fool in a much more melancholic and contemplative light. For example, in "King Lear", the Fool becomes the only one capable of speaking truth to the King and often takes on the role of revealing life's tragic nature to those around him. For Shakespeare, the trope became so well-known that when Olivia says of the clown Feste in Twelfth Night, "This fellow is wise enough to play the fool" (III.i.60), his audiences recognized it as a popular convention.

Numerous other authors rendered interpretations of the wise fool across the sixteenth and seventeenth centuries from Hans Sachs to Montaigne. The image of the wise fool is as well found in numerous Renaissance artworks by a range of artists including Breughel, Bosch, and Holbein the Younger. In Spain, Cervantes' novel Don Quixote exemplifies the world of the wise fool through both its title character and his companion, Sancho Panza.








</doc>
<doc id="10399772" url="https://en.wikipedia.org/wiki?curid=10399772" title="Agnotology">
Agnotology

Agnotology (formerly agnatology) is the study of culturally induced ignorance or doubt, particularly the publication of inaccurate or misleading scientific data. It was coined in 1995 by Robert N. Proctor, a Stanford University professor, and linguist Iain Boal. The word is based on the Neoclassical Greek word , "agnōsis", "not knowing" (cf. Attic Greek ἄγνωτος "unknown"), and , "-logia". Proctor cites as a prime example the tobacco industry's advertising campaign to manufacture doubt about the cancerous and other health effects of tobacco use. More generally, the term also highlights the condition where more knowledge of a subject leaves one more uncertain than before. 

David Dunning of Cornell University warns that "the internet is helping propagate ignorance... which makes [users] prey for powerful interests wishing to deliberately spread ignorance". Irvin C. Schick refers to "unknowledge" "to distinguish it from ignorance. He uses the example of "terra incognita" in early maps, noting that "The reconstruction of parts of the globe as uncharted territory is ... the "production of unknowledge", the transformation of those parts into potential objects of Western political and economic attention. It is the enabling of colonialism".

The causes of culturally induced ignorance include the influence of the media, Corporations, and governmental agencies, through secrecy and suppression of information, document destruction, and selective memory. Another example is climate denial, where oil companies paid teams of scientists to downplay the effects of climate change.

Agnotology also focuses on how and why diverse forms of knowledge do "not" "come to be", or are ignored or delayed. For example, knowledge about plate tectonics was censored and delayed for at least a decade because some evidence remained classified military information related to undersea warfare.

The term "agnotology" was first coined in a footnote in Proctor's 1995 book, "The Cancer Wars: How Politics Shapes What We Know and Don't Know About Cancer": "Historians and philosophers of science have tended to treat ignorance as an ever-expanding vacuum into which knowledge is sucked – or even, as Johannes Kepler once put it, as the mother who must die for science to be born. Ignorance, though, is more complex than this. It has a distinct and changing political geography that is often an excellent indicator of the politics of knowledge. We need a political agnotology to complement our political epistemologies".

Proctor was quoted using the term to describe his research "only half jokingly", as "agnotology" in a 2001 interview about his lapidary work with the colorful rock agate. He connected the two seemingly unrelated topics by noting the lack of geologic knowledge and study of agate since its first known description by Theophrastus in 300 BC, relative to the extensive research on other rocks and minerals such as diamonds, asbestos, granite, and coal, all of which have much higher commercial value. He said agate was a "victim of scientific disinterest", the same "structured apathy" he called "the social construction of ignorance".

He was later quoted as calling it "agnotology, the study of ignorance", in a 2003 "The New York Times" story on medical historians testifying as expert witnesses.

Proctor co-organized a pair of events with Londa Schiebinger, his wife, who is also a science history professor: the first was a workshop at the Pennsylvania State University in 2003 titled "Agnatology: The Cultural Production of Ignorance", and later a conference at Stanford University in 2005 titled "Agnotology: The Cultural Production of Ignorance".

In 2004, Londa Schiebinger
gave a more precise definition of agnotology in a paper on 18th-century voyages of scientific discovery and gender relations, and contrasted it with epistemology, the theory of knowledge, saying that the latter questions how we know while the former questions why we do not know: "Ignorance is often not merely the absence of knowledge but an outcome of cultural and political struggle".

Its use as a critical description of the political economy has been expanded upon by Michael Betancourt in a 2010 article titled "Immaterial Value and Scarcity in Digital Capitalism" and expanded in the book "The Critique of Digital Capitalism". His analysis is focused on the housing bubble as well as the bubble economy of the period from 1980 to 2008. Betancourt argues that this political economy should be termed "agnotologic capitalism" because the systemic production and maintenance of ignorance is a major feature that enables the economy to function as it allows the creation of a "bubble economy".

Betancourt's argument is posed in relation to the idea of affective labor. He states that The creation of systemic unknowns where any potential "fact" is always already countered by an alternative of apparently equal weight and value renders engagement with the conditions of reality – the very situations affective labor seeks to assuage – contentious and a source of confusion, reflected by the inability of participants in bubbles to be aware of the imminent collapse until after it has happened. The biopolitical paradigm of distraction, what [Juan Martin] Prada calls "life to enjoy", can only be maintained if the underlying strictures remain hidden from view. If affective labor works to reduce alienation, agnotology works to eliminate the potential for dissent.
In his view, the role of affective labor is to enable the continuation of the agnotologic effects that enable the maintenance of the capitalist status quo.

A similar word from the same Greek roots, "agnoiology", meaning "the science or study of ignorance, which determines its quality and conditions" or "the doctrine concerning those things of which we are necessarily ignorant" describes a branch of philosophy studied by James Frederick Ferrier in the 19th century.

Anthropologist Glenn Stone points out that most of the examples of agnotology (such as work promoting tobacco use) do not actually create a lack of knowledge so much as they create confusion. A more accurate term for such writing would be "ainigmology", from the root "ainigma" (as in "enigma"); in Greek this refers to riddles or to language that obscures the true meaning of a story.

The availability of such large amounts of knowledge in this information age may not necessarily be producing a knowledgeable citizenry. Instead it may be allowing many people to cherry-pick information in blogs or news that reinforces their existing beliefs.
and to be distracted from new knowledge by repetitive or base entertainments. There is conflicting evidence on how television viewing affects value formation and intelligence.

An emerging new scientific discipline that has connections to agnotology is cognitronics:
The field of cognitronics appears to be growing as international conferences have centered on the topic. The 2013 conference was held in Slovenia.





</doc>
<doc id="851927" url="https://en.wikipedia.org/wiki?curid=851927" title="Obscurantism">
Obscurantism

Obscurantism ( and ) is the practice of deliberately presenting information in an imprecise and recondite manner, often designed to forestall further inquiry and understanding. There are two historical and intellectual denotations of "Obscurantism": (1) the deliberate restriction of knowledge—opposition to disseminating knowledge; and (2) deliberate obscurity—an abstruse style (as in literature and art) characterized by deliberate vagueness. 

The term "obscurantism" derives from the title of the 16th-century satire "Epistolæ Obscurorum Virorum" ("Letters of Obscure Men", 1515–19), that was based upon the intellectual dispute between the German humanist Johann Reuchlin and the monk Johannes Pfefferkorn of the Dominican Order, about whether or not all Jewish books should be burned as un-Christian heresy. Earlier, in 1509, the monk Pfefferkorn had obtained permission from Maximilian I, Holy Roman Emperor (1486–1519), to burn all copies of the Talmud (Jewish law and Jewish ethics) known to be in the Holy Roman Empire (AD 926–1806); the "Letters of Obscure Men" satirized the Dominican arguments for burning "un-Christian" works.

In the 18th century, Enlightenment philosophers applied the term "obscurantist" to any enemy of intellectual enlightenment and the liberal diffusion of knowledge. In the 19th century, in distinguishing the varieties of obscurantism found in metaphysics; and theology from the "more subtle" obscurantism of the critical philosophy of Immanuel Kant, and of modern philosophical skepticism, Friedrich Nietzsche said: "The essential element in the black art of obscurantism is not that it wants to darken individual understanding, but that it wants to blacken our picture of the world, and darken our idea of existence."

In restricting knowledge to an élite ruling class of "the few", obscurantism is fundamentally anti-democratic, because its component anti-intellectualism and elitism exclude the people as intellectually unworthy of knowing the facts and truth about the government of their City-State. 

In 18th century monarchic France, the Marquis de Condorcet, as a political scientist, documented the aristocracy's obscurantism about the social problems that provoked the French Revolution (1789–99) that deposed them and their King, Louis XVI of France.

In the 19th century, the mathematician William Kingdon Clifford, an early proponent of Darwinism, devoted some writings to uprooting obscurantism in England, after hearing clerics—who privately agreed with him about evolution—publicly denounce evolution as un-Christian. Moreover, in the realm of organized religion, obscurantism is a distinct strain of thought independent of theologic allegiance. The distinction is that fundamentalism presupposes sincere religious belief, whereas obscurantism is based upon minority manipulation of the popular faith as political praxis; cf. Censorship.

In the 20th century, the American conservative political philosopher Leo Strauss, for whom philosophy and politics intertwined, and his Neo-conservative adherents adopted the notion of government by the enlightened few as political strategy. He noted that intellectuals, dating from Plato, confronted the dilemma of either an informed populace "interfering" with government, or if it were possible for good politicians to be truthful and still govern to maintain a stable society—hence the noble lie necessary in securing public acquiescence. In "The City and Man" (1964), he discusses the myths in "The Republic" that Plato proposes effective governing requires, among them, the belief that the country (land) ruled by the State belongs to it (despite some having been conquered from others), and that citizenship derives from more than the accident of birth in the City-State. Thus, in the "New Yorker" magazine article "Selective Intelligence", Seymour Hersh observes that Strauss endorsed the "noble lie" concept: the myths politicians use in maintaining a cohesive society.

Shadia Drury criticized Strauss's acceptance of dissembling and deception of the populace as "the peculiar justice of the wise", whereas Plato proposed the Noble Lie as based upon moral good. In criticizing "Natural Right and History" (1953), she said that "Strauss thinks that the superiority of the ruling philosophers is an intellectual superiority and not a moral one ... [he] is the only interpreter who gives a sinister reading to Plato, and then celebrates him."

Leo Strauss also was criticized for proposing the notion of "esoteric" meanings to ancient texts, obscure knowledge inaccessible to the "ordinary" intellect. In "Persecution and the Art of Writing" (1952), he proposes that some philosophers write esoterically to avert persecution by the political or religious authorities, and, per his knowledge of Maimonides, Al Farabi, and Plato, proposed that an esoteric writing style is proper for the philosophic text. Rather than explicitly presenting his thoughts, the philosopher's esoteric writing compels the reader to think independently of the text, and so learn. In the "Phædrus", Socrates notes that writing does not reply to questions, but invites dialogue with the reader, thereby minimizing the problems of grasping the written word. Strauss noted that one of writing's political dangers is students' too-readily accepting dangerous ideas—as in the trial of Socrates, wherein the relationship with Alcibiades was used to prosecute him.

For Leo Strauss, philosophers' texts offered the reader lucid "exoteric" (salutary) and obscure "esoteric" (true) teachings, which are concealed to the reader of ordinary intellect; emphasizing that writers often left contradictions and other errors to encourage the reader's more scrupulous (re-)reading of the text. In observing and maintaining the "exoteric – esoteric" dichotomy, Strauss was accused of obscurantism, and for writing esoterically. 

In the "Wired" magazine article, "Why the Future Doesn't Need Us" (April 2000), the computer scientist Bill Joy, then a Sun Microsystems chief scientist, in the sub-title proposed that: "Our most powerful twenty-first-century technologies—robotics, genetic engineering, and nanotech[nology]—are threatening to make humans an endangered species"; in the body, he posits that:

Joy's proposal for limiting the dissemination of "certain" knowledge, in behalf of preserving society, was quickly likened to obscurantism. A year later, the American Association for the Advancement of Science, in the "Science and Technology Policy Yearbook 2001", published the article "A Response to Bill Joy and the Doom-and-Gloom Technofuturists", wherein the computer scientists John Seely Brown and Paul Duguid countered his proposal as technological tunnel vision, and the predicted technologically derived problems as infeasible, for disregarding the influence of non-scientists upon such societal problems.

In the essay "Why I Am Not a Conservative" (1960), the economist Friedrich von Hayek said that political conservatism is ideologically unrealistic, because of the conservative person's inability to adapt to changing human realities and refusal to offer a positive political program that benefits everyone in a society. In that context, Hayek used the term "obscurantism" differently, to denote and describe the denial of the empirical truth of scientific theory, because of the disagreeable moral consequences that might arise from acceptance of fact.

The second sense of "obscurantism" denotes making knowledge abstruse, that is, difficult to grasp. In the 19th and 20th centuries obscurantism became a polemical term for accusing an author of deliberately writing obscurely, in order to hide his or her intellectual vacuousness. Philosophers who are neither empiricists nor positivists often are considered obscurantists when describing the abstract concepts of their disciplines. For philosophic reasons, such authors might modify or reject verifiability, falsifiability, and logical non-contradiction. From that perspective, obscure (clouded, vague, abstruse) writing does not necessarily indicate that the writer has a poor grasp of the subject, because unintelligible writing sometimes is purposeful and philosophically considered.

In contemporary discussions of virtue ethics, Aristotle's "Nicomachean Ethics" ("The Ethics") stands accused of ethical obscurantism, because of the technical, philosophic language and writing style, and their purpose being the education of a cultured governing elite.

Immanuel Kant employed technical terms that were not commonly understood by the layman. Arthur Schopenhauer contended that post-Kantian philosophers such as Johann Gottlieb Fichte, Friedrich Wilhelm Joseph Schelling, and Georg Wilhelm Friedrich Hegel deliberately imitated the abstruse style of writing practiced by Kant.

G. W. F. Hegel's philosophy, and the philosophies of those he influenced, especially Karl Marx, have been accused of obscurantism. Analytic and positivistic philosophers, such as A. J. Ayer, Bertrand Russell, and the critical-rationalist Karl Popper, accused Hegel and Hegelianism of being obscure. About Hegel's philosophy, Schopenhauer wrote that it is: "... a colossal piece of mystification, which will yet provide posterity with an inexhaustible theme for laughter at our times, that it is a pseudo-philosophy paralyzing all mental powers, stifling all real thinking, and, by the most outrageous misuse of language, putting in its place the hollowest, most senseless, thoughtless, and, as is confirmed by its success, most stupefying verbiage. ..."

Nevertheless, biographer Terry Pinkard notes "Hegel has refused to go away, even in analytic philosophy, itself." Hegel was aware of his obscurantism, and perceived it as part of philosophical thinking: to accept and transcend the limitations of quotidian (everyday) thought and its concepts. In the essay "Who Thinks Abstractly?", he said that it is not the philosopher who thinks abstractly, but the layman, who uses concepts as givens that are immutable, without context. It is the philosopher who thinks concretely, because he transcends the limits of quotidian concepts, in order to understand their broader context. This makes philosophical thought and language appear obscure, esoteric, and mysterious to the layman.

In his early works, Karl Marx criticized German and French philosophy, especially German Idealism, for its traditions of German irrationalism and ideologically motivated obscurantism. Later thinkers whom he influenced, such as the philosopher György Lukács and social theorist Jürgen Habermas, followed with similar arguments of their own. However, philosophers such as Karl Popper and Friedrich Hayek in turn criticized Marx and Marxist philosophy as obscurantist (however, see above for Hayek's particular interpretation of the term).

Martin Heidegger, and those influenced by him, such as Jacques Derrida and Emmanuel Levinas, have been labeled obscurantists by critics from analytic philosophy and the Frankfurt School of critical theory. Of Heidegger, Bertrand Russell wrote, "his philosophy is extremely obscure. One cannot help suspecting that language is here running riot. An interesting point in his speculations is the insistence that nothingness is something positive. As with much else in Existentialism, this is a psychological observation made to pass for logic." That is Russell's complete entry on Heidegger, and it expresses the sentiments of many 20th-century analytic philosophers concerning Heidegger.

In their obituaries, "Jacques Derrida, Abstruse Theorist, Dies at 74" (10 October 2004) and "Obituary of Jacques Derrida, French intellectual" (21 October 2004), "The New York Times" newspaper and "The Economist" magazine, described Derrida as a deliberately obscure philosopher.

In "Contingency, Irony, and Solidarity" (1989), Richard Rorty proposed that in "The Post Card: From Socrates to Freud and Beyond" (1978), Jacques Derrida purposefully used undefinable words (e.g. Différance), and used defined words in contexts so diverse that they render the words unintelligible, hence, the reader is unable to establish a context for his literary self. In that way, the philosopher Derrida escapes metaphysical accounts of his work. Since the work ostensibly contains no metaphysics, Derrida has, consequently, escaped metaphysics.

Derrida's philosophic work is especially controversial among American and British academics, as when the University of Cambridge awarded him an honorary doctorate, despite opposition from among the Cambridge philosophy faculty and analytical philosophers worldwide. In opposing the decision, philosophers including Barry Smith, W. V. O. Quine, David Armstrong, Ruth Barcan Marcus, René Thom, and twelve others, published a letter of protestation in "The Times" of London, arguing that "his works employ a written style that defies comprehension ... [thus] Academic status based on what seems to us to be little more than semi-intelligible attacks upon the values of reason, truth, and scholarship is not, we submit, sufficient grounds for the awarding of an honorary degree in a distinguished university."

In the "New York Review of Books" article "An Exchange on Deconstruction" (February 1984), John Searle comments on Deconstruction: "... anyone who reads deconstructive texts with an open mind is likely to be struck by the same phenomena that initially surprised me: the low level of philosophical argumentation, the deliberate obscurantism of the prose, the wildly exaggerated claims, and the constant striving to give the appearance of profundity, by making claims that seem paradoxical, but under analysis often turn out to be silly or trivial."

Jacques Lacan was an intellectual who defended obscurantism to a degree. To his students' complaint about the deliberate obscurity of his lectures, he replied: "The less you understand, the better you listen." In the 1973 seminar "Encore", he said that his "Écrits" ("Writings") were not to be understood, but would effect a meaning in the reader, like that induced by mystical texts. The obscurity is not in his writing style, but in the repeated allusions to Hegel, derived from Alexandre Kojève's lectures on Hegel, and similar theoretic divergences.

The Sokal Affair (1996) was a publishing hoax that the professor of physics Alan Sokal perpetrated on the editors and readers of "Social Text", an academic journal of post-modern cultural studies that was not then a peer-reviewed publication. In 1996, as an experiment testing editorial integrity (fact-checking, verification, peer review, etc.), Sokal submitted "Transgressing the Boundaries: Towards a Transformative Hermeneutics of Quantum Gravity", a pseudoscientific article proposing that physical reality is a social construct, in order to learn if "Social Text" would "publish an article liberally salted with nonsense if: (a) it sounded good, and, (b) it flattered the editors' ideological preconceptions". Sokal's fake article was published in the Spring/Summer 1996 issue of "Social Text", which was dedicated to the Science Wars about the conceptual validity of scientific objectivity and the nature of scientific theory, among scientific realists and postmodern critics in American universities.

Sokal's reason for publication of a false article was that postmodernist critics questioned the objectivity of science, by criticising the scientific method and the nature of knowledge, usually in the disciplines of cultural studies, cultural anthropology, feminist studies, comparative literature, media studies, and science and technology studies. Whereas the scientific realists countered that objective scientific knowledge exists, riposting that postmodernist critics almost knew nothing of the science they criticized. In the event, editorial deference to "Academic Authority" (the Author-Professor) prompted the editors of "Social Text" not to fact-check Sokal's manuscript by submitting it to peer review by a scientist.

Concerning the lack of editorial integrity shown by the publication of his fake article in "Social Text" magazine, Sokal addressed the matter in the May 1996 edition of the "Lingua Franca" journal, in the article "A Physicist Experiments With Cultural Studies", in which he revealed that his transformative hermeneutics article was a parody, submitted "to test the prevailing intellectual standards", and concluded that, as an academic publication, "Social Text" ignored the requisite intellectual rigor of verification and "felt comfortable publishing an article on quantum physics without bothering to consult anyone knowledgeable in the subject".

Moreover, as a public intellectual, Sokal said his hoax was an action protesting against the contemporary tendency towards obscurantism—abstruse, esoteric, and vague writing in the social sciences:

In short, my concern over the spread of subjectivist thinking is both intellectual and political. Intellectually, the problem with such doctrines is that they are false (when not simply meaningless). There is a real world; its properties are not merely social constructions; facts and evidence do matter. What sane person would contend otherwise? And yet, much contemporary academic theorizing consists precisely of attempts to blur these obvious truths—the utter absurdity of it all being concealed through obscure and pretentious language.

Moreover, independent of the hoax, as a pseudoscientific opus, the article "Transgressing the Boundaries: Towards a Transformative Hermeneutics of Quantum Gravity" is described as an exemplar "pastiche of left-wing cant, fawning references, grandiose quotations, and outright nonsense, centered on the claim that physical reality is merely a social construct." Similarly to whataboutism, obscurantism is used by elevating the readers prejudices to a grandiose value-laden assumption, belief, principle(s) or pseudoscience that does not deconstruct opposing claims and is stalling a priori and/or asserting confusing jargon or technical speak to describe events, which may deny the real world existence of physical properties.



</doc>
<doc id="2231743" url="https://en.wikipedia.org/wiki?curid=2231743" title="Explicit knowledge">
Explicit knowledge

Explicit knowledge (also expressive knowledge) is knowledge that can be readily articulated, codified, stored and accessed. It can be easily transmitted to others. Most forms of explicit knowledge can be stored in certain media. Explicit knowledge is often seen as complementary to tacit knowledge.

The information contained in encyclopedias and textbooks are good examples of explicit knowledge. The most common forms of explicit knowledge are manuals, documents, procedures, and how-to videos. Knowledge also can be audio-visual. Engineering works and product design can be seen as other forms of explicit knowledge where human skills, motives and knowledge are externalized. 




</doc>
<doc id="10115122" url="https://en.wikipedia.org/wiki?curid=10115122" title="Community of practice">
Community of practice

A community of practice (CoP) is a group of people who share a craft or a profession. The concept was first proposed by cognitive anthropologist Jean Lave and educational theorist Etienne Wenger in their 1991 book "Situated Learning" . Wenger then significantly expanded on the concept in his 1998 book "Communities of Practice" .

A CoP can evolve naturally because of the members' common interest in a particular domain or area, or it can be created deliberately with the goal of gaining knowledge related to a specific field. It is through the process of sharing information and experiences with the group that members learn from each other, and have an opportunity to develop personally and professionally .

CoPs can exist in physical settings, for example, a lunch room at work, a field setting, a factory floor, or elsewhere in the environment, but members of CoPs do not have to be co-located. They form a "virtual community of practice" (VCoP) when they collaborate online, such as within discussion boards and newsgroups, or a "mobile community of practice" (MCoP) when members communicate with one another via mobile phones and participate in community work on the go.

Communities of practice are not new phenomena: this type of learning has existed for as long as people have been learning and sharing their experiences through storytelling. The idea is rooted in American pragmatism, especially C. S. Peirce's concept of the "community of inquiry" , but also John Dewey's principle of learning through occupation .

Since the publication of "Situated Learning: Legitimate Peripheral Participation" , communities of practice have been the focus of attention, first as a theory of learning and later as part of the field of knowledge management. See for a review of how the concept has changed over the years. offers a more critical view of the different ways in which the term communities of practice can be interpreted.

To understand how learning occurs outside the classroom while at the Institute for Research on Learning, Lave and Wenger studied how newcomers or novices to informal groups become established members of those groups . Lave and Wenger first used the term communities of practice to describe learning through practice and participation, which they named situated learning.

The structure of the community was created over time through a process of legitimate peripheral participation. Legitimation and participation together define the characteristic ways of belonging to a community whereas peripherality and participation are concerned with location and identity in the social world .

Lave and Wenger's research looked at how apprenticeships help people learn. They found that when newcomers join an established group or community, they spend some time initially observing and perhaps performing simple tasks in basic roles as they learn how the group works and how they can participate (an apprentice electrician, for example would watch and learn before actually doing any electrical work; initially taking on small simple jobs and eventually more complicated ones). Lave and Wenger described this socialization process as legitimate peripheral participation. The term "community of practice" is that group that Lave and Wenger referred to, who share a common interest and a desire to learn from and contribute to the community with their variety of experiences .

In his later work, abandoned the concept of legitimate peripheral participation and used the idea of an inherent tension in a duality instead. He identifies four dualities that exist in communities of practice, participation-reification, designed-emergent, identification-negotiability and local-global, although the participation-reification duality has been the focus of particular interest because of its links to knowledge management.

He describes the structure of a CoP as consisting of three interrelated terms: 'mutual engagement', 'joint enterprise' and 'shared repertoire' .


For Etienne Wenger, learning is central to human identity. A primary focus of Wenger's more recent work is on learning as social participation – the individual as an active participant in the practices of social communities, and in the construction of his/her identity through these communities . In this context, a community of practice is a group of individuals participating in communal activity, and experiencing/continuously creating their shared identity through engaging in and contributing to the practices of their communities.

The structural characteristics of a community of practice are again redefined to a domain of knowledge, a notion of community and a practice.




In many organizations, communities of practice have become an integral part of the organization structure . These communities take on knowledge stewarding tasks that were formerly covered by more formal organizational structures. In some organizations there are both formal and informal communities of practice. There is a great deal of interest within organizations to encourage, support, and sponsor communities of practice in order to benefit from shared knowledge that may lead to higher productivity . Communities of practice are now viewed by many in the business setting as a means to capturing the tacit knowledge, or the know-how that is not so easily articulated.

An important aspect and function of communities of practice is increasing organization performance. identify four areas of organizational performance that can be affected by communities of practice: 

The communities Lave and Wenger studied were naturally forming as practitioners of craft and skill-based activities met to share experiences and insights .

Lave and Wenger observed situated learning within a community of practice among Yucatán midwives, Liberian tailors, navy quartermasters and meat cutters as well as insurance claims processors. . Other fields have made use of the concept of CoPs. Examples include education , sociolinguistics, material anthropology, medical education, second language acquisition , Parliamentary Budget Offices , health care and business sectors, and child mental health practice (AMBIT).

A famous example of a community of practice within an organization is that which developed around the Xerox customer service representatives who repaired the machines in the field . The Xerox reps began exchanging tips and tricks over informal meetings over breakfast or lunch and eventually Xerox saw the value of these interactions and created the Eureka project to allow these interactions to be shared across the global network of representatives. The Eureka database has been estimated to have saved the corporation $100 million.

Collaboration constellations differ in various ways. Some are under organizational control (e.g., teams, see below) others, like CoPs, are self-organized or under the control of individuals. For examples of how these and other collaboration types vary in terms of their temporal or boundary focus and the basis of their members' relationships, see .

A project team differs from a community of practice in several significant ways . 

By contrast,

In addition to the distinction between CoP and other types of organizational groupings found in the workplace, in some cases it is useful to differentiate CoP from community of interest (CoI).

Community of interest

Community of practice

Social capital is said to be a multi-dimensional concept, with both public and private facets (Bourdieu 1991). That is, social capital may provide value to both the individual and the group as a whole. Through informal connections that participants build in their community of practice, and in the process of sharing their expertise, learning from others, and participating in the group, members are said to be acquiring social capital – especially those members who demonstrate expertise and experience.

 describe three kinds of knowledge: "knowledge as object", "knowledge embedded within individuals", and "knowledge embedded in a community". Communities of Practice have become associated with finding, sharing, transferring, and archiving knowledge, as well as making explicit "expertise", or tacit knowledge. Tacit knowledge is considered to be those valuable context-based experiences that cannot easily be captured, codified and stored , also .

Because knowledge management is seen "primarily as a problem of capturing, organizing, and retrieving information, evoking notions of databases, documents, query languages, and data mining" , the community of practice, collectively and individually, is considered a rich potential source of helpful information in the form of actual experiences; in other words, best practices.

Thus, for knowledge management, a community of practice is one source of content and context that if codified, documented and archived can be accessed for later use.

Members of communities of practice are thought to be more efficient and effective conduits of information and experiences. While organizations tend to provide manuals to meet the training needs of their employees, CoPs help foster the process of storytelling among colleagues which, in turn, helps them strengthen their skills on the job .

Studies have shown that workers spend a third of their time looking for information and are five times more likely to turn to a co-worker rather than an explicit source of information (book, manual, or database) . Time is saved by conferring with members of a CoP. Members of the community have tacit knowledge, which can be difficult to store and retrieve outside. For example, one person can share the best way to handle a situation based on his experiences, which may enable the other person to avoid mistakes and shorten the learning curve. In a CoP, members can openly discuss and brainstorm about a project, which can lead to new capabilities. The type of information that is shared and learned in a CoP is boundless . clarifies the difference between tacit knowledge, or knowing "how", and explicit knowledge, or knowing "what". Performing optimally in a job requires being able to convert theory into practice. Communities of practice help the individual bridge the gap between knowing "what" and knowing "how" .

As members of communities of practice, individuals report increased communication with people (professionals, interested parties, hobbyists), less dependence on geographic proximity, and the generation of new knowledge .

Communicating with others in a community of practice involves creating "social presence". defines social presence as "the degree of salience of another person in an interaction and the consequent salience of an interpersonal relationship" (p. 38). It is believed that social presence affects how likely an individual is of participating in a CoP (especially in online environments) . Management of a community of practice often faces many barriers that inhibit individuals from engaging in knowledge exchange. Some of the reasons for these barriers are egos and personal attacks, large overwhelming CoPs, and time constraints .

Motivation to share knowledge is critical to success in communities of practice. Studies show that members are motivated to become active participants in a CoP when they view knowledge as meant for the public good, a moral obligation and/or as a community interest . Members of a community of practice can also be motivated to participate by using methods such as tangible returns (promotion, raises or bonuses), intangible returns (reputation, self-esteem) and community interest (exchange of practice related knowledge, interaction).

Collaboration is essential to ensuring that communities of practice thrive. Research has found that certain factors can indicate a higher level of collaboration in knowledge exchange in a business network . Sveiby and Simons found that more seasoned colleagues tend to foster a more collaborative culture. Additionally they noted that a higher educational level also predicts a tendency to favor collaboration.

What makes a community of practice succeed depends on the purpose and objective of the community as well as the interests and resources of the members of that community. Wenger identified seven actions that could be taken in order to cultivate communities of practice:




</doc>
<doc id="397245" url="https://en.wikipedia.org/wiki?curid=397245" title="Numeracy">
Numeracy

Numeracy is the ability to reason and to apply simple numerical concepts. Basic numeracy skills consist of comprehending fundamental arithmetics like addition, subtraction, multiplication, and division. For example, if one can understand simple mathematical equations such as 2 + 2 = 4, then one would be considered possessing at least basic numeric knowledge. Substantial aspects of numeracy also include number sense, operation sense, computation, measurement, geometry, probability and statistics. A numerically literate person can manage and respond to the mathematical demands of life.

By contrast, innumeracy (the lack of numeracy) can have a negative impact. Numeracy has an influence on career decisions, and risk perception towards health decisions. For example, innumeracy distorts risk perception towards health decisions and may negatively affect economic choices. "Greater numeracy has been associated with reduced susceptibility to framing effects, less influence of nonnumerical information such as mood states, and greater sensitivity to different levels of numerical risk".

Humans have evolved to mentally represent numbers in two major ways from observation (not formal math). These representations are often thought to be innate (see Numerical cognition), to be shared across human cultures, to be common to multiple species, and not to be the result of individual learning or cultural transmission. They are:

Approximate representations of numerical magnitude imply that one can relatively estimate and comprehend an amount if the number is large (see Approximate number system). For example, one experiment showed children and adults arrays of many dots. After briefly observing them, both groups could accurately estimate the approximate number of dots. However, distinguishing differences between large numbers of dots proved to be more challenging.

Precise representations of distinct individuals demonstrate that people are more accurate in estimating amounts and distinguishing differences when the numbers are relatively small (see Subitizing). For example, in one experiment, an experimenter presented an infant with two piles of crackers, one with two crackers the other with three. The experimenter then covered each pile with a cup. When allowed to choose a cup, the infant always chose the cup with more crackers because the infant could distinguish the difference.

Both systems—approximate representation of magnitude and precise representation quantity of individual items—have limited power. For example, neither allows representations of fractions or negative numbers. More complex representations require education. However, achievement in school mathematics correlates with an individual's unlearned approximate number sense.

Fundamental (or rudimentary) numeracy skills include understanding of the real number line, time, measurement, and estimation. Fundamental skills include basic skills (the ability to identify and understand numbers) and computational skills (the ability to perform simple arithmetical operations and compare numerical magnitudes).

More sophisticated numeracy skills include understanding of ratio concepts (notably fractions, proportions, percentages, and probabilities), and knowing when and how to perform multistep operations. Two categories of skills are included at the higher levels: the analytical skills (the ability to understand numerical information, such as required to interpret graphs and charts) and the statistical skills (the ability to apply higher probabilistic and statistical computation, such as conditional probabilities).

A variety of tests have been developed for assessing numeracy and health numeracy.

The first couple of years of childhood are considered to be a vital part of life for the development of numeracy and literacy. There are many components that play key roles in the development of numeracy at a young age, such as Socioeconomic Status (SES), parenting, Home Learning Environment (HLE), and age.

Children who are brought up in families with high SES tend to be more engaged in developmentally enhancing activities. These children are more likely to develop the necessary abilities to learn and to become more motivated to learn. More specifically, a mother's education level is considered to have an effect on the child's ability to achieve in numeracy. That is, mothers with a high level of education will tend to have children who succeed more in numeracy.

A number of studies have, moreover, proved that the education level of the mother is strongly correlated with the average age of getting married. More precisely, females who entered the marriage later, tend to have greater autonomy, chances for skills premium and level of education (i.e. numeracy). Hence, they were more likely to share this experience with children.

Parents are suggested to collaborate with their child in simple learning exercises, such as reading a book, painting, drawing, and playing with numbers. On a more expressive note, the act of using complex language, being more responsive towards the child, and establishing warm interactions are recommended to parents with the confirmation of positive numeracy outcomes. When discussing beneficial parenting behaviors, a feedback loop is formed because pleased parents are more willing to interact with their child, which in essence promotes better development in the child.

Along with parenting and SES, a strong home-learning environment increases the likelihood of the child being prepared for comprehending complex mathematical schooling. For example, if a child is influenced by many learning activities in the household, such as puzzles, coloring books, mazes, or books with picture riddles, then they will be more prepared to face school activities.

Age is accounted for when discussing the development of numeracy in children. Children under the age of 5 have the best opportunity to absorb basic numeracy skills. After the age of seven, achievement of basic numeracy skills become less influential. For example, a study was conducted to compare the reading and mathematical abilities between children of ages five and seven, each in three different mental capacity groups (underachieving, average, and overachieving). The differences in the amount of knowledge retained were greater between the three different groups aged five than between the groups aged seven. This reveals that those of younger ages have an opportunity to retain more information, like numeracy.

There seems to be a relationship between literacy and numeracy, which can be seen in young children. Depending on the level of literacy or numeracy at a young age, one can predict the growth of literacy and/ or numeracy skills in future development. There is some evidence that humans may have an inborn sense of number. In one study for example, five-month-old infants were shown two dolls, which were then hidden with a screen. The babies saw the experimenter pull one doll from behind the screen. Without the child's knowledge, a second experimenter could remove, or add dolls, unseen behind the screen. When the screen was removed, the infants showed more surprise at an unexpected number (for example, if there were still two dolls). Some researchers have concluded that the babies were able to count, although others doubt this and claim the infants noticed surface area rather than number.

Numeracy has a huge impact on employment. In a work environment, numeracy can be a controlling factor affecting career achievements and failures. Many professions require individuals to have a well-developed sense of numeracy, for example: mathematician, physicist, accountant, actuary, Risk Analyst, financial analyst, engineer, and architect. Even outside these specialized areas, the lack of proper numeracy skills can reduce employment opportunities and promotions, resulting in unskilled manual careers, low-paying jobs, and even unemployment. For example, carpenters and interior designers need to be able to measure, use fractions, and handle budgets. Another example pertaining to numeracy influencing employment was demonstrated at the Poynter Institute. The Poynter Institute has recently included numeracy as one of the skills required by competent journalists. Max Frankel, former executive editor of "The New York Times", argues that "deploying numbers skillfully is as important to communication as deploying verbs". Unfortunately, it is evident that journalists often show poor numeracy skills. In a study by the Society of Professional Journalists, 58% of job applicants interviewed by broadcast news directors lacked an adequate understanding of statistical materials.

With regards to assessing applicants for an employment position, psychometric numerical reasoning tests have been created by occupational psychologists, who are involved in the study of numeracy. These psychometric numerical reasoning tests are used to assess an applicant's ability to comprehend and apply numbers. These tests are sometimes administered with a time limit, resulting in the need for the test-taker to think quickly and concisely. Research has shown that these tests are very useful in evaluating potential applicants because they do not allow the applicants to prepare for the test, unlike interview questions. This suggests that an applicant's results are reliable and accurate

These psychometric numerical reasoning tests first became prevalent during the 1980s, following the pioneering work of psychologists, such as P. Kline. In 1986, P. Kline published a book entitled "A handbook of test construction: Introduction to psychometric design", which explained that psychometric testing could provide reliable and objective results. These findings could then be used to effectively assess a candidate's abilities in numeracy. In the future, psychometric numerical reasoning tests will continue to be used in employment assessments to fairly and accurately differentiate and evaluate possible employment applicants.

The term "innumeracy" is a neologism, coined by analogy with illiteracy. "Innumeracy" refers to a lack of ability to reason with numbers. The term was coined by cognitive scientist Douglas Hofstadter; however, it was popularized in 1989 by mathematician John Allen Paulos in his book "Innumeracy: Mathematical Illiteracy and its Consequences".

Developmental dyscalculia refers to a persistent and specific impairment of basic numerical-arithmetical skills learning in the context of normal intelligence.

The root cause of innumeracy varies. Innumeracy has been seen in those suffering from poor education and childhood deprivation of numeracy. Innumeracy is apparent in children during the transition of numerical skills obtained before schooling and the new skills taught in the education departments because of their memory capacity to comprehend the material. Patterns of innumeracy have also been observed depending on age, gender, and race. Older adults have been associated with lower numeracy skills than younger adults. Men have been identified to have higher numeracy skills than women. Some studies seem to indicate young people of African heritage tend to have lower numeracy skills. The Trends in International Mathematics and Science Study (TIMSS) in which children at fourth-grade (average 10 to 11 years) and eighth-grade (average 14 to 15 years) from 49 countries were tested on mathematical comprehension. The assessment included tests for number, algebra (also called patterns and relationships at fourth grade), measurement, geometry, and data. The latest study, in 2003, found that children from Singapore at both grade levels had the highest performance. Countries like Hong Kong SAR, Japan, and Taiwan also shared high levels of numeracy. The lowest scores were found in countries like South Africa, Ghana, and Saudi Arabia. Another finding showed a noticeable difference between boys and girls with some exceptions. For example, girls performed significantly better in Singapore, and boys performed significantly better in the United States.

There is a theory that innumeracy is more common than illiteracy when dividing cognitive abilities into two separate categories. David C. Geary, a notable cognitive developmental and evolutionary psychologist from the University of Missouri, created the terms "biological primary abilities" and "biological secondary abilities". Biological primary abilities evolve over time and are necessary for survival. Such abilities include speaking a common language or knowledge of simple mathematics. Biological secondary abilities are attained through personal experiences and cultural customs, such as reading or high level mathematics learned through schooling. Literacy and numeracy are similar in the sense that they are both important skills used in life. However, they differ in the sorts of mental demands each makes. Literacy consists of acquiring vocabulary and grammatical sophistication, which seem to be more closely related to memorization, whereas numeracy involves manipulating concepts, such as in calculus or geometry, and builds from basic numeracy skills. This could be a potential explanation of the challenge of being numerate.

Health numeracy has been defined as "the degree to which individuals have the capacity to access, process, interpret, communicate, and act on numerical, quantitative, graphical, biostatistical, and probabilistic health information needed to make effective health decisions". The concept of health numeracy is a component of the concept of health literacy. Health numeracy and health literacy can be thought of as the combination of skills needed for understanding risk and making good choices in health-related behavior.

Health numeracy requires basic numeracy but also more advanced analytical and statistical skills. For instance, health numeracy also requires the ability to understand probabilities or relative frequencies in various numerical and graphical formats, and to engage in Bayesian inference, while avoiding errors sometimes associated with Bayesian reasoning (see Base rate fallacy, Conservatism (Bayesian)). Health numeracy also requires understanding terms with definitions that are specific to the medical context. For instance, although 'survival' and 'mortality' are complementary in common usage, these terms are not complementary in medicine (see five-year survival rate). Innumeracy is also a very common problem when dealing with risk perception in health-related behavior; it is associated with patients, physicians, journalists and policymakers. Those who lack or have limited health numeracy skills run the risk of making poor health-related decisions because of an inaccurate perception of information. For example, if a patient has been diagnosed with breast cancer, being innumerate may hinder her ability to comprehend her physician's recommendations, or even the severity of the health concern. One study found that people tended to overestimate their chances of survival or even to choose lower-quality hospitals. Innumeracy also makes it difficult or impossible for some patients to read medical graphs correctly. Some authors have distinguished graph literacy from numeracy. Indeed, many doctors exhibit innumeracy when attempting to explain a graph or statistics to a patient. A misunderstanding between a doctor and patient, due to either the doctor, patient, or both being unable to comprehend numbers effectively, could result in serious harm to health.

Different presentation formats of numerical information, for instance natural frequency icon arrays, have been evaluated to assist both low-numeracy and high-numeracy individuals.

In the field of economic history, numeracy is often used to assess human capital at times when there was no data on schooling or other educational measures. Using a method called age-heaping, researchers like Professor Jörg Baten study the development and inequalities of numeracy over time and throughout regions. For example, Baten and Hippe find a numeracy gap between regions in western and central Europe and the rest of Europe for the period 1790–1880. At the same time, their data analysis reveals that these differences as well as within country inequality decreased over time. Taking a similar approach, Baten and Fourie find overall high levels of numeracy for people in the Cape Colony (late 17th to early 19th century).

In contrast to these studies comparing numeracy over countries or regions, it is also possible to analyze numeracy within countries. For example, Baten, Crayen and Voth look at the effects of war on numeracy in England, and Baten and Priwitzer find a "military bias" in what is today western Hungary: people opting for a military career had - on average - better numeracy indicators (1 BCE to 3CE).




</doc>
<doc id="21312310" url="https://en.wikipedia.org/wiki?curid=21312310" title="Remember versus know judgements">
Remember versus know judgements

There is evidence suggesting that different processes are involved in remembering something versus knowing whether it is familiar. It appears that "remembering" and "knowing" represent relatively different characteristics of memory as well as reflect different ways of using memory.

To remember is the conscious "recollection" of many vivid contextual details, such as "when" and "how" the information was learned. Remembering utilizes episodic memory and requires a deeper level of processing (e.g. undivided attention) than knowing. Errors in recollection may be due to source-monitoring errors that prevent an individual from remembering where exactly a piece of information was received. On the other hand, source monitoring may be very effective in aiding the retrieval of episodic memories. Remembering is a knowledge-based and conceptually-driven form of processing that can be influenced by many things. It is relevant to note that under this view both kinds of judgments are characteristics of individuals and thus any distinctions between the two are correlational, not causal, events.

To know is a feeling (unconscious) of "familiarity". It is the sensation that the item has been seen before, but not being able to pin down the reason why. Knowing simply reflects the familiarity of an item without recollection. Knowing utilizes semantic memory that requires perceptually based, data-driven processing. Knowing is the result of shallow maintenance rehearsal that can be influenced by many of the same aspects as semantic memory.

Remember and know responses are quite often differentiated by their functional correlates in specific areas in the brain. For instance, during "remember" situations it is found that there is greater EEG activity than "knowing", specifically, due to an interaction between frontal and posterior regions of the brain. It is also found that the hippocampus is differently activated during recall of "remembered" (vs. familiar) stimuli. On the other hand, items that are only "known", or seem familiar, are associated with activity in the rhinal cortex.

The remember-know paradigm began its journey in 1985 from the mind of Endel Tulving. He suggested that there are only two ways in which an individual can access their past. For instance, we can recall what we did last night by simply traveling back in time through memory and episodically imagining what we did (remember) or we can know something about our past such as a phone number, but have no specific memory of where the specific memory came from (know). Recollection is based on the episodic memory system, and familiarity is based on the semantic memory system. Tulving argued that the remember-know paradigm could be applied to all aspects of recollection.

In 1988 the application of the paradigm was refined to a set of instructions that could elicit reliable judgments from subjects that can be found using many variables. The remember-know paradigm has changed the way in which researchers can study memory tasks and has had implications on what were originally considered purely "episodic" memories, which can now be thought of as a combination of both remembering and knowing or episodic and semantic.

Remembering and knowing have been linked to dual-component theories, as well as unitary trace-strength/signal detection models.

Episodic and semantic memory give rise to two different states of consciousness, autonoetic and noetic, which influence two kinds of subjective experience, remembering and knowing, respectively. Autonoetic consciousness refers to the ability of recovering the episode in which an item originally occurred. In noetic consciousness, an item is familiar but the episode in which it was first encountered is absent and cannot be recollected. Remembering involves retrieval from episodic memory and knowing involves retrieval from semantic memory.

In his SPI model, Tulving stated that encoding into episodic and semantic memory is serial, storage is parallel, and retrieval is independent. By this model, events are first encoded in semantic memory before being encoded in episodic memory; thus, both systems may have an influence on the recognition of the event.

The original high-threshold model held that recognition is a probabilistic process. It is assumed that there is some probability that previously studied items will exceed a memory threshold. If an item exceeds the threshold then it is in a discrete memory state. If an item does not exceed the threshold then it is not remembered, but it may still be endorsed as old on the basis of a random guess. According to this model, a test item is either recognized (i.e., it falls above a threshold) or it is not (i.e., it falls below a threshold), with no degrees of recognition occurring between these extremes. Only target items can generate an above-threshold recognition response because only they appeared on the list. The lures, along with any targets that are forgotten, fall below threshold, which means that they generate no memory signal whatsoever. For these items, the participant has the option of declaring them to be new (as a conservative participant might do) or guessing that some of them are old (as a more liberal participant might do). False alarms in this model reflect memory-free guesses that are made to some of the lures. 
This simple and intuitively appealing model yields the once widely used correction for guessing formula, and it predicts a linear receiver operating characteristic (ROC). An ROC is simply a plot of the hit rate versus the false alarm rate for different levels of bias. A typical ROC is obtained by asking participants to supply confidence ratings for their recognition memory decisions. Several pairs of hit and false alarm rates can then be computed by accumulating ratings from different points on the confidence scale (beginning with the most confident responses). The high-threshold model of recognition memory predicts that a plot of the hit rate versus the false alarm rate (i.e., the ROC) will be linear it also predicts that the z-ROC will be curvilinear.

The dual-process account states that recognition decisions are based on the processes of recollection and familiarity. Recollection is a conscious, effortful process in which specific details of the context in which an item was encountered are retrieved. Familiarity is a relatively fast, automatic process in which one gets the feeling the item has been encountered before, but the context in which it was encountered is not retrieved. According to this view, remember responses reflect recollections of past experiences and know responses are associated with recognition on the basis of familiarity.

According to this theory, recognition decisions are based on the strength of a memory trace in reference to a certain decision threshold. A memory that exceeds this threshold is perceived as old, and trace that does not exceed the threshold is perceived as new. According to this theory, remember and know responses are products of different degrees of memory strength. There are two criteria on a decision axis; a point low on the axis is associated with a know decision, and a point high on the axis is associated with a remember decision. If memory strength is high, individuals make a "remember" response, and if memory strength is low, individuals make a "know" response.

Probably the strongest support for the use of signal detection theory in recognition memory came from the analysis of ROCs. An ROC is the function that relates the proportion of correct recognitions (hit rate) to the proportion of incorrect recognitions (false-alarm rate).

Signal-detection theory assumed a preeminent position in the field of recognition memory in large part because its predictions about the shape of the ROC were almost always shown to be more accurate than the predictions of the intuitively plausible high-threshold model. More specifically, the signal-detection model, which assumes that memory strength is a graded phenomenon (not a discrete, probabilistic phenomenon) predicts that the ROC will be curvilinear, and because every recognition memory ROC analyzed between 1958 and 1997 was curvilinear, the high-threshold model was abandoned in favor of signal-detection theory. Although signal-detection theory predicts a curvilinear ROC when the hit rate is plotted against the false alarm rate, it predicts a linear ROC when the hit and false alarm rates are converted to z scores (yielding a z-ROC).

“The predictive power of the signal detection modem seems to rely on know responses being related to transient feelings of familiarity without conscious recollection, rather than Tulving’s (1985) original definition of know awareness.

The dual-process signal-detection/high-threshold theory tries to reconcile dual-process theory and signal-detection theory into one main theory. This theory states that recollection is governed by a threshold process, while familiarity is not. Recollection is a high-threshold process (i.e., recollection either occurs or does not occur), whereas familiarity is a continuous variable that is governed by an equal-variance detection model. On a recognition test, item recognition is based on recollection if the target item has exceeded threshold, producing an "old" response. If the target item does not reach threshold, the individual must make an item recognition decision based on familiarity. According to this theory, an individual makes a "remember" response when recollection has occurred. A know response is made when recollection has not occurred, and the individual must decide whether they recognize the target item solely on familiarity.
Thus, in this model, the participant is thought to resort to familiarity as a backup process whenever recollection fails to occur.

In the past, it was suggested that remembering is associated with conceptual processing and knowing is associated with perceptual processing. However, recent studies have reported that there are some conceptual factors that influence knowing and some perceptual factors that influence remembering. Findings suggest that regardless of perceptual or conceptual factors, distinctiveness of processing at encoding is what affects remembering, and fluency of processing is what affects knowing. Remembering is associated with distinctiveness because it is seen as an effortful, consciously controlled process. Knowing, on the other hand, depends on fluency as it is more automatic and reflexive and requires much less effort.

Items of low-frequency are generally better recognized and receive more remember responses than high-frequency items. In a study, 96 words consisting of 48 low-frequency words and 48 high-frequency words were chosen by a psycholinguistic database. There were two alternate study lists, each consisting of 24 low-frequency words and 24 high-frequency words. Half of the subjects received one study list, while the other half of the participants received the other. The recognition test, which involved all 96 words, required participants to first acknowledge whether the target item was old or new; if the item was considered old, participants were further asked to distinguish whether the item was remembered (they could recollect the context in which it had been studied) or known (the item seemed familiar but they couldn't recollect contextual details). The results of this experiment were that low-frequency words received many more remember responses than high-frequency words. Since remember words are affected by distinctiveness, this makes sense; low-frequency words are experienced less than high-frequency words which makes them more distinctive. Also, there seemed to be no significant difference in the number of know responses made for low-frequency words and high-frequency words.

Items which are generated by a person receive more remember responses than items which are read, seen, or heard by a person. In addition, the generation of images to words enhances remember responses. In one study, all participants were asked to study a list of 24 common pairs of opposites, 12 had to be generated and 12 were read. The generated pairs required participants to generate them in the context of a given rule. The recognition test consisted of 48 words, 24 targets and 24 distractors. Participants were asked if items were old or new; if participants replied "old", they were then asked whether they remembered (could recollect contextual details of when it was studied) the pair or if they knew (recognized it but recollection was absent) the pair. Generation effects were seen in remember responses; items which were generated received more remember responses than read items. However, generation effects were not seen in know responses.

Remember responses depend on the amount of attention available during learning. Divided attention at learning has a negative impact on remember responses. A study was done which consisted of 72 target words which were divided into two study lists. Half of the participants were required to study the list in an undivided attention condition and half of the subjects studied the list in a divided attention condition. In the divided attention condition, subjects had to study the list while listening to and reporting high, low, or medium tone sequences. The recognition test consisted of participants deciding whether items were old or new; if items were deemed old, participants were then required to say whether items were remembered or known. It was found that the divided attention condition impaired the level of correct remember responses; however, the know responses seemed unaffected.

When more detailed, elaborate encoding and associations are made, more remember responses are reported than know responses. The opposite occurs with shallow, surface encoding which results in fewer remember responses.

The primacy effect is related to enhanced remembering. In a study, a free recall test was conducted on some lists of words and no test on other lists of words prior to a recognition test. They found that testing led to positive recency effects for remembered items; on the other hand, with no prior test, negative recency effects occurred for remembered items. Thus, both primary and recency effects can be seen in remember responses.

Masked recognition priming is a manipulation which is known to increase perceptual fluency. Since know responses increase with increased fluency of processing, masked recognition priming enhances know responses.

Briefly presenting a prime prior to test causes an increase in fluency of processing and an associated increase in feeling of familiarity. Short duration primes tend to enhance know responses. In contrast to briefly presented primes, primes which are presented for long durations are said to disrupt processing fluency as the prime saturates the representation of the target word. Thus, longer duration primes tend to have a negative impact on know responses.

Know responses are enhanced when stimuli modality match during study and test; therefore, shifting the modality of a stimulus has been found to negatively impact know responses.

Knowing is influenced by the fluency in which an item is processed, regardless of whether information is conceptual or perceptual. Know responses are enhanced by manipulations which increase perceptual and conceptual fluency. For example, masked repetition priming, modality match during study and test, and the use of easy word-fragments in word-fragment recall are all perceptual manipulations which increase know responses. An example of a conceptual manipulation which enhances know responses is when a prime item is semantically related to a target item. Manipulations which increase processing fluency do not seem to affect remember responses.

Normal aging tends to disrupt remember responses to a greater extent than know responses. This decrease in remember responses is associated with poor encoding and frontal lobe dysfunction. It has been found that older individuals fail to use elaborative encoding in comparison to younger individuals. In addition to poor encoding, older individuals tend to have problems with retrieving information that is highly specific because they are less effective at controlling their retrieval processes. It is difficult for older individuals to constrain retrieval processes to the context of the specific item that is to be retrieved.

When words are used as stimuli, more remember responses and fewer know responses are produced in comparison to when nonwords are used as stimuli.

Gradual presentation of stimuli causes an increase in familiarity and thus an increase in associated know responses; however, gradual presentation causes a decrease in remember responses.

The amygdala plays an important role during encoding and retrieval of emotional information. It has been found that although negative and positive items are remembered or known to the same extent, the processes involved in remembering and knowing differs with emotional valence.

Activity in the orbitofrontal and ventrolateral prefrontal cortex are associated with remembering for both positive and negative items. When it comes to remembering, it has been suggested that negative items may be remembered with more detail in comparison to positive or neutral items; support for this has been found in the temporo-occipital regions, which showed activity specific to negative items that were "remembered". A study found that in addition to being remembered in more detail, negative items also tended to be remembered for longer durations than positive items.

Activity in the mid-cingulate gyrus, the inferior parietal lobe, and the superior frontal lobe are all associated with knowing for both positive and negative items. These regions are said to be involved in the retrieval of both semantic and episodic information. It has been suggested that the encoding of items which people forget details for or items which are forgotten as a whole are associated with these regions. This forgetting has to do with retrieval-related processes being active at the same time as encoding-related processes. Thus, the process of retrieval may come at the expense of encoding vivid details of the item.

In addition, disproportionate activity along the cingulate gyrus, within the parietal lobe, and in the prefrontal cortex is associated with the encoding of "known" positive items. This increased activity may cause the trade-off between retrieval-related processes and encoding-related processes to occur more significantly for positive items. This supports the idea that when people are in a positive mood, they have a more holistic, general thought process and disregard details.

The functional account of remembering states that remember responses are determined by the context in which they're made; in general, recollection is based on the type of info that was encouraged by the deeper level processing task. Remember responses occur when retrieved information allow subjects to finish a memory test. The same item may elicit a remember response or a know response, depending on the context in which it is found.

In the expectancy heuristic, items that reach beyond a level of distinctiveness (the likelihood an item would later be recognized in a recognition test) elicit a remember response. Items that do not reach this level of distinctiveness elicit a know response. The level of distinctiveness is determined by the context in which items are studied and/or tested. In a given context, there is an expected level of distinctiveness; in contexts where subjects expect many distinct items, participants give fewer remember responses than when they expect few distinct items. Therefore, remember responses are affected by the expected strength of distinctiveness of items in a given context.

In addition, context can affect remember and know responses while not affecting recollection and familiarity processes. Remember and know responses are subjective decisions that can be affected by underlying memory processes. While changing recollection and familiarity processes can influence remember and know judgments, context can affect how items are weighted for remember-know decisions.

According to the distinctiveness-fluency model, items are seen as distinct when they exceed a level of memorability and items are seen as fluent when they do not reach this level but give a feeling of familiarity. Distinct items are usually unusual in comparison to the context in which they're found. Therefore, distinct items generally elicit a remember response, and fluent items elicit a know response.

In this study, the presence of source memory was utilized to estimate the extent to which episodic details were recalled; feelings of familiarity were accompanied by retrieval of partial contextual details, which are considered sufficient for an accurate source decision but not for a recollection response. Subjects that remembered the stimuli were able to differentiate the corresponding source correctly. The findings were consistent with the idea that "remember" responses, unlike "know" responses, are accompanied by memory for episodic detail, and that the loss of memory for episodic detail over time parallels the conversion of "remember" responses to "know" responses.

In the preceding task, participants are given a list of items to study in the primary learning phase. Subsequently, during the recognition stage, participants are asked to make a decision about whether presented test items existed in the previously studied list. If the participant responds "yes", they are asked why they recognized the specified item. From this, a conclusion was obtained based on whether the item was remembered or simply known.

Primarily, experimenters recorded eye movements while participants studied a series of photos. Individuals were then involved in a recognition task in which their eye movements were recorded for the second time. From the previous tasks, it was discovered that eye fixations, maintaining a visual gaze on a single location, were more clustered for remembering rather than knowing tasks. This suggests that remembering is associated with encoding a specific salient component of an item whereas recognition is activated by an augmented memory for this part of the stimulus.

In the above experiment, participants were presented with a list of 100 familiar words and were asked to read them aloud while simultaneously trying to remember each one. Subsequent to this, participants were asked to make a recognition decision based on the number of "yes" responses that were accompanied by some recollective experience. The results demonstrate the differing relationships between the "yes" and "no" conditions and "remember" and "know" memory performance. The outcome confirms that although familiarity and recollection may involve different processes, the remember/know exemplar does not probe them directly.

In the previous study, two different remember-know paradigms are explored. The first is the "remember-first method" in which a remember response is solicited prior to a know response for non-remembered items. Secondly, a trinary paradigm, in which a single response judges the "remember vs. know" and "new" alternatives is investigated. Participants are asked to subjectively decide whether their response within these studies is attributed to a recollection of specific details, "remembering", or familiarity "knowing". In the presently discussed experiment, "remember" and "know" responses generally depend on a single strength variable.

Remembering (recollection) accesses memory for separate contextual details (e.g. screen location and font size); i.e. involves retrieval of a particular context configuration.

This model uses the idea that remembering and knowing each represent different levels of confidence. In this sense remember/know judgments are viewed as quantitatively different judgments that vary along the same continuum. Subjects place their "know" and "remember" judgments on a continuum of strength. When people are very confident about recognizing an item, they assign it a "remember" response, and when they are less confident about a response it is labelled as a "know" response. A potential problem with this model is that it lacks explanatory power; it may be difficult to determine where the criteria should be placed on the continuum.

The remember-know paradigm has had great usage in clinical studies. Using this paradigm, researchers can look into the mechanisms of neuro-biological functions as well as social aspects of disorders and illnesses that plague humans. Recognition memory has been linked to advancements in the understanding of schizophrenia, epilepsy and even explaining simple autobiographical memory loss as we grow older.

The remember-know paradigm has been used to settle the debate over whether the hippocampus plays a critical role in familiarity of objects. Studies done with patients suffering from epilepsy suggest that the hippocampus plays a critical role in the familiarity of objects. A study was conducted using the remember-know distinction to understand this idea of familiarity and whether it is in fact the hippocampus that plays this critical role. This study found that the hippocampus is essentially a system based on familiarity. The hippocampus actually suppresses any sort of arousal response that would normally occur if the stimuli were novel. It is almost as though familiarity is a qualitative characteristic just as is colour or loudness.

The remember-know paradigm with epilepsy patients to distinguish whether a stimulus (picture of a face) was familiar. Patients that were found to have right temporal lobe epilepsy showed relatively lower face recognition response than those with left temporal lobe epilepsy due to damage of secondary sensory regions (including fusiform gyrus) in the brain's right hemisphere, which is responsible for perception and encoding (esp. face memory).

A remember-know paradigm was used to test whether patients with schizophrenia would exhibit abnormalities in conscious recollection due to a deterioration of frontal memory processes that are involved in encoding/retrieval of memories as well as executive functions linked to reality monitoring and decision making. Using the "remember-know" paradigm, participants first identify stimuli that they previously studied. If an item is identified as a known stimulus, the participants are asked to distinguish whether they can remember aspects of the original presentation of the identified item (remember response) or whether they know that the item was on the study list, but have no episodic memory of specifically learning it.

Results showed that patients with schizophrenia exhibit a deficit in conscious recollection for auditory stimuli. These findings, when considered together with remember/know data collected from the same set of patients for olfactory and visual recognition memory, support proposals that the abnormalities in conscious recollection stem from a breakdown in central processes rather than domain-specific processes. This study depended greatly on the remember-know paradigm to test for conscious recollection differences in these patients.

The remember-know paradigm has been used in studies that focus on the idea of a reminiscence bump and the age effects on autobiographical memory. Previous studies suggested old people had more "know" than "remember" and it was also found that younger individuals often excelled in the "remember" category but lacked in the "know".

A specific study used the remember-know paradigm while asking elderly people about their episodic memories from their university graduation. They were asked to determine whether their self reported memories were "remembered" or "known". It was hypothesized that reminiscence component of elderly adults' autobiographical recall would be strong for "remember" responses, but less so for "know" responses. It was also expected that recent memories would hold the opposite effect, that those individuals would be better at "know" responses than with "remember" responses.

Results showed that there was good retention after reminiscence bump and equal "remember" to "know" responses were reported. It was concluded that autobiographical memories were tied to both episodic and semantic memories. These results are important to demonstrate that aging is not accompanied by a decline in episodic memory due to a reliance on semantic memory as previously thought. The remember-know distinction was integral in achieving these results as well as understanding the ways in which autobiographical memory works and the prevalence of the reminiscence bump. The findings of Rybash are supported by other research.

The tip-of-the-tongue state is the phenomenon that occurs when people fail to recall information but still feel as if they are close to retrieving it from memory. In this sense an individual feels as if they "know" but cannot "remember" the actual information desired. It is a frustrating but common problem that typically occurs for individuals about once a week, is frequent among nouns and is typically resolved on its own. The occurrence of the tip-of-the-tongue state increases with age throughout adulthood. Such a feeling is indication that remembering will occur or is about to occur.

The knew-it-all-along effect is a variant of the hindsight bias. It is the tendency for people to misremember and think that they knew more in the past than they actually did. In such situations it is difficult for us to remember what it was like when we did not have an understanding of something. For example, one might have a hard time teaching a concept to another individual because they can not remember what it is like to not understand the material.

Hindsight bias is the phenomenon where people tend to view events as more predictable than they really are. This occurs because one's current knowledge influences the recollection of previous beliefs. In this phenomenon, what someone "knows" is affecting what they "remember". This inaccurate assessment of reality after it has occurred is also referred to as "creeping determinism". The hindsight bias has been found among a number of domains such as historical events, political elections and the outcome of sporting events. The hindsight bias is a common phenomenon that occurs regularly among individuals in everyday life and can be generated in a laboratory setting to help increase the understanding of memory and specifically memory distortions.


</doc>
<doc id="16757040" url="https://en.wikipedia.org/wiki?curid=16757040" title="Democratization of knowledge">
Democratization of knowledge

The democratization of knowledge is the acquisition and spread of knowledge amongst the common people, not just privileged elites such as clergy and academics. Libraries, in particular public libraries, and modern digital technology such as the Internet play a key role, as they provide the masses with open access to information.

Over the centuries, dissemination of information has risen to an unprecedented level. The start of this process can be marked from the printing press, the purpose of which was to spread information uniformly among masses. Today, in digitized world, the availability of online content outnumbers the information published in books, journals or in any print form.

The printing press was one of the early steps towards the democratization of knowledge. Another small example of this during the Industrial Revolution was the creation of libraries for miners in some Scottish villages in the 18th century.

Wikipedia is rapidly turning into a real-time reference tool in which public entries can be updated by anyone at any time. This phenomenon—a product of the digital age—has greatly contributed to the democratization of knowledge in the post-modern era. At the same time, it has raised a number of valid criticisms in this regard (see Reliability of Wikipedia). For instance, one could draw a distinction between the mere spread of information and the spread of "accurate" or "credible" information. Wikipedia may thus be a more reliable source of information in certain spheres, but not necessarily in others.

The democratization of technology has played a major facilitating role. Wikipedia co-founder, Larry Sanger, states in his article, that "Professionals are no longer needed for the bare purpose of the mass distribution of information and the shaping of opinion." Sanger's article confronts the existence of "common knowledge" and pits it against knowledge that everyone agrees on.

In terms of democratization of knowledge, Wikipedia has played a major role. For instance, Wikipedia has attracted 400 million viewers across the globe and has communicated with them in over 300 languages.

Google Book Search has been pointed to as an example of democratization of knowledge, but Malte Herwig in Der Spiegel raised concerns that the virtual monopoly Google has in the search market, combined with Google's hiding of the details of its search algorithms, could undermine this move towards democratization.

After the most powerful search engine, Google, and the most viewed encyclopedia, Wikipedia, the most viewed information based website is Encyclopædia Britannica.

An article written in 2005 by the editors of "Reference & User Services Quarterly" calls the library the greatest force for the democratization of knowledge or information. It continues to say that public libraries in particular are inextricably linked with the history and evolution of the United States, but school library media centers, college and university libraries, and special libraries have all also been influential in their support for democracy. Libraries play an essential role in the democratization of knowledge and information by providing communities with the resources and tools to find information free of charge. Democratic access to knowledge has also been co-opted to mean providing information in a variety of formats, which essentially means electronic and digital formats for use by library patrons. Public libraries help further the democratization of information by guaranteeing freedom of access to information, by providing an unbiased variety of information sources and access to government services, as well as the promotion of democracy and an active citizenship.

Dan Cohen, the founding executive director of the Digital Public Library of America, writes that the democratic access to knowledge is a profound idea that requires constant tending and revitalization. In 2004, a World Social Forum and International workshop was held entitled "Democratization of Information: Focus on Libraries". The focus of the forum was to bring awareness to the social, technological, and financial challenges facing libraries dealing with the democratization of information. Social challenges included globalization and the digital divide, technological challenges included information sources, and financial challenges constituted shrinking budgets and manpower. Longtime Free Library of Philadelphia director Elliot Shelkrot said that "Democracy depends on an informed population. And where can people get all the information they need? —At the Library." 



</doc>
<doc id="750101" url="https://en.wikipedia.org/wiki?curid=750101" title="Domain knowledge">
Domain knowledge

Domain knowledge is knowledge of a specific, specialized discipline or field, in contrast to general knowledge, or domain-independent knowledge. The term is often used in reference to a more general discipline, as, for example, in describing a software engineer who has general knowledge of programming, as well as domain knowledge about the pharmaceutical industry. People who have domain knowledge, are often considered specialists or experts in the field.

In software engineering "domain knowledge" is knowledge about the environment in which the target system operates, for example, software agents. Domain knowledge usually must be learned from software users in the domain (as domain specialists/experts), rather than from software developers. It may include user workflows, data pipelines, business policies, configurations and constraints and is crucial in the development of a software application. Expert's domain knowledge (frequently informal and ill-structured) is transformed in computer programs and active data, for example in a set of rules in knowledge bases, by knowledge engineers.

Communicating between end-users and software developers is often difficult. They must find a common language to communicate in. Developing enough shared vocabulary to communicate can often take a while.

The same knowledge can be included in different domain knowledge.
Knowledge which may be applicable across a number of domains is called "domain-independent" knowledge, for example logics and mathematics.
Operations on domain knowledge are performed by meta-knowledge.




</doc>
<doc id="8957879" url="https://en.wikipedia.org/wiki?curid=8957879" title="Body of knowledge">
Body of knowledge

A body of knowledge (BOK or BoK) is the complete set of concepts, terms and activities that make up a professional domain, as defined by the relevant learned society or professional association. It is a type of knowledge representation by any knowledge organization. Several definitions of BOK have been developed, for example:
A body of knowledge is the accepted ontology for a specific domain. A BOK is more than simply a collection of terms; a professional reading list; a library; a website or a collection of websites; a description of professional functions; or even a collection of information.

The following are examples of bodies of knowledge from professional organisations:




</doc>
<doc id="3067655" url="https://en.wikipedia.org/wiki?curid=3067655" title="Scientia potentia est">
Scientia potentia est

The phrase " (or " or also "") is a Latin aphorism meaning "knowledge is power". It is commonly attributed to Sir Francis Bacon, although there is no known occurrence of this precise phrase in Bacon's English or Latin writings. However, the expression "" ('knowledge itself is power') occurs in Bacon's "Meditationes Sacrae" (1597). The exact phrase "" was written for the first time in the 1668 version of the work "Leviathan" by Thomas Hobbes, who was secretary to Bacon as a young man.

The related phrase "" is often translated as "wisdom is power".

The earliest documented occurrence of the phrase "Knowledge is power" is from Imam Ali (599-661 CE), as recorded in the tenth-century book Nahj Al-Balagha:
Knowledge is power and it can command obedience ("maʿrifatu al-ʿilmi dīnun yadānu bihi"). A man of knowledge during his lifetime can make people obey and follow him and he is praised and venerated after his death. Remember that knowledge is a ruler and wealth is its subject. (Saying 147.5)
Another account of this concept is found in the Shahnameh by the Persian poet Ferdowsi (940–1020 CE) who wrote: "Capable is he who is wise" (in ). This hemistich is translated to English as "knowledge is power" or "One who has wisdom is powerful".

A proverb in practically the same wording is found in Hebrew, in the Biblical Book of Proverbs (24:5): . This was translated in the Latin Vulgata as "" and in the King James Version, the first English official edition, as "A wise man is strong, a man of knowledge increaseth strength".

The first known reference of the exact phrase appeared in the Latin edition of "Leviathan" (1668; the English version had been published in 1651). This passage from Part 1 ("De Homine"), Chapter X ("De Potentia, Dignitate et Honore") occurs in a list of various attributes of man which constitute power; in this list, "sciences" or "the sciences" are given a minor position:
In the English version this passage reads as thus:
On a later work, "De Corpore" (1655), also written in Latin, Hobbes expanded the same idea:
In Jean Hampton, "Hobbes and the social contract tradition" (1988), Hampton indicates that this quote is 'after Bacon' and in a footnote, that 'Hobbes was Bacon's secretary as a young man and had philosophical discussions with him (Aubrey 1898, 331).

The closest expression in Bacon's works is, perhaps, the expression "", found in his "" (1597), which is perhaps better translated as "knowledge is His power", because the context of the sentence refers to the qualities of God and is imbedded in a discussion of heresies that deny the power of God:

The English translation of this section includes the following:

Interpretation of the notion of power meant by Bacon must therefore take into account his distinction between the power of knowing and the power of working and acting, the opposite of what is assumed when the maxim is taken out of context. Indeed, the quotation has become a cliche.

In another place, Bacon wrote, "Human knowledge and human power meet in one; for where the cause is not known the effect cannot be produced. Nature to be commanded must be obeyed; and that which in contemplation is as the cause is in operation as the rule."

Ralph Waldo Emerson wrote in his essay "Old Age", included in the collection "Society and Solitude" (1870):

After the 1871, unification of Germany, "" (Knowledge is power, geographical knowledge is world power) was often used in German geography and the public discussion to support efforts for a German colonial empire after 1880. Julius Perthes e.g., used the motto for his publishing house. However, the installation of geographical research followed popular requests and was not imposed by the government. Especially Count Bismarck was not much interested in German colonial adventures; his envoy Gustav Nachtigal started with the first protective areas, but was more interested in ethnological aspects.

After World War I, German geography tried to contribute to efforts to regain a world power. Scholars like Karl Haushofer, a former general, and his son Albrecht Haushofer (both in close contact with Rudolf Hess) got worldwide attention with their concept of geopolitics. Associations of German geographers and school teachers welcomed the Machtergreifung and hoped to get further influence in the new regime.

The postwar geography was much more cautious; concepts of political geography and projection of power had not been widespread scholarly topics until 1989 in Germany.

Geographical knowledge is however still of importance in Germany. Germans tend to mock US politicians' and celebrities' comparable lack of interest in the topic. A Sponti (Außerparlamentarische Opposition) version of the slogan is "Wissen ist Macht, nichts wissen, macht auch nichts", a pun about the previous motto along the line "Knowledge is power, but being ignorant doesn't bother anyway". Joschka Fischer and Daniel Cohn-Bendit belong to those Spontis that nevertheless held powerful positions, in Fischer's case with no more formal education than a taxi driver's licence.

The German Bundeswehr Bataillon Elektronische Kampfführung 932, an electronic warfare unit based in Frankenberg (Eder), still uses the Latin version as its motto.


Terry Brooks. "First King of Shannara" Ballantine. C. 1996




</doc>
<doc id="434103" url="https://en.wikipedia.org/wiki?curid=434103" title="Experiential knowledge">
Experiential knowledge

Experiential knowledge is knowledge gained through experience, as opposed to a priori (before experience) knowledge: it can also be contrasted both with propositional (textbook) knowledge, and with practical knowledge.

Experiential knowledge is cognate to Michael Polanyi's personal knowledge, as well as to Bertrand Russell's contrast of Knowledge by Acquaintance and by Description.

In the philosophy of mind, the phrase often refers to knowledge that can "only" be acquired through experience, such as, for example, the knowledge of what it is like to see colours, which could not be explained to someone born blind: the necessity of experiential knowledge becomes clear if one was asked to explain to a blind person a colour like blue.

The question of a posteriori knowledge might be formulated as: can Adam or Eve know what water feels like on their skin prior to touching it for the first time?

Zen emphasises the importance of the experiential element in religious experience, as opposed to what it sees as the trap of conceptualization: as D. T. Suzuki put it, "fire. Mere talking of it will not make the mouth burn".

Experiential knowledge has also been used in the philosophy of religion as an argument against God's omniscience, questioning whether God could genuinely know everything, since he (supposedly) cannot know what it is like to sin. Commenting on the distinction between experiential knowledge and propositional knowledge, analytic philosopher and theologian William Lane Craig has stated in an interview with Robert Lawrence Kuhn for the PBS series "Closer to Truth" that because experiential knowledge is appropriate to the mind which does the knowing, in order for omniscience to be a cognitive perfection God's omniscience must entail God know only and all propositional truths and have only appropriate experiential knowledge.

Writer Barry Lopez writes about experiential knowledge and how it relates back to the environment, arguing that without experiencing nature, one cannot fully "know" and understand the relationships within ecosystems.

Carl Rogers stressed the importance of experiential knowledge both for the therapist formulating his or her theories, and for the client in therapy – both things with which most counsellors would agree.

As defined by Thomasina Borkman (Emeritus Professor of Sociology, George Mason University) experiential knowledge is the cornerstone of therapy in self-help groups, as opposed to both lay (general) and professional knowledge. Sharing in such groups is the narration of significant life experiences in a process through which the knowledge derived thereof is validated by the group and transformed into a corpus that becomes their fundamental resource and product.

Neville Symington has argued that one of the central features of the narcissist is a shying away from experiential knowledge, in favour of adopting wholesale a ready-made way of living drawn from other people's experience.

Helen Vendler has characterised Seamus Heaney's art as, in one respect, recording an experiential learning curve: "we are earthworms of the earth, and all that / has gone through us is what will be our trace".


</doc>
<doc id="216180" url="https://en.wikipedia.org/wiki?curid=216180" title="Understanding">
Understanding

Understanding is a psychological process related to an abstract or physical object, such as a person, situation, or message whereby one is able to think about it and use concepts to deal adequately with that object.
Understanding is a relation between the knower and an object of understanding. Understanding implies abilities and dispositions with respect to an object of knowledge that are sufficient to support intelligent behaviour.

Understanding is often, though not always, related to learning concepts, and sometimes also the theory or theories associated with those concepts. However, a person may have a good ability to predict the behaviour of an object, animal or system—and therefore may, in some sense, understand it—without necessarily being familiar with the concepts or theories associated with that object, animal or system in their culture. They may have developed their own distinct concepts and theories, which may be equivalent, better or worse than the recognised standard concepts and theories of their culture. Thus, understanding is correlated with the ability to make inferences.


Someone who has a more sophisticated understanding, more predictively accurate understanding, and/or an understanding that allows them to make explanations that others commonly judge to be better, of something, is said to understand that thing "deeply". Conversely, someone who has a more limited understanding of a thing is said to have a "shallow" understanding. However, the depth of understanding required to usefully participate in an occupation or activity may vary greatly.

For example, consider multiplication of integers. Starting from the most shallow level of understanding, we have (at least) the following possibilities:


For the purpose of operating a cash register at McDonald's, a person does not need a very deep understanding of the multiplication involved in calculating the total price of two Big Macs. However, for the purpose of contributing to number theory research, a person would need to have a relatively deep understanding of multiplication — along with other relevant arithmetical concepts such as division and prime numbers.

It is possible for a person, or a piece of "intelligent" software, that in reality only has a shallow understanding of a topic, to appear to have a deeper understanding than they actually do, when the right questions are asked of it. The most obvious way this can happen is by memorization of correct answers to known questions, but there are other, more subtle ways that a person or computer can (intentionally or otherwise) deceive somebody about their level of understanding, too. This is particularly a risk with artificial intelligence, in which the ability of a piece of artificial intelligence software to very quickly try out millions of possibilities (attempted solutions, theories, etc.) could create a misleading impression of the real depth of its understanding. Supposed AI software could in fact come up with impressive answers to questions that were difficult for unaided humans to answer, without really understanding the concepts "at all", simply by dumbly applying rules very quickly. (However, see the Chinese room argument for a controversial philosophical extension of this argument.)

Examinations are designed to assess students' understanding (and sometimes also other things such as knowledge and writing abilities) without falling prey to these risks. They do this partly by asking multiple different questions about a topic to reduce the risk of measurement error, and partly by forbidding access to reference works and the outside world to reduce the risk of someone else's understanding being passed off as one's own. Because of the faster and more accurate computation and memorization abilities of computers, such tests would arguably often have to be modified if they were to be used to accurately assess the understanding of an artificial intelligence.

Conversely, it is even easier for a person or artificial intelligence to fake a "shallower" level of understanding than they actually have; they simply need to respond with the same kind of answers that someone with a more limited understanding, or no understanding, would respond with — such as "I don't know", or obviously wrong answers. This is relevant for judges in Turing tests; it is unlikely to be effective to simply ask the respondents to mentally calculate the answer to a very difficult arithmetical question, because the computer is likely to simply dumb itself down and pretend not to know the answer.

Gregory Chaitin, a noted computer scientist, propounds a view that comprehension is a kind of data compression. In his essay "The Limits of Reason", he argues that "understanding" something means being able to figure out a simple set of rules that explains it. For example, we understand why day and night exist because we have a simple model—the rotation of the earth—that explains a tremendous amount of data—changes in brightness, temperature, and atmospheric composition of the earth. We have compressed a large amount of information by using a simple model that predicts it. Similarly, we understand the number 0.33333... by thinking of it as one-third. The first way of representing the number requires five concepts ("0", "decimal point", "3", "infinity", "infinity of 3"); but the second way can produce all the data of the first representation, but uses only three concepts ("1", "division", "3"). Chaitin argues that comprehension is this ability to compress data.

Cognition is the process by which sensory inputs are transformed. Affect refers to the experience of feelings or emotions. Cognition and affect constitute understanding.

In Catholicism and Anglicanism, understanding is one of the Seven gifts of the Holy Spirit.




</doc>
<doc id="859096" url="https://en.wikipedia.org/wiki?curid=859096" title="Forbidden knowledge">
Forbidden knowledge

Forbidden knowledge, which is different from secret knowledge, is used to describe forbidden books or other information to which access is restricted or deprecated for political or religious reasons. Forbidden knowledge is commonly not secret, rather a society or various institutions will use repressive mechanisms to either completely prevent the publication of information they find objectionable or dangerous (censorship), or failing that, to try to reduce the public's trust in such information (propaganda). Public repression can create paradoxical situation where the proscribed information is generally common knowledge but publicly citing it is disallowed.

A rich set of examples exist through history. 
In many cases this resulted in people defending themselves by creating political jokes. Jokes throughout history have been a powerful instrument to undermine state authority and the public truth associated with it.

Some form of public repression of facts or speculation not desirable to some people or even a majority of the population seems inevitable as societies need to create some common basis of facts to create a unified identity. Critical to political and personal freedom is the level to which this repression is organized through the state or powerful private organizations. Western secular societies have reached the consensus through the late 19th and early 20th centuries that private organizations should not be allowed to engage in compulsory censorship, forcing people to obey their dictates. For example, the separation of church and state in most Western societies mostly prevents religious organizations from repressing individuals based on their personal opinions and beliefs. As well, people are generally allowed to leave employment with a company which may regulate such personal expressions for whatever reason and find employment in less restrictive circumstances.



</doc>
<doc id="6727454" url="https://en.wikipedia.org/wiki?curid=6727454" title="Growth of knowledge">
Growth of knowledge

A term coined by Karl Popper in his work "The Logic of Scientific Discovery" to denote what he regarded as the main problem of methodology and the philosophy of science, i.e. to explain and promote the further growth of scientific knowledge. To this purpose, Popper advocated his theory of falsifiability, testability and testing. He wrote "The central problem of epistemology has always been and still is the problem of the growth of knowledge. And the growth of knowledge can be studied best by studying the growth of scientific knowledge."


</doc>
<doc id="3467849" url="https://en.wikipedia.org/wiki?curid=3467849" title="Half-life of knowledge">
Half-life of knowledge

The half-life of knowledge or half-life of facts is the amount of time that has to elapse before half of the knowledge or facts in a particular area is superseded or shown to be untrue. These coined terms belong to the field of quantitative analysis of science known as scientometrics.

These ideas of half-life applied to different fields differ from the concept of half-life in physics in that there is no guarantee that the knowledge or facts in areas of study are declining exponentially. It is unclear that there is any way to establish what constitutes "knowledge" in a particular area, as opposed to mere opinion or theory.

An engineering degree went from having a half life of 35 years in ca. 1930 to about 10 years in 1960.

A Delphi Poll showed that the half life of psychology as measured in 2016 ranged from 3.3 to 19 years depending on the specialty, with an average of a little over 7 years.

It has also been used in Christian missiology to increase the effectiveness of their teachings.

The concept of "half-life of knowledge" is attributed to Fritz Machlup (1962).

The phrase "half-life of facts" increased in popularity after the 2012 book "The Half-Life of Facts: Why Everything We Know Has an Expiration Date" was written by Samuel Arbesman.




</doc>
<doc id="14908508" url="https://en.wikipedia.org/wiki?curid=14908508" title="Foolishness">
Foolishness

Foolishness is the unawareness or lack of social norms which causes offence, annoyance, trouble and/or injury. The things such as impulsivity and/or influences may affect a person's ability to make otherwise reasonable decisions. In this sense, it differs from stupidity, which is the lack of intelligence. An act of foolishness is called folly.

Andreas Maercker in 1995 defined foolishness as rigid, dogmatic, and inflexible thinking which makes feelings of bitterness and probable annoyance. It is considered the foundation of illusions of grandiosity like omniscience, omnipotence and inviolability.

The Book of Proverbs characterizes traits of foolishness. Foolishness and wisdom are contrasted in Paul's letter to the Corinthians. He condemns intellectual arrogance and advocates a humble attitude instead of foolishness, in which it is then possible to learn. 

Plato transvalued reason over foolishness, to him integrity of acceptance of a state itself was the beginning of wisdom, he said "He is the wisest man who knows himself to be ill-equipped for the study of wisdom".



</doc>
<doc id="5042367" url="https://en.wikipedia.org/wiki?curid=5042367" title="Common knowledge">
Common knowledge

Common knowledge is knowledge that is known by everyone or nearly everyone, usually with reference to the community in which the term is used. Common knowledge need not concern one specific subject, e.g., science or history. Rather, common knowledge can be about a broad range of subjects, such as science, literature, history, and entertainment. Often, common knowledge does not need to be cited. Common knowledge is distinct from general knowledge.

In broader terms, common knowledge is used to refer to information that a reader would accept as valid, such as information that many users may know. As an example, this type of information may include the temperature in which water freezes or boils. To determine if information should be considered common knowledge, you can ask yourself who your audience is, are you able to assume they already have some familiarity with the topic, or will the information's credibility come into question.

Many techniques have been developed in response to the question of distinguishing truth from fact in matters that have become "common knowledge". The scientific method is usually applied in cases involving phenomena associated with astronomy, mathematics, physics, and the general laws of nature. In legal settings, rules of evidence generally exclude hearsay (which may draw on "facts" someone believes to be "common knowledge").

"Conventional wisdom" is a similar term also referring to ostensibly pervasive knowledge or analysis.

Examples of common knowledge:





</doc>
<doc id="6082368" url="https://en.wikipedia.org/wiki?curid=6082368" title="Specialization of knowledge">
Specialization of knowledge

A modern development and belief that the progress of knowledge is the result of distinct and independent spheres, and that knowledge in one discipline has little connection with knowledge in another discipline. Thus, "specialists" pursue their work in isolation from one another rather than as aspects of a unity or whole.



</doc>
<doc id="9527250" url="https://en.wikipedia.org/wiki?curid=9527250" title="Knowledge organization">
Knowledge organization

Knowledge organization (KO), organization of knowledge, organization of information, or information organization is an intellectual discipline concerned with activities such as document description, indexing, and classification that serve to provide systems of representation and order for knowledge and information objects. It addresses the "activities carried out and tools used by people who work in places that accumulate information resources (e.g., books, maps, documents, datasets, images) for the use of humankind, both immediately and for posterity. It discusses the processes that are in place to make resources findable, whether someone is searching for a single known item or is browsing through hundreds of resources just hoping to discover something useful. Information organization supports a myriad of information-seeking scenarios." Traditional human-based approaches performed by librarians, archivists, and subject specialists are increasingly challenged by computational (big data) algorithmic techniques. KO as a field of study is concerned with the nature and quality of such knowledge organizing processes (KOP) (such as taxonomy and ontology) as well as the resulting knowledge organizing systems (KOS).

Divergent historical and theoretical approaches towards organizing knowledge are based on different views of knowledge, cognition, language, and social organization. This richness lends itself to many complementary ways to consider knowledge organization. The academic International Society for Knowledge Organization (ISKO) engages with these issues via the research journal "Knowledge Organization".

One widely used analysis of organizational principles summarizes them as Location, Alphabet, Time, Category, Hierarchy (LATCH).

Among the major figures in the history of KO are Melvil Dewey (1851–1931) and Henry Bliss (1870–1955). 

Dewey's goal was an efficient way to manage library collections; not an optimal system to support users of libraries. His system was meant to be used in many libraries as a standardized way to manage collections. The first version of this system was created in 1876. 

An important characteristic in Henry Bliss' (and many contemporary thinkers of KO) was that the sciences tend to reflect the order of Nature and that library classification should reflect the order of knowledge as uncovered by science: 
Natural order --> Scientific Classification --> Library classification (KO)

The implication is that librarians, in order to classify books, should know about scientific developments. This should also be reflected in their education: “Again from the standpoint of the higher education of librarians, the teaching of systems of classification . . . would be perhaps better conducted by including courses in the systematic encyclopedia and methodology of all the sciences, that is to say, outlines which try to summarize the most recent results in the relation to one another in which they are now studied together. . . .” (Ernest Cushing Richardson, quoted from Bliss, 1935, p. 2).

Among the other principles, which may be attributed to the traditional approach to KO are: 


Today, after more than 100 years of research and development in LIS, the “traditional” approach still has a strong position in KO and in many ways its principles still dominate.

The date of the foundation of this approach may be chosen as the publication of S. R. Ranganathan’s Colon Classification in 1933. The approach has been further developed by, in particular, the British Classification Research Group. In many ways this approach has dominated what might be termed “modern classification theory.” 

The best way to explain this approach is probably to explain its analytico-synthetic methodology. The meaning of the term “analysis” is: Breaking down each subject into its basic concepts. The meaning of the term synthesis is: Combining the relevant units and concepts to describe the subject matter of the information package in hand.

Given subjects (as they appear in, for example, book titles) are first analyzed into a few common categories, which are termed “facets”. Ranganathan proposed his PMEST formula — Personality, Matter, Energy, Space and Time:


Important in the IR-tradition have been, among others, the Cranfield experiments, which were founded in the 1950s, and the TREC experiments (Text Retrieval Conferences) starting in 1992. It was the Cranfield experiments, which introduced the measures “recall” and “precision” as evaluation criteria for systems efficiency. The Cranfield experiments found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems (“UNITERM”). The Cranfield I test found, according to Ellis (1996, 3–6) the following results:

Although these results have been criticized and questioned, the IR-tradition became much more influential while library classification research lost influence. The dominant trend has been to regard only statistical averages. What has largely been neglected is to ask: Are there certain kinds of questions in relation to which other kinds of representation, for example, controlled vocabularies, may improve recall and precision?

The best way to define this approach is probably by method: Systems based upon user-oriented approaches must specify how the design of a system is made on the basis of empirical studies of users. 

User studies demonstrated very early that users prefer verbal search systems as opposed to systems based on classification notations. This is one example of a principle derived from empirical studies of users. Adherents of classification notations may, of course, still have an argument: That notations are well-defined and that users may miss important information by not considering them. 

Folksonomies is a recent kind of KO based on users' rather than on librarians' or subject specialists' indexing.

These approaches are primarily based on using bibliographical references to organize networks of papers, mainly by bibliographic coupling (introduced by Kessler 1963) or co-citation analysis ( independently suggested by Marshakova 1973 and Small 1973). In recent years it has become a popular activity to construe bibliometric maps as structures of research fields. 

Two considerations are important in considering bibliometric approaches to KO: 


Domain analysis is a sociological-epistemological standpoint. The indexing of a given document should reflect the needs of a given group of users or a given ideal purpose. In other words, any description or representation of a given document is more or less suited to the fulfillment of certain tasks. A description is never objective or neutral, and the goal is not to standardize descriptions or make one description once and for all for different target groups. 

The development of the Danish library “KVINFO” may serve as an example that explains the domain-analytic point of view. 

KVINFO was founded by the librarian and writer Nynne Koch and its history goes back to 1965. Nynne Koch was employed at the Royal Library in Copenhagen in a position without influence on book selection. She was interested in women’s’ studies and began personally to collect printed catalog cards of books in the Royal Library, which were considered relevant for women’s studies. She developed a classification system for this subject. Later she became the head of KVINFO and got a budget for buying books and journals, and still later, KVINFO became an independent library. The important theoretical point of view is that the Royal Library had an official systematic catalog of a high standard. Normally it is assumed that such a catalog is able to identify relevant books for users whatever their theoretical orientation. This example demonstrates, however, that for a specific user group (feminist scholars), an alternative way of organizing catalog cards was important. In other words: Different points of view need different systems of organization. 

DA is the only approach to KO which has seriously examined epistemological issues in the field, i.e. comparing the assumptions made in different approaches to KO and examining the questions regarding subjectivity and objectivity in KO. Subjectivity is not just about individual differences. Such differences are of minor interest because they cannot be used as guidelines for KO. What seems important are collective views shared by many users. A kind of subjectivity about many users is related to philosophical positions. In any field of knowledge different views are always at play. In arts, for example, different views of art are always present. Such views determine views on art works, writing on art works, how art works are organized in exhibitions and how writings on art are organized in libraries (see Ørom 2003). In general it can be stated that different philosophical positions on any issue have implications for relevance criteria, information needs and for criteria of organizing knowledge.




</doc>
<doc id="30384825" url="https://en.wikipedia.org/wiki?curid=30384825" title="Cognitive justice">
Cognitive justice

The concept of cognitive justice is based on the recognition of the plurality of knowledge and expresses the right of the different forms of knowledge to co-exist. 

Indian scholar Shiv Visvanathan coined the term cognitive justice in his 1997 book "A Carnival for Science: Essays on science, technology and development". Commenting on the destructive impact of hegemonic Western science on developing countries and non-Western cultures, Visvanathan calls for the recognition of alternative sciences or non-Western forms of knowledge. He argues that different knowledges are connected with different livelihoods and lifestyles and should therefore be treated equally.

Cognitive justice is a critique on the dominant paradigm of modern science and promotes the recognition of alternative paradigms or alternative sciences by facilitating and enabling dialogue between, often incommensurable, knowledges. These dialogues of knowledge are perceived as contributing to a more sustainable, equitable, and democratic world. 

The call for cognitive justice is found in a growing variety of fields, such as ethnobiology, technology and database design, and in information and communication technology for development (ICT4D).

South-African scholar and UNESCO education expert Catherine Odora Hoppers wrote about cognitive justice in the field of education. She argued that Indigenous knowledges have to be included in the dialogues of knowledge without having to fit in the structures and standards of Western knowledge. When Indigenous knowledges are treated equally, they can play their role in making a more democratic and dialogical science, which remains connected to the livelihoods and survival of all cultures.


</doc>
<doc id="246176" url="https://en.wikipedia.org/wiki?curid=246176" title="Gettier problem">
Gettier problem

The Gettier problem, in the field of epistemology, is a landmark philosophical problem concerning our understanding of descriptive knowledge. Attributed to American philosopher Edmund Gettier, Gettier-type counterexamples (called "Gettier-cases") challenge the long-held justified true belief (JTB) account of knowledge. The JTB account holds that knowledge is equivalent to justified true belief; if all three conditions (justification, truth, and belief) are met of a given claim, then we have knowledge of that claim. In his 1963 three-page paper titled "Is Justified True Belief Knowledge?", Gettier attempts to illustrate by means of two counterexamples that there are cases where individuals can have a justified, true belief regarding a claim but still fail to know it because the reasons for the belief, while justified, turn out to be false. Thus, Gettier claims to have shown that the JTB account is inadequate; that it does not account for all of the necessary and sufficient conditions for knowledge.

The term "Gettier problem", "Gettier case", or even the adjective "Gettiered", is sometimes used to describe any case in the field of epistemology that purports to repudiate the JTB account of knowledge.

Responses to Gettier's paper have been numerous; some reject Gettier's examples, while others seek to adjust the JTB account of knowledge and blunt the force of these counterexamples. Gettier problems have even found their way into sociological experiments, where the intuitive responses from people of varying demographics to Gettier cases have been studied.

The question of what constitutes "knowledge" is as old as philosophy itself. Early instances are found in Plato's dialogues, notably "Meno" (97a–98b) and "Theaetetus". Gettier himself was not actually the first to raise the problem named after him; its existence was acknowledged by both Alexius Meinong and Bertrand Russell, the latter of which discussed the problem in his book "Human knowledge: Its scope and limits". In fact, the problem has been known since the Middle Ages, and both Indian philosopher Dharmottara and scholastic logician Peter of Mantua presented examples of it.

Russell's case, called the stopped clock case, goes as follows: Alice sees a clock that reads two o'clock and believes that the time is two o'clock. It is, in fact, two o'clock. There's a problem, however: unknown to Alice, the clock she's looking at stopped twelve hours ago. Alice thus has an accidentally true, justified belief. Russell provides an answer of his own to the problem. Edmund Gettier's formulation of the problem was important as it coincided with the rise of the sort of philosophical naturalism promoted by W. V. O. Quine and others, and was used as a justification for a shift towards externalist theories of justification. John L. Pollock and Joseph Cruz have stated that the Gettier problem has "fundamentally altered the character of contemporary epistemology" and has become "a central problem of epistemology since it poses a clear barrier to analyzing knowledge".

Alvin Plantinga rejects the historical analysis:
Despite this, Plantinga "does" accept that some philosophers before Gettier have advanced a JTB account of knowledge, specifically C. I. Lewis and A. J. Ayer.

The JTB account of knowledge is the claim that knowledge can be conceptually analyzed as justified true belief, which is to say that the "meaning" of sentences such as "Smith knows that it rained today" can be given with the following set of conditions, which are necessary and sufficient for knowledge to obtain:

The JTB account was first credited to Plato, though Plato argued against this very account of knowledge in the "Theaetetus" (210a). This account of knowledge is what Gettier subjected to criticism.

Gettier's paper used counterexamples (see also thought experiment) to argue that there are cases of beliefs that are both true and justified—therefore satisfying all three conditions for knowledge on the JTB account—but that do not appear to be genuine cases of knowledge. Therefore, Gettier argued, his counterexamples show that the JTB account of knowledge is false, and thus that a different conceptual analysis is needed to correctly track what we mean by "knowledge".

Gettier's case is based on two counterexamples to the JTB analysis. Each relies on two claims. Firstly, that justification is preserved by entailment, and secondly that this applies coherently to Smith's putative "belief". That is, that if Smith is justified in believing P, and Smith realizes that the truth of P entails the truth of Q, then Smith would "also" be justified in believing Q. Gettier calls these counterexamples "Case I" and "Case II":

In both of Gettier's actual examples (see also counterfactual conditional), the justified true belief came about, if Smith's purported claims are disputable, as the result of entailment (but see also material conditional) from justified false beliefs that "Jones will get the job" (in case I), and that "Jones owns a Ford" (in case II). This led some early responses to Gettier to conclude that the definition of knowledge could be easily adjusted, so that knowledge was justified true belief that does not depend on false premises. The interesting issue that arises is then of how to know which premises are in reality false or true when deriving a conclusion, because as in the Gettier cases, one sees that premises can be very reasonable to believe and be likely true, but unknown to the believer there are confounding factors and extra information that may have been missed while concluding something. The question that arises is therefore to what extent would one have to be able to go about attempting to "prove" all premises in the argument before solidifying a conclusion.

In a 1966 scenario known as "The sheep in the field", Roderick Chisholm asks us to imagine that someone is standing outside a field looking at something that looks like a sheep (although in fact, it is a dog disguised as a sheep). They believe there is a sheep in the field, and in fact, they are right because there is a sheep behind the hill in the middle of the field. Hence, they have a justified true belief that there is a sheep in the field. But is that belief knowledge? A similar problem which seeks to be more plausible called the "Cow in the Field" appears in Martin Cohen's book "101 Philosophy Problems", where it is supposed that a farmer checking up on his favourite cow confuses a piece of black and white paper caught up in a distant bush for his cow. However, since the animal actually is in the field, but hidden in a hollow, again, the farmer has a justified, true belief which seems nonetheless not to qualify as "knowledge".

Another scenario by Brian Skyrms is "The Pyromaniac", in which a struck match lights not for the reasons the pyromaniac imagines but because of some unknown "Q radiation".

A different perspective on the issue is given by Alvin Goldman in the "fake barns" scenario (crediting Carl Ginet with the example). In this one, a man is driving in the countryside, and sees what looks exactly like a barn. Accordingly, he thinks that he is seeing a barn. In fact, that is what he is doing. But what he does not know is that the neighborhood generally consists of many fake barns — barn facades designed to look exactly like real barns when viewed from the road, as in the case of a visit in the countryside by Catherine II of Russia, just to please her. Since if he had been looking at one of them, he would have been unable to tell the difference, his "knowledge" that he was looking at a barn would seem to be poorly founded. A similar process appears in Robert A. Heinlein's "Stranger in a Strange Land" as an example of Fair Witness behavior.

The "no false premises" (or "no false lemmas") solution which was proposed early in the discussion proved to be somewhat problematic, as more general Gettier-style problems were then constructed or contrived in which the justified true belief does not seem to be the result of a chain of reasoning from a justified false belief.

For example:

Again, it seems as though Luke does not "know" that Mark is in the room, even though it is claimed he has a justified true belief that Mark is in the room, but it is not nearly so clear that the "perceptual belief" that "Mark is in the room" was inferred from any premises at all, let alone any false ones, nor led to significant conclusions on its own; Luke did not seem to be reasoning about anything; "Mark is in the room" seems to have been part of what he "seemed to see".

To save the "no false lemmas" solution, one must logically say that Luke's inference from sensory data does not count as a justified belief unless he consciously or unconsciously considers the possibilities of deception and self-deception. A justified version of Luke's thought process, by that logic, might go like this:

The second line counts as a false premise. However, by the previous argument, this suggests we have fewer justified beliefs than we think we do.

The main idea behind Gettier's examples is that the justification for the belief is flawed or incorrect, but the belief turns out to be true by sheer luck. Thus, a general scenario can be constructed as such:

Bob believes A is true because of B. Argument B is flawed, but A turns out to be true by a different argument C. Since A is true, Bob believes A is true, and Bob has justification for B, all of the conditions (JTB) are satisfied. However, Bob had no knowledge of A.

The Gettier problem is formally a problem in first-order logic, but the introduction by Gettier of terms such as "believes" and "knows" moves the discussion into the field of epistemology. Here, the sound (true) arguments ascribed to Smith then need also to be valid (believed) and convincing (justified) if they are to issue in the real-world discussion about "justified true belief".
Responses to Gettier problems have fallen into one of three categories:


One response, therefore, is that in none of the above cases was the belief justified because it is impossible to justify anything that is not true. Conversely, the fact that a proposition turns out to be untrue is proof that it was not sufficiently justified in the first place. Under this interpretation, the JTB definition of knowledge survives. This shifts the problem to a definition of justification, rather than knowledge. Another view is that justification and non-justification are not in binary opposition. Instead, justification is a matter of degree, with an idea being more or less justified. This account of justification is supported by mainstream philosophers such as Paul Boghossian and Stephen Hicks. In common sense usage, an idea can not only be more justified or less justified, but it can also be partially justified (Smith's boss told him X) and partially unjustified (Smith's boss is a liar). Gettier's cases involve propositions that were true, believed, but which had weak justification. In case 1, the premise that the testimony of Smith's boss is "strong evidence" is rejected. The case itself depends on the boss being either wrong or deceitful (Jones did not get the job) and therefore unreliable. In case 2, Smith again has accepted a questionable idea (Jones owns a Ford) with unspecified justification. Without justification, both cases do not undermine the JTB account of knowledge.

Other epistemologists accept Gettier's conclusion. Their responses to the Gettier problem, therefore, consist of trying to find alternative analyses of knowledge. They have struggled to discover and agree upon as a beginning any single notion of truth, or belief, or justifying which is wholly and obviously accepted. Truth, belief, and justifying have not yet been satisfactorily defined, so that JTB (justified true belief) may be defined satisfactorily is still problematical, on account or otherwise of Gettier's examples. Gettier, for many years a professor at University of Massachusetts Amherst later also was interested in the epistemic logic of Hintikka, a Finnish philosopher at Boston University, who published "Knowledge and Belief" in 1962.
The most common direction for this sort of response to take is what might be called a "JTB+G" analysis: that is, an analysis based on finding some "fourth" condition—a "no-Gettier-problem" condition—which, when added to the conditions of justification, truth, and belief, will yield a set of necessary and jointly sufficient conditions.

One such response is that of Alvin Goldman (1967), who suggested the addition of a "causal" condition: a subject's belief is justified, for Goldman, only if the truth of a belief has "caused" the subject to have that belief (in the appropriate way); and for a justified true belief to count as knowledge, the subject must "also" be able to "correctly reconstruct" (mentally) that causal chain. Goldman's analysis would rule out Gettier cases in that Smith's beliefs are not caused by the truths of those beliefs; it is merely "accidental" that Smith's beliefs in the Gettier cases happen to be true, or that the prediction made by Smith: "The winner of the job will have 10 coins", on the basis of his putative belief, (see also bundling) came true in this one case. This theory is challenged by the difficulty of giving a principled explanation of how an appropriate causal relationship differs from an inappropriate one (without the circular response of saying that the appropriate sort of causal relationship is the knowledge-producing one); or retreating to a position in which justified true belief is weakly defined as the consensus of learned opinion. The latter would be useful, but not as useful nor desirable as the unchanging definitions of scientific concepts such as momentum. Thus, adopting a causal response to the Gettier problem usually requires one to adopt (as Goldman gladly does) some form of reliabilism about justification. See "Goldman"s Theory of justification.

Keith Lehrer and Thomas Paxson (1969) proposed another response, by adding a "defeasibility condition" to the JTB analysis. On their account, knowledge is "undefeated justified true belief" — which is to say that a justified true belief counts as knowledge if and only if it is also the case that there is no further truth that, had the subject known it, would have defeated her present justification for the belief. (Thus, for example, Smith's justification for believing that the person who will get the job has ten coins in his pocket is his justified belief that Jones will get the job, combined with his justified belief that Jones has ten coins in his pocket. But if Smith had known the truth that Jones will "not" get the job, that would have defeated the justification for his belief.) 
Pragmatism was developed as a philosophical doctrine by C.S.Peirce and William James (1842–1910). In Peirce's view, the truth is nominally defined as a sign's correspondence to its object and pragmatically defined as the ideal final opinion to which sufficient investigation "would" lead sooner or later. James' epistemological model of truth was that which "works" in the way of belief, and a belief was true if in the long run it "worked" for all of us, and guided us expeditiously through our semihospitable world.
Peirce argued that metaphysics could be cleaned up by a pragmatic approach.
Consider what effects that might "conceivably" have practical bearings you "conceive" the objects of your "conception" to have. Then, your "conception" of those effects is the whole of your "conception" of the object.
From a pragmatic viewpoint of the kind often ascribed to James, defining on a particular occasion whether a particular belief can rightly be said to be both true and justified is seen as no more than an exercise in pedantry, but being able to discern whether that belief led to fruitful outcomes is a fruitful enterprise. Peirce emphasized fallibilism, considered the assertion of absolute certainty a barrier to inquiry, and in 1901 defined truth as follows: "Truth is that concordance of an abstract statement with the ideal limit towards which endless investigation would tend to bring scientific belief, which concordance the abstract statement may possess by virtue of the confession of its inaccuracy and one-sidedness, and this confession is an essential ingredient of truth." In other words, any unqualified assertion is likely to be at least a little wrong or, if right, still right for not entirely the right reasons. Therefore one is more veracious by being Socratic, including a recognition of one's own ignorance and knowing one may be proved wrong. This is the case, even though in practical matters one sometimes must act, if one is to act at all, with decision and complete confidence.

The difficulties involved in producing a viable fourth condition have led to claims that attempting to repair the JTB account is a deficient strategy. For example, one might argue that what the Gettier problem shows is not the need for a fourth independent condition in addition to the original three, but rather that the attempt to build up an account of knowledging by conjoining a set of independent conditions was misguided from the outset. Those who have adopted this approach generally argue that epistemological terms like justification, evidence, certainty, etc. should be analyzed in terms of a primitive notion of "knowledge," rather than vice versa. Knowledge is understood as "factive," that is, as embodying a sort of epistemological "tie" between a truth and a belief. The JTB account is then criticized for trying to get and encapsulate the factivity of knowledge "on the cheap," as it were, or via a circular argument, by replacing an irreducible notion of factivity with the conjunction of some of the properties that accompany it (in particular, truth and justification). Of course, the introduction of irreducible primitives into a philosophical theory is always problematical (some would say a sign of desperation), and such anti-reductionist accounts are unlikely to please those who have other reasons to hold fast to the method behind JTB+G accounts.

Fred Dretske developed an account of knowledge which he called "conclusive reasons", revived by Robert Nozick as what he called the "subjunctive" or truth-tracking account. Nozick's formulation posits that proposition p is an instance of knowledge when:


Nozick's definition is intended to preserve Goldman's intuition that Gettier cases should be ruled out by disacknowledging "accidentally" true justified beliefs, but without risking the potentially onerous consequences of building a causal requirement into the analysis. This tactic though, invites the riposte that Nozick's account merely hides the problem and does not solve it, for it leaves open the question of "why" Smith would not have had his belief if it had been false. The most promising answer seems to be that it is because Smith's belief was "caused" by the truth of what he believes; but that puts us back in the causalist camp.

Criticisms and counter examples (notably the "Grandma case") prompted a revision, which resulted in the alteration of (3) and (4) to limit themselves to the same method (i.e. vision):


Saul Kripke has pointed out that this view remains problematic and uses a counterexample called the "Fake Barn Country example", which describes a certain locality containing a number of fake barns or facades of barns. In the midst of these fake barns is one real barn, which is painted red. There is one more piece of crucial information for this example: the fake barns cannot be painted red.

Jones is driving along the highway, looks up and happens to see the real barn, and so forms the belief


Though Jones has gotten lucky, he could have just as easily been deceived and not have known it. Therefore it doesn't fulfill premise 4, for if Jones saw a fake barn he wouldn't have any idea it was a fake barn. So this is not knowledge.

An alternate example is if Jones looks up and forms the belief


According to Nozick's view this fulfills all four premises. Therefore this is knowledge, since Jones couldn't have been wrong, since the fake barns cannot be painted red. This is a troubling account however, since it seems the first statement "I see a barn" can be inferred from "I see a red barn"; however by Nozick's view the first belief is "not" knowledge and the second is knowledge.

In the first chapter of his book "Pyrronian Reflexions on Truth and Justification", Robert Fogelin gives a diagnosis that leads to a dialogical solution to Gettier's problem. The problem always arises when the given justification has nothing to do with what really makes the proposition true. Now, he notes that in such cases there is always a mismatch between the information disponible to the person who makes the knowledge-claim of some proposition p and the information disponible to the evaluator of this knowledge-claim (even if the evaluator is the same person in a later time). A Gettierian counterexample arises when the justification given by the person who makes the knowledge-claim cannot be accepted by the knowledge evaluator because it does not fit with his wider informational setting. For instance, in the case of the fake barn the evaluator knows that a superficial inspection from someone who does not know the peculiar circumstances involved isn't a justification acceptable as making the proposition p (that it is a real barn) true.
Richard Kirkham has proposed that it is best to start with a definition of knowledge so strong that giving a counterexample to it is logically impossible. Whether it can be weakened without becoming subject to a counterexample should then be checked. He concludes that there will always be a counterexample to any definition of knowledge in which the believer's evidence does not logically necessitate the belief. Since in most cases the believer's evidence does not necessitate a belief, Kirkham embraces skepticism about knowledge. He notes that a belief can still be rational even if it is not an item of knowledge. (see also: fallibilism)

One might respond to Gettier by finding a way to avoid his conclusion(s) in the first place. However, it can hardly be argued that knowledge is justified true belief if there are cases that are justified true belief without being knowledge; thus, those who want to avoid Gettier's conclusions have to find some way to defuse Gettier's counterexamples. In order to do so, within the parameters of the particular counter-example or exemplar, they must then either accept that


or, demonstrate a case in which it is possible to circumvent surrender to the exemplar by eliminating any necessity for it to be considered that JTB apply in just those areas that Gettier has rendered obscure, without thereby lessening the force of JTB to apply in those cases where it actually is crucial.
Then, though Gettier's cases "stipulate" that Smith has a certain belief and that his belief is true, it seems that in order to propose (1), one must argue that Gettier, (or, that is, the writer responsible for the particular form of words on this present occasion known as case (1), and who makes assertion's about Smith's "putative" beliefs), goes wrong because he has the wrong notion of "justification." Such an argument often depends on an externalist account on which "justification" is understood in such a way that whether or not a belief is "justified" depends not just on the internal state of the believer, but also on how that internal state is related to the outside world. Externalist accounts typically are constructed such that Smith's putative beliefs in Case I and Case II are not really justified (even though it seems to Smith that they are), because his beliefs are not lined up with the world in the right way, or that it is possible to show that it is invalid to assert that "Smith" has any significant "particular" belief at all, in terms of JTB or otherwise. Such accounts, of course, face the same burden as causalist responses to Gettier: they have to explain what sort of relationship between the world and the believer counts as a justificatory relationship.

Those who accept (2) are by far in the minority in analytic philosophy; generally those who are willing to accept it are those who have independent reasons to say that more things count as knowledge than the intuitions that led to the JTB account would acknowledge. Chief among these are epistemic minimalists such as Crispin Sartwell, who hold that all true belief, including both Gettier's cases and lucky guesses, counts as knowledge.

Some early work in the field of experimental philosophy suggested that traditional intuitions about Gettier cases might vary cross-culturally. However, subsequent studies have consistently failed to replicate these results, instead finding that participants from different cultures do share the traditional intuition. Indeed, more recent studies have actually been providing evidence for the opposite hypothesis, that people from a variety of different cultures have surprisingly similar intuitions in these cases.



</doc>
<doc id="1255319" url="https://en.wikipedia.org/wiki?curid=1255319" title="Functional illiteracy">
Functional illiteracy

Functional illiteracy is reading and writing skills that are inadequate "to manage daily living and employment tasks that require reading skills beyond a basic level". Functional illiteracy is contrasted with illiteracy in the strict sense, meaning the inability to read or write simple sentences in any language.

People who can read and write only in a language other than the predominant language of where they live may also be considered functionally illiterate.

Illiteracy as well as functional illiteracy were defined on the 20th session of UNESCO in 1978 as follows:

The characteristics of functional illiteracy vary from one culture to another, as some cultures require better reading and writing skills than others. A reading level that might be sufficient to make a farmer functionally literate in a rural area of a developing country might qualify as functional illiteracy in an urban area of a technologically advanced country. In languages with phonemic spelling, functional illiteracy might be defined simply as reading too slow for practical use, inability to effectively use dictionaries and written manuals, etc.

In developed countries, the level of functional literacy of an individual is proportional to income level and inversely proportional to the risk of committing certain kinds of crime. For example, according to the National Center for Educational Statistics in the United States:

In the United States, according to "Business" magazine, an estimated 15 million functionally illiterate adults held jobs at the beginning of the 21st century. The American Council of Life Insurers reported that 75% of the Fortune 500 companies provide some level of remedial training for their workers. As of 2003, 30 million (14% of adults) were unable to perform simple and everyday literacy activities.

The National Center for Education Statistics provides more detail. Literacy is broken down into three parameters: prose, document, and quantitative literacy. Each parameter has four levels: below basic, basic, intermediate, and proficient. For prose literacy, for example, a below basic level of literacy means that a person can look at a short piece of text to get a small piece of uncomplicated information, while a person who is below basic in quantitative literacy would be able to do simple addition. In the US, 14% of the adult population is at the "below basic" level for prose literacy; 12% are at the "below basic" level for document literacy; and 22% are at that level for quantitative literacy. Only 13% of the population is proficient in these three areas—able to compare viewpoints in two editorials; interpret a table about blood pressure, age, and physical activity; or compute and compare the cost per ounce of food items.

The UK government's Department for Education reported in 2006 that 47% of school children left school at age 16 without having achieved a basic level in functional mathematics, and 42% fail to achieve a basic level of functional English. Every year, 100,000 pupils leave school functionally illiterate in the UK.

While in Russia, where more than 99% percent of the population is technically literate, only one third of high school graduates can comprehend the content of scientific and literary texts, according to a 2015 study.

A Literacy at Work study, published by the Northeast Institute in 2001, found that business losses attributed to basic skill deficiencies run into billions of dollars a year due to low productivity, errors, and accidents attributed to functional illiteracy.

Sociological research has demonstrated that countries with lower levels of functional illiteracy among their adult populations tend to be those with the highest levels of scientific literacy among the lower stratum of young people nearing the end of their formal academic studies. This correspondence suggests that the capacity of schools to ensure students attain the functional literacy required to comprehend the basic texts and documents associated with competent citizenship contributes to a society's level of civic literacy.




</doc>
<doc id="3657390" url="https://en.wikipedia.org/wiki?curid=3657390" title="Nous">
Nous

Nous (, ), sometimes equated to intellect or intelligence, is a term from classical philosophy for the faculty of the human mind necessary for understanding what is true or real. English words such as "understanding" are sometimes used, but three commonly used philosophical terms come directly from classical languages: "νοῦς" or "νόος" (from Ancient Greek), "intellēctus" and "intellegentia" (from Latin). To describe the activity of this faculty, the word "intellection" is sometimes used in philosophical contexts, as well as the Greek words "noēsis" and "noeîn" ("νόησις", "νοεῖν"). This activity is understood in a similar way (at least in some contexts) to the modern concept of intuition.

In philosophy, common English translations include "understanding" and "mind"; or sometimes "thought" or "reason" (in the sense of that which reasons, not the activity of reasoning). It is also often described as something equivalent to perception except that it works within the mind ("the mind's eye"). It has been suggested that the basic meaning is something like "awareness". In colloquial British English, "nous" also denotes "good sense", which is close to one everyday meaning it had in Ancient Greece.
In Aristotle's influential works, the term was carefully distinguished from sense perception, imagination, and reason, although these terms are closely inter-related. The term was apparently already singled out by earlier philosophers such as Parmenides, whose works are largely lost. In post-Aristotelian discussions, the exact boundaries between perception, understanding of perception, and reasoning have not always agreed with the definitions of Aristotle, even though his terminology remains influential.

In the Aristotelian scheme, "nous" is the basic understanding or awareness that allows human beings to think rationally. For Aristotle, this was distinct from the processing of sensory perception, including the use of imagination and memory, which other animals can do. This therefore connects discussion of "nous" to discussion of how the human mind sets definitions in a consistent and communicable way, and whether people must be born with some innate potential to understand the same universal categories in the same logical ways. Deriving from this it was also sometimes argued, especially in classical and medieval philosophy, that the individual "nous" must require help of a spiritual and divine type. By this type of account, it came to be argued that the human understanding ("nous") somehow stems from this cosmic "nous", which is however not just a recipient of order, but a creator of it. Such explanations were influential in the development of medieval accounts of God, the immortality of the soul, and even the motions of the stars, in Europe, North Africa and the Middle East, amongst both eclectic philosophers and authors representing all the major faiths of their times.

In early Greek uses, Homer used "nous" to signify mental activities of both mortals and immortals, for example what they really have on their mind as opposed to what they say aloud. It was one of several words related to thought, thinking, and perceiving with the mind. In pre-Socratic philosophy, it became increasingly distinguished as a source of knowledge and reasoning opposed to mere sense perception or thinking influenced by the body such as emotion. For example, Heraclitus complained that "much learning does not teach "nous"".

Among some Greek authors, a faculty of intelligence known as a "higher mind" came to be considered as a property of the cosmos as a whole.

The work of Parmenides set the scene for Greek philosophy to come and the concept of "nous" was central to his radical proposals. He claimed that reality as the senses perceive it is not a world of truth at all, because sense perception is so unreliable, and what is perceived is so uncertain and changeable. Instead he argued for a dualism wherein "nous" and related words (the verb for thinking which describes its mental perceiving activity, "noein", and the unchanging and eternal objects of this perception, "noēta") describe a form of perception which is not physical, but intellectual only, distinct from sense perception and the objects of sense perception.

Anaxagoras, born about 500 BC, is the first person who is definitely known to have explained the concept of a "nous" (mind), which arranged all other things in the cosmos in their proper order, started them in a rotating motion, and continuing to control them to some extent, having an especially strong connection with living things. (However Aristotle reports an earlier philosopher, Hermotimus of Clazomenae, who had taken a similar position.) Amongst the pre-Socratic philosophers before Anaxagoras, other philosophers had proposed a similar ordering human-like principle causing life and the rotation of the heavens. For example, Empedocles, like Hesiod much earlier, described cosmic order and living things as caused by a cosmic version of love, and Pythagoras and Heraclitus, attributed the cosmos with "reason" ("logos").

According to Anaxagoras the cosmos is made of infinitely divisible matter, every bit of which can inherently become anything, except Mind ("nous"), which is also matter, but which can only be found separated from this general mixture, or else mixed into living things, or in other words in the Greek terminology of the time, things with a soul ("psychē"). Anaxagoras wrote:

Concerning cosmology, Anaxagoras, like some Greek philosophers already before him, believed the cosmos was revolving, and had formed into its visible order as a result of such revolving causing a separating and mixing of different types of chemical elements. "Nous", in his system, originally caused this revolving motion to start, but it does not necessarily continue to play a role once the mechanical motion has started. His description was in other words (shockingly for the time) corporeal or mechanical, with the moon made of earth, the sun and stars made of red hot metal (beliefs Socrates was later accused of holding during his trial) and "nous" itself being a physical fine type of matter which also gathered and concentrated with the development of the cosmos. This "nous" (mind) is not incorporeal; it is the thinnest of all things. The distinction between "nous" and other things nevertheless causes his scheme to sometimes be described as a peculiar kind of dualism.

Anaxagoras' concept of "nous" was distinct from later platonic and neoplatonic cosmologies in many ways, which were also influenced by Eleatic, Pythagorean and other pre-Socratic ideas, as well as the Socratics themselves.

In some schools of Hindu philosophy, a "higher mind" came to be considered a property of the cosmos as a whole that exists within all matter (known as buddhi or mahat). In Samkhya, this faculty of intellect (buddhi) serves to differentiate matter (prakrti) from pure consciousness (purusha). The lower aspect of mind that corresponds to the senses is referred to as "manas".

Xenophon, the less famous of the two students of Socrates whose written accounts of him have survived, recorded that he taught his students a kind of teleological justification of piety and respect for divine order in nature. This has been described as an "intelligent design" argument for the existence of God, in which nature has its own "nous". For example, in his "Memorabilia" 1.4.8, he describes Socrates asking a friend sceptical of religion, "Are you, then, of the opinion that intelligence ("nous") alone exists nowhere and that you by some good chance seized hold of it, while—as you think—those surpassingly large and infinitely numerous things [all the earth and water] are in such orderly condition through some senselessness?" Later in the same discussion he compares the "nous", which directs each person's body, to the good sense ("phronēsis") of the god, which is in everything, arranging things to its pleasure (1.4.17). Plato describes Socrates making the same argument in his "Philebus" 28d, using the same words "nous" and "phronēsis".

Plato used the word "nous" in many ways that were not unusual in the everyday Greek of the time, and often simply meant "good sense" or "awareness". On the other hand, in some of his Platonic dialogues it is described by key characters in a higher sense, which was apparently already common. In his "Philebus" 28c he has Socrates say that "all philosophers agree—whereby they really exalt themselves—that mind ("nous") is king of heaven and earth. Perhaps they are right." and later states that the ensuing discussion "confirms the utterances of those who declared of old that mind ("nous") always rules the universe".

In his "Cratylus", Plato gives the etymology of Athena's name, the goddess of wisdom, from "Atheonóa" (Ἀθεονόα) meaning "god's ("theos") mind ("nous")". In his "Phaedo", Plato's teacher Socrates is made to say just before dying that his discovery of Anaxagoras' concept of a cosmic "nous" as the cause of the order of things, was an important turning point for him. But he also expressed disagreement with Anaxagoras' understanding of the implications of his own doctrine, because of Anaxagoras' materialist understanding of causation. Socrates said that Anaxagoras would "give voice and air and hearing and countless other things of the sort as causes for our talking with each other, and should fail to mention the real causes, which are, that the Athenians decided that it was best to condemn me". On the other hand, Socrates seems to suggest that he also failed to develop a fully satisfactory teleological and dualistic understanding of a mind of nature, whose aims represent the Good, which all parts of nature aim at.

Concerning the "nous" that is the source of understanding of individuals, Plato is widely understood to have used ideas from Parmenides in addition to Anaxagoras. Like Parmenides, Plato argued that relying on sense perception can never lead to true knowledge, only opinion. Instead, Plato's more philosophical characters argue that "nous" must somehow perceive truth directly in the ways gods and daimons perceive. What our mind sees directly in order to really understand things must not be the constantly changing material things, but unchanging entities that exist in a different way, the so-called "forms" or "ideas". However he knew that contemporary philosophers often argued (as in modern science) that "nous" and perception are just two aspects of one physical activity, and that perception is the source of knowledge and understanding (not the other way around).

Just exactly how Plato believed that the "nous" of people lets them come to understand things in any way that improves upon sense perception and the kind of thinking which animals have, is a subject of long running discussion and debate. On the one hand, in the "Republic" Plato's Socrates, in the Analogy of the sun and Allegory of the Cave describes people as being able to perceive more clearly because of something from outside themselves, something like when the sun shines, helping eyesight. The source of this illumination for the intellect is referred to as the Form of the Good. On the other hand, in the "Meno" for example, Plato's Socrates explains the theory of "anamnesis" whereby people are born with ideas already in their soul, which they somehow remember from previous lives. Both theories were to become highly influential.

As in Xenophon, Plato's Socrates frequently describes the soul in a political way, with ruling parts, and parts that are by nature meant to be ruled. "Nous" is associated with the rational ("logistikon") part of the individual human soul, which by nature should rule. In his "Republic", in the so-called "analogy of the divided line", it has a special function within this rational part. Plato tended to treat "nous" as the only immortal part of the soul.

Concerning the cosmos, in the "Timaeus", the title character also tells a "likely story" in which "nous" is responsible for the creative work of the demiurge or maker who brought rational order to our universe. This craftsman imitated what he perceived in the world of eternal Forms. In the "Philebus" Socrates argues that "nous" in individual humans must share in a cosmic "nous", in the same way that human bodies are made up of small parts of the elements found in the rest of the universe. And this "nous" must be in the "genos" of being a cause of all particular things as particular things.

Like Plato, Aristotle saw the "nous" or intellect of an individual as somehow similar to sense perception but also distinct. Sense perception in action provides images to the "nous", via the "sensus communis" and imagination, without which thought could not occur. But other animals have "sensus communis" and imagination, whereas none of them have "nous". Aristotelians divide perception of forms into the animal-like one which perceives "species sensibilis" or "sensible forms", and "species intelligibilis" that are perceived in a different way by the "nous".

Like Plato, Aristotle linked "nous" to "logos" (reason) as uniquely human, but he also distinguished "nous" from "logos", thereby distinguishing the faculty for setting definitions from the faculty that uses them to reason with. In his "Nicomachean Ethics", Aristotle divides the soul ("psychē") into two parts, one which has reason and one which does not, but then divides the part which has reason into the reasoning ("logistikos") part itself which is lower, and the higher "knowing" ("epistēmonikos") part which contemplates general principles ("archai"). "Nous", he states, is the source of the first principles or sources ("archai") of definitions, and it develops naturally as people gain experience. This he explains after first comparing the four other truth revealing capacities of soul: technical know how ("technē"), logically deduced knowledge ("epistēmē", sometimes translated as "scientific knowledge"), practical wisdom ("phronēsis"), and lastly theoretical wisdom ("sophia"), which is defined by Aristotle as the combination of "nous" and "epistēmē". All of these others apart from "nous" are types of reason ("logos").
Aristotle's philosophical works continue many of the same Socratic themes as his teacher Plato. Amongst the new proposals he made was a way of explaining causality, and "nous" is an important part of his explanation. As mentioned above, Plato criticized Anaxagoras' materialism, or understanding that the intellect of nature only set the cosmos in motion, but is no longer seen as the cause of physical events. Aristotle explained that the changes of things can be described in terms of four causes at the same time. Two of these four causes are similar to the materialist understanding: each thing has a material which causes it to be how it is, and some other thing which set in motion or initiated some process of change. But at the same time according to Aristotle each thing is also caused by the natural forms they are tending to become, and the natural ends or aims, which somehow exist in nature as causes, even in cases where human plans and aims are not involved. These latter two causes (the "formal" and "final"), are concepts no longer used in modern science, and encompass the continuous effect of the intelligent ordering principle of nature itself. Aristotle's special description of causality is especially apparent in the natural development of living things. It leads to a method whereby Aristotle analyses causation and motion in terms of the potentialities and actualities of all things, whereby all matter possesses various possibilities or potentialities of form and end, and these possibilities become more fully real as their potential forms become actual or active reality (something they will do on their own, by nature, unless stopped because of other natural things happening). For example, a stone has in its nature the potentiality of falling to the earth and it will do so, and actualize this natural tendency, if nothing is in the way.

Aristotle analyzed thinking in the same way. For him, the possibility of understanding rests on the relationship between intellect and sense perception. Aristotle's remarks on the concept of what came to be called the "active intellect" and "passive intellect" (along with various other terms) are amongst "the most intensely studied sentences in the history of philosophy". The terms are derived from a single passage in Aristotle's "De Anima", Book III. Following is the translation of one of those passages with some key Greek words shown in square brackets.
...since in nature one thing is the material ["hulē"] for each kind ["genos"] (this is what is in potency all the particular things of that kind) but it is something else that is the causal and productive thing by which all of them are formed, as is the case with an art in relation to its material, it is necessary in the soul ["psychē"] too that these distinct aspects be present;
the one sort is intellect ["nous"] by becoming all things, the other sort by forming all things, in the way an active condition ["hexis"] like light too makes the colors that are in potency be at work as colors ["to phōs poiei ta dunamei onta chrōmata energeiai chrōmata"].
This sort of intellect [which is like light in the way it makes potential things work as what they are] is separate, as well as being without attributes and unmixed, since it is by its thinghood a being-at-work ["energeia"], for what acts is always distinguished in stature above what is acted upon, as a governing source is above the material it works on.
Knowledge ["epistēmē"], in its being-at-work, is the same as the thing it knows, and while knowledge in potency comes first in time in any one knower, in the whole of things it does not take precedence even in time.
This does not mean that at one time it thinks but at another time it does not think, but when separated it is just exactly what it is, and this alone is deathless and everlasting (though we have no memory, because this sort of intellect is not acted upon, while the sort that is acted upon is destructible), and without this nothing thinks.

The passage tries to explain "how the human intellect passes from its original state, in which it does not think, to a subsequent state, in which it does" according to his distinction between potentiality and actuality. Aristotle says that the passive intellect receives the intelligible forms of things, but that the active intellect is required to make the potential knowledge into actual knowledge, in the same way that light makes potential colours into actual colours. As Davidson remarks:
Just what Aristotle meant by potential intellect and active intellect - terms not even explicit in the "De anima" and at best implied - and just how he understood the interaction between them remains moot. Students of the history of philosophy continue to debate Aristotle's intent, particularly the question whether he considered the active intellect to be an aspect of the human soul or an entity existing independently of man.

The passage is often read together with "Metaphysics", Book XII, ch.7-10, where Aristotle makes "nous" as an actuality a central subject within a discussion of the cause of being and the cosmos. In that book, Aristotle equates active "nous", when people think and their "nous" becomes what they think about, with the "unmoved mover" of the universe, and God: "For the actuality of thought ("nous") is life, and God is that actuality; and the essential actuality of God is life most good and eternal." Alexander of Aphrodisias, for example, equated this active intellect which is God with the one explained in "De Anima", while Themistius thought they could not be simply equated. (See below.)

Like Plato before him, Aristotle believes Anaxagoras' cosmic "nous" implies and requires the cosmos to have intentions or ends: "Anaxagoras makes the Good a principle as causing motion; for Mind ("nous") moves things, but moves them for some end, and therefore there must be some other Good—unless it is as we say; for on our view the art of medicine is in a sense health."

In the philosophy of Aristotle the soul (psyche) of a body is what makes it alive, and is its actualized form; thus, every living thing, including plant life, has a soul. The mind or intellect ("nous") can be described variously as a power, faculty, part, or aspect of the human soul. For Aristotle, soul and "nous" are not the same. He did not rule out the possibility that "nous" might survive without the rest of the soul, as in Plato, but he specifically says that this immortal "nous" does not include any memories or anything else specific to an individual's life. In his "Generation of Animals" Aristotle specifically says that while other parts of the soul come from the parents, physically, the human "nous", must come from outside, into the body, because it is divine or godly, and it has nothing in common with the "energeia" of the body. This was yet another passage which Alexander of Aphrodisias would link to those mentioned above from "De Anima" and the "Metaphysics" in order to understand Aristotle's intentions.

Until the early modern era, much of the discussion which has survived today concerning "nous" or intellect, in Europe, Africa and the Middle East, concerned how to correctly interpret Aristotle and Plato. However, at least during the classical period, materialist philosophies, more similar to modern science, such as Epicureanism, were still relatively common also. The Epicureans believed that the bodily senses themselves were not the cause of error, but the interpretations can be. The term "prolepsis" was used by Epicureans to describe the way the mind forms general concepts from sense perceptions.

To the Stoics, more like Heraclitus than Anaxagoras, order in the cosmos comes from an entity called logos, the cosmic reason. But as in Anaxagoras this cosmic reason, like human reason but higher, is connected to the reason of individual humans. The Stoics however, did not invoke incorporeal causation, but attempted to explain physics and human thinking in terms of matter and forces. As in Aristotelianism, they explained the interpretation of sense data requiring the mind to be stamped or formed with ideas, and that people have shared conceptions that help them make sense of things ("koine ennoia"). "Nous" for them is soul "somehow disposed" ("pôs echon"), the soul being somehow disposed "pneuma", which is fire or air or a mixture. As in Plato, they treated "nous" as the ruling part of the soul.

Plutarch criticized the Stoic idea of "nous" being corporeal, and agreed with Plato that the soul is more divine than the body while "nous" (mind) is more divine than the soul. The mix of soul and body produces pleasure and pain; the conjunction of mind and soul produces reason which is the cause or the source of virtue and vice. (From: “On the Face in the Moon”)

Albinus was one of the earliest authors to equate Aristotle's "nous" as prime mover of the Universe, with Plato's Form of the Good.

Alexander of Aphrodisias was a Peripatetic (Aristotelian) and his "On the Soul" (referred to as "De anima" in its traditional Latin title), explained that by his interpretation of Aristotle, potential intellect in man, that which has no nature but receives one from the active intellect, is material, and also called the "material intellect" ("nous hulikos") and it is inseparable from the body, being "only a disposition" of it. He argued strongly against the doctrine of immortality. On the other hand, he identified the active intellect ("nous poietikos"), through whose agency the potential intellect in man becomes actual, not with anything from within people, but with the divine creator itself. In the early Renaissance his doctrine of the soul's mortality was adopted by Pietro Pomponazzi against the Thomists and the Averroists. For him, the only possible human immortality is an immortality of a detached human thought, more specifically when the "nous" has as the object of its thought the active intellect itself, or another incorporeal intelligible form.

Alexander was also responsible for influencing the development of several more technical terms concerning the intellect, which became very influential amongst the great Islamic philosophers, Al-Farabi, Avicenna, and Averroes.

Themistius, another influential commentator on this matter, understood Aristotle differently, stating that the passive or material intellect does "not employ a bodily organ for its activity, is wholly unmixed with the body, impassive, and separate [from matter]". This means the human potential intellect, and not only the active intellect, is an incorporeal substance, or a disposition of incorporeal substance. For Themistius, the human soul becomes immortal "as soon as the active intellect intertwines with it at the outset of human thought".

This understanding of the intellect was also very influential for Al-Farabi, Avicenna, and Averroes, and "virtually all Islamic and Jewish philosophers". On the other hand, concerning the active intellect, like Alexander and Plotinus, he saw this as a transcendent being existing above and outside man. Differently from Alexander, he did not equate this being with the first cause of the Universe itself, but something lower. However he equated it with Plato's Idea of the Good.

Of the later Greek and Roman writers Plotinus, the initiator of neoplatonism, is particularly significant. Like Alexander of Aphrodisias and Themistius, he saw himself as a commentator explaining the doctrines of Plato and Aristotle. But in his "Enneads" he went further than those authors, often working from passages which had been presented more tentatively, possibly inspired partly by earlier authors such as the neopythagorean Numenius of Apamea. Neoplatonism provided a major inspiration to discussion concerning the intellect in late classical and medieval philosophy, theology and cosmology.

In neoplatonism there exists several levels or "hypostases" of being, including the natural and visible world as a lower part.
This was based largely upon Plotinus' reading of Plato, but also incorporated many Aristotelian concepts, including the Unmoved Mover as "energeia". They also incorporated a theory of "anamnesis", or knowledge coming from the past lives of our immortal souls, like that found in some of Plato's dialogues.

Later Platonists distinguished a hierarchy of three separate manifestations of "nous", like Numenius of Apamea had. Notable later neoplatonists include Porphyry and Proclus.

Greek philosophy had an influence on the major religions that defined the Middle Ages, and one aspect of this was the concept of "nous".

Gnosticism was a late classical movement that incorporated ideas inspired by neoplatonism and neopythagoreanism, but which was more a syncretic religious movement than an accepted philosophical movement.

In Valentinianism, Nous is the first male Aeon. Together with his conjugate female Aeon, Aletheia (truth), he emanates from the Propator Bythos ( "Forefather Depths") and his co-eternal Ennoia ( "Thought") or Sigē ( "Silence"); and these four form the primordial Tetrad. Like the other male Aeons he is sometimes regarded as androgynous, including in himself the female Aeon who is paired with him. He is the Only Begotten; and is styled the Father, the Beginning of All, inasmuch as from him are derived immediately or mediately the remaining Aeons who complete the Ogdoad (eight), thence the Decad (ten), and thence the Dodecad (twelve); in all, thirty Aeons constitute the Pleroma.

He alone is capable of knowing the Propator; but when he desired to impart like knowledge to the other Aeons, was withheld from so doing by Sigē. When Sophia ("Wisdom"), youngest Aeon of the thirty, was brought into peril by her yearning after this knowledge, Nous was foremost of the Aeons in interceding for her. From him, or through him from the Propator, Horos was sent to restore her. After her restoration, Nous, according to the providence of the Propator, produced another pair, Christ and the Holy Spirit, "in order to give fixity and steadfastness () to the Pleroma." For this Christ teaches the Aeons to be content to know that the Propator is in himself incomprehensible, and can be perceived only through the Only Begotten (Nous).

A similar conception of Nous appears in the later teaching of the Basilideans, according to which he is the first begotten of the Unbegotten Father, and himself the parent of "Logos", from whom emanate successively "Phronesis", "Sophia", and "Dunamis". But in this teaching, Nous is identified with Christ, is named Jesus, is sent to save those that believe, and returns to Him who sent him, after a Passion which is apparent only, Simon of Cyrene being substituted for him on the cross. It is probable, however, that Nous had a place in the original system of Basilides himself; for his "Ogdoad", "the great Archon of the universe, the ineffable" is apparently made up of the five members named by Irenaeus (as above), together with two whom we find in Clement of Alexandria, "Dikaiosyne" and "Eirene", added to the originating Father.

The antecedent of these systems is that of Simon, of whose six "roots" emanating from the Unbegotten Fire, "Nous" is first. The correspondence of these "roots" with the first six "Aeons" that Valentinus derives from "Bythos", is noted by Hippolytus. Simon says in his "Apophasis Megalē",

To Nous and "Epinoia" correspond Heaven and Earth, in the list given by Simon of the six material counterparts of his six emanations. The identity of this list with the six material objects alleged by Herodotus to be worshipped by the Persians, together with the supreme place given by Simon to Fire as the primordial power, leads us to look to Iran for the origin of these systems in one aspect. In another, they connect themselves with the teaching of Pythagoras and of Plato.

According to the "Gospel of Mary", Jesus himself articulates the essence of "Nous":

During the Middle Ages, philosophy itself was in many places seen as opposed to the prevailing monotheistic religions, Islam, Christianity and Judaism. The strongest philosophical tradition for some centuries was amongst Islamic philosophers, who later came to strongly influence the late medieval philosophers of western Christendom, and the Jewish diaspora in the Mediterranean area. While there were earlier Muslim philosophers such as Al Kindi, chronologically the three most influential concerning the intellect were Al Farabi, Avicenna, and finally Averroes, a westerner who lived in Spain and was highly influential in the late Middle Ages amongst Jewish and Christian philosophers.

The exact precedents of Al Farabi's influential philosophical scheme, in which "nous" (Arabic "ʿaql") plays an important role, are no longer perfectly clear because of the great loss of texts in the Middle Ages which he would have had access to. He was apparently innovative in at least some points. He was clearly influenced by the same late classical world as neoplatonism, neopythagoreanism, but exactly how is less clear. Plotinus, Themistius and Alexander of Aphrodisias are generally accepted to have been influences. However while these three all placed the active intellect "at or near the top of the hierarchy of being", Al Farabi was clear in making it the lowest ranking in a series of distinct transcendental intelligences. He is the first known person to have done this in a clear way. He was also the first philosopher known to have assumed the existence of a causal hierarchy of celestial spheres, and the incorporeal intelligences parallel to those spheres. Al Farabi also fitted an explanation of prophecy into this scheme, in two levels. According to Davidson (p. 59):The lower of the two levels, labeled specifically as "prophecy" ("nubuwwa"), is enjoyed by men who have not yet perfected their intellect, whereas the higher, which Alfarabi sometimes specifically names "revelation" ("w-ḥ-y"), comes exclusively to those who stand at the stage of acquired intellect.
This happens in the imagination (Arabic "mutakhayyila"; Greek "phantasia"), a faculty of the mind already described by Aristotle, which al Farabi described as serving the rational part of the soul (Arabic "ʿaql"; Greek "nous"). This faculty of imagination stores sense perceptions ("maḥsūsāt"), disassembles or recombines them, creates figurative or symbolic images ("muḥākāt") of them which then appear in dreams, visualizes present and predicted events in a way different from conscious deliberation ("rawiyya"). This is under the influence, according to Al Farabi, of the active intellect. Theoretical truth can only be received by this faculty in a figurative or symbolic form, because the imagination is a physical capability and can not receive theoretical information in a proper abstract form. This rarely comes in a waking state, but more often in dreams. The lower type of prophecy is the best possible for the imaginative faculty, but the higher type of prophecy requires not only a receptive imagination, but also the condition of an "acquired intellect", where the human "nous" is in "conjunction" with the active intellect in the sense of God. Such a prophet is also a philosopher. When a philosopher-prophet has the necessary leadership qualities, he becomes philosopher-king.

In terms of cosmology, according to Davidson (p. 82) "Avicenna's universe has a structure virtually identical with the structure of Alfarabi's" but there are differences in details. As in Al Farabi, there are several levels of intellect, intelligence or "nous", each of the higher ones being associated with a celestial sphere. Avicenna however details three different types of effect which each of these higher intellects has, each "thinks" both the necessary existence and the possible being of the intelligence one level higher. And each "emanates" downwards the body and soul of its own celestial sphere, and also the intellect at the next lowest level. The active intellect, as in Alfarabi, is the last in the chain. Avicenna sees active intellect as the cause not only of intelligible thought and the forms in the "sublunar" world we people live, but also the matter. (In other words, three effects.)

Concerning the workings of the human soul, Avicenna, like Al Farabi, sees the "material intellect" or potential intellect as something that is not material. He believed the soul was incorporeal, and the potential intellect was a disposition of it which was in the soul from birth. As in Al Farabi there are two further stages of potential for thinking, which are not yet actual thinking, first the mind acquires the most basic intelligible thoughts which we can not think in any other way, such as "the whole is greater than the part", then comes a second level of derivative intelligible thoughts which could be thought. Concerning the actualization of thought, Avicenna applies the term "to two different things, to actual human thought, irrespective of the intellectual progress a man has made, and to actual thought when human intellectual development is complete", as in Al Farabi.

When reasoning in the sense of deriving conclusions from syllogisms, Avicenna says people are using a physical "cogitative" faculty ("mufakkira, fikra") of the soul, which can err. The human cogitative faculty is the same as the "compositive imaginative faculty ("mutakhayyila") in reference to the animal soul". But some people can use "insight" to avoid this step and derive conclusions directly by conjoining with the active intellect.

Once a thought has been learned in a soul, the physical faculties of sense perception and imagination become unnecessary, and as a person acquires more thoughts, their soul becomes less connected to their body. For Avicenna, different from the normal Aristotelian position, all of the soul is by nature immortal. But the level of intellectual development does affect the type of afterlife that the soul can have. Only a soul which has reached the highest type of conjunction with the active intellect can form a perfect conjunction with it after the death of the body, and this is a supreme "eudaimonia". Lesser intellectual achievement means a less happy or even painful afterlife.

Concerning prophecy, Avicenna identifies a broader range of possibilities which fit into this model, which is still similar to that of Al Farabi.

Averroes came to be regarded even in Europe as "the Commentator" to "the Philosopher", Aristotle, and his study of the questions surrounding the "nous" were very influential amongst Jewish and Christian philosophers, with some aspects being quite controversial. According to Herbert Davidson, Averroes' doctrine concerning "nous" can be divided into two periods. In the first, neoplatonic emanationism, not found in the original works of Aristotle, was combined with a naturalistic explanation of the human material intellect. "It also insists on the material intellect's having an active intellect as a direct object of thought and conjoining with the active intellect, notions never expressed in the Aristotelian canon." It was this presentation which Jewish philosophers such as Moses Narboni and Gersonides understood to be Averroes'. In the later model of the universe, which was transmitted to Christian philosophers, Averroes "dismisses emanationism and explains the generation of living beings in the sublunar world naturalistically, all in the name of a more genuine Aristotelianism. Yet it abandons the earlier naturalistic conception of the human material intellect and transforms the material intellect into something wholly un-Aristotelian, a single transcendent entity serving all mankind. It nominally salvages human conjunction with the active intellect, but in words that have little content."

This position, that humankind shares one active intellect, was taken up by Parisian philosophers such as Siger of Brabant, but also widely rejected by philosophers such as Albertus Magnus, Thomas Aquinas, Ramon Lull, and Duns Scotus. Despite being widely considered heretical, the position was later defended by many more European philosophers including John of Jandun, who was the primary link bringing this doctrine from Paris to Bologna. After him this position continued to be defended and also rejected by various writers in northern Italy. In the 16th century it finally became a less common position after the renewal of an "Alexandrian" position based on that of Alexander of Aphrodisias, associated with Pietro Pomponazzi.

The Christian New Testament makes mention of the "nous" or "noos", generally translated in modern English as "mind", but also showing a link to God's will or law:

In the writings of the Christian fathers a sound or pure "nous" is considered essential to the cultivation of wisdom.

While philosophical works were not commonly read or taught in the early Middle Ages in most of Europe, the works of authors like Boethius and Augustine of Hippo formed an important exception. Both were influenced by neoplatonism, and were amongst the older works that were still known in the time of the Carolingian Renaissance, and the beginnings of Scholasticism.

In his early years Augustine was heavily influenced by Manichaeism and afterwards by the Neoplatonism of Plotinus. After his conversion to Christianity and baptism (387), he developed his own approach to philosophy and theology, accommodating a variety of methods and different perspectives.

Augustine used Neoplatonism selectively. He used both the neoplatonic "Nous", and the Platonic Form of the Good (or "The Idea of the Good") as equivalent terms for the Christian God, or at least for one particular aspect of God. For example, God, "nous", can act directly upon matter, and not only through souls, and concerning the souls through which it works upon the world experienced by humanity, some are treated as angels.

Scholasticism becomes more clearly defined much later, as the peculiar native type of philosophy in medieval catholic Europe. In this period, Aristotle became "the Philosopher", and scholastic philosophers, like their Jewish and Muslim contemporaries, studied the concept of the "intellectus" on the basis not only of Aristotle, but also late classical interpreters like Augustine and Boethius. A European tradition of new and direct interpretations of Aristotle developed which was eventually strong enough to argue with partial success against some of the interpretations of Aristotle from the Islamic world, most notably Averroes' doctrine of their being one "active intellect" for all humanity. Notable "Catholic" (as opposed to Averroist) Aristotelians included Albertus Magnus and Thomas Aquinas, the founder of Thomism, which exists to this day in various forms. Concerning the "nous", Thomism agrees with those Aristotelians who insist that the intellect is immaterial and separate from any bodily organs, but as per Christian doctrine, the whole of the human soul is immortal, not only the intellect.

The human "nous" in Eastern Orthodox Christianity is the "eye of the heart or soul" or the "mind of the heart". The soul of man, is created by God in His image, man's soul is intelligent and noetic. Saint Thalassius of Syria wrote that God created beings "with a capacity to receive the Spirit and to attain knowledge of Himself; He has brought into existence the senses and sensory perception to serve such beings". Eastern Orthodox Christians hold that God did this by creating mankind with intelligence and noetic faculties.

Human reasoning is not enough: there will always remain an "irrational residue" which escapes analysis and which can not be expressed in concepts: it is this unknowable depth of things, that which constitutes their true, indefinable essence that also reflects the origin of things in God. In Eastern Christianity it is by faith or intuitive truth that this component of an objects existence is grasped. Though God through his energies draws us to him, his essence remains inaccessible. The operation of faith being the means of free will by which mankind faces the future or unknown, these noetic operations contained in the concept of insight or "noesis". Faith ("pistis") is therefore sometimes used interchangeably with "noesis" in Eastern Christianity.

Angels have intelligence and "nous", whereas men have reason, both "logos" and "dianoia", "nous" and sensory perception. This follows the idea that man is a microcosm and an expression of the whole creation or macrocosmos. The human "nous" was darkened after the Fall of Man (which was the result of the rebellion of reason against the "nous"), but after the purification (healing or correction) of the "nous" (achieved through ascetic practices like hesychasm), the human "nous" (the "eye of the heart") will see God's uncreated Light (and feel God's uncreated love and beauty, at which point the nous will start the unceasing prayer of the heart) and become illuminated, allowing the person to become an orthodox theologian.

In this belief, the soul is created in the image of God. Since God is Trinitarian, Mankind is "Nous", reason, both "logos" and "dianoia", and Spirit. The same is held true of the soul (or heart): it has "nous", word and spirit. To understand this better first an understanding of Saint Gregory Palamas's teaching that man is a representation of the trinitarian mystery should be addressed. This holds that God is not meant in the sense that the Trinity should be understood anthropomorphically, but man is to be understood in a triune way. Or, that the Trinitarian God is not to be interpreted from the point of view of individual man, but man is interpreted on the basis of the Trinitarian God. And this interpretation is revelatory not merely psychological and human. This means that it is only when a person is within the revelation, as all the saints lived, that he can grasp this understanding completely (see "theoria"). The second presupposition is that mankind has and is composed of "nous", word and spirit like the trinitarian mode of being. Man's "nous", word and spirit are not hypostases or individual existences or realities, but activities or energies of the soul - whereas in the case with God or the Persons of the Holy Trinity, each are indeed hypostases. So these three components of each individual man are 'inseparable from one another' but they do not have a personal character" when in speaking of the being or ontology that is mankind. The "nous" as the eye of the soul, which some Fathers also call the heart, is the centre of man and is where true (spiritual) knowledge is validated. This is seen as true knowledge which is "implanted in the "nous" as always co-existing with it".

The so-called "early modern" philosophers of western Europe in the 17th and 18th centuries established arguments which led to the establishment of modern science as a methodical approach to improve the welfare of humanity by learning to control nature. As such, speculation about metaphysics, which cannot be used for anything practical, and which can never be confirmed against the reality we experience, started to be deliberately avoided, especially according to the so-called "empiricist" arguments of philosophers such as Bacon, Hobbes, Locke and Hume. The Latin motto "nihil in intellectu nisi prius fuerit in sensu" (nothing in the intellect without first being in the senses) has been described as the "guiding principle of empiricism" in the "Oxford Dictionary of Philosophy". (This was in fact an old Aristotelian doctrine, which they took up, but as discussed above Aristotelians still believed that the senses on their own were not enough to explain the mind.)

These philosophers explain the intellect as something developed from experience of sensations, being interpreted by the brain in a physical way, and nothing else, which means that absolute knowledge is impossible. For Bacon, Hobbes and Locke, who wrote in both English and Latin, "intellectus" was translated as "understanding". Far from seeing it as secure way to perceive the truth about reality, Bacon, for example, actually named the "intellectus" in his "Novum Organum", and the proœmium to his "Great Instauration", as a major source of wrong conclusions, because it is biased in many ways, for example towards over-generalizing. For this reason, modern science should be methodical, in order not to be misled by the weak human intellect. He felt that lesser known Greek philosophers such as Democritus "who did not suppose a mind or reason in the frame of things", have been arrogantly dismissed because of Aristotelianism leading to a situation in his time wherein "the search of the physical causes hath been neglected, and passed in silence". The intellect or understanding was the subject of Locke's "Essay Concerning Human Understanding".

These philosophers also tended not to emphasize the distinction between reason and intellect, describing the peculiar universal or abstract definitions of human understanding as being man-made and resulting from reason itself. Hume even questioned the distinctness or peculiarity of human understanding and reason, compared to other types of associative or imaginative thinking found in some other animals. In modern science during this time, Newton is sometimes described as more empiricist compared to Leibniz.

On the other hand, into modern times some philosophers have continued to propose that the human mind has an in-born ("a priori") ability to know the truth conclusively, and these philosophers have needed to argue that the human mind has direct and intuitive ideas about nature, and this means it can not be limited entirely to what can be known from sense perception. Amongst the early modern philosophers, some such as Descartes, Spinoza, Leibniz, and Kant, tend to be distinguished from the empiricists as rationalists, and to some extent at least some of them are called idealists, and their writings on the intellect or understanding present various doubts about empiricism, and in some cases they argued for positions which appear more similar to those of medieval and classical philosophers.

The first in this series of modern rationalists, Descartes, is credited with defining a "mind-body problem" which is a major subject of discussion for university philosophy courses. According to the presentation his , the human mind and body are different in kind, and while Descartes agrees with Hobbes for example that the human body works like a clockwork mechanism, and its workings include memory and imagination, the real human is the thinking being, a soul, which is not part of that mechanism. Descartes explicitly refused to divide this soul into its traditional parts such as intellect and reason, saying that these things were indivisible aspects of the soul. Descartes was therefore a dualist, but very much in opposition to traditional Aristotelian dualism. In his he deliberately uses traditional terms and states that his active faculty of giving ideas to his thought must be corporeal, because the things perceived are clearly external to his own thinking and corporeal, while his passive faculty must be incorporeal (unless God is deliberately deceiving us, and then in this case the active faculty would be from God). This is the opposite of the traditional explanation found for example in Alexander of Aphrodisias and discussed above, for whom the passive intellect is material, while the active intellect is not. One result is that in many Aristotelian conceptions of the "nous", for example that of Thomas Aquinas, the senses are still a source of all the intellect's conceptions. However, with the strict separation of mind and body proposed by Descartes, it becomes possible to propose that there can be thought about objects never perceived with the body's senses, such as a thousand sided geometrical figure. Gassendi objected to this distinction between the imagination and the intellect in Descartes. Hobbes also objected, and according to his own philosophical approach asserted that the "triangle in the mind comes from the triangle we have seen" and "essence in so far as it is distinguished from existence is nothing else than a union of names by means of the verb is". Descartes, in his reply to this objection insisted that this traditional distinction between essence and existence is "known to all".

His contemporary Blaise Pascal, criticised him in similar words to those used by Plato's Socrates concerning Anaxagoras, discussed above, saying that "I cannot forgive Descartes; in all his philosophy, Descartes did his best to dispense with God. But Descartes could not avoid prodding God to set the world in motion with a snap of his lordly fingers; after that, he had no more use for God."

Descartes argued that when the intellect does a job of helping people interpret what they perceive, not with the help of an intellect which enters from outside, but because each human mind comes into being with innate God-given ideas, more similar then, to Plato's theory of "anamnesis", only not requiring reincarnation. Apart from such examples as the geometrical definition of a triangle, another example is the idea of God, according to the 3rd "Meditation". Error, according to the 4th "Meditation", comes about because people make judgments about things which are not in the intellect or understanding. This is possible because the human will, being free, is not limited like the human intellect.

Spinoza, though considered a Cartesian and a rationalist, rejected Cartesian dualism and idealism. In his "pantheistic" approach, explained for example in his "Ethics", God is the same as nature, the human intellect is just the same as the human will. The divine intellect of nature is quite different from human intellect, because it is finite, but Spinoza does accept that the human intellect is a part of the infinite divine intellect.

Leibniz, in comparison to the guiding principle of the empiricists described above, added some words "nihil in intellectu nisi prius fuerit in sensu", nisi intellectus ipsi ("nothing in the intellect without first being in the senses" "except the intellect itself"). Despite being at the forefront of modern science, and modernist philosophy, in his writings he still referred to the active and passive intellect, a divine intellect, and the immortality of the active intellect.

Berkeley, partly in reaction to Locke, also attempted to reintroduce an "immaterialism" into early modern philosophy (later referred to as "subjective idealism" by others). He argued that individuals can only know sensations and ideas of objects, not abstractions such as "matter", and that ideas depend on perceiving minds for their very existence. This belief later became immortalized in the dictum, "esse est percipi" ("to be is to be perceived"). As in classical and medieval philosophy, Berkeley believed understanding had to be explained by divine intervention, and that all our ideas are put in our mind by God.

Hume accepted some of Berkeley's corrections of Locke, but in answer insisted, as had Bacon and Hobbes, that absolute knowledge is not possible, and that all attempts to show how it could be possible have logical problems. Hume's writings remain highly influential on all philosophy afterwards, and are for example considered by Kant to have shaken him from an intellectual slumber.

Kant, a turning point in modern philosophy, agreed with some classical philosophers and Leibniz that the intellect itself, although it needed sensory experience for understanding to begin, needs something else in order to make sense of the incoming sense information. In his formulation the intellect ("Verstand") has "a priori" or innate principles which it has before thinking even starts. Kant represents the starting point of German idealism and a new phase of modernity, while empiricist philosophy has also continued beyond Hume to the present day.

One of the results of the early modern philosophy has been the increasing creation of specialist fields of science, in areas that were once considered part of philosophy, and infant cognitive development and perception now tend to be discussed more within the sciences of psychology and neuroscience than in philosophy.

Modern mainstream thinking on the mind is not dualist, and sees anything innate in the mind as being a result of genetic and developmental factors which allow the mind to develop. Overall it accepts far less innate "knowledge" (or clear pre-dispositions to particular types of knowledge) than most of the classical and medieval theories derived from philosophers such as Plato, Aristotle, Plotinus and Al Farabi.

Apart from discussions about the history of philosophical discussion on this subject, contemporary philosophical discussion concerning this point has continued concerning what the ethical implications are of the different alternatives still considered likely.

Classical conceptions of nous are still discussed seriously in theology. There is also still discussion of classical nous in non-mainstream metaphysics or spiritualism, such as Noetics, promoted for example by the Institute of Noetic Sciences.



</doc>
<doc id="32116915" url="https://en.wikipedia.org/wiki?curid=32116915" title="Faith literate">
Faith literate

Faith literate describes the ability of an individual to become knowledgeable of other religions and faith other than the one a person believes in.

A faith literate individual understands the key effects of each religion/belief system by means of the values, attitudes and influence it causes in individuals, families and communities. Faith literates believe in recognizing religious and secular worldviews in practice and thoughts and take hold of what makes each religion/belief system what it is. IT involves understanding and knowing the fundamental formative attribute of all religions.

In the United Kingdom, there are institutes and consultancies being set up that offer religious understanding training for the public and private sectors. Even the government is also committed to a program of faith literacy in the public sector. This is aimed to be significantly enhance organizational multiplicity among other things. Faith literacy is also intended to facilitate a move beyond the functional levels of conversation.

Tony Blair, former British Prime Minister, also mentioned in an interview that he reads Quran and Bible every day since it is crucial to be faith literate in a globalised world like ours. In Uganda, the Bishop of Kigezi also urged the government patrons to be more ‘faith literate’. Realizing the importance of this concept, the Economic and Social Research Council in UK started a three-year research paper in faith literacy.



</doc>
<doc id="3002191" url="https://en.wikipedia.org/wiki?curid=3002191" title="Network of practice">
Network of practice

Network of practice (often abbreviated as NoP) is a concept originated by John Seely Brown and Paul Duguid. This concept, related to the work on communities of practice by Jean Lave and Etienne Wenger, refers to the overall set of various types of informal, emergent social networks that facilitate information exchange between individuals with practice-related goals. In other words, networks of practice range from communities of practice where learning occurs to electronic networks of practice (often referred to as virtual or electronic communities).

To further define the concept, first the term network implies a set of individuals who are connected through social relationships, whether they be strong or weak. Terms such as community tend to denote a stronger form of relationship, but networks refer to all networks of social relationships, be they weak or strong. Second, the term practice represents the substrate that connects individuals in their networks. The principal ideas are that practice implies the actions of individuals and groups when conducting their work, e.g., the practice of software engineers, journalists, educators, etc., and that practice involves interaction among individuals.

What distinguishes a network of practice from other networks is that the primary reason for the emergence of relationships within a network of practice is that individuals interact through information exchange in order to perform their work, asking for and sharing knowledge with each other. A network of practice can be distinguished from other networks that emerge due to other factors, such as interests in common hobbies or discussing sports while taking the same bus to work, etc. Finally, practice need not necessarily be restricted to include those within one occupation or functional discipline. Rather it may include individuals from a variety of occupations; thus, the term, practice, is more appropriate than others such as occupation.

As indicated above, networks of practice incorporate a range of informal, emergent networks, from communities of practice to electronic networks of practice. In line with Lave & Wenger's original work (1991), Brown & Duguid propose that communities of practice are a localized and specialized subset of networks of practice, typically consisting of strong ties linking individuals engaged in a shared practice who typically interact in face-to-face situations. At the opposite end of the spectrum are electronic networks of practice, which are often referred to as virtual or electronic communities and consisting of weak ties. In electronic networks of practice, individuals may never get to know one another or meet face-to-face, and they generally coordinate through means such as blogs, electronic mailing lists, or bulletin boards.

In contrast to the use of formal controls to support knowledge exchange often used in formal work groups, such as contractual obligation, organizational hierarchies, monetary incentives, or mandated rules, networks of practice promote knowledge flows along lines of practice through informal social networks. Therefore, one way to distinguish between networks of practice and work groups created through formal organizational mandate is by the nature of the "control mechanisms".

A second group of distinguishing properties refers to their "composition". Networks of practice and formal work groups vary in terms of their "size" since networks of practice may range from a few select individuals to very large, open electronic networks consisting of thousands of participants while groups are generally smaller. They also vary in terms of "who can participate". Work groups and virtual teams typically consist of members who are formally designated and assigned. In contrast, networks of practice consist of volunteers without formal restrictions placed on membership.

Finally, networks of practice and formal work groups vary in terms of "expectations about participation". In formal work groups and virtual teams, participation is jointly determined and members are expected to achieve a specific work goal. Participation in communities of practice is jointly determined, such that individuals generally approach specific others for help. In electronic networks of practice, participation is individually determined; knowledge seekers have no control over who responds to their questions or the quality of the responses. In turn, knowledge contributors have no assurances that seekers will understand the answer provided or be willing to reciprocate the favor.




</doc>
<doc id="3947034" url="https://en.wikipedia.org/wiki?curid=3947034" title="General knowledge">
General knowledge

General knowledge is information that has been accumulated over time through various mediums. It excludes specialized learning that can only be obtained with extensive training and information confined to a single medium. General knowledge is an essential component of crystallized intelligence. It is strongly associated with general intelligence and with openness to experience.

Studies have found that people who are highly knowledgeable in a particular domain tend to be knowledgeable in many. General knowledge is thought to be supported by long-term semantic memory ability. General knowledge also supports schemata for textual understanding. In 2019, it was found in a survey that New Zealanders had "concerning" gaps in their general knowledge.

High scorers on tests of general knowledge tend to also score highly on intelligence tests. IQ has been found to robustly predict general knowledge scores even after accounting for differences in age, sex, and five factor model personality traits. However, many general knowledge tests are designed to create a normal distribution of answers, creating a bell shaped curve.

General knowledge is also moderately associated with verbal ability, though only weakly or not at all with numerical and spatial ability. As with crystallized intelligence, general knowledge has been found to increase with age.

General knowledge is stored as semantic memory. Most semantic memory is preserved through old age, though there are deficits in retrieval of certain specific words correlated with aging. In addition, stress or various emotional levels can negatively affect semantic memory retrieval.

People high in general knowledge tend to be highly open to new experiences and in typical intellectual engagement. The relationship between openness to experience and general knowledge remains robust even when IQ is taken into account. People high in openness may be more motivated to engage in intellectual pursuits that increase their knowledge. Relationships between general knowledge and other five factor model traits tend to be weak and inconsistent. Though one study found that extraversion and neuroticism were negatively correlated with general knowledge, others found that they were unrelated. Inconsistent results have also been found for conscientiousness.

A number of studies have assessed whether performance on a general knowledge test can predict achievement in particular areas, namely in academics, proofreading, and creativity.

General knowledge has been found to predict exam results in a study of British schoolchildren. The study examined cognitive ability and personality predictors of exam performance and found that general knowledge was positively correlated with GCSE English, mathematics, and overall exam results. General knowledge test scores predicted exam results, even after controlling for IQ, five factor model personality traits, and learning styles.

General knowledge has been found to robustly predict proofreading skills in university students. A study found that proofreading had a larger correlation with general knowledge than with general intelligence, verbal reasoning, or openness to experience. In a multiple regression analysis using general knowledge, general intelligence, verbal reasoning, five factor personality traits, and learning styles as predictors, only general knowledge was a significant predictor.

General knowledge has been found to have weak associations with measures of creativity. In a study examining contributions of personality and intelligence to creativity, general knowledge was positively correlated with tests of divergent thinking, but was unrelated to a biographical measure of creative achievement, self-rated creativity, or a composite measure of creativity. The relationship between general knowledge and divergent thinking became non-significant when controlling for fluid intelligence.

Many game shows use general knowledge questions. Game shows such as "Who Wants to Be a Millionaire?" and "Fifteen to One" centre their questions on general knowledge, while others shows focus questions more on specific subjects. Some shows ask questions both on specific subjects and on general knowledge, including "Eggheads" and ""Mastermind"". In "Mastermind", contestants choose their own "specialist subject" before answering general knowledge questions, whereas in "Eggheads" the subjects are chosen at random.


</doc>
<doc id="1409006" url="https://en.wikipedia.org/wiki?curid=1409006" title="Common knowledge (logic)">
Common knowledge (logic)

Common knowledge is a special kind of knowledge for a group of agents. There is "common knowledge" of "p" in a group of agents "G" when all the agents in "G" know "p", they all know that they know "p", they all know that they all know that they know "p", and so on "ad infinitum".

The concept was first introduced in the philosophical literature by David Kellogg Lewis in his study "Convention" (1969). The sociologist Morris Friedell defined common knowledge in a 1969 paper. It was first given a mathematical formulation in a set-theoretical framework by Robert Aumann (1976). Computer scientists grew an interest in the subject of epistemic logic in general – and of common knowledge in particular – starting in the 1980s. There are numerous puzzles based upon the concept which have been extensively investigated by mathematicians such as John Conway.

The philosopher Stephen Schiffer, in his 1972 book "Meaning", independently developed a notion he called "mutual knowledge" which functions quite similarly to Lewis's and Friedel's 1969 "common knowledge".

The idea of common knowledge is often introduced by some variant of induction puzzles:

On an island, there are "k" people who have blue eyes, and the rest of the people have green eyes. At the start of the puzzle, no one on the island ever knows their own eye color. By rule, if a person on the island ever discovers they have blue eyes, that person must leave the island at dawn; anyone not making such a discovery always sleeps until after dawn. On the island, each person knows every other person's eye color, there are no reflective surfaces, and there is no communication of eye color.

At some point, an outsider comes to the island, calls together all the people on the island, and makes the following public announcement: "At least one of you has blue eyes". The outsider, furthermore, is known by all to be truthful, and all know that all know this, and so on: it is common knowledge that he is truthful, and thus it becomes common knowledge that there is at least one islander who has blue eyes. The problem: assuming all persons on the island are completely logical and that this too is common knowledge, what is the eventual outcome?

The answer is that, on the "k"th dawn after the announcement, all the blue-eyed people will leave the island.

The solution can be seen with an inductive argument. If "k" = 1 (that is, there is exactly one blue-eyed person), the person will recognize that he alone has blue eyes (by seeing only green eyes in the others) and leave at the first dawn. If "k" = 2, no one will leave at the first dawn. The two blue-eyed people, seeing only one person with blue eyes, "and" that no one left on the 1st dawn (and thus that "k" > 1), will leave on the second dawn. Inductively, it can be reasoned that no one will leave at the first "k" − 1 dawns if and only if there are at least "k" blue-eyed people. Those with blue eyes, seeing "k" − 1 blue-eyed people among the others and knowing there must be at least "k", will reason that they must have blue eyes and leave.

What's most interesting about this scenario is that, for "k" > 1, the outsider is only telling the island citizens what they already know: that there are blue-eyed people among them. However, before this fact is announced, the fact is not "common knowledge".

For "k" = 2, it is merely "first-order" knowledge. Each blue-eyed person knows that there is someone with blue eyes, but each blue eyed person does "not" know that the other blue-eyed person has this same knowledge.

For "k" = 3, it is "second order" knowledge. Each blue-eyed person knows that a second blue-eyed person knows that a third person has blue eyes, but no one knows that there is a "third" blue-eyed person with that knowledge, until the outsider makes his statement.

In general: For "k" > 1, it is "("k" − 1)th order" knowledge. Each blue-eyed person knows that a second blue-eyed person knows that a third blue-eyed person knows that... (repeat for a total of "k" − 1 levels) a "k"th person has blue eyes, but no one knows that there is a ""k"th" blue-eyed person with that knowledge, until the outsider makes his statement. The notion of "common knowledge" therefore has a palpable effect. Knowing that everyone knows does make a difference. When the outsider's public announcement (a fact already known to all) becomes common knowledge, the blue-eyed people on this island eventually deduce their status, and leave.

Common knowledge can be given a logical definition in multi-modal logic systems in which the modal operators are interpreted epistemically. At the propositional level, such systems are extensions of propositional logic. The extension consists of the introduction of a group "G" of "agents", and of "n" modal operators "K" (with "i" = 1, ..., "n") with the intended meaning that "agent "i" knows." Thus "K formula_1" (where formula_1 is a formula of the calculus) is read "agent "i" knows formula_1." We can define an operator "E" with the intended meaning of "everyone in group "G" knows" by defining it with the axiom

By abbreviating the expression formula_5 with formula_6 and defining formula_7, we could then define common knowledge with the axiom

There is however a complication. The languages of epistemic logic are usually "finitary", whereas the axiom above defines common knowledge as an infinite conjunction of formulas, hence not a well-formed formula of the language. To overcome this difficulty, a "fixed-point" definition of common knowledge can be given. Intuitively, common knowledge is thought of as the fixed point of the "equation" formula_9. In this way, it is possible to find a formula formula_10 implying formula_11 from which, in the limit, we can infer common knowledge of formula_1.

This "syntactic" characterization is given semantic content through so-called "Kripke structures". A Kripke structure is given by (i) a set of states (or possible worlds) "S", (ii) "n" "accessibility relations" formula_13, defined on formula_14, intuitively representing what states agent "i" considers possible from any given state, and (iii) a valuation function formula_15 assigning a truth value, in each state, to each primitive proposition in the language. The semantics for the knowledge operator is given by stipulating that formula_16 is true at state "s" iff formula_1 is true at "all" states "t" such that formula_18. The semantics for the common knowledge operator, then, is given by taking, for each group of agents "G", the reflexive and transitive closure of the formula_19, for all agents "i" in "G", call such a relation formula_20, and stipulating that formula_21 is true at state "s" iff formula_1 is true at "all" states "t" such that formula_23.

Alternatively (yet equivalently) common knowledge can be formalized using set theory (this was the path taken by the Nobel laureate Robert Aumann in his seminal 1976 paper). We will start with a set of states "S". We can then define an event "E" as a subset of the set of states "S". For each agent "i", define a partition on "S", "P". This partition represents the state of knowledge of an agent in a state. In state "s", agent "i" knows that one of the states in "P"("s") obtains, but not which one. (Here "P"("s") denotes the unique element of "P" containing "s". Note that this model excludes cases in which agents know things that are not true.)

We can now define a knowledge function "K" in the following way:

That is, "K"("e") is the set of states where the agent will know that event "e" obtains. It is a subset of "e".

Similar to the modal logic formulation above, we can define an operator for the idea that "everyone knows "e"".

As with the modal operator, we will iterate the "E" function, formula_26 and formula_27. Using this we can then define a common knowledge function,

The equivalence with the syntactic approach sketched above can easily be seen: consider an Aumann structure as the one just defined. We can define a correspondent Kripke structure by taking (i) the same space "S", (ii) accessibility relations formula_19 that define the equivalence classes corresponding to the partitions formula_30, and (iii) a valuation function such that it yields value "true" to the primitive proposition "p" in all and only the states "s" such that formula_31, where formula_32 is the event of the Aumann structure corresponding to the primitive proposition "p". It is not difficult to see that the common knowledge accessibility function formula_20 defined in the previous section corresponds to the finest common coarsening of the partitions formula_30 for all formula_35, which is the finitary characterization of common knowledge also given by Aumann in the 1976 article.

Common knowledge was used by David Lewis in his pioneering game-theoretical account of convention. In this sense, common knowledge is a concept still central for linguists and philosophers of language (see Clark 1996) maintaining a Lewisian, conventionalist account of language.

Robert Aumann introduced a set theoretical formulation of common knowledge (theoretically equivalent to the one given above) and proved the so-called agreement theorem through which: if two agents have common prior probability over a certain event, and the posterior probabilities are common knowledge, then such posterior probabilities are equal. A result based on the agreement theorem and proven by Milgrom shows that, given certain conditions on market efficiency and information, speculative trade is impossible.

The concept of common knowledge is central in game theory. For several years it has been thought that the assumption of common knowledge of rationality for the players in the game was fundamental. It turns out (Aumann and Brandenburger 1995) that, in 2-player games, common knowledge of rationality is not needed as an epistemic condition for Nash equilibrium strategies.

Computer scientists use languages incorporating epistemic logics (and common knowledge) to reason about distributed systems. Such systems can be based on logics more complicated than simple propositional epistemic logic, see Wooldridge "Reasoning about Artificial Agents", 2000 (in which he uses a first-order logic incorporating epistemic and temporal operators) or van der Hoek et al. "Alternating Time Epistemic Logic".

In his 2007 book, "The Stuff of Thought: Language as a Window into Human Nature," Steven Pinker uses the notion of common knowledge to analyze the kind of indirect speech involved in innuendoes.






</doc>
<doc id="10969154" url="https://en.wikipedia.org/wiki?curid=10969154" title="Bildung">
Bildung

Bildung (, ""education, formation, etc."") refers to the German tradition of self-cultivation (as related to the German for: creation, image, shape), wherein philosophy and education are linked in a manner that refers to a process of both personal and cultural maturation. This maturation is described as a harmonization of the individual's mind and heart and in a unification of selfhood and identity within the broader society, as evidenced with the literary tradition of "Bildungsroman".

In this sense, the process of harmonization of mind, heart, selfhood and identity is achieved through personal transformation, which presents a challenge to the individual's accepted beliefs. In Hegel's writings, the challenge of personal growth often involves an agonizing alienation from one's "natural consciousness" that leads to a reunification and development of the self. Similarly, although social unity requires well-formed institutions, it also requires a diversity of individuals with the freedom (in the positive sense of the term) to develop a wide-variety of talents and abilities and this requires personal agency. However, rather than an end state, both individual and social unification is a process that is driven by unrelenting negations.

In this sense, education involves the shaping of the human being with regard to their own humanity as well as their innate intellectual skills. So, the term refers to a process of becoming that can be related to a process of becoming within Existentialism.

The term "Bildung" also corresponds to the Humboldtian model of higher education from the work of Prussian philosopher and educational administrator Wilhelm von Humboldt (1767–1835). Thus, in this context, the concept of education becomes a lifelong process of human development, rather than mere training in gaining certain external knowledge or skills. Such training in skills is known by the German words "Erziehung", and "Ausbildung". "Bildung" in contrast is seen as a process wherein an individual's spiritual and cultural sensibilities as well as life, personal and social skills are in process of continual expansion and growth. Bildung is seen as a way to become more free due to higher self-reflection. Von Humboldt wrote with respect to "Bildung" in 1793/1794: "Education ["Bildung"], truth and virtue" must be disseminated to such an extent that the "concept of mankind" takes on a great and dignified form in each individual (GS, I, p. 284). However, this shall be achieved personally by each individual, who must "absorb the great mass of material offered to him by the world around him and by his inner existence, using all the possibilities of his receptiveness; he must then reshape that material with all the energies of his own activity and appropriate it to himself so as to create an interaction between his own personality and nature in a most general, active and harmonious form".Most explicitly in Hegel's writings, the "Bildung" tradition rejects the pre-Kantian metaphysics of being for a post-Kantian metaphysics of experience that rejects universal narratives. Much of Hegel's writings were about the nature of education (both "Bildung" and "Erziehung"), reflecting his own role as a teacher and administrator in German secondary schools, and in his more general writings. 

A more contemporary view was developed by Tony Waters: "Bildung", I discovered in my 2 years in Germany, is an organizing cultural principle for German higher education that trumps both careerism and disciplinary silos. It is generally translated as "education", but in fact it means more—dictionary definitions often refer to "self-cultivation", "philosophy", "personal and cultural maturation" and even "existentialism". Bildung is the cry of the land of poets and thinkers against the demands of credentialism, professionalism, careerism and the financial temptations dangled to graduating students.In this way, fulfillment is achieved through practical activity that promotes the development of one's own individual talents and abilities which in turn lead to the development of one's society. In this way, Bildung does not simply accept the socio-political status quo, but rather it includes the ability to engage in a critique of one's society, and to ultimately challenge the society to actualize its own highest ideals.

In their book The Nordic Secret -- A European Story of Beauty and Freedom, Lene Rachel Andersen and Tomas Björkman define Bildung like this:"Bildung" is the way that the individual matures and takes upon him- or herself ever bigger personal responsibility towards family, friends, fellow citizens, society, humanity, our globe, and the global heritage of our species, while enjoying ever bigger personal, moral and existential freedoms. It is the enculturation and life-long learning that forces us to grow and change, it is existential and emotional depth, it is life-long interaction and struggles with new knowledge, culture, art, science, new perspectives, new people, and new truths, and it is being an active citizen in adulthood. Bildung is a constant process that never ends.The European Bildung Network define it like this:"Bildung" is the combination of the education and knowledge necessary to thrive in your society, and the moral and emotional maturity to both be a team player and have personal autonomy. Bildung is also knowing your roots and being able to imagine the future.The philosopher Jonathan Rowson has argued for the relevance of Bildung in attaining sustainable prosperity in the 21st century:
Our understanding of the world is not a spectator sport, but more like an active ingredient in societal renewal. Bildung is about our responsibility for and participation in an evolving process of social maturation that reimagines culture, technology, institutions and policies for the greater good. 




</doc>
<doc id="33788375" url="https://en.wikipedia.org/wiki?curid=33788375" title="Mutual knowledge (logic)">
Mutual knowledge (logic)

Mutual knowledge is a fundamental concept about information in game theory, (epistemic) logic, and epistemology. An event is mutual knowledge if all agents know that the event occurred. However, mutual knowledge by itself implies nothing about what agents know about other agents' knowledge: i.e. it is possible that an event is mutual knowledge but that each agent is unaware that the other agents know it has occurred. Common knowledge is a related but stronger notion; any event that is common knowledge is also mutual knowledge.

The philosopher Stephen Schiffer, in his book "Meaning", developed a notion he called "mutual knowledge" which functions quite similarly to David K. Lewis's "common knowledge".



</doc>
<doc id="34023953" url="https://en.wikipedia.org/wiki?curid=34023953" title="Perspicacity">
Perspicacity

Perspicacity (also called perspicaciousness) is a penetrating discernment (from the Latin perspicācitās, meaning throughsightedness, discrimination)—a clarity of vision or intellect which provides a deep understanding and insight. It takes the concept of wisdom deeper in the sense that it denotes a keenness of sense and intelligence applied to insight. 

Another definition refers to it as the "ability to recognize subtle differences between similar objects or ideas". It has also been described as a deeper level of internalization. Perspicacity is different from "acuity", which also describes a keen insight, since it does not include physical abilities such as sight or hearing. 

In 17th-century Europe, René Descartes devised systematic rules for clear thinking in his work "Regulæ ad directionem ingenii" (Rules for the direction of natural intelligence). In Descartes' scheme, intelligence consisted of two faculties: perspicacity, which provided an understanding or intuition of distinct detail; and sagacity, which enabled reasoning about the details in order to make deductions. Rule 9 was "De Perspicacitate Intuitionis" (On the Perspicacity of Intuition). He summarised the rule as 

In his study of the elements of wisdom, the modern psychometrician Robert Sternberg identified perspicacity as one of its six components or dimensions; the other five being reasoning, sagacity, learning, judgement and the expeditious use of information. In his analysis, the perspicacious individual is someone who 

In an article dated October 7, 1966, the journal "Science" discussed NASA scientist-astronaut program recruitment efforts:
Being perspicacious about other people, rather than having false illusions, is a sign of good mental health. The quality is needed in psychotherapists who engage in person-to-person dialogue and counselling of the mentally ill.

The artist René Magritte illustrated the quality in his 1936 painting "Perspicacity". The picture shows an artist at work who studies his subject intently: it is an egg. But the painting which he is creating is not of an egg; it is an adult bird in flight.

Perspicacity is also used to indicate practical wisdom in the areas of politics and finance. 



</doc>
<doc id="221284" url="https://en.wikipedia.org/wiki?curid=221284" title="Dispersed knowledge">
Dispersed knowledge

Dispersed knowledge in economics is the notion that no single agent has information as to all of the factors which influence prices and production throughout the system.

Each agent in a market for assets, goods, or services possesses incomplete knowledge as to most of the factors which affect prices in that market. For example, no agent has full information as to other agents' budgets, preferences, resources or technologies, not to mention their plans for the future and numerous other factors which affect prices in those markets.

Market prices are the result of price discovery, in which each agent participating in the market makes use of its current knowledge and plans to decide on the prices and quantities at which it chooses to transact. The resulting prices and quantities of transactions may be said to reflect the current state of knowledge of the agents currently in the market, even though no single agent commands information as to the entire set of such knowledge.

Some economists believe that market transactions provide the basis for a society to benefit from the knowledge that is dispersed among its constituent agents. For example, in his "Principles of Political Economy", John Stuart Mill states that one of the justifications for a laissez faire government policy is his belief that self-interested individuals throughout the economy, acting independently, can make better use of dispersed knowledge than could the best possible government agency.
Friedrich Hayek claimed that "dispersed knowledge is "essentially" dispersed, and cannot possibly be gathered together and conveyed to an authority charged with the task of deliberately creating order".



Dispersed knowledge will give rise to uncertainty which will lead to different kinds of results.
Richard LeFauve highlights the advantages of organizational structure in companies:

"Before if we had a tough decision to make, we would have two or three different perspectives with strong support of all three. In a traditional organization the bossman decides after he’s heard all three alternatives. At Saturn we take time to work it out, and what generally happens is that you end up with a fourth answer which none of the portions had in the first place. but one that all three portions of the organization fully support (AutoWeeR, Oct. 8, 1990. p. 20)."

Companies are supposed to think highly of the dispersed knowledge and make adjustments to meet demands.
Tsoukas stated:

"A firm’s knowledge is distributed, not only in a computational sense . . . or in Hayek’s (1945, p. 521) sense that the factual knowledge of the particular circumstances of time and place cannot be surveyed as a whole. But, more radically, a firm’s knowledge is distributed in the sense that it is inherently indeterminate: nobody knows in advance what that knowledge is or need be. Firms are faced with radical uncertainty: they do not, they cannot, know what they need to know."

There are several strategies targeting at the problems caused by dispersed knowledge.

First of all, replacing knowledge by getting access to knowledge can be one of the strategies.

What's more, the capability to complete incomplete knowledge can deal with knowledge gaps created by the dispersed knowledge.

In addition, making a design of institutions with reasonable coordination mechanisms can be regarded as the third strategy.

Besides, resolving organization units into smaller ones should be taken into consideration.

Last but not least, providing more data to decision maker will be helpful for making a correct decision.



</doc>
<doc id="34215300" url="https://en.wikipedia.org/wiki?curid=34215300" title="Success trap">
Success trap

The success trap refers to business organizations that focus on the exploitation of their (historically successful) current business activities and as such neglect the need to explore new territory and enhance their long-term viability.

The success trap arises when a firm overemphasizes exploitation investments, even if explorative investments are required for successful adaptation. Exploitation draws on processes that serve to incrementally improve existing knowledge, while exploration involves the pursuit and acquisition of new knowledge. Firms and other organizations that have been performing well over an extended period of time are exposed to strong path dependence in exploitative activities, at the cost of explorative activities with which they have little experience. For example, in the 1990s Polaroid’s management failed to respond to the transition from analogue to digital photography, although the rise of digital technology had been evident since the 1980s. Other well-known examples of companies that got caught in the success trap include Kodak, Rubbermaid and Caterpillar.

A key condition giving rise to a firm getting caught in the success trap is the company culture, having been created based on the understanding of what makes success, the culture then solidifies. When the environment changes there is an initial dismissing of the significance of the change and the (over time) subsequent failure to adjust the strategy of the firm. Thus, top managers do not ‘see’ the upcoming exogenous change, because their thinking and policies tend to constrain exploration and experimentation within the firm and inhibit the ability to bring about strategic change. A broader perspective arises from how exploration activities are suppressed in publicly owned companies as a result of the interplay between the CEO and other top executives, the Board of Directors, the pressure for short-term (improvements in) results arising from the capital market, and the substantial delay between the investment in exploration efforts and the return on these efforts.

The success trap can be best avoided early on, for example, by closely monitoring how other (e.g. leading) firms maintain a balance between exploitation and exploration activities, as well as by continually collecting information about changing customer needs, newly emerging technologies and other changes in the market and competitive environment. Drawing on this type of information, the executive board and board of directors together need to develop and sustain a shared long-term vision and strategy regarding the investments in exploitation and exploration activities. Once a publicly owned corporation has been suppressing exploration over an extended period of time, it tends to be almost impossible to get out of the success trap without major interventions - such as a hostile takeover by another corporation or an exit from the stock exchange.

Firms that fall into the success trap suffer long term consequences. They grow their revenues at a lower pace than other companies and also create less shareholder value than more exploratory companies. These patterns can be observed for S&P 500 companies in the USA in the aggregate and also within industries.



</doc>
<doc id="10044813" url="https://en.wikipedia.org/wiki?curid=10044813" title="Noogony">
Noogony

Noogony is a general term for any theory of knowledge that attempts to explain the origin of concepts in the human mind by considering sense or "a posteriori" data as solely relevant.

The word was used, famously, by Kant in his "Critique of Pure Reason" to refer to what he understood to be Locke's account of the origin of concepts. While Kant himself maintained that some concepts, e.g. cause and effect, did not "arise" from experience, he took Locke to be suggesting that "all" concepts came from experience.

Historically, Kant presents a caricature of Locke's position, not a completely accurate account of Locke's epistemology. Locke's actual theory of knowledge was more subtle than Kant seems to render it in his "Critique". As Guyer/Wood note in their edition of the "Critique":Presumably Kant here has in mind Locke's claim that sensation and reflection are the two sources of all our ideas, and is understanding Locke's reflection to be reflection on sensation only. This would be a misunderstanding of Locke, since Locke says that we get simple ideas from reflection on the "operations of our own Mind," a doctrine which is actually a precursor to Kant's view that the laws of our own intuition and thinking furnish the forms of knowledge to be added to the empirical contents furnished by sensation, although of course Locke did not go very far in developing this doctrine; in particular, he did not see that mathematics and logic could be used as sources of information about the operations of the mind.



</doc>
<doc id="15201" url="https://en.wikipedia.org/wiki?curid=15201" title="Interdisciplinarity">
Interdisciplinarity

Interdisciplinarity or interdisciplinary studies involves the combining of two or more academic disciplines into one activity (e.g., a research project). It draws knowledge from several other fields like sociology, anthropology, psychology, economics etc. It is about creating something by thinking across boundaries. It is related to an "interdiscipline" or an "interdisciplinary field," which is an organizational unit that crosses traditional boundaries between academic disciplines or schools of thought, as new needs and professions emerge. Large engineering teams are usually interdisciplinary, as a power station or mobile phone or other project requires the melding of several specialties. However, the term "interdisciplinary" is sometimes confined to academic settings.

The term "interdisciplinary" is applied within education and training pedagogies to describe studies that use methods and insights of several established disciplines or traditional fields of study. Interdisciplinarity involves researchers, students, and teachers in the goals of connecting and integrating several academic schools of thought, professions, or technologies—along with their specific perspectives—in the pursuit of a common task. The epidemiology of HIV/AIDS or global warming requires understanding of diverse disciplines to solve complex problems. "Interdisciplinary" may be applied where the subject is felt to have been neglected or even misrepresented in the traditional disciplinary structure of research institutions, for example, women's studies or ethnic area studies. Interdisciplinarity can likewise be applied to complex subjects that can only be understood by combining the perspectives of two or more fields.

The adjective "interdisciplinary" is most often used in educational circles when researchers from two or more disciplines pool their approaches and modify them so that they are better suited to the problem at hand, including the case of the team-taught course where students are required to understand a given subject in terms of multiple traditional disciplines. For example, the subject of land use may appear differently when examined by different disciplines, for instance, biology, chemistry, economics, geography, and politics.

Although "interdisciplinary" and "interdisciplinarity" are frequently viewed as twentieth century terms, the concept has historical antecedents, most notably Greek philosophy. Julie Thompson Klein attests that "the roots of the concepts lie in a number of ideas that resonate through modern discourse—the ideas of a unified science, general knowledge, synthesis and the integration of knowledge", while Giles Gunn says that Greek historians and dramatists took elements from other realms of knowledge (such as medicine or philosophy) to further understand their own material. The building of Roman roads required men who understood surveying, material science, logistics and several other disciplines. Any broadminded humanist project involves interdisciplinarity, and history shows a crowd of cases, as seventeenth-century Leibniz's task to create a system of universal justice, which required linguistics, economics, management, ethics, law philosophy, politics, and even sinology.

Interdisciplinary programs sometimes arise from a shared conviction that the traditional disciplines are unable or unwilling to address an important problem. For example, social science disciplines such as anthropology and sociology paid little attention to the social analysis of technology throughout most of the twentieth century. As a result, many social scientists with interests in technology have joined science, technology and society programs, which are typically staffed by scholars drawn from numerous disciplines. They may also arise from new research developments, such as nanotechnology, which cannot be addressed without combining the approaches of two or more disciplines. Examples include quantum information processing, an amalgamation of quantum physics and computer science, and bioinformatics, combining molecular biology with computer science. Sustainable development as a research area deals with problems requiring analysis and synthesis across economic, social and environmental spheres; often an integration of multiple social and natural science disciplines. Interdisciplinary research is also key to the study of health sciences, for example in studying optimal solutions to diseases. Some institutions of higher education offer accredited degree programs in Interdisciplinary Studies.

At another level, interdisciplinarity is seen as a remedy to the harmful effects of excessive specialization and isolation in information silos. On some views, however, interdisciplinarity is entirely indebted to those who specialize in one field of study—that is, without specialists, interdisciplinarians would have no information and no leading experts to consult. Others place the focus of interdisciplinarity on the need to transcend disciplines, viewing excessive specialization as problematic both epistemologically and politically. When interdisciplinary collaboration or research results in new solutions to problems, much information is given back to the various disciplines involved. Therefore, both disciplinarians and interdisciplinarians may be seen in complementary relation to one another.

Because most participants in interdisciplinary ventures were trained in traditional disciplines, they must learn to appreciate differing of perspectives and methods. For example, a discipline that places more emphasis on quantitative rigor may produce practitioners who are more scientific in their training than others; in turn, colleagues in "softer" disciplines who may associate quantitative approaches with difficulty grasp the broader dimensions of a problem and lower rigor in theoretical and qualitative argumentation. An interdisciplinary program may not succeed if its members remain stuck in their disciplines (and in disciplinary attitudes). Those who lack experience in interdisciplinary collaborations may also not fully appreciate the intellectual contribution of colleagues from those discipline. From the disciplinary perspective, however, much interdisciplinary work may be seen as "soft", lacking in rigor, or ideologically motivated; these beliefs place barriers in the career paths of those who choose interdisciplinary work. For example, interdisciplinary grant applications are often refereed by peer reviewers drawn from established disciplines; not surprisingly, interdisciplinary researchers may experience difficulty getting funding for their research. In addition, untenured researchers know that, when they seek promotion and tenure, it is likely that some of the evaluators will lack commitment to interdisciplinarity. They may fear that making a commitment to interdisciplinary research will increase the risk of being denied tenure.

Interdisciplinary programs may also fail if they are not given sufficient autonomy. For example, interdisciplinary faculty are usually recruited to a joint appointment, with responsibilities in both an interdisciplinary program (such as women's studies) and a traditional discipline (such as history). If the traditional discipline makes the tenure decisions, new interdisciplinary faculty will be hesitant to commit themselves fully to interdisciplinary work. Other barriers include the generally disciplinary orientation of most scholarly journals, leading to the perception, if not the fact, that interdisciplinary research is hard to publish. In addition, since traditional budgetary practices at most universities channel resources through the disciplines, it becomes difficult to account for a given scholar or teacher's salary and time. During periods of budgetary contraction, the natural tendency to serve the primary constituency (i.e., students majoring in the traditional discipline) makes resources scarce for teaching and research comparatively far from the center of the discipline as traditionally understood. For these same reasons, the introduction of new interdisciplinary programs is often resisted because it is perceived as a competition for diminishing funds.

Due to these and other barriers, interdisciplinary research areas are strongly motivated to become disciplines themselves. If they succeed, they can establish their own research funding programs and make their own tenure and promotion decisions. In so doing, they lower the risk of entry. Examples of former interdisciplinary research areas that have become disciplines, many of them named for their parent disciplines, include neuroscience, cybernetics, biochemistry and biomedical engineering. These new fields are occasionally referred to as "interdisciplines". On the other hand, even though interdisciplinary activities are now a focus of attention for institutions promoting learning and teaching, as well as organizational and social entities concerned with education, they are practically facing complex barriers, serious challenges and criticism. The most important obstacles and challenges faced by interdisciplinary activities in the past two decades can be divided into "professional", "organizational", and "cultural" obstacles.

An initial distinction should be made between interdisciplinary studies, which can be found spread across the academy today, and the study of interdisciplinarity, which involves a much smaller group of researchers. The former is instantiated in thousands of research centers across the US and the world. The latter has one US organization, the Association for Interdisciplinary Studies (founded in 1979), two international organizations, the International Network of Inter- and Transdisciplinarity (founded in 2010) and the Philosophy of/as Interdisciplinarity Network (founded in 2009), and one research institute devoted to the theory and practice of interdisciplinarity, the Center for the Study of Interdisciplinarity at the University of North Texas (founded in 2008). As of 1 September 2014, the Center for the Study of Interdisciplinarity has ceased to exist. This is the result of administrative decisions at the University of North Texas.

An interdisciplinary study is an academic program or process seeking to synthesize broad perspectives, knowledge, skills, interconnections, and epistemology in an educational setting. Interdisciplinary programs may be founded in order to facilitate the study of subjects which have some coherence, but which cannot be adequately understood from a single disciplinary perspective (for example, women's studies or medieval studies). More rarely, and at a more advanced level, interdisciplinarity may itself become the focus of study, in a critique of institutionalized disciplines' ways of segmenting knowledge.

In contrast, studies of interdisciplinarity raise to self-consciousness questions about how interdisciplinarity works, the nature and history of disciplinarity, and the future of knowledge in post-industrial society. Researchers at the Center for the Study of Interdisciplinarity have made the distinction between philosophy 'of' and 'as' interdisciplinarity, the former identifying a new, discrete area within philosophy that raises epistemological and metaphysical questions about the status of interdisciplinary thinking, with the latter pointing toward a philosophical practice that is sometimes called 'field philosophy'.

Perhaps the most common complaint regarding interdisciplinary programs, by supporters and detractors alike, is the lack of synthesis—that is, students are provided with multiple disciplinary perspectives, but are not given effective guidance in resolving the conflicts and achieving a coherent view of the subject. Others have argued that the very idea of synthesis or integration of disciplines presupposes questionable politico-epistemic commitments. Critics of interdisciplinary programs feel that the ambition is simply unrealistic, given the knowledge and intellectual maturity of all but the exceptional undergraduate; some defenders concede the difficulty, but insist that cultivating interdisciplinarity as a habit of mind, even at that level, is both possible and essential to the education of informed and engaged citizens and leaders capable of analyzing, evaluating, and synthesizing information from multiple sources in order to render reasoned decisions.

While much has been written on the philosophy and promise of interdisciplinarity in academic programs and professional practice, social scientists are increasingly interrogating academic discourses on interdisciplinarity, as well as how interdisciplinarity actually works—and does not—in practice. Some have shown, for example, that some interdisciplinary enterprises that aim to serve society can produce deleterious outcomes for which no one can be held to account.

Since 1998, there has been an ascendancy in the value of interdisciplinary research and teaching and a growth in the number of bachelor's degrees awarded at U.S. universities classified as multi- or interdisciplinary studies. The number of interdisciplinary bachelor's degrees awarded annually rose from 7,000 in 1973 to 30,000 a year by 2005 according to data from the National Center of Educational Statistics (NECS). In addition, educational leaders from the Boyer Commission to Carnegie's President Vartan Gregorian to Alan I. Leshner, CEO of the American Association for the Advancement of Science have advocated for interdisciplinary rather than disciplinary approaches to problem solving in the 21st century. This has been echoed by federal funding agencies, particularly the National Institutes of Health under the direction of Elias Zerhouni, who has advocated that grant proposals be framed more as interdisciplinary collaborative projects than single researcher, single discipline ones.

At the same time, many thriving longstanding bachelor's in interdisciplinary studies programs in existence for 30 or more years, have been closed down, in spite of healthy enrollment. Examples include Arizona International (formerly part of the University of Arizona), the School of Interdisciplinary Studies at Miami University, and the Department of Interdisciplinary Studies at Wayne State University; others such as the Department of Interdisciplinary Studies at Appalachian State University, and George Mason University's New Century College, have been cut back. Stuart Henry has seen this trend as part of the hegemony of the disciplines in their attempt to recolonize the experimental knowledge production of otherwise marginalized fields of inquiry. This is due to threat perceptions seemingly based on the ascendancy of interdisciplinary studies against traditional academia.

There are many examples of when a particular idea, almost on the same period, arises in different disciplines. One case is the shift from the approach of focusing on "specialized segments of attention" (adopting one particular perspective), to the idea of "instant sensory awareness of the whole", an attention to the "total field", a "sense of the whole pattern, of form and function as a unity", an "integral idea of structure and configuration". This has happened in painting (with cubism), physics, poetry, communication and educational theory. According to Marshall McLuhan, this paradigm shift was due to the passage from an era shaped by mechanization, which brought sequentiality, to the era shaped by the instant speed of electricity, which brought simultaneity.

An article in the "Social Science Journal" attempts to provide a simple, common-sense, definition of interdisciplinarity, bypassing the difficulties of defining that concept and obviating the need for such related concepts as transdisciplinarity, pluridisciplinarity, and multidisciplinarity:"To begin with, a discipline can be conveniently defined as any comparatively self-contained and isolated domain of human experience which possesses its own community of experts. Interdisciplinarity is best seen as bringing together distinctive components of two or more disciplines. In academic discourse, interdisciplinarity typically applies to four realms: knowledge, research, education, and theory. Interdisciplinary knowledge involves familiarity with components of two or more disciplines. Interdisciplinary research combines components of two or more disciplines in the search or creation of new knowledge, operations, or artistic expressions. Interdisciplinary education merges components of two or more disciplines in a single program of instruction. Interdisciplinary theory takes interdisciplinary knowledge, research, or education as its main objects of study."In turn, interdisciplinary "richness" of any two instances of knowledge, research, or education can be ranked by weighing four variables: number of disciplines involved, the "distance" between them, the novelty of any particular combination, and their extent of integration.

Interdisciplinary knowledge and research are important because:

"The modern mind divides, specializes, thinks in categories: the Greek instinct was the opposite, to take the widest view, to see things as an organic whole [...]. The Olympic games were designed to test the arete of the whole man, not a merely specialized skill [...]. The great event was the pentathlon, if you won this, you were a man. Needless to say, the Marathon race was never heard of until modern times: the Greeks would have regarded it as a monstrosity.""Previously, men could be divided simply into the learned and the ignorant, those more or less the one, and those more or less the other. But your specialist cannot be brought in under either of these two categories. He is not learned, for he is formally ignorant of all that does not enter into his specialty; but neither is he ignorant, because he is 'a scientist,' and 'knows' very well his own tiny portion of the universe. We shall have to say that he is a learned ignoramus, which is a very serious matter, as it implies that he is a person who is ignorant, not in the fashion of the ignorant man, but with all the petulance of one who is learned in his own special line.""It is the custom among those who are called "practical" men to condemn any man capable of a wide survey as a visionary: no man is thought worthy of a voice in politics unless he ignores or does not know nine-tenths of the most important relevant facts."





</doc>
<doc id="21758835" url="https://en.wikipedia.org/wiki?curid=21758835" title="Institutional memory">
Institutional memory

Institutional memory is a collective set of facts, concepts, experiences and knowledge held by a group of people.

Institutional memory has been defined as "the stored knowledge within the organization." Within any organization, tools and techniques will need to be adapted to meet that organization's needs. These adaptations are developed over time and taught to new members of the group, keeping them from encountering the same problems and having to develop a solution that already exists. In this way, organizations save time and resources that might otherwise be wasted.

For example, two automobile repair shops might have the same model of car lift. The lifts themselves and the written instructions for them are identical. However, if one shop has a lower ceiling than the other, its employees may determine that raising a car beyond a certain height can cause it to be damaged by the ceiling. The current employees inform new employees of this workaround. They, in turn, inform future new employees, even if the person who originally discovered the problem no longer works there. Such information is in the repair shop's institutional memory.

Institutional memory requires the ongoing transmission of memories between members of the group. As such, it relies on a continuity of group membership. If everyone at the aforementioned auto shop quit at once, the employees hired to replace them would not be able to benefit from the previous group's experience. In such a case, the organization would have lost its institutional memory and operate less efficiently until the workarounds that composed it could be developed again.

Elements of institutional memory may be found in corporations, professional groups, government bodies, religious groups, academic collaborations, and by extension in entire cultures. There are different ideas about how institutional memory is transferred, whether it is between people or through written sources. 

Institutional memory may be encouraged to preserve an ideology or way of work in such a group. Conversely, institutional memory may be ingrained to the point that it becomes hard to challenge, even the conditions that caused it arise have changed. An example of this would be an organization continuing to submit a form, even after the law requiring that document has been repealed, for fear of legal consequences that no longer exist. Institutional memory may also have influence on organizational identity, choice of individuals, and actions of the individuals interacting with the institution.

Institutional knowledge is gained by organizations translating historical data into useful knowledge and wisdom. Memory depends upon the preservation of data and also the analytical skills necessary for its effective use within the organization.

Religion is one of the significant institutional forces acting on the collective memory attributed to humanity. Alternatively, the evolution of ideas in Marxist theory, is that the mechanism whereby knowledge and wisdom are passed down through the generations is subject to economic determinism. In all instances, social systems, cultures, and organizations have an interest in controlling and using institutional memories.

Organizational structure determines the training requirements and expectations of behaviour associated with various roles. This is part of the implicit institutional knowledge. Progress to higher echelons requires assimilation of this, and when outsiders enter at a high level, effectiveness tends to deteriorate if this morale is unjustly ignored.



</doc>
<doc id="628450" url="https://en.wikipedia.org/wiki?curid=628450" title="Activity theory">
Activity theory

Activity theory (AT; ) is an umbrella term for a line of eclectic social sciences theories and research with its roots in the Soviet psychological activity theory pioneered by Lev Vygotsky, Alexei Leont'ev and Sergei Rubinstein. These scholars sought to understand human activities as systemic and socially situated phenomena and to go beyond paradigms of reflexology (the teaching of Vladimir Bekhterev and his followers) and classical conditioning (the teaching of Ivan Pavlov and his school), psychoanalysis and behaviorism. It became one of the major psychological approaches in the former USSR, being widely used in both theoretical and applied psychology, and in education, professional training, ergonomics, social psychology and work psychology.

Activity theory is more of a descriptive meta-theory or framework than a predictive theory. It considers an entire work/activity system (including teams, organizations, etc.) beyond just one actor or user. It accounts for environment, history of the person, culture, role of the artifact, motivations, and complexity of real-life activity. One of the strengths of AT is that it bridges the gap between the individual subject and the social reality—it studies both through the mediating activity. The unit of analysis in AT is the concept of object-oriented, collective and culturally mediated human activity, or "activity system". This system includes the object (or objective), subject, mediating artifacts (signs and tools), rules, community and division of labor. The motive for the activity in AT is created through the tensions and contradictions within the elements of the system. According to ethnographer Bonnie Nardi, a leading theorist in AT, activity theory "focuses on practice, which obviates the need to distinguish 'applied' from 'pure' science—understanding everyday practice in the real world is the very objective of scientific practice. ... The object of activity theory is to understand the unity of consciousness and activity." Sometimes called "Cultural-Historical Activity Theory", this approach is particularly useful for studying a group that exists "largely in virtual form, its communications mediated largely through electronic and printed texts."

AT is particularly useful as a lens in qualitative research methodologies (e.g., ethnography, case study). AT provides a method of understanding and analyzing a phenomenon, finding patterns and making inferences across interactions, describing phenomena and presenting phenomena through a built-in language and rhetoric. A particular activity is a goal-directed or purposeful interaction of a subject with an object through the use of tools. These tools are exteriorized forms of mental processes manifested in constructs, whether physical or psychological. AT recognizes the internalization and externalization of cognitive processes involved in the use of tools, as well as the transformation or development that results from the interaction.

The origins of activity theory can be traced to several sources, which have subsequently given rise to various complementary and intertwined strands of development. This account will focus on three of the most important of these strands. The first is associated with the Moscow Institute of Psychology and in particular the "troika" of young Russian researchers, Vygotsky, Leont'ev and Luria. Vygotsky founded cultural-historical psychology, a field that became the basis for modern AT; Leont'ev, one of the principal founders of activity theory, both developed and reacted against Vygotsky's work. Leont'ev's formulation of general activity theory is currently the most influential in post-Soviet developments in AT, which have largely been in social-scientific, organizational, and writing-studies rather than psychological research.

The second major line of development within activity theory involves Russian scientists, such as P. K. Anokhin and Nikolai Bernstein, more directly concerned with the neurophysiological basis of activity; its foundation is associated with the Soviet philosopher of psychology Sergei Rubinstein. This work was subsequently developed by researchers such as Pushkin, Zinchenko & Gordeeva, Ponomarenko, Zarakovsky and others, and is currently most well-known through the work on systemic-structural activity theory being carried out by G. Z. Bedny and his associates.

Finally, in the Western world, discussions and use of AT are primarily framed within the Scandinavian activity theory strand, developed by Yrjö Engeström.

After Vygotsky's early death, Leont'ev became the leader of the research group nowadays known as the Kharkov School of Psychology and extended Vygotsky's research framework in significantly new ways. Leont'ev first examined the psychology of animals, looking at the different degrees to which animals can be said to have mental processes. He concluded that Pavlov's reflexionism was not a sufficient explanation of animal behaviour and that animals have an active relation to reality, which he called "activity". In particular, the behaviour of higher primates such as chimpanzees could only be explained by the ape's formation of multi-phase plans using tools.

Leont'ev then progressed to humans and pointed out that people engage in "actions" that do not in themselves satisfy a need, but contribute towards the eventual satisfaction of a need. Often, these actions only make sense in a social context of a shared work activity. This led him to a distinction between "activities", which satisfy a need, and the "actions" that constitute the activities. Leont'ev also argued that the activity in which a person is involved is reflected in their mental activity, that is (as he puts it) material reality is "presented" to consciousness, but only in its vital meaning or significance.

Activity theory also influenced the development of organizational-activity game as developed by Georgy Shchedrovitsky.

AT remained virtually unknown outside the Soviet Union until the mid-1980s, when it was picked up by Scandinavian researchers. The first international conference on activity theory was not held until 1986. The earliest non-Soviet paper cited by Nardi is a 1987 paper by Yrjö Engeström: "Learning by expanding". This resulted in a reformulation of AT. Kuutti notes that the term "activity theory" "can be used in two senses: referring to the original Soviet tradition or referring to the international, multi-voiced community applying the original ideas and developing them further."

The Scandinavian AT school of thought seeks to integrate and develop concepts from Vygotsky's Cultural-historical psychology and Leont'ev's activity theory with Western intellectual developments such as Cognitive Science, American Pragmatism, Constructivism, and Actor-Network Theory. It is known as Scandinavian activity theory. Work in the systems-structural theory of activity is also being carried on by researchers in the US and UK.

Some of the changes are a systematisation of Leont'ev's work. Although Leont'ev's exposition is clear and well structured, it is not as well-structured as the formulation by Yrjö Engeström. Kaptelinin remarks that Engeström "proposed a scheme of activity different from that by Leont'ev; it contains three interacting entities—the individual, the object and the community—instead of the two components—the individual and the object—in Leont'ev's original scheme."

Some changes were introduced, apparently by importing notions from human–computer interaction theory. For instance, the notion of "rules", which is not found in Leont'ev, was introduced. Also, the notion of collective subject was introduced in the 1970s and 1980s (Leont'ev refers to "joint labour activity", but only has individuals, not groups, as activity subjects).

The goal of activity theory is understanding the mental capabilities of a single individual. However, it rejects the "isolated" individuals as insufficient unit of analysis, analyzing the cultural and technical aspects of human actions.

Activity theory is most often used to describe actions in a socio-technical system through six related elements (Bryant et al. as defined by Leonti'ev 1981 and redefined in Engeström 1987) of a conceptual system expanded by more nuanced theories:

Activity theory helps explain how social artifacts and social organization mediate social action (Bryant et al.).

The application of activity theory to information systems derives from the work of Bonnie Nardi and Kari Kuutti. Kuutti's work is addressed below. Nardi's approach is, briefly, as follows: Nardi (p. 6) described activity theory as "...a powerful and clarifying descriptive tool rather than a strongly predictive theory. The object of activity theory is to understand the unity of consciousness and activity...Activity theorists argue that consciousness is not a set of discrete disembodied cognitive acts (decision making, classification, remembering), and certainly it is not the brain; rather, consciousness is located in everyday practice: you are what you do." Nardi (p. 5) also argued that "activity theory proposes a strong notion of "mediation"—all human experience is shaped by the tools and sign systems we use." Nardi (p. 6) explained that "a basic tenet of activity theory is that a notion of consciousness is central to a depiction of activity. Vygotsky described consciousness as a phenomenon that unifies attention, intention, memory, reasoning, and speech..." and (p. 7) "Activity theory, with its emphasis on the importance of motive and consciousness—which belongs only to humans—sees people and things as fundamentally different. People are not reduced to 'nodes' or 'agents' in a system; 'information processing' is not seen as something to be modelled in the same way for people and machines."

In a later work, Nardi et al. in comparing activity theory with cognitive science, argue that "activity theory is above all a social theory of consciousness" and therefore "... activity theory wants to define consciousness, that is, all the mental functioning including
remembering, deciding, classifying, generalising, abstracting and so forth, as a product of our social interactions with other people and of our use of tools." For Activity Theorists "consciousness" seems to refer to any mental functioning, whereas most other approaches to psychology distinguish conscious from unconscious functions.

Over the last 15 years the use and exploration of activity theory in information systems has grown. One stream of research has focused on technology mediated change and the implementation of technologies and how they disrupt, change and improve organisational work activity. In these studies, activity systems are used to understand emergent contradictions in the work activity, which are temporarily resolved using information systems (tools) and/or arising from the introduction of information systems. Information science studies use a similar approach to activity theory in order to understand information behaviour "in context".
In the field of ICT and development (a field of study within information systems) the use of activity theory has also been used to inform development of IT systems and to frame the study of ICT in development settings.

In addition, Etengoff & Daiute have conducted recent work exploring how social media interfaces can be productively used to mediate conflicts. Their work has illustrated this perspective with analyses of online interactions between gay men and their religious family members and Sunni-Muslim emerging adults' efforts to maintain a positive ethnic identity via online religious forums in post 9/11 contexts.

The rise of the personal computer challenged the focus in traditional systems developments on mainframe systems for automation of existing work routines. It furthermore brought forth a need to focus on how to work on materials and objects through the computer. In the search of theoretical and methodical perspectives suited to deal with issues of flexibility and more advanced mediation between the human being, material and outcomes through the interface, it seemed promising to turn to the still rather young HCI research tradition that had emerged primarily in the US (for further discussion see Bannon & Bødker, 1991).

Specifically the cognitive science-based theories lacked means of addressing a number of issues that came out of the empirical projects (see Bannon & Bødker, 1991): 1. Many of the early advanced user interfaces assumed that the users were the designers themselves, and accordingly built on an assumption of a generic user, without concern for qualifications, work environment, division of work, etc. 2.In particular the role of the artifact as it stands between the user and her materials, objects and outcomes was ill understood. 3. In validating findings and designs there was a heavy focus on novice users whereas everyday use by experienced users and concerns for the development of expertise were hardly addressed. 4.Detailed task analysis and the idealized models created through task analysis failed to capture the complexity and contingency of real-life action. 5.From the point of view of complex work settings, it was striking how most HCI focused on one user – one computer in contrast to the ever-ongoing cooperation and coordination of real work situations (this problem later lead to the development of CSCW). 6.Users were mainly seen as objects of study.

Because of these shortcomings, it was necessary to move outside cognitive science-based HCI to find or develop the necessary theoretical platform. European psychology had taken different paths than had American with much inspiration from dialectical materialism (Hydén 1981, Engeström, 1987). Philosophers such as Heidegger and Wittgenstein came to play an important role, primarily through discussions of the limitations of AI (Winograd & Flores 1986, Dreyfus & Dreyfus 1986). Suchman (1987) with a similar focus introduced ethnomethodology into the discussions, and Ehn (1988) based his treatise of design of computer artifacts on Marx, Heidegger and Wittgenstein.
The development of the activity theoretical angle was primarily carried out by Bødker (1991, 1996) and by Kuutti (Bannon & Kuutti, 1993, Kuutti, 1991, 1996), both with strong inspiration from Scandinavian activity theory groups in psychology. Bannon (1990, 1991) and Grudin (1990a and b) made significant contributions to the furthering of the approach by making it available to the HCI audience. The work of Kaptelinin (1996) has been important to connect to the earlier development of activity theory in Russia. Nardi produced the, hitherto, most applicable collection of activity theoretical HCI literature (Nardi, 1996).

At the end of the 1990s, a group of Russian and American activity theorists working in the systems-cybernetic tradition of Bernshtein and Anokhin began to publish English-language articles and books dealing with topics in human factors and ergonomics and, latterly, human–computer interaction. Under the rubric of systemic-structural activity theory (SSAT), this work represents a modern synthesis within activity theory which brings together the cultural-historical and systems-structural strands of the tradition (as well as other work within Soviet psychology such as the Psychology of Set) with findings and methods from Western human factors/ergonomics and cognitive psychology.

The development of SSAT has been specifically oriented toward the analysis and design of the basic elements of human work activity: tasks, tools, methods, objects and results, and the skills, experience and abilities of involved subjects. SSAT has developed techniques for both the qualitative and quantitative description of work activity. Its design-oriented analyses specifically focus on the interrelationship between the structure and self-regulation of work activity and the configuration of its material components.

This section presents a short introduction to activity theory, and some brief comments on human creativity in activity theory and the implications of activity theory for tacit knowledge and learning.

Activity theory begins with the notion of activity. An activity is seen as a system of human "doing" whereby a subject works on an object in order to obtain a desired outcome. In order to do this, the subject employs tools, which may be external (e.g. an axe, a computer) or internal (e.g. a plan). As an illustration, an activity might be the operation of an automated call centre. As we shall see later, many subjects may be involved in the activity and each subject may have one or more motives (e.g. improved supply management, career advancement or gaining control over a vital organisational power source). A simple example of an activity within a call centre might be a telephone operator (subject) who is modifying a customer's billing record (object) so that the billing data is correct (outcome) using a graphical front end to a database (tool).

Kuutti formulates activity theory in terms of the structure of an activity. "An activity is a form of doing directed to an object, and activities are distinguished from each other according
to their objects. Transforming the object into an outcome motivates the existence of an activity. An object can be a material thing, but it can also be less tangible."

Kuutti then adds a third term, the tool, which 'mediates' between the activity and the object. "The tool is at the same time both enabling and limiting: it empowers the subject in the transformation process with the historically collected experience and skill 'crystallised' to it, but it also restricts the interaction to be from the perspective of that particular tool or instrument; other potential features of an object remain invisible to the subject...".

As Verenikina remarks, tools are "social objects with certain modes of operation developed socially in the course of labour and are only possible because they correspond to the objectives of a practical action."

An activity is modelled as a three-level hierarchy. Kuutti schematises processes in activity theory as a three-level system.

Verenikina paraphrases Leont'ev as explaining that "the non-coincidence of action and operations... appears in actions with tools, that is, material objects which are crystallised operations, not actions nor goals. If a person is confronted with a specific goal of, say, dismantling a machine, then they must make use of a variety of operations; it makes no difference how the individual operations were learned because the formulation of the operation proceeds differently to the formulation of the goal that initiated the action."

The levels of activity are also characterised by their purposes: "Activities are oriented to motives, that is, the objects that are impelling by themselves. Each motive is an object, material or ideal, that satisfies a need. Actions are the processes functionally subordinated to activities; they are directed at specific conscious goals... Actions are realised through operations that are determined by the actual conditions of activity."

Engeström developed an extended model of an activity, which adds another component, community ("those who share the same object"), and then adds rules to mediate between subject and community, and the division of labour to mediate between object and community.

Kuutti asserts that "These three classes should be understood broadly. A tool can be anything used in the transformation process, including both material tools and tools for thinking. Rules cover both explicit and implicit norms, conventions, and social relations within a community. Division of labour refers to the explicit and implicit organisation of the community as related to the transformation process of the object into the outcome."

Activity theory therefore includes the notion that an activity is carried out within a social context, or specifically in a community. The way in which the activity fits into the context is thus established by two resulting concepts:

Activity theory provides a number of useful concepts that can be used to address the lack of expression for 'soft' factors which are inadequately represented by most process modelling frameworks. One such concept is the internal plane of action. Activity theory recognises that each activity takes place in two planes: the external plane and the internal plane. The external plane represents the objective components of the action while the internal plane represents the subjective components of the action. Kaptelinin defines the "internal plane of actions" as "[...] a concept developed in activity theory that refers to the human ability to perform manipulations with an internal representation of external objects before starting actions with these objects in reality."

The concepts of motives, goals and conditions discussed above also contribute to the modelling of soft factors. One principle of activity theory is that many activities have multiple motivation ('polymotivation'). For instance, a programmer in writing a program may address goals aligned towards multiple motives such as increasing his or her annual bonus, obtaining relevant career experience and contributing to organisational objectives.

Activity theory further argues that subjects are grouped into communities, with rules mediating between subject and community and a division of labour mediating between object and community. A subject may be part of several communities and a community, itself, may be part of other communities.

Human creativity plays an important role in activity theory, that "human beings... are essentially creative beings" in "the creative, non-predictable character". Tikhomirov also analyses the importance of "creative activity", contrasting it to "routine" activity, and notes the important shift brought about by computerisation in the balance towards creative activity.

Karl Marx, a sociological theorist, argued that humans are unique compared to other species in that humans create everything they need to survive. According to Marx, this is described as species-being. Marx believed we find our true identity in what we produce in our personal labor.

Activity theory has an interesting approach to the difficult problems of "learning" and, in particular, "tacit knowledge". Learning has been a favourite subject of management theorists, but it has often been presented in an abstract way separated from the work processes to which the learning should apply. Activity theory provides a potential corrective to this tendency. For instance, Engeström's review of Nonaka's work on "knowledge creation" suggests enhancements based on activity theory, in particular suggesting that the organisational learning process includes preliminary stages of goal and problem formation not found in Nonaka. Lompscher, rather than seeing learning as "transmission", sees the formation of learning goals and the
student's understanding of which things they need to acquire as the key to the formation of the learning activity.

Of particular importance to the study of learning in organisations is the problem of "tacit knowledge", which according to Nonaka, "is highly personal and hard to formalise, making it difficult to communicate to others or to share with others." Leont'ev's concept of operation provides an important insight into this problem. In addition, the key idea of "internalisation" was originally introduced by Vygotsky as "the internal reconstruction of an external operation." Internalisation has subsequently become a key term of the theory of tacit knowledge and has been defined as "a process of embodying explicit knowledge into tacit knowledge." Internalisation has been described by Engeström as the "key psychological mechanism" discovered by Vygotsky and is further discussed by Verenikina.






</doc>
<doc id="2738697" url="https://en.wikipedia.org/wiki?curid=2738697" title="Autoepistemic logic">
Autoepistemic logic

The autoepistemic logic is a formal logic for the representation and reasoning of knowledge about knowledge. While propositional logic can only express facts, autoepistemic logic can express knowledge and lack of knowledge about facts.

The stable model semantics, which is used to give a semantics to logic programming with negation as failure, can be seen as a simplified form of autoepistemic logic.

The syntax of autoepistemic logic extends that of propositional logic by a modal operator formula_1 indicating knowledge: if formula_2 is a formula, formula_3 indicates that formula_2 is known. As a result, formula_5 indicates that formula_6 is known and formula_7 indicates that formula_2 is not known.

This syntax is used for allowing reasoning based on knowledge of facts. For example, formula_9 means that formula_2 is assumed false if it is not known to be true. This is a form of negation as failure.

The semantics of autoepistemic logic is based on the "expansions" of a theory, which have a role similar to models in propositional logic. While a propositional model specifies which axioms are true or false, an expansion specifies which formulae formula_3 are true and which ones are false. In particular, the expansions of an autoepistemic formula formula_12 makes this distinction for every subformula formula_3 contained in formula_12. This distinction allows formula_12 to be treated as a propositional formula, as all its subformulae containing formula_1 are either true or false. In particular, checking whether formula_12 entails formula_2 in this condition can be done using the rules of the propositional calculus. In order for an initial assumption to be an expansion, it must be that a subformula formula_2 is entailed if and only if formula_3 has been initially assumed true.

In terms of possible world semantics, an expansion of formula_12 consists of an S5 model of formula_12 in which the possible worlds consist only of worlds where formula_12 is true. [The possible worlds need not contain all such consistent worlds; this corresponds to the fact that modal propositions are assigned truth values before checking derivability of the ordinary propositions.] Thus, autoepistemic logic extends S5; the extension is proper, since formula_24 and formula_25 are tautologies of autoepistemic logic, but not of S5.

For example, in the formula formula_26, there is only a single “boxed subformula”, which is formula_27. Therefore, there are only two candidate expansions, assuming it true or false, respectively. The check for them being actual expansions is as follows.

formula_27 is false : with this assumption, formula_12 becomes tautological, as formula_30 is equivalent to formula_31, and formula_32 is assumed true; therefore, formula_33 is not entailed. This result confirms the assumption implicit in formula_27 being false, that is, that formula_33 is not currently known. Therefore, the assumption that formula_27 is false is an expansion.

formula_27 is true : together with this assumption, formula_12 entails formula_33; therefore, the initial assumption that is implicit in formula_27 being true, i.e., that formula_33 is known to be true, is satisfied. As a result, this is another expansion.

The formula formula_12 has therefore two expansions, one in which formula_33 is not known and one in which formula_33 is known. The second one has been regarded as unintuitive, as the initial assumption that formula_27 is true is the only reason why formula_33 is true, which confirms the assumption. In other words, this is a self-supporting assumption. A logic allowing such a self-support of beliefs is called "not strongly grounded" to differentiate them from "strongly grounded" logics, in which self-support is not possible. Strongly grounded variants of autoepistemic logic exist.

In uncertain inference, the known/unknown duality of truth values is replaced by a degree of certainty of a fact or deduction; certainty may vary from 0 (completely uncertain/unknown) to 1 (certain/known). In probabilistic logic networks, truth values are also given a probabilistic interpretation ("i.e." truth values may be uncertain, and, even if almost certain, they may still be "probably" true (or false).)




</doc>
<doc id="1106205" url="https://en.wikipedia.org/wiki?curid=1106205" title="Cognitive closure (philosophy)">
Cognitive closure (philosophy)

In philosophy of science and philosophy of mind, cognitive closure is the proposition that human minds are constitutionally incapable of solving certain perennial philosophical problems. Owen Flanagan calls this position anti-constructive naturalism or the "new mysterianism" and the primary advocate of the hypothesis, Colin McGinn, calls it transcendental naturalism acknowledging the possibility that solutions may be an intelligent non-human of some kind. According to McGinn, such philosophical questions include the mind-body problem, identity of the self, foundations of meaning, free will, and knowledge, both "a priori" and empirical.

For Friedrich Hayek, "The whole idea of the mind explaining itself is a logical contradiction"... and "takes this incompleteness—the constitutional inability of mind to explain itself—to be a generalized case of Gödel's incompleteness theorem... Hayek is "not" a naturalistic agnostic, that is, the view that science "currently" cannot offer an explanation of the mind-body relationship, but in principle it could."

Noam Chomsky argues that the cognitive capabilities of all organisms are limited by biology and that certain problems may be beyond our understanding:
As argued in Kant's "Critique of Pure Reason", human thinking is unavoidably structured by categories of the understanding:
These are ideas from which there is no escape and thus they pose a limit to thinking. What can be known through the categories is called phenomena and what is outside the categories is called noumena, the unthinkable "things in themselves".

In his (famous) essay "What Is It Like to Be a Bat?" Thomas Nagel mentions the possibility of cognitive closure to the subjective character of experience and the (deep) implications that it has for materialist reductionist science. Owen Flanagan noted in his 1991 book "Science of the Mind" that some modern thinkers have suggested that consciousness will never be completely explained. Flanagan called them "the new mysterians" after the rock group Question Mark and the Mysterians. According to McGinn, the solution to the mind-body problem cannot be grasped, despite the fact that the solution is "written in our genes".

Emergent materialism is a similar but different claim that humans are not smart enough to determine "the relationship between mind and matter."

While the nature of consciousness is complex, according to some philosophers, that does not imply closure, thus, McGinn's argument is flawed.



</doc>
<doc id="33456414" url="https://en.wikipedia.org/wiki?curid=33456414" title="A Causal Theory of Knowing">
A Causal Theory of Knowing

"A Causal Theory of Knowing" is a philosophical essay written by Alvin Goldman in 1967, published in "The Journal of Philosophy". It is based on existing theories of knowledge in the realm of epistemology, the study of philosophy through the scope of knowledge. The essay attempts to define knowledge by connecting facts, beliefs and knowledge through underlying and connective series called causal chains. It provides a causal theory of knowledge.

A causal chain is repeatedly described as a sequence of events for which one event in a chain causes the next. According to Goldman, these chains can only exist with the presence of an accepted fact, a belief in the fact, and a cause for the subject to believe the fact. The essay also explores the ideas of perception and memory through the use of the causal chains and the concept of knowledge.

The essay is regarded as an improvement and rebuttal of Edmund Gettier's "Is Justified True Belief Knowledge?", which is one of many attempts to explain the necessary conditions for knowledge to develop. Goldman implements the causal connection to reiterate his own theory of knowledge. Knowledge exists, says Goldman, if and only if the belief is justified by a reaction to the accepted fact.

Goldman's theory later counters that of Michael Clark, stating that his own theory including figures and diagrams is more appropriate than Clark's. "A Causal Theory of Knowing" uses figures which make explicit references to causal beliefs. Clark's model does not utilize these arrows, and Goldman states that the lack of these arrows deems Clark's model deficient.

Alvin Goldman, currently a professor of philosophy at Rutgers University, wrote "A Causal Theory of Knowing" when he was in his late twenties. Goldman received his Ph.D. from Princeton University, and has taught at numerous universities.

Goldman's research deals mainly with epistemology and other cognitive sciences. "A Causal Theory of Knowing" was Goldman's first published paper explaining his own views of epistemology. Currently, Goldman has written more than ten essays focusing on knowledge and cognitive science.

The essay starts with a definition of Gettier's theory, followed by multiple reiterations of the idea of causal connections, figures to explain knowledge through a visual perspective, and references to perception and memory through causal chains.

The essay tends to focus on examples in which knowledge or other sensations do not exist, rather than proving a certain fact to be known. Goldman also states on multiple occasions that he does not wish to explain the causal process in detail, instead pointing out counterexamples. At numerous times in the essay, he also points out that he does not intend to give definitive answers to each of the propositions mentioned.

Goldman also refocuses the idea of perception, or knowledge through sensation (specifically sight) using his own theory of knowing. The concept of causal perception indicates that one observes something only if the object itself causes the sensation of sight to be accepted as known. Thus, the object's existence must be factual and one must believe its existence. While all knowledge comes from facts, inferred knowledge emerges when physical object facts cause sense data which can be perceived as senses. The sense data can also be used to make conclusions, known as inferred knowledge, about certain physical object facts.

From "A Causal Theory of Knowing", Goldman constructs the idea that memory is also a causal process. Memory is explained as being an extension of knowledge into the future, and remembering is the act of recalling a fact that has already been known. Further, the theory states that if knowledge is forgotten at one time, it cannot be considered a memory in the future. According to Goldman, if a fact is known at Time 1 but forgotten at Time 2, and then at Time 3 that the fact is perceived again but not known, at Time 3 the original fact is not a memory because there is no causal connection between the fact and the memory.



</doc>
<doc id="639511" url="https://en.wikipedia.org/wiki?curid=639511" title="Episteme">
Episteme

"Episteme" is a philosophical term derived from the Ancient Greek word ἐπιστήμη "epistēmē", which can refer to knowledge, science or understanding, and which comes from the verb , meaning "to know, to understand, or to be acquainted with". 

Plato contrasts episteme with "doxa": common belief or opinion. Episteme is also distinguished from "techne": a craft or applied practice. The word "epistemology" is derived from episteme.

The French philosopher Michel Foucault used the term "épistémè" in a specialized sense in his work "The Order of Things" to mean the historical, but non-temporal, "a priori" which grounds knowledge and its discourses and thus represents the condition of their possibility within a particular epoch. 

In subsequent writings, he made it clear that several "épistémè" may co-exist and interact at the same time, being parts of various power-knowledge systems. But he did not discard the concept:
Yet in Foucault's "The Order of Things" he describes "épistémè" as:

In any given culture and at any given moment, there is always only one "episteme" that defines the conditions of possibility of all knowledge, whether expressed in a theory or silently invested in a practice. (Foucault, 168)

Foucault's use of "épistémè" has been asserted as being similar to Thomas Kuhn's notion of a "paradigm", as for example by Jean Piaget. However, there are decisive differences. 

Whereas Kuhn's "paradigm" is an all-encompassing collection of beliefs and assumptions that result in the organization of scientific worldviews and practices, Foucault's "episteme" is not confined to science—it provides the grounding for a broad range of discourses (all of science itself would fall under the "episteme" of the epoch). One might say that a paradigm is subsumed within an episteme.

Kuhn's paradigm shifts are a consequence of a series of conscious decisions made by scientists to pursue a neglected set of questions. Foucault's "episteme" is something like the 'epistemological unconscious' of an era; the resultant configuration of knowledge of a particular "episteme" is, to Foucault, based on a set of primordial, fundamental assumptions that are so basic to the "episteme" that they're experientially "invisible" to the constituents (such as people, organizations, or systems) operating within the "episteme."

Moreover, Kuhn's concept corresponds to what Foucault calls "theme" or theory of a science, but Foucault analyzed how "opposing" theories and themes could "co-exist" within a science. Kuhn doesn't search for the conditions of possibility of opposing discourses within a science, but simply for the invariant dominant paradigm governing scientific research (supposing that "one" paradigm always "is" pervading, except under paradigmatic transition). 

Foucault attempts to demonstrate the constitutive limits of discourse, and in particular, the rules enabling their productivity; however, Foucault maintained that though ideology may infiltrate and form science, it need not do so: it must be demonstrated how ideology actually forms the science in question; contradictions and lack of objectivity is not an indicator of ideology. "Truth is a thing of this world: it is produced only by virtue of multiple forms of constraint. And it induces regular effects of power. Each society has its regime of truth, its "general politics” of truth: that is, the types of discourse which it accepts and makes function as true; the mechanisms and instances which enable one to distinguish true and false statements, the means by which each is sanctioned; the techniques and procedures accorded value in the acquisition of truth; the status of those who are charged with saying what counts as true." Kuhn's and Foucault's notions are possibly influenced by the French philosopher of science Gaston Bachelard's notion of an "epistemological rupture", as indeed was Althusser.

In 1997, Judith Butler used the concept of episteme in her book "", examining the use of speech-act theory for political purposes.




</doc>
<doc id="582687" url="https://en.wikipedia.org/wiki?curid=582687" title="Jnana">
Jnana

In Indian philosophy and religion, jñana (Sanskrit: ज्ञान, pronounced "gyah-nuh", IPA: [ɡjɑ́ː.n̪ɐ] or [d͡ʑɲɑ́ː.n̪ɐ]) (Pali: "ñāṇa") or (Hindi: "gyān") is "knowledge".

The idea of "jnana" centers on a cognitive event which is recognized when experienced. It is knowledge inseparable from the total experience of reality, especially a total or divine reality (Brahman). 

The root jñā- is cognate to English "know", as well as to the Greek γνώ- (as in γνῶσις "gnosis") and Russian "знание". Its antonym is "ajñāna" "ignorance".

In Tibetan Buddhism, it refers to pure awareness that is free of conceptual encumbrances, and is contrasted with "vijnana", which is a moment of 'divided knowing'. Entrance to, and progression through the ten stages of "Jnana/Bhimis", will lead one to complete enlightenment and nirvana.

In the Vipassanā tradition of Buddhism there are the following "ñanas" according to Mahasi Sayadaw. As a person meditates these "ñanas" or "knowledges" will be experienced in order. The experience of each may be brief or may last for years and the subjective intensity of each is variable. Each "ñana" could also be considered a "jhāna" although many are not stable and the mind has no way to remain embedded in the experience. Experiencing all the "ñanas" will lead to the first of the Four stages of enlightenment then the cycle will start over at a subtler level.

Sahu explains:
Jnana yoga (Yoga of Knowledge) is one of the three main paths ("margas"), which are supposed to lead towards "moksha" (liberation) from material miseries. The other two main paths are Karma yoga and Bhakti Yoga. Rāja yoga (classical yoga) which includes several yogas, is also said to lead to "moksha". It is said that each path is meant for a different temperament of personality.

According to the Jain texts like Tattvārthsūtra and Sarvārthasiddhi, knowledge is of five kinds:

"Gyan" or "Gian" refers to spiritual knowledge. It is mentioned throughout the Guru Granth Sahib.



</doc>
<doc id="1628055" url="https://en.wikipedia.org/wiki?curid=1628055" title="Know thyself">
Know thyself

The Ancient Greek aphorism "know thyself" (Greek: , transliterated: '; also ' with the ε contracted), is one of the Delphic maxims and was inscribed in the pronaos (forecourt) of the Temple of Apollo at Delphi according to the Greek writer Pausanias (10.24.1). In Latin the phrase, "know thyself," is given as "nosce te ipsum" or "temet nosce".

The maxim, or aphorism, "know thyself" has had a variety of meanings attributed to it in literature.

The Greek aphorism has been attributed to at least the following ancient Greek sages:

Diogenes Laërtius attributes it to Thales ("Lives" I.40), but also notes that Antisthenes in his "Successions of Philosophers" attributes it to Phemonoe, a mythical Greek poet, though admitting that it was appropriated by Chilon. In a discussion of moderation and self-awareness, the Roman poet Juvenal quotes the phrase in Greek and states that the precept descended "e caelo" (from heaven) ("" 11.27). The 10th-century Byzantine encyclopedia the "Suda" recognized Chilon and Thales as the sources of the maxim "Know Thyself."

The authenticity of all such attributions is doubtful; according to Parke and Wormell (1956), "The actual authorship of the three maxims set up on the Delphian temple may be left uncertain. Most likely they were popular proverbs, which tended later to be attributed to particular sages."

Listed chronologically:
The ancient Greek playwright Aeschylus uses the maxim "know thyself" in his play "Prometheus Bound." The play about a mythological sequence, thereby places the maxim within the context of Greek mythology. In this play, the demi-god Prometheus first rails at the Olympian gods, and against what he believes to be the injustice of his having been bound to a cliffside by Zeus, king of the Olympian gods. The demi-god Oceanus comes to Prometheus to reason with him, and cautions him that he should "know thyself". In this context, Oceanus is telling Prometheus that he should know better than to speak ill of the one who decides his fate and accordingly, perhaps he should better know his place in the "great order of things."

One of Socrates's students, the historian Xenophon, described some of the instances of Socrates's use of the Delphic maxim 'Know Thyself' in his history titled: "Memorabilia." In this writing, Xenophon portrayed his teacher's use of the maxim as an organizing theme for Socrates's lengthy dialogue with Euthydemus.

Plato, another student of Socrates, employs the maxim 'Know Thyself' extensively by having the character of Socrates use it to motivate his dialogues. Benjamin Jowett's index to his translation of the "Dialogues of Plato" lists six dialogues which discuss or explore the Delphic maxim: 'know thyself.' These dialogues (and the Stephanus numbers indexing the pages where these discussions begin) are "Charmides" (164D), "Protagoras" (343B), "Phaedrus" (229E), "Philebus" (48C), "Laws" (II.923A), "Alcibiades I" (124A, 129A, 132C).

In Plato's "Charmides", Critias refers to the maxim consistently with the view expressed in the "Suda", with Critias saying, "for they imagined that 'Know Thyself!' was a piece of advice which the god gave and not his salutation of the worshippers at their first coming in." In modern words Critias gives his opinion that 'Know Thyself!' was an admonition to those entering the sacred temple to remember or know their place and Critias says, " 'know thyself!' and 'be temperate!' are the same. Notice that when the words of Critias 'thyself' and 'temperate' are punctuated with exclamation marks in the English translations, as if they were commands. In the balance of the "Charmides", Plato has Socrates lead a longer inquiry as to how we may gain knowledge of ourselves.

In Plato's "Phaedrus", Socrates uses the maxim 'know thyself' as his explanation to Phaedrus to explain why he has no time for the attempts to rationally explain mythology or other far flung topics. Socrates says, "But I have no leisure for them at all; and the reason, my friend, is this: I am not yet able, as the Delphic inscription has it, to know myself; so it seems to me ridiculous, when I do not yet know that, to investigate irrelevant things."

In Plato's "Protagoras", Socrates lauds the authors of pithy and concise sayings delivered precisely at the right moment and says that Lacedaemon, or Sparta, educates its people to that end. Socrates lists the Seven Sages as Thales, Pittacus, Bias, Solon, Cleobulus, Myson, and Chilon, who he says are gifted in that Lacedaemonian art of concise words "twisted together, like a bowstring, where a slight effort gives great force." Socrates says examples of them are, "the far-famed inscriptions, which are in all men's mouths,--'Know thyself,' and 'Nothing too much'.". Having lauded the maxims, Socrates then spends a great deal of time getting to the bottom of what one of them means, the saying of Pittacus, 'Hard is it to be good.' The irony here is that although the sayings of Delphi bear 'great force,' it is not clear how to live life in accordance with their meanings. Although, the concise and broad nature of the sayings suggests the active partaking in the usage and personal discovery of each maxim; as if the intended nature of the saying lay not in the words but the self-reflection and self-referencing of the person thereof.
In Plato's "Philebus" dialogue, Socrates refers back to the same usage of 'know thyself' from "Phaedrus" to build an example of the ridiculous for Protarchus. Socrates says, as he did in Phaedrus, that people make themselves appear ridiculous when they are trying to know obscure things before they know themselves. Plato also alluded to the fact that understanding 'thyself,' would have a greater yielded factor of understanding the nature of a human being. Syllogistically, understanding oneself would enable thyself to have an understanding of others as a result.

The "Suda", a 10th-century encyclopedia of Greek knowledge, says: "the proverb is applied to those whose boasts exceed what they are", and that "know thyself" is a warning to pay no attention to the opinion of the multitude.

One work by the Medieval philosopher Peter Abelard is entitled "Scito te ipsum" (“know yourself”) or "Ethica".

From 1539 onwards the phrase "nosce te ipsum" and its Latin variants were often used in the anonymous texts written for anatomical fugitive sheets printed in Venice as well as for later anatomical atlases printed throughout Europe. The 1530s fugitive sheets are the first instances in which the phrase was applied to knowledge of the human body attained through dissection.

In 1651 Thomas Hobbes used the term "nosce teipsum" which he translated as 'read thyself' in his famous work, "The Leviathan". He was responding to a popular philosophy at the time that you can learn more by studying others than you can from reading books. He asserts that one learns more by studying oneself: particularly the feelings that influence our thoughts and motivate our actions. As Hobbes states, "but to teach us that for the similitude of the thoughts and passions of one man, to the thoughts and passions of another, whosoever looketh into himself and considereth what he doth when he does think, opine, reason, hope, fear, etc., and upon what grounds; he shall thereby read and know what are the thoughts and passions of all other men upon the like occasions."

In 1734 Alexander Pope wrote a poem entitled "An Essay on Man, Epistle II", which begins "Know then thyself, presume not God to scan, The proper study of mankind is Man."

In 1735 Carl Linnaeus published the first edition of "Systema Naturae" in which he described humans ("Homo") with the simple phrase "Nosce te ipsum".

In 1750 Benjamin Franklin, in his "Poor Richard's Almanack", observed the great difficulty of knowing one's self, with: "There are three Things extremely hard, Steel, a Diamond, and to know one's self."

In 1754 Jean-Jacques Rousseau lauded the "inscription of the Temple at Delphi" in his "Discourse on the Origin of Inequality".

In 1831, Ralph Waldo Emerson wrote a poem entitled "Γνώθι Σεαυτόν", or Gnothi Seauton ('Know Thyself'), on the theme of 'God in thee.' The poem was an anthem to Emerson's belief that to 'know thyself' meant knowing the God which Emerson felt existed within each person.

In 1832 Samuel T. Coleridge wrote a poem entitled "Self Knowledge" in which the text centers on the Delphic maxim 'Know Thyself' beginning, 'Gnôthi seauton!--and is this the prime And heaven-sprung adage of the olden time!--' and ending with 'Ignore thyself, and strive to know thy God!' Coleridge's text references JUVENAL, xi. 27.

In 1902 Hugo von Hofmannsthal has his 16th-century alter ego in his letter to Francis Bacon mention a book he intended to call "Nosce te ipsum".

In 1904 Sarah Ida Shaw and Elanor Dorcas Pond founded the Delta Delta Delta sorority. Part of their motto is Self-Knowledge, Self-Reverence, and Self-Discipline.

The Wachowskis used one of the Latin versions ("temet nosce") of this aphorism as inscription over the Oracle's door in their movies "The Matrix" (1999) and "The Matrix Revolutions" (2003). The transgender character Nomi in the Netflix show Sense8, again directed by The Wachowskis, has a tattoo on her arm with the Greek version of this phrase.

LiveReal.com published 45 dimensions of the various ways a person can "know themselves."

"Know Thyself" is the motto of Hamilton College, of Lyceum International School (Nugegoda, Sri Lanka) and of İpek University (Ankara, Turkey). The Latin phrase "Nosce te ipsum" is the motto of Landmark College.

"Nosce te ipsum" is also the motto for the Scottish clan Thompson. It is featured on the family crest or coat of arms.

In the trilogy of novels “Double Exposure” by Piers Anthony, “Know thyself” plays a major role in the hero/protagonist’s journey.




</doc>
<doc id="13886123" url="https://en.wikipedia.org/wiki?curid=13886123" title="Noology">
Noology

Noology derives from the ancient Greek words νοῦς, "nous" or "mind" and λόγος, "logos". Noology thus outlines a systematic study and organization of thought, knowledge and the mind.

In the "Critique of Pure Reason", Immanuel Kant uses "noology" synonymously with rationalism, distinguishing it from empiricism: 

The Spanish philosopher Xavier Zubiri developed his own notion of noology.

The term is also used to describe the science of intellectual phenomena. It is the study of images of thought, their emergence, their genealogy, and their creation.




</doc>
<doc id="38724482" url="https://en.wikipedia.org/wiki?curid=38724482" title="Meta-Functional Expertise">
Meta-Functional Expertise

Meta-functional expertise is the breadth of one’s strategically important knowledge. This is different from the traditional conceptualization of expertise, which is generally considered to be a great depth of knowledge in a defined area. Thus, experts are people who are distinguished as knowing a lot about a particular subject.

Meta-functional experts, on the other hand, are considered be somewhat knowledgeable in many different areas but not necessarily an expert in any single domain. Someone high on meta-functional expertise is similar to a generalist in that they have a wide array of knowledge. However, where generalists know many different things meta-functional experts have enough depth of knowledge in each area to be considered knowledgeable by other members of their team at work.

Individuals high on meta-functional expertise are: 

Groups with more meta-functional experts on them perform better because they:


</doc>
<doc id="39380347" url="https://en.wikipedia.org/wiki?curid=39380347" title="Encyclopedic knowledge">
Encyclopedic knowledge

To have encyclopedic knowledge is to have "vast and complete" knowledge about a large number of diverse subjects. A person having such knowledge is called a human encyclopedia or a walking encyclopedia. 

It is no longer considered realistic, or feasible, for any one person to be truthfully described as having encyclopedic knowledge. The concept has been subsumed into the discourses on the production of knowledge and artificial intelligence. Instead, we are now preoccupied with knowledge bases distributed as software or web services.

The concept of encyclopedic knowledge was once attributed to exceptionally well-read or knowledgeable persons such as Plato, Aristotle, Hildegard von Bingen, Leonardo da Vinci, Immanuel Kant, or G. W. F. Hegel. Tom Rockmore described Hegel, for example, as a polymath and "a modern Aristotle, perhaps the last person to know everything of value that was known during his lifetime." Such persons are generally described as such based on their deep cognitive grasp of multiple and diverse fields of inquiry—an intellectually exceptional subset of philosophers who might also be differentiated from the multi-talented, the genius, or the "Renaissance man."

The idea of encyclopedic knowledge has made many appearances in popular culture, being especially widespread in detective fiction. In 1887, Sir Arthur Conan Doyle introduced his fictional master sleuth, Sherlock Holmes, who applied his keen deductive acumen and prodigious range of knowledge to solve his cases. "Encyclopedia Brown" is a series of books by Donald J. Sobol featuring the adventures of boy detective Leroy Brown, nicknamed "Encyclopedia" for his intelligence and range of knowledge that was first published in 1963.

One of the most celebrated is the fictional "Hitchhiker's Guide to the Galaxy" by the late Douglas Adams which began its evolution through numerous mediums as a British radio program in 1978. In 2004, NPR contributor A.J. Jacobs published "The Know-It-All", about his experience reading the entire "Encyclopædia Britannica" from start to finish.

While deep encyclopedic knowledge across numerous fields of inquiry by a single person is no longer feasible, encyclopedic knowledge within a field of inquiry or topic has great historical precedent and is still often ascribed to individuals. For example, it has been said of Raphael Lemkin that "his knowledge of the logic behind the Nazi war machine was encyclopedic."

In 1900, Alexander Graham Bell, who set out to read the entire "Encyclopædia Britannica" himself, served as the second president of the National Geographic Society and declared the Society should cover "the world and all that is in it." While this goal sounds all-encompassing, it is in fact a statement towards comprehensive geographic knowledge, meaning the scope of the National Geographic Society's enterprise should attempt to be terrestrially unbounded.

In an era of specialization, be it academic or functional or epistemological, obtaining "domain-specific" encyclopedic knowledge as an expert is typically celebrated and often rewarded by institutions in modern society. (This appreciation for having extensive niche knowledge, however, should not be confused with the historical experimentation and debate surrounding the "division of labor" which has been argued to limit the knowledge of workers compelled to perform repetitive tasks for the sake of an overall increase in economic productivity.)

Edward Said, in his seminal postcolonial work, "Orientalism", examines the encyclopedic endeavor in great detail, revealing it to be an historically . Orientalists' "unremitting ambition was to master "all" of a world, not some easily delimited part of it such as an author or a collection of texts."

Tim Chambers, an early Wikipedian who proposed the name "Wikipedia", has a page of historical interest in the Wikimedia archives entitled "The Value of Encyclopedic Knowledge" in which he describes a new model for growing encyclopedic knowledge "powered by "numerous" scholars around the world." The idea of encyclopedic knowledge being re-constellated as a community of knowledge is central to the theory of Connectivism as established by George Siemens and Stephen Downes.



</doc>
<doc id="46821647" url="https://en.wikipedia.org/wiki?curid=46821647" title="Internet science">
Internet science

Internet science is an interdisciplinary science that examines all aspects of the co-evolution in Internet networks and society. It works in the intersection of and in the gaps among a wide range of disciplines that have had to respond to the impact of the Internet on their 'home turf' and/or offer specific conceptual or methodological contributions. These include many natural sciences (e.g., complexity science, computer science, engineering, life sciences, mathematics, physics, psychology, statistics, systems and evolutionary biology), social sciences (e.g. anthropology, economics, philosophy, sociology, and political science), humanities (e.g., art, history, linguistics, literature and history) and some existing interdisciplines that cross traditional Faculty boundaries (e.g., technology, medicine, law). Professor Noshir Contractor and others have located it at the intersection of computational social science, network science, network engineering and Web science. By understanding the role of society in shaping Internet networks and being shaped by them Internet science aims to take care of the Internet in a way similar to that in which Web science aims to take care of the Web. The lingua franca in this interdisciplinary area include Internet standards and associated implementation, social processes, Internet infrastructure and policy.

Many disciplines support Internet science with different analysis tools, designs, and languages. To have a productive and effective dialogue between disciplines requires incentives for cooperation. The three main elements of Internet science are: multidisciplinary convergence, observability and constructive experimentation.

The European Commission funded a "Network of Excellence on Internet Science" (project acronym EINS) over the period December 2011-May 2015 under the FP7 funding programme. The Network in May 2015 had 48 member universities and research organisations and 180 individual affiliate researchers. Two major international Internet science conferences were held in April 2013 and May 2015 together with an unconference at the University of Bologna in May 2014 and official workshops at international academic conferences such as Human Behavior and the Evolution of Society and international inter-governmental and multistakeholder conferences such as the 2013 United Nations Internet Governance Forum.

Significant areas of current Internet science research include:

Net neutrality is the rule where Internet service providers should treat all the traffic on their networks equally. This means that companies should not slow down access or block any website content on the Web. In the United States, high-speed Internet service providers (ISPs), including AT&T, Comcast, Time Warner and Verizon, have sought support for a two-tiered Internet service model.

In 2014, President Obama announced a new plan to preserve "net neutrality" and to prevent Internet service providers from blocking or slowing websites or creating different tiers of speed. He said-, "No service should be stuck in a ‘slow lane’ because it does not pay a fee," he wrote in a statement. "That kind of gatekeeping would undermine the level playing field essential to the Internet’s growth."

Internet privacy (online privacy) is an opportunity of individuals to regulate the flow of information and have access to data, which is generated during a browsing session. Moreover, internet privacy may include some risks, like phishing, pharming, spyware and malware.

Google has signed two contracts with wind developers to power its data center in Finland with 100% renewable energy. Facebook decided to build a data center in Iowa, and has helped drive the local energy provider to scrap plans to build a nuclear power plant, and instead build a $2bn(£1.23bn) wind farm, which has led to the biggest single order of wind turbines on record.

Infrastructure provides a large range of vital services—such as the ability to move goods, people, and information. Infrastructural services like gas, electricity, water, transport, and banking are highly interconnected and mutually dependent in various complex ways. They are linked physically, and through important ICT systems, to prevent breakdowns from escalating into whole infrastructure failure.

There is ongoing activity on the development of Internet Science curricula, initially on a postgraduate level.

1934: The first person who imagined a 'Radiated Library' in 1934 was Paul Otlet.

1965: Two different computers started to communicate at MIT Lincoln Lab by using a packet-switching technology.

1968: Beranek and Newman have discovered an effectiveness and final version of the Interface Message Processor (IMP) specifications.

1969: The nodes were installed by UCLA’s Network Measurement Centre, Stanford Research Institute (SRI), University of California-Santa Barbara and University of Utah.

1972: Ray Tomlinson introduces a network e-mail system, the Internetworking Working Group (INWG) forms to address, which afterwards needs to be established for standard protocols.

1973: The term 'Internet' was born. Also a global networking becomes a reality as the University College of London (England) and Royal Radar Establishment (Norway), which connects to ARPANET.

1974: The first Internet Service Provider (ISP) was born with the introduction of a commercial version of ARPANET. This is also known as a 'Telenet'.

1974: Vinton Cerf and Bob Kahn have published "A Protocol for Packet Network Interconnection," which details the design of TCP.

1976: Queen Elizabeth II sends her first e-mail.

1979: USENET forms to host news and discussion groups.

1981: The National Science Foundation (NSF) provided a grant to demonstrate the Computer Science Network (CSNET) and afterwards to provide networking services to university computer scientists.

1982: Transmission Control Protocol (TCP) and Internet Protocol (IP) arise the protocol for ARPANET.

1983: The Domain Name System (DNS) established the familiar .edu, .gov, .com, .mil, .org, .net, and .int system to name websites.

1984: William Gibson was the first person who used the term "cyberspace."

1985: Symbolics.com, the website for Symbolics Computer Corp. in Massachusetts, was the first registered domain.

1986: The National Science Foundation’s NSFNET goes online to connected supercomputer centers at 56,000 bits per second — the speed of a typical dial-up computer modem.

1987: The number of hosts on the Internet exceeds 20,000. Cisco ships its first router.

1989: World.std.com becomes the first commercial provider of dial-up access to the Internet.

1990: Tim Berners-Lee develops HyperText Markup Language (HTML). This technology continues to have a large impact on ways how humans view and navigate the Internet in present days.

1991: CERN introduces the World Wide Web to the public.

1992: The first audio and video were distributed over the Internet. The phrase "surfing the Internet" was very popular.

1993: The number of websites reached 600 and the White House and United Nations go online.

1994: Netscape Communications was born. Microsoft created a Web browser for Windows 95.

1995: Compuserve, America Online and Prodigy began to provide Internet access.

1996: The browser war, primarily between the two major players Microsoft and Netscape, heated up.

1997: PC makers removed or hid Microsoft’s Internet software on new versions of Windows 95.

1998: The Google search engine was born and changed the way users engage with the Internet.

1999: The Netscape has been bought by AOL.

2000: The dot-com bubble bursted.

2001: A federal judge shouted down Napster.

2003. The SQL Slammer worm has spread worldwide in just 10 minutes.

2004: Facebook went online and the era of social networking began.

2005: YouTube.com has been launched.

2006: AOL changed its business model and offered the most services for free and relyied on advertising to generate revenue.

2009: 40th anniversary of the Internet.

2010: 400 million active users have been reached in Facebook.

2011: Twitter and Facebook played a large role in the Middle East revolts.


</doc>
<doc id="605511" url="https://en.wikipedia.org/wiki?curid=605511" title="Foreknowledge">
Foreknowledge

Foreknowledge is the concept of knowledge regarding future events.

Types of foreknowledge include:


</doc>
<doc id="1941913" url="https://en.wikipedia.org/wiki?curid=1941913" title="Self-knowledge (psychology)">
Self-knowledge (psychology)

Self-knowledge is a term used in psychology to describe the information that an individual draws upon when finding an answer to the question "What am I like?".

While seeking to develop the answer to this question, self-knowledge requires ongoing self-awareness and self-consciousness (which is not to be confused with consciousness). Young infants and chimpanzees display some of the traits of self-awareness and agency/contingency, yet they are not considered as also having self-consciousness. At some greater level of cognition, however, a self-conscious component emerges in addition to an increased self-awareness component, and then it becomes possible to ask "What am I like?", and to answer with self-knowledge.

Self-knowledge is a component of the self or, more accurately, the self-concept. It is the knowledge of oneself and one's properties and the "desire" to seek such knowledge that guide the development of the self-concept. Self-knowledge informs us of our mental representations of ourselves, which contain attributes that we uniquely pair with ourselves, and theories on whether these attributes are stable or dynamic.

The self-concept is thought to have three primary aspects:

The affective and executive selves are also known as the "felt" and "active" selves respectively, as they refer to the emotional and behavioral components of the self-concept.
Self-knowledge is linked to the cognitive self in that its motives guide our search to gain greater clarity and assurance that our own self-concept is an accurate representation of our "true self"; for this reason the cognitive self is also referred to as the "known self". The cognitive self is made up of everything we know (or "think we know" about ourselves). This implies physiological properties such as hair color, race, and height etc.; and psychological properties like beliefs, values, and dislikes to name but a few.

Self-knowledge and its structure affect how events we experience are encoded, how they are selectively retrieved/recalled, and what conclusions we draw from how we interpret the memory. The analytical interpretation of our own memory can also be called "meta memory", and is an important factor of "meta cognition".

The connection between our memory and our self-knowledge has been recognized for many years by leading minds in both philosophy and psychology, yet the precise specification of the relation remains a point of controversy.


Self-theories have traditionally failed to distinguish between different source that inform self-knowledge, these are "episodic memory" and "semantic memory". Both episodic and semantic memory are facets of "declarative memory", which contains memory of facts. Declarative memory is the explicit counterpart to "procedural memory", which is implicit in that it applies to skills we have learnt; they are not "facts" that can be "stated".

Episodic memory is the autobiographical memory that individuals possess which contains events, emotions, and knowledge associated with a given context.

Semantic memory does not refer to concept-based knowledge stored about a specific experience like episodic memory. Instead it includes the memory of meanings, understandings, general knowledge about the world, and factual information etc. This makes semantic knowledge independent of context and personal information. Semantic memory enables an individual to know information, including information about their selves, without having to consciously recall the experiences that taught them such knowledge.

People are able to maintain a sense of self that is supported by semantic knowledge of personal facts in the absence of direct access to the memories that describe the episodes on which the knowledge is based.
This evidence for the dissociation between episodic and semantic self-knowledge has made several things clear:

People have goals that lead them to seek, notice, and interpret information about themselves. These goals begin the quest for self-knowledge.
There are three primary motives that lead us in the search for self-knowledge:

Self-enhancement refers to the fact that people seem motivated to experience positive emotional states and to avoid experiencing negative emotional states. People are motivated to feel good about themselves in order to maximize their feelings of self-worth, thus enhancing their self-esteem.
The emphasis on "feelings" differs slightly from how other theories have previously defined self-enhancement needs, for example the "Contingencies of Self-Worth Model".
Other theorists have taken the term to mean that people are motivated to think about themselves in highly favorable terms, rather than feel they are "good".
In many situations and cultures, feelings of self-worth are promoted by thinking of oneself as highly capable or "better" than one's peers. However, in some situations and cultures, feelings of self-worth are promoted by thinking of oneself as "average" or even "worse" than others. In both cases, thoughts about the self still serve to enhance feelings of self-worth.
The universal need is not a need to think about oneself in any specific way, rather a need to maximize one's feelings of self-worth. This is the meaning of the self enhancement motive with respect to self-knowledge.

In Western societies, feelings of self-worth "are" in fact promoted by thinking of oneself in favorable terms.

See "Self-verification theory" section.

Accuracy needs influence the way in which people search for self-knowledge. People frequently wish to know the truth about themselves without regard as to whether they learn something positive or negative.
There are three considerations which underlie this need:
Accurate self-knowledge can also be instrumental in maximizing feelings of self-worth. Success is one of the number of things that make people feel good about themselves, and knowing what we are like can make successes more likely, so self-knowledge can again be adaptive. This is because self-enhancement needs can be met by knowing that one "can not" do something particularly well, thus protecting the person from pursuing a dead-end dream that is likely to end in failure.

Many theorists believe that we have a motive to protect the self-concept (and thus our self-knowledge) from change. This motive to have consistency leads people to look for and welcome information that is consistent with what they believe to be true about themselves; likewise, they will avoid and reject information which presents inconsistencies with their beliefs. This phenomenon is also known as self-verification theory.
Not everyone has been shown to pursue a self-consistency motive; but it has played an important role in various other influential theories, such as cognitive dissonance theory.

This theory was put forward by William Swann of the University of Texas at Austin in 1983 to put a name to the aforementioned phenomena. The theory states that once a person develops an idea about what they are like, they will strive to verify the accompanying self-views.
Two considerations are thought to drive the search for self-verifying feedback:
These factors of self-verification theory create controversy when persons suffering from low-self-esteem are taken into consideration. People who hold negative self-views about themselves "selectively seek negative feedback" in order to verify their self-views. This is in stark contrast to self-enhancement motives that suggest people are driven by the desire to feel good about themselves.

There are three sources of information available to an individual through which to search for knowledge about the self:

The physical world is generally a highly visible, and quite easily measurable source of information about one's self. Information one may be able to obtain from the physical world may include:


The comparative nature of self-views means that people rely heavily on the social world when seeking information about their selves. Two particular processes are important:

People compare attributes with others and draw inferences about what they themselves are like. However, the conclusions a person ultimately draws depend on whom in particular they compare themselves with. The need for accurate self-knowledge was originally thought to guide the social comparison process, and researchers assumed that comparing with others who are similar to us in the "important" ways is more informative.

People are also known to compare themselves with people who are slightly better off than they themselves are (known as an "upward comparison"); and with people who are slightly worse off or disadvantaged (known as a "downward comparison").
There is also substantial evidence that the need for "accurate" self-knowledge is neither the only, nor most important factor that guides the social comparison process, the need to feel good about ourselves affects the social comparison process.

Reflected appraisals occur when a person observes how others respond to them. The process was first explained by the sociologist Charles H. Cooley in 1902 as part of his discussion of the "looking-glass self", which describes how we see ourselves reflected in other peoples' eyes. He argued that a person's feelings towards themselves are socially determined via a three-step process:

"A self-idea of this sort seems to have three principled elements: the imagination of our appearance to the other person; the imagination of his judgment of that appearance; and some sort of self-feeling, such as pride or mortification. The comparison with a looking-glass hardly suggests the second element, the imagined judgment which is quite essential. The thing that moves us to pride or shame is not the mere mechanical reflection of ourselves, but an imputed sentiment, the imagined effect of this reflection upon another's mind." (Cooley, 1902, p. 153)

In simplified terms, Cooley's three stages are:
Note that this model is of a phenomenological nature.

In 1963, John W. Kinch adapted Cooley's model to explain how a person's "thoughts" about themselves develop rather than their "feelings".

Kinch's three stages were:
This model is also of a phenomenological approach.

Research has only revealed limited support for the models and various arguments raise their heads:

The sequence of reflected appraisals may accurately characterize patterns in early childhood due to the large amount of feedback infants receive from their parents, yet it appears to be less relevant later in life. This is because people are not passive, as the model assumes. People "actively" and "selectively" process information from the social world. Once a person's ideas about themselves take shape, these also influence the manner in which new information is gathered and interpreted, and thus the cycle continues.

The psychological world describes our "inner world". There are three processes that influence how people acquire knowledge about themselves:

Introspection involves looking inwards and directly consulting our attitudes, feelings and thoughts for meaning.
Consulting one's own thoughts and feelings can sometimes result in meaningful self-knowledge. The accuracy of introspection, however, has been called into question since the 1970s. Generally, introspection relies on people's explanatory theories of the self and their world, the accuracy of which is not necessarily related to the form of self-knowledge that they are attempting to assess. 

Comparing sources of introspection. People believe that spontaneous forms of thought provide more meaningful self-insight than more deliberate forms of thinking. Morewedge, Giblin, and Norton (2014) found that the more spontaneous a kind of thought, the more spontaneous a particular thought, and the more spontaneous thought a particular thought was perceived to be, the more insight into the self it was attributed. In addition, the more meaning the thought was attributed, the more the particular thought influenced their judgment and decision making. People asked to let their mind wander until they randomly thought of a person to whom they were attracted to, for example, reported that the person they identified provided them with more self-insight than people asked to simply think of a person to whom they were attracted to. Moreover, the greater self-insight attributed to the person identified by the (former) random thought process than by the latter deliberate thought process led those people in the random condition to report feeling more attracted to the person they identified.

Whether introspection always fosters self-insight is not entirely clear. Thinking too much about why we feel the way we do about something can sometimes confuse us and undermine true self-knowledge. Participants in an introspection condition are less accurate when predicting their own future behavior than controls and are less satisfied with their choices and decisions. In addition, it is important to notice that introspection allows the exploration of the conscious mind only, and does not take into account the unconscious motives and processes, as found and formulated by Freud. 

Wilson's work is based on the assumption that people are not always aware of "why" they feel the way they do. Bem's self-perception theory makes a similar assumption.
The theory is concerned with how people "explain" their behavior. It argues that people don't always "know" why they do what they do. When this occurs, they infer the causes of their behavior by analyzing their behavior in the context in which it occurred. Outside observers of the behavior would reach a similar conclusion as the individual performing it. The individuals then draw logical conclusions about why they behaved as they did.

"Individuals come to "know" their own attitudes, emotions, and other internal states partially by inferring them from observations of their own overt behavior and/or the circumstances in which this behavior occurs. Thus, to the extent that internal cues are weak, ambiguous, or uninterpretable, the individual is functionally in the same position as an outside observer, an observer who must necessarily rely upon those same external cues to infer the individual's inner states." (Bem, 1972, p.2)

The theory has been applied to a wide range of phenomena. Under particular conditions, people have been shown to infer their attitudes, emotions, and motives, in the same manner described by the theory.

Similar to introspection, but with an important difference: with introspection we "directly examine" our attitudes, feelings and motives. With self-perception processes we "indirectly infer" our attitudes, feelings, and motives by "analyzing our behavior".

Causal attributions are an important source of self-knowledge, especially when people make attributions for positive and negative events. The key elements in self-perception theory are explanations people give for their actions, these explanations are known as causal attributions.

Causal attributions provide answers to "Why?" questions by attributing a person's behavior (including our own) to a cause.

People also gain self-knowledge by making attributions for "other people's" behavior; for example "If nobody wants to spend time with me it must be because I'm boring".

Individuals think of themselves in many different ways, yet only some of these ideas are active at any one given time. The idea that is specifically active at a given time is known as the Current Self-Representation. Other theorists have referred to the same thing in several different ways:
The current self-representation influences information processing, emotion, and behavior and is influenced by both "personal" and "situational" factors.

Self-concept, or how people "usually" think of themselves is the most important personal factor that influences current self-representation. This is especially true for attributes that are important and self-defining.

Self-concept is also known as the self-schema, made of innumerable smaller self-schemas that are "chronically accessible".

Self-esteem affects the way people feel about themselves. People with high self-esteem are more likely to be thinking of themselves in positive terms at a given time than people suffering low self-esteem.

Mood state influences the accessibility of positive and negative self-views.

When we are happy we tend to think more about our positive qualities and attributes, whereas when we are sad our negative qualities and attributes become more accessible.

This link is particularly strong for people suffering low self-esteem.

People can deliberately activate particular self-views. We select appropriate images of ourselves depending on what role we wish to play in a given situation.

One particular goal that influences activation of self-views is the desire to feel good.

How a person thinks of themselves depends largely on the social role they are playing. Social roles influence our personal identities.

People tend to think of themselves in ways that distinguish them from their social surroundings.
Distinctiveness also influences the salience of group identities.

The size of the group affects the salience of group-identities. Minority groups are more distinctive, so group identity should be more salient among minority group members than majority group members.

Group status interacts with group size to affect the salience of social identities.

The social environment has an influence on the way people evaluate themselves as a result of social-comparison processes.

People regard themselves as at the opposite end of the spectrum of a given trait to the people in their company. However, this effect has come under criticism as to whether it is a primary effect, as it seems to share space with the assimilation effect, which states that people evaluate themselves more positively when they are in the company of others who are exemplary on some dimension.

Imagining how one appears to others has an effect on how one thinks about oneself.

Recent events can cue particular views of the self, either as a direct result of failure, or via mood.
Memory for prior events influence how people think about themselves.







</doc>
<doc id="49870344" url="https://en.wikipedia.org/wiki?curid=49870344" title="Creative computing">
Creative computing

Creative computing covers the interdisciplinary area at the cross-over of the creative arts and computing. Issues of creativity include knowledge discovery, for example. 

The "International Journal of Creative Computing" describes creative computing as follows:

Creative computing is interdisciplinary in nature and topics relating to it include applications, development method, evaluation, modeling, philosophy, principles, support environment, and theory.

The term "creative computing" is used both in the United Kingdom and the United States (e.g., at Harvard University and MIT).

A number of university degree programmes in Creative Computing exist, for example at:

The "International Journal of Creative Computing" is a quarterly peer-reviewed scientific journal published by Inderscience Publishers, covering creativity in computing and the other way around. The editor-in-chief is Hongji Yang (Bath Spa University).

The journal was established in 2013 and is abstracted and indexed in CSA, ProQuest, and DBLP databases. As of 2019, the journal appears to be defunct.


</doc>
<doc id="9106784" url="https://en.wikipedia.org/wiki?curid=9106784" title="Open knowledge">
Open knowledge

Open knowledge also known as free knowledge is knowledge that one is free to use, reuse, and redistribute without legal, social or technological restriction. Open knowledge is a set of principles and methodologies related to the production and distribution of how knowledge works in an "open" manner. Knowledge is interpreted broadly to include data, content and general information.

The concept is related to open source and the "Open Knowledge Definition" is directly derived from the Open Source Definition. Open knowledge can be seen as being a superset of open data, open content and libre open access with the aim of highlighting the commonalities between these different groups.

Similarly to other 'open' concepts such as open data and open content, though the term is rather new, the concept is old. For example, one of the earliest printed texts of which we have record is a copy of the Buddhist Diamond sutra produced in China around 868 AD, and in it can be found the dedication: "for universal free distribution".

In the early twentieth century a debate about intellectual property rights developed within the German Social Democratic Party. A key contributor was Karl Kautsky who in 1902 devoted a section of a pamphlet to "Intellectual Production" which he distinguished from material production:

This view was based on an analysis according to which Karl Marx's Law of value only affected material production, not intellectual production.

With the development of the public Internet from the early 1990s, it became far easier to copy and share information across the world. The phrase 'information wants to be free' became a rallying cry for people who wanted to create an internet without the commercial barriers that they felt inhibited creative expression in traditional material production.

Wikipedia was founded in 2001 with the ethos of providing information which could be edited and modified to improve its quality. The success of Wikipedia became instrumental in making open knowledge something that millions of people interacted with and contributed to.



</doc>
<doc id="7030491" url="https://en.wikipedia.org/wiki?curid=7030491" title="Pantomath">
Pantomath

A pantomath is a person who wants to know and knows everything. The word itself is not to be found in common online English dictionaries, the "OED", dictionaries of obscure words, or dictionaries of neologisms.

Logic dictates that there are no literal nonfictional pantomaths, but the word pantomath seems to have been used to imply a polymath in a superlative sense, a "ne plus ultra" ("nothing more beyond") as it were, one who satisfies requirements even stricter than those to be applied to the polymath. In theory, a pantomath is not to be confused with a polymath in its less strict sense, much less with the related but very different terms philomath and know-it-all.

A pantomath ("pantomathēs", παντομαθής, meaning "having learnt all", from the Greek roots παντ- "all", "every" and the root μαθ-, meaning "learning", "understanding") is a person whose astonishingly wide interests and knowledge span the entire range of the arts and sciences.

Pantomath is typically used to convey the sense that a great individual has achieved a pinnacle of learning, that an "automath" has taken autodidacticism to an endpoint. As an example, the obscure and rare term seems to have been applied to those with an astonishingly wide knowledge and interests by these two authors from different eras: G. M. Young has been called a pantomath, as has Rupert Hart-Davis.

According to a critical view, Goethe's monumental breadth of knowledge and accomplishments, together with his serene, supernal wisdom, a wisdom which has been described as aloof, even inhuman, made him worthy of the denomination Olympian.



</doc>
<doc id="22781" url="https://en.wikipedia.org/wiki?curid=22781" title="Omniscience">
Omniscience

Omniscience () is the capacity to know everything. In monotheistic religions, such as Sikhism and the Abrahamic religions, this is an attribute of God. In Jainism, omniscience is an attribute that any individual can eventually attain. In Buddhism, there are differing beliefs about omniscience among different schools.

The topic of omniscience has been much debated in various Indian traditions, but no more so than by the Buddhists. After Dharmakirti's excursions into the subject of what constitutes a valid cognition, Śāntarakṣita and his student Kamalaśīla thoroughly investigated the subject in the Tattvasamgraha and its commentary the Panjika. The arguments in the text can be broadly grouped into four sections:

Some modern Christian theologians argue that God's omniscience is inherent rather than total, and that God chooses to limit his omniscience in order to preserve the free will and dignity of his creatures. John Calvin, among other theologians of the 16th century, comfortable with the definition of God as being omniscient in the total sense, in order for worthy beings' abilities to choose freely, embraced the doctrine of predestination.

In Islam, Allah is attributed with absolute omniscience. He knows the past, the present and the future. It is compulsory for a Muslim to believe that Allah is indeed omniscient as stated in one of the six articles of faith which is:

It is however, believed that human can only change their predestination (wealth, health, deed etc.) and not divine decree (date of birth, date of death, family etc.), thus allowing free wills.

In Jainism, omniscience is considered the highest type of perception. In the words of a Jain scholar,
"The perfect manifestation of the innate nature of the self, arising on the complete annihilation of the obstructive veils, is called omniscience."

Jainism views infinite knowledge as an inherent capability of every soul. "Arihanta" is the word used by Jains to refer to those human beings who have conquered all inner passions (like attachment, greed, pride, anger) and possess "Kevala Jnana" (infinite knowledge). They are said to be of two kinds:

Whether omniscience, particularly regarding the choices that a human will make, is compatible with free will has been debated by theologians and philosophers. The argument that divine foreknowledge is not compatible with free will is known as theological fatalism. It is argued that if humans are free to choose between alternatives, God could not know what this choice will be.

A question arises: if an omniscient entity knows everything, even about its own decisions in the future, does it therefore forbid any free will to that entity? William Lane Craig states that the question subdivides into two:

However, this kind of argument fails to recognize its use of the modal fallacy. It is possible to show that the first premise of arguments like these is fallacious. 

Some philosophers, such as Patrick Grim, Linda Zagzebski, Stephan Torre and William Mander have discussed the issue of whether the apparent exclusively first-person nature of conscious experience is compatible with God's omniscience. There is a strong sense in which conscious experience is private, meaning that no outside observer can gain knowledge of what it is like to be me "as me". If a subject cannot know what it is like to be another subject in an objective manner, the question is whether that limitation applies to God as well. If it does, then God cannot be said to be omniscient since there is then a form of knowledge that God lacks access to.

The philosopher Patrick Grim most notably raised this issue. Linda Zagzebski tried to avoid it by introducing the notion of "perfect empathy", a proposed relation that God can have to subjects that would allow God to have perfect knowledge of their conscious experience. William Mander argued that God can only have such knowledge if our experiences are part of God's broader experience. Stephan Torre claimed that God can have such knowledge if self-knowledge involves the ascription of properties, either to oneself or to others. Patrick Grim saw this line of reasoning as a motivation for accepting atheism.





</doc>
<doc id="49304088" url="https://en.wikipedia.org/wiki?curid=49304088" title="Book desert">
Book desert

A book desert is a geographic area (country, state, county, city, neighborhood, home) where printed books and other reading material are allegedly hard to obtain, particularly without access to an automobile or other form of transportation. Some researchers have defined book deserts by linking them to poverty and low income, while others use a combination of factors that include census data, income, ethnicity, geography, language, and the number of books in a home.

Initiatives that increase the availability of books by such measures as bookmobiles and librarians on bicycles have been offered as possible solutions to book deserts, as have Little Free Libraries and offering children's literature available online, free of charge.

In the past, researchers have studied how the absence or scarcity of books impact how a child's early literacy and language skills develop. The term "book desert" came into regular use in the mid-2010s and the social enterprise Unite for Literacy is credited as having coined the term. Unite for Literacy created an operational definition of a book desert when they published the U.S. Book Desert Map: A geographic area (country, state, county, census tract) where it is predicted that a majority of homes have less than 100 printed books. In March 2014, James LaRue, director of the American Library Association's Office for Intellectual Freedom and the Freedom to Read Foundation, used the term in an issue of "American Libraries", where he described the term as applying to houses with 25 or fewer books in them and discussed ways to lessen or eradicate the problem.

In July 2016, professors Susan B. Neuman and Naomi Moland published a study in "Urban Education", where they examined how the lack of printed reading material among low-income and poverty stricken neighborhoods impacts early childhood development and used the term to describe areas and homes with little access to written materials. This study built upon other research Neuman had conducted in 2001 on the same topic and the researchers found few stores in Detroit, Los Angeles, and Washington, D.C.; the focus areas of their research had print resources for children ages 0 through 18. Of those stores, many were dollar stores. "The Atlantic" reported that in 2015 Neuman and JetBlue Airways held an experiment to foster literacy by providing book vending machines in a low-income Washington D.C. neighborhood. Over 20,000 books were given away, prompting Neuman to conclude that the neighborhood's parents did care about their children's education but lacked "the resources to enable their children to be successful."

Multiple factors are credited as contributing to the formation of book deserts, the most frequently highlighted of which tends to be poverty and low income. Other factors tend to include language and geography, as some areas lack access to bookstores or public or community school libraries that would provide books. Book store closures due to bankruptcy or other financial difficulties are also occasionally cited as a contributing factor, when the closure leaves the area without a bookseller.

Unite for Literacy has developed a book desert map of the United States powered by Esri's ArcGIS platform, which provides a visual presentation of the lack of books in the nation, states, counties and census tracts. To create the map, Unite for Literacy performed a statistical analyses of data from the National Assessment of Educational Progress and the American Community Survey. Data used in the map includes the number of books in 4th graders' homes, average community income, ethnic diversity, geographic location and home language. Unite for Literacy unveiled the map during the Clinton Global Initiative America (CGI America) meeting held in Denver, Colorado, in June 2014.



</doc>
<doc id="33570327" url="https://en.wikipedia.org/wiki?curid=33570327" title="History of knowledge">
History of knowledge

The history of knowledge is the field covering the accumulated and known human knowledge created or discovered during the history of the world and its historic forms, focus, accumulation, bearers, impacts, mediations, distribution, applications, societal contexts, conditions and methods of production. It is related to, separate from the history of science, the history of scholarship and the history of philosophy. The scope of the history of knowledge encompass all the discovered and created fields of human derived knowledge such as logic, philosophy, mathematics, science, sociology, psychology, data mining etc.


Burke, Peter. "Writing the Social History of Knowledge." Theory, Culture & Society, December 21, 2010.

History of Knowledge (blog), http://historyofknowledge.net

Lässig, Simone. "The History of Knowledge and the Expansion of the Historical Research Agenda." "Bulletin of the German Historical Institute" (Fall 2016): 29–58.


</doc>
<doc id="9892" url="https://en.wikipedia.org/wiki?curid=9892" title="Expert">
Expert

An expert is someone who has a broad and deep competence in terms of knowledge, skill and experience through practice and education in a particular field. Informally, an expert is someone widely recognized as a reliable source of technique or skill whose faculty for judging or deciding rightly, justly, or wisely is accorded authority and status by peers or the public in a specific well-distinguished domain. An expert, more generally, is a person with extensive knowledge or ability based on research, experience, or occupation and in a particular area of study. Experts are called in for advice on their respective subject, but they do not always agree on the particulars of a field of study. An expert can be believed, by virtue of credentials, training, education, profession, publication or experience, to have special knowledge of a subject beyond that of the average person, sufficient that others may officially (and legally) rely upon the individual's opinion on that topic. Historically, an expert was referred to as a sage (Sophos). The individual was usually a profound thinker distinguished for wisdom and sound judgment.

In specific fields, the definition of expert is well established by consensus and therefore it is not always necessary for individuals to have a professional or academic qualification for them to be accepted as an expert. In this respect, a shepherd with 50 years of experience tending flocks would be widely recognized as having complete expertise in the use and training of sheep dogs and the care of sheep. Another example from computer science is that an expert system may be taught by a human and thereafter considered an expert, often outperforming human beings at particular tasks. In law, an expert witness must be recognized by argument and authority.

Research in this area attempts to understand the relation between expert knowledge, skills and personal characteristics and exceptional performance. Some researchers have investigated the cognitive structures and processes of experts. The fundamental aim of this research is to describe what it is that experts know and how they use their knowledge to achieve performance that most people assume requires extreme or extraordinary ability. Studies have investigated the factors that enable experts to be fast and accurate.

Expertise characteristics, skills and knowledge of a person (that is, expert) or of a system, which distinguish experts from novices and less experienced people. In many domains there are objective measures of performance capable of distinguishing experts from novices: expert chess players will almost always win games against recreational chess players; expert medical specialists are more likely to diagnose a disease correctly; etc.

The word expertise is used to refer also to Expert Determination, where an expert is invited to decide a disputed issue. The decision may be binding or advisory, according to the agreement between the parties in dispute.

There are two academic approaches to the understanding and study of expertise. The first understands expertise as an emergent property of communities of practice. In this view expertise is socially constructed; tools for thinking and scripts for action are jointly constructed within social groups enabling that group jointly to define and acquire expertise in some domain.

In the second view expertise is a characteristic of individuals and is a consequence of the human capacity for extensive adaptation to physical and social environments. Many accounts of the development of expertise emphasize that it comes about through long periods of deliberate practice. In many domains of expertise estimates of 10 years' experience deliberate practice are common. Recent research on expertise emphasizes the nurture side of the nature and nurture argument. Some factors not fitting the nature-nurture dichotomy are biological but not genetic, such as starting age, handedness, and season of birth.

In the field of education there is a potential "expert blind spot" (see also Dunning–Kruger effect) in newly practicing educators who are experts in their content area. This is based on the "expert blind spot hypothesis" researched by Mitchell Nathan and Andrew Petrosino (2003: 906). Newly practicing educators with advanced subject-area expertise of an educational content area tend to use the formalities and analysis methods of their particular area of expertise as a major guiding factor of student instruction and knowledge development, rather than being guided by student learning and developmental needs that are prevalent among novice learners.

The blind spot metaphor refers to the physiological blind spot in human vision in which perceptions of surroundings and circumstances are strongly impacted by their expectations. Beginning practicing educators tend to overlook the importance of novice levels of prior knowledge and other factors involved in adjusting and adapting pedagogy for learner understanding. This expert blind spot is in part due to an assumption that novices’ cognitive schemata are less elaborate, interconnected, and accessible than experts’ and that their pedagogical reasoning skills are less well developed (Borko & Livingston, 1989: 474). Essential knowledge of subject matter for practicing educators consists of overlapping knowledge domains: subject matter knowledge and pedagogical content matter (Borko, Eisenhart, Brown, Underhill, Jones, & Agard, 1992: 195). Pedagogical content matter consists of an understanding of how to represent certain concepts in ways appropriate to the learner contexts, including abilities and interests. The expert blind spot is a pedagogical phenomenon that is typically overcome through educators’ experience with instructing learners over time.
In line with the socially constructed view of expertise, expertise can also be understood as a form of power; that is, experts have the ability to influence others as a result of their defined social status. By a similar token, a fear of experts can arise from fear of an intellectual elite's power. In earlier periods of history, simply being able to read made one part of an intellectual elite. The introduction of the printing press in Europe during the fifteenth century and the diffusion of printed matter contributed to higher literacy rates and wider access to the once-rarefied knowledge of academia. The subsequent spread of education and learning changed society, and initiated an era of widespread education whose elite would now instead be those who produced the written content itself for consumption, in education and all other spheres.

Plato's "Noble Lie", concerns expertise. Plato did not believe most people were clever enough to look after their own and society's best interest, so the few clever people of the world needed to lead the rest of the flock. Therefore, the idea was born that only the elite should know the truth in its complete form and the rulers, Plato said, must tell the people of the city "the noble lie" to keep them passive and content, without the risk of upheaval and unrest.

In contemporary society, doctors and scientists, for example, are considered to be experts in that they hold a body of dominant knowledge that is, on the whole, inaccessible to the layman. However, this inaccessibility and perhaps even mystery that surrounds expertise does not cause the layman to disregard the opinion of the experts on account of the unknown. Instead, the complete opposite occurs whereby members of the public believe in and highly value the opinion of medical professionals or of scientific discoveries, despite not understanding it.

A number of computational models have been developed in cognitive science to explain the development from novice to expert. In particular, Herbert A. Simon and Kevin Gilmartin proposed a model of learning in chess called MAPP (Memory-Aided Pattern Recognizer). Based on simulations, they estimated that about 50,000 chunks (units of memory) are necessary to become an expert, and hence the many years needed to reach this level. More recently, the CHREST model (Chunk Hierarchy and REtrieval STructures) has simulated in detail a number of phenomena in chess expertise (eye movements, performance in a variety of memory tasks, development from novice to expert) and in other domains.

An important feature of expert performance seems to be the way in which experts are able to rapidly retrieve complex configurations of information from long-term memory. They recognize situations because they have meaning. It is perhaps this central concern with meaning and how it attaches to situations which provides an important link between the individual and social approaches to the development of expertise. Work on "Skilled Memory and Expertise" by Anders Ericsson and James J. Staszewski confronts the paradox of expertise and claims that people not only acquire content knowledge as they practice cognitive skills, they also develop mechanisms that enable them to use a large and familiar knowledge base efficiently.

Work on expert systems (computer software designed to provide an answer to a problem, or clarify uncertainties where normally one or more human experts would need to be consulted) typically is grounded on the premise that expertise is based on acquired repertoires of rules and frameworks for decision making which can be elicited as the basis for computer supported judgment and decision-making. However, there is increasing evidence that expertise does not work in this fashion. Rather, experts recognize situations based on experience of many prior situations. They are in consequence able to make rapid decisions in complex and dynamic situations.

In a critique of the expert systems literature suggest:

The role of long term memory in the skilled memory effect was first articulated by Chase and Simon in their classic studies of chess expertise. They asserted that organized patterns of information stored in long term memory (chunks) mediated experts' rapid encoding and superior retention. Their study revealed that all subjects retrieved about the same number of chunks, but the size of the chunks varied with subjects' prior experience. Experts' chunks contained more individual pieces than those of novices. This research did not investigate how experts find, distinguish, and retrieve the right chunks from the vast number they hold without a lengthy search of long term memory.

Skilled memory enables experts to rapidly encode, store, and retrieve information within the domain of their expertise and thereby circumvent the capacity limitations that typically constrain novice performance. For example, it explains experts' ability to recall large amounts of material displayed for only brief study intervals, provided that the material comes from their domain of expertise. When unfamiliar material (not from their domain of expertise) is presented to experts, their recall is no better than that of novices.

The first principle of skilled memory, the "meaningful encoding principle," states that experts exploit prior knowledge to durably encode information needed to perform a familiar task successfully. Experts form more elaborate and accessible memory representations than novices. The elaborate semantic memory network creates meaningful memory codes that create multiple potential cues and avenues for retrieval.

The second principle, the "retrieval structure principle" states that experts develop memory mechanisms called retrieval structures to facilitate the retrieval of information stored in long term memory. These mechanisms operate in a fashion consistent with the meaningful encoding principle to provide cues that can later be regenerated to retrieve the stored information efficiently without a lengthy search.

The third principle, the "speed up principle" states that long term memory encoding and retrieval operations speed up with practice, so that their speed and accuracy approach the speed and accuracy of short term memory storage and retrieval.

Examples of skilled memory research described within the Ericsson and Stasewski study include:

Much of the research regarding expertise involves the studies of how experts and novices differ in solving problems (Chi, M. T. H., Glasser R., & Rees, E.,1982). Mathematics (Sweller, J., Mawer, R. F., & Ward, M. R., 1983) and physics (Chi, Feltovich, & Glaser, 1981) are common domains for these studies.

One of the most cited works in this area, Chi et al. (1981), examines how experts (PhD students in physics) and novices (undergraduate students that completed one semester of mechanics) categorize and represent physics problems. They found that novices sort problems into categories based upon surface features (e.g., keywords in the problem statement or visual configurations of the objects depicted). Experts, however, categorize problems based upon their deep structures (i.e., the main physics principle used to solve the problem).

Their findings also suggest that while the schemas of both novices and experts are activated by the same features of a problem statement, the experts’ schemas contain more procedural knowledge which aid in determining which principle to apply, and novices’ schemas contain mostly declarative knowledge which do not aid in determining methods for solution.

Relative to a specific field, an expert has:

Marie-Line Germain (Germain, 2006) developed a psychometric measure of perception of employee expertise called the Generalized Expertise Measure (GEM). She defined a behavioral dimension in experts, in addition to the dimensions suggested by Swanson and Holton (2001). Her 16-item scale contains objective expertise items and subjective expertise items. Objective items were named Evidence-Based items. Subjective items (the remaining 11 items from the measure below) were named Self-Enhancement items because of their behavioral component.

(Condensed from Germain, 2006).


Scholars in rhetoric have also turned their attention to the concept of the expert. Considered an appeal to ethos or "the personal character of the speaker", established expertise allows a speaker to make statements regarding special topics of which the audience may be ignorant. In other words, the expert enjoys the deference of the audience's judgment and can appeal to authority where a non-expert cannot.

In The Rhetoric of Expertise, E. Johanna Hartelius defines two basic modes of expertise: autonomous and attributed expertise. While an autonomous expert can "possess expert knowledge without recognition from other people," attributed expertise is "a performance that may or may not indicate genuine knowledge." With these two categories, Hartelius isolates the rhetorical problems faced by experts: just as someone with autonomous expertise may not possess the skill to persuade people to hold their points of view, someone with merely attributed expertise may be persuasive but lack the actual knowledge pertaining to a given subject. The problem faced by audiences follows from the problem facing experts: when faced with competing claims of expertise, what resources do non-experts have to evaluate claims put before them?

Hartelius and other scholars have also noted the challenges that projects such as Wikipedia pose to how experts have traditionally constructed their authority. In "Wikipedia and the Emergence of Dialogic Expertise", she highlights Wikipedia as an example of the "dialogic expertise" made possible by collaborative digital spaces. Predicated upon the notion that "truth emerges from dialogue", Wikipedia challenges traditional expertise both because anyone can edit it and because no single person, regardless of their credentials, can end a discussion by fiat. In other words, the community, rather than single individuals, direct the course of discussion. The production of knowledge, then, as a process of dialogue and argumentation, becomes an inherently rhetorical activity.

Hartelius calls attention to two competing norm systems of expertise: “network norms of dialogic collaboration” and “deferential norms of socially sanctioned professionalism”; Wikipedia being evidence of the first. Drawing on a Bakhtinian framework, Hartelius posits that Wikipedia is an example of an epistemic network that is driven by the view that individuals’ ideas clash with one another so as to generate expertise collaboratively. Hartelius compares Wikipedia's methodology of open-ended discussions of topics to that of Bakhtin's theory of speech communication, where genuine dialogue is considered a live event, which is continuously open to new additions and participants. Hartelius acknowledges that knowledge, experience, training, skill, and qualification are important dimensions of expertise but posits that the concept is more complex than sociologists and psychologists suggest. Arguing that expertise is rhetorical, then, Hartelius explains that expertise: “(...) is not simply about one person’s skills being different from another’s. It is also fundamentally contingent on a struggle for ownership and legitimacy.”. Effective communication is an inherent element in expertise in the same style as knowledge is. Rather than leaving each other out, substance and communicative style are complementary. Hartelius further suggests that Wikipedia's dialogic construction of expertise illustrates both the instrumental and the constitutive dimensions of rhetoric; instrumentally as it challenges traditional encyclopedias and constitutively as a function of its knowledge production. Going over the historical development of the encyclopedic project, Hartelius argues that changes in traditional encyclopedias have led to changes in traditional expertise. Wikipedia's use of hyperlinks to connect one topic to another depends on, and develops, electronic interactivity meaning that Wikipedia's way of knowing is dialogic. Dialogic expertise then, emerges from multiple interactions between utterances within the discourse community. The ongoing dialogue between contributors on Wikipedia not only results in the emergence of truth; it also explicates the topics one can be an expert of. As Hartelius explains: “The very act of presenting information about topics that are not included in traditional encyclopedias is a construction of new expertise.”. While Wikipedia insists that contributors must only publish preexisting knowledge, the dynamics behind dialogic expertise creates new information nonetheless. Knowledge production is created as a function of dialogue. According to Hartelius, dialogic expertise has emerged on Wikipedia not only because of its interactive structure but also because of the site's hortative discourse which is not found in traditional encyclopedias. By Wikipedia's hortative discourse, Hartelius means various encouragements to edit certain topics and instructions on how to do so that appear on the site. One further reason to the emergence of dialogic expertise on Wikipedia is the site's , which function as a techne; explicating Wikipedia's expert methodology.

Building on Hartelius, Damien Pfister developed the concept of "networked expertise". Noting that Wikipedia employs a "many to many" rather than a "one to one" model of communication, he notes how expertise likewise shifts to become a quality of a group rather than an individual. With the information traditionally associated with individual experts now stored within a text produced by a collective, knowing about something is less important than knowing how to find something. As he puts it, "With the internet, the historical power of subject matter expertise is eroded: the archival nature of the Web means that what and how to information is readily available." The rhetorical authority previously afforded to subject matter expertise, then, is given to those with the procedural knowledge of how to find information called for by a situation.

An expert differs from the specialist in that a specialist has to "be able to solve" a problem and an expert has to "know its solution". The opposite of an expert is generally known as a layperson, while someone who occupies a middle grade of understanding is generally known as a technician and often employed to assist experts. A person may well be an expert in one field and a layperson in many other fields. The concepts of experts and expertise are debated within the field of epistemology under the general heading of expert knowledge. In contrast, the opposite of a specialist would be a generalist or polymath.

The term is widely used informally, with people being described as 'experts' in order to bolster the relative value of their opinion, when no objective criteria for their expertise is available. The term crank is likewise used to disparage opinions. Academic elitism arises when experts become convinced that only their opinion is useful, sometimes on matters beyond their personal expertise.

In contrast to an expert, a novice (known colloquially as a newbie or 'greenhorn') is any person that is new to any science or field of study or activity or social cause and who is undergoing training in order to meet normal requirements of being regarded a mature and equal participant.

"Expert" is also being mistakenly interchanged with the term "authority" in new media. An expert can be an authority if through relationships to people and technology, that expert is allowed to control access to his expertise. However, a person who merely wields authority is not by right an expert. In new media, users are being misled by the term "authority". Many sites and search engines such as Google and Technorati use the term "authority" to denote the link value and traffic to a particular topic. However, this authority only measures populist information. It in no way assures that the author of that site or blog is an expert.

Some characteristics of the development of an expert have been found to include

Mark Twain defined an expert as "an ordinary fellow from another town". Will Rogers described an expert as "A man fifty miles from home with a briefcase." Danish scientist and Nobel laureate Niels Bohr defined an expert as "A person that has made every possible mistake within his or her field."
Malcolm Gladwell describes expertise as a matter of practicing the correct way for a total of around 10,000 hours.








</doc>
<doc id="3009184" url="https://en.wikipedia.org/wiki?curid=3009184" title="Discernment">
Discernment

Discernment is the ability to obtain sharp perceptions or to judge well (or the activity of so doing). In the case of judgement, discernment can be psychological, moral or aesthetic in nature. Within judgment, discernment 
involves going past the mere perception of something and making nuanced judgments about its properties or qualities. Considered as a virtue, a discerning individual is considered to possess wisdom, and be of good judgement; especially so with regard to subject matter often overlooked by others. Discernment can be scientific (that is discerning what is true about the real world), normative (discerning value including what ought to be) and formal (deductive reasoning). 

In Christianity, the word may have several meanings. Discernment can describe the process of determining God's desire in a situation or for one's life or identifying the true nature of a thing, such as discerning whether a thing is good, evil, or may even transcend the limiting notion of duality. In large part, it describes the interior search for an answer to the question of one's vocation, namely, determining whether or not God is calling one to the married life, single life, consecrated life, ordained ministry or any other calling.

Discernment of Spirits is a term used in both Roman Catholic and Charismatic (Pentacostal) Christian theology to indicate judging various spiritual agents for their moral influence.

The process of individual discernment has required tasks in order for good discernment. These tasks include taking time in making the decision, using both the head and heart, and assessing important values. Time is necessary to make a good choice, decisions made in a hurry are often not the best decisions. When time is available to assess the situation it improves the discernment process. When time allots the tentative decision can be revisited days later to make sure that the individual is satisfied with their choice after the discernment process. Making decisions with the head means to first reflect on the situation and emphasize the rational aspect of the decision making process. In order to make a decision that is ours it also requires the heart in which the individual makes based on feelings as well as rationality. Values in the discernment process are weighing options that decide what is most important to the individual. Every individuals value system is different which effects each individual discernment process. Combining values, using both the head and heart and taking sufficient time when making decision are the main steps for a successful discernment process.

Group discernment is a separate branch of discernment. In group discernment each individual must first undergo their own discernment process. The individual must keep in mind what is best for the group as a whole as well as the individual when making a decision. The same principles of values, using the head and heart, as well as giving the decision making process ample time all still apply in group discernment. Group discernment is different because it requires multiple people to have a unanimous decision in order to move forward. Group discernment requires discussion and persuasion between individuals to arrive at a decision.

Christian spiritual discernment can be separated from other types of discernment because every decision is to be made in accordance with God's will. The fundamental definition for Christian discernment is a decision making process in which an individual makes a discovery that can lead to future action. In the process of Christian spiritual discernment God guides the individual to help them arrive at the best decision. The way to arrive at the best decision in Christian spiritual discernment is to seek out internal external signs of God's action and then apply them to the decision at hand. Christian Discernment also has an emphasis on Jesus, and making decisions that align with those of Jesus within the New Testament. The focus on God and Jesus when making decisions is what separates Christian discernment from secular discernment. Ignatius of Loyola is often regarded as the master of the discernment of spirits. Ignatian discernment comes from Ignatius of Loyola (1491-1556) when he created his own unique way of Catholic discernment. Ignatian discernment uses a series of Spiritual Exercises for discerning life choices and focuses on noticing God in all aspects of life. The Spiritual Exercises are designed to help people who are facing a major life decision. There are seven steps of discernment to be followed that include identifying the issue, taking time to pray about the choice, making a wholehearted decision, discussing the choice with a mentor and then finally trusting the decision made.


</doc>
<doc id="43249770" url="https://en.wikipedia.org/wiki?curid=43249770" title="Knowledge acquisition">
Knowledge acquisition

Knowledge acquisition is the process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies. 

Expert systems were one of the first successful applications of artificial intelligence technology to real world business problems. Researchers at Stanford and other AI laboratories worked with doctors and other highly skilled experts to develop systems that could automate complex tasks such as medical diagnosis. Until this point computers had mostly been used to automate highly data intensive tasks but not for complex reasoning. Technologies such as inference engines allowed developers for the first time to tackle more complex problems. 

As expert systems scaled up from demonstration prototypes to industrial strength applications it was soon realized that the acquisition of domain expert knowledge was one of if not the most critical task in the knowledge engineering process. This knowledge acquisition process became an intense area of research on its own. One of the earlier works on the topic used Batesonian theories of learning to guide the process.

One approach to knowledge acquisition investigated was to use natural language parsing and generation to facilitate knowledge acquisition. Natural language parsing could be performed on manuals and other expert documents and an initial first pass at the rules and objects could be developed automatically. Text generation was also extremely useful in generating explanations for system behavior. This greatly facilitated the development and maintenance of expert systems.

A more recent approach to knowledge acquisition is a re-use based approach. Knowledge can be developed in ontologies that conform to standards such as the Web Ontology Language (OWL). In this way knowledge can be standardized and shared across a broad community of knowledge workers. One example domain where this approach has been successful is bioinformatics. 


</doc>
<doc id="2572627" url="https://en.wikipedia.org/wiki?curid=2572627" title="Knowledge ark">
Knowledge ark

A knowledge ark (also known as a doomsday ark or doomsday vault) is a collection of knowledge preserved in such a way that future generations would have access to said knowledge if current means of access were lost. 

Scenarios where availability to information (such as the Internet) would be lost could be described as Existential Risks or Extinction Level Events. A knowledge ark could take the form of a traditional Library or a modern computer database. It could also include images only (such as photographs of important information, or diagrams of critical processes). 

A knowledge ark would have to be resistant to the effects of natural or man-made disasters to be viable. Such an ark should include, but would not be limited to, information or material relevant to the survival and prosperity of human civilization.

Current examples include the Svalbard Global Seed Vault, a seedbank which is intended to preserve a wide variety of plant seeds (such as important crops) in case of their extinction.

A Lunar ark has been proposed which would store and transmit valuable information to receiver stations on Earth. The success of this would also depend on the availability of compatible receiver equipment on Earth, and adequate knowledge of that equipment's operation.

Other types of knowledge arks might include genetic material. With the potential for widespread personal DNA sequencing becoming a reality, an individual might agree to store their genetic code in a digital or analog storage format which would enable later retrieval of that code. If a species was sequenced before extinction, its genome would remain available for study even in the case of extinction.

The Phoenix mars lander (landed on surface of Mars in 2008) included the Visions of Mars DVD, a library on Mars. 



</doc>
<doc id="7983699" url="https://en.wikipedia.org/wiki?curid=7983699" title="Knowledge divide">
Knowledge divide

The knowledge divide is the gap between those who can find, create, manage, process, and disseminate information or knowledge, and those who are impaired in this process. According to a 2005 UNESCO World Report, the rise in the 21st century of a global information society has resulted in the emergence of knowledge as a valuable resource, increasingly determining who has access to power and profit. The rapid dissemination of information on a potentially global scale as a result of new information media and the globally uneven ability to assimilate knowledge and information has resulted in potentially expanding gaps in knowledge between individuals and nations. The digital divide is an extension of the knowledge divide, dividing people who have access to the internet and those who do not. The knowledge divide also represents the inequalities of knowledge amongst different identities, including but nor limited to race, economic status, and gender. 

In the 21st century, the emergence of the knowledge society becomes pervasive. The transformations of world's economy and of each society have a fast pace. Together with information and communication technologies (ICT), these new paradigms have the power to reshape the global economy. In order to keep pace with innovations, to come up with new ideas, people need to produce and manage knowledge. This is why knowledge has become essential for all societies. While knowledge has become essential for all societies due to the growth of new technologies, the increase of mass media information continues to facilitatwe the knowledge divide between those with educational differences . 

According to UNESCO and the World Bank, knowledge gaps between nations may occur due to the varying degrees by which individual nations incorporate the following elements:


The term digital divide was coined in the late 20th century to refer to the gap between those who had access to those who had access to the internet and those who did not, explaining the gap between people and societies who have the ability to participate in sharing and disseminating information in the digital age and those who do not. Today, the digital divide primarily refers to the gap in the nature of internet use rather than a gap in access to it.

Technology has expanded knowledge but it has not democratized it. In other words, technology has helped bridge the digital divide but has not bridged the knowledge gap. To bridge the digital divide, beyond providing access to computers and other technologies, importance must be out on developing digital literacy to bridge the gap. Despite worldwide use of the internet, users are disproportionately concentrated in more Developed countries such as the United States, South Korea, and Japan. Democratizing knowledge requires increased access to digital technology and equipping people to make effective use out of them.

In the book Digital Dead End, Virginia Eubanks criticizes the way that the digital divide is generally thought of as a division between haves and have-nots, where the solution is distribution. This over-simplistic depiction obscures the fact that often social and structural inequality is at the root of the divide. According to a study done by Eubanks with women of the YWCA, the women of the community "insisted that have-nots possess many different kinds of crucial information and skills." In other words, it is not simply knowledge of the technology itself that is the issue but the structural system based on perpetuating the status quo in which the haves "hoard" knowledge.

First, it was noticed that a great difference exists between the North and the South (rich countries vs. poor countries). The development of knowledge depends on spreading Internet and computer technology and also on the development of education in these countries. If a country has attained a higher literacy level then this will result in a higher level of knowledge.
Indeed, UNESCO's report details many social issues in knowledge divide related to globalization. There was noticed a knowledge divide with respect to

Scholars have made similar possibilities in closing or minimizing the knowledge divide between individuals, communities, and nations. Providing access to computers and other technologies that disseminate knowledge is not enough to bridge the digital divide, rather importance must be out on developing digital literacy to bridge the gap. Addressing the digital divide will not be enough to close the knowledge divide, disseminating relevant knowledge also depends on training and cognitive skills.




</doc>
<doc id="46287275" url="https://en.wikipedia.org/wiki?curid=46287275" title="Knowledge enterprise">
Knowledge enterprise

Knowledge enterprise, also named as knowledge company or knowledge-intensive company, organization or enterprise. According to D. Jemielniak, origin, and scope of this term is unclear. How this can be understood depends on how much company depends on knowledge, that in such a configuration, should be a critical asset of an organization. There is no agreement on how knowledge-intensive (to what extent) companies should be to be named like so. However, there are some hints to distinguish knowledge companies, since in economies, there are two groups of companies, of which one is labor-intensive, and another knowledge-intensive.

Knowledge enterprises are defined as enterprises where knowledge and knowledge-based products are offered to the market. The products and services can vary from plans to prototypes or mass-produced products where R&D costs are a large part. Employees of knowledge enterprises usually have an academic education. It's not a must but it is seen as an indicator of theoretical and analytical abilities and facilitating them. Education is also legitimisation of expert status and high fees. The work in knowledge enterprises is based on employees' intellectual skills and the tasks are not routine. The skill of combining different knowledge is required. 

According to Jemielniak, knowledge enterprises have emerged due to changes in the global economy, which throughout decades has been giving greater priorities to services. The emergence of knowledge companies is also called as a symptom of the third industrial revolution where boundaries between owners of production resources, and workers. On the example of IBM it can be seen that such a change have influenced the structure of income of companies. In 1924, IBM's profits were generated by leasing of manufacturing equipment in 96%, while punched cards were responsible for 4% of the profit. In 1970's, 80% of profit came from equipment divisions, 15% from software division and 5% from services. In 1990's services contributed to 30% of IBM's profits, later in 2007 it was already 45% that the company had earned from rendering services, and 20% from software. This example only reflects the overall change, that is manifested by the reversed proportion between tangible and intangible assets of companies. This evolution has forced a shift in the access to these resources from manual to non-manual (knowledge) workers. Also decision making power is handed in top-down, from owners, and top managers to mid-managers and specialists. These developments accompany the emergence and growing importance of knowledge enterprises.

Knowledge enterprises, according to Lowendahl, can be divided into:
and example companies are: management consultancy companies.
In another approach knowledge companies are divided into professional service companies, and research and development companies.

Companies with multiple units can have a situation where only some of those units are so called knowledge-intensive units. They work for the whole enterprise and their services are usually not offered outside the company. For example R&D, designing, engineering, accounting and law units can be seen as knowledge-intensive units.

Knowledge enterprises, due to their high-tech profile, chiefly have to base on IT technologies, including hardware and software to conduct managerial processes, and to organize working environments for all the staff, from executive to top management. This is why software development is crucial for existence and evolution of such companies. Software applications are developed for many areas within such organizations, since without them it is difficult to control, and coordinate work that is dedicated to innovation, and problem solving.

The main reason of ‘drain brain’ phenomenon and involvement of knowledge companies into it, is a great gap between educational infrastructure in the origin countries that IT professionals can get (but it is not limited to this profession), and low wages that they can be proposed in the origin country. The problem is that educational infrastructure in the transition countries (Central and Eastern Europe) does not actually have to catch-up with the counterparts in knowledge-based economies, like the United States. However salaries of the knowledge workers in both groups of countries differ very much. Knowledge-intensive companies from knowledge-based economies may propose much better incentives to have them move and work for them. This is the reason why human resources from transition countries are drained to those countries, that are characterized by the salary competitive advantage.


</doc>
<doc id="5702463" url="https://en.wikipedia.org/wiki?curid=5702463" title="Knowledge environment">
Knowledge environment

Knowledge environments are social practices, technological and physical arrangements intended to facilitate collaborative knowledge building, decision making, inference or discovery, depending on the epistemological premises and goals.

Knowledge environments departing from constructivist epistemology assume that domain knowledge is built in and results from cognitive and/or social practices. From this perspective the primary purpose of knowledge environments is to host and support activities of knowledge building, the means including cognitive ergonomics, social software, immediate information access exploiting means of multimedia and hypertext, content contribution functionalities and structured ontologies. Wikipedia itself is a prototypical example of a knowledge environment in this sense.

From another perspective, the purpose of a knowledge environment can be defined as a method to facilitate consistent knowledge outcomes. Knowledge outcomes reveal themselves as learning, communication, goals, decisions, etc. Consistent knowledge outcomes imply predictable learning results or replicable communication results and predictable quality of decisions. The design of knowledge environments is both commonplace activity and specialised expert work. At a simplistic level every teacher, every author, every librarian and every database manager is a creator of a knowledge environment. At a specialized level, knowledge environments need sophisticated architecture and modeling capabilities. This approach is necessary when the creator of the knowledge environment wants to deliver replicable results in hundreds of specific instances of the same knowledge environment. On the other hand, the strengthening trend of public authorship leads to open-ended ontologies by means of, say, tagging or folksonomies. In a significant sense, knowledge environments are in such cases created not only by their authors or owners but also by the contributors of their ontologies.

There are various kinds of knowledge environments:

Knowledge environments are all pervasive but difficult to build on a scalable and a replicable basis. This is because of two groups of interacting variables:

Some of the issues in development are:

Knowledge environments are useful for designing:



</doc>
<doc id="31002435" url="https://en.wikipedia.org/wiki?curid=31002435" title="Knowledge extraction">
Knowledge extraction

Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criteria is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.

The RDB2RDF W3C group is currently standardizing a language for extraction of resource description frameworks (RDF) from relational databases. Another popular example for knowledge extraction is the transformation of Wikipedia into structured data and also the mapping to existing knowledge (see DBpedia and Freebase).

After the standardization of knowledge representation languages such as RDF and OWL, much research has been conducted in the area, especially regarding transforming relational databases into RDF, identity resolution, knowledge discovery and ontology learning. The general process uses traditional methods from information extraction and extract, transform, and load (ETL), which transform the data from the sources into structured formats.

The following criteria can be used to categorize approaches in this topic (some of them only account for extraction from relational databases):


President Obama called Wednesday on Congress to extend a tax break for students included in last year's economic stimulus package, arguing that the policy provides more generous assistance. 


When building a RDB representation of a problem domain, the starting point is frequently an entity-relationship diagram (ERD). Typically, each entity is represented as a database table, each attribute of the entity becomes a column in that table, and relationships between entities are indicated by foreign keys. Each table typically defines a particular class of entity, each column one of its attributes. Each row in the table describes an entity
instance, uniquely identified by a primary key. The table rows collectively describe an entity set. In an equivalent RDF representation of the same entity set:

So, to render an equivalent view based on RDF semantics, the basic mapping algorithm would be as follows:

Early mentioning of this basic or direct mapping can be found in Tim Berners-Lee's comparison of the ER model to the RDF model.

The 1:1 mapping mentioned above exposes the legacy data as RDF in a straightforward way, additional refinements can be employed to improve the usefulness of RDF output respective the given Use Cases. Normally, information is lost during the transformation of an entity-relationship diagram (ERD) to relational tables (Details can be found in object-relational impedance mismatch) and has to be reverse engineered. From a conceptual view, approaches for extraction can come from two directions. The first direction tries to extract or learn an OWL schema from the given database schema. Early approaches used a fixed amount of manually created mapping rules to refine the 1:1 mapping. More elaborate methods are employing heuristics or learning algorithms to induce schematic information (methods overlap with ontology learning). While some approaches try to extract the information from the structure inherent in the SQL schema (analysing e.g. foreign keys), others analyse the content and the values in the tables to create conceptual hierarchies (e.g. a columns with few values are candidates for becoming categories). The second direction tries to map the schema and its contents to a pre-existing domain ontology (see also: ontology alignment). Often, however, a suitable domain ontology does not exist and has to be created first.

As XML is structured as a tree, any data can be easily represented in RDF, which is structured as a graph. XML2RDF is one example of an approach that uses RDF blank nodes and transforms XML elements and attributes to RDF properties. The topic however is more complex as in the case of relational databases. In a relational table the primary key is an ideal candidate for becoming the subject of the extracted triples. An XML element, however, can be transformed - depending on the context- as a subject, a predicate or object of a triple. XSLT can be used a standard transformation language to manually convert XML to RDF.

The largest portion of information contained in business documents (about 80%) is encoded in natural language and therefore unstructured. Because unstructured data is rather a challenge for knowledge extraction, more sophisticated methods are required, which generally tend to supply worse results compared to structured data. The potential for a massive acquisition of extracted knowledge, however, should compensate the increased complexity and decreased quality of extraction. In the following, natural language sources are understood as sources of information, where the data is given in an unstructured fashion as plain text. If the given text is additionally embedded in a markup document (e. g. HTML document), the mentioned systems normally remove the markup elements automatically.

Traditional information extraction is a technology of natural language processing, which extracts information from typically natural language texts and structures these in a suitable manner. The kinds of information to be identified must be specified in a model before beginning the process, which is why the whole process of traditional Information Extraction is domain dependent. The IE is split in the following five subtasks.


The task of named entity recognition is to recognize and to categorize all named entities contained in a text (assignment of a named entity to a predefined category). This works by application of grammar based methods or statistical models.

Coreference resolution identifies equivalent entities, which were recognized by NER, within a text. There are two relevant kinds of equivalence relationship. The first one relates to the relationship between two different represented entities (e.g. IBM Europe and IBM) and the second one to the relationship between an entity and their anaphoric references (e.g. it and IBM). Both kinds can be recognized by coreference resolution.

During template element construction the IE system identifies descriptive properties of entities, recognized by NER and CO. These properties correspond to ordinary qualities like red or big.

Template relation construction identifies relations, which exist between the template elements. These relations can be of several kinds, such as works-for or located-in, with the restriction, that both domain and range correspond to entities.

In the template scenario production events, which are described in the text, will be identified and structured with respect to the entities, recognized by NER and CO and relations, identified by TR.

Ontology-based information extraction is a subfield of information extraction, with which at least one ontology is used to guide the process of information extraction from natural language text. The OBIE system uses methods of traditional information extraction to identify concepts, instances and relations of the used ontologies in the text, which will be structured to an ontology after the process. Thus, the input ontologies constitute the model of information to be extracted.

Ontology learning is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time consuming, there is great motivation to automate the process.

During semantic annotation, natural language text is augmented with metadata (often represented in RDFa), which should make the semantics of contained terms machine-understandable. At this process, which is generally semi-automatic, knowledge is extracted in the sense, that a link between lexical terms and for example concepts from ontologies is established. Thus, knowledge is gained, which meaning of a term in the processed context was intended and therefore the meaning of the text is grounded in machine-readable data with the ability to draw inferences. Semantic annotation is typically split into the following two subtasks.


At the terminology extraction level, lexical terms from the text are extracted. For this purpose a tokenizer determines at first the word boundaries and solves abbreviations. Afterwards terms from the text, which correspond to a concept, are extracted with the help of a domain-specific lexicon to link these at entity linking.

In entity linking a link between the extracted lexical terms from the source text and the concepts from an ontology or knowledge base such as DBpedia is established. For this, candidate-concepts are detected appropriately to the several meanings of a term with the help of a lexicon. Finally, the context of the terms is analyzed to determine the most appropriate disambiguation and to assign the term to the correct concept.

The following criteria can be used to categorize tools, which extract knowledge from natural language text.

The following table characterizes some tools for Knowledge Extraction from natural language sources.

Knowledge discovery describes the process of automatically searching large volumes of data for patterns that can be considered knowledge "about" the data. It is often described as "deriving" knowledge from the input data. Knowledge discovery developed out of the data mining domain, and is closely related to it both in terms of methodology and terminology.

The most well-known branch of data mining is knowledge discovery, also known as knowledge discovery in databases (KDD). Just as many other forms of knowledge discovery it creates abstractions of the input data. The "knowledge" obtained through the process may become additional "data" that can be used for further usage and discovery. Often the outcomes from knowledge discovery are not actionable, actionable knowledge discovery, also known as domain driven data mining, aims to discover and deliver actionable knowledge and insights.

Another promising application of knowledge discovery is in the area of software modernization, weakness discovery and compliance which involves understanding existing software artifacts. This process is related to a concept of reverse engineering. Usually the knowledge obtained from existing software is presented in the form of models to which specific queries can be made when necessary. An entity relationship is a frequent format of representing knowledge obtained from existing software. Object Management Group (OMG) developed the specification Knowledge Discovery Metamodel (KDM) which defines an ontology for the software assets and their relationships for the purpose of performing knowledge discovery in existing code. Knowledge discovery from existing software systems, also known as software mining is closely related to data mining, since existing software artifacts contain enormous value for risk management and business value, key for the evaluation and evolution of software systems. Instead of mining individual data sets, software mining focuses on metadata, such as process flows (e.g. data flows, control flows, & call maps), architecture, database schemas, and business rules/terms/process.





</doc>
