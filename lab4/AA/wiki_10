<doc id="12362106" url="https://en.wikipedia.org/wiki?curid=12362106" title="Knowledge mobilization">
Knowledge mobilization

The term knowledge mobilization (KMb) refers to moving available knowledge (often from formal research) into active use. More than just "bridging the gap", KMb seeks to make connections between research/expertise and policy/practice in order to improve outcomes in various organizations or sectors. KMb involves knowledge sharing between research producers (e.g. university researchers) and research users (including professionals or others whose work can benefit from research findings), often with the help of third parties or intermediaries. The concept has grown out of increasing recognition that verified empirical knowledge should be the basis for many policies and practices.
Social science research deals with the people side of quality of life issues and nation-building that are so crucial to the future of humanity. Human, technological and cultural developments are needed for economic prosperity, environmental sustainability, social harmony and cultural vitality. Yet using research in the social sciences presents particular challenges because the issues are often complex and long-term, and deeply affected by local contexts.

The term KMb gained wider use following the publication of the evaluation report of the Community-University Research Alliance (CURA) program of the Social Sciences and Humanities Research Council of Canada (SSHRC) in 2004. This led SSHRC to create a division of Knowledge Products and Mobilization to enhance and accelerate the movement of research findings into policy and program development. Although many other terms are used to describe this same work, KMb, or knowledge mobilization, is the term most often used by the social science and humanities fields in Canada.

As in many other areas of social science, many different terms and approaches are used to define the knowledge mobilization process in different sectors and disciplines. The health sector often uses the term knowledge translation, whereas the business sector uses knowledge management, and so on. The Research Supporting Practice in Education (RSPE) Team at the University of Toronto – Ontario Institute for Studies in Education has created a synthesized list of the various terms and definitions currently being used.

There is considerable overlap between different terms but the subtle differences can affect our understanding of the topic. For example, the term Knowledge Transfer, implies that knowledge is like an object that can be given from one person to another, whereas terms such as Knowledge Exchange or Knowledge Mobilization imply that knowledge is altered as it passes from person to person. However, regardless of the term, the underlying intent in all cases is to make research matter more in policy and practice for organizational and system improvement.
The term 'knowledge' also carries multiple meanings. Some literature describes two types of knowledge; explicit and tacit. Tacit knowledge is gained through personal experience, and is difficult to codify and transfer; where explicit knowledge is often instrumental and can be more easily transported through various mediums. KMb tends to focus on explicit knowledge derived from formal research, while recognizing that tacit knowledge is also very important in practice.

Even within formal research settings, there is often disagreement about how much and what kinds of evidence provide sufficient warrant to claim that something is 'knowledge'.
A similar debate exists over what can be regarded as 'use' of research, with considerable evidence showing that research has an impact in diffuse ways and sometimes over long periods of time A detailed definition of knowledge mobilization, in relation to other practices such as community engagement, can be found on the website of the Institute for Community Engaged Scholarship.

Knowledge mobilization is a proactive process that involves specific efforts to build relationships between research producers and users, such as formal and informal events, networks, and collaboration on resources for research use. The broad concept of knowledge mobilization encompasses a variety of strategies, including producer push (where researchers do the work of trying to share knowledge), user pull (where research users seek information), knowledge exchange and the co-production of knowledge. These strategies may be informal or formal and can take place in many different places and ways.

The internet has become the prime vehicle through which research knowledge is shared, although direct personal connections among people remain a powerful means of sharing knowledge. In some fields or organizations there are specific roles for knowledge mobilization specialists (whose roles may have various names) to find, evaluate, synthesize and summarize findings to tailor and maximize the use of relevant and appropriate research. The goal is to replace practices based on belief with evidence-based practices in order to produce more effective outcomes. Researchers and research users can both benefit from the interaction required by KMb. Collaboration among various parties can improve the research enterprise as well by facilitating more relevant and effective scholarship as well as increased take-up of research findings.

Knowledge brokers or intermediaries act as bridges between the users and producers of the knowledge. They have to ensure that relevant information is made available in the right format. These ideas of quality research informed by the needs of research users, accurate interpretation, open access and just-in-time service are the bases for good researcher-user interface, often provided by knowledge brokers who can synthesize a large body of research and look for policy and practice implications that facilitate use of research results. Many different individuals and organizations play a brokering or intermediary role, from think tanks to lobby groups to political parties to professional or trade associations, to promoters of particular products or strategies. These various bodies may have very different motivations and degrees of objectivity in their approach. Much brokering occurs as part of political debate, in which the contention among ideas is part of broader political processes.

Research into the role of knowledge brokering in the UK health sector found that key individuals often play a major role in knowledge mobilization. A large study of knowledge mobilization by University of Oxford researchers found effective 'knowledge leadership' is crucial to moving evidence-based research into organizational practice, and creating 'epistemic fit'.

The Canadian Health Services Research Foundation (CHSRF), which has made extensive use of knowledge brokering and promoted evidence-informed decision making in the health services field, has developed extensive tools and resources that are finding use outside the health field. Similarly, the Canadian Institutes of Health Research (CIHR) have developed the idea of knowledge translation as a means to make better use of research in the health field. The RSPE provides many examples of and resources for knowledge brokering.

Networks are an important mechanism to support KMb. A number of KMb networks support professional knowledge brokers and the practice of KMb. ResearchImpact-RéseauImpact Recherche (RIR) is Canada's KMb network. Led by York University in Toronto, and pioneered by the work of David Phipps, it includes knowledge brokers from York University, Memorial University of Newfoundland and Labrador, Université du Québec à Montréal, Université de Montréal, Carleton University, University of Guelph, University of Saskatchewan, Kwantlen Polytechnic University, University of Victoria, McMaster University, and University of New Brunswick. (DRUSSA) is a network of 24 universities developing professional supports for research uptake (=KMb). The Canadian (KTECoP) has over 800 members and has chapters in Toronto, Ottawa and Vancouver. Ed Comms (www.educationcommunities.org) is the open to all knowledge mobilization network supported by a world wide network universities and educational organisations that form the Education Futures Collaboration (www.edfuturescollaboration.org). A core goal of Ed Comms is to link researchers and research users and to support educators to find others interested in collaborative working particularly to scale up small scale research so that educational research can be more impactful.

Movements such as open access and open data can play a positive role in KMb. Access to scholarly research and data sets has increased partly as a result of these movements, as well as greater digitization and dissemination of resources by government and NGOs. However, many institutions do not yet make their research publicly available, lack an open access repository or directory, or have well organized websites for posting research-based resources. However access to research is not nearly sufficient for Knowledge Mobilization, as the main barriers to research use are less a matter of knowing than they are of the capacity and motivation to use that knowledge in practice.

The field of KMb has been developing for about 50 years now. A good review of this work is in Estabrooks et al. (2008) There are many excellent research articles examining research utilization. In 2007, Sandra Nutley and colleagues from the Research Unit for Research Utilization (RURU) at the University of Edinburgh published one of the most comprehensive guides to KMb, "Using Evidence: How Research Can Inform Public Services", which identifies three interacting domains: research production, research use, and the intermediary process that links these two domains. This book provides an extensive review of the literature on research utilization from traditional constructs to contemporary design, from the practice and policy perspectives, and examines strategies to enhance research utilization and measure the impact of research use.

Many resources regarding this field, including definitions, conceptual models and links to other organizations, can be found on the website of the RSPE team at the University of Toronto's Ontario Institute for Studies in Education (OISE) and at KMbeing.com. A recent review of current research in knowledge mobilization can be found in the London Review of Education.




</doc>
<doc id="50239310" url="https://en.wikipedia.org/wiki?curid=50239310" title="Knowledge neglect">
Knowledge neglect

Knowledge neglect refers to cases when people fail to retrieve and apply previously stored knowledge appropriately into a current situation (Marsh, Umanath, 2014).
Perhaps the most famous example of knowledge neglect is the Moses Illusion, discovered in 1981 by Erickson and Mattson. For the illusion, participants are asked to answer the question, “How many pairs of each animal did Moses bring on his ark?” If a participant answers the question by simply saying, “2,” then this is an example of knowledge neglect because the person has failed to apply their previously learned knowledge that Noah was the individual who constructed the ark and herded the animals, not Moses. Another example would be a teacher asking the class, "Who was the main villain in Stephen King's "Harry Potter" series?" Any fan of the "Harry Potter" series knows that J. K. Rowling authored the books, however someone might still answer this question without applying their previous knowledge about the correct author, demonstrating knowledge neglect.

A more general example of knowledge neglect can also occur, for instance, when someone is rehearsing lines for a play and then forgets some of the lines while they are performing. The lines were available to the person in their memory but that person failed to access or retrieve them from their memory and use them for the situation, also demonstrating knowledge neglect.

One possible reason that people fall victim to knowledge neglect is because people tend to have a truth bias, meaning that people tend to believe that the information they hear is true. With the truth bias, people are inclined to believe that plausible information is true, regardless of the source of such information or their own prior knowledge. For this reason, individuals may fall victim to knowledge neglect simply because they aren't expecting that what they are being told or reading about will be incorrect.

Knowledge neglect could also be explained by the idea that people's attention is often fragmented, and that their cognitive ability is being used to examine the meaning of what they are reading or hearing about, rather than detecting errors in validity. For example, while reading stories or detecting/answering distorted questions, the participant is doing a lot and may not have the processing resources available to assess whether or not the information is true (Marsh, Umanath 2014). The reader of a story is processing a plot line, keeping track of characters, and more generally, building a mental model of the text (e.g., Bower & Morrow, 1990; Johnson‐ Laird, 1983); catching contradictions with stored knowledge is thus, not the main focus of the reader (Marsh, Umanath 2014).

The simple fact of believing something when it is presented to you is common. This can be a significant cause for knowledge neglect. When something is believed, the meaning is represented, coded, or presented in a mental system and usually is treated as if it is true (Gilbert 1991). If you have no prior knowledge about a subject when you encode it and you believe such information to be true, you are more likely to retrieve this information at a later time believing it is true, putting you at risk for knowledge neglect.

Rapp, D., & Braasch, J. L. (n.d.). Processing inaccurate information: Theoretical and applied perspectives from cognitive science and the educational sciences.

Bottoms, H. C., Eslick, A. N., & Marsh, E. J. (2010). Memory and the Moses illusion: Failures to detect contradictions with stored knowledge yield negative memorial consequences. Memory, 18(6), 670-678

Park, H., & Reder, L. M. (2004). Moses illusion: Implications for human cognition. In R. F. Pohl (Ed.), Cognitive illusions: A handbook on fallacies and biases in thinking, judgment, and memory (pp. 275"292). Hove, UK: Psychology Press.

Knowledge does not protect against illusory truth.
Fazio, Lisa K.; Brashier, Nadia M.; Payne, B. Keith; Marsh, Elizabeth J.
Journal of Experimental Psychology: General, Vol 144(5), Oct 2015, 993-1002.

Allison D. Cantor & Elizabeth J. Marsh (2016): Expertise effects in the Moses illusion: detecting contradictions with stored knowledge, Memory, 

Gilbert, D. T. (1991). How mental systems believe. American psychologist, 46(2), 107.

Marsh, E. J., & Umanath, S. (2014). Knowledge Neglect: Failures to Notice Contradictions with Stored Knowledge. Chapter in D. N. Rapp and J. Braasch (Eds.) Processing Inaccurate Information: Theoretical and Applied Perspectives from Cognitive Science and the Educational Sciences. MIT Press.


</doc>
<doc id="52906318" url="https://en.wikipedia.org/wiki?curid=52906318" title="Knowledge regime">
Knowledge regime

A knowledge regime is a type of system involving a specific set of actors, organizations, and institutions that create policy ideas used to alter the organization and overall operation of the policy-making and production process. Knowledge regimes are beneficial for the implementation of public policy because they introduce new sets of data, research, theories, recommendations, and many other influential ideas directed towards an end goal of economic competition.

These regimes became increasingly important following WWII when many capitalist societies were facing long periods of high inflation and economic stagnation associated with high levels of unemployment. Following this era, knowledge regimes were used in the study of comparative political economies, which examined both policymaking regimes as well as production regimes. The information and policies that arises from knowledge regimes differs depending on the political economy that a state operates in.

In a liberal market economy (LME) with a decentralized, open market, the knowledge regimes tend to be more market-oriented, meaning there are less external constraints from the government. In open markets there is more competition between companies and actors, resulting in a more adversarial environment. In an LME with a more centralized and closed market, there is nonpartisan state-involvement, all the while in a hostile environment. In coordinated market economies (CME) with decentralized, open states, there are strong associational institutional arrangements, which results in more consensus-oriented knowledge regimes whereas in a CME with a centralized closed market, policy changes come from within the state.

Depending on the political economy of a state, there are four corresponding knowledge-producing organizations. The first organization is composed of scholars who have specific knowledge on a topic; these members tend to be university students and/or professors. Another organization is made up of advocacy research units with individuals who have dedicated their time to one given topic. A third organization includes the individuals closely associated with a political party who conduct research that will provide expert advice and analysis for party members. The last knowledge-producing organization includes state research units who are situated within the government and are readily available for members within the government at any given time.

Capitalist countries began acknowledging the importance of knowledge regimes following the end of the Golden Age; this period was characterized by economic stagflation, eventually leading to inflation throughout the 1970s and 1980s. Economic bubbles and crises occur as a result of the processes that countries previously adapted, typically over the past 30 years, which are ingrained into the economy and hard to alter. The high rates of stagflation that occurred in the mid 1960s after the end of the Golden Age created the questions of what the adequate policies would be in order to pursue macroeconomic goals of long-term economic growth. This era contradicted the traditional Keynesian policy ideas, which led many advanced countries to turn to the use of theories, data, and ideologies as tools in the fight over different economic policies. This placed heavy emphasis on the creation and use of knowledge regimes.

The end of the Golden Age introduced the era of neoliberalism, characterized by a decrease in public spending, lower taxes, and less state intervention. Through neoliberalism and neoliberal ways of operation, many capitalist countries began globalizing at increased rates, which brought about new forms of economic organizations, such as global outsourcing and international commodity chains. Given this level of globalization, knowledge regimes were developed to create new policies and advise firms on appropriate production measures.

Varieties of capitalism is used to describe is used to describe the way a firm operates in two differentiating political capitalist economies, placing the emphasis on the state-firm relationship rather than the state-citizen relationship. The two political economies that nations will operate in are either liberal market economies (LMEs) or coordinated market economies (CMEs). There are five sectors that firms address when developing relationships with the state, including industrial relations, vocational training and education, corporate governance, inter-firm relations, and employees. Firms use the five sectors to design different policies that are best suited for their political economy.

Liberal market economies operate in capitalist nations that use a system of hierarchy and competitive markets to determine relationships between firms and the state. LMEs operate in highly commodified economies whereby there individuals are particularly dependent on the sale of their own labour as a reliable source of income. The commodification of labour is strongly affected by the price signals that are indicated by the markets, and these fluctuating price levels lead to either an increase or decrease in a firm's willingness to supply and a customer's willingness to consume. In liberal market economies, firms will use market mechanisms to solve any issues incurred, specifically through the exchange of money by buyers and sellers, who have a well-rounded understanding of the value of trade-offs so as to ensure the most efficient distribution of goods and services within an economy.

In a liberal market economy, firms require vocational education and training that is either individually funded or provided through on-the-job training to provide firm-specific education. These firms tend to focus their decisions on publicly accessible data of performance, allowing for greater venture capital and easily switchable assets that result in higher returns for investors. Due to the hierarchical nature of management, there are very few unions created in the work environment as labour is highly commodified and it is easy for a firm to hire and fire employees. The relationships between firms and employees operate on a contract basis. As a result of the individually funded vocational training, there is greater incentive to pursue individual success through innovation, which is generally funded by the larger venture capital opportunities.

LMEs can also operate alternatively, contingent on the type of market a state follows. If an economy has a more decentralized and open market, knowledge regimes will tend to be me more market-oriented with less external constraints from the government, as is the case with United States; if an economy is more centralized with a closed market, there is more state-led involvement and guidance, much like Great Britain.

The United States of America is a country with a liberal market economy, accentuated by decentralized government with a relatively open market. Through the open market, firms and investors are able to import and export different goods and services with very little interference from the government, allowing for the price levels of those goods and services to be set solely by the supply and demand in the economy.

The political economy in the United States is run primarily by two political parties, the Republican Party and the Democratic Party. Since the United States is a highly decentralized state, the two main parties operate using privately funded resources, resulting in a distinct separation of power at the legislative and executive branches of government. The United States demonstrates how a decentralized open market economy tends to have more market-oriented knowledge regimes. Through privately funded channels, individuals have a higher incentive to create knowledge regimes in attempt to influence the public policy making process.

Great Britain represents a group of countries (Wales, Scotland, England) that operate in an LME with a centralized, closed market. These countries follow a unitary system of government, whereby there is a single power dictating the operations of the country as a whole. The market system in Great Britain operates with greater state involvement and control, and the price levels of goods and services are more commonly dictated by government-imposed market tools, such as tariffs, taxes, and subsidies.

The central authority holds power in both the legislative branch and executive branch of government, allowing the central authority to impose new policies and decisions without opposition from other political parties. As a result, knowledge regimes are publicly funded and not as common as they are in LMEs with an open market. State research units are more prevalent throughout Great Britain and are embedded throughout the civil society, creating more politically tempered knowledge regimes.

Coordinated market economies (CME) are characterized by very little state involvement with firms focused on building close relationships between individuals in managerial positions and employees. Through the strong relationships created within the firm, there is a highly decommodified environment with a strong sense of social entitlement and separation from market dependency and state participation. When firms incur a problem of any sort, they use strategic interactions to create a feasible solution. These strategic interactions incorporate employees at every level of the firm so as to ensure opinions and viewpoints from all sectors are being taken into account; this typically allows for a stronger built solution. In addition, firms use strategic interaction to determine at which point the equilibrium should be set between the firm and their consumers, which establishes the appropriate combination between how much of a product is supplied and subsequently purchased.

In a coordinated market economy, there is publicly subsidized training to support high-skilled education for greater industry-specific labour needs; this is a result of the multilateral decisions that are made through strategic interactions. Additionally, there is greater confidence instilled in potential investors through the promotion of dense networks. Through the structure of the market, there are several unions and long-term worker contracts, which ensures job security and decreases the likelihood of hostile takeovers. Due to the interaction and cooperation throughout multiple firms, there is greater society-wide success through innovative tendencies, such as the green energy industry in Germany.

CMEs operate differently depending on if a state has a closed market or an open market. In a CME with a decentralized and open market, such as Germany, there tends to be strong associational institutional arrangements, producing more consensus-oriented knowledge regimes; a CME with a centralized and closed market, such as France, many policy changes are encouraged and led by state involvement.

Germany is a country with a coordinated market economy, characterized by a decentralized government with an open market system. Firms in this economy operate through well-established networks, while developing and maintaining relationships to promote and coordinate economic activity. There is very little state-involvement throughout the market, with the emphasis and responsibility being placed on the firms and corporations to set and adjust prices accordingly. Germany follows a proportional representation electoral system, creating a stronger consensus-based environment whereby policies are formulated at the regional level of government rather than the federal level.

Through regional-based policy formulation tactics, there is a higher rate of scholarly research units affiliated with several different universities, churches, and other knowledge-producing institutions. There is also a stronger incentive for political parties to create their own party research units to be used for future elections and policy-making tactics. The knowledge regimes created throughout Germany represent the institutional tendencies that firms rely on, such as the negotiation among different political parties, problem-solving mediation, and general consensus building.

France is a country with a coordinated market economy, distinguished by a central government and a closed market. In France, the central government is the main actor when making economic decisions and solving financial problems; the government holds sole ownership of several of the main infrastructure sectors throughout the country, weakening state-firm relationships. With a central government, the same political party holds control of both the legislative and executive branches, resulting in much of policy making taking place at the national level.

As a result of the highly centralized state, many of the knowledge regimes throughout France emerge from state-led research units, as individuals are given very few incentives or means to create or pursue their own research units. Other knowledge regimes emerge from scholarly research units that are funded through the central government.

Knowledge regimes are organized in different ways depending on the type of political economy they are operating in. There are 4 common knowledge producing organizations, which correspond to either liberal market economies or coordinated market economies. These knowledge producing organizations also differentiate if a country has either a centralized or decentralized government, or if there is an open or closed market.

Academic research units are made up of academia from many institutions. The most commonly found participants in these units are students and professors, however many firms with a coordinated market economy send their employees to these research units. These units are often publicly funded through the government, although they may be privately funded as well, and tend to be both politically and ideologically impartial. Academic research units are most common in liberal market economies with a decentralized government and open market. Prior to WWII, many of these research units were privately funded through philanthropic organizations, but following WWII, many governments began providing funding for academic research units by contracting out for policy research.

Advocacy research units are created by individuals who are independently motivated through their own values and morals. These research units are privately funded through organizations that directly correlate to the topic or issue being addressed through the unit. These research units are both politically and ideologically partisan in nature, and are more focused on discrediting research that has already been done by using the media to persuade politicians to alter certain policies. Advocacy research units are most common in political economies with decentralized states and open markets, as politicians are less likely to conform to political party standards and will seek advice through external resources.

Party research units are independently funded research units on behalf of a specific political party. These units are both politically and ideologically biased towards the values of a political party, composed of individuals who conduct research on behalf of the leaders. Political leaders will use the research from these units to pursue designated policy goals that will generate as much public approval and support as possible. These research units are most common in economies that operate through highly decentralized governments with several different political parties running for office during an election.

State-led research units are the only form of research unit that does not directly involve the participation of the civil society when conducting research. These units are closely affiliated with specific government departments and ministries that conduct research on a very specific matter that is then reported back to the central authority. These research units operate in economies with centralized and closed markets and are publicly funded through the central government.


</doc>
<doc id="39845988" url="https://en.wikipedia.org/wiki?curid=39845988" title="Knowledge society">
Knowledge society

A knowledge society generates, shares and makes available to all members of the society knowledge that may be used to improve the human condition. A knowledge society differs from an information society in that the former serves to transform information into resources that allow society to take effective action while the latter only creates and disseminates the raw data. The capacity to gather and analyze information has existed throughout human history. However, the idea of the present-day knowledge society is based on the vast increase in data creation and information dissemination that results from the innovation of information technologies. The UNESCO World Report addresses the definition, content and future of knowledge societies.

The growth of Information and communication technology (ICT) has significantly increased the world’s capacity for creation of raw data and the speed at which it is produced. The advent of the Internet delivered unheard-of quantities of information to people. The evolution of the internet from Web 1.0 to Web 2.0 offered individuals tools to connect with each other worldwide as well as become content users and producers. Innovation in digital technologies and mobile devices offers individuals a means to connect anywhere anytime where digital technologies are accessible. Tools of ICT have the potential to transform education, training, employment and access to life-sustaining resources for all members of society.
However, this capacity for individuals to produce and use data on a global scale does not necessarily result in knowledge creation. Contemporary media delivers seemingly endless amounts of information and yet, the information alone does not create knowledge. For knowledge creation to take place, reflection is required to create awareness, meaning and understanding. The improvement of human circumstances requires critical analysis of information to develop the knowledge that assists humankind. Absent reflection and critical thinking, information can actually become "non-knowledge", that which is false or inaccurate. The anticipated Semantic Web 3.0 and Ubiquitous Web 4.0 will move both information and knowledge creation forward in their capacities to use intelligence to digitally create meaning independent of user-driven ICT.

The social theory of a knowledge society explains how knowledge is fundamental to the politics, economics, and culture of modern society. Associated ideas include the knowledge economy created by economists and the learning society created by educators. Knowledge is a commodity to be traded for economic prosperity. In a knowledge society, individuals, communities, and organizations produce knowledge-intensive work. Peter Drucker viewed knowledge as a key economic resource and coined the term knowledge worker in 1969. Fast forward to the present day, and in this knowledge-intensive environment, knowledge begets knowledge, new competencies develop, and the result is innovation. 
A knowledge society promotes human rights and offers equal, inclusive, and universal access to all knowledge creation. The UNESCO World Report establishes four principles that are essential for development of an equitable knowledge society:

However, they acknowledge that the digital divide is an obstacle to achievement of genuine knowledge societies. Access to the internet is available to 39 percent of the world’s population. This statistic represents growth as well as a continued gap. Among the many challenges that contribute to a global digital divide are issues regarding economic resources, geography, age, gender, language, education, social and cultural background, employment and disabilities.

To reduce the span of the digital divide, leaders and policymakers worldwide must first develop and understanding of knowledge societies and second, create and deploy initiatives that will universally benefit all populations. The public expects politicians and public institutions to act rationally and rely on relevant knowledge for decision-making. Yet, in many cases, there are no definitive answers for some of the issues that impact humankind. Science is no longer viewed as the provider of unquestionable knowledge and sometimes raises more uncertainty in its search for knowledge. The very advancement of knowledge creates the existence of increased ignorance or non-knowledge. This means that public policy must learn to manage doubt, probability, risk and uncertainty while making the best decisions possible.

To confront the uncertainty that comes from an increase in both knowledge and the resulting lack of knowledge, members of a society disagree and make decisions using justification and observation of consequences. Public policy may operate with the intent to prevent the worst possible outcome versus find the perfect solution. Democratization of expert knowledge occurs when a knowledge society produces and relies on more experts. Expert knowledge is no longer exclusive to certain individuals, professional or organizations. If in a knowledge society, knowledge is a public good to which all people have access, any individual may also serve as a creator of knowledge and receive credit as an expert. Since politicians rely on expert knowledge for decision making, the layperson who may lack specialized knowledge might hold a view that serves as expertise to the political process.

As technologies are deployed to improve global information access, the role of education will continue to grow and change. Education is viewed as a basic human right. For a society where reading and counting are a requisite for daily living, skills in reading, writing, and basic arithmetic are critical for future learning. However, in a knowledge society, education is not restricted to school. The advent of ICT allows learners to seek information and develop knowledge at any time and any place where access is available and unrestricted. In these circumstances, the skill of learning to learn is one of the most important tools to help people acquire formal and informal education. In a knowledge society supported by ICT, the ability to locate, classify and sort information is essential. Equipped with this skill, the use of ICT becomes an active versus a passive endeavor and integral to literacy and lifelong learning.

One marker of a knowledge society is continuous innovation that demands lifelong learning, knowledge development, and knowledge sharing. The institution of education will need to become responsive to changing demands. Education professionals will need to learn along with everyone else, and as leaders of changing designs in learning, they will serve as a bridge between technology and teaching. The ability to individually reflect on personal learning requirements and seek knowledge in whatever method is appropriate characterizes lifelong learning. One model that supports this type of learning is the W. Edwards Deming Plan-do-check-act cycle that promotes continuous improvement. Educational professionals will need to prepare learners to be accountable for their own lifelong learning.



</doc>
<doc id="31938666" url="https://en.wikipedia.org/wiki?curid=31938666" title="Knowledge space (philosophy)">
Knowledge space (philosophy)

In philosophy and media studies, a knowledge space is described as an emerging anthropological space in which the knowledge of individuals becomes the primary focus for social structure, values, and beliefs. The concept is put forward and explored by philosopher and media critic Pierre Lévy in his 1997 book "Collective Intelligence".

Levy's notion of the "knowledge space" relies on his conception of anthropological spaces, which he defines as "a system of proximity (space) unique to the world of humanity (anthropological), and thus dependent on human technologies, significations, language, culture, conventions, representations, and emotions" (5). Building on the language of the philosophers Gilles Deleuze and Félix Guattari, he states that "anthropological spaces in themselves are neither infrastructures nor superstructures but planes of existence, frequencies, velocities, determined within the social spectrum" (147). Each space contains "worlds of signification" (149) by which humans come to understand and make sense of the world. Furthermore, although one space may dominate, many spaces can and do exist simultaneously.

Levy describes three existing anthropological spaces. They are:

The knowledge space is an emerging anthropological space which, while it has always existed (139), is only now coming into fruition as a guiding space of humanity. In this space, singularities (individuals) are recognized as singularities and knowledge becomes the guiding value for humanity. Since all human experience represents unique knowledge, within the knowledge space all individuals are valued for their unique knowledge regardless of race (earth space), nationality (territorial space), or economic status (commodity space). Within this space static identity gives way to the "quantum identities" as individuals become participates and the distinction between of "us" and "them" disappears (159). Instead, humanity forms "collective intelligences" in which knowledge is valued and freely traded. What is "real" becomes "that which implies the practical activity, intellectual and imaginary, of living subjects" (168). Life, experiences, and knowledge become the underlying and ever changing guiding path for human societies.

Levy's theories rely heavily on the technological developments of the 1990s, particularly the rise of biotechnology, nanotechnology, the Internet, new media and information technologies. In chapter 3, he describes how technologies have made a shift from the molar to the molecular (a move which makes literal a distinction by Delueze and Guattari) in that technologies now handle units as individuals (his term is "singularities") rather than in mass. He suggests that this mirrors our rising recognition of the individuals as singularities rather than massive conglomerated groups.


</doc>
<doc id="37374009" url="https://en.wikipedia.org/wiki?curid=37374009" title="Knowledge translation">
Knowledge translation

Knowledge translation (KT) is the activities involved in moving research from the laboratory, the research journal, and the academic conference into the hands of people and organizations who can put it to practical use. Knowledge translation is most often used in the health professions, including medicine, nursing, pharmaceuticals, rehabilitation, physical therapy, and public health.

Depending on the type of research being translated, the "practical user" might be a medical doctor, a nurse, a teacher, a school administrator, an occupational or physical therapist, a legislator, an epidemiologist, a community health worker, or a parent.

KT is not an action, but a spectrum of activities which will change according to the type of research, the time frame, and the audience being targeted.

The most widely used definition of knowledge translation was published in 2000 by the Canadian Institutes of Health Research (CIHR): "Knowledge translation (KT) is defined as a dynamic and iterative process that includes synthesis, dissemination, exchange and ethically-sound application of knowledge to improve the health of Canadians, provide more effective health services and products and strengthen the health care system."

Using the CIHR definition as a basis, the National Center for the Dissemination of Disability Research (NCDDR) published this definition of KT in 2005: "The collaborative and systematic review, assessment, identification, aggregation, and practical application of high-quality disability and rehabilitation research by key stakeholders (i.e., consumers, researchers, practitioners, and policymakers) for the purpose of improving the lives of individuals with disabilities."

In 2006, Graham, et al., acknowledged the proliferation of related terms for the activity of knowledge translation, documenting 29 different terms used by 33 different health research funding agencies in their publications, including knowledge transfer, knowledge mobilization, knowledge exchange, implementation, and translational research.

In 2007, NCDDR re-published an overview written by Pimjai Sudsawad, then with the University of Wisconsin-Madison, now with the U.S. Department of Education, entitled: "Knowledge Translation: Introduction to Models, Strategies, and Measures". The overview correlates a variety of KT models which have been in development since at least 1976, including the Stetler Model of Research Utilization, the Coordinated Implementation Model, the Promoting Action on Research Implementation in Health Services (PARIHS) framework, the Ottawa Model of Research Utilization (OMRU), and the Knowledge to Action (KTA) process framework.

The activity of knowledge translation is observable as far back as agricultural extension services established by the Smith-Lever Act of 1914. The Smith-Lever Act formalized the relationship between United States land-grant universities and the United States Department of Agriculture (USDA) for the performance of agricultural extension. Agricultural extension agents based at the land-grant universities disseminated information to farmers and ranchers on seed development, land management and animal husbandry.

In their Technical Brief #10 2005, NCDDR points out: "KT is a relatively new term that is used to describe a relatively old problem - the underutilization of evidence-based research in systems of care. Underutilization of evidence-based research is often described as a gap between 'what is known' and 'what is currently done' in practice settings."

While evaluations of research utilization in the health fields have been going on since at least the mid-1960s, institutional interest in this long-standing issue has accelerated in the last 25 years. In 1989, the U.S. Department of Health and Human Services established the Agency for Healthcare Research and Quality. In 1997, the Canadian government endowed the Canadian Health Services Research Foundation (CHSRF) - now called the Canadian Foundation for Healthcare Improvement, or CFHI. In 2000, the Canadian government consolidated several existing agencies into the Canadian Institutes for Health Research. In 2006, the U.S. National Institutes of Health created the Clinical and Translational Science Awards, currently funding about 60 academic medical institutions across the country. The role of health research funders is increasingly playing a role in how evidence is being moved to practice, reducing the time between research and implementation.

More recently, the challenges of filtering information for knowledge translation is being increasingly addressed with Moloney, Taylor & Ralph proposing a "spillway model" to better control information flow and improve the implementation of research in healthcare. Other recent studies look at the role of design artefacts such as sketches, visual representations and prototypes to support knowledge translation in research and development projects.




</doc>
<doc id="5946039" url="https://en.wikipedia.org/wiki?curid=5946039" title="Knowledge triangle">
Knowledge triangle

The knowledge triangle refers to the interaction between research, education and innovation, which are key drivers of a knowledge-based society. In the European Union, it also refers to an attempt to better link together these key concepts, with research and innovation already highlighted by the development of the Lisbon Strategy and, more recently, lies behind the creation of the European Institute of Technology (EIT).



</doc>
<doc id="10307426" url="https://en.wikipedia.org/wiki?curid=10307426" title="Knowledge value">
Knowledge value

The idea that knowledgeable value is ancient. In the 1st century AD, Juvenal (55-130) stated “All wish to know but none wish to pay the price". In 1775, Samuel Johnson wrote: “All knowledge is of itself of some value.”

In the 19th century, Coleridge (1825) stated that : “The worth and value of knowledge is in proportion to the worth anAndroid apps development projects at once and for all your help in this email in between nee to be happy anniversary Mr Mrs the day dear I

d value of its object.” Auerbach (1865) asked: “What is all our knowledge worth?" although he proposed no answer. Largely the same ideas are already expressed in the term "intellectual capital" or the more ancient "knowledge is power" - given that power is a value in its own right.

Only towards of the end of the 20th century, however, was the value of knowledge in a business context generally recognized. The idea has since become something of a management fad, although many authors indicate that the underlying principles will become standard business practice. It is now understood that knowledge about how to produce products and provide services as well as their embedded knowledge is often more valuable than the products and services themselves or the materials they contain. Although measuring the value of knowledge remains elusive, describing its flow through value chains is a step in the right direction.

Firestone was the first to relate knowledge to business when he noted that “Thought, not money is the real business capital.” Alvin Toffler (1990) proposed that knowledge is a wealth and force multiplier, in that it augments what is available or reduces the amount needed to achieve a given purpose.

In comparing knowledge and product value, Amidon (1997) observes that knowledge about how to produce products may be more valuable than the products themselves. Leonard similarly points out that products are physical manifestations of knowledge and that their worth depends largely on the value of the embedded knowledge.

Davis (1999) further notes that the computer chips in a high-end automobile are worth more than the steel, plastics, glass, or rubber. However, Davis and Botin (1994) indicate that awareness of the value of knowledge exceeds the ability of many businesses to extract it from the goods and services in which it is embedded.

Measuring the value of knowledge has not progressed much beyond an awareness that traditional accounting practices are misleading and can lead to wrong business decisions (Martin, 1996). Amidon (1997) points out that the shift from tangible to intangible assets will revolutionize the way that enterprises are measured and that there is an entirely new way to value economic wealth.

Simard et al. (2007) developed a content value chain describing the flow of content through a sequence of stages in which its form is changed and its value or utility to users are notably increased at each stage: objects, data, information, knowledge, and wisdom. They also developed a knowledge services value chain, which describes the flow of knowledge services through a sequence of stages, in which value is embedded, advanced, or extracted.

The stages are: generate, transform, manage, use internally, transfer, add value, use professionally, use personally, and evaluate.




</doc>
<doc id="243391" url="https://en.wikipedia.org/wiki?curid=243391" title="Knowledge">
Knowledge

Knowledge is a familiarity, awareness, or understanding of someone or something, such as facts, information, descriptions, or skills, which is acquired through experience or education by perceiving, discovering, or learning.

Knowledge can refer to a theoretical or practical understanding of a subject. It can be implicit (as with practical skill or expertise) or explicit (as with the theoretical understanding of a subject); it can be more or less formal or systematic. In philosophy, the study of knowledge is called epistemology; the philosopher Plato famously defined knowledge as "justified true belief", though this definition is now thought by some analytic philosophers to be problematic because of the Gettier problems, while others defend the platonic definition. However, several definitions of knowledge and theories to explain it exist.

Knowledge acquisition involves complex cognitive processes: perception, communication, and reasoning; while knowledge is also said to be related to the capacity of "acknowledgement" in human beings.

The definition of knowledge is a matter of ongoing debate among philosophers in the field of epistemology. The classical definition, described but not ultimately endorsed by Plato, specifies that a statement must meet three criteria in order to be considered knowledge: it must be justified, true, and believed. Some claim that these conditions are not sufficient, as Gettier case examples allegedly demonstrate. There are a number of alternatives proposed, including Robert Nozick's arguments for a requirement that knowledge 'tracks the truth' and Simon Blackburn's additional requirement that we do not want to say that those who meet any of these conditions 'through a defect, flaw, or failure' have knowledge. Richard Kirkham suggests that our definition of knowledge requires that the evidence for the belief necessitates its truth.

In contrast to this approach, Ludwig Wittgenstein observed, following Moore's paradox, that one can say "He believes it, but it isn't so," but not "He knows it, but it isn't so." He goes on to argue that these do not correspond to distinct mental states, but rather to distinct ways of talking about conviction. What is different here is not the mental state of the speaker, but the activity in which they are engaged. For example, on this account, to "know" that the kettle is boiling is not to be in a particular state of mind, but to perform a particular task with the statement that the kettle is boiling. Wittgenstein sought to bypass the difficulty of definition by looking to the way "knowledge" is used in natural languages. He saw knowledge as a case of a family resemblance. Following this idea, "knowledge" has been reconstructed as a cluster concept that points out relevant features but that is not adequately captured by any definition.

Symbolic representations can be used to indicate meaning and can be thought of as a dynamic process. Hence the transfer of the symbolic representation can be viewed as one ascription process whereby knowledge can be transferred. Other forms of communication include observation and imitation, verbal exchange, and audio and video recordings. Philosophers of language and semioticians construct and analyze theories of knowledge transfer or communication.

While many would agree that one of the most universal and significant tools for the transfer of knowledge is writing and reading (of many kinds), argument over the usefulness of the written word exists nonetheless, with some scholars skeptical of its impact on societies. In his collection of essays "", Neil Postman demonstrates the argument against the use of writing through an excerpt from Plato's work "Phaedrus" (Postman, Neil (1992) "Technopoly", Vintage, New York, p. 73). In this excerpt, the scholar Socrates recounts the story of Thamus, the Egyptian king and Theuth the inventor of the written word. In this story, Theuth presents his new invention "writing" to King Thamus, telling Thamus that his new invention "will improve both the wisdom and memory of the Egyptians" (Postman, Neil (1992) Technopoly, Vintage, New York, p. 74). King Thamus is skeptical of this new invention and rejects it as a tool of recollection rather than retained knowledge. He argues that the written word will infect the Egyptian people with fake knowledge as they will be able to attain facts and stories from an external source and will no longer be forced to mentally retain large quantities of knowledge themselves (Postman, Neil (1992) "Technopoly", Vintage, New York, p. 74).

Classical early modern theories of knowledge, especially those advancing the influential empiricism of the philosopher John Locke, were based implicitly or explicitly on a model of the mind which likened ideas to words. This analogy between language and thought laid the foundation for a graphic conception of knowledge in which the mind was treated as a table, a container of content, that had to be stocked with facts reduced to letters, numbers or symbols. This created a situation in which the spatial alignment of words on the page carried great cognitive weight, so much so that educators paid very close attention to the visual structure of information on the page and in notebooks.

Major libraries today can have millions of books of knowledge (in addition to works of fiction). It is only recently that audio and video technology for recording knowledge have become available and the use of these still requires replay equipment and electricity. Verbal teaching and handing down of knowledge is limited to those who would have contact with the transmitter or someone who could interpret written work. Writing is still the most available and most universal of all forms of recording and transmitting knowledge. It stands unchallenged as mankind's primary technology of knowledge transfer down through the ages and to all cultures and languages of the world.

Situated knowledge is knowledge specific to a particular situation. It was used by Donna Haraway as an extension of the feminist approaches of "successor science" suggested by Sandra Harding, one which "offers a more adequate, richer, better account of a world, in order to live in it well and in critical, reflexive relation to our own as well as others' practices of domination and the unequal parts of privilege and oppression that makes up all positions." This situation partially transforms science into a narrative, which Arturo Escobar explains as, "neither fictions nor supposed facts." This narrative of situation is historical textures woven of fact and fiction, and as Escobar explains further, "even the most neutral scientific domains are narratives in this sense," insisting that rather than a purpose dismissing science as a trivial matter of contingency, "it is to treat (this narrative) in the most serious way, without succumbing to its mystification as 'the truth' or to the ironic skepticism common to many critiques."

Haraway's argument stems from the limitations of the human perception, as well as the overemphasis of the sense of vision in science. According to Haraway, vision in science has been, "used to signify a leap out of the marked body and into a conquering gaze from nowhere." This is the "gaze that mythically inscribes all the marked bodies, that makes the unmarked category claim the power to see and not be seen, to represent while escaping representation." This causes a limitation of views in the position of science itself as a potential player in the creation of knowledge, resulting in a position of "modest witness". This is what Haraway terms a "god trick", or the aforementioned representation while escaping representation. In order to avoid this, "Haraway perpetuates a tradition of thought which emphasizes the importance of the subject in terms of both ethical and political accountability".

Some methods of generating knowledge, such as trial and error, or learning from experience, tend to create highly situational knowledge.
Situational knowledge is often embedded in language, culture, or traditions. This integration of situational knowledge is an allusion to the community, and its attempts at collecting subjective perspectives into an embodiment "of views from somewhere." 

Even though Haraway's arguments are largely based on feminist studies, this idea of different worlds, as well as the skeptic stance of situated knowledge is present in the main arguments of post-structuralism. Fundamentally, both argue the contingency of knowledge on the presence of history; power, and geography, as well as the rejection of universal rules or laws or elementary structures; and the idea of power as an inherited trait of objectification.

One discipline of epistemology focuses on partial knowledge. In most cases, it is not possible to understand an information domain exhaustively; our knowledge is always "incomplete" or partial. Most real problems have to be solved by taking advantage of a partial understanding of the problem context and problem data, unlike the typical math problems one might solve at school, where all data is given and one is given a complete understanding of formulas necessary to solve them.

This idea is also present in the concept of bounded rationality which assumes that in real life situations people often have a limited amount of information and make decisions accordingly.

The development of the scientific method has made a significant contribution to how knowledge of the physical world and its phenomena is acquired. To be termed scientific, a method of inquiry must be based on gathering observable and measurable evidence subject to specific principles of reasoning and experimentation. The scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses. Science, and the nature of scientific knowledge have also become the subject of Philosophy. As science itself has developed, scientific knowledge now includes a broader usage in the soft sciences such as biology and the social sciences – discussed elsewhere as meta-epistemology, or genetic epistemology, and to some extent related to "theory of cognitive development". Note that "epistemology" is the study of knowledge and how it is acquired. Science is "the process used everyday to logically complete thoughts through inference of facts determined by calculated experiments." Sir Francis Bacon was critical in the historical development of the scientific method; his works established and popularized an inductive methodology for scientific inquiry. His famous aphorism, "knowledge is power", is found in the Meditations Sacrae (1597).

Until recent times, at least in the Western tradition, it was simply taken for granted that knowledge was something possessed only by humans – and probably "adult" humans at that. Sometimes the notion might stretch to "Society-as-such", as in (e. g.) "the knowledge possessed by the Coptic culture" (as opposed to its individual members), but that was not assured either. Nor was it usual to consider "unconscious" knowledge in any systematic way until this approach was popularized by Freud.

Other biological domains where "knowledge" might be said to reside, include: (iii) the "immune system", and (iv) in the "DNA of the genetic code". See the list of four "epistemological domains": Popper, (1975); and Traill (2008: Table S, p. 31) – also references by both to Niels Jerne.

Such considerations seem to call for a separate definition of "knowledge" to cover the biological systems. For biologists, knowledge must be usefully "available" to the system, though that system need not be conscious. Thus the criteria seem to be:

Scientific knowledge may not involve a claim to certainty, maintaining skepticism means that a scientist will never be absolutely certain when they are correct and when they are not. It is thus an irony of proper scientific method that one must doubt even when correct, in the hopes that this practice will lead to greater convergence on the truth in general.

In many expressions of Christianity, such as Catholicism and Anglicanism, knowledge is one of the seven gifts of the Holy Spirit.

The Old Testament's tree of the knowledge of good and evil contained the knowledge that separated Man from God: "And the LORD God said, Behold, the man is become as one of us, to know good and evil..." ()

In Gnosticism, divine knowledge or gnosis is hoped to be attained.

विद्या दान (Vidya Daan) i.e. knowledge sharing is a major part of Daan, a tenet of all Dharmic Religions.
Hindu Scriptures present two kinds of knowledge, "Paroksh Gyan" and "Prataksh Gyan". "Paroksh Gyan" (also spelled "Paroksha-Jnana") is secondhand knowledge: knowledge obtained from books, hearsay, etc. "Pratyaksh Gyan" (also spelled "Pratyaksha-Jnana") is the knowledge borne of direct experience, i.e., knowledge that one discovers for oneself. Jnana yoga ("path of knowledge") is one of three main types of yoga expounded by Krishna in the Bhagavad Gita. (It is compared and contrasted with Bhakti Yoga and Karma yoga.)

In Islam, knowledge (Arabic: علم, "ʿilm") is given great significance. "The Knowing" ("al-ʿAlīm") is one of the 99 names reflecting distinct attributes of God. The Qur'an asserts that knowledge comes from God () and various "hadith" encourage the acquisition of knowledge. Muhammad is reported to have said "Seek knowledge from the cradle to the grave" and "Verily the men of knowledge are the inheritors of the prophets". Islamic scholars, theologians and jurists are often given the title "alim", meaning "knowledgeble". 

In Jewish tradition, knowledge (Hebrew: דעת "da'ath") is considered one of the most valuable traits a person can acquire. Observant Jews recite three times a day in the Amidah "Favor us with knowledge, understanding and discretion that come from you. Exalted are you, Existent-One, the gracious giver of knowledge." The Tanakh states, "A wise man gains power, and a man of knowledge maintains power", and "knowledge is chosen above gold".

According to the sociologist Mervin F. Verbit, knowledge may be understood as one of the key components of religiosity. Religious knowledge itself may be broken down into four dimensions:
The content of one's religious knowledge may vary from person to person, as will the degree to which it may occupy the person's mind (frequency), the intensity of the knowledge, and the centrality of the information (in that religious tradition, or to that individual).




</doc>
<doc id="9005414" url="https://en.wikipedia.org/wiki?curid=9005414" title="Sensemaking">
Sensemaking

Sensemaking or sense-making is the process by which people give meaning to their collective experiences. It has been defined as "the ongoing retrospective development of plausible images that rationalize what people are doing" (Weick, Sutcliffe, & Obstfeld, 2005, p. 409). The concept was introduced to organizational studies by Karl E. Weick in the 1970s and has affected both theory and practice. Weick intended to encourage a shift away from the traditional focus of organization theorists on decision-making and towards the processes that constitute the meaning of the decisions that are enacted in behavior.

In 1966, Daniel Katz and Robert L. Kahn published "The Social Psychology of Organizations" (Katz & Kahn, 1966). In 1969, Karl Weick played on this title in his book "The Social Psychology of Organizing", shifting the focus from organizations as entities to organiz"ing" as an activity. It was especially the second edition, published ten years later (Weick, 1979) that established Weick's approach in organization studies.

Weick identified seven properties of sensemaking (Weick, 1995):


Each of these seven aspects interact and intertwine as individuals interpret events. Their interpretations become evident through narratives – written and spoken – which convey the sense they have made of events (Currie & Brown, 2003), as well as through diagrammatic reasoning and associated material practices (Huff, 1990; Stigliani & Ravasi, 2012).

The rise of the sensemaking perspective marks a shift of focus in organization studies from how decisions shape organizations to how meaning drives organizing (Weick, 1993). The aim was to focus attention on the largely cognitive activity of framing experienced situations as meaningful. It is a collaborative process of creating shared awareness and understanding out of different individuals' perspectives and varied interests.

Sensemaking scholars are less interested in the intricacies of planning than in the details of action (Weick, 1995, p. 55).

The sensemaking approach is often used to provide insight into factors that surface as organizations address either uncertain or ambiguous situations (Weick 1988, 1993; Weick et al., 2005). Beginning in the 1980s with an influential re-analysis of the Bhopal disaster, Weick's name has come to be associated with the study of the situated sensemaking that influences the outcomes of disasters (Weick 1993).

A 2014 review of the literature on sensemaking in organizations identified a dozen different categories of sensemaking and a half-dozen sensemaking related concepts (Maitlis & Christianson, 2014). The categories of sensemaking included: constituent-minded, cultural, ecological, environmental, future-oriented, intercultural, interpersonal, market, political, prosocial, prospective, and resourceful. The sensemaking-related concepts included: sensebreaking, sensedemanding, sense-exchanging, sensegiving, sensehiding, and sense specification.

Sensemaking is central to the conceptual framework for military network-centric operations (NCO) espoused by the United States Department of Defense (Garstka and Alberts, 2004). In a joint/coalition military environment, sensemaking is complicated by numerous technical, social, organizational, cultural, and operational factors. A central hypothesis of NCO is that the quality of shared sensemaking and collaboration will be better in a "robustly networked" force than in a platform-centric force, empowering people to make better decisions. According to NCO theory, there is a mutually-reinforcing relationship among and between individual sensemaking, shared sensemaking, and collaboration.

In defense applications, sensemaking theorists have primarily focused on how shared awareness and understanding are developed within command and control organizations at the operational level. At the tactical level, individuals monitor and assess their immediate physical environment in order to predict where different elements will be in the next moment. At the operational level, where the situation is far broader, more complex and more uncertain, and evolves over hours and days, the organization must collectively make sense of enemy dispositions, intentions and capabilities, as well as anticipate the (often unintended) effects of own-force actions on a complex system of systems.

Sensemaking has been studied in the patient safety literature (Battles, et al. 2006). It has been used as a conceptual framework for identifying and detecting high risk patient situations. For example, Rhodes, et al. (2015) examined sensemaking and the co-production of safety of primary medical care patients.



</doc>
<doc id="1958462" url="https://en.wikipedia.org/wiki?curid=1958462" title="Subject-matter expert">
Subject-matter expert

A subject-matter expert (SME) or domain expert is a person who is an authority in a particular area or topic. The term "domain expert" is frequently used in expert systems software development, and there the term always refers to the domain other than the software domain. A domain expert is a person with special knowledge or skills in a particular area of endeavour (e.g. an accountant is an expert in the domain of accountancy). The development of accounting software requires knowledge in two different domains: accounting and software. Some of the development workers may be experts in one domain and not the other.

In general, the term is used when developing materials (a book, an examination, a manual, etc.) about a topic, and expertise on the topic is needed by the personnel developing the material. For example, tests are often created by a team of psychometricians and a team of SMEs. The psychometricians understand how to engineer a test while the SMEs understand the actual content of the exam. Books, manuals, and technical documentation are developed by technical writers and instructional designers in conjunctions with SMEs. Technical communicators interview SMEs to extract information and convert it into a form suitable for the audience. SMEs are often required to sign off on the documents or training developed, checking it for technical accuracy. SMEs are also necessary for the development of training materials.

In software engineering environments, the term is used to describe professionals with expertise in the field of application. The term "SME" also has a broader definition in engineering and high tech as one who has the greatest expertise in a technical topic. SMEs are often asked to review, improve, and approve technical work; to guide others; and to teach. According to Six Sigma an SME "exhibits the highest level of expertise in performing a specialized job, task, or skill of broad definition."

In pharmaceuticals and biotechnology areas, ASTM standard E 2500 specifies SMEs for various functions in project and process management. SME is defined as an individual who is an expert on that subject. In one project, there will be many SMEs who are experts on air, water, utilities, process machines, process, packaging, storage, distribution and supply chain management, to name a few.

In software development, as in the development of "complex computer systems" (e.g., artificial intelligence, expert systems, control, simulation, or business software) an SME is a person who is knowledgeable about the domain being represented (but often not knowledgeable about the programming technology used to represent it in the system). The SME tells the software developers what needs to be done by the computer system, and how the SME intends to use it. The SME may interact directly with the system, possibly through a simplified interface, or may codify domain knowledge for use by knowledge engineers or ontologists. An SME is also involved in validating the resulting system. SME has formal meaning in certain contexts such as CMMs.

In electronic discovery environments, the term "SME" labels professionals with expertise using Computer Assisted Review technology (CAR)/Technology Assisted Review (TAR) to perform searches designed to produce precisely refined results that identify groups of data as potentially responsive or non-responsive to relevant issues. E-discovery SMEs also typically have experience in constructing the search strings used in the search process.

A lawyer in an administrative agency may be designated an SME if he or she specializes in a particular field of law, such as tort, intellectual property rights etc.

In engineering and technical field, an SME is the one who is an authority in the design concept, calculations and performance of a system or process.




</doc>
<doc id="57147939" url="https://en.wikipedia.org/wiki?curid=57147939" title="Non-science">
Non-science

A non-science is an area of study that is not scientific, especially one that is not a natural science or a social science that is an object of scientific inquiry. In this model, history, art, and religion are all examples of non-sciences.

Since the 17th century, some writers have used the word "science" to exclude some areas of studies, such as the arts and the liberal arts. The word "nonscience", to describe non-scientific academic disciplines, was first used in the middle of the 19th century.

In some cases, it can be difficult to identify exact boundaries between science and non-science. The demarcation problem is the study of the difficulties in determining whether certain fields of study, near the boundaries of science and non-science, should be considered as one or the other. No single test has yet been devised that can clearly separate science from non-science, but some factors, taken as a whole and evaluated over time, are commonly used. In the view of Thomas Kuhn, these factors include the desire of scientists to investigate a question as if it were a puzzle. Kuhn's view of science is also focused on the process of scientific inquiry, rather than the result.

Boundary-work is the process of advocating for a desired outcome in the process of classifying fields of study that are near the borders. The rewards associated with winning a particular classification suggest that the boundary between science and non-science is socially constructed and ideologically motivated rather than representing a stark natural difference between science and non-science. The belief that scientific knowledge (e.g., biology) is more valuable than other forms of knowledge (e.g., ethics) is called "scientism".

Non-science includes all areas of study that are not science. Non-science encompasses all of the humanities, including:


The philosopher Martin Mahner proposed calling these academic fields the "parasciences", to distinguish them from disreputable forms of non-science, such as pseudoscience.

Non-sciences offer information about the meaning of life, human values, the human condition, and ways of interacting with other people, including studies of cultures, morality and ethics.

Philosophers disagree about whether areas of study involving abstract concepts, such as pure mathematics, are scientific or non-scientific.

Interdisciplinary studies may cover knowledge-generating work that includes both scientific and non-scientific studies. Archaeology is an example of a field that borrows from both the natural sciences and history.

Fields of inquiry may change status over time. For many centuries, alchemy was accepted as scientific: it produced some useful information, and it supported experiments and open inquiry in the pursuit of understanding the physical world. Since the 20th century, it has been considered a pseudoscience. Modern chemistry, which developed out of alchemy, is considered a major natural science.

Some philosophers, such as Paul Feyerabend, object to the effort to classify knowledge into science and non-science. The distinction is artificial, as there is little or nothing that ties together all of the bodies of knowledge that are called "sciences".

Some systems of organizing knowledge separate systematic knowledge from non-systematic methods of knowing or learning something, such as personal experiences, intuition, and innate knowledge. Wissenschaft is a broad concept that encompasses reliable knowledge without making a distinction between subject area. The "Wissenschaft" concept is more useful than the distinction between science and non-science in distinguishing between knowledge and pseudo-knowledge, as the errors made in all forms of pseudo-scholarship, from pseudohistory to pseudoscience, are similar. This "Wissenschaft" concept is used in the 2006 list of Fields of Science and Technology published by the Organisation for Economic Co-operation and Development, which defines "science and technology" as encompassing all humanistic disciplines, including religion and fine art.




</doc>
<doc id="22500921" url="https://en.wikipedia.org/wiki?curid=22500921" title="Outline of knowledge">
Outline of knowledge

The following outline is provided as an overview of and topical guide to knowledge:

Knowledge – familiarity with someone or something, which can include facts, information, descriptions, and/or skills acquired through experience or education. It can refer to the theoretical or practical understanding of a subject. It can be implicit (as with practical skill or expertise) or explicit (as with the theoretical understanding of a subject); and it can be more or less formal or systematic.



Taxonomies –



Epistemology – philosophy of knowledge. It is the study of knowledge and justified belief. It questions what knowledge is and how it can be acquired, and the extent to which knowledge pertinent to any given subject or entity can be acquired. Much of the debate in this field has focused on the philosophical analysis of the nature of knowledge and how it relates to connected notions such as truth, belief, and justification.

Knowledge management – 

Methods of obtaining knowledge –

Knowledge can be stored in:


Knowledge retrieval – Stored knowledge can be retrieved by:

Imparting knowledge means spreading or disseminating knowledge to others.




Sociology of knowledge –


The world's knowledge (knowledge possessed by human civilization)







</doc>
<doc id="57429694" url="https://en.wikipedia.org/wiki?curid=57429694" title="Surprisingly popular">
Surprisingly popular

The surprisingly popular answer is a wisdom of the crowd technique that taps into the expert minority opinion within a crowd. For a given question, a group is asked both "What do you think the right answer is?" and "What do you think the popular answer will be?" The answer that maximizes the average difference between the "right" answer and the "popular" answer is the "surprisingly popular" answer.

Question to be determined:<br>
Is Philadelphia the capital of Pennsylvania?
<br><br>Questions asked to the group and the response rates:<br>
Is Philadelphia the capital of Pennsylvania?
What do you think most people will respond to that question?

The difference between the answers to the "right" question and the "popular" question:

Thus, the "No" answer is surprisingly popular (10% > −10%). Because of the relatively high margin of 10%, there can be high confidence that the correct answer is "No". (The capital is indeed not Philadelphia, but Harrisburg.)

An illustrative breakdown of this follows. There are four groups of people.

This technique causes groups A and C to be eliminated from consideration and measures the difference in size between groups B and D.

Both groups B and D think they know something other people don't, but B is wrong and D is right. In cases where people feel like they have "inside" knowledge, it's more often the case that it's because they are correct and knowledgeable (group D), not because they are misled (group B).



</doc>
<doc id="18311131" url="https://en.wikipedia.org/wiki?curid=18311131" title="Privileged access">
Privileged access

In the fields of epistemology and philosophy of mind, a person (the subject, the self) has privileged access to their own thoughts. This implies the subject has access to, and knows, their own thoughts (has self-knowledge) in such a way that others do not. Privileged access can be characterized in two ways:


The still prevailing traditional position argues each of us do in fact have privileged access to our own thoughts. Descartes is the paradigmatic proponent of such kind of view (even though "privileged access" is an anachronic label for his thesis):

For Descartes, we still have privileged access even in the doubt scenario. That is, for him we would retain self-knowledge even in those extreme situations in which we can't have knowledge about anything else.

Gilbert Ryle, on the other hand, maintains a diametrically opposed view. According to the behaviorism of Ryle, each of us knows our own thoughts in the same way we know other's thoughts. We only come to know the thoughts of others through their linguistic and bodily behaviors, and must do exactly the same in order to know our own thoughts. There is no privileged access. We only have access to what we think upon evidences supplied through our own actions.



</doc>
<doc id="23369987" url="https://en.wikipedia.org/wiki?curid=23369987" title="Descriptive knowledge">
Descriptive knowledge

Descriptive knowledge, also declarative knowledge, propositional knowledge, or constative knowledge, is the type of knowledge that is, by its very nature, expressed in declarative sentences or indicative propositions. This distinguishes descriptive knowledge from what is commonly known as "knowing-how", or procedural knowledge (the knowledge of how, and especially how best, to perform some task), and "knowing of", or knowledge by acquaintance (the non-propositional knowledge of something through direct awareness of it). Descriptive knowledge is also identified as "knowing-that" or knowledge of fact, embodying concepts, principles, ideas, schemas, and theories. The entire descriptive knowledge of an individual constitute his understanding of the world and more specifically how it or a part of it works.

The distinction between knowing-how and knowing-that was introduced in epistemology by Gilbert Ryle. For Ryle, the former differs in its emphasis and purpose since it is primarily practical knowledge whereas the latter focuses on indicative or explanatory knowledge.



</doc>
<doc id="1120085" url="https://en.wikipedia.org/wiki?curid=1120085" title="Distributed knowledge">
Distributed knowledge

In multi-agent system research, distributed knowledge is all the knowledge that a community of agents possesses and might apply in solving a problem. Distributed knowledge is approximately what "a wise man knows" or what someone who has complete knowledge of what each member of the community knows knows. Distributed knowledge might also be called the aggregate knowledge of a community, as it represents all the knowledge that a community might bring to bear to solve a problem. Other related phrasings include cumulative knowledge, collective knowledge, pooled knowledge, or the wisdom of the crowd. Distributed knowledge is the union of all the knowledge of individuals in a community.

The logicians Aaleyah and Isko are sitting in their dark office wondering whether or not it is raining outside. Now, none of them actually knows, but Aaleyah knows something about her friend Yu Yan, namely that Yu Yan wears her red coat only if it is raining. Bob does not know this, but he just saw Yu Yan, and noticed that she was wearing her red coat. Even though none of them knows whether or not it is raining, it is "distributed knowledge" amongst them that it is raining. If either one of them tells the other what they know, it will be clear to the other that it is raining.

If we denote by formula_1 that Yu Yan wears a red coat and with formula_2 that if Yu Yan wears a red coat, it is raining, we have

Directly translated: Bob knows that Carol wears a red coat and Aaleyah knows that if Carol wears a red coat it is raining so together they know that it is raining.

Distributed knowledge is related to the concept Wisdom of the crowd. Distributed knowledge reflects the fact that "no one of us is smarter than all of us."




</doc>
<doc id="154170" url="https://en.wikipedia.org/wiki?curid=154170" title="Intuition">
Intuition

Intuition is the ability to acquire knowledge without recourse to conscious reasoning. Different writers give the word "intuition" a great variety of different meanings, ranging from direct access to unconscious knowledge, unconscious cognition, inner sensing, inner insight to unconscious pattern-recognition and the ability to understand something instinctively, without the need for conscious reasoning.

The word "intuition" comes from the Latin verb "intueri" translated as "consider" or from the late middle English word "intuit", "to contemplate".

Both Eastern and Western philosophers have studied the concept in great detail. Philosophy of mind deals with the concept.

In the East intuition is mostly intertwined with religion and spirituality, and various meanings exist from different religious texts.

In Hinduism various attempts have been made to interpret the Vedic and other esoteric texts.

For Sri Aurobindo intuition comes under the realms of knowledge by identity; he describes the psychological plane in humans (often referred to as mana in sanskrit) having two arbitrary natures, the first being imprinting of psychological experiences which is constructed through sensory information (mind seeking to become aware of external world). The second nature being the action when it seeks to be aware of itself, resulting in humans being aware of their existence or aware of being angry & aware of other emotions. He terms this second nature as knowledge by identity.
He finds that at present as the result of evolution the mind has accustomed itself to depend upon certain physiological functioning and their reactions as its normal means of entering into relations with the outer material world. As a result, when we seek to know about the external world the dominant habit is through arriving at truths about things via what our senses convey to us. However, knowledge by identity, which we currently only give the awareness of human beings' existence, can be extended further to outside of ourselves resulting in intuitive knowledge.

He finds this intuitive knowledge was common to older humans (Vedic) and later was taken over by reason which currently organises our perception, thoughts and actions resulting from Vedic to metaphysical philosophy and later to experimental science. He finds that this process, which seems to be decent, is actually a circle of progress, as a lower faculty is being pushed to take up as much from a higher way of working. He finds when self-awareness in the mind is applied to one's self and the outer (other) -self, results in luminous self-manifesting identity; the reason also converts itself into the form of the self-luminous intuitional knowledge.

Osho believed consciousness of human beings to be in increasing order from basic animal instincts to intelligence and intuition, and humans being constantly living in that conscious state often moving between these states depending on their affinity. He also suggests living in the state of intuition is one of the ultimate aims of humanity.

Advaita vedanta (a school of thought) takes intuition to be an experience through which one can come in contact with an experience Brahman.

Buddhism finds intuition to be a faculty in the mind of immediate knowledge and puts the term intuition beyond the mental process of conscious thinking, as the conscious thought cannot necessarily access subconscious information, or render such information into a communicable form. In Zen Buddhism various techniques have been developed to help develop one's intuitive capability, such as koans – the resolving of which leads to states of minor enlightenment (satori). In parts of Zen Buddhism intuition is deemed a mental state between the Universal mind and one's individual, discriminating mind.

In Islam there are various scholars with varied interpretations of intuition (often termed as hadas (Arabic: حدس), hitting correctly on a mark), sometimes relating the ability of having intuitive knowledge to prophethood. 
Siháb al Din-al Suhrawadi, in his book "Philosophy Of Illumination" (ishraq), finds that intuition is knowledge acquired through illumination, is mystical in nature, and also suggests mystical contemplation (mushahada) to bring about correct judgment. Ibn Sīnā finds the ability of having intuition as a "prophetic capacity" and describes it as knowledge obtained without intentionally acquiring it. He finds that regular knowledge is based on imitation while intuitive knowledge is based on intellectual certitude.

In the West, intuition does not appear as a separate field of study, and early mentions and definitions can be traced back to Plato. In his book "Republic" he tries to define intuition as a fundamental capacity of human reason to comprehend the true nature of reality. In his works "Meno" and "Phaedo", he describes intuition as a pre-existing knowledge residing in the "soul of eternity", and a phenomenon by which one becomes conscious of pre-existing knowledge. He provides an example of mathematical truths, and posits that they are not arrived at by reason. He argues that these truths are accessed using a knowledge already present in a dormant form and accessible to our intuitive capacity. This concept by Plato is also sometimes referred to as anamnesis. The study was later continued by his followers.

In his book "Meditations on First Philosophy", Descartes refers to an “intuition” as a pre-existing knowledge gained through rational reasoning or discovering truth through contemplation. This definition is commonly referred to as rational intuition. Later philosophers, such as Hume, have more ambiguous interpretations of intuition. Hume claims intuition is a recognition of relationships (relation of time, place, and causation) while he states that "the resemblance" (recognition of relations) "will strike the eye" (which would not require further examination) but goes on to state, "or rather in mind"—attributing intuition to power of mind, contradicting the theory of empiricism.

Immanuel Kant’s notion of “intuition” differs considerably from the Cartesian notion, and consists of the basic sensory information provided by the cognitive faculty of sensibility (equivalent to what might loosely be called perception). Kant held that our mind casts all of our external intuitions in the form of space, and all of our internal intuitions (memory, thought) in the form of time. Intuitionism is a position advanced by Luitzen Egbertus Jan Brouwer in philosophy of mathematics derived from Kant's claim that all mathematical knowledge is knowledge of the pure forms of the intuition—that is, intuition that is not empirical. Intuitionistic logic was devised by Arend Heyting to accommodate this position (and has been adopted by other forms of constructivism in general). It is characterized by rejecting the law of excluded middle: as a consequence it does not in general accept rules such as double negation elimination and the use of reductio ad absurdum to prove the existence of something.

Intuitions are customarily appealed to independently of any particular theory of how intuitions provide evidence for claims, and there are divergent accounts of what sort of mental state intuitions are, ranging from mere spontaneous judgment to a special presentation of a necessary truth. In recent years a number of philosophers, especially George Bealer have tried to defend appeals to intuition against Quinean doubts about conceptual analysis. A different challenge to appeals to intuition has recently come from experimental philosophers, who argue that appeals to intuition must be informed by the methods of social science.

The metaphilosophical assumption that philosophy ought to depend on intuitions has recently been challenged by experimental philosophers (e.g., Stephen Stich). One of the main problems adduced by experimental philosophers is that intuitions differ, for instance, from one culture to another, and so it seems problematic to cite them as evidence for a philosophical claim. Timothy Williamson has responded to such objections against philosophical methodology by arguing that intuition plays no special role in philosophy practice, and that skepticism about intuition cannot be meaningfully separated from a general skepticism about judgment. On this view, there are no qualitative differences between the methods of philosophy and common sense, the sciences or mathematics. Others like Ernest Sosa seek to support intuition by arguing that the objections against intuition merely highlight a verbal disagreement.

According to Sigmund Freud, knowledge could only be attained through the intellectual manipulation of carefully made observations and rejected any other means of acquiring knowledge such as intuition, and his findings could have been an analytic turn of his mind towards the subject.

In Carl Jung's theory of the ego, described in 1916 in "Psychological Types", intuition is an "irrational function", opposed most directly by sensation, and opposed less strongly by the "rational functions" of thinking and feeling. Jung defined intuition as "perception via the unconscious": using sense-perception only as a starting point, to bring forth ideas, images, possibilities, ways out of a blocked situation, by a process that is mostly unconscious.

Jung said that a person in whom intuition is dominant, an "intuitive type", acts not on the basis of rational judgment but on sheer intensity of perception. An extraverted intuitive type, "the natural champion of all minorities with a future", orients to new and promising but unproven possibilities, often leaving to chase after a new possibility before old ventures have borne fruit, oblivious to his or her own welfare in the constant pursuit of change. An introverted intuitive type orients by images from the unconscious, ever exploring the psychic world of the archetypes, seeking to perceive the meaning of events, but often having no interest in playing a role in those events and not seeing any connection between the contents of the psychic world and him- or herself. Jung thought that extraverted intuitive types were likely entrepreneurs, speculators, cultural revolutionaries, often undone by a desire to escape every situation before it becomes settled and constraining—even repeatedly leaving lovers for the sake of new romantic possibilities. His introverted intuitive types were likely mystics, prophets, or cranks, struggling with a tension between protecting their visions from influence by others and making their ideas comprehensible and reasonably persuasive to others—a necessity for those visions to bear real fruit.

In more-recent psychology, intuition can encompass the ability to know valid solutions to problems and decision making. For example, the recognition primed decision (RPD) model explains how people can make relatively fast decisions without having to compare options. Gary Klein found that under time pressure, high stakes, and changing parameters, experts used their base of experience to identify similar situations and intuitively choose feasible solutions. Thus, the RPD model is a blend of intuition and analysis. The intuition is the pattern-matching process that quickly suggests feasible courses of action. The analysis is the mental simulation, a conscious and deliberate review of the courses of action.

Instinct is often misinterpreted as intuition and its reliability considered to be dependent on past knowledge and occurrences in a specific area. For example, someone who has had more experiences with children will tend to have a better instinct about what they should do in certain situations with them. This is not to say that one with a great amount of experience is always going to have an accurate intuition.

Intuitive abilities were quantitatively tested at Yale University in the 1970s. While studying nonverbal communication, researchers noted that some subjects were able to read nonverbal facial cues before reinforcement occurred. In employing a similar design, they noted that highly intuitive subjects made decisions quickly but could not identify their rationale. Their level of accuracy, however, did not differ from that of non-intuitive subjects.

According to the works of Daniel Kahneman, intuition is the ability to automatically generate solutions without long logical arguments or evidence.

Intuition, as a gut feeling based on experience, has been found to be useful for business leaders for making judgement about people, culture and strategy. Law enforcement officers often claim to observe suspects and immediately "know" that they possess a weapon or illicit narcotic substances, which could also be action of instincts. Often unable to articulate why they reacted or what prompted them at the time of the event, they sometimes retrospectively can plot their actions based upon what had been clear and present danger signals. Such examples liken intuition to "gut feelings" and when viable illustrate preconscious activity.

Intuition Peak in Antarctica is so named "in appreciation of the role of scientific intuition for the advancement of human knowledge."



</doc>
<doc id="59301865" url="https://en.wikipedia.org/wiki?curid=59301865" title="Scholar">
Scholar

A scholar is a person who pursues academic and intellectual activities, particularly those that develop expertise in an area of study. A scholar may also be an academic, who works as a professor, teacher or researcher at a university or other higher education institution. An academic usually holds an advanced degree or terminal degree such as a PhD. Some independent scholars, such as writers and public intellectuals work outside of academia. They may still contribute to academic journals and participate in scholarly conferences even though they are unaffiliated with a university.

The term scholar is sometimes used with equivalent meaning to that of "academic" and describes in general those who attain mastery in a research discipline. However, it has wider application, with it also being used to describe those whose occupation was researched prior to organized higher education. In 1847, minister Emanuel Vogel Gerhart delivered an extensive address on the role of the scholar in society, writing:

Gerhart argued that a scholar can not be focused on a single discipline, contending that knowledge of multiple disciplines is necessary to put each into context and to inform the development of each:

A 2011 examination outlined the following attributes commonly accorded to scholars as "described by many writers, with some slight variations in the definition":

Scholars may rely on the scholarly method or scholarship, a body of principles and practices used by scholars to make their claims about the world as valid and trustworthy as possible, and to make them known to the scholarly public. It is the methods that systemically advance the teaching, research, and practice of a given scholarly or academic field of study through rigorous inquiry. Scholarship is creative, can be documented, can be replicated or elaborated, and can be and is peer-reviewed through various methods.

Scholars have generally been upheld as creditable figures of high social standing, who are engaged in work important to society. In Imperial China, in the period from 206 BC until AD 1912, the intellectuals were the "Scholar-officials" ("Scholar-gentlemen"), who were civil servants appointed by the Emperor of China to perform the tasks of daily governance. Such civil servants earned academic degrees by means of Imperial examination, and also were skilled calligraphers, and knew Confucian philosophy. Historian Wing-Tsit Chan concludes that:

In Joseon Korea (1392–1910), the intellectuals were the "literati", who knew how to read and write, and had been designated, as the chungin (the "middle people"), in accordance with the Confucian system. Socially, they constituted the petite bourgeoisie, composed of scholar-bureaucrats (scholars, professionals, and technicians) who administered the dynastic rule of the Joseon dynasty.

In his 1847 address, Gerhart asserted that scholars have an obligation to constantly continue their studies so as to remain aware of new knowledge being generated, and to contribute their own insights to the body of knowledge available to all:

Many scholars are also professors engaged in the teaching of others. In a number of countries, the title "research professor" refers to a professor who is exclusively or mainly engaged in research, and who has few or no teaching obligations. For example, the title is used in this sense in the United Kingdom (where it is known as research professor at some universities and professorial research fellow at some other institutions) and in northern Europe. 

Research professor is usually the most senior rank of a research-focused career pathway in those countries, and regarded as equal to the ordinary full professor rank. Most often they are permanent employees, and the position is often held by particularly distinguished scholars; thus the position is often seen as more prestigious than an ordinary full professorship. The title is used in a somewhat similar sense in the United States, with the exception that research professors in the United States are often not permanent employees and often must fund their salary from external sources, which is usually not the case elsewhere.

An independent scholar is anyone who conducts scholarly research outside universities and traditional academia. In 2010, twelve percent of US history scholars were independent. Independent scholars typically have a Master's degree or PhD.
In history, independent scholars can be differentiated from popular history hosts for television shows and amateur historians "by the level to which their publications utilize the analytical rigour and academic writing style".

In previous centuries, some independent scholars achieved renown, such as Samuel Johnson and Edward Gibbon during the eighteenth century and Charles Darwin and Karl Marx in the nineteenth century, and Sigmund Freud in the twentieth century. As well, there was a tradition of the man of letters, such as Evelyn Waugh. The term "man of letters" derives from the French term "belletrist" or "homme de lettres" but is not synonymous with "an academic". In the 17th and 18th centuries, the term "Belletrist(s)" came to be applied to the "literati": the French participants in—sometimes referred to as "citizens" of—the Republic of Letters, which evolved into the salon aimed at edification, education, and cultural refinement.

In the United States, a professional association exists for independent scholars: this association is the National Coalition of Independent Scholars. In Canada, the equivalent professional association is the Canadian Academy of Independent Scholars (in association with Simon Fraser University). Similar organizations exist around the world. Membership in a professional association generally entails a degree of post-secondary education and established research. When independent scholars participate in academic conferences, they may be referred to as an unaffiliated scholar, since they do not hold a position in a university or other institution.

While independent scholars may earn an income from part-time teaching, speaking engagements, or consultant work, the University of British Columbia calls earning an income the biggest challenge of being an independent scholar. Due to challenges of making a living as a scholar without an academic position, "[m]any independent scholars depend on having a gainfully employed partner". To get access to libraries and other research facilities, independent scholars have to seek permission from universities.

Writer Megan Kate Nelson's article "Stop Calling Me Independent" says the term "marginalizes unaffiliated scholars" and is unfairly seen as an indicator of "professional failure". Rebecca Bodenheimer says that independent scholars like her attending conferences who do not have a university name on their official name badge feel like the "independent scholar" term is perceived as a "signal that a scholar is either unwanted by the academy or unwilling to commit to the sacrifices necessary to succeed as an academic."




</doc>
<doc id="61032" url="https://en.wikipedia.org/wiki?curid=61032" title="Rationality">
Rationality

Rationality is the quality or state of being rational – that is, being based on or agreeable to reason. Rationality implies the conformity of one's beliefs with one's reasons to believe, and of one's actions with one's reasons for action. "Rationality" has different specialized meanings in philosophy, economics, sociology, psychology, evolutionary biology, game theory and political science.

To determine what behavior is the most rational, one needs to make several key assumptions, and also needs a logical formulation of the problem. When the goal or problem involves making a decision, rationality factors in all information that is available (e.g. complete or incomplete knowledge). Collectively, the formulation and background assumptions are the models within which rationality applies. Rationality is relative: if one accepts a model in which benefitting oneself is optimal, then rationality is equated with behavior that is self-interested to the point of being selfish; whereas if one accepts a model in which benefiting the group is optimal, then purely selfish behavior is deemed irrational. It is thus meaningless to assert rationality without also specifying the background model assumptions describing how the problem is framed and formulated.

The German sociologist Max Weber proposed an interpretation of social action that distinguished between four different idealized types of rationality. The first, which he called "Zweckrational" or purposive/instrumental rationality, is related to the expectations about the behavior of other human beings or objects in the environment. These expectations serve as means for a particular actor to attain ends, ends which Weber noted were "rationally pursued and calculated." The second type, Weber called "Wertrational" or value/belief-oriented. Here the action is undertaken for what one might call reasons intrinsic to the actor: some ethical, aesthetic, religious or other motives, independent of whether it will lead to success. The third type was affectual, determined by an actor's specific affect, feeling, or emotion—to which Weber himself said that this was a kind of rationality that was on the borderline of what he considered "meaningfully oriented." The fourth was traditional or conventional, determined by ingrained habituation. Weber emphasized that it was very unusual to find only one of these orientations: combinations were the norm. His usage also makes clear that he considered the first two as more significant than the others, and it is arguable that the third and fourth are subtypes of the first two.

The advantage in Weber's interpretation of rationality is that it avoids a value-laden assessment, say, that certain kinds of beliefs are irrational. Instead, Weber suggests that ground or motive can be given—for religious or affect reasons, for example—that may meet the criterion of explanation or justification even if it is not an explanation that fits the "Zweckrational" orientation of means and ends. The opposite is therefore also true: some means-ends explanations will not satisfy those whose grounds for action are "Wertrational".

Weber's constructions of rationality have been critiqued both from a Habermasian (1984) perspective (as devoid of social context and under-theorised in terms of social power) and also from a feminist perspective (Eagleton, 2003) whereby Weber's rationality constructs are viewed as imbued with masculine values and oriented toward the maintenance of male power. An alternative position on rationality (which includes both bounded rationality, as well as the affective and value-based arguments of Weber) can be found in the critique of Etzioni (1988), who reframes thought on decision-making to argue for a reversal of the position put forward by Weber. Etzioni illustrates how purposive/instrumental reasoning is subordinated by normative considerations (ideas on how people 'ought' to behave) and affective considerations (as a support system for the development of human relationships).

In the psychology of reasoning, psychologists and cognitive scientists have defended different positions on human rationality. One prominent view, due to Philip Johnson-Laird and Ruth M. J. Byrne among others is that humans are rational in principle but they err in practice, that is, humans have the competence to be rational but their performance is limited by various factors. However, it has been argued that many standard tests of reasoning, such as those on the conjunction fallacy, on the Wason selection task, or the base rate fallacy suffer from methodological and conceptual problems. This has led to disputes in psychology over whether researchers should (only) use standard rules of logic, probability theory and statistics, or rational choice theory as norms of good reasoning. Opponents of this view, such as Gerd Gigerenzer, favor a conception of bounded rationality, especially for tasks under high uncertainty.

Richard Brandt proposed a "reforming definition" of rationality, arguing someone is rational if their notions survive a form of cognitive-psychotherapy.

Abulof argues that rationality has become an "essentially contested concept," as its "proper use… inevitably involves endless disputes." He identifies "four fronts" for the disputes about the meaning of rationality: 

It is believed by some philosophers (notably A. C. Grayling) that a good rationale must be independent of emotions, personal feelings or any kind of instincts. Any process of evaluation or analysis, that may be called rational, is expected to be highly objective, logical and "mechanical". If these minimum requirements are not satisfied i.e. if a person has been, even slightly, influenced by personal emotions, feelings, instincts, or culturally specific moral codes and norms, then the analysis may be termed irrational, due to the injection of subjective bias.

Modern cognitive science and neuroscience show that studying the role of emotion in mental function (including topics ranging from flashes of scientific insight to making future plans), that no human has ever satisfied this criterion, except perhaps a person with no affective feelings, for example, an individual with a massively damaged amygdala or severe psychopathy. Thus, such an idealized form of rationality is best exemplified by computers, and not people. However, scholars may productively appeal to the idealization as a point of reference. 

Kant had distinguished theoretical from practical reason. Rationality theorist Jesús Mosterín makes a parallel distinction between theoretical and practical rationality, although, according to him, reason and rationality are not the same: reason would be a psychological faculty, whereas rationality is an optimizing strategy. Humans are not rational by definition, but they can think and behave rationally or not, depending on whether they apply, explicitly or implicitly, the strategy of theoretical and practical rationality to the thoughts they accept and to the actions they perform.

The distinction is also described as that between epistemic rationality, the attempt to form beliefs in an unbiased manner, and instrumental rationality.

Theoretical rationality has a formal component that reduces to logical consistency and a material component that reduces to empirical support, relying on our inborn mechanisms of signal detection and interpretation. Mosterín distinguishes between involuntary and implicit belief, on the one hand, and voluntary and explicit acceptance, on the other. Theoretical rationality can more properly be said to regulate our acceptances than our beliefs. Practical rationality is the strategy for living one's best possible life, achieving your most important goals and your own preferences in as far as possible.

As the study of arguments that are correct in virtue of their form, logic is of fundamental importance in the study of rationality. The study of rationality in logic is more concerned with epistemic rationality, that is, attaining beliefs in a rational manner, than instrumental rationality.

Rationality plays a key role in economics and there are several strands to this. Firstly, there is the concept of instrumentality—basically the idea that people and organisations are instrumentally rational—that is, adopt the best actions to achieve their goals. Secondly, there is an axiomatic concept that rationality is a matter of being logically consistent within your preferences and beliefs. Thirdly, people have focused on the accuracy of beliefs and full use of information—in this view, a person who is not rational has beliefs that don't fully use the information they have.

Debates within economic sociology also arise as to whether or not people or organizations are "really" rational, as well as whether it makes sense to model them as such in formal models. Some have argued that a kind of bounded rationality makes more sense for such models.

Others think that any kind of rationality along the lines of rational choice theory is a useless concept for understanding human behavior; the term "homo economicus" (economic man: the imaginary man being assumed in economic models who is logically consistent but amoral) was coined largely in honor of this view. Behavioral economics aims to account for economic actors as they actually are, allowing for psychological biases, rather than assuming idealized instrumental rationality.

Within artificial intelligence, a "rational agent" is typically one that maximizes its expected utility, given its current knowledge. Utility is the usefulness of the consequences of its actions. The utility function is arbitrarily defined by the designer, but should be a function of "performance", which is the directly measurable consequences, such as winning or losing money. In order to make a safe agent that plays defensively, a nonlinear function of performance is often desired, so that the reward for winning is lower than the punishment for losing. An agent might be rational within its own problem area, but finding the rational decision for arbitrarily complex problems is not practically possible. The rationality of human thought is a key problem in the psychology of reasoning.

There is an ongoing debate over the merits of using “rationality” in the study of international relations (IR). Some scholars hold it indispensable. Others are more critical. Still, the pervasive and persistent usage of "rationality" in political science and IR is beyond dispute. "Rationality" remains ubiquitous in this field. Abulof finds that Some 40% of all scholarly references to "foreign policy" allude to "rationality"—and this ratio goes up to more than half of pertinent academic publications in the 2000s. He further argues that when it comes to concrete security and foreign policies, IR employment of rationality borders on "malpractice": rationality-based descriptions are largely either false or unfalsifiable; many observers fail to explicate the meaning of "rationality" they employ; and the concept is frequently used politically to distinguish between "us and them."



</doc>
<doc id="319762" url="https://en.wikipedia.org/wiki?curid=319762" title="Logos">
Logos

Logos (, ; ; from , , ) is a term in Western philosophy, psychology, rhetoric, and religion derived from a Greek word variously meaning "ground", "plea", "opinion", "expectation", "word", "speech", "account", "reason", "proportion", and "discourse". It became a technical term in Western philosophy beginning with Heraclitus (), who used the term for a principle of order and knowledge.

Ancient Greek philosophers used the term in different ways. The sophists used the term to mean discourse. Aristotle applied the term to refer to "reasoned discourse" or "the argument" in the field of rhetoric, and considered it one of the three modes of persuasion alongside "ethos" and "pathos". Pyrrhonist philosophers used the term to refer to dogmatic accounts of non-evident matters. The Stoics spoke of the "logos spermatikos" (the generative principle of the Universe) which foreshadows related concepts in Neoplatonism.

Within Hellenistic Judaism, Philo () adopted the term into Jewish philosophy.
Philo distinguished between "logos prophorikos" ("the uttered word") and the "logos endiathetos" ("the word remaining within").

The Gospel of John identifies the Christian Logos, through which all things are made, as divine ("theos"), and further identifies Jesus Christ as the incarnate Logos. Early translators of the Greek New Testament such as Jerome (in the 4th century AD) were frustrated by the inadequacy of any single Latin word to convey the meaning of the word "logos" as used to describe Jesus Christ in the Gospel of John. The Vulgate Bible usage of was thus constrained to use the (perhaps inadequate) noun for "word", but later Romance language translations had the advantage of nouns such as in French. Reformation translators took another approach. Martin Luther rejected (verb) in favor of (word), for instance, although later commentators repeatedly turned to a more dynamic use involving "the living word" as felt by Jerome and Augustine. The term is also used in Sufism, and the analytical psychology of Carl Jung.

Despite the conventional translation as "word", "logos" is not used for a word in the grammatical sense; instead, the term "lexis" (, ) was used. However, both "logos" and "lexis" derive from the same verb (), meaning "(I) count, tell, say, speak".

The writing of Heraclitus () was the first place where the word "logos" was given special attention in ancient Greek philosophy, although Heraclitus seems to use the word with a meaning not significantly different from the way in which it was used in ordinary Greek of his time. For Heraclitus, "logos" provided the link between rational discourse and the world's rational structure.

What "logos" means here is not certain; it may mean "reason" or "explanation" in the sense of an objective cosmic law, or it may signify nothing more than "saying" or "wisdom". Yet, an independent existence of a universal "logos" was clearly suggested by Heraclitus.

Aristotle identifies two specific types of persuasion methods: artistic and inartistic. He defines artistic proofs as arguments that the rhetor generates and creates on their own. Examples of these include relationships, testimonies, and conjugates. He defines inartistic proofs as arguments that the rhetor quotes using information from a non-self-generated source. Examples of these include laws, contracts, and oaths.

Following one of the other meanings of the word, Aristotle gave "logos" a different technical definition in the "Rhetoric", using it as meaning argument from reason, one of the three modes of persuasion. The other two modes are "pathos" (, ), which refers to persuasion by means of emotional appeal, "putting the hearer into a certain frame of mind"; and "ethos" (, ), persuasion through convincing listeners of one's "moral character". According to Aristotle, "logos" relates to "the speech itself, in so far as it proves or seems to prove". In the words of Paul Rahe:

"Logos", "pathos", and "ethos" can all be appropriate at different times. Arguments from reason (logical arguments) have some advantages, namely that data are (ostensibly) difficult to manipulate, so it is harder to argue against such an argument; and such arguments make the speaker look prepared and knowledgeable to the audience, enhancing "ethos". On the other hand, trust in the speaker—built through "ethos"—enhances the appeal of arguments from reason.

Robert Wardy suggests that what Aristotle rejects in supporting the use of "logos" "is not emotional appeal per se, but rather emotional appeals that have no 'bearing on the issue', in that the "pathē" [, ] they stimulate lack, or at any rate are not shown to possess, any intrinsic connection with the point at issue—as if an advocate were to try to whip an antisemitic audience into a fury because the accused is Jewish; or as if another in drumming up support for a politician were to exploit his listeners's reverential feelings for the politician's ancestors".

Aristotle comments on the three modes by stating: 
The Pyrrhonist philosopher Sextus Empiricus defined the Pyrrhonist usage of "logos" as "When we say 'To every logos an equal logos is opposed,' by 'every logos' we mean 'every logos that has been considered by us,' and we use 'logos' not in its ordinary sense but for that which establishes something dogmatically, that is to say, concerning the non-evident, and which establishes it in any way at all, not necessarily by means of premises and conclusion."

Stoic philosophy began with Zeno of Citium , in which the "logos" was the active reason pervading and animating the Universe. It was conceived as material and is usually identified with God or Nature. The Stoics also referred to the "seminal logos" (""logos spermatikos"), or the law of generation in the Universe, which was the principle of the active reason working in inanimate matter. Humans, too, each possess a portion of the divine "logos".

The Stoics took all activity to imply a "logos" or spiritual principle. As the operative principle of the world, the "logos" was "anima mundi" to them, a concept which later influenced Philo of Alexandria, although he derived the contents of the term from Plato. In his Introduction to the 1964 edition of Marcus Aurelius' "Meditations", the Anglican priest Maxwell Staniforth wrote that "Logos" ... had long been one of the leading terms of Stoicism, chosen originally for the purpose of explaining how deity came into relation with the universe".

Public discourse on ancient Greek rhetoric has historically emphasized Aristotle's appeals to "logos", "pathos", and "ethos", while less attention has been directed to Isocrates' teachings about philosophy and "logos", and their partnership in generating an ethical, mindful "polis". Isocrates does not provide a single definition of "logos" in his work, but Isocratean "logos" characteristically focuses on speech, reason, and civic discourse. He was concerned with establishing the "common good" of Athenian citizens, which he believed could be achieved through the pursuit of philosophy and the application of "logos".

In the Septuagint the term "logos" is used for the word of God in the creation of heaven in Psalm 33:6, and in some related contexts.

Philo (), a Hellenized Jew, used the term "logos" to mean an intermediary divine being or demiurge. Philo followed the Platonic distinction between imperfect matter and perfect Form, and therefore intermediary beings were necessary to bridge the enormous gap between God and the material world. The "logos" was the highest of these intermediary beings, and was called by Philo "the first-born of God".
Philo also wrote that "the Logos of the living God is the bond of everything, holding all things together and binding all the parts, and prevents them from being dissolved and separated".

Plato's Theory of Forms was located within the "logos", but the "logos" also acted on behalf of God in the physical world. In particular, the Angel of the Lord in the Hebrew Bible (Old Testament) was identified with the "logos" by Philo, who also said that the "logos" was God's instrument in the creation of the Universe.

In Christology, the "Logos" () is a name or title of Jesus Christ, seen as the pre-existent second person of the Trinity. The concept derives from , which in the Douay–Rheims, King James, New International, and other versions of the Bible, reads:

Neoplatonist philosophers such as Plotinus (270 AD) used "logos" in ways that drew on Plato and the Stoics, but the term "logos" was interpreted in different ways throughout Neoplatonism, and similarities to Philo's concept of "logos" appear to be accidental. The "logos" was a key element in the meditations of Plotinus regarded as the first Neoplatonist. Plotinus referred back to Heraclitus and as far back as Thales in interpreting "logos" as the principle of meditation, existing as the interrelationship between the hypostases—the soul, the intellect ("nous"), and the One.

Plotinus used a trinity concept that consisted of "The One", the "Spirit", and "Soul". The comparison with the Christian Trinity is inescapable, but for Plotinus these were not equal and "The One" was at the highest level, with the "Soul" at the lowest. For Plotinus, the relationship between the three elements of his trinity is conducted by the outpouring of "logos" from the higher principle, and "eros" (loving) upward from the lower principle. Plotinus relied heavily on the concept of "logos", but no explicit references to Christian thought can be found in his works, although there are significant traces of them in his doctrine. Plotinus specifically avoided using the term "logos" to refer to the second person of his trinity. However, Plotinus influenced Gaius Marius Victorinus, who then influenced Augustine of Hippo. Centuries later, Carl Jung acknowledged the influence of Plotinus in his writings.

Victorinus differentiated between the "logos" interior to God and the "logos" related to the world by creation and salvation.

Augustine of Hippo, often seen as the father of medieval philosophy, was also greatly influenced by Plato and is famous for his re-interpretation of Aristotle and Plato in the light of early Christian thought. A young Augustine experimented with, but failed to achieve ecstasy using the meditations of Plotinus. In his "Confessions", Augustine described "logos" as the "Divine Eternal Word", by which he, in part, was able to motivate the early Christian thought throughout the Hellenized world (of which the Latin speaking West was a part) Augustine's "logos" "had taken body" in Christ, the man in whom the "logos" (i.e. or ) was present as in no other man.

The concept of the "logos" also exists in Islam, where it was definitively articulated primarily in the writings of the classical Sunni mystics and Islamic philosophers, as well as by certain Shi'a thinkers, during the Islamic Golden Age. In Sunni Islam, the concept of the "logos" has been given many different names by the denomination's metaphysicians, mystics, and philosophers, including "ʿaql" ("Intellect"), "al-insān al-kāmil" ("Universal Man"), "kalimat Allāh" ("Word of God"), "haqīqa muḥammadiyya" ("The Muhammadan Reality"), and "nūr muḥammadī" ("The Muhammadan Light").

One of the names given to a concept very much like the Christian Logos by the classical Muslim metaphysicians is "ʿaql", which is the "Arabic equivalent to the Greek (intellect)." In the writings of the Islamic Neoplatonist philosophers, such as al-Farabi () and Avicenna (d. 1037), the idea of the "ʿaql" was presented in a manner that both resembled "the late Greek doctrine" and, likewise, "corresponded in many respects to the Logos Christology."

The concept of "logos" in Sufism is used to relate the "Uncreated" (God) to the "Created" (humanity). In Sufism, for the Deist, no contact between man and God can be possible without the "logos". The "logos" is everywhere and always the same, but its personification is "unique" within each region. Jesus and Muhammad are seen as the personifications of the "logos", and this is what enables them to speak in such absolute terms.

One of the boldest and most radical attempts to reformulate the Neoplatonic concepts into Sufism arose with the philosopher Ibn Arabi, who traveled widely in Spain and North Africa. His concepts were expressed in two major works "The Ringstones of Wisdom" ("Fusus al-Hikam") and "The Meccan Illuminations" ("Al-Futūḥāt al-Makkiyya"). To Ibn Arabi, every prophet corresponds to a reality which he called a "logos" ("Kalimah"), as an aspect of the unique divine being. In his view the divine being would have for ever remained hidden, had it not been for the prophets, with "logos" providing the link between man and divinity.

Ibn Arabi seems to have adopted his version of the "logos" concept from Neoplatonic and Christian sources, although (writing in Arabic rather than Greek) he used more than twenty different terms when discussing it. For Ibn Arabi, the "logos" or "Universal Man" was a mediating link between individual human beings and the divine essence.

Other Sufi writers also show the influence of the Neoplatonic "logos". In the 15th century Abd al-Karīm al-Jīlī introduced the "Doctrine of Logos and the Perfect Man". For al-Jīlī, the "perfect man" (associated with the "logos" or the Prophet) has the power to assume different forms at different times and to appear in different guises.

In Ottoman Sufism, Şeyh Gâlib (d. 1799) articulates Sühan ("logos"-"Kalima") in his "Hüsn ü Aşk" ("Beauty and Love") in parallel to Ibn Arabi's Kalima. In the romance, "Sühan" appears as an embodiment of Kalima as a reference to the Word of God, the Perfect Man, and the Reality of Muhammad.

Carl Jung contrasted the critical and rational faculties of "logos" with the emotional, non-reason oriented and mythical elements of "eros". In Jung's approach, "logos" vs "eros" can be represented as "science vs mysticism", or "reason vs imagination" or "conscious activity vs the unconscious".

For Jung, "logos" represented the masculine principle of rationality, in contrast to its feminine counterpart, "eros":

Jung attempted to equate "logos" and "eros", his intuitive conceptions of masculine and feminine consciousness, with the alchemical Sol and Luna. Jung commented that in a man the lunar anima and in a woman the solar animus has the greatest influence on consciousness. Jung often proceeded to analyze situations in terms of "paired opposites", e.g. by using the analogy with the eastern yin and yang and was also influenced by the Neoplatonists.

In his book "Mysterium Coniunctionis" Jung made some important final remarks about anima and animus:
And in this book Jung again emphasized that the animus compensates "eros", while the anima'compensates "logos".

Author and professor Jeanne Fahnestock describes "logos" as a "premise". She states that, to find the reason behind a rhetor's backing of a certain position or stance, one must acknowledge the different "premises" that the rhetor applies via his or her chosen diction. The rhetor's success, she argues, will come down to "certain objects of agreement...between arguer and audience". "Logos is logical appeal, and the term logic is derived from it. It is normally used to describe facts and figures that support the speaker's topic." Furthermore, "logos" is credited with appealing to the audience's sense of logic, with the definition of "logic" being concerned with the thing as it is known.
Furthermore, one can appeal to this sense of logic in two ways. The first is through inductive reasoning, providing the audience with relevant examples and using them to point back to the overall statement. The second is through deductive enthymeme, providing the audience with general scenarios and then indicating commonalities among them.

The word "logos" has been used in different senses along with "rhema". Both Plato and Aristotle used the term "logos" along with "rhema" to refer to sentences and propositions.

The Septuagint translation of the Hebrew Bible into Greek uses the terms "rhema" and "logos" as equivalents and uses both for the Hebrew word "dabar", as the Word of God.

Some modern usage in Christian theology distinguishes "rhema" from "logos" (which here refers to the written scriptures) while "rhema" refers to the revelation received by the reader from the Holy Spirit when the Word ("logos") is read, although this distinction has been criticized.



</doc>
<doc id="60841932" url="https://en.wikipedia.org/wiki?curid=60841932" title="Arationality">
Arationality

Arationality is the state or characteristic of being arational, of being outside the domain of reason. The term is distinct from irrationality, which describes a state that goes "against" reason rather than beyond it. In this regard, that of going beyond reason, arationality is also contrary to positivism, the belief that reality can be understood rationally. 

Arationality is also identified with certain pre-modern modes of thinking, including magic and ritual.

The concept of arationality can be viewed in relation to the question of novelty. A new song for instance can always be reduced to the musical notes that compose it, all of which preexisted the song's creation. But despite being composed of "old" things (musical notes), one could still ask: to what extend did something new come into existence when the song was first composed. A real theory of novelty then, i.e. a theory that can account for "newness", should be able to show how things come into being without first reducing them to things that already are. To avoid this reductionist trap we might need to go beyond the very methods that work by way of reduction, i.e. scientific reason and formal logic. A true theory of novelty therefore might need to acknowledge in the creative process, whether artistic or physical, an arational source.

As an example we could think of the event we call the Big Bang. Cosmologists affirm that our universe started with an explosion that brought into existence, not only the matter and energy that constitutes it, but also the very fabric of time and space. This means that the event itself must have happened outside of time and space, in a "non-place" and "non-time". Considering that our logic works exclusively within the categories of time and space, something that happened before these categories even existed is necessarily beyond logic, or in other words, arational.



</doc>
<doc id="6171525" url="https://en.wikipedia.org/wiki?curid=6171525" title="Internal monologue">
Internal monologue

An internal monologue, also called self-talk, inner speech, inner discourse or internal discourse, is a person's inner voice which provides a running verbal monologue of thoughts while they are conscious. It is usually tied to a person's sense of self. It is particularly important in planning, problem solving, self-reflection, self-image, critical thinking, emotions, and subvocalization (reading in one's head). As a result, it is relevant to a number of mental disorders, such as depression, and treatments like cognitive behavioural therapy which seek to alleviate symptoms by providing strategies to regulate cognitive behaviour. It may reflect both conscious and subconscious beliefs.

In some cases people may think of inner speech as coming from an external source, as with schizophrenic auditory hallucinations. Additionally, not everyone has a verbal internal monologue (see ). The looser flow of thoughts and experiences, verbal or not, is called a stream of consciousness, which can also refer to a related technique in literature.

In a theory of child development formulated by Lev Vygotsky, inner speech has a precursor in private speech (talking to oneself) at a young age.

An inner discourse, or internal discourse, is a constructive act of the human mind and a tool for discovering new knowledge and making decisions. Along with feelings such as joy, anger, fear, etc., and sensory awareness, it is one of the few aspects of the processing of information and other mental activities of which humans can be directly aware. Inner discourse is so prominent in the human awareness of mental functioning that it may often seem to be synonymous with "mind". The view is then that "mind" means "what one experiences when thinking things out", and that "thinking things out" is believed to consist only of the words heard in internal discourse. This common sense idea of the mind must either block out the fact that the mind is constantly processing all kinds of information below the level of awareness, or else rename that activity to some putatively "non-mental" status such as "reflex reaction" or even, sometimes, "demon possession". 

An inner discourse takes place much as would a discussion with a second person. One might think, "I need $27 for the paper boy. I have some cash in my wallet. Ten plus ten plus five... I have $25. Maybe I dropped coins in the sofa. Ah, here they are..." The ideal form of inner discourse would seem to be one that starts with statements about matters of fact and proceeds with logical rigor until a solution is achieved.

On this view of thinking, progress toward better thinking is made when one learns how to evaluate how well "statements of fact" are actually grounded, and when one learns how to avoid logical errors. But one must also take account of questions like why one is seeking a solution (Why do I want to contribute money to this charity?), and why one may keep getting results that turn out to be biased in fairly consistent patterns (Why do I never give to charities that benefit a certain group?).

Negative self-talk has been implicated in contributing to psychological disorders including depression, anxiety, and bulimia nervosa.
Cognitive therapy aims to improve functioning by helping people identify and change negative self-talk. It involves identifying the beliefs that colour our perception of the world. Reminding yourself to never say anything to yourself that you would not say to a friend is a good strategy to develop positive self-talk.

Challenging unhelpful or negative thoughts can include questions that:

Negative self-talk (also known as unhelpful self-talk) refers to inner critical dialogue. It is based on beliefs about ourselves that develop during childhood based on feedback of others, particularly parents. These beliefs create a lens through which the present is viewed. Examples of these core beliefs that lead to negative self-talk are: "I am worthless", "I am a failure", "I am unlovable".

Positive self-talk (also known as helpful self-talk) involves noticing the reality of the situation, overriding beliefs and biases that can lead to negative self-talk.

Coping self-talk is a particular form of positive self-talk that helps improve performance. It is more effective than generic positive self-talk. and improves engagement in a task. It has three components:
An example of coping self-talk is, "John, you're anxious about doing the presentation. Most of the other students are as well. You will be fine."
Coping self-talk is a healthy coping strategy.

Instructional self-talk focuses attention on the components of a task and can improve performance on physical tasks that are being learnt, however it can be detrimental for people who are already skilled in the task.

Inner speech is strongly associated with a sense of self, and the development of this sense in children is tied to the development of language. There are, however, examples of an internal monologue or inner voice being considered external to the self, such as auditory hallucinations, the conceptualisation of negative or critical thoughts as an inner critic, and as a kind of divine intervention. As a delusion, this can be called "thought insertion".

Though not necessarily external, a conscience is also often thought of as an "inner voice".

According to one study, there is wide variation in how often people report experiencing internal monologue, and some people report very little or none. Younger children are less likely to report using inner speech instead of visual thinking than older children and adults, though it is not known whether this is due to lack of inner speech, or due to insufficiently developed introspection. This has been cited as evidence for the "language of thought" hypothesis, which posits an underlying language of the brain, or "mentalese", distinct from a thinker's native tongue.

The ways in which the inner voice acts have been correlated with certain mental conditions such as posttraumatic stress disorder and anxiety. This form of internal monologue may be inherently critical of the person, and even go so far as to feature direct insults or challenges to the individual's behaviour. According to Dr. Lisa Firestone, this "inner critic" is "based on implicit memories of trauma experienced in childhood", and may be the result of both significant traumas (that result in PTSD or other stress disorders) or minor ones.

One study found that inner speech use was reported most frequently for self-regulation (e.g. planning and problem solving), self‐reflection (e.g. emotions, self‐motivation, appearance, behavior/performance, and autobiography), and critical thinking (e.g., evaluating, judging, and criticizing).

In the 1920s, Swiss developmental psychologist Jean Piaget proposed the idea that private (or "egocentric") speech—speaking to yourself out loud—is the initial form of speech, from which "social speech" develops, and that it dies out as children grow up. In the 1930s, Russian psychologist Lev Vygotsky proposed instead that private speech develops "from" social speech, and later becomes internalised as an internal monologue, rather than dying out. This interpretation has come to be the more widely accepted, and is supported by empirical research.

Implicit in the idea of a social origin to inner speech is the possibility of "inner "dialogue"" – a form of "internal collaboration with oneself." However, Vygotsky believed inner speech takes on its own syntactic peculiarities, with heavy use of abbreviation and omission compared with oral speech (even more so compared with written speech).

Andy Clark (1998) writes that social language is "especially apt to be co-opted for more private purposes of [...] self-inspection and self-criticism," although others have defended the same conclusions on different grounds.

The concept of internal monologue is not new, but the emergence of the functional MRI has led to a better understanding of the mechanisms of internal speech by allowing researchers to see localized brain activity.

Studies have revealed the differences in neural activations of inner dialogues versus those of monologues. Functional MRI imaging studies have shown that monologic internal speech involves the activation of the superior temporal gyrus and the left inferior frontal gyrus, which is the standard language system that is activated during any kind of speech. However, dialogical inner speech implicates several additional neural regions. Studies have indicated overlap with regions involved with thinking about other minds.

In regard to research on inner speech Fernyhough stated, "The new science of inner speech tells us that it is anything but a solitary process. Much of the power of self-talk comes from the way it orchestrates a dialogue between different points of view." Based on interpretation of functional medical-imaging, Fernyhough believes that language system of internal dialogue works in conjunction with a part of the social cognition system (localized in the right hemisphere close to the intersection between the temporal and parietal lobes). Neural imaging seems to support Vygotsky's theory that when individuals are talking to themselves, they are having an actual conversation. Intriguingly, individuals did not exhibit this same arrangement of neural activation with silent monologues. In past studies, it has been supported that these two brain hemispheres to have different functions. Based on Functional magnetic resonance imaging studies, inner speech has been shown to more significant activations farther back in the temporal lobe, in Heschl's gyrus.

However, the results of neural imaging have to be taken with caution because the regions of the brain activated during spontaneous, natural internal speech diverge from those that are activated on demand. In research studies, individuals are asked to talk to themselves on demand, which is different than the natural development of inner speech within one's mind. The concept of internal monologue is an elusive study and is subjective to many implications with future studies.

In literary criticism there is a similar term, interior monologue. This, sometimes, is used as a synonym for stream of consciousness: a narrative mode or method that attempts to depict the multitudinous thoughts and feelings which pass through the mind. However, the "Oxford Dictionary of Literary Terms" suggests, that "they can also be distinguished psychologically and literarily. In a psychological sense, stream of consciousness is the subject‐matter, while interior monologue is the technique for presenting it". And for literature, "while an interior monologue always presents a character's thoughts 'directly', without the apparent intervention of a summarizing and selecting narrator, it does not necessarily mingle them with impressions and perceptions, nor does it necessarily violate the norms of grammar, or logic—but the stream of consciousness technique also does one or both of these things".




</doc>
<doc id="637990" url="https://en.wikipedia.org/wiki?curid=637990" title="Logical reasoning">
Logical reasoning

Two kinds of logical reasoning can be distinguished in addition to formal deduction: induction and abduction. Given a precondition or "premise", a conclusion or "logical consequence" and a rule or "material conditional" that implies the "conclusion" given the "precondition", one can explain the following.


Within the context of a mathematical model, the three kinds of reasoning can be described as follows. The construction/creation of the structure of the model is "abduction". Assigning values (or probability distributions) to the parameters of the model is "induction". Executing/running the model is "deduction".




</doc>
<doc id="832518" url="https://en.wikipedia.org/wiki?curid=832518" title="Intellectualism">
Intellectualism

Intellectualism denotes the use, development, and exercise of the intellect; the practice of being an intellectual; and the Life of the Mind. In the field of philosophy, “intellectualism” occasionally is synonymous with “rationalism”, that is, knowledge mostly derived from reason and ratiocination. Socially, “intellectualism” negatively connotes: single-mindedness of purpose (“too much attention to thinking”) and emotional coldness (“the absence of affection and feeling”).

In the view of Socrates (c. 470 – 399 BC), intellectualism allows that “one will do what is right or best just as soon as one truly understands what is right or best”; that virtue is a purely intellectual matter, since virtue and knowledge are familial relatives, which a person accrues and improves with dedication to reason. So defined, Socratic intellectualism became a key philosophic doctrine of Stoicism. The apparent, problematic consequences of this view are “Socratic paradoxes”, such as the view that there is no weakness of will — that no one knowingly does, or seeks to do, evil (moral wrong); that anyone who does, or seeks to do, moral wrong does so involuntarily; and that virtue is knowledge, that there are not many virtues, but that all virtues are one.

Contemporary philosophers dispute that Socrates’s conceptions of knowing truth, and of ethical conduct, can be equated with modern, post–Cartesian conceptions of knowledge and of rational intellectualism. As such, Michel Foucault demonstrated, with detailed historical study, that in Classical Antiquity (800 BC – AD 1000), “knowing the truth” is akin to “spiritual knowledge”, in the contemporarily understanding of the concept. Hence, without exclusively concerning the rational intellect, spiritual knowledge is integral to the broader principle of “caring for the self”.

Typically, such care of the self-involved specific ascetic exercises meant to ensure that not only was knowledge of truth memorized, but learned, and then integrated to the self, in the course of transforming oneself into a good person. Therefore, to understand truth meant “intellectual knowledge” requiring one’s integration to the (universal) truth, and authentically living it in one’s speech, heart, and conduct. Achieving that difficult task required continual care of the self, but also meant being someone who embodies truth, and so can readily practice the Classical-era rhetorical device of parrhesia: “to speak candidly, and to ask forgiveness for so speaking”; and, by extension, practice the moral obligation to speak the truth for the common good, even at personal risk. This ancient, Socratic moral philosophic perspective contradicts the contemporary understanding of truth and knowledge as rational undertakings.

Medieval theological intellectualism is a doctrine of divine action, wherein the faculty of intellect precedes, and is superior to, the faculty of the will ("voluntas intellectum sequitur"). As such, Intellectualism is contrasted with voluntarism, which proposes the Will as superior to the intellect, and to the emotions; hence, the stance that “according to intellectualism, choices of the Will result from that which the intellect recognizes as good; the will, itself, is determined. For voluntarism, by contrast, it is the Will which identifies which objects are good, and the Will, itself, is indetermined”. From that philosophical perspective and historical context, the Spanish Muslim polymath Averroës (1126–1198) in the 12th century, the Italian Christian theologian Thomas Aquinas (1225–1274), and the German Christian theologian Meister Eckhart (1260–1327) in the 13th century, are recognised intellectualists.



</doc>
<doc id="161999" url="https://en.wikipedia.org/wiki?curid=161999" title="Idea">
Idea

In philosophy, ideas (E I D I E A)are usually taken as mental representational images of some object. Ideas can also be abstract concepts that do not present as mental images. Many philosophers have considered ideas to be a fundamental ontological category of being. The capacity to create and understand the meaning of ideas is considered to be an essential and defining feature of human beings. In a popular sense, an idea arises in a reflexive, spontaneous manner, even without thinking or serious reflection, for example, when we talk about the "idea" of a person or a place. A new or original idea can often lead to innovation.

The word "idea" comes from Greek ἰδέα "idea" "form, pattern," from the root of ἰδεῖν "idein", "to see." 

One view on the nature of ideas is that there exist some ideas (called "innate ideas") which are so general and abstract that they could not have arisen as a representation of an object of our perception but rather were in some sense always present. These are distinguished from "adventitious ideas" which are images or concepts which are accompanied by the judgment that they are caused or occasioned by an external object.

Another view holds that we only discover ideas in the same way that we discover the real world, from personal experiences. The view that humans acquire all or almost all their behavioral traits from nurture (life experiences) is known as "tabula rasa" ("blank slate"). Most of the confusions in the way ideas arise is at least in part due to the use of the term "idea" to cover both the representation perceptics and the object of conceptual thought. This can be always illustrated in terms of the scientific doctrines of innate ideas, "concrete ideas versus abstract ideas", as well as "simple ideas versus complex ideas".

Plato in Ancient Greece was one of the earliest philosophers to provide a detailed discussion of ideas and of the thinking process (in Plato's Greek the word "idea" carries a rather different sense from our modern English term). Plato argued in dialogues such as the "Phaedo", "Symposium", "Republic", and "Timaeus" that there is a realm of ideas or forms ("eidei"), which exist independently of anyone who may have thoughts on these ideas, and it is the ideas which distinguish mere opinion from knowledge, for unlike material things which are transient and liable to contrary properties, ideas are unchanging and nothing but just what they are. Consequently, Plato seems to assert forcefully that material things can only be the objects of opinion; real knowledge can only be had of unchanging ideas. Furthermore, ideas for Plato appear to serve as universals; consider the following passage from the "Republic":
Descartes often wrote of the meaning of "idea" as an image or representation, often but not necessarily "in the mind", which was well known in the vernacular. Despite that Descartes is usually credited with the invention of the non-Platonic use of the term, he at first followed this vernacular use. In his "Meditations on First Philosophy" he says, "Some of my thoughts are like images of things, and it is to these alone that the name 'idea' properly belongs." He sometimes maintained that ideas were innate and uses of the term "idea" diverge from the original primary scholastic use. He provides multiple non-equivalent definitions of the term, uses it to refer to as many as six distinct kinds of entities, and divides "ideas" inconsistently into various genetic categories. For him knowledge took the form of ideas and philosophical investigation is the deep consideration of these entities.

In striking contrast to Plato's use of idea is that of John Locke. In his Introduction to An Essay Concerning Human Understanding, Locke defines "idea" as "that term which, I think, serves best to stand for whatsoever is the object of the understanding when a man thinks, I have used it to express whatever is meant by phantasm, notion, species, or whatever it is which the mind can be employed about in thinking; and I could not avoid frequently using it." He said he regarded the book necessary to examine our own abilities and see what objects our understandings were, or were not, fitted to deal with. In his philosophy other outstanding figures followed in his footsteps — Hume and Kant in the 18th century, Arthur Schopenhauer in the 19th century, and Bertrand Russell, Ludwig Wittgenstein, and Karl Popper in the 20th century. Locke always believed in "good sense" — not pushing things to extremes and on taking fully into account the plain facts of the matter. He considered his common-sense ideas "good-tempered, moderate, and down-to-earth."

As John Locke studied humans in his work “An Essay Concerning Human Understanding” he continually referenced Descartes for ideas as he asked this fundamental question: “When we are concerned with something about which we have no certain knowledge, what rules or standards should guide how confident we allow ourselves to be that our opinions are right?” A simpler way of putting it is how do humans know ideas, and what are the different types of ideas. An idea to Locke “can simply mean some sort of brute experience.” He shows that there are “No innate principles in the mind.”. Thus, he concludes that “our ideas are all experiential in nature.” An experience can either be a sensation or a reflection: “consider whether there are any innate ideas in the mind before any are brought in by the impression from sensation or reflection.” Therefore, an idea was an experience in which the human mind apprehended something.

In a Lockean view, there are really two types of ideas: complex and simple. Simple ideas are the building blocks for much more complex ideas, and “While the mind is wholly passive in the reception of simple ideas, it is very active in the building of complex ideas…” Complex ideas, therefore, can either be modes, substances, or relations. Modes are when ideas are combined in order to convey new information. For instance, David Banach gives the example of beauty as a mode. He says that it is the combination of color and form. Substances, however, is different. Substances are certain objects, that can either be dogs, cats, or tables. And relations represent the relationship between two or more ideas. In this way, Locke did, in fact, answer his own questions about ideas and humans.

Hume differs from Locke by limiting "idea" to the more or less vague mental reconstructions of perceptions, the perceptual process being described as an "impression." Hume shared with Locke the basic empiricist premise that it is only from life experiences (whether their own or others') that humans' knowledge of the existence of anything outside of themselves can be ultimately derived, that they shall carry on doing what they are prompted to do by their emotional drives of varying kinds. In choosing the means to those ends, they shall follow their accustomed associations of ideas. Hume has contended and defended the notion that "reason alone is merely the 'slave of the passions'." 

Immanuel Kant defines an "idea" as opposed to a "concept". "Regulative ideas" are ideals that one must tend towards, but by definition may not be completely realized. Liberty, according to Kant, is an idea. The autonomy of the rational and universal subject is opposed to the determinism of the empirical subject. Kant felt that it is precisely in knowing its limits that philosophy exists. The business of philosophy he thought was not to give rules, but to analyze the private judgement of good common sense.

Whereas Kant declares limits to knowledge ("we can never know the thing in itself"), in his epistemological work, Rudolf Steiner sees "ideas" as "objects of experience" which the mind apprehends, much as the eye apprehends light. In "Goethean Science" (1883), he declares, "Thinking ... is no more and no less an organ of perception than the eye or ear. Just as the eye perceives colors and the ear sounds, so thinking perceives ideas." He holds this to be the premise upon which Goethe made his natural-scientific observations.

Wundt widens the term from Kant's usage to include "conscious representation of some object or process of the external world". In so doing, he includes not only ideas of memory and imagination, but also perceptual processes, whereas other psychologists confine the term to the first two groups. One of Wundt's main concerns was to investigate conscious processes in their own context by experiment and introspection. He regarded both of these as "exact methods", interrelated in that experimentation created optimal conditions for introspection. Where the experimental method failed, he turned to other "objectively valuable aids", specifically to "those products of cultural communal life which lead one to infer particular mental motives. Outstanding among these are speech, myth, and social custom." Wundt designed the basic mental activity apperception — a unifying function which should be understood as an activity of the will. Many aspects of his empirical physiological psychology are used today. One is his principles of mutually enhanced contrasts and of assimilation and dissimilation (i.e. in color and form perception and his advocacy of "objective" methods of expression and of recording results, especially in language. Another is the principle of heterogony of ends — that multiply motivated acts lead to unintended side effects which in turn become motives for new actions.

C. S. Peirce published the first full statement of pragmatism in his important works "" (1878) and "" (1877). In "How to Make Our Ideas Clear" he proposed that a "clear idea" (in his study he uses concept and "idea" as synonymic) is defined as one, when it is apprehended such as it will be recognized wherever it is met, and no other will be mistaken for it. If it fails of this clearness, it is said to be obscure. He argued that to understand an idea clearly we should ask ourselves what difference its application would make to our evaluation of a proposed solution to the problem at hand. Pragmatism (a term he appropriated for use in this context), he defended, was a method for ascertaining the meaning of terms (as a theory of meaning). The originality of his ideas is in their rejection of what was accepted as a view and understanding of knowledge by scientists for some 250 years, i.e. that, he pointed, knowledge was an impersonal fact. Peirce contended that we acquire knowledge as "participants", not as "spectators". He felt "the real", sooner or later, is information acquired through ideas and knowledge with the application of logical reasoning would finally result in. He also published many papers on logic in relation to "ideas".

G. F. Stout and J. M. Baldwin, in the "Dictionary of Philosophy and Psychology", define "idea" as "the reproduction with a more or less adequate image, of an object not actually present to the senses." They point out that an idea and a perception are by various authorities contrasted in various ways. "Difference in degree of intensity", "comparative absence of bodily movement on the part of the subject", "comparative dependence on mental activity", are suggested by psychologists as characteristic of an idea as compared with a perception.

It should be observed that an idea, in the narrower and generally accepted sense of a mental reproduction, is frequently composite. That is, as in the example given above of the idea of a chair, a great many objects, differing materially in detail, all call a single idea. When a man, for example, has obtained an idea of chairs in general by comparison with which he can say "This is a chair, that is a stool", he has what is known as an "abstract idea" distinct from the reproduction in his mind of any particular chair (see abstraction). Furthermore, a complex idea may not have any corresponding physical object, though its particular constituent elements may severally be the reproductions of actual perceptions. Thus the idea of a centaur is a complex mental picture composed of the ideas of man and horse, that of a mermaid of a woman and a fish.

Diffusion studies explore the spread of ideas from culture to culture. Some anthropological theories hold that all cultures imitate ideas from one or a few original cultures, the Adam of the Bible, or several cultural circles that overlap. Evolutionary diffusion theory holds that cultures are influenced by one another but that similar ideas can be developed in isolation.

In the mid-20th century, social scientists began to study how and why ideas spread from one person or culture to another. Everett Rogers pioneered diffusion of innovations studies, using research to prove factors in adoption and profiles of adopters of ideas. In 1976, in his book "The Selfish Gene", Richard Dawkins suggested applying biological evolutionary theories to the spread of ideas. He coined the term "meme" to describe an abstract unit of selection, equivalent to the gene in evolutionary biology.

James Boswell recorded Samuel Johnson's opinion about ideas. Johnson claimed that they are mental images or internal visual pictures. As such, they have no relation to words or the concepts which are designated by verbal names.

To protect the cause of invention and innovation, the legal constructions of Copyrights and Patents were established. Patent law regulates various aspects related to the functional manifestation of inventions based on new ideas or incremental improvements to existing ones. Thus, patents have a direct relationship to ideas.

In some cases, authors can be granted limited legal monopolies on the manner in which certain works are expressed. This is known colloquially as copyright, although the term intellectual property is used mistakenly in place of "copyright". Copyright law regulating the aforementioned monopolies generally does not cover the actual ideas. The law does not bestow the legal status of property upon ideas per se. Instead, laws purport to regulate events related to the usage, copying, production, sale and other forms of exploitation of the fundamental expression of a work, that may or may not carry ideas. Copyright law is fundamentally different from patent law in this respect: patents do grant monopolies on ideas (more on this below).

A copyright is meant to regulate some aspects of the usage of expressions of a work, "not" an idea. Thus, copyrights have a negative relationship to ideas.

Work means a tangible medium of expression. It may be an original or derivative work of art, be it literary, dramatic, musical recitation, artistic, related to sound recording, etc. In (at least) countries adhering to the Berne Convention, copyright automatically starts covering the work upon the original creation and fixation thereof, without any extra steps. While creation usually involves an idea, the idea in itself does not suffice for the purposes of claiming copyright. 
Confidentiality and nondisclosure agreements are legal instruments that assist corporations and individuals in keeping ideas from escaping to the general public. Generally, these instruments are covered by contract law.




</doc>
<doc id="2951899" url="https://en.wikipedia.org/wiki?curid=2951899" title="Epistemological rupture">
Epistemological rupture

Epistemological rupture (epistemological break or epistemological obstacle; ), is a notion introduced in 1938 by French philosopher Gaston Bachelard, and later used by Louis Althusser. 

Bachelard proposed that the history of science is replete with "epistemological obstacles"—or unthought/unconscious structures that were immanent within the realm of the sciences, such as principles of division (e.g., mind/body). The history of science, Bachelard asserted, consisted in the formation and establishment of these epistemological obstacles, and then the subsequent tearing down of the obstacles. This latter stage is an epistemological rupture—where an unconscious obstacle to scientific thought is thoroughly ruptured or broken away from.

Epistemology, from the Greek words "episteme" ("knowledge") and "logos" ("word, speech") is the branch of philosophy that deals with the nature, origin and scope of knowledge. Rupture, from Old French "rupture" or Latin "ruptura", is defined as an instance of breaking or bursting suddenly and completely, as well as a breach of a harmonious link in a figurative way.



</doc>
<doc id="42446" url="https://en.wikipedia.org/wiki?curid=42446" title="Reason">
Reason

Reason is the capacity of consciously making sense of things, applying logic, and adapting or justifying practices, institutions, and beliefs based on new or existing information. It is closely associated with such characteristically human activities as philosophy, science, language, mathematics, and art, and is normally considered to be a distinguishing ability possessed by humans.
Reason, or an aspect of it, is sometimes referred to as rationality.

Reasoning is associated with thinking, cognition, and intellect. The field of logic studies ways in which humans reason formally through argument. Reasoning may be subdivided into forms of logical reasoning (forms associated with the strict sense): deductive reasoning, inductive reasoning, abductive reasoning; and other modes of reasoning considered more informal, such as intuitive reasoning and verbal reasoning. Along these lines, a distinction is often drawn between logical, discursive reasoning (reason proper), and intuitive reasoning, in which the reasoning process through intuition—however valid—may tend toward the personal and the subjectively opaque. In some social and political settings logical and intuitive modes of reasoning may clash, while in other contexts intuition and formal reason are seen as complementary rather than adversarial. For example, in mathematics, intuition is often necessary for the creative processes involved with arriving at a formal proof, arguably the most difficult of formal reasoning tasks.

Reasoning, like habit or intuition, is one of the ways by which thinking moves from one idea to a related idea. For example, reasoning is the means by which rational individuals understand sensory information from their environments, or conceptualize abstract dichotomies such as cause and effect, truth and falsehood, or ideas regarding notions of good or evil. Reasoning, as a part of executive decision making, is also closely identified with the ability to self-consciously change, in terms of goals, beliefs, attitudes, traditions, and institutions, and therefore with the capacity for freedom and self-determination.

In contrast to the use of "reason" as an abstract noun, a reason is a consideration given which either explains or justifies events, phenomena, or behavior. Reasons justify decisions, reasons support explanations of natural phenomena; reasons can be given to explain the actions (conduct) of individuals.

Using reason, or reasoning, can also be described more plainly as providing good, or the best, reasons. For example, when evaluating a moral decision, "morality is, at the very least, the effort to guide one's conduct by "reason"—that is, doing what there are the best reasons for doing—while giving equal [and impartial] weight to the interests of all those affected by what one does."

Psychologists and cognitive scientists have attempted to study and explain how people reason, e.g. which cognitive and neural processes are engaged, and how cultural factors affect the inferences that people draw. The field of automated reasoning studies how reasoning may or may not be modeled computationally. Animal psychology considers the question of whether animals other than humans can reason.

In the English language and other modern European languages, "reason", and related words, represent words which have always been used to translate Latin and classical Greek terms in the sense of their philosophical usage.

The earliest major philosophers to publish in English, such as Francis Bacon, Thomas Hobbes, and John Locke also routinely wrote in Latin and French, and compared their terms to Greek, treating the words ""logos", "ratio", "raison"" and "reason" as interchangeable. The meaning of the word "reason" in senses such as "human reason" also overlaps to a large extent with "rationality" and the adjective of "reason" in philosophical contexts is normally "rational", rather than "reasoned" or "reasonable". Some philosophers, Thomas Hobbes for example, also used the word "ratiocination" as a synonym for "reasoning".

The proposal that reason gives humanity a special position in nature has been argued to be a defining characteristic of western philosophy and later western modern science, starting with classical Greece. Philosophy can be described as a way of life based upon reason, and in the other direction reason has been one of the major subjects of philosophical discussion since ancient times. Reason is often said to be reflexive, or "self-correcting", and the critique of reason has been a persistent theme in philosophy. It has been defined in different ways, at different times, by different thinkers about human nature.

For many classical philosophers, nature was understood teleologically, meaning that every type of thing had a definitive purpose which fit within a natural order that was itself understood to have aims. Perhaps starting with Pythagoras or Heraclitus, the cosmos is even said to have reason. Reason, by this account, is not just one characteristic that humans happen to have, and that influences happiness amongst other characteristics. Reason was considered of higher stature than other characteristics of human nature, such as sociability, because it is something humans share with nature itself, linking an apparently immortal part of the human mind with the divine order of the cosmos itself. Within the human mind or soul ("psyche"), reason was described by Plato as being the natural monarch which should rule over the other parts, such as spiritedness ("thumos") and the passions. Aristotle, Plato's student, defined human beings as rational animals, emphasizing reason as a characteristic of human nature. He "defined" the highest human happiness or well being ("eudaimonia") as a life which is lived consistently, excellently and completely in accordance with reason.

The conclusions to be drawn from the discussions of Aristotle and Plato on this matter are amongst the most debated in the history of philosophy. But teleological accounts such as Aristotle's were highly influential for those who attempt to explain reason in a way which is consistent with monotheism and the immortality and divinity of the human soul. For example, in the neo-platonist account of Plotinus, the cosmos has one soul, which is the seat of all reason, and the souls of all individual humans are part of this soul. Reason is for Plotinus both the provider of form to material things, and the light which brings individuals souls back into line with their source. Such neo-Platonist accounts of the rational part of the human soul were standard amongst medieval Islamic philosophers, and under this influence, mainly via Averroes, came to be debated seriously in Europe until well into the renaissance, and they remain important in Iranian philosophy.

The early modern era was marked by a number of significant changes in the understanding of reason, starting in Europe. One of the most important of these changes involved a change in the metaphysical understanding of human beings. Scientists and philosophers began to question the teleological understanding of the world. Nature was no longer assumed to be human-like, with its own aims or reason, and human nature was no longer assumed to work according to anything other than the same "laws of nature" which affect inanimate things. This new understanding eventually displaced the previous world view that derived from a spiritual understanding of the universe.

Accordingly, in the 17th century, René Descartes explicitly rejected the traditional notion of humans as "rational animals", suggesting instead that they are nothing more than "thinking things" along the lines of other "things" in nature. Any grounds of knowledge outside that understanding was, therefore, subject to doubt.

In his search for a foundation of all possible knowledge, Descartes deliberately decided to throw into doubt "all" knowledge – "except" that of the mind itself in the process of thinking:

At this time I admit nothing that is not necessarily true. I am therefore precisely nothing but a thinking thing; that is a mind, or intellect, or understanding, or reason – words of whose meanings I was previously ignorant.

This eventually became known as epistemological or "subject-centred" reason, because it is based on the "knowing subject", who perceives the rest of the world and itself as a set of objects to be studied, and successfully mastered by applying the knowledge accumulated through such study. Breaking with tradition and many thinkers after him, Descartes explicitly did not divide the incorporeal soul into parts, such as reason and intellect, describing them as one indivisible incorporeal entity.

A contemporary of Descartes, Thomas Hobbes described reason as a broader version of "addition and subtraction" which is not limited to numbers. This understanding of reason is sometimes termed "calculative" reason. Similar to Descartes, Hobbes asserted that "No discourse whatsoever, can end in absolute knowledge of fact, past, or to come" but that "sense and memory" is absolute knowledge.

In the late 17th century, through the 18th century, John Locke and David Hume developed Descartes' line of thought still further. Hume took it in an especially skeptical direction, proposing that there could be no possibility of deducing relationships of cause and effect, and therefore no knowledge is based on reasoning alone, even if it seems otherwise.

Hume famously remarked that, "We speak not strictly and philosophically when we talk of the combat of passion and of reason. Reason is, and ought only to be the slave of the passions, and can never pretend to any other office than to serve and obey them." Hume also took his definition of reason to unorthodox extremes by arguing, unlike his predecessors, that human reason is not qualitatively different from either simply conceiving individual ideas, or from judgments associating two ideas, and that "reason is nothing but a wonderful and unintelligible instinct in our souls, which carries us along a certain train of ideas, and endows them with particular qualities, according to their particular situations and relations." It followed from this that animals have reason, only much less complex than human reason.

In the 18th century, Immanuel Kant attempted to show that Hume was wrong by demonstrating that a "transcendental" self, or "I", was a necessary condition of all experience. Therefore, suggested Kant, on the basis of such a self, it is in fact possible to reason both about the conditions and limits of human knowledge. And so long as these limits are respected, reason can be the vehicle of morality, justice, aesthetics, theories of knowledge (epistemology), and understanding.

In the formulation of Kant, who wrote some of the most influential modern treatises on the subject, the great achievement of reason () is that it is able to exercise a kind of universal law-making. Kant was able therefore to reformulate the basis of moral-practical, theoretical and aesthetic reasoning, on "universal" laws.

Here practical reasoning is the self-legislating or self-governing formulation of universal norms, and theoretical reasoning the way humans posit universal laws of nature.

Under practical reason, the moral autonomy or freedom of human beings depends on their ability to behave according to laws that are given to them by the proper exercise of that reason. This contrasted with earlier forms of morality, which depended on religious understanding and interpretation, or nature for their substance.

According to Kant, in a free society each individual must be able to pursue their goals however they see fit, so long as their actions conform to principles given by reason. He formulated such a principle, called the "categorical imperative", which would justify an action only if it could be universalized:

Act only according to that maxim whereby you can, at the same time, will that it should become a universal law.

In contrast to Hume then, Kant insists that reason itself (German "Vernunft") has natural ends itself, the solution to the metaphysical problems, especially the discovery of the foundations of morality. Kant claimed that this problem could be solved with his "transcendental logic" which unlike normal logic is not just an instrument, which can be used indifferently, as it was for Aristotle, but a theoretical science in its own right and the basis of all the others.

According to Jürgen Habermas, the "substantive unity" of reason has dissolved in modern times, such that it can no longer answer the question "How should I live?" Instead, the unity of reason has to be strictly formal, or "procedural". He thus described reason as a group of three autonomous spheres (on the model of Kant's three critiques):


For Habermas, these three spheres are the domain of experts, and therefore need to be mediated with the "lifeworld" by philosophers. In drawing such a picture of reason, Habermas hoped to demonstrate that the substantive unity of reason, which in pre-modern societies had been able to answer questions about the good life, could be made up for by the unity of reason's formalizable procedures.

Hamann, Herder, Kant, Hegel, Kierkegaard, Nietzsche, Heidegger, Foucault, Rorty, and many other philosophers have contributed to a debate about what reason means, or ought to mean. Some, like Kierkegaard, Nietzsche, and Rorty, are skeptical about subject-centred, universal, or instrumental reason, and even skeptical toward reason as a whole. Others, including Hegel, believe that it has obscured the importance of intersubjectivity, or "spirit" in human life, and attempt to reconstruct a model of what reason should be.

Some thinkers, e.g. Foucault, believe there are other "forms" of reason, neglected but essential to modern life, and to our understanding of what it means to live a life according to reason.

In the last several decades, a number of proposals have been made to "re-orient" this critique of reason, or to recognize the "other voices" or "new departments" of reason:

For example, in opposition to subject-centred reason, Habermas has proposed a model of communicative reason that sees it as an essentially cooperative activity, based on the fact of linguistic intersubjectivity.

Nikolas Kompridis has proposed a widely encompassing view of reason as "that ensemble of practices that contributes to the opening and preserving of openness" in human affairs, and a focus on reason's possibilities for social change.

The philosopher Charles Taylor, influenced by the 20th century German philosopher Martin Heidegger, has proposed that reason ought to include the faculty of disclosure, which is tied to the way we make sense of things in everyday life, as a new "department" of reason.

In the essay "What is Enlightenment?", Michel Foucault proposed a concept of critique based on Kant's distinction between "private" and "public" uses of reason. This distinction, as suggested, has two dimensions:

The terms "logic" or "logical" are sometimes used as if they were identical with the term "reason" or with the concept of being "rational", or sometimes logic is seen as the most pure or the defining form of reason. For example in modern economics, rational choice is assumed to equate to logically consistent choice.

Reason and logic can however be thought of as distinct, although logic is one important aspect of reason. Author Douglas Hofstadter, in "Gödel, Escher, Bach", characterizes the distinction in this way. Logic is done inside a system while reason is done outside the system by such methods as skipping steps, working backward, drawing diagrams, looking at examples, or seeing what happens if you change the rules of the system.

Reason is a type of thought, and the word "logic" involves the attempt to describe rules or norms by which reasoning operates, so that orderly reasoning can be taught. The oldest surviving writing to explicitly consider the rules by which reason operates are the works of the Greek philosopher Aristotle, especially "Prior Analysis" and "Posterior Analysis". Although the Ancient Greeks had no separate word for logic as distinct from language and reason, Aristotle's newly coined word "syllogism" ("syllogismos") identified logic clearly for the first time as a distinct field of study. When Aristotle referred to "the logical" ("hē logikē"), he was referring more broadly to rational thought.

As pointed out by philosophers such as Hobbes, Locke and Hume, some animals are also clearly capable of a type of "associative thinking", even to the extent of associating causes and effects. A dog once kicked, can learn how to recognize the warning signs and avoid being kicked in the future, but this does not mean the dog has reason in any strict sense of the word. It also does not mean that humans acting on the basis of experience or habit are using their reason.

Human reason requires more than being able to associate two ideas, even if those two ideas might be described by a reasoning human as a cause and an effect, perceptions of smoke, for example, and memories of fire. For reason to be involved, the association of smoke and the fire would have to be thought through in a way which can be explained, for example as cause and effect. In the explanation of Locke, for example, reason requires the mental use of a third idea in order to make this comparison by use of syllogism.

More generally, reason in the strict sense requires the ability to create and manipulate a system of symbols, as well as indices and icons, according to Charles Sanders Peirce, the symbols having only a nominal, though habitual, connection to either smoke or fire. One example of such a system of artificial symbols and signs is language.

The connection of reason to symbolic thinking has been expressed in different ways by philosophers. Thomas Hobbes described the creation of "Markes, or Notes of remembrance" ("Leviathan" Ch. 4) as "speech". He used the word "speech" as an English version of the Greek word "logos" so that speech did not need to be communicated. When communicated, such speech becomes language, and the marks or notes or remembrance are called "Signes" by Hobbes. Going further back, although Aristotle is a source of the idea that only humans have reason ("logos"), he does mention that animals with imagination, for whom sense perceptions can persist, come closest to having something like reasoning and "nous", and even uses the word "logos" in one place to describe the distinctions which animals can perceive in such cases.

Reason and imagination rely on similar mental processes. Imagination is not only found in humans. Aristotle, for example, stated that "phantasia" (imagination: that which can hold images or "phantasmata") and "phronein" (a type of thinking that can judge and understand in some sense) also exist in some animals. According to him, both are related to the primary perceptive ability of animals, which gathers the perceptions of different senses and defines the order of the things that are perceived without distinguishing universals, and without deliberation or "logos". But this is not yet reason, because human imagination is different.

The recent modern writings of Terrence Deacon and Merlin Donald, writing about the origin of language, also connect reason connected to not only language, but also mimesis. More specifically they describe the ability to create language as part of an internal modeling of reality specific to humankind. Other results are consciousness, and imagination or fantasy. In contrast, modern proponents of a genetic predisposition to language itself include Noam Chomsky and Steven Pinker, to whom Donald and Deacon can be contrasted.

As reason is symbolic thinking, and peculiarly human, then this implies that humans have a special ability to maintain a clear consciousness of the distinctness of "icons" or images and the real things they represent. Starting with a modern author, Merlin Donald writes
A dog might perceive the "meaning" of a fight that was realistically play-acted by humans, but it could not reconstruct the message or distinguish the representation from its referent (a real fight). [...] Trained apes are able to make this distinction; young children make this distinction early – hence, their effortless distinction between play-acting an event and the event itself

In classical descriptions, an equivalent description of this mental faculty is "eikasia", in the philosophy of Plato. This is the ability to perceive whether a perception is an image of something else, related somehow but not the same, and therefore allows humans to perceive that a dream or memory or a reflection in a mirror is not reality as such. What Klein refers to as "dianoetic eikasia" is the "eikasia" concerned specifically with thinking and mental images, such as those mental symbols, icons, "signes", and marks discussed above as definitive of reason. Explaining reason from this direction: human thinking is special in the way that we often understand visible things as if they were themselves images of our intelligible "objects of thought" as "foundations" ("hypothēses" in Ancient Greek). This thinking ("dianoia") is "...an activity which consists in making the vast and diffuse jungle of the visible world depend on a plurality of more 'precise' "noēta"".

Both Merlin Donald and the Socratic authors such as Plato and Aristotle emphasize the importance of "mimesis", often translated as "imitation" or "representation". Donald writes
Imitation is found especially in monkeys and apes [... but ...] Mimesis is fundamentally different from imitation and mimicry in that it involves the invention of intentional representations. [...] Mimesis is not absolutely tied to external communication.

"Mimēsis" is a concept, now popular again in academic discussion, that was particularly prevalent in Plato's works, and within Aristotle, it is discussed mainly in the "Poetics". In Michael Davis's account of the theory of man in this work.
It is the distinctive feature of human action, that whenever we choose what we do, we imagine an action for ourselves as though we were inspecting it from the outside. Intentions are nothing more than imagined actions, internalizings of the external. All action is therefore imitation of action; it is poetic...

Donald like Plato (and Aristotle, especially in "On Memory and Recollection"), emphasizes the peculiarity in humans of voluntary initiation of a search through one's mental world. The ancient Greek "anamnēsis", normally translated as "recollection" was opposed to "mneme" or "memory". Memory, shared with some animals, requires a consciousness not only of what happened in the past, but also "that" something happened in the past, which is in other words a kind of "eikasia" "...but nothing except man is able to recollect." Recollection is a deliberate effort to search for and recapture something once known. Klein writes that, "To become aware of our having forgotten something means to begin recollecting." Donald calls the same thing "autocueing", which he explains as follows: "Mimetic acts are reproducible on the basis of internal, self-generated cues. This permits voluntary recall of mimetic representations, without the aid of external cues – probably the earliest form of representational "thinking"."

In a celebrated paper in modern times, the fantasy author and philologist J.R.R. Tolkien wrote in his essay "On Fairy Stories" that the terms "fantasy" and "enchantment" are connected to not only "...the satisfaction of certain primordial human desires..." but also "...the origin of language and of the mind".

Looking at logical categorizations of different types of reasoning the traditional main division made in philosophy is between deductive reasoning and inductive reasoning. Formal logic has been described as "the science of deduction". The study of inductive reasoning is generally carried out within the field known as informal logic or critical thinking.

A subdivision of Philosophy is Logic. Logic is the study of reasoning. Deduction is a form of reasoning in which a conclusion follows necessarily from the stated premises. A deduction is also the conclusion reached by a deductive reasoning process. One classic example of deductive reasoning is that found in syllogisms like the following:

The reasoning in this argument is valid, because there is no way in which the premises, 1 and 2, could be true and the conclusion, 3, be false.

Induction is a form of inference producing propositions about unobserved objects or types, either specifically or generally, based on previous observation. It is used to ascribe properties or relations to objects or types based on previous observations or experiences, or to formulate general statements or laws based on limited observations of recurring phenomenal patterns.

Inductive reasoning contrasts strongly with deductive reasoning in that, even in the best, or strongest, cases of inductive reasoning, the truth of the premises does not guarantee the truth of the conclusion. Instead, the conclusion of an inductive argument follows with some degree of probability. Relatedly, the conclusion of an inductive argument contains more information than is already contained in the premises. Thus, this method of reasoning is ampliative.

A classic example of inductive reasoning comes from the empiricist David Hume:

Analogical reasoning is a form of inductive reasoning from a particular to a particular. It is often used in case-based reasoning, especially legal reasoning. An example follows:

Analogical reasoning is a weaker form of inductive reasoning from a single example, because inductive reasoning typically uses a large number of examples to reason from the particular to the general. Analogical reasoning often leads to wrong conclusions. For example:

Abductive reasoning, or argument to the best explanation, is a form of reasoning that doesn't fit in deductive or inductive, since it starts with incomplete set of observations and proceeds with likely possible explanations so the conclusion in an abductive argument does not follow with certainty from its premises and concerns something unobserved. What distinguishes abduction from the other forms of reasoning is an attempt to favour one conclusion above others, by subjective judgement or attempting to falsify alternative explanations or by demonstrating the likelihood of the favoured conclusion, given a set of more or less disputable assumptions. For example, when a patient displays certain symptoms, there might be various possible causes, but one of these is preferred above others as being more probable.

Flawed reasoning in arguments is known as fallacious reasoning. Bad reasoning within arguments can be because it commits either a formal fallacy or an informal fallacy.

Formal fallacies occur when there is a problem with the form, or structure, of the argument. The word "formal" refers to this link to the "form" of the argument. An argument that contains a formal fallacy will always be invalid.

An informal fallacy is an error in reasoning that occurs due to a problem with the "content", rather than mere "structure", of the argument.

Philosophy is sometimes described as a life of reason, with normal human reason pursued in a more consistent and dedicated way than usual. Two categories of problem concerning reason have long been discussed by philosophers concerning reason, essentially being reasonings about reasoning itself as a human aim, or philosophizing about philosophizing. The first question is concerning whether we can be confident that reason can achieve knowledge of truth better than other ways of trying to achieve such knowledge. The other question is whether a life of reason, a life that aims to be guided by reason, can be expected to achieve a happy life more so than other ways of life (whether such a life of reason results in knowledge or not).

Since classical times a question has remained constant in philosophical debate (which is sometimes seen as a conflict between movements called Platonism and Aristotelianism) concerning the role of reason in confirming truth. People use logic, deduction, and induction, to reach conclusions they think are true. Conclusions reached in this way are considered, according to Aristotle, more certain than sense perceptions on their own. On the other hand, if such reasoned conclusions are only built originally upon a foundation of sense perceptions, then, our most logical conclusions can never be said to be certain because they are built upon the very same fallible perceptions they seek to better.

This leads to the question of what types of first principles, or starting points of reasoning, are available for someone seeking to come to true conclusions. In Greek, "first principles" are "archai", "starting points", and the faculty used to perceive them is sometimes referred to in Aristotle and Plato as "nous" which was close in meaning to "awareness" or "consciousness".

Empiricism (sometimes associated with Aristotle but more correctly associated with British philosophers such as John Locke and David Hume, as well as their ancient equivalents such as Democritus) asserts that sensory impressions are the only available starting points for reasoning and attempting to attain truth. This approach always leads to the controversial conclusion that absolute knowledge is not attainable. Idealism, (associated with Plato and his school), claims that there is a "higher" reality, from which certain people can directly arrive at truth without needing to rely only upon the senses, and that this higher reality is therefore the primary source of truth.

Philosophers such as Plato, Aristotle, Al-Farabi, Avicenna, Averroes, Maimonides, Aquinas and Hegel are sometimes said to have argued that reason must be fixed and discoverable—perhaps by dialectic, analysis, or study. In the vision of these thinkers, reason is divine or at least has divine attributes. Such an approach allowed religious philosophers such as Thomas Aquinas and Étienne Gilson to try to show that reason and revelation are compatible. According to Hegel, "...the only thought which Philosophy brings with it to the contemplation of History, is the simple conception of reason; that reason is the Sovereign of the World; that the history of the world, therefore, presents us with a rational process."

Since the 17th century rationalists, reason has often been taken to be a subjective faculty, or rather the unaided ability (pure reason) to form concepts. For Descartes, Spinoza and Leibniz, this was associated with mathematics. Kant attempted to show that pure reason could form concepts (time and space) that are the conditions of experience. Kant made his argument in opposition to Hume, who denied that reason had any role to play in experience.

After Plato and Aristotle, western literature often treated reason as being the faculty that trained the passions and appetites. Stoic philosophy by contrast considered all passions undesirable. After the critiques of reason in the early Enlightenment the appetites were rarely discussed or conflated with the passions. Some Enlightenment camps took after the Stoics to say Reason should oppose Passion rather than order it, while others like the Romantics believed that Passion displaces Reason, as in the maxim "follow your heart".

Reason has been seen as a slave, or judge, of the passions, notably in the work of David Hume, and more recently of Freud. Reasoning which claims that the object of a desire is demanded by logic alone is called "rationalization".

Rousseau first proposed, in his second "Discourse", that reason and political life is not natural and possibly harmful to mankind. He asked what really can be said about what is natural to mankind. What, other than reason and civil society, "best suits his constitution"? Rousseau saw "two principles prior to reason" in human nature. First we hold an intense interest in our own well-being. Secondly we object to the suffering or death of any sentient being, especially one like ourselves. These two passions lead us to desire more than we could achieve. We become dependent upon each other, and on relationships of authority and obedience. This effectively puts the human race into slavery. Rousseau says that he almost dares to assert that nature does not destine men to be healthy. According to Velkley, "Rousseau outlines certain programs of rational self-correction, most notably the political legislation of the "Contrat Social" and the moral education in "". All the same, Rousseau understands such corrections to be only ameliorations of an essentially unsatisfactory condition, that of socially and intellectually corrupted humanity."

This quandary presented by Rousseau led to Kant's new way of justifying reason as freedom to create good and evil. These therefore are not to be blamed on nature or God. In various ways, German Idealism after Kant, and major later figures such Nietzsche, Bergson, Husserl, Scheler, and Heidegger, remain preoccupied with problems coming from the metaphysical demands or "urges" of "reason". The influence of Rousseau and these later writers is also large upon art and politics. Many writers (such as Nikos Kazantzakis) extol passion and disparage reason. In politics modern nationalism comes from Rousseau's argument that rationalist cosmopolitanism brings man ever further from his natural state.

Another view on reason and emotion was proposed in the 1994 book titled "Descartes' Error" by Antonio Damasio. In it, Damasio presents the "Somatic Marker Hypothesis" which states that emotions guide behavior and decision-making. Damasio argues that these somatic markers (known collectively as "gut feelings") are "intuitive signals" that direct our decision making processes in a certain way that cannot be solved with rationality alone. Damasio further argues that rationality requires emotional input in order to function.

There are many religious traditions, some of which are explicitly fideist and others of which claim varying degrees of rationalism. Secular critics sometimes accuse all religious adherents of irrationality, since they claim such adherents are guilty of ignoring, suppressing, or forbidding some kinds of reasoning concerning some subjects (such as religious dogmas, moral taboos, etc.). Though the theologies and religions such as classical monotheism typically do not claim to be irrational, there is often a perceived conflict or tension between faith and tradition on the one hand, and reason on the other, as potentially competing sources of wisdom, law and truth.

Religious adherents sometimes respond by arguing that faith and reason can be reconciled, or have different non-overlapping domains, or that critics engage in a similar kind of irrationalism:
Some commentators have claimed that Western civilization can be almost defined by its serious testing of the limits of tension between "unaided" reason and faith in "revealed" truths—figuratively summarized as Athens and Jerusalem, respectively. Leo Strauss spoke of a "Greater West" that included all areas under the influence of the tension between Greek rationalism and Abrahamic revelation, including the Muslim lands. He was particularly influenced by the great Muslim philosopher Al-Farabi. To consider to what extent Eastern philosophy might have partaken of these important tensions, Strauss thought it best to consider whether dharma or tao may be equivalent to Nature (by which we mean "physis" in Greek). According to Strauss the beginning of philosophy involved the "discovery or invention of nature" and the "pre-philosophical equivalent of nature" was supplied by "such notions as 'custom' or 'ways, which appear to be "really universal in all times and places". The philosophical concept of nature or natures as a way of understanding "archai" (first principles of knowledge) brought about a peculiar tension between reasoning on the one hand, and tradition or faith on the other.

Although there is this special history of debate concerning reason and faith in the Islamic, Christian and Jewish traditions, the pursuit of reason is sometimes argued to be compatible with the other practice of other religions of a different nature, such as Hinduism, because they do not define their tenets in such an absolute way.

Aristotle famously described reason (with language) as a part of human nature, which means that it is best for humans to live "politically" meaning in communities of about the size and type of a small city state ("polis" in Greek). For example...
It is clear, then, that a human being is more of a political ["politikon" = of the "polis"] animal ["zōion"] than is any bee or than any of those animals that live in herds. For nature, as we say, makes nothing in vain, and humans are the only animals who possess reasoned speech ["logos"]. Voice, of course, serves to indicate what is painful and pleasant; that is why it is also found in other animals, because their nature has reached the point where they can perceive what is painful and pleasant and express these to each other. But speech ["logos"] serves to make plain what is advantageous and harmful and so also what is just and unjust. For it is a peculiarity of humans, in contrast to the other animals, to have perception of good and bad, just and unjust, and the like; and the community in these things makes a household or city ["polis"]. [...] By nature, then, the drive for such a community exists in everyone, but the first to set one up is responsible for things of very great goodness. For as humans are the best of all animals when perfected, so they are the worst when divorced from law and right. The reason is that injustice is most difficult to deal with when furnished with weapons, and the weapons a human being has are meant by nature to go along with prudence and virtue, but it is only too possible to turn them to contrary uses. Consequently, if a human being lacks virtue, he is the most unholy and savage thing, and when it comes to sex and food, the worst. But justice is something political [to do with the "polis"], for right is the arrangement of the political community, and right is discrimination of what is just. (Aristotle's Politics 1253a 1.2. Peter Simpson's translation, with Greek terms inserted in square brackets.)

The concept of human nature being fixed in this way, implied, in other words, that we can define what type of community is always best for people. This argument has remained a central argument in all political, ethical and moral thinking since then, and has become especially controversial since firstly Rousseau's Second Discourse, and secondly, the Theory of Evolution. Already in Aristotle there was an awareness that the "polis" had not always existed and had needed to be invented or developed by humans themselves. The household came first, and the first villages and cities were just extensions of that, with the first cities being run as if they were still families with Kings acting like fathers.
Friendship ["philia"] seems to prevail [in] man and woman according to nature ["kata phusin"]; for people are by nature ["tēi phusei"] pairing ["sunduastikon"] more than political ["politikon" = of the "polis"], in as much as the household ["oikos"] is prior ["proteron" = earlier] and more necessary than the "polis" and making children is more common ["koinoteron"] with the animals. In the other animals, community ["koinōnia"] goes no further than this, but people live together ["sumoikousin"] not only for the sake of making children, but also for the things for life; for from the start the functions ["erga"] are divided, and are different [for] man and woman. Thus they supply each other, putting their own into the common ["eis to koinon"]. It is for these [reasons] that both utility ["chrēsimon"] and pleasure ["hēdu"] seem to be found in this kind of friendship. (Nicomachean Ethics, VIII.12.1162a. Rough literal translation with Greek terms shown in square brackets.)

Rousseau in his Second Discourse finally took the shocking step of claiming that this traditional account has things in reverse: with reason, language and rationally organized communities all having developed over a long period of time merely as a result of the fact that some habits of cooperation were found to solve certain types of problems, and that once such cooperation became more important, it forced people to develop increasingly complex cooperation—often only to defend themselves from each other.

In other words, according to Rousseau, reason, language and rational community did not arise because of any conscious decision or plan by humans or gods, nor because of any pre-existing human nature. As a result, he claimed, living together in rationally organized communities like modern humans is a development with many negative aspects compared to the original state of man as an ape. If anything is specifically human in this theory, it is the flexibility and adaptability of humans. This view of the animal origins of distinctive human characteristics later received support from Charles Darwin's Theory of Evolution.

The two competing theories concerning the origins of reason are relevant to political and ethical thought because, according to the Aristotelian theory, a best way of living together exists independently of historical circumstances. According to Rousseau, we should even doubt that reason, language and politics are a good thing, as opposed to being simply the best option given the particular course of events that lead to today. Rousseau's theory, that human nature is malleable rather than fixed, is often taken to imply, for example by Karl Marx, a wider range of possible ways of living together than traditionally known.

However, while Rousseau's initial impact encouraged bloody revolutions against traditional politics, including both the French Revolution and the Russian Revolution, his own conclusions about the best forms of community seem to have been remarkably classical, in favor of city-states such as Geneva, and rural living.

Scientific research into reasoning is carried out within the fields of psychology and cognitive science. Psychologists attempt to determine whether or not people are capable of rational thought in a number of different circumstances.

Assessing how well someone engages in reasoning is the project of determining the extent to which the person is rational or acts rationally. It is a key research question in the psychology of reasoning. Rationality is often divided into its respective theoretical and practical counterparts.

Experimental cognitive psychologists carry out research on reasoning behaviour. Such research may focus, for example, on how people perform on tests of reasoning such as intelligence or IQ tests, or on how well people's reasoning matches ideals set by logic (see, for example, the Wason test). Experiments examine how people make inferences from conditionals e.g., "If A then B" and how they make inferences about alternatives, e.g., "A or else B". They test whether people can make valid deductions about spatial and temporal relations, e.g., "A is to the left of B", or "A happens after B", and about quantified assertions, e.g., "All the A are B". Experiments investigate how people make inferences about factual situations, hypothetical possibilities, probabilities, and counterfactual situations.

Developmental psychologists investigate the development of reasoning from birth to adulthood. Piaget's theory of cognitive development was the first complete theory of reasoning development. Subsequently, several alternative theories were proposed, including the neo-Piagetian theories of cognitive development.

The biological functioning of the brain is studied by neurophysiologists and neuropsychologists. Research in this area includes research into the structure and function of normally functioning brains, and of damaged or otherwise unusual brains. In addition to carrying out research into reasoning, some psychologists, for example, clinical psychologists and psychotherapists work to alter people's reasoning habits when they are unhelpful.

In artificial intelligence and computer science, scientists study and use automated reasoning for diverse applications including automated theorem proving the formal semantics of programming languages, and formal specification in software engineering.

Meta-reasoning is reasoning about reasoning. In computer science, a system performs meta-reasoning when it is reasoning about its own operation. This requires a programming language capable of reflection, the ability to observe and modify its own structure and behaviour.

A species could benefit greatly from better abilities to reason about, predict and understand the world. French social and cognitive scientists Dan Sperber and Hugo Mercier argue that there could have been other forces driving the evolution of reason. They point out that reasoning is very difficult for humans to do effectively, and that it is hard for individuals to doubt their own beliefs (confirmation bias). Reasoning is most effective when it is done as a collective – as demonstrated by the success of projects like science. They suggest that there are not just individual, but group selection pressures at play. Any group that managed to find ways of reasoning effectively would reap benefits for all its members, increasing their fitness. This could also help explain why humans, according to Sperber, are not optimized to reason effectively alone. Their argumentative theory of reasoning claims that reason may have more to do with winning arguments than with the search for the truth.




</doc>
<doc id="4102640" url="https://en.wikipedia.org/wiki?curid=4102640" title="Meaning (philosophy of language)">
Meaning (philosophy of language)

In the philosophy of language, the nature of meaning, its definition, elements, and types, was discussed by philosophers Aristotle, Augustine, and Aquinas. According to them "meaning is a relationship between two sorts of things: "signs" and the kinds of things they "mean" (intend, express or signify)". One term in the relationship of meaning necessarily causes something else to come to the mind. In other words: "a sign is defined as an entity that indicates another entity to some agent for some purpose". As Augustine states, a sign is "something that shows itself to the senses and something other than itself to the mind" ("Signum est quod se ipsum sensui et praeter se aliquid animo ostendit"; "De dial.", 1975, 86).

The types of meanings vary according to the types of the thing that is being represented. Namely:

All subsequent inquiries emphasize some particular perspectives within the general AAA framework.

The major contemporary positions of meaning come under the following partial definitions of meaning:

The evaluation of meaning according to each one of the five major substantive theories of meaning and truth is presented below. The question of what is a proper basis for deciding how words, symbols, ideas and beliefs may properly be considered to truthfully denote meaning, whether by a single person or an entire society, is dealt with by the five most prevalent substantive theories listed below. Each theory of meaning as evaluated by these respective theories of truth are each further researched by the individual scholars supporting each one of the respective theories of truth and meaning.

Both hybrid theories of meaning and alternative theories of meaning and truth have also been researched, and are subject to further assessment according to their respective and relative merits.

Correspondence theories emphasise that true beliefs and true statements of meaning correspond to the actual state of affairs and that associated meanings must be in agreement with these beliefs and statements. This type of theory stresses a relationship between thoughts or statements on one hand, and things or objects on the other. It is a traditional model tracing its origins to ancient Greek philosophers such as Socrates, Plato, and Aristotle. This class of theories holds that the truth or the falsity of a representation is determined in principle entirely by how it relates to "things", by whether it accurately describes those "things." An example of correspondence theory is the statement by the Thirteenth Century philosopher/theologian Thomas Aquinas: "Veritas est adaequatio rei et intellectus" ("Truth is the equation [or adequation] of things and intellect"), a statement which Aquinas attributed to the Ninth Century neoplatonist Isaac Israeli. Aquinas also restated the theory as: "A judgment is said to be true when it conforms to the external reality".

Correspondence theory centres heavily around the assumption that truth and meaning are a matter of accurately copying what is known as "objective reality" and then representing it in thoughts, words and other symbols. Many modern theorists have stated that this ideal cannot be achieved without analysing additional factors. For example, language plays a role in that all languages have words to represent concepts that are virtually undefined in other languages. The German word "Zeitgeist" is one such example: one who speaks or understands the language may "know" what it means, but any translation of the word apparently fails to accurately capture its full meaning (this is a problem with many abstract words, especially those derived in agglutinative languages). Thus, some words add an additional parameter to the construction of an accurate truth predicate. Among the philosophers who grappled with this problem is Alfred Tarski, whose semantic theory is summarized further below in this article.

For coherence theories in general, the assessment of meaning and truth requires a proper fit of elements within a whole system. Very often, though, coherence is taken to imply something more than simple logical consistency; often there is a demand that the propositions in a coherent system lend mutual inferential support to each other. So, for example, the completeness and comprehensiveness of the underlying set of concepts is a critical factor in judging the validity and usefulness of a coherent system. A pervasive tenet of coherence theories is the idea that truth is primarily a property of whole systems of propositions, and can be ascribed to individual propositions only according to their coherence with the whole. Among the assortment of perspectives commonly regarded as coherence theory, theorists differ on the question of whether coherence entails many possible true systems of thought or only a single absolute system.

Some variants of coherence theory are claimed to describe the essential and intrinsic properties of formal systems in logic and mathematics. However, formal reasoners are content to contemplate axiomatically independent and sometimes mutually contradictory systems side by side, for example, the various alternative geometries. On the whole, coherence theories have been rejected for lacking justification in their application to other areas of truth, especially with respect to assertions about the natural world, empirical data in general, assertions about practical matters of psychology and society, especially when used without support from the other major theories of truth.

Coherence theories distinguish the thought of rationalist philosophers, particularly of Spinoza, Leibniz, and G.W.F. Hegel, along with the British philosopher F.H. Bradley. Other alternatives may be found among several proponents of logical positivism, notably Otto Neurath and Carl Hempel.

Social constructivism holds that meaning and truth are constructed by social processes, is historically and culturally specific, and that it is in part shaped through the power struggles within a community. Constructivism views all of our knowledge as "constructed," because it does not reflect any external "transcendent" realities (as a pure correspondence theory might hold). Rather, perceptions of truth are viewed as contingent on convention, human perception, and social experience. It is believed by constructivists that representations of physical and biological reality, including race, sexuality, and gender, are socially constructed.

Giambattista Vico was among the first to claim that history and culture along with their meaning were man-made. Vico's epistemological orientation gathers the most diverse rays and unfolds in one axiom"verum ipsum factum""truth itself is constructed". Hegel and Marx were among the other early proponents of the premise that truth is, or can be, socially constructed. Marx, like many critical theorists who followed, did not reject the existence of objective truth but rather distinguished between true knowledge and knowledge that has been distorted through power or ideology. For Marx, scientific and true knowledge is "in accordance with the dialectical understanding of history" and ideological knowledge is "an epiphenomenal expression of the relation of material forces in a given economic arrangement".

Consensus theory holds that meaning and truth are whatever is agreed upon, or in some versions, might come to be agreed upon, by some specified group. Such a group might include all human beings, or a subset thereof consisting of more than one person.

Among the current advocates of consensus theory as a useful accounting of the concept of "truth" is the philosopher Jürgen Habermas. Habermas maintains that truth is what would be agreed upon in an ideal speech situation. Among the current strong critics of consensus theory is the philosopher Nicholas Rescher.

The three most influential forms of the "pragmatic theory of truth" and meaning were introduced around the turn of the 20th century by Charles Sanders Peirce, William James, and John Dewey. Although there are wide differences in viewpoint among these and other proponents of pragmatic theory, they hold in common that meaning and truth are verified and confirmed by the results of putting one's concepts into practice.

Peirce defines truth as follows: "Truth is that concordance of an abstract statement with the ideal limit towards which endless investigation would tend to bring scientific belief, which concordance the abstract statement may possess by virtue of the confession of its inaccuracy and one-sidedness, and this confession is an essential ingredient of truth." This statement stresses Peirce's view that ideas of approximation, incompleteness, and partiality, what he describes elsewhere as "fallibilism" and "reference to the future", are essential to a proper conception of meaning and truth. Although Peirce uses words like "concordance" and "correspondence" to describe one aspect of the pragmatic sign relation, he is also quite explicit in saying that definitions of truth based on mere correspondence are no more than "nominal" definitions, which he accords a lower status than "real" definitions.

William James's version of pragmatic theory, while complex, is often summarized by his statement that "the 'true' is only the expedient in our way of thinking, just as the 'right' is only the expedient in our way of behaving." By this, James meant that truth is a "quality", the value of which is confirmed by its effectiveness when applying concepts to practice (thus, "pragmatic").

John Dewey, less broadly than James but more broadly than Peirce, held that inquiry, whether scientific, technical, sociological, philosophical or cultural, is self-corrective over time "if" openly submitted for testing by a community of inquirers in order to clarify, justify, refine and/or refute proposed meanings and truths.

Though not widely known, a new variation of the pragmatic theory was defined and wielded successfully from the 20th century forward. Defined and named by William Ernest Hocking, this variation is known as "negative pragmatism". Essentially, what works may or may not be true, but what fails cannot be true because the truth and its meaning always works. James and Dewey's ideas also ascribe meaning and truth to repeated testing which is "self-corrective" over time.

Pragmatism and negative pragmatism are also closely aligned with the coherence theory of truth in that any testing should not be isolated but rather incorporate knowledge from all human endeavors and experience. The universe is a whole and integrated system, and testing should acknowledge and account for its diversity. As Feynman said, "... if it disagrees with experiment, it is wrong."

Some have asserted that meaning is nothing substantially more or less than the truth conditions they involve. For such theories, an emphasis is placed upon reference to actual things in the world to account for meaning, with the caveat that reference more or less explains the greater part (or all) of meaning itself.

The logical positivists argued that the meaning of a statement arose from how it is verified.

In his paper "Über Sinn und Bedeutung" (now usually translated as "On Sense and Reference"), Gottlob Frege argued that proper names present at least two problems in explaining meaning.


Frege can be interpreted as arguing that it was therefore a mistake to think that the meaning of a name is the thing it refers to. Instead, the meaning must be something else—the "sense" of the word. Two names for the same person, then, can have different senses (or meanings): one referent might be picked out by more than one sense. This sort of theory is called a mediated reference theory. Frege argued that, ultimately, the same bifurcation of meaning must apply to most or all linguistic categories, such as to quantificational expressions like "All boats float".

Logical analysis was further advanced by Bertrand Russell and Alfred North Whitehead in their groundbreaking "Principia Mathematica", which attempted to produce a formal language with which the truth of all mathematical statements could be demonstrated from first principles.

Russell differed from Frege greatly on many points, however. He rejected Frege's sense-reference distinction. He also disagreed that language was of fundamental significance to philosophy, and saw the project of developing formal logic as a way of eliminating all of the confusions caused by ordinary language, and hence at creating a perfectly transparent medium in which to conduct traditional philosophical argument. He hoped, ultimately, to extend the proofs of the "Principia" to all possible true statements, a scheme he called logical atomism. For a while it appeared that his pupil Wittgenstein had succeeded in this plan with his "Tractatus Logico-Philosophicus".

Russell's work, and that of his colleague G. E. Moore, developed in response to what they perceived as the nonsense dominating British philosophy departments at the turn of the 20th century, which was a kind of British Idealism most of which was derived (albeit very distantly) from the work of Hegel. In response Moore developed an approach ("Common Sense Philosophy") which sought to examine philosophical difficulties by a close analysis of the language used in order to determine its meaning. In this way Moore sought to expunge philosophical absurdities such as "time is unreal". Moore's work would have significant, if oblique, influence (largely mediated by Wittgenstein) on Ordinary language philosophy.

The Vienna Circle, a famous group of logical positivists from the early 20th century (closely allied with Russell and Frege), adopted the verificationist theory of meaning. The verificationist theory of meaning (in at least one of its forms) states that to say that an expression is meaningful is to say that there are some conditions of experience that could exist to show that the expression is true. As noted, Frege and Russell were two proponents of this way of thinking.

A semantic theory of truth was produced by Alfred Tarski for formal semantics. According to Tarski's account, meaning consists of a recursive set of rules that end up yielding an infinite set of sentences, "'p' is true if and only if p", covering the whole language. His innovation produced the notion of propositional functions discussed on the section on universals (which he called "sentential functions"), and a model-theoretic approach to semantics (as opposed to a proof-theoretic one). Finally, some links were forged to the correspondence theory of truth (Tarski, 1944).

Perhaps the most influential current approach in the contemporary theory of meaning is that sketched by Donald Davidson in his introduction to the collection of essays "Truth and Meaning" in 1967. There he argued for the following two theses:

The result is a theory of meaning that rather resembles, by no accident, Tarski's account.

Davidson's account, though brief, constitutes the first systematic presentation of truth-conditional semantics. He proposed simply translating natural languages into first-order predicate calculus in order to reduce meaning to a function of truth.

Saul Kripke examined the relation between sense and reference in dealing with possible and actual situations. He showed that one consequence of his interpretation of certain systems of modal logic was that the reference of a proper name is "necessarily" linked to its referent, but that the sense is not. So for instance "Hesperus" necessarily refers to Hesperus, even in those imaginary cases and worlds in which perhaps Hesperus is not the evening star. That is, Hesperus is necessarily Hesperus, but only contingently the morning star.

This results in the curious situation that part of the meaning of a name — that it refers to some particular thing — is a necessary fact about that name, but another part — that it is used in some particular way or situation — is not.

Kripke also drew the distinction between speaker's meaning and semantic meaning, elaborating on the work of ordinary language philosophers Paul Grice and Keith Donnellan. The speaker's meaning is what the speaker intends to refer to by saying something; the semantic meaning is what the words uttered by the speaker mean according to the language.

In some cases, people do not say what they mean; in other cases, they say something that is in error. In both these cases, the speaker's meaning and the semantic meaning seem to be different. Sometimes words do not actually express what the speaker wants them to express; so words will mean one thing, and what people intend to convey by them might mean another. The meaning of the expression, in such cases, is ambiguous.

W.V. Quine attacked both verificationism and the very notion of meaning in his famous essay, "Two Dogmas of Empiricism". In it, he suggested that meaning was nothing more than a vague and dispensable notion. Instead, he asserted, what was more interesting to study was the synonymy between signs. He also pointed out that verificationism was tied to the distinction between analytic and synthetic statements, and asserted that such a divide was defended ambiguously. He also suggested that the unit of analysis for any potential investigation into the world (and, perhaps, meaning) would be the entire body of statements taken as a collective, not just individual statements on their own.

Other criticisms can be raised on the basis of the limitations that truth-conditional theorists themselves admit to. Tarski, for instance, recognized that truth-conditional theories of meaning only make sense of statements, but fail to explain the meanings of the lexical parts that make up statements. Rather, the meaning of the parts of statements is presupposed by an understanding of the truth-conditions of a whole statement, and explained in terms of what he called "satisfaction conditions".

Still another objection (noted by Frege and others) was that some kinds of statements don't seem to have any truth-conditions at all. For instance, "Hello!" has no truth-conditions, because it doesn't even attempt to tell the listener anything about the state of affairs in the world. In other words, different propositions have different grammatical moods.

Deflationist accounts of truth, sometimes called 'irrealist' accounts, are the staunchest source of criticism of truth-conditional theories of meaning. According to them, "truth" is a word with no serious meaning or function in discourse. For instance, for the deflationist, the sentences "It's true that Tiny Tim is trouble" and "Tiny Tim is trouble" are equivalent. In consequence, for the deflationist, any appeal to truth as an account of meaning has little explanatory power.

The sort of truth-theories presented here can also be attacked for their formalism both in practice and principle. The principle of formalism is challenged by the informalists, who suggest that language is largely a construction of the speaker, and so, not compatible with formalization. The practice of formalism is challenged by those who observe that formal languages (such as present-day quantificational logic) fail to capture the expressive power of natural languages (as is arguably demonstrated in the awkward character of the quantificational explanation of definite description statements, as laid out by Bertrand Russell).

Finally, over the past century, forms of logic have been developed that are not dependent exclusively on the notions of truth and falsity. Some of these types of logic have been called modal logics. They explain how certain logical connectives such as "if-then" work in terms of necessity and possibility. Indeed, modal logic was the basis of one of the most popular and rigorous formulations in modern semantics called the Montague grammar. The successes of such systems naturally give rise to the argument that these systems have captured the natural meaning of connectives like if-then far better than an ordinary, truth-functional logic ever could.

Throughout the 20th century, English philosophy focused closely on analysis of language. This style of analytic philosophy became very influential and led to the development of a wide range of philosophical tools.

The philosopher Ludwig Wittgenstein was originally an artificial language philosopher, following the influence of Russell and Frege. In his "Tractatus Logico-Philosophicus" he had supported the idea of an ideal language built up from atomic statements using logical connectives (see picture theory of meaning and logical atomism). However, as he matured, he came to appreciate more and more the phenomenon of natural language. "Philosophical Investigations", published after his death, signalled a sharp departure from his earlier work with its focus upon ordinary language use (see use theory of meaning and ordinary language philosophy). His approach is often summarised by the aphorism "the meaning of a word is its use in a language". However, following in Frege's footsteps, in the "Tractatus", Wittgenstein declares: "... Only in the context of a proposition has a name meaning."

His work would come to inspire future generations and spur forward a whole new discipline, which explained meaning in a new way. Meaning in a natural language was seen as primarily a question of how the speaker uses words within the language to express intention.

This close examination of natural language proved to be a powerful philosophical technique. Practitioners who were influenced by Wittgenstein's approach have included an entire tradition of thinkers, featuring P. F. Strawson, Paul Grice, R. M. Hare, R. S. Peters, and Jürgen Habermas.

At around the same time Ludwig Wittgenstein was re-thinking his approach to language, reflections on the complexity of language led to a more expansive approach to meaning. Following the lead of George Edward Moore, J. L. Austin examined the use of words in great detail. He argued against fixating on the meaning of words. He showed that dictionary definitions are of limited philosophical use, since there is no simple "appendage" to a word that can be called its meaning. Instead, he showed how to focus on the way in which words are used in order to do things. He analysed the structure of utterances into three distinct parts: locutions, illocutions and perlocutions. His pupil John Searle developed the idea under the label "speech acts". Their work greatly influenced pragmatics.

Past philosophers had understood reference to be tied to words themselves. However, Sir Peter Strawson disagreed in his seminal essay, "On Referring", where he argued that there is nothing true about statements on their own; rather, only the uses of statements could be considered to be true or false.

Indeed, one of the hallmarks of the ordinary use perspective is its insistence upon the distinctions between meaning and use. "Meanings", for ordinary language philosophers, are the "instructions" for usage of words — the common and conventional definitions of words. "Usage", on the other hand, is the actual meanings that individual speakers have — the things that an individual speaker in a particular context wants to refer to. The word "dog" is an example of a meaning, but pointing at a nearby dog and shouting "This dog smells foul!" is an example of usage. From this distinction between usage and meaning arose the divide between the fields of Pragmatics and Semantics.

Yet another distinction is of some utility in discussing language: "mentioning". "Mention" is when an expression refers to itself as a linguistic item, usually surrounded by quotation marks. For instance, in the expression "'Opopanax' is hard to spell", what is referred to is the word itself ("opopanax") and not what it means (an obscure gum resin). Frege had referred to instances of mentioning as "opaque contexts".

In his essay, "Reference and Definite Descriptions", Keith Donnellan sought to improve upon Strawson's distinction. He pointed out that there are two uses of definite descriptions: "attributive" and "referential". Attributive uses provide a description of whoever is being referred to, while referential uses point out the actual referent. Attributive uses are like mediated references, while referential uses are more directly referential.

The philosopher Paul Grice, working within the ordinary language tradition, understood "meaning" — in his 1957 article — to have two kinds: "natural" and "non-natural". "Natural meaning" had to do with cause and effect, for example with the expression "these spots mean measles". "Non-natural" meaning, on the other hand, had to do with the intentions of the speaker in communicating something to the listener.

In his essay, "Logic and Conversation", Grice went on to explain and defend an explanation of how conversations work. His guiding maxim was called the "cooperative principle", which claimed that the speaker and the listener will have mutual expectations of the kind of information that will be shared. The principle is broken down into four maxims: "Quality" (which demands truthfulness and honesty), "Quantity" (demand for just enough information as is required), "Relation" (relevance of things brought up), and "Manner" (lucidity). This principle, if and when followed, lets the speaker and listener figure out the meaning of certain implications by way of inference.

The works of Grice led to an avalanche of research and interest in the field, both supportive and critical. One spinoff was called Relevance theory, developed by Dan Sperber and Deirdre Wilson during the mid-1980s, whose goal was to make the notion of "relevance" more clear. Similarly, in his work, "Universal pragmatics", Jürgen Habermas began a program that sought to improve upon the work of the ordinary language tradition. In it, he laid out the goal of a valid conversation as a pursuit of mutual understanding.

Although he has focused on the structure and functioning of human syntax, in many works Noam Chomsky has discussed many philosophical problems too, including the problem of meaning and reference in human language. Chomsky has formulated a strong criticism against both the externalist notion of reference (reference consists in a direct or causal relation among words and objects) and the internalist one (reference is a mind-mediated relation holding among words and reality). According to Chomsky, both these notions (and many others widely used in philosophy, such as that of truth) are basically inadequate for the naturalistic (= scientific) inquiry on human mind: they are common sense notions, not scientific notions, which cannot, as such, enter in the scientific discussion. Chomsky argues that the notion of reference can be used only when we deal with scientific (i.e. artificial) languages, whose symbols refers to specific things or entities; but when we consider human language expressions, we immediately understand that their reference is vague, in the sense that they can be used to denote many things. For example, the word “book” can be used to denote an abstract object (e.g., “he is reading the book”) or a concrete one (e.g., “the book is on the chair”); the name “London” can denote at the same time a set of buildings, the air of a place and the character of a population (think to the sentence “London is so gray, polluted and sad”). These and other cases induce Chomsky to argue that the only plausible (although not scientific) notion of reference is that of act of reference, a complex phenomenon of language use (performance) which includes many factors (linguistic and not: i.e. beliefs, desires, assumptions about the world, premises, etc.). As Chomsky himself has pointed out 
, this conception of meaning is very close to that adopted by John Austin, Peter Strawson and the late Wittgenstein.

Michael Dummett argued against the kind of truth-conditional semantics presented by Davidson. Instead, he argued that basing semantics on "assertion conditions" avoids a number of difficulties with truth-conditional semantics, such as the transcendental nature of certain kinds of truth condition. He leverages work done in proof-theoretic semantics to provide a kind of inferential role semantics, where:
A semantics based upon assertion conditions is called a verificationist semantics: cf. the verificationism of the Vienna Circle.

This work is closely related, though not identical, to one-factor theories of conceptual role semantics.

Sometimes between the 1950-1990s, cognitive scientist Jerry Fodor said that use theories (of the Wittgensteinian kind) seem to assume that language is solely a public phenomenon, that there is no such thing as a "private language". Fodor thinks it is necessary to create or describe the "language of thought", which would seemingly require the existence of a "private language".

In the 1960s, David Kellogg Lewis described meaning as use, a feature of a social convention and conventions as regularities of a specific sort. Lewis' work was an application of game theory in philosophical topics. Conventions, he argued, are a species of coordination equilibria.

The idea theory of meaning (also ideational theory of meaning), most commonly associated with the British empiricist John Locke, claims that meanings are mental representations provoked by signs. 

The term "ideas" is used to refer to either mental representations, or to mental activity in general. Those who seek an explanation for meaning in the former sort of account endorse a stronger sort of idea theory of mind than the latter. Those who seek an explanation for meaning in the former sort of account endorse a stronger sort of idea theory of meaning than the latter.

Each idea is understood to be necessarily "about" something external and/or internal, real or imaginary. For example, in contrast to the abstract meaning of the universal "dog", the referent "this dog" may mean a particular real life chihuahua. In both cases, the word is about something, but in the former it is about the class of dogs as generally understood, while in the latter it is about a very real and particular dog in the real world.

John Locke, considered all ideas to be both imaginable objects of sensation and the very "un"imaginable objects of reflection. Locke said in his "Essay Concerning Human Understanding", that words are used both as signs for ideas—but also to signify the lack of certain ideas. David Hume held that thoughts were kinds of imaginable entities. (See Hume's "Enquiry Concerning Human Understanding", section 2). Hume argued that any words that could not call upon any past experience were without meaning.

Nonetheless, George Berkeley and Ludwig Wittgenstein held, in contrast to Locke and Hume, that ideas alone are unable to account for the different variations within a general meaning. For example, any hypothetical image of the meaning of "dog" has to include such varied images as a chihuahua, a pug, and a Black Lab; and this seems impossible to imagine, all of those particular breeds looking very different from one another. Another way to see this point is to question why it is that, if we have an image of a specific type of dog (say of a chihuahua), it should be entitled to represent the entire concept.

Another criticism is that some meaningful words, known as non-lexical items, don't have any meaningfully associated image. For example, the word "the" has a meaning, but one would be hard-pressed to find a mental representation that fits it. Still another objection lies in the observation that certain linguistic items name something in the real world, and are meaningful, yet which we have no mental representations to deal with. For instance, it is not known what Newton's father looked like, yet the phrase "Newton's father" still has meaning.

Another problem is that of composition — that it is difficult to explain how words and phrases combine into sentences if only ideas were involved in meaning.

Eleanor Rosch and George Lakoff advanced the theory of prototypes, which suggests that many lexical categories, at least on the face of things, have "radial structures". That is to say, there are some ideal member(s) in the category that seem to represent the category better than other members. For example, the category of "birds" may feature the "robin" as the prototype, or the ideal kind of bird. With experience, subjects might come to evaluate membership in the category of "bird" by comparing candidate members to the prototype and evaluating for similarities. So, for example, a penguin or an ostrich would sit at the fringe of the meaning of "bird", because a penguin is unlike a robin.

Intimately related to these researches is the notion of a "psychologically basic level", which is both the first level named and understood by children, and "the highest level at which a single mental image can reflect the entire category". (Lakoff 1987:46) The "basic level" of cognition is understood by Lakoff as crucially drawing upon "image-schemas" along with various other cognitive processes.

The philosophers (Ned Block, Gilbert Harman, H. Field) and the cognitive scientists (G. Miller and P. Johnson-Laird) say that the meaning of a term can be found by investigating its role in relation to other concepts and mental states. They endorse a view called "conceptual role semantics". Those proponents of this view who understand meanings to be exhausted by the content of mental states can be said to endorse "one-factor" accounts of conceptual role semantics. and thus fit within the tradition of idea theories.




</doc>
<doc id="30758" url="https://en.wikipedia.org/wiki?curid=30758" title="Age of Enlightenment">
Age of Enlightenment

The Age of Enlightenment (also known as the Age of Reason or simply the Enlightenment) was an intellectual and philosophical movement that dominated the world of ideas in Europe during the 17th to 19th centuries.

The Enlightenment emerged out of a European intellectual and scholarly movement known as Renaissance humanism. Some consider the publication of Isaac Newton's "Principia Mathematica" in 1687 as the first major enlightenment work. French historians traditionally date the Enlightenment from 1715 to 1789, from the death of Louis XIV of France until the outbreak of the French Revolution that ended the Ancien Regime. Most end it with the beginning of the 19th century. Philosophers and scientists of the period widely circulated their ideas through meetings at scientific academies, Masonic lodges, literary salons, coffeehouses and in printed books, journals, and pamphlets. The ideas of the Enlightenment undermined the authority of the monarchy and the Church and paved the way for the political revolutions of the 18th and 19th centuries. A variety of 19th-century movements, including liberalism and neoclassicism, trace their intellectual heritage to the Enlightenment.

The Enlightenment included a range of ideas centered on the sovereignty of reason and the evidence of the senses as the primary sources of knowledge and advanced ideals such as liberty, progress, toleration, fraternity, constitutional government and separation of church and state. In France, the central doctrines of the Enlightenment philosophers were individual liberty and religious tolerance, in opposition to an absolute monarchy and the fixed dogmas of the Catholic Church. The Enlightenment was marked by an emphasis on the scientific method and reductionism, along with increased questioning of religious orthodoxy—an attitude captured by Immanuel Kant's essay "," where the phrase "Sapere aude" (Dare to know) can be found.

The Age of Enlightenment was preceded by and closely associated with the scientific revolution. Earlier philosophers whose work influenced the Enlightenment included Bacon and Descartes. The major figures of the Enlightenment included Beccaria, Baruch Spinoza, Diderot, Kant, Hume, Rousseau and Adam Smith. Some European rulers, including Catherine II of Russia, Joseph II of Austria and Frederick II of Prussia, tried to apply Enlightenment thought on religious and political tolerance, which became known as enlightened absolutism.

Many of the leading political and intellectual figures behind the American Revolution associated themselves closely with the Enlightenment: Benjamin Franklin visited Europe repeatedly and contributed actively to the scientific and political debates there and brought the newest ideas back to Philadelphia; Thomas Jefferson closely followed European ideas and later incorporated some of the ideals of the Enlightenment into the Declaration of Independence; and James Madison incorporated these ideals into the United States Constitution during its framing in 1787.

The most influential publication of the Enlightenment was the "" ("Encyclopaedia"). Published between 1751 and 1772 in thirty-five volumes, it was compiled by Diderot, d'Alembert (until 1759) and a team of 150 scientists and philosophers. It helped spread the ideas of the Enlightenment across Europe and beyond. Other landmark publications were Voltaire's "Dictionnaire philosophique" ("Philosophical Dictionary"; 1764) and "Letters on the English" (1733); Rousseau's "Discourse on Inequality" (1754) and "The Social Contract" (1762); Adam Smith's "The Theory of Moral Sentiments" (1759) and "The Wealth of Nations" (1776); and Montesquieu's "The Spirit of the Laws" (1748). The ideas of the Enlightenment played a major role in inspiring the French Revolution, which began in 1789. After the Revolution, the Enlightenment was followed by the intellectual movement known as Romanticism.

René Descartes' rationalist philosophy laid the foundation for enlightenment thinking. His attempt to construct the sciences on a secure metaphysical foundation was not as successful as his method of doubt applied in philosophic areas leading to a dualistic doctrine of mind and matter. His skepticism was refined by John Locke's "Essay Concerning Human Understanding" (1690) and David Hume's writings in the 1740s. His dualism was challenged by Spinoza's uncompromising assertion of the unity of matter in his "Tractatus" (1670) and "Ethics" (1677).

According to Jonathan Israel, these laid down two distinct lines of Enlightenment thought: first, the moderate variety, following Descartes, Locke and Christian Wolff, which sought accommodation between reform and the traditional systems of power and faith, and second, the radical enlightenment, inspired by the philosophy of Spinoza, advocating democracy, individual liberty, freedom of expression and eradication of religious authority. The moderate variety tended to be deistic, whereas the radical tendency separated the basis of morality from theology. Both lines of thought were eventually opposed by a conservative Counter-Enlightenment, which sought a return to faith.

In the mid-18th century, Paris became the center of an explosion of philosophic and scientific activity challenging traditional doctrines and dogmas. The philosophical movement was led by Voltaire and Jean-Jacques Rousseau, who argued for a society based upon reason as in ancient Greece rather than faith and Catholic doctrine, for a new civil order based on natural law, and for science-based on experiments and observation. The political philosopher Montesquieu introduced the idea of a separation of powers in a government, a concept which was enthusiastically adopted by the authors of the United States Constitution. While the "Philosophes" of the French Enlightenment were not revolutionaries and many were members of the nobility, their ideas played an important part in undermining the legitimacy of the Old Regime and shaping the French Revolution.

Francis Hutcheson, a moral philosopher, described the utilitarian and consequentialist principle that virtue is that which provides, in his words, "the greatest happiness for the greatest numbers." Much of what is incorporated in the scientific method (the nature of knowledge, evidence, experience, and causation) and some modern attitudes towards the relationship between science and religion were developed by his protégés David Hume and Adam Smith. Hume became a major figure in the skeptical philosophical and empiricist traditions of philosophy.

Immanuel Kant (1724–1804) tried to reconcile rationalism and religious belief, individual freedom and political authority, as well as map out a view of the public sphere through private and public reason. Kant's work continued to shape German thought and indeed all of European philosophy, well into the 20th century.

Mary Wollstonecraft was one of England's earliest feminist philosophers. She argued for a society based on reason and that women as well as men should be treated as rational beings. She is best known for her work "A Vindication of the Rights of Woman" (1791).

Science played an important role in Enlightenment discourse and thought. Many Enlightenment writers and thinkers had backgrounds in the sciences and associated scientific advancement with the overthrow of religion and traditional authority in favor of the development of free speech and thought. Scientific progress during the Enlightenment included the discovery of carbon dioxide (fixed air) by the chemist Joseph Black, the argument for deep time by the geologist James Hutton and the invention of the condensing steam engine by James Watt. The experiments of Lavoisier were used to create the first modern chemical plants in Paris and the experiments of the Montgolfier Brothers enabled them to launch the first human-crewed flight in a hot-air balloon on 21 November 1783 from the Château de la Muette, near the Bois de Boulogne.

Enlightenment science greatly valued empiricism and rational thought and was embedded with the Enlightenment ideal of advancement and progress. The study of science, under the heading of natural philosophy, was divided into physics and a conglomerate grouping of chemistry and natural history, which included anatomy, biology, geology, mineralogy and zoology. As with most Enlightenment views, the benefits of science were not seen universally: Rousseau criticized the sciences for distancing man from nature and not operating to make people happier. Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centers of scientific research and development. Societies and academies were also the backbones of the maturation of the scientific profession. Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the "Encyclopédie" and the popularization of Newtonianism by Voltaire and Émilie du Châtelet. Some historians have marked the 18th century as a drab period in the history of science. However, the century saw significant advancements in the practice of medicine, mathematics and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry.

Scientific academies and societies grew out of the Scientific Revolution as the creators of scientific knowledge in contrast to the scholasticism of the university. During the Enlightenment, some societies created or retained links to universities, but contemporary sources distinguished universities from scientific societies by claiming that the university's utility was in the transmission of knowledge while societies functioned to create knowledge. As the role of universities in institutionalized science began to diminish, learned societies became the cornerstone of organized science. The state chartered official scientific societies to provide technical expertise. Most societies were granted permission to oversee their publications, control the election of new members and the administration of the society. After 1700, a tremendous number of official academies and societies were founded in Europe, and by 1789 there were over seventy official scientific societies. About this growth, Bernard de Fontenelle coined the term "the Age of Academies" to describe the 18th century.

The influence of science also began appearing more commonly in poetry and literature during the Enlightenment. Some poetry became infused with scientific metaphor and imagery, while other poems were written directly about scientific topics. Sir Richard Blackmore committed the Newtonian system to verse in "Creation, a Philosophical Poem in Seven Books" (1712). After Newton died in 1727, poems were composed in his honor for decades. James Thomson (1700–1748) penned his "Poem to the Memory of Newton", which mourned the loss of Newton, but also praised his science and legacy.

Hume and other Scottish Enlightenment thinkers developed a "science of man", which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement and Hume's philosophical concepts that directly influenced James Madison (and thus the U.S. Constitution) and as popularised by Dugald Stewart, would be the basis of classical liberalism.

In 1776, Adam Smith published "The Wealth of Nations", often considered the first work on modern economics as it had an immediate impact on British economic policy that continues into the 21st century. It was immediately preceded and influenced by Anne-Robert-Jacques Turgot, Baron de Laune drafts of "Reflections on the Formation and Distribution of Wealth" (Paris, 1766). Smith acknowledged indebtedness and possibly was the original English translator.

Cesare Beccaria, a jurist, criminologist, philosopher, and politician and one of the great Enlightenment writers, became famous for his masterpiece "Of Crimes and Punishments" (1764), later translated into 22 languages, which condemned torture and the death penalty and was a founding work in the field of penology and the Classical School of criminology by promoting criminal justice. Another prominent intellectual was Francesco Mario Pagano, who wrote relevant studies such as "Saggi Politici" (Political Essays, 1783), one of the significant works of the Enlightenment in Naples; and "Considerazioni sul processo criminale" (Considerations on the criminal trial, 1787), which established him as an international authority on criminal law.

The Enlightenment has long been hailed as the foundation of modern Western political and intellectual culture. The Enlightenment brought political modernization to the West, in terms of introducing democratic values and institutions and the creation of modern, liberal democracies. This thesis has been widely accepted by Anglophone scholars and has been reinforced by the large-scale studies by Robert Darnton, Roy Porter and most recently by Jonathan Israel.

John Locke, one of the most influential Enlightenment thinkers, based his governance philosophy in social contract theory, a subject that permeated Enlightenment political thought. The English philosopher Thomas Hobbes ushered in this new debate with his work "Leviathan" in 1651. Hobbes also developed some of the fundamentals of European liberal thought: the right of the individual; the natural equality of all men; the artificial character of the political order (which led to the later distinction between civil society and the state); the view that all legitimate political power must be "representative" and based on the consent of the people; and a liberal interpretation of the law which leaves people free to do whatever the law does not explicitly forbid.

Both Locke and Rousseau developed social contract theories in "Two Treatises of Government" and "Discourse on Inequality", respectively. While quite different works, Locke, Hobbes, and Rousseau agreed that a social contract, in which the government's authority lies in the consent of the governed, is necessary for man to live in a civil society. Locke defines the state of nature as a condition in which humans are rational and follow natural law, in which all men are born equal and with the right to life, liberty, and property. However, when one citizen breaks the Law of Nature, both the transgressor and the victim enter into a state of war, from which it is virtually impossible to break free. Therefore, Locke said that individuals enter into civil society to protect their natural rights via an "unbiased judge" or provincial authority, such as courts, to appeal to. Contrastingly, Rousseau's conception relies on the supposition that "civil man" is corrupted, while "natural man" has no want he cannot fulfill himself. The natural man is only taken out of the state of nature when the inequality associated with private property is established. Rousseau said that people join into civil society via the social contract to achieve unity while preserving individual freedom. This is embodied in the sovereignty of the general will, the moral and collective legislative body constituted by citizens.

Locke is known for his statement that individuals have a right to "Life, Liberty, and Property" and his belief that the natural right to property is derived from labor. Tutored by Locke, Anthony Ashley-Cooper, 3rd Earl of Shaftesbury wrote in 1706: "There is a mighty Light which spreads its self over the world especially in those two free Nations of England and Holland; on whom the Affairs of Europe now turn." Locke's theory of natural rights has influenced many political documents, including the United States Declaration of Independence and the French National Constituent Assembly's Declaration of the Rights of Man and of the Citizen.

The "philosophes" argued that the establishment of a contractual basis of rights would lead to the market mechanism and capitalism, the scientific method, religious tolerance and the organization of states into self-governing republics through democratic means. In this view, the tendency of the "philosophes" in particular to apply rationality to every problem is considered the essential change.

Although social contract theorists dominated much of Enlightenment political thought, both David Hume and Adam Ferguson criticized this camp. Hume's essay "Of the Original Contract" argues that governments derived from consent are rarely seen, and civil government is grounded in a ruler's habitual authority and force. It is precisely because of the ruler's power over-and-against the subject that the subject tacitly consents. Hume says that the subjects would "never imagine that their consent made him sovereign," instead, the authority did so. Similarly, Ferguson did not believe citizens built the state; instead polities grew out of social development. In his 1767 "An Essay on the History of Civil Society", Ferguson uses the four stages of progress, a theory that was very popular in Scotland at the time, to explain how humans advance from a hunting and gathering society to a commercial and civil society without "signing" a social contract.

Both Rousseau's and Locke's social contract theories rest on the presupposition of natural rights, which are not a result of law or custom but are things that all men have in pre-political societies and are therefore universal and inalienable. The most famous natural right formulation comes from John Locke in his "Second Treatise", when he introduces the state of nature. For Locke, the law of nature is grounded on mutual security or the idea that one cannot infringe on another's natural rights, as every man is equal and has the same inalienable rights. These natural rights include perfect equality and freedom, as well as the right to preserve life and property. Locke also argued against slavery on the basis that enslaving oneself goes against the law of nature because one cannot surrender one's rights. In essence, one's freedom is absolute, and no-one can take it away. Additionally, Locke argues that one person cannot enslave another because it is morally reprehensible. However, he introduces a caveat by saying that enslavement of a lawful captive in time of war would not go against one's natural rights.

As a spill-over of the Enlightenment, nonsecular beliefs expressed first by Quakers and then by Protestant evangelicals in Britain and the United States emerged. To these groups, slavery became "repugnant to our religion" and a "crime in the sight of God." These ideas added to those expressed by Enlightenment thinkers, leading many in Britain to believe that slavery was "not only morally wrong and economically inefficient, but also politically unwise." As these notions gained more adherents, Britain was forced to end its participation in the slave trade.

The leaders of the Enlightenment were not especially democratic, as they more often look to absolute monarchs as the key to imposing reforms designed by the intellectuals. Voltaire despised democracy and said the absolute monarch must be enlightened and must act as dictated by reason and justice – in other words, be a "philosopher-king".

In several nations, rulers welcomed leaders of the Enlightenment at court and asked them to help design laws and programs to reform the system, typically to build more influential states. These rulers are called "enlightened despots" by historians. They included Frederick the Great of Prussia, Catherine the Great of Russia, Leopold II of Tuscany and Joseph II of Austria. Joseph was over-enthusiastic, announcing many reforms that had little support so that revolts broke out and his regime became a comedy of errors and nearly all his programs were reversed. Senior ministers Pombal in Portugal and Johann Friedrich Struensee in Denmark also governed according to Enlightenment ideals. In Poland, the model constitution of 1791 expressed Enlightenment ideals but was in effect for only one year before the nation was partitioned among its neighbors. More enduring were the cultural achievements, which created a nationalist spirit in Poland.

Frederick the Great, the king of Prussia from 1740 to 1786, saw himself as a leader of the Enlightenment and patronized philosophers and scientists at his court in Berlin. Voltaire, who had been imprisoned and maltreated by the French government, was eager to accept Frederick's invitation to live at his palace. Frederick explained: "My principal occupation is to combat ignorance and prejudice ... to enlighten minds, cultivate morality, and to make people as happy as it suits human nature, and as the means at my disposal permit".

The Enlightenment has been frequently linked to the French Revolution of 1789. One view of the political changes that occurred during the Enlightenment is that the "consent of the governed" philosophy as delineated by Locke in "Two Treatises of Government" (1689) represented a paradigm shift from the old governance paradigm under feudalism known as the "divine right of kings." In this view, the revolutions of the late 1700s and early 1800s were caused by the fact that this governance paradigm shift often could not be resolved peacefully, and therefore violent revolution was the result. Clearly, a governance philosophy where the king was never wrong was in direct conflict with one whereby citizens by natural law had to consent to the acts and rulings of their government.

Alexis de Tocqueville proposed the French Revolution as the inevitable result of the radical opposition created in the 18th century between the monarchy and the men of letters of the Enlightenment. These men of letters constituted a sort of "substitute aristocracy that was both all-powerful and without real power." This illusory power came from the rise of "public opinion," born when absolutist centralization removed the nobility and the bourgeoisie from the political sphere. The "literary politics" that resulted promoted a discourse of equality and was hence in fundamental opposition to the monarchical regime. De Tocqueville "clearly designates  ... the cultural effects of transformation in the forms of the exercise of power".

Enlightenment-era religious commentary was a response to the preceding century of religious conflict in Europe, especially the Thirty Years' War. Theologians of the Enlightenment wanted to reform their faith to its generally non-confrontational roots and to limit the capacity for religious controversy to spill over into politics and warfare while still maintaining a true faith in God. For moderate Christians, this meant a return to simple Scripture. John Locke abandoned the corpus of theological commentary in favor of an "unprejudiced examination" of the Word of God alone. He determined the essence of Christianity to be a belief in Christ the redeemer and recommended avoiding more detailed debate. In the "Jefferson Bible", Thomas Jefferson went further and dropped any passages dealing with miracles, visitations of angels and the resurrection of Jesus after his death, as he tried to extract the practical Christian moral code of the New Testament.

Enlightenment scholars sought to curtail the political power of organized religion and thereby prevent another age of intolerant religious war. Spinoza determined to remove politics from contemporary and historical theology (e.g., disregarding Judaic law). Moses Mendelssohn advised affording no political weight to any organized religion, but instead recommended that each person follow what they found most convincing. They believed a good religion based in instinctive morals and a belief in God should not theoretically need force to maintain order in its believers, and both Mendelssohn and Spinoza judged religion on its moral fruits, not the logic of its theology.

Several novel ideas about religion developed with the Enlightenment, including deism and talk of atheism. According to Thomas Paine, deism is the simple belief in God the Creator, with no reference to the Bible or any other miraculous source. Instead, the deist relies solely on personal reason to guide his creed, which was eminently agreeable to many thinkers of the time. Atheism was much discussed, but there were few proponents. Wilson and Reill note: "In fact, very few enlightened intellectuals, even when they were vocal critics of Christianity, were true atheists. Rather, they were critics of orthodox belief, wedded rather to skepticism, deism, vitalism, or perhaps pantheism". Some followed Pierre Bayle and argued that atheists could indeed be moral men. Many others like Voltaire held that without belief in a God who punishes evil, the moral order of society was undermined. That is, since atheists gave themselves to no Supreme Authority and no law and had no fear of eternal consequences, they were far more likely to disrupt society. Bayle (1647–1706) observed that, in his day, "prudent persons will always maintain an appearance of [religion]," and he believed that even atheists could hold concepts of honor and go beyond their own self-interest to create and interact in society. Locke said that if there were no God and no divine law, the result would be moral anarchy: every individual "could have no law but his own will, no end but himself. He would be a god to himself, and the satisfaction of his own will the sole measure and end of all his actions."

The "Radical Enlightenment" promoted the concept of separating church and state, an idea that is often credited to English philosopher John Locke (1632–1704). According to his principle of the social contract, Locke said that the government lacked authority in the realm of individual conscience, as this was something rational people could not cede to the government for it or others to control. For Locke, this created a natural right in the liberty of conscience, which he said must, therefore, remain protected from any government authority.

These views on religious tolerance and the importance of individual conscience, along with the social contract, became particularly influential in the American colonies and the drafting of the United States Constitution. Thomas Jefferson called for a "wall of separation between church and state" at the federal level. He previously had supported successful efforts to disestablish the Church of England in Virginia and authored the Virginia Statute for Religious Freedom. Jefferson's political ideals were greatly influenced by the writings of John Locke, Francis Bacon, and Isaac Newton, whom he considered the three greatest men that ever lived.

The Enlightenment took hold in most European countries, often with a specific local emphasis. For example, in France it became associated with anti-government and anti-Church radicalism. At the same time, in Germany it reached deep into the middle classes, where it expressed a spiritualistic and nationalistic tone without threatening governments or established churches. Government responses varied widely. In France, the government was hostile, and the "philosophes" fought against its censorship, sometimes being imprisoned or hounded into exile. The British government, for the most part, ignored the Enlightenment's leaders in England and Scotland, although it did give Isaac Newton a knighthood and a very lucrative government office.

Scholars have hotly debated the very existence of an English Enlightenment. The majority of textbooks on British history make little or no mention of an English Enlightenment. Some surveys of the entire Enlightenment include England, and others ignore it, although they do include coverage of such major intellectuals as Joseph Addison, Edward Gibbon, John Locke, Isaac Newton, Alexander Pope, Joshua Reynolds and Jonathan Swift. Roy Porter argues that the reasons for this neglect were the assumptions that the movement was primarily French-inspired, that it was largely a-religious or anti-clerical, and that it stood in outspoken defiance to the established order. Porter admits that, after the 1720s, England could claim thinkers to equal Diderot, Voltaire or Rousseau. However, its leading intellectuals such as Edward Gibbon, Edmund Burke and Samuel Johnson were all quite conservative and supportive of the standing order. Porter says the reason was that Enlightenment had come early to England and had succeeded so that the culture had accepted political liberalism, philosophical empiricism, and religious toleration of the sort that intellectuals on the continent had to fight against great odds. Furthermore, England rejected the collectivism of the continent and emphasized the improvement of individuals as the main goal of enlightenment.

In the Scottish Enlightenment, Scotland's major cities created an intellectual infrastructure of mutually supporting institutions such as universities, reading societies, libraries, periodicals, museums, and masonic lodges. The Scottish network was "predominantly liberal Calvinist, Newtonian, and 'design' oriented in character which played a major role in the further development of the transatlantic Enlightenment." In France, Voltaire said that "we look to Scotland for all our ideas of civilization". The focus of the Scottish Enlightenment ranged from intellectual and economic matters to the specifically scientific as in the work of William Cullen, physician and chemist; James Anderson, an agronomist; Joseph Black, physicist and chemist; and James Hutton, the first modern geologist.

Several Americans, especially Benjamin Franklin and Thomas Jefferson, played a major role in bringing Enlightenment ideas to the New World and in influencing British and French thinkers. Franklin was influential for his political activism and his advances in physics. The cultural exchange during the Age of Enlightenment ran in both directions across the Atlantic. Thinkers such as Paine, Locke, and Rousseau all take Native American cultural practices as examples of natural freedom. The Americans closely followed English and Scottish political ideas, as well as some French thinkers such as Montesquieu. As deists, they were influenced by ideas of John Toland (1670–1722) and Matthew Tindal (1656–1733). During the Enlightenment, there was a great emphasis on liberty, republicanism and religious tolerance. There was no respect for monarchy or inherited political power. Deists reconciled science and religion by rejecting prophecies, miracles, and Biblical theology. Leading deists included Thomas Paine in "The Age of Reason" and by Thomas Jefferson in his short "Jefferson Bible" – from which all supernatural aspects were removed.

Prussia took the lead among the German states in sponsoring the political reforms that Enlightenment thinkers urged absolute rulers to adopt. There were important movements as well in the smaller states of Bavaria, Saxony, Hanover, and the Palatinate. In each case, Enlightenment values became accepted and led to significant political and administrative reforms that laid the groundwork for the creation of modern states. The princes of Saxony, for example, carried out an impressive series of fundamental fiscal, administrative, judicial, educational, cultural, and general economic reforms. The reforms were aided by the country's strong urban structure and influential commercial groups and modernized pre-1789 Saxony along the lines of classic Enlightenment principles.

Before 1750, the German upper classes looked to France for intellectual, cultural, and architectural leadership, as French was the language of high society. By the mid-18th century, the "Aufklärung" (The Enlightenment) had transformed German high culture in music, philosophy, science, and literature. Christian Wolff (1679–1754) was the pioneer as a writer who expounded the Enlightenment to German readers and legitimized German as a philosophic language.

Johann Gottfried von Herder (1744–1803) broke new ground in philosophy and poetry, as a leader of the Sturm und Drang movement of proto-Romanticism. Weimar Classicism ("Weimarer Klassik") was a cultural and literary movement based in Weimar that sought to establish a new humanism by synthesizing Romantic, classical and Enlightenment ideas. The movement (from 1772 until 1805) involved Herder as well as polymath Johann Wolfgang von Goethe (1749–1832) and Friedrich Schiller (1759–1805), a poet and historian. Herder argued that every folk had its own particular identity, which was expressed in its language and culture. This legitimized the promotion of German language and culture and helped shape the development of German nationalism. Schiller's plays expressed the restless spirit of his generation, depicting the hero's struggle against social pressures and the force of destiny.

German music, sponsored by the upper classes, came of age under composers Johann Sebastian Bach (1685–1750), Joseph Haydn (1732–1809) and Wolfgang Amadeus Mozart (1756–1791).

In remote Königsberg, philosopher Immanuel Kant (1724–1804) tried to reconcile rationalism and religious belief, individual freedom, and political authority. Kant's work contained basic tensions that would continue to shape German thought – and indeed all of European philosophy – well into the 20th century.

The German Enlightenment won the support of princes, aristocrats, and the middle classes and it permanently reshaped the culture. However, there was a conservatism among the elites that warned against going too far.

In the 1780s, Lutheran ministers Johann Heinrich Schulz and Karl Wilhelm Brumbey got in trouble with their preaching as they were attacked and ridiculed by Immanuel Kant, Wilhelm Abraham Teller and others. In 1788, Prussia issued an "Edict on Religion" that forbade preaching any sermon that undermined popular belief in the Holy Trinity and the Bible. The goal was to avoid skepticism, deism, and theological disputes that might impinge on domestic tranquility. Men who doubted the value of Enlightenment favored the measure, but so too did many supporters. German universities had created a closed elite that could debate controversial issues among themselves, but spreading them to the public was seen as too risky. The state favored this intellectual elite, but that might be reversed if the process of the Enlightenment proved politically or socially destabilizing.

The Enlightenment played a distinctive if small, role in the history of Italy. Although most of Italy was controlled by conservative Habsburgs or the pope, Tuscany had some opportunities for reform. Leopold II of Tuscany abolished the death penalty in Tuscany and reduced censorship. From Naples, Antonio Genovesi (1713–1769) influenced a generation of southern Italian intellectuals and university students. His textbook "Diceosina, o Sia della Filosofia del Giusto e dell' Onesto" (1766) was a controversial attempt to mediate between the history of moral philosophy on the one hand and the specific problems encountered by 18th-century commercial society on the other. It contained the greater part of Genovesi's political, philosophical and economic thought – a guidebook for Neapolitan economic and social development. Science flourished as Alessandro Volta and Luigi Galvani made break-through discoveries in electricity. Pietro Verri was a leading economist in Lombardy. Historian Joseph Schumpeter states he was "the most important pre-Smithian authority on Cheapness-and-Plenty." The most influential scholar on the Italian Enlightenment has been Franco Venturi. Italy also produced some of the Enlightenment's greatest legal theorists, including Cesare Beccaria, Giambattista Vico, and Francesco Mario Pagano. Beccaria, in particular, is now considered one of the fathers of classical criminal theory as well as modern penology. Beccaria is famous for his masterpiece "On Crimes and Punishments" (1764), a treatise (later translated into 22 languages) that served as one of the earliest prominent condemnations of torture and the death penalty and thus a landmark work in anti-death penalty philosophy.

In Russia, the government began to actively encourage the proliferation of arts and sciences in the mid-18th century. This era produced the first Russian university, library, theatre, public museum, and independent press. Like other enlightened despots, Catherine the Great played a key role in fostering the arts, sciences, and education. She used her interpretation of Enlightenment ideals, assisted by notable international experts such as Voltaire (by correspondence) and in residence world-class scientists such as Leonhard Euler and Peter Simon Pallas. The national Enlightenment differed from its Western European counterpart in that it promoted further modernization of all aspects of Russian life and was concerned with attacking the institution of serfdom in Russia. The Russian enlightenment centered on the individual instead of societal enlightenment and encouraged the living of an enlightened life. A powerful element was "prosveshchenie", which combined religious piety, erudition, and commitment to the spread of learning. However, it lacked the skeptical and critical spirit of the Western European Enlightenment.

The enlightenment in Portugal ("iluminismo") was marked by the rule of the Prime Minister Marquis of Pombal under King Joseph I of Portugal from 1756 to 1777. Following the 1755 Lisbon earthquake which destroyed a great part of Lisbon, the Marquis of Pombal implemented important economic policies to regulate commercial activity (in particular with Brazil and England) and to standardize quality throughout the country (for example by introducing the first integrated industries in Portugal). His reconstruction of Lisbon's riverside district in straight and perpendicular streets, methodically organized to facilitate commerce and exchange (for example, by assigning to each street a different product or service), can be seen as a direct application of the Enlightenment ideas to governance and urbanism. His urbanistic ideas, also being the first large-scale example of earthquake engineering, became collectively known as Pombaline style, and were implemented throughout the kingdom during his stay in office. His governance was as enlightened as ruthless, see, for example, the Távora affair.

In literature, the first Enlightenment ideas in Portugal can be traced back to the diplomat, philosopher, and writer António Vieira (1608-1697), who spent a considerable amount of his life in colonial Brazil denouncing discriminations against New Christians and the Indigenous peoples in Brazil. His works remain today as one of the best pieces of Portuguese literature. During the 18th century, liberal literary movements such as the Arcádia Lusitana (lasting from 1756 until 1776, then replaced by the Nova Arcádia in 1790 until 1794) surfaced in the academic medium, in particular involving former students of the University of Coimbra. A distinct member of this group was the poet Manuel Maria Barbosa du Bocage.

The ideas of the enlightenment also influenced various economists and anti-colonial intellectuals throughout the Portuguese Empire, such as José de Azeredo Coutinho, José da Silva Lisboa, Cláudio Manoel da Costa, and Tomás de Antônio Gonzaga.

Enlightenment ideas ("oświecenie") emerged late in Poland, as the Polish middle class was weaker and szlachta (nobility) culture (Sarmatism) together with the Polish-Lithuanian Commonwealth political system (Golden Liberty) were in deep crisis. The political system was built on republicanism, but was unable to defend itself against powerful neighbors Russia, Prussia, and Austria as they repeatedly sliced off regions until nothing was left of independent Poland. The period of Polish Enlightenment began in the 1730s–1740s, and especially in theatre and the arts peaked in the reign of King Stanisław August Poniatowski (second half of the 18th century). Warsaw was a main center after 1750, with an expansion of schools and educational institutions and the arts patronage held at the Royal Castle. Leaders promoted tolerance and more education. They included King Stanislaw II Poniatowski and reformers Piotr Switkowski, Antoni Poplawski, Josef Niemcewicz, and Jósef Pawlikowski, as well as Baudouin de Cortenay, a Polonized dramatist. Opponents included Florian Jaroszewicz, Gracjan Piotrowski, Karol Wyrwicz and Wojciech Skarszewski.

The movement went into decline with the Third Partition of Poland (1795) – a national tragedy inspiring a short period of sentimental writing – and ended in 1822, replaced by Romanticism.

The Enlightenment has always been contested territory. According to Keith Thomas, its supporters "hail it as the source of everything progressive about the modern world. For them, it stands for freedom of thought, rational inquiry, critical thinking, religious tolerance, political liberty, scientific achievement, the pursuit of happiness, and hope for the future." Thomas adds that its detractors accuse it of shallow rationalism, naïve optimism, unrealistic universalism, and moral darkness. From the start, conservative and clerical defenders of traditional religion attacked materialism and skepticism as evil forces that encouraged immorality. By 1794, they pointed to the Terror during the French Revolution as confirmation of their predictions. As the Enlightenment was ending, Romantic philosophers argued that excessive dependence on reason was a mistake perpetuated by the Enlightenment because it disregarded the bonds of history, myth, faith, and tradition that were necessary to hold society together.

The term "Enlightenment" emerged in English in the later part of the 19th century, with particular reference to French philosophy, as the equivalent of the French term "Lumières" (used first by Dubos in 1733 and already well established by 1751). From Immanuel Kant's 1784 essay "Beantwortung der Frage: Was ist Aufklärung?" (""), the German term became "Aufklärun"g ("aufklären" = to illuminate; "sich aufklären" = to clear up). However, scholars have never agreed on a definition of the Enlightenment, or on its chronological or geographical extent. Terms like "les Lumières" (French), "illuminism"o (Italian), "ilustración" (Spanish) and "Aufklärung" (German) referred to partly overlapping movements. Not until the late nineteenth century did English scholars agree they were talking about "the Enlightenment".

Enlightenment historiography began in the period itself, from what Enlightenment figures said about their work. A dominant element was the intellectual angle they took. D'Alembert's "Preliminary Discourse of l'Encyclopédie" provides a history of the Enlightenment which comprises a chronological list of developments in the realm of knowledge – of which the "Encyclopédie" forms the pinnacle. In 1783, Jewish philosopher Moses Mendelssohn referred to Enlightenment as a process by which man was educated in the use of reason. Immanuel Kant called Enlightenment "man's release from his self-incurred tutelage," tutelage being "man's inability to make use of his understanding without direction from another." "For Kant, Enlightenment was mankind's final coming of age, the emancipation of the human consciousness from an immature state of ignorance." The German scholar Ernst Cassirer called the Enlightenment "apart and a special phase of that whole intellectual development through which modern philosophic thought gained its characteristic self-confidence and self-consciousness." According to historian Roy Porter, the liberation of the human mind from a dogmatic state of ignorance, is the epitome of what the Age of Enlightenment was trying to capture.

Bertrand Russell saw the Enlightenment as a phase in a progressive development which began in antiquity and that reason and challenges to the established order were constant ideals throughout that time. Russell said that the Enlightenment was ultimately born out of the Protestant reaction against the Catholic counter-reformation. That philosophical views such as affinity for democracy against monarchy originated among 16th-century Protestants to justify their desire to break away from the Catholic Church. Although many of these philosophical ideals were picked up by Catholics, Russell argues that by the 18th century the Enlightenment was the principal manifestation of the schism that began with Martin Luther.

Jonathan Israel rejects the attempts of postmodern and Marxian historians to understand the revolutionary ideas of the period purely as by-products of social and economic transformations. He instead focuses on the history of ideas in the period from 1650 to the end of the 18th century and claims that it was the ideas themselves that caused the change that eventually led to the revolutions of the latter half of the 18th century and the early 19th century. Israel argues that until the 1650s Western civilization "was based on a largely shared core of faith, tradition and authority".

There is little consensus on the precise beginning of the Age of Enlightenment, though several historians and philosophers argue that it was marked by Descartes' 1637 philosophy of "Cogito, ergo sum" ("I think; therefore I Am"), which shifted the epistemological basis from external authority to inner certainty. In France, many cited the publication of Isaac Newton's "Principia Mathematica" (1687). The middle of the 17th century (1650) or the beginning of the 18th century (1701) are often used as epochs. French historians usually place the "Siècle des Lumières" ("Century of Enlightenments") between 1715 and 1789: from the beginning of the reign of Louis XV until the French Revolution. Most scholars use the last years of the century, often choosing the French Revolution of 1789 or the beginning of the Napoleonic Wars (1804–1815) as a convenient point in time with which to date the end of the Enlightenment.

In the 1947 book "Dialectic of Enlightenment", Frankfurt School philosophers Max Horkheimer and Theodor W. Adorno argued: 

Extending Horkheimer and Adorno's argument, intellectual historian Jason Josephson-Storm has argued that any idea of the Age of Enlightenment as a clearly defined period that is separate from the earlier Renaissance and later Romanticism or Counter-Enlightenment constitutes a myth. Josephson-Storm points out that there are vastly different and mutually contradictory periodizations of the Enlightenment depending on the nation, a field of study, and school of thought; that the term and category of "Enlightenment" referring to the scientific revolution was applied after the fact; that the Enlightenment did not see an increase in disenchantment or the dominance of the mechanistic worldview; and that a blur in the early modern ideas of the Humanities and natural sciences makes it hard to circumscribe a Scientific Revolution. Josephson-Storm defends his categorization of the Enlightenment as "myth" by noting the regulative role ideas of a period of Enlightenment and disenchantment play in modern Western culture, such that belief in magic, spiritualism, and even religion appears somewhat taboo in intellectual strata.

In the 1970s, the study of the Enlightenment expanded to include the ways Enlightenment ideas spread to European colonies and how they interacted with indigenous cultures and how the Enlightenment took place in formerly unstudied areas such as Italy, Greece, the Balkans, Poland, Hungary and Russia.

Intellectuals such as Robert Darnton and Jürgen Habermas have focused on the social conditions of the Enlightenment. Habermas described the creation of the "bourgeois public sphere" in 18th-century Europe, containing the new venues and modes of communication allowing for rational exchange. Habermas said that the public sphere was bourgeois, egalitarian, intelligent, and independent from the state, making it the ideal venue for intellectuals to critically examine contemporary politics and society, away from the interference of established authority. While the public sphere is generally an integral component of the social study of the Enlightenment, other historians have questioned whether the public sphere had these characteristics.

In contrast to the intellectual historiographical approach of the Enlightenment, which examines the various currents or discourses of rational thought within the European context during the 17th and 18th centuries, the cultural (or social) approach examines the changes that occurred in European society and culture. This approach studies the process of changing sociabilities and cultural practices during the Enlightenment.

One of the primary elements of the culture of the Enlightenment was the rise of the public sphere, a "realm of communication marked by new arenas of debate, more open and accessible forms of urban public space and sociability, and an explosion of print culture," in the late 17th century and 18th century. Elements of the public sphere included that it was egalitarian, that it discussed the domain of "common concern," and that argument was founded on reason. Habermas uses the term "common concern" to describe those areas of political/social knowledge and discussion that were previously the exclusive territory of the state and religious authorities, now open to critical examination by the public sphere. The values of this bourgeois public sphere included holding reason to be supreme, considering everything to be open to criticism (the public sphere is critical), and the opposition of secrecy of all sorts.
The creation of the public sphere has been associated with two long-term historical trends: the rise of the modern nation-state and the rise of capitalism. The modern nation-state, in its consolidation of public power, created by counterpoint a private realm of society independent of the state, which allowed for the public sphere. Capitalism also increased society's autonomy and self-awareness, as well as an increasing need for the exchange of information. As the nascent public sphere expanded, it embraced a large variety of institutions, and the most commonly cited were coffee houses and cafés, salons, and the literary public sphere, figuratively localized in the Republic of Letters. In France, the creation of the public sphere was helped by the aristocracy's move from the King's palace at Versailles to Paris in about 1720, since their rich spending stimulated the trade in luxuries and artistic creations, especially fine paintings.

The context for the rise of the public sphere was the economic and social change commonly associated with the Industrial Revolution: "Economic expansion, increasing urbanization, rising population and improving communications in comparison to the stagnation of the previous century." Rising efficiency in production techniques and communication lowered the prices of consumer goods. It increased the amount and variety of goods available to consumers (including the literature essential to the public sphere). Meanwhile, the colonial experience (most European states had colonial empires in the 18th century) began to expose European society to extremely different cultures, leading to the breaking down of "barriers between cultural systems, religious divides, gender differences, and geographical areas." 

The word "public" implies the highest level of inclusivity – the public sphere, by definition, should be open to all. However, this sphere was only public to relative degrees. Enlightenment thinkers frequently contrasted their conception of the "public" with that of the people: Condorcet contrasted "opinion" with the populace, Marmontel "the opinion of men of letters" with "the opinion of the multitude" and d'Alembert the "truly enlightened public" with "the blind and noisy multitude." Additionally, most institutions of the public sphere excluded both women and the lower classes. Cross-class influences occurred through noble and lower class participation in areas such as the coffeehouses and the Masonic lodges.

Because of the focus on reason over superstition, the Enlightenment cultivated the arts. Emphasis on learning, art and music became more widespread, especially with the growing middle class. Areas of study such as literature, philosophy, science, and the fine arts increasingly explored subject matter to which the general public, in addition to the previously more segregated professionals and patrons, could relate.
As musicians depended more and more on public support, public concerts became increasingly popular and helped supplement performers' and composers' incomes. The concerts also helped them to reach a wider audience. Handel, for example, epitomized this with his highly public musical activities in London. He gained considerable fame there with performances of his operas and oratorios. The music of Haydn and Mozart, with their Viennese Classical styles, are usually regarded as being the most in line with the Enlightenment ideals.

The desire to explore, record, and systematize knowledge had a meaningful impact on music publications. Jean-Jacques Rousseau's "Dictionnaire de musique" (published 1767 in Geneva and 1768 in Paris) was a leading text in the late 18th century. This widely available dictionary gave short definitions of words like genius and taste and was influenced by the Enlightenment movement. Another text influenced by Enlightenment values was Charles Burney's "A General History of Music: From the Earliest Ages to the Present Period" (1776), which was a historical survey and an attempt to rationalize elements in music systematically over time. Recently, musicologists have shown renewed interest in the ideas and consequences of the Enlightenment. For example, Rose Rosengard Subotnik's "Deconstructive Variations" (subtitled "Music and Reason in Western Society") compares Mozart's "Die Zauberflöte" (1791) using the Enlightenment and Romantic perspectives and concludes that the work is "an ideal musical representation of the Enlightenment." 

As the economy and the middle class expanded, there was an increasing number of amateur musicians. One manifestation of this involved women, who became more involved with music on a social level. Women were already engaged in professional roles as singers and increased their presence in the amateur performers' scene, especially with keyboard music. Music publishers begin to print music that amateurs could understand and play. The majority of the works that were published were for keyboard, voice, and keyboard and chamber ensemble. After these initial genres were popularized, from the mid-century on, amateur groups sang choral music, which then became a new trend for publishers to capitalize on. The increasing study of the fine arts, as well as access to amateur-friendly published works, led to more people becoming interested in reading and discussing music. Music magazines, reviews and critical works which suited amateurs, as well as connoisseurs, began to surface.

The "philosophes" spent a great deal of energy disseminating their ideas among educated men and women in cosmopolitan cities. They used many venues, some of them quite new.

The term "Republic of Letters" was coined in 1664 by Pierre Bayle in his journal "Nouvelles de la Republique des Lettres". Towards the end of the 18th century, the editor of "Histoire de la République des Lettres en France", a literary survey, described the Republic of Letters as being: 

The Republic of Letters was the sum of several Enlightenment ideals: an egalitarian realm governed by knowledge that could act across political boundaries and rival state power. It was a forum that supported "free public examination of questions regarding religion or legislation." Immanuel Kant considered written communication essential to his conception of the public sphere; once everyone was a part of the "reading public," then society could be said to be enlightened. The people who participated in the Republic of Letters, such as Diderot and Voltaire, are frequently known today as prominent Enlightenment figures. Indeed, the men who wrote Diderot's "Encyclopédie" arguably formed a microcosm of the more extensive "republic." 
Many women played an essential part in the French Enlightenment, due to the role they played as "salonnières" in Parisian salons, as the contrast to the male "philosophes". The salon was the principal social institution of the republic and "became the civil working spaces of the project of Enlightenment." Women, as salonnières, were "the legitimate governors of [the] potentially unruly discourse" that took place within. While women were marginalized in the public culture of the Old Regime, the French Revolution destroyed the old cultural and economic restraints of patronage and corporatism (guilds), opening French society to female participation, particularly in the literary sphere.

In France, the established men of letters ("gens de lettres") had fused with the elites ("les grands") of French society by the mid-18th century. This led to the creation of an oppositional literary sphere, Grub Street, the domain of a "multitude of versifiers and would-be authors." These men came to London to become authors, only to discover that the literary market simply could not support large numbers of writers, who in any case were very poorly remunerated by the publishing-bookselling guilds.

The writers of Grub Street, the Grub Street Hacks, were left feeling bitter about the relative success of the men of letters and found an outlet for their literature which was typified by the "libelle". Written mostly in the form of pamphlets, the "libelles" "slandered the court, the Church, the aristocracy, the academies, the salons, everything elevated and respectable, including the monarchy itself." "Le Gazetier cuirassé" by Charles Théveneau de Morande was a prototype of the genre. It was Grub Street literature that was most read by the public during the Enlightenment. According to Darnton, more importantly, the Grub Street hacks inherited the "revolutionary spirit" once displayed by the "philosophes" and paved the way for the French Revolution by desacralizing figures of political, moral and religious authority in France.

The increased consumption of reading materials of all sorts was one of the key features of the "social" Enlightenment. Developments in the Industrial Revolution allowed consumer goods to be produced in greater quantities at lower prices, encouraging the spread of books, pamphlets, newspapers, and journals – "media of the transmission of ideas and attitudes." Commercial development likewise increased the demand for information, along with rising populations and increased urbanization. However, demand for reading material extended outside of the realm of the commercial and outside the realm of the upper and middle classes, as evidenced by the Bibliothèque Bleue. Literacy rates are difficult to gauge, but in France, the rates doubled over the 18th century. Reflecting the decreasing influence of religion, the number of books about science and art published in Paris doubled from 1720 to 1780, while the number of books about religion dropped to just one-tenth of the total.

Reading underwent serious changes in the 18th century. In particular, Rolf Engelsing has argued for the existence of a "Reading Revolution". Until 1750, reading was done intensively: people tended to own a small number of books and read them repeatedly, often to a small audience. After 1750, people began to read "extensively," finding as many books as they could, increasingly reading them alone. This is supported by increasing literacy rates, particularly among women.

The vast majority of the reading public could not afford to own a private library, and while most of the state-run "universal libraries" set up in the 17th and 18th centuries were open to the public, they were not the only sources of reading material. On one end of the spectrum was the "Bibliothèque Bleue", a collection of cheaply produced books published in Troyes, France. Intended for a largely rural and semi-literate audience, these books included almanacs, retellings of medieval romances, and condensed versions of popular novels, among other things. While some historians have argued against the Enlightenment's penetration into the lower classes, the "Bibliothèque Bleue" represents at least a desire to participate in Enlightenment sociability. Moving up the classes, a variety of institutions offered readers access to material without needing to buy anything. Libraries that lent out their material for a small price started to appear, and occasionally bookstores would offer a small lending library to their patrons. Coffee houses commonly offered books, journals, and sometimes even popular novels to their customers. "The Tatler" and "The Spectator", two influential periodicals sold from 1709 to 1714, were closely associated with coffee house culture in London, being both read and produced in various establishments in the city. This is an example of the triple or even quadruple function of the coffee house: reading material was often obtained, read, discussed and even produced on the premises.
It is challenging to determine what people read during the Enlightenment. For example, examining the catalogs of private libraries gives an image skewed in favor of the classes wealthy enough to afford libraries and also ignores censored works unlikely to be publicly acknowledged. For this reason, a study of publishing would be much more fruitful for discerning reading habits.

Across continental Europe, but in France especially, booksellers and publishers had to negotiate censorship laws of varying strictness. For example, the "Encyclopédie" narrowly escaped seizure and had to be saved by Malesherbes, the man in charge of the French censor. Indeed, many publishing companies were conveniently located outside France to avoid overzealous French censors. They would smuggle their merchandise across the border, where it would then be transported to clandestine booksellers or small-time peddlers. The records of clandestine booksellers may give a better representation of what literate Frenchmen might have truly read since their clandestine nature provided a less restrictive product choice. In one case, political books were the most popular category, primarily libels and pamphlets. Readers were more interested in sensationalist stories about criminals and political corruption than they were in political theory itself. The second most popular category, "general works" (those books "that did not have a dominant motif and that contained something to offend almost everyone in authority"), demonstrated a high demand for generally low-brow subversive literature. However, these works never became part of the literary canon and are largely forgotten today as a result.

A healthy, legal publishing industry existed throughout Europe, although established publishers and booksellers occasionally ran afoul of the law. For example, the "Encyclopédie" condemned not only by the King but also by Clement XII, nevertheless found its way into print with the help of the Malesherbes as mentioned above and creative use of French censorship law. However, many works were sold without running into any legal trouble at all. Borrowing records from libraries in England, Germany, and North America indicate that more than 70 percent of books borrowed were novels. Less than 1 percent of the books were religious, indicating the general trend of declining religiosity.

A genre that greatly rose in importance was that of scientific literature. Natural history, in particular, became increasingly popular among the upper classes. Works of natural history include René-Antoine Ferchault de Réaumur's "Histoire naturelle des insectes" and Jacques Gautier d'Agoty's "La Myologie complète, ou description de tous les muscles du corps humain" (1746). Outside ancien régime France, natural history was an important part of medicine and industry, encompassing the fields of botany, zoology, meteorology, hydrology, and mineralogy. Students in Enlightenment universities and academies were taught these subjects to prepare them for careers as diverse as medicine and theology. As shown by Matthew Daniel Eddy, natural history in this context was a very middle-class pursuit and operated as a fertile trading zone for the interdisciplinary exchange of diverse scientific ideas.

The target audience of natural history was polite French society, evidenced more by the specific discourse of the genre than by the generally high prices of its works. Naturalists catered to polite society's desire for erudition – many texts had an explicit instructive purpose. However, natural history was often a political affair. As Emma Spary writes, the classifications used by naturalists "slipped between the natural world and the social ... to establish not only the expertise of the naturalists over the natural, but also the dominance of the natural over the social". The idea of taste ("le goût") was a social indicator: to be able to categorize nature truly; one had to have the proper taste, an ability of discretion shared by all members of polite society. In this way, natural history spread many of the scientific developments of the time, but also provided a new source of legitimacy for the dominant class. From this basis, naturalists could then develop their social ideals based on their scientific works.

The first scientific and literary journals were established during the Enlightenment. The first journal, the Parisian "Journal des Sçavans", appeared in 1665. However, it was not until 1682 that periodicals began to be more widely produced. French and Latin were the dominant languages of publication, but there was also a steady demand for material in German and Dutch. There was generally low demand for English publications on the Continent, which was echoed by England's similar lack of desire for French works. Languages commanding less of an international market—such as Danish, Spanish, and Portuguese—found journal success more difficult, and more often than not, a more international language was used instead. French slowly took over Latin's status as the "lingua franca" of scholarly circles. This, in turn, gave precedence to the publishing industry in Holland, where the vast majority of these French language periodicals were produced.

Jonathan Israel called the journals the most influential cultural innovation of European intellectual culture. They shifted the attention of the "cultivated public" away from established authorities to novelty and innovation and instead promoted the "enlightened" ideals of toleration and intellectual objectivity. Being a source of knowledge derived from science and reason, they were an implicit critique of existing notions of universal truth monopolized by monarchies, parliaments, and religious authorities. They also advanced Christian enlightenment that upheld "the legitimacy of God-ordained authority"—the Bible—in which there had to be an agreement between the biblical and natural theories.

Although the existence of dictionaries and encyclopedias spanned into ancient times, the texts changed from simply defining words in a long-running list to far more detailed discussions of those words in 18th-century encyclopedic dictionaries. The works were part of an Enlightenment movement to systematize knowledge and provide education to a wider audience than the elite. As the 18th century progressed, the content of encyclopedias also changed according to readers' tastes. Volumes tended to focus more strongly on secular affairs, particularly science and technology, rather than matters of theology.

Along with secular matters, readers also favored an alphabetical ordering scheme over cumbersome works arranged along thematic lines. Commenting on alphabetization, the historian Charles Porset has said that "as the zero degrees of taxonomy, alphabetical order authorizes all reading strategies; in this respect, it could be considered an emblem of the Enlightenment." For Porset, the avoidance of thematic and hierarchical systems thus allows free interpretation of the works and becomes an example of egalitarianism. Encyclopedias and dictionaries also became more popular during the Age of Enlightenment as the number of educated consumers who could afford such texts began to multiply. In the latter half of the 18th century, the number of dictionaries and encyclopedias published by decade increased from 63 between 1760 and 1769 to approximately 148 in the decade proceeding the French Revolution (1780–1789). Along with the growth in numbers, dictionaries and encyclopedias also grew in length, often having multiple prints runs that sometimes included in supplemented editions.

The first technical dictionary was drafted by John Harris and entitled "Lexicon Technicum: Or, An Universal English Dictionary of Arts and Sciences". Harris' book avoided theological and biographical entries, and instead it concentrated on science and technology. Published in 1704, the "Lexicon technicum" was the first book to be written in English that took a systematic approach to describe mathematics and commercial arithmetic along with the physical sciences and navigation. Other technical dictionaries followed Harris' model, including Ephraim Chambers' "Cyclopaedia" (1728), which included five editions. They was a substantially larger work than Harris'. The folio edition of the work even included foldout engravings. The "Cyclopaedia" emphasized Newtonian theories, Lockean philosophy and contained thorough examinations of technologies, such as engraving, brewing and dyeing. In Germany, practical reference works intended for the uneducated majority became popular in the 18th century. The "Marperger Curieuses Natur-, Kunst-, Berg-, Gewerkund Handlungs-Lexicon" (1712) explained terms that usefully described the trades and scientific and commercial education. "Jablonksi Allgemeines Lexicon" (1721) was better known than the "Handlungs-Lexicon" and underscored technical subjects rather than a scientific theory. For example, over five columns of text were dedicated to wine while geometry and logic were allocated only twenty-two and seventeen lines, respectively. The first edition of the "Encyclopædia Britannica" (1771) was modeled along the same lines as the German lexicons.

However, the prime example of reference works that systematized scientific knowledge in the age of Enlightenment was universal encyclopedias rather than technical dictionaries. It was the goal of universal encyclopedias to record all human knowledge in a comprehensive reference work. The most well-known of these works is Denis Diderot and Jean le Rond d'Alembert's "Encyclopédie, ou dictionnaire raisonné des sciences, des arts et des métiers". The work, which began publication in 1751, was composed of thirty-five volumes and over 71 000 separate entries. A great number of the entries were dedicated to describing the sciences and crafts in detail and provided intellectuals across Europe with a high-quality survey of human knowledge. In d'Alembert's "Preliminary Discourse to the Encyclopedia of Diderot", the work's goal to record the extent of human knowledge in the arts and sciences is outlined:

The massive work was arranged according to a "tree of knowledge." The tree reflected the marked division between the arts and sciences, which was largely a result of the rise of empiricism. Both areas of knowledge were united by philosophy or the trunk of the tree of knowledge. The Enlightenment's desacralization of religion was pronounced in the tree's design, particularly where theology accounted for a peripheral branch, with black magic as a close neighbor. As the "Encyclopédie" gained popularity, it was published in quarto and octavo editions after 1777. The quarto and octavo editions were much less expensive than previous editions, making the "Encyclopédie" more accessible to the non-elite. Robert Darnton estimates that there were approximately 25 000 copies of the "Encyclopédie" in circulation throughout France and Europe before the French Revolution. The extensive, yet affordable encyclopedia came to represent the transmission of Enlightenment and scientific education to an expanding audience.

One of the most important developments that the Enlightenment era brought to the discipline of science was its popularization. An increasingly literate population seeking knowledge and education in both the arts and the sciences drove the expansion of print culture and the dissemination of scientific learning. The new literate population was due to a high rise in the availability of food. This enabled many people to rise out of poverty, and instead of paying more for food, they had the money for education. Popularization was generally part of an overarching Enlightenment ideal that endeavored "to make information available to the greatest number of people." As public interest in natural philosophy grew during the 18th century, public lecture courses and the publication of popular texts opened up new roads to money and fame for amateurs and scientists who remained on the periphery of universities and academies. More formal works included explanations of scientific theories for individuals lacking the educational background to comprehend the original scientific text. Sir Isaac Newton's celebrated "Philosophiae Naturalis Principia Mathematica" was published in Latin and remained inaccessible to readers without education in the classics until Enlightenment writers began to translate and analyze the text in the vernacular.
The first significant work that expressed scientific theory and knowledge expressly for the laity, in the vernacular and with the entertainment of readers in mind, was Bernard de Fontenelle's "Conversations on the Plurality of Worlds" (1686). The book was produced specifically for women with interest in scientific writing and inspired a variety of similar works. These famous works were written in a discursive style, which was laid out much more clearly for the reader than the complicated articles, treatises, and books published by the academies and scientists. Charles Leadbetter's "Astronomy" (1727) was advertised as "a Work entirely New" that would include "short and easie Rules and Astronomical Tables". The first French introduction to Newtonianism and the "Principia" was "Eléments de la philosophie de Newton", published by Voltaire in 1738. Émilie du Châtelet's translation of the "Principia", published after her death in 1756, also helped to spread Newton's theories beyond scientific academies and the university. Writing for a growing female audience, Francesco Algarotti published "Il Newtonianism per le dame", which was a tremendously popular work and was translated from Italian into English by Elizabeth Carter. A similar introduction to Newtonianism for women was produced by Henry Pemberton. His "A View of Sir Isaac Newton's Philosophy" was published by subscription. Extant records of subscribers show that women from a wide range of social standings purchased the book, indicating the growing number of scientifically inclined female readers among the middling class. During the Enlightenment, women also began producing popular scientific works themselves. Sarah Trimmer wrote a successful natural history textbook for children titled "The Easy Introduction to the Knowledge of Nature" (1782), which was published for many years after in eleven editions.

Most work on the Enlightenment emphasizes the ideals discussed by intellectuals, rather than the actual state of education at the time. Leading educational theorists like England's John Locke and Switzerland's Jean Jacques Rousseau both emphasized the importance of shaping young minds early. By the late Enlightenment, there was a rising demand for a universal approach to education, particularly after the American and French Revolutions.

The predominant educational psychology from the 1750s onward, especially in northern European countries, was associationism, the notion that the mind associates or dissociates ideas through repeated routines. In addition to being conducive to Enlightenment ideologies of liberty, self-determination and personal responsibility, it offered a practical theory of the mind that allowed teachers to transform longstanding forms of print and manuscript culture into effective graphic tools of learning for the lower and middle orders of society. Children were taught to memorize facts through oral and graphic methods that originated during the Renaissance.

Many of the leading universities associated with Enlightenment progressive principles were located in northern Europe, with the most renowned being the universities of Leiden, Göttingen, Halle, Montpellier, Uppsala, and Edinburgh. These universities, especially Edinburgh, produced professors whose ideas had a significant impact on Britain's North American colonies and later the American Republic. Within the natural sciences, Edinburgh's medical school also led the way in chemistry, anatomy and pharmacology. In other parts of Europe, the universities and schools of France and most of Europe were bastions of traditionalism and were not hospitable to the Enlightenment. In France, the major exception was the medical university at Montpellier.

The history of Academies in France during the Enlightenment begins with the Academy of Science, founded in 1635 in Paris. It was closely tied to the French state, acting as an extension of a government seriously lacking in scientists. It helped promote and organize new disciplines, and it trained new scientists. It also contributed to the enhancement of scientists' social status, considering them to be the "most useful of all citizens." Academies demonstrate the rising interest in science along with its increasing secularization, as evidenced by the small number of clerics who were members (13 percent). The presence of the French academies in the public sphere cannot be attributed to their membership, as although the majority of their members were bourgeois, the elite institution was only open to elite Parisian scholars. They perceived themselves as "interpreters of the sciences for the people." For example, it was with this in mind that academicians took it upon themselves to disprove the popular pseudo-science of mesmerism.

The strongest contribution of the French Academies to the public sphere comes from the "concours académiques" (roughly translated as "academic contests") they sponsored throughout France. These academic contests were perhaps the most public of any institution during the Enlightenment. The practice of contests dated back to the Middle Ages and was revived in the mid-17th century. The subject matter had previously been generally religious and monarchical, featuring essays, poetry, and painting. However, by roughly 1725, this subject matter had radically expanded and diversified, including "royal propaganda, philosophical battles, and critical ruminations on the social and political institutions of the Old Regime." Topics of public controversy were also discussed such as the theories of Newton and Descartes, the slave trade, women's education and justice in France.
More importantly, the contests were open to all, and the enforced anonymity of each submission guaranteed that neither gender nor social rank would determine the judging. Indeed, although the "vast majority" of participants belonged to the wealthier strata of society ("the liberal arts, the clergy, the judiciary, and the medical profession"), there were some cases of the popular classes submitting essays and even winning. Similarly, a significant number of women participated—and won—the competitions. Of a total of 2,300 prize competitions offered in France, women won 49—perhaps a small number by modern standards, but very significant in an age in which most women did not have any academic training. Indeed, the majority of the winning entries were for poetry competitions; a genre commonly stressed in women's education.

In England, the Royal Society of London also played a significant role in the public sphere and the spread of Enlightenment ideas. It was founded by a group of independent scientists and given a royal charter in 1662. The Society played a large role in spreading Robert Boyle's experimental philosophy around Europe and acted as a clearinghouse for intellectual correspondence and exchange. Boyle was "a founder of the experimental world in which scientists now live and operate" and his method based knowledge on experimentation, which had to be witnessed to provide proper empirical legitimacy. This is where the Royal Society came into play: witnessing had to be a "collective act" and the Royal Society's assembly rooms were ideal locations for relatively public demonstrations. However, not just any witness was considered to be credible: "Oxford professors were accounted more reliable witnesses than Oxfordshire peasants." Two factors were taken into account: a witness's knowledge in the area and a witness's "moral constitution." In other words, only civil society was considered for Boyle's public.

It was the place in which philosophes got reunited and talked about old, actual, or new ideas. Salons were the place where intellectual and enlightened ideas were built.

Coffeehouses were especially important to the spread of knowledge during the Enlightenment because they created a unique environment in which people from many different walks of life gathered and shared ideas. They were frequently criticized by nobles who feared the possibility of an environment in which class and its accompanying titles and privileges were disregarded. Such an environment was especially intimidating to monarchs who derived much of their power from the disparity between classes of people. If classes were to join together under the influence of Enlightenment thinking, they might recognize the all-encompassing oppression and abuses of their monarchs and, because of their size might be able to carry out successful revolts. Monarchs also resented the idea of their subjects convening as one to discuss political matters, especially those concerning foreign affairs—rulers thought political affairs to be their business only, a result of their supposed divine right to rule.

Coffeehouses represent a turning point in history during which people discovered that they could have enjoyable social lives within their communities. Coffeeshops became homes away from home for many who sought, for the first time, to engage in discourse with their neighbors and discuss intriguing and thought-provoking matters, especially those regarding philosophy to politics. Coffeehouses were essential to the Enlightenment, for they were centers of free-thinking and self-discovery. Although many coffeehouse patrons were scholars, a great deal was not. Coffeehouses attracted a diverse set of people, including not only the educated wealthy but also members of the bourgeoisie and the lower class. While it may seem positive that patrons, being doctors, lawyers, merchants, etc. represented almost all classes, the coffeeshop environment sparked fear in those who sought to preserve the class distinction. One of the most popular critiques of the coffeehouse claimed that it "allowed promiscuous association among people from different rungs of the social ladder, from the artisan to the aristocrat" and was therefore compared to Noah's Ark, receiving all types of animals, clean or unclean. This unique culture served as a catalyst for journalism when Joseph Addison and Richard Steele recognized its potential as an audience. Together, Steele and Addison published "The Spectator (1711)", a daily publication which aimed, through fictional narrator Mr. Spectator, both to entertain and to provoke discussion regarding serious philosophical matters.

The first English coffeehouse opened in Oxford in 1650. Brian Cowan said that Oxford coffeehouses developed into "penny universities," offering a locus of learning that was less formal than structured institutions. These penny universities occupied a significant position in Oxford academic life, as they were frequented by those consequently referred to as the "virtuosi", who conducted their research on some of the resulting premises. According to Cowan, "the coffeehouse was a place for like-minded scholars to congregate, to read, as well as learn from and to debate with each other, but was emphatically not a university institution, and the discourse there was of a far different order than any university tutorial." 

The Café Procope was established in Paris in 1686, and by the 1720s, there were around 400 cafés in the city. The Café Procope, in particular, became a center of Enlightenment, welcoming such celebrities as Voltaire and Rousseau. The Café Procope was where Diderot and D'Alembert decided to create the "Encyclopédie". The cafés were one of the various "nerve centers" for "bruits publics", public noise, or rumor. These "bruits" were allegedly a much better source of information than were the actual newspapers available at the time.

The debating societies are an example of the public sphere during the Enlightenment. Their origins include:
In the late 1770s, popular debating societies began to move into more "genteel" rooms, a change which helped establish a new standard of sociability. The backdrop to these developments was "an explosion of interest in the theory and practice of public elocution." The debating societies were commercial enterprises that responded to this demand, sometimes very successfully. Some societies welcomed from 800 to 1,200 spectators a night.

The debating societies discussed a vast range of topics. Before the Enlightenment, most intellectual debates revolved around "confessional" – that is, Catholic, Lutheran, Reformed (Calvinist) or Anglican issues and the main aim of these debates was to establish which bloc of faith ought to have the "monopoly of truth and a God-given title to authority". After this date, everything thus previously rooted in tradition was questioned and often replaced by new concepts in the light of philosophical reason. After the second half of the 17th century and during the 18th century, a "general process of rationalization and secularization set in" and confessional disputes were reduced to a secondary status in favor of the "escalating contest between faith and incredulity".

In addition to debates on religion, societies discussed issues such as politics and the role of women. However, it is important to note that the critical subject matter of these debates did not necessarily translate into opposition to the government. In other words, the results of the debate quite frequently upheld the "status quo". From a historical standpoint, one of the most important features of the debating society was their openness to the public, as women attended and even participated in almost every debating society, which was likewise open to all classes providing they could pay the entrance fee. Once inside, spectators were able to participate in a largely egalitarian form of sociability that helped spread Enlightenment ideas.

Historians have long debated the extent to which the secret network of Freemasonry was a main factor in the Enlightenment. The leaders of the Enlightenment included Freemasons such as Diderot, Montesquieu, Voltaire, Lessing, Pope, Horace Walpole, Sir Robert Walpole, Mozart, Goethe, Frederick the Great, Benjamin Franklin and George Washington. Norman Davies said that Freemasonry was a powerful force on behalf of liberalism in Europe from about 1700 to the twentieth century. It expanded rapidly during the Age of Enlightenment, reaching practically every country in Europe. It was especially attractive to powerful aristocrats and politicians as well as intellectuals, artists and political activists.

During the Age of Enlightenment, Freemasons comprised an international network of like-minded men, often meeting in secret in ritualistic programs at their lodges. They promoted the ideals of the Enlightenment and helped diffuse these values across Britain and France and other places. Freemasonry as a systematic creed with its myths, values, and set of rituals originated in Scotland around 1600. It spread first to England and then across the Continent in the eighteenth century. They fostered new codes of conduct—including a communal understanding of liberty and equality inherited from guild sociability—"liberty, fraternity, and equality." Scottish soldiers and Jacobite Scots brought to the Continent ideals of fraternity which reflected not the local system of Scottish customs but the institutions and ideals originating in the English Revolution against royal absolutism. Freemasonry was particularly prevalent in France—by 1789, there were perhaps as many as 100,000 French Masons, making Freemasonry the most popular of all Enlightenment associations. The Freemasons displayed a passion for secrecy and created new degrees and ceremonies. Similar societies, partially imitating Freemasonry, emerged in France, Germany, Sweden, and Russia. One example was the Illuminati founded in Bavaria in 1776, which was copied after the Freemasons, but was never part of the movement. The Illuminati was an overtly political group, which most Masonic lodges decidedly were not.

Masonic lodges created a private model for public affairs. The "reconstituted the polity and established a constitutional form of self-government, complete with constitutions and laws, elections and representatives." In other words, the micro-society set up within the lodges constituted a normative model for society as a whole. This was especially true on the continent: when the first lodges began to appear in the 1730s, their embodiment of British values was often seen as threatening by state authorities. For example, the Parisian lodge that met in the mid-1720s was composed of English Jacobite exiles. Furthermore, freemasons all across Europe explicitly linked themselves to the Enlightenment as a whole. For example, in French lodges, the line "As the means to be enlightened, I search for the enlightened" was a part of their initiation rites. British lodges assigned themselves the duty to "initiate the unenlightened." This did not necessarily link lodges to the irreligious, but neither did this exclude them from the occasional heresy. Many lodges praised the Grand Architect, the masonic terminology for the deistic divine being who created a scientifically ordered universe.

German historian Reinhart Koselleck claimed: "On the Continent, there were two social structures that left a decisive imprint on the Age of Enlightenment: the Republic of Letters and the Masonic lodges." Scottish professor Thomas Munck argues that "although the Masons did promote international and cross-social contacts which were essentially non-religious and broadly in agreement with enlightened values, they can hardly be described as a major radical or reformist network in their own right." Many of the Masons values seemed to appeal to Enlightenment values and thinkers greatly. Diderot discusses the link between Freemason ideals and the enlightenment in D'Alembert's Dream, exploring masonry as a way of spreading enlightenment beliefs. Historian Margaret Jacob stresses the importance of the Masons in indirectly inspiring enlightened political thought. On the negative side, Daniel Roche contests claim that Masonry promoted egalitarianism and he argues that the lodges only attracted men of similar social backgrounds. The presence of noblewomen in the French "lodges of adoption" that formed in the 1780s was largely due to the close ties shared between these lodges and aristocratic society.

The major opponent of Freemasonry was the Roman Catholic Church so that in countries with a large Catholic element, such as France, Italy, Spain, and Mexico, much of the ferocity of the political battles involve the confrontation between what Davies calls the reactionary Church and enlightened Freemasonry. Even in France, Masons did not act as a group. American historians, while noting that Benjamin Franklin and George Washington were indeed active Masons, have downplayed the importance of Freemasonry in causing the American Revolution because the Masonic order was non-political and included both Patriots and their enemy the Loyalists.

The art produced during the Enlightenment was about a search for morality that was absent from prior art. At the same time, the Classical art of Greece and Rome became interesting to people again, since archaeological teams discovered Pompeii and Herculaneum. People took inspiration from it and revived the classical art into neo-classical art. This can be especially seen in early American art, where, throughout their art and architecture, they used arches, goddesses, and other classical architectural designs.




</doc>
<doc id="47678053" url="https://en.wikipedia.org/wiki?curid=47678053" title="Point of view (philosophy)">
Point of view (philosophy)

A point of view, in philosophy, is an attitude – how one sees or thinks: a specified (or stated) manner of consideration as in "my personal point of view". 

In this meaning, the usage is synonymous with one of the meanings of the term perspective. This figurative usage of the expression is from 1760.

The concept of the "point of view" is highly multifunctional and ambiguous. Many things may be judged from certain personal, traditional or moral points of view (as in "beauty is in the eye of the beholder"). Our knowledge about reality is often relative to a certain point of view.

Vázquez Campos and Manuel Liz Gutierrez suggested to analyse the concept of "point of view" using two approaches: one based on the concept of "propositional attitudes", the other on the concepts of "location" and "access".
Margarita Vázquez Campos and Antonio Manuel Liz Gutiérrez in their work, "The Notion of Point of View", give a comprehensive analysis of the structure of the concept. They point out that despite being crucial in many discourses, the notion has not been adequately analyzed, though some important works do exist. They mention that early classical Greek philosophers, starting from Parmenides and Heraclitus discussed the relation between "appearance" and reality, i.e., how our points of view are connected with reality. They specifically point out Ludwig Wittgenstein's "Tractatus Logico-Philosophicus". They consider Wittgenstein's theory of "pictures" or "models" (Wittgenstein used the German word "Bild", which means both "picture" and "model") as an illustration of the relationship between points of view and reality.

The internal structure of a point of view may be analysed similarly to the concept of a propositional attitude. A propositional attitude is an attitude, i.e., a mental state held by an agent toward a proposition. Examples of such attitudes are "to believe in something", "to desire something", "to guess something", "to remember something", etc. Vazques Campos and Gutierrez suggest that points of view may be analyzed as structured sets of propositional attitudes. The authors draw on Christopher Peacocke's "Sense and Content".

Within this approach one may carry out ontological classification of various distinctions, such as individual vs. collective points of view, personal vs. non-personal, non-conceptual vs. conceptual, etc.

Whereas propositional attitudes approach is to analyze points of view internally, the "location/access" approach analyzes points of view externally, by their role. The term "access" refers to the statement of Liz Gutierrez that "points of views, or perspectives, are ways of having access to the world and to ourselves", and the term "location" is in reference to the provided quotation of Jon Moline that points of view are "ways of viewing things and events from certain locations". Moline rejects the notion that points of view are reducible to some rules based on some theories, maxims or dogmas. Moline considers the concept of "location" in two ways: in a direct way as a vantage point, and in an extended way, the way how a given vantage point provides a perspective, i.e., influences the perception.

This approach may address epistemological issues, such as relativism, existence of the absolute point of view, compatibility of points of view (including "faultless disagreement"), possibility of a point of view without a bearer, etc.





</doc>
<doc id="19378" url="https://en.wikipedia.org/wiki?curid=19378" title="Mind">
Mind

The mind is the set of cognitive faculties including consciousness, imagination, perception, thinking, judgement, language and memory, which is housed in the brain (sometimes including the central nervous system). It is usually defined as the faculty of an entity's thoughts and consciousness. It holds the power of imagination, recognition, and appreciation, and is responsible for processing feelings and emotions, resulting in attitudes and actions.

There is a lengthy tradition in philosophy, religion, psychology, and cognitive science about what constitutes a mind and what are its distinguishing properties.

One open question regarding the nature of the mind is the mind–body problem, which investigates the relation of the mind to the physical brain and nervous system. Older viewpoints included dualism and idealism, which considered the mind somehow non-physical. Modern views often center around physicalism and functionalism, which hold that the mind is roughly identical with the brain or reducible to physical phenomena such as neuronal activity, though dualism and idealism continue to have many supporters. Another question concerns which types of beings are capable of having minds (New Scientist 8 September 2018 p10). For example, whether mind is exclusive to humans, possessed also by some or all animals, by all living things, whether it is a strictly definable characteristic at all, or whether mind can also be a property of some types of human-made machines.

Whatever its nature, it is generally agreed that mind is that which enables a being to have subjective awareness and intentionality towards their environment, to perceive and respond to stimuli with some kind of agency, and to have consciousness, including thinking and feeling.

The concept of mind is understood in many different ways by many different cultural and religious traditions. Some see mind as a property exclusive to humans whereas others ascribe properties of mind to non-living entities (e.g. panpsychism and animism), to animals and to deities. Some of the earliest recorded speculations linked mind (sometimes described as identical with soul or spirit) to theories concerning both life after death, and cosmological and natural order, for example in the doctrines of Zoroaster, the Buddha, Plato, Aristotle, and other ancient Greek, Indian and, later, Islamic and medieval European philosophers.

Important philosophers of mind include Plato, Patanjali, Descartes, Leibniz, Locke, Berkeley, Hume, Kant, Hegel, Schopenhauer, Searle, Dennett, Fodor, Nagel, and Chalmers. Psychologists such as Freud and James, and computer scientists such as Turing and Putnam developed influential theories about the nature of the mind. The possibility of nonbiological minds is explored in the field of artificial intelligence, which works closely in relation with cybernetics and information theory to understand the ways in which information processing by nonbiological machines is comparable or different to mental phenomena in the human mind.

The mind is also portrayed as the stream of consciousness where sense impressions and mental phenomena are constantly changing.

The original meaning of Old English "gemynd" was the faculty of memory, not of thought in general. Hence "call to mind", "come to mind", "keep in mind", "to have mind of", etc. The word retains this sense in Scotland. Old English had other words to express "mind", such as "hyge" "mind, spirit".

The meaning of "memory" is shared with Old Norse, which has "munr". The word is originally from a PIE verbal root "", meaning "to think, remember", whence also Latin" mens" "mind", Sanskrit "" "mind" and Greek μένος "mind, courage, anger".

The generalization of "mind" to include all mental faculties, thought, volition, feeling and memory, gradually develops over the 14th and 15th centuries.

The attributes that make up the mind are debated. Some psychologists argue that only the "higher" intellectual functions constitute mind, particularly reason and memory. In this view the emotions — love, hate, fear, and joy — are more "primitive "or subjective in nature and should be seen as different from the mind as such. Others argue that various rational and emotional states cannot be so separated, that they are of the same nature and origin, and should therefore be considered all part of it as mind.

In popular usage, "mind "is frequently synonymous with "thought": the private conversation with ourselves that we carry on "inside our heads." Thus we "make up our minds," "change our minds" or are "of two minds" about something. One of the key attributes of the mind in this sense is that it is a private sphere to which no one but the owner has access. No one else can "know our mind." They can only interpret what we consciously or unconsciously communicate.

Broadly speaking, mental faculties are the various functions of the mind, or things the mind can "do".

Thought is a mental act that allows humans to make sense of things in the world, and to represent and interpret them in ways that are significant, or which accord with their needs, attachments, goals, commitments, plans, ends, desires, etc. Thinking involves the symbolic or semiotic mediation of ideas or data, as when we form concepts, engage in problem solving, reasoning, and making decisions. Words that refer to similar concepts and processes include deliberation, cognition, ideation, discourse and imagination.

Thinking is sometimes described as a "higher" cognitive function and the analysis of thinking processes is a part of cognitive psychology. It is also deeply connected with our capacity to make and use tools; to understand cause and effect; to recognize patterns of significance; to comprehend and disclose unique contexts of experience or activity; and to respond to the world in a meaningful way.

Memory is the ability to preserve, retain, and subsequently recall, knowledge, information or experience. Although memory has traditionally been a persistent theme in philosophy, the late nineteenth and early twentieth centuries also saw the study of memory emerge as a subject of inquiry within the paradigms of cognitive psychology. In recent decades, it has become one of the pillars of a new branch of science called cognitive neuroscience, a marriage between cognitive psychology and neuroscience.

Imagination is the activity of generating or evoking novel situations, images, ideas or other qualia in the mind. It is a characteristically subjective "activity", rather than a direct or passive experience. The term is technically used in psychology for the process of reviving in the mind percepts of objects formerly given in sense perception. Since this use of the term conflicts with that of ordinary language, some psychologists have preferred to describe this process as "imaging" or "imagery" or to speak of it as "reproductive" as opposed to "productive" or "constructive" imagination. Things imagined are said to be seen in the "mind's eye". Among the many practical functions of imagination are the ability to project possible futures (or histories), to "see" things from another's perspective, and to change the way something is perceived, including to make decisions to respond to, or enact, what is imagined.

Consciousness in mammals (this includes humans) is an aspect of the mind generally thought to comprise qualities such as subjectivity, sentience, and the ability to perceive the relationship between oneself and one's environment. It is a subject of much research in philosophy of mind, psychology, neuroscience, and cognitive science. Some philosophers divide consciousness into phenomenal consciousness, which is subjective experience itself, and access consciousness, which refers to the global availability of information to processing systems in the brain. Phenomenal consciousness has many different experienced qualities, often referred to as qualia. Phenomenal consciousness is usually consciousness "of" something or "about" something, a property known as intentionality in philosophy of mind.

Mental contents are those items that are thought of as being "in" the mind, and capable of being formed and manipulated by mental processes and faculties. Examples include thoughts, concepts, memories, emotions, percepts and intentions. Philosophical theories of mental content include internalism, externalism, representationalism and intentionality.

Memetics is a theory of mental content based on an analogy with Darwinian evolution, which was originated by Richard Dawkins and Douglas Hofstadter in the 1980s. It is an evolutionary model of cultural information transfer. A meme, analogous to a gene, is an idea, belief, pattern of behaviour (etc.) "hosted" in one or more individual minds, and can reproduce itself from mind to mind. Thus what would otherwise be regarded as one individual influencing another to adopt a belief, is seen memetically as a meme reproducing itself.

In animals, the brain, or "encephalon" (Greek for "in the head"), is the control center of the central nervous system, responsible for thought. In most animals, the brain is located in the head, protected by the skull and close to the primary sensory apparatus of vision, hearing, equilibrioception, taste and olfaction. While all vertebrates have a brain, most invertebrates have either a centralized brain or collections of individual ganglia. Primitive animals such as sponges do not have a brain at all. Brains can be extremely complex. For example, the human brain contains around 86 billion neurons, each linked to as many as 10,000 others.

Understanding the relationship between the brain and the mind – mind–body problem is one of the central issues in the history of philosophy – is a challenging problem both philosophically and scientifically. There are three major philosophical schools of thought concerning the answer: dualism, materialism, and idealism. Dualism holds that the mind exists independently of the brain; materialism holds that mental phenomena are identical to neuronal phenomena; and idealism holds that only mental phenomena exist.

Through most of history many philosophers found it inconceivable that cognition could be implemented by a physical substance such as brain tissue (that is neurons and synapses). Descartes, who thought extensively about mind-brain relationships, found it possible to explain reflexes and other simple behaviors in mechanistic terms, although he did not believe that complex thought, and language in particular, could be explained by reference to the physical brain alone.

The most straightforward scientific evidence of a strong relationship between the physical brain matter and the mind is the impact physical alterations to the brain have on the mind, such as with traumatic brain injury and psychoactive drug use. Philosopher Patricia Churchland notes that this drug-mind interaction indicates an intimate connection between the brain and the mind.

In addition to the philosophical questions, the relationship between mind and brain involves a number of scientific questions, including understanding the relationship between mental activity and brain activity, the exact mechanisms by which drugs influence cognition, and the neural correlates of consciousness.

Theoretical approaches to explain how mind emerges from the brain include connectionism, computationalism and Bayesian brain.

The evolution of human intelligence refers to several theories that aim to describe how human intelligence has evolved in relation to the evolution of the human brain and the origin of language.

The timeline of human evolution spans some 7 million years, from the separation of the genus "Pan" until the emergence of behavioral modernity by 50,000 years ago. Of this timeline, the first 3 million years concern "Sahelanthropus", the following 2 million concern "Australopithecus", while the final 2 million span the history of actual "Homo" species (the Paleolithic).

Many traits of human intelligence, such as empathy, theory of mind, mourning, ritual, and the use of symbols and tools, are already apparent in great apes although in lesser sophistication than in humans.

There is a debate between supporters of the idea of a sudden emergence of intelligence, or "Great leap forward" and those of a gradual or continuum hypothesis.

Theories of the evolution of intelligence include:

Philosophy of mind is the branch of philosophy that studies the nature of the mind, mental events, mental functions, mental properties, consciousness and their relationship to the physical body. The "mind–body problem", i.e. the relationship of the mind to the body, is commonly seen as the central issue in philosophy of mind, although there are other issues concerning the nature of the mind that do not involve its relation to the physical body. José Manuel Rodriguez Delgado writes, "In present popular usage, soul and mind are not clearly differentiated and some people, more or less consciously, still feel that the soul, and perhaps the mind, may enter or leave the body as independent entities."

"Dualism" and "monism" are the two major schools of thought that attempt to resolve the mind–body problem. Dualism is the position that mind and body are in some way separate from each other. It can be traced back to Plato, Aristotle and the Nyaya, Samkhya and Yoga schools of Hindu philosophy, but it was most precisely formulated by René Descartes in the 17th century. "Substance dualists" argue that the mind is an independently existing substance, whereas "Property dualists" maintain that the mind is a group of independent properties that emerge from and cannot be reduced to the brain, but that it is not a distinct substance.

The 20th century philosopher Martin Heidegger suggested that subjective experience and activity (i.e. the "mind") cannot be made sense of in terms of Cartesian "substances" that bear "properties" at all (whether the mind itself is thought of as a distinct, separate kind of substance or not). This is because the nature of subjective, "qualitative" experience is incoherent in terms of – or semantically incommensurable with the concept of – substances that bear properties. This is a fundamentally ontological argument.

The philosopher of cognitive science Daniel Dennett, for example, argues there is no such thing as a narrative center called the "mind", but that instead there is simply a collection of sensory inputs and outputs: different kinds of "software" running in parallel. Psychologist B.F. Skinner argued that the mind is an explanatory fiction that diverts attention from environmental causes of behavior; he considered the mind a "black box" and thought that mental processes may be better conceived of as forms of covert verbal behavior.

Philosopher David Chalmers has argued that the third person approach to uncovering mind and consciousness is not effective, such as looking into other's brains or observing human conduct, but that a first person approach is necessary. Such a first person perspective indicates that the mind must be conceptualized as something distinct from the brain.

The mind has also been described as manifesting from moment to moment, one thought moment at a time as a fast flowing stream, where sense impressions and mental phenomena are constantly changing.

"Monism" is the position that mind and body are not physiologically and ontologically distinct kinds of entities. This view was first advocated in Western Philosophy by Parmenides in the 5th Century BC and was later espoused by the 17th Century rationalist Baruch Spinoza. According to Spinoza's dual-aspect theory, mind and body are two aspects of an underlying reality which he variously described as "Nature" or "God".

The most common monisms in the 20th and 21st centuries have all been variations of physicalism; these positions include behaviorism, the type identity theory, anomalous monism and functionalism.

Many modern philosophers of mind adopt either a "reductive" or "non-reductive physicalist" position, maintaining in their different ways that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, e.g. in the fields of sociobiology, computer science, evolutionary psychology and the various neurosciences. Other philosophers, however, adopt a non-physicalist position which challenges the notion that the mind is a purely physical construct.

Continued progress in neuroscience has helped to clarify many of these issues, and its findings have been taken by many to support physicalists' assertions. Nevertheless, our knowledge is incomplete, and modern philosophers of mind continue to discuss how subjective qualia and the intentional mental states can be naturally explained. Then, of course, there is the problem of Quantum Mechanics, which is best understood as a form of perspectivism.

Neuroscience studies the nervous system, the physical basis of the mind. At the systems level, neuroscientists investigate how biological neural networks form and physiologically interact to produce mental functions and content such as reflexes, multisensory integration, motor coordination, circadian rhythms, emotional responses, learning, and memory. The underlying physical basis of learning and memory is likely dynamic changes in gene expression that occur in brain neurons. Such expression changes are introduced by epigenetic mechanisms. Epigenetic regulation of gene expression ordinarily involves chemical modification of DNA or DNA-associated histone proteins. Such chemical modifications can cause long-lasting changes in gene expression. Epigenetic mechanisms employed in learning and memory include the methylation and demethylation of neuronal DNA as well as methylation, acetylation and deacetylation of neuronal histone proteins. 

At a larger scale, efforts in computational neuroscience have developed large-scale models that simulate simple, functioning brains. As of 2012, such models include the thalamus, basal ganglia, prefrontal cortex, motor cortex, and occipital cortex, and consequentially simulated brains can learn, respond to visual stimuli, coordinate motor responses, form short-term memories, and learn to respond to patterns. Currently, researchers aim to program the hippocampus and limbic system, hypothetically imbuing the simulated mind with long-term memory and crude emotions.

By contrast, affective neuroscience studies the neural mechanisms of personality, emotion, and mood primarily through experimental tasks.

Cognitive science examines the mental functions that give rise to information processing, termed cognition. These include perception, attention, working memory, long-term memory, producing and understanding language, learning, reasoning, problem solving, and decision making. Cognitive science seeks to understand thinking "in terms of representational structures in the mind and computational procedures that operate on those structures".

Psychology is the scientific study of human behavior, mental functioning, and experience. As both an academic and applied discipline, Psychology involves the scientific study of mental processes such as perception, cognition, emotion, personality, as well as environmental influences, such as social and cultural influences, and interpersonal relationships, in order to devise theories of human behavior. Psychological patterns can be understood as low cost ways of information processing. Psychology also refers to the application of such knowledge to various spheres of human activity, including problems of individuals' daily lives and the treatment of mental health problems.

Psychology differs from the other social sciences (e.g. anthropology, economics, political science, and sociology) due to its focus on experimentation at the scale of the individual, or individuals in small groups as opposed to large groups, institutions or societies. Historically, psychology differed from biology and neuroscience in that it was primarily concerned with mind rather than brain. Modern psychological science incorporates physiological and neurological processes into its conceptions of perception, cognition, behaviour, and mental disorders.

By analogy with the health of the body, one can speak metaphorically of a state of health of the mind, or mental health. Merriam-Webster defines mental health as "A state of emotional and psychological well-being in which an individual is able to use his or her cognitive and emotional capabilities, function in society, and meet the ordinary demands of everyday life." According to the World Health Organization (WHO), there is no one "official" definition of mental health. Cultural differences, subjective assessments, and competing professional theories all affect how "mental health" is defined. In general, most experts agree that "mental health" and "mental disorder" are not opposites. In other words, the absence of a recognized mental disorder is not necessarily an indicator of mental health.

One way to think about mental health is by looking at how effectively and successfully a person functions. Feeling capable and competent; being able to handle normal levels of stress, maintaining satisfying relationships, and leading an independent life; and being able to "bounce back," or recover from difficult situations, are all signs of mental health.

Psychotherapy is an interpersonal, relational intervention used by trained psychotherapists to aid clients in problems of living. This usually includes increasing individual sense of well-being and reducing subjective discomforting experience. Psychotherapists employ a range of techniques based on experiential relationship building, dialogue, communication and behavior change and that are designed to improve the mental health of a client or patient, or to improve group relationships (such as in a family). Most forms of psychotherapy use only spoken conversation, though some also use various other forms of communication such as the written word, art, drama, narrative story, or therapeutic touch. Psychotherapy occurs within a structured encounter between a trained therapist and client(s). Purposeful, theoretically based psychotherapy began in the 19th century with psychoanalysis; since then, scores of other approaches have been developed and continue to be created.

Animal cognition, or cognitive ethology, is the title given to a modern approach to the mental capacities of animals. It has developed out of comparative psychology, but has also been strongly influenced by the approach of ethology, behavioral ecology, and evolutionary psychology. Much of what used to be considered under the title of "animal intelligence" is now thought of under this heading. Animal language acquisition, attempting to discern or understand the degree to which animal cognition can be revealed by linguistics-related study, has been controversial among cognitive linguists.

 In 1950 Alan M. Turing published "Computing machinery and intelligence" in "Mind", in which he proposed that machines could be tested for intelligence using questions and answers. This process is now named the Turing Test. The term Artificial Intelligence (AI) was first used by John McCarthy who considered it to mean "the science and engineering of making intelligent machines". It can also refer to intelligence as exhibited by an artificial ("man-made", "non-natural", "manufactured") entity. AI is studied in overlapping fields of computer science, psychology, neuroscience and engineering, dealing with intelligent behavior, learning and adaptation and usually developed using customized machines or computers.

Research in AI is concerned with producing machines to automate tasks requiring intelligent behavior. Examples include control, planning and scheduling, the ability to answer diagnostic and consumer questions, handwriting, natural language, speech and facial recognition. As such, the study of AI has also become an engineering discipline, focused on providing solutions to real life problems, knowledge mining, software applications, strategy games like computer chess and other video games. One of the biggest limitations of AI is in the domain of actual machine comprehension. Consequentially natural language understanding and connectionism (where behavior of neural networks is investigated) are areas of active research and development.

The debate about the nature of the mind is relevant to the development of artificial intelligence. If the mind is indeed a thing separate from or higher than the functioning of the brain, then hypothetically it would be much more difficult to recreate within a machine, if it were possible at all. If, on the other hand, the mind is no more than the aggregated functions of the brain, then it will be possible to create a machine with a recognisable mind (though possibly only with computers much different from today's), by simple virtue of the fact that such a machine already exists in the form of the human brain.

Many religions associate spiritual qualities to the human mind. These are often tightly connected to their mythology and ideas of afterlife.

The Indian philosopher-sage Sri Aurobindo attempted to unite the Eastern and Western psychological traditions with his integral psychology, as have many philosophers and New religious movements. Judaism teaches that "moach shalit al halev", the mind rules the heart. Humans can approach the Divine intellectually, through learning and behaving according to the Divine Will as enclothed in the Torah, and use that deep logical understanding to elicit and guide emotional arousal during prayer. Christianity has tended to see the mind as distinct from the soul (Greek "nous") and sometimes further distinguished from the spirit. Western esoteric traditions sometimes refer to a mental body that exists on a plane other than the physical. Hinduism's various philosophical schools have debated whether the human soul (Sanskrit "atman") is distinct from, or identical to, "Brahman", the divine reality. Taoism sees the human being as contiguous with natural forces, and the mind as not separate from the body. Confucianism sees the mind, like the body, as inherently perfectible.

Buddhist teachings explain the moment-to-moment manifestation of the mind-stream. The components that make up the mind are known as the five aggregates (i.e., material form, feelings, perception, volition, and sensory consciousness), which arise and pass away continuously. The arising and passing of these aggregates in the present moment is described as being influenced by five causal laws: biological laws, psychological laws, physical laws, volitional laws, and universal laws. The Buddhist practice of mindfulness involves attending to this constantly changing mind-stream.

According to Buddhist philosopher Dharmakirti, the mind has two fundamental qualities: "clarity and cognizes". If something is not those two qualities, it cannot validly be called mind. "Clarity" refers to the fact that mind has no color, shape, size, location, weight, or any other physical characteristic, and "cognizes" that it functions to know or perceive objects. "Knowing" refers to the fact that mind is aware of the contents of experience, and that, in order to exist, mind must be cognizing an object. You cannot have a mind – whose function is to cognize an object – existing without cognizing an object.

Mind, in Buddhism, is also described as being "space-like" and "illusion-like". Mind is space-like in the sense that it is not physically obstructive. It has no qualities which would prevent it from existing. In Mahayana Buddhism, mind is illusion-like in the sense that it is empty of inherent existence. This does not mean it does not exist, it means that it exists in a manner that is counter to our ordinary way of misperceiving how phenomena exist, according to Buddhism. When the mind is itself cognized properly, without misperceiving its mode of existence, it appears to exist like an illusion. There is a big difference however between being "space and illusion" and being "space-like" and "illusion-like". Mind is not composed of space, it just shares some descriptive similarities to space. Mind is not an illusion, it just shares some descriptive qualities with illusions.

Buddhism posits that there is no inherent, unchanging identity (Inherent I, Inherent Me) or phenomena (Ultimate self, inherent self, Atman, Soul, Self-essence, Jiva, Ishvara, humanness essence, etc.) which is the experiencer of our experiences and the agent of our actions. In other words, human beings consist of merely a body and a mind, and nothing extra. Within the body there is no part or set of parts which is – by itself or themselves – the person. Similarly, within the mind there is no part or set of parts which are themselves "the person". A human being merely consists of five aggregates, or "skandhas" and nothing else.

In the same way, "mind" is what can be validly conceptually labelled onto our mere experience of clarity and knowing. There is something separate and apart from clarity and knowing which is "Awareness", in Buddhism. "Mind" is that part of experience the sixth sense door, which can be validly referred to as mind by the concept-term "mind". There is also not "objects out there, mind in here, and experience somewhere in-between". There is a third thing called "awareness" which exists being aware of the contents of mind and what mind cognizes. There are five senses (arising of mere experience: shapes, colors, the components of smell, components of taste, components of sound, components of touch) and mind as the sixth institution; this means, expressly, that there can be a third thing called "awareness" and a third thing called "experiencer who is aware of the experience". This awareness is deeply related to "no-self" because it does not judge the experience with craving or aversion.

Clearly, the experience arises and is known by mind, but there is a third thing calls Sati what is the "real experiencer of the experience" that sits apart from the experience and which can be aware of the experience in 4 levels. (Maha Sathipatthana Sutta.)

To be aware of these four levels one needs to cultivate equanimity toward Craving and Aversion. This is Called Vipassana which is different from the way of reacting with Craving and Aversion. This is the state of being aware and equanimous to the complete experience of here and now. This is the way of Buddhism, with regards to mind and the ultimate nature of minds (and persons).

Due to the mind–body problem, a lot of interest and debate surrounds the question of what happens to one's conscious mind as one's body dies. During brain death all brain function permanently ceases. According to some neuroscientific views which see these processes as the physical basis of mental phenomena, the mind fails to survive brain death and ceases to exist. This permanent loss of consciousness after death is sometimes called "eternal oblivion". The belief that some spiritual or incorporeal component (soul) exists and that it is preserved after death is described by the term "afterlife".

Parapsychology is a study of certain types of paranormal phenomena, or of phenomena which appear to be paranormal or not have any scientific basis , for instance, precognition, telekinesis and telepathy.

The term is based on the Greek para (beside/beyond), psyche (soul/mind), and logos (account/explanation) and was coined by psychologist Max Dessoir in or before 1889. J.B. Rhine tried to popularize "parapsychology" using fraudulent techniques as a replacement for the earlier term "psychical research", during a shift in methodologies which brought experimental methods to the study of psychic phenomena. Parapsychology is not accepted among the scientific community as science, as psychic abilities have not been demonstrated to exist. The status of parapsychology as a science has also been disputed, with many scientists regarding the discipline as pseudoscience.




</doc>
<doc id="36797" url="https://en.wikipedia.org/wiki?curid=36797" title="Occam's razor">
Occam's razor

Occam's razor (also Ockham's razor or Ocham's razor: ; or law of parsimony: ) is the problem-solving principle that states that "Entities should not be multiplied without necessity." The idea is attributed to English Franciscan friar William of Ockham ( 1287–1347), a scholastic philosopher and theologian who used a preference for simplicity to defend the idea of divine miracles. It is sometimes paraphrased by a statement like "the simplest solution is most likely the right one". Occam's razor says that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions, and it is not meant to be a way of choosing between hypotheses that make different predictions.

Similarly, in science, Occam's razor is used as an abductive heuristic in the development of theoretical models rather than as a rigorous arbiter between candidate models. In the scientific method, Occam's razor is not considered an irrefutable principle of logic or a scientific result; the preference for simplicity in the scientific method is based on the falsifiability criterion. For each accepted explanation of a phenomenon, there may be an extremely large, perhaps even incomprehensible, number of possible and more complex alternatives. Since one can always burden failing explanations with "ad hoc" hypotheses to prevent them from being falsified, simpler theories are preferable to more complex ones because they are more testable.

The phrase "Occam's razor" did not appear until a few centuries after William of Ockham's death in 1347. Libert Froidmont, in his "On Christian Philosophy of the Soul", takes credit for the phrase, speaking of "novacula occami". Ockham did not invent this principle, but the "razor"—and its association with him—may be due to the frequency and effectiveness with which he used it. Ockham stated the principle in various ways, but the most popular version, "Entities are not to be multiplied without necessity" () was formulated by the Irish Franciscan philosopher John Punch in his 1639 commentary on the works of Duns Scotus.

The origins of what has come to be known as Occam's razor are traceable to the works of earlier philosophers such as John Duns Scotus (1265–1308), Robert Grosseteste (1175–1253), Maimonides (Moses ben-Maimon, 1138–1204), and even Aristotle (384–322 BC). Aristotle writes in his "Posterior Analytics", "We may assume the superiority [other things being equal] of the demonstration which derives from fewer postulates or hypotheses." Ptolemy () stated, "We consider it a good principle to explain the phenomena by the simplest hypothesis possible."

Phrases such as "It is vain to do with more what can be done with fewer" and "A plurality is not to be posited without necessity" were commonplace in 13th-century scholastic writing. Robert Grosseteste, in "Commentary on" [Aristotle's] "the Posterior Analytics Books" ("Commentarius in Posteriorum Analyticorum Libros") (c. 1217–1220), declares: "That is better and more valuable which requires fewer, other circumstances being equal... For if one thing were demonstrated from many and another thing from fewer equally known premises, clearly that is better which is from fewer because it makes us know quickly, just as a universal demonstration is better than particular because it produces knowledge from fewer premises. Similarly in natural science, in moral science, and in metaphysics the best is that which needs no premises and the better that which needs the fewer, other circumstances being equal."

The "Summa Theologica" of Thomas Aquinas (1225–1274) states that "it is superfluous to suppose that what can be accounted for by a few principles has been produced by many." Aquinas uses this principle to construct an objection to God's existence, an objection that he in turn answers and refutes generally (cf. "quinque viae"), and specifically, through an argument based on causality. Hence, Aquinas acknowledges the principle that today is known as Occam's razor, but prefers causal explanations to other simple explanations (cf. also Correlation does not imply causation).

William of Ockham ("circa" 1287–1347) was an English Franciscan friar and theologian, an influential medieval philosopher and a nominalist. His popular fame as a great logician rests chiefly on the maxim attributed to him and known as Occam's razor. The term "razor" refers to distinguishing between two hypotheses either by "shaving away" unnecessary assumptions or cutting apart two similar conclusions.

While it has been claimed that Occam's razor is not found in any of William's writings, one can cite statements such as ("Plurality must never be posited without necessity"), which occurs in his theological work on the "Sentences of Peter Lombard" ("Quaestiones et decisiones in quattuor libros Sententiarum Petri Lombardi"; ed. Lugd., 1495, i, dist. 27, qu. 2, K).

Nevertheless, the precise words sometimes attributed to William of Ockham, (Entities must not be multiplied beyond necessity), are absent in his extant works; this particular phrasing comes from John Punch, who described the principle as a "common axiom" ("axioma vulgare") of the Scholastics. William of Ockham's contribution seems to restrict the operation of this principle in matters pertaining to miracles and God's power; so, in the Eucharist, a plurality of miracles is possible, simply because it pleases God.

This principle is sometimes phrased as ("Plurality should not be posited without necessity"). In his "Summa Totius Logicae", i. 12, William of Ockham cites the principle of economy, ("It is futile to do with more things that which can be done with fewer"; Thorburn, 1918, pp. 352–53; Kneale and Kneale, 1962, p. 243.)

To quote Isaac Newton, "We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances. Therefore, to the same natural effects we must, as far as possible, assign the same causes."

Bertrand Russell offers a particular version of Occam's razor: "Whenever possible, substitute constructions out of known entities for inferences to unknown entities."

Around 1960, Ray Solomonoff founded the theory of universal inductive inference, the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. This theory is a mathematical formalization of Occam's razor.

Another technical approach to Occam's razor is ontological parsimony. Parsimony means spareness and is also referred to as the Rule of Simplicity. This is considered a strong version of Occam's razor. A variation used in medicine is called the "Zebra": a physician should reject an exotic medical diagnosis when a more commonplace explanation is more likely, derived from Theodore Woodward's dictum "When you hear hoofbeats, think of horses not zebras".

Ernst Mach formulated the stronger version of Occam's razor into physics, which he called the Principle of Economy stating: "Scientists must use the simplest means of arriving at their results and exclude everything not perceived by the senses."

This principle goes back at least as far as Aristotle, who wrote "Nature operates in the shortest way possible." The idea of parsimony or simplicity in deciding between theories, though not the intent of the original expression of Occam's razor, has been assimilated into our culture as the widespread layman's formulation that "the simplest explanation is usually the correct one."

Prior to the 20th century, it was a commonly held belief that nature itself was simple and that simpler hypotheses about nature were thus more likely to be true. This notion was deeply rooted in the aesthetic value that simplicity holds for human thought and the justifications presented for it often drew from theology. Thomas Aquinas made this argument in the 13th century, writing, "If a thing can be done adequately by means of one, it is superfluous to do it by means of several; for we observe that nature does not employ two instruments [if] one suffices."

Beginning in the 20th century, epistemological justifications based on induction, logic, pragmatism, and especially probability theory have become more popular among philosophers.

Occam's razor has gained strong empirical support in helping to converge on better theories (see "Applications" section below for some examples).

In the related concept of overfitting, excessively complex models are affected by statistical noise (a problem also known as the bias-variance trade-off), whereas simpler models may capture the underlying structure better and may thus have better predictive performance. It is, however, often difficult to deduce which part of the data is noise (cf. model selection, test set, minimum description length, Bayesian inference, etc.).

The razor's statement that "other things being equal, simpler explanations are generally better than more complex ones" is amenable to empirical testing. Another interpretation of the razor's statement would be that "simpler hypotheses are generally better than the complex ones". The procedure to test the former interpretation would compare the track records of simple and comparatively complex explanations. If one accepts the first interpretation, the validity of Occam's razor as a tool would then have to be rejected if the more complex explanations were more often correct than the less complex ones (while the converse would lend support to its use). If the latter interpretation is accepted, the validity of Occam's razor as a tool could possibly be accepted if the simpler hypotheses led to correct conclusions more often than not.

Some increases in complexity are sometimes necessary, so there remains a justified general bias toward the simpler of two competing explanations. To understand why, consider that for each accepted explanation of a phenomenon, there is always an infinite number of possible, more complex, and ultimately incorrect, alternatives. This is so because one can always burden a failing explanation with an ad hoc hypothesis. Ad hoc hypotheses are justifications that prevent theories from being falsified. Even other empirical criteria, such as consilience, can never truly eliminate such explanations as competition. Each true explanation, then, may have had many alternatives that were simpler and false, but also an infinite number of alternatives that were more complex and false. But if an alternative ad hoc hypothesis were indeed justifiable, its implicit conclusions would be empirically verifiable. On a commonly accepted repeatability principle, these alternative theories have never been observed and continue to escape observation. In addition, one does not say an explanation is true if it has not withstood this principle.

Put another way, any new, and even more complex, theory can still possibly be true. For example, if an individual makes supernatural claims that leprechauns were responsible for breaking a vase, the simpler explanation would be that he is mistaken, but ongoing ad hoc justifications (e.g. "... and that's not me on the film; they tampered with that, too") successfully prevent outright disproval. This endless supply of elaborate competing explanations, called saving hypotheses, cannot be ruled out – except by using Occam's razor. A study of the predictive validity of Occam's razor found 32 published papers that included 97 comparisons of economic forecasts from simple and complex forecasting methods. None of the papers provided a balance of evidence that complexity of method improved forecast accuracy. In the 25 papers with quantitative comparisons, complexity increased forecast errors by an average of 27 percent.

One justification of Occam's razor is a direct result of basic probability theory. By definition, all assumptions introduce possibilities for error; if an assumption does not improve the accuracy of a theory, its only effect is to increase the probability that the overall theory is wrong.

There have also been other attempts to derive Occam's razor from probability theory, including notable attempts made by Harold Jeffreys and E. T. Jaynes. The probabilistic (Bayesian) basis for Occam's razor is elaborated by David J. C. MacKay in chapter 28 of his book "Information Theory, Inference, and Learning Algorithms", where he emphasizes that a prior bias in favour of simpler models is not required.

William H. Jefferys and James O. Berger (1991) generalize and quantify the original formulation's "assumptions" concept as the degree to which a proposition is unnecessarily accommodating to possible observable data. They state, "A hypothesis with fewer adjustable parameters will automatically have an enhanced posterior probability, due to the fact that the predictions it makes are sharp." The model they propose balances the precision of a theory's predictions against their sharpness—preferring theories that sharply make correct predictions over theories that accommodate a wide range of other possible results. This, again, reflects the mathematical relationship between key concepts in Bayesian inference (namely marginal probability, conditional probability, and posterior probability).

The bias–variance tradeoff is a framework that incorporates the Occam's razor principal in its balance between overfitting (i.e. variance minimization) and underfitting (i.e. bias minimization).

Karl Popper argues that a preference for simple theories need not appeal to practical or aesthetic considerations. Our preference for simplicity may be justified by its falsifiability criterion: we prefer simpler theories to more complex ones "because their empirical content is greater; and because they are better testable". The idea here is that a simple theory applies to more cases than a more complex one, and is thus more easily falsifiable. This is again comparing a simple theory to a more complex theory where both explain the data equally well.

The philosopher of science Elliott Sober once argued along the same lines as Popper, tying simplicity with "informativeness": The simplest theory is the more informative, in the sense that it requires less information to a question. He has since rejected this account of simplicity, purportedly because it fails to provide an epistemic justification for simplicity. He now believes that simplicity considerations (and considerations of parsimony in particular) do not count unless they reflect something more fundamental. Philosophers, he suggests, may have made the error of hypostatizing simplicity (i.e., endowed it with a "sui generis" existence), when it has meaning only when embedded in a specific context (Sober 1992). If we fail to justify simplicity considerations on the basis of the context in which we use them, we may have no non-circular justification: "Just as the question 'why be rational?' may have no non-circular answer, the same may be true of the question 'why should simplicity be considered in evaluating the plausibility of hypotheses?'"

Richard Swinburne argues for simplicity on logical grounds:

According to Swinburne, since our choice of theory cannot be determined by data (see Underdetermination and Duhem-Quine thesis), we must rely on some criterion to determine which theory to use. Since it is absurd to have no logical method for settling on one hypothesis amongst an infinite number of equally data-compliant hypotheses, we should choose the simplest theory: "Either science is irrational [in the way it judges theories and predictions probable] or the principle of simplicity is a fundamental synthetic a priori truth." (Swinburne 1997).

From the "Tractatus Logico-Philosophicus":


and on the related concept of "simplicity":


In science, Occam's razor is used as a heuristic to guide scientists in developing theoretical models rather than as an arbiter between published models. In physics, parsimony was an important heuristic in Albert Einstein's formulation of special relativity, in the development and application of the principle of least action by Pierre Louis Maupertuis and Leonhard Euler, and in the development of quantum mechanics by Max Planck, Werner Heisenberg and Louis de Broglie.

In chemistry, Occam's razor is often an important heuristic when developing a model of a reaction mechanism. Although it is useful as a heuristic in developing models of reaction mechanisms, it has been shown to fail as a criterion for selecting among some selected published models. In this context, Einstein himself expressed caution when he formulated Einstein's Constraint: "It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience". An often-quoted version of this constraint (which cannot be verified as posited by Einstein himself) says "Everything should be kept as simple as possible, but not simpler."

In the scientific method, parsimony is an epistemological, metaphysical or heuristic preference, not an irrefutable principle of logic or a scientific result. As a logical principle, Occam's razor would demand that scientists accept the simplest possible theoretical explanation for existing data. However, science has shown repeatedly that future data often support more complex theories than do existing data. Science prefers the simplest explanation that is consistent with the data available at a given time, but the simplest explanation may be ruled out as new data become available. That is, science is open to the possibility that future experiments might support more complex theories than demanded by current data and is more interested in designing experiments to discriminate between competing theories than favoring one theory over another based merely on philosophical principles.

When scientists use the idea of parsimony, it has meaning only in a very specific context of inquiry. Several background assumptions are required for parsimony to connect with plausibility in a particular research problem. The reasonableness of parsimony in one research context may have nothing to do with its reasonableness in another. It is a mistake to think that there is a single global principle that spans diverse subject matter.

It has been suggested that Occam's razor is a widely accepted example of extraevidential consideration, even though it is entirely a metaphysical assumption. There is little empirical evidence that the world is actually simple or that simple accounts are more likely to be true than complex ones.

Most of the time, Occam's razor is a conservative tool, cutting out "crazy, complicated constructions" and assuring "that hypotheses are grounded in the science of the day", thus yielding "normal" science: models of explanation and prediction. There are, however, notable exceptions where Occam's razor turns a conservative scientist into a reluctant revolutionary. For example, Max Planck interpolated between the Wien and Jeans radiation laws and used Occam's razor logic to formulate the quantum hypothesis, even resisting that hypothesis as it became more obvious that it was correct.

Appeals to simplicity were used to argue against the phenomena of meteorites, ball lightning, continental drift, and reverse transcriptase. One can argue for atomic building blocks for matter, because it provides a simpler explanation for the observed reversibility of both mixing and chemical reactions as simple separation and rearrangements of atomic building blocks. At the time, however, the atomic theory was considered more complex because it implied the existence of invisible particles that had not been directly detected. Ernst Mach and the logical positivists rejected John Dalton's atomic theory until the reality of atoms was more evident in Brownian motion, as shown by Albert Einstein.

In the same way, postulating the aether is more complex than transmission of light through a vacuum. At the time, however, all known waves propagated through a physical medium, and it seemed simpler to postulate the existence of a medium than to theorize about wave propagation without a medium. Likewise, Newton's idea of light particles seemed simpler than Christiaan Huygens's idea of waves, so many favored it. In this case, as it turned out, neither the wave—nor the particle—explanation alone suffices, as light behaves like waves and like particles.

Three axioms presupposed by the scientific method are realism (the existence of objective reality), the existence of natural laws, and the constancy of natural law. Rather than depend on provability of these axioms, science depends on the fact that they have not been objectively falsified. Occam's razor and parsimony support, but do not prove, these axioms of science. The general principle of science is that theories (or models) of natural law must be consistent with repeatable experimental observations. This ultimate arbiter (selection criterion) rests upon the axioms mentioned above.

There are examples where Occam's razor would have favored the wrong theory given the available data. Simplicity principles are useful philosophical preferences for choosing a more likely theory from among several possibilities that are all consistent with available data. A single instance of Occam's razor favoring a wrong theory falsifies the razor as a general principle. Michael Lee and others provide cases in which a parsimonious approach does not guarantee a correct conclusion and, if based on incorrect working hypotheses or interpretations of incomplete data, may even strongly support a false conclusion.

If multiple models of natural law make exactly the same testable predictions, they are equivalent and there is no need for parsimony to choose a preferred one. For example, Newtonian, Hamiltonian and Lagrangian classical mechanics are equivalent. Physicists have no interest in using Occam's razor to say the other two are wrong. Likewise, there is no demand for simplicity principles to arbitrate between wave and matrix formulations of quantum mechanics. Science often does not demand arbitration or selection criteria between models that make the same testable predictions.

Biologists or philosophers of biology use Occam's razor in either of two contexts both in evolutionary biology: the units of selection controversy and systematics. George C. Williams in his book "Adaptation and Natural Selection" (1966) argues that the best way to explain altruism among animals is based on low-level (i.e., individual) selection as opposed to high-level group selection. Altruism is defined by some evolutionary biologists (e.g., R. Alexander, 1987; W. D. Hamilton, 1964) as behavior that is beneficial to others (or to the group) at a cost to the individual, and many posit individual selection as the mechanism that explains altruism solely in terms of the behaviors of individual organisms acting in their own self-interest (or in the interest of their genes, via kin selection). Williams was arguing against the perspective of others who propose selection at the level of the group as an evolutionary mechanism that selects for altruistic traits (e.g., D. S. Wilson & E. O. Wilson, 2007). The basis for Williams' contention is that of the two, individual selection is the more parsimonious theory. In doing so he is invoking a variant of Occam's razor known as Morgan's Canon: "In no case is an animal activity to be interpreted in terms of higher psychological processes, if it can be fairly interpreted in terms of processes which stand lower in the scale of psychological evolution and development." (Morgan 1903).

However, more recent biological analyses, such as Richard Dawkins' "The Selfish Gene", have contended that Morgan's Canon is not the simplest and most basic explanation. Dawkins argues the way evolution works is that the genes propagated in most copies end up determining the development of that particular species, i.e., natural selection turns out to select specific genes, and this is really the fundamental underlying principle that automatically gives individual and group selection as emergent features of evolution.

Zoology provides an example. Muskoxen, when threatened by wolves, form a circle with the males on the outside and the females and young on the inside. This is an example of a behavior by the males that seems to be altruistic. The behavior is disadvantageous to them individually but beneficial to the group as a whole and was thus seen by some to support the group selection theory. Another interpretation is kin selection: if the males are protecting their offspring, they are protecting copies of their own alleles. Engaging in this behavior would be favored by individual selection if the cost to the male musk ox is less than half of the benefit received by his calf – which could easily be the case if wolves have an easier time killing calves than adult males. It could also be the case that male musk oxen would be individually less likely to be killed by wolves if they stood in a circle with their horns pointing out, regardless of whether they were protecting the females and offspring. That would be an example of regular natural selection – a phenomenon called "the selfish herd".

Systematics is the branch of biology that attempts to establish patterns of genealogical relationship among biological taxa. It is also concerned with their classification. There are three primary camps in systematics: cladists, pheneticists, and evolutionary taxonomists. The cladists hold that genealogy alone should determine classification, pheneticists contend that overall similarity is the determining criterion, while evolutionary taxonomists say that both genealogy and similarity count in classification.

It is among the cladists that Occam's razor is to be found, although their term for it is "cladistic parsimony". Cladistic parsimony (or maximum parsimony) is a method of phylogenetic inference in the construction of types of phylogenetic trees (more specifically, cladograms). Cladograms are branching, tree-like structures used to represent hypotheses of relative degree of relationship, based on shared, derived character states. Cladistic parsimony is used to select as the preferred hypothesis of relationships the cladogram that requires the fewest implied character state transformations. Critics of the cladistic approach often observe that for some types of tree, parsimony consistently produces the wrong results, regardless of how much data is collected (this is called statistical inconsistency, or long branch attraction). However, this criticism is also potentially true for any type of phylogenetic inference, unless the model used to estimate the tree reflects the way that evolution actually happened. Because this information is not empirically accessible, the criticism of statistical inconsistency against parsimony holds no force. For a book-length treatment of cladistic parsimony, see Elliott Sober's "Reconstructing the Past: Parsimony, Evolution, and Inference" (1988). For a discussion of both uses of Occam's razor in biology, see Sober's article "Let's Razor Ockham's Razor" (1990).

Other methods for inferring evolutionary relationships use parsimony in a more traditional way. Likelihood methods for phylogeny use parsimony as they do for all likelihood tests, with hypotheses requiring few differing parameters (i.e., numbers of different rates of character change or different frequencies of character state transitions) being treated as null hypotheses relative to hypotheses requiring many differing parameters. Thus, complex hypotheses must predict data much better than do simple hypotheses before researchers reject the simple hypotheses. Recent advances employ information theory, a close cousin of likelihood, which uses Occam's razor in the same way.

Francis Crick has commented on potential limitations of Occam's razor in biology. He advances the argument that because biological systems are the products of (an ongoing) natural selection, the mechanisms are not necessarily optimal in an obvious sense. He cautions: "While Ockham's razor is a useful tool in the physical sciences, it can be a very dangerous implement in biology. It is thus very rash to use simplicity and elegance as a guide in biological research."

In biogeography, parsimony is used to infer ancient migrations of species or populations by observing the geographic distribution and relationships of existing organisms. Given the phylogenetic tree, ancestral migrations are inferred to be those that require the minimum amount of total movement.

In the philosophy of religion, Occam's razor is sometimes applied to the existence of God. William of Ockham himself was a Christian. He believed in God, and in the authority of Scripture; he writes that "nothing ought to be posited without a reason given, unless it is self-evident (literally, known through itself) or known by experience or proved by the authority of Sacred Scripture." Ockham believed that an explanation has no sufficient basis in reality when it does not harmonize with reason, experience, or the Bible. However, unlike many theologians of his time, Ockham did not believe God could be logically proven with arguments. To Ockham, science was a matter of discovery, but theology was a matter of revelation and faith. He states: "only faith gives us access to theological truths. The ways of God are not open to reason, for God has freely chosen to create a world and establish a way of salvation within it apart from any necessary laws that human logic or rationality can uncover."

St. Thomas Aquinas, in the "Summa Theologica", uses a formulation of Occam's razor to construct an objection to the idea that God exists, which he refutes directly with a counterargument:

Further, it is superfluous to suppose that what can be accounted for by a few principles has been produced by many. But it seems that everything we see in the world can be accounted for by other principles, supposing God did not exist. For all natural things can be reduced to one principle which is nature; and all voluntary things can be reduced to one principle which is human reason, or will. Therefore there is no need to suppose God's existence.

In turn, Aquinas answers this with the "quinque viae", and addresses the particular objection above with the following answer:

Since nature works for a determinate end under the direction of a higher agent, whatever is done by nature must needs be traced back to God, as to its first cause. So also whatever is done voluntarily must also be traced back to some higher cause other than human reason or will, since these can change or fail; for all things that are changeable and capable of defect must be traced back to an immovable and self-necessary first principle, as was shown in the body of the Article.

Rather than argue for the necessity of a god, some theists base their belief upon grounds independent of, or prior to, reason, making Occam's razor irrelevant. This was the stance of Søren Kierkegaard, who viewed belief in God as a leap of faith that sometimes directly opposed reason. This is also the doctrine of Gordon Clark's presuppositional apologetics, with the exception that Clark never thought the leap of faith was contrary to reason (see also Fideism).

Various arguments in favor of God establish God as a useful or even necessary assumption. Contrastingly some anti-theists hold firmly to the belief that assuming the existence of God introduces unnecessary complexity (Schmitt 2005, e.g., the Ultimate Boeing 747 gambit).

Another application of the principle is to be found in the work of George Berkeley (1685–1753). Berkeley was an idealist who believed that all of reality could be explained in terms of the mind alone. He invoked Occam's razor against materialism, stating that matter was not required by his metaphysic and was thus eliminable. One potential problem with this belief is that it's possible, given Berkeley's position, to find solipsism itself more in line with the razor than a God-mediated world beyond a single thinker.

Occam's razor may also be recognized in the apocryphal story about an exchange between Pierre-Simon Laplace and Napoleon. It is said that in praising Laplace for one of his recent publications, the emperor asked how it was that the name of God, which featured so frequently in the writings of Lagrange, appeared nowhere in Laplace's. At that, he is said to have replied, "It's because I had no need of that hypothesis." Though some point to this story as illustrating Laplace's atheism, more careful consideration suggests that he may instead have intended merely to illustrate the power of methodological naturalism, or even simply that the fewer logical premises one assumes, the stronger is one's conclusion.

In his article "Sensations and Brain Processes" (1959), J. J. C. Smart invoked Occam's razor with the aim to justify his preference of the mind-brain identity theory over spirit-body dualism. Dualists state that there are two kinds of substances in the universe: physical (including the body) and spiritual, which is non-physical. In contrast, identity theorists state that everything is physical, including consciousness, and that there is nothing nonphysical. Though it is impossible to appreciate the spiritual when limiting oneself to the physical, Smart maintained that identity theory explains all phenomena by assuming only a physical reality. Subsequently, Smart has been severely criticized for his use (or misuse) of Occam's razor and ultimately retracted his advocacy of it in this context. Paul Churchland (1984) states that by itself Occam's razor is inconclusive regarding duality. In a similar way, Dale Jacquette (1994) stated that Occam's razor has been used in attempts to justify eliminativism and reductionism in the philosophy of mind. Eliminativism is the thesis that the ontology of folk psychology including such entities as "pain", "joy", "desire", "fear", etc., are eliminable in favor of an ontology of a completed neuroscience.

In penal theory and the philosophy of punishment, parsimony refers specifically to taking care in the distribution of punishment in order to avoid excessive punishment. In the utilitarian approach to the philosophy of punishment, Jeremy Bentham's "parsimony principle" states that any punishment greater than is required to achieve its end is unjust. The concept is related but not identical to the legal concept of proportionality. Parsimony is a key consideration of the modern restorative justice, and is a component of utilitarian approaches to punishment, as well as the prison abolition movement. Bentham believed that true parsimony would require punishment to be individualised to take account of the sensibility of the individual—an individual more sensitive to punishment should be given a proportionately lesser one, since otherwise needless pain would be inflicted. Later utilitarian writers have tended to abandon this idea, in large part due to the impracticality of determining each alleged criminal's relative sensitivity to specific punishments.

Marcus Hutter's universal artificial intelligence builds upon Solomonoff's mathematical formalization of the razor to calculate the expected value of an action.

There are various papers in scholarly journals deriving formal versions of Occam's razor from probability theory, applying it in statistical inference, and using it to come up with criteria for penalizing complexity in statistical inference. Papers have suggested a connection between Occam's razor and Kolmogorov complexity.

One of the problems with the original formulation of the razor is that it only applies to models with the same explanatory power (i.e., it only tells us to prefer the simplest of equally good models). A more general form of the razor can be derived from Bayesian model comparison, which is based on Bayes factors and can be used to compare models that don't fit the observations equally well. These methods can sometimes optimally balance the complexity and power of a model. Generally, the exact Occam factor is intractable, but approximations such as Akaike information criterion, Bayesian information criterion, Variational Bayesian methods, false discovery rate, and Laplace's method are used. Many artificial intelligence researchers are now employing such techniques, for instance through work on Occam Learning or more generally on the Free energy principle.

Statistical versions of Occam's razor have a more rigorous formulation than what philosophical discussions produce. In particular, they must have a specific definition of the term "simplicity", and that definition can vary. For example, in the Kolmogorov–Chaitin minimum description length approach, the subject must pick a Turing machine whose operations describe the basic operations "believed" to represent "simplicity" by the subject. However, one could always choose a Turing machine with a simple operation that happened to construct one's entire theory and would hence score highly under the razor. This has led to two opposing camps: one that believes Occam's razor is objective, and one that believes it is subjective.

The minimum instruction set of a universal Turing machine requires approximately the same length description across different formulations, and is small compared to the Kolmogorov complexity of most practical theories. Marcus Hutter has used this consistency to define a "natural" Turing machine of small size as the proper basis for excluding arbitrarily complex instruction sets in the formulation of razors. Describing the program for the universal program as the "hypothesis", and the representation of the evidence as program data, it has been formally proven under Zermelo–Fraenkel set theory that "the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized." Interpreting this as minimising the total length of a two-part message encoding model followed by data given model gives us the minimum message length (MML) principle.

One possible conclusion from mixing the concepts of Kolmogorov complexity and Occam's razor is that an ideal data compressor would also be a scientific explanation/formulation generator. Some attempts have been made to re-derive known laws from considerations of simplicity or compressibility.

According to Jürgen Schmidhuber, the appropriate mathematical theory of Occam's razor already exists, namely, Solomonoff's theory of optimal inductive inference and its extensions. See discussions in David L. Dowe's "Foreword re C. S. Wallace" for the subtle distinctions between the algorithmic probability work of Solomonoff and the MML work of Chris Wallace, and see Dowe's "MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness" both for such discussions and for (in section 4) discussions of MML and Occam's razor. For a specific example of MML as Occam's razor in the problem of decision tree induction, see Dowe and Needham's "Message Length as an Effective Ockham's Razor in Decision Tree Induction".

Occam's razor is not an embargo against the positing of any kind of entity, or a recommendation of the simplest theory come what may. Occam's razor is used to adjudicate between theories that have already passed "theoretical scrutiny" tests and are equally well-supported by evidence. Furthermore, it may be used to prioritize empirical testing between two equally plausible but unequally testable hypotheses; thereby minimizing costs and wastes while increasing chances of falsification of the simpler-to-test hypothesis.

Another contentious aspect of the razor is that a theory can become more complex in terms of its structure (or syntax), while its ontology (or semantics) becomes simpler, or vice versa. Quine, in a discussion on definition, referred to these two perspectives as "economy of practical expression" and "economy in grammar and vocabulary", respectively.

Galileo Galilei lampooned the "misuse" of Occam's razor in his "Dialogue". The principle is represented in the dialogue by Simplicio. The telling point that Galileo presented ironically was that if one really wanted to start from a small number of entities, one could always consider the letters of the alphabet as the fundamental entities, since one could construct the whole of human knowledge out of them.

Occam's razor has met some opposition from people who have considered it too extreme or rash. Walter Chatton (c. 1290–1343) was a contemporary of William of Ockham who took exception to Occam's razor and Ockham's use of it. In response he devised his own "anti-razor": "If three things are not enough to verify an affirmative proposition about things, a fourth must be added, and so on." Although there have been a number of philosophers who have formulated similar anti-razors since Chatton's time, no one anti-razor has perpetuated in as much notability as Chatton's anti-razor, although this could be the case of the Late Renaissance Italian motto of unknown attribution ("Even if it is not true, it is well conceived") when referred to a particularly artful explanation.

Anti-razors have also been created by Gottfried Wilhelm Leibniz (1646–1716), Immanuel Kant (1724–1804), and Karl Menger (1902–1985). Leibniz's version took the form of a principle of plenitude, as Arthur Lovejoy has called it: the idea being that God created the most varied and populous of possible worlds. Kant felt a need to moderate the effects of Occam's razor and thus created his own counter-razor: "The variety of beings should not rashly be diminished."

Karl Menger found mathematicians to be too parsimonious with regard to variables, so he formulated his Law Against Miserliness, which took one of two forms: "Entities must not be reduced to the point of inadequacy" and "It is vain to do with fewer what requires more." A less serious but (some might say) even more extremist anti-razor is 'Pataphysics, the "science of imaginary solutions" developed by Alfred Jarry (1873–1907). Perhaps the ultimate in anti-reductionism, "'Pataphysics seeks no less than to view each event in the universe as completely unique, subject to no laws but its own." Variations on this theme were subsequently explored by the Argentine writer Jorge Luis Borges in his story/mock-essay "Tlön, Uqbar, Orbis Tertius". There is also Crabtree's Bludgeon, which cynically states that "[n]o set of mutually inconsistent observations can exist for which some human intellect cannot conceive a coherent explanation, however complicated."





</doc>
<doc id="276872" url="https://en.wikipedia.org/wiki?curid=276872" title="Critical thinking">
Critical thinking

Critical thinking is the analysis of facts to form a judgment. The subject is complex, and several different definitions exist, which generally include the rational, skeptical, unbiased analysis, or evaluation of factual evidence. Critical thinking is self-directed, self-disciplined, self-monitored, and self-corrective thinking. It presupposes assent to rigorous standards of excellence and mindful command of their use. It entails effective communication and problem-solving abilities as well as a commitment to overcome native egocentrism and sociocentrism.

The earliest records of critical thinking are the teachings of Socrates recorded by Plato. These included a part in Plato's early dialogues, where Socrates engages with one or more interlocutors on the issue of ethics such as question whether it was right for Socrates to escape from prison. The philosopher considered and reflected on this question and came to the conclusion that escape violates all the things that he holds higher than himself: the laws of Athens and the guiding voice that Socrates claims to hear.

Socrates established the fact that one cannot depend upon those in "authority" to have sound knowledge and insight. He demonstrated that persons may have power and high position and yet be deeply confused and irrational. Socrates maintained that for an individual to have a good life or to have one that is worth living, he must be critical questioner or must have an interrogative soul. He established the importance of asking deep questions that probe profoundly into thinking before we accept ideas as worthy of belief.

Socrates established the importance of "seeking evidence, closely examining reasoning and assumptions, analyzing basic concepts, and tracing out implications not only of what is said but of what is done as well." His method of questioning is now known as "Socratic questioning" and is the best known critical thinking teaching strategy. In his mode of questioning, Socrates highlighted the need for thinking for clarity and logical consistency. He asked people questions to reveal their irrational thinking or lack of reliable knowledge. Socrates demonstrated that having authority does not ensure accurate knowledge. He established the method of questioning beliefs, closely inspecting assumptions and relying on evidence and sound rationale. Plato recorded Socrates' teachings and carried on the tradition of critical thinking. Aristotle and subsequent Greek skeptics refined Socrates' teachings, using systematic thinking and asking questions to ascertain the true nature of reality beyond the way things appear from a glance.

Socrates set the agenda for the tradition of critical thinking, namely, to reflectively question common beliefs and explanations, carefully distinguishing beliefs that are reasonable and logical from those that—however appealing to our native egocentrism, however much they serve our vested interests, however comfortable or comforting they may be—lack adequate evidence or rational foundation to warrant belief.

Critical thinking was described by Richard W. Paul as a movement in two waves (1994). The "first wave" of critical thinking is often referred to as
a 'critical analysis' that is clear, rational thinking involving critique. Its details vary amongst those who define it. According to Barry K. Beyer (1995), critical thinking means making clear, reasoned judgments. During the process of critical thinking, ideas should be reasoned, well thought out, and judged. The U.S. National Council for Excellence in Critical Thinking defines critical thinking as the "<nowiki>intellectually disciplined process of actively and skillfully conceptualizing, applying, analyzing, synthesizing, or evaluating information gathered from, or generated by, observation, experience, reflection, reasoning, or communication, as a guide to belief and action.</nowiki>"

In the term "critical thinking", the word "critical", (Grk. κριτικός = "kritikos" = "critic") derives from the word "critic" and implies a critique; it identifies the intellectual capacity and the means "of judging", "of judgement", "for judging", and of being "able to discern". The intellectual roots of critical thinking are as ancient as its etymology, traceable, ultimately, to the teaching practice and vision of Socrates 2,500 years ago who discovered by a method of probing questioning that people could not rationally justify their confident claims to knowledge.

Traditionally, critical thinking has been variously defined as follows:

Contemporary critical thinking scholars have expanded these traditional definitions to include qualities, concepts, and processes such as creativity, imagination, discovery, reflection, empathy, connecting knowing, feminist theory, subjectivity, ambiguity, and inconclusiveness. Some definitions of critical thinking exclude these subjective practices.


The ability to reason logically is a fundamental skill of rational agents, hence the study of the form of correct argumentation is relevant to the study of critical thinking. It is, therefore, linked to the field of logic, which is concerned with the analysis of arguments, including the appraisal of their correctness or incorrectness. Another interpretation holds that in the field of epistemology, critical thinking is considered as the logically correct thinking, which allows the differentiation between the logically true and the logically false statements. 

"First wave" logical thinking consists of understanding the connections between two concepts or points in thought. It follows a philosophy where the thinker is removed from the train of thought, while the connections and its analysis are devoid of any bias. Kerry S. Walters describes this ideology in his essay "Beyond Logicism in Critical Thinking" as follows: "A logistic approach to critical thinking conveys the message to students that thinking is legitimate only when it conforms to the procedures of informal (and, to a lesser extent, formal) logic and that the good thinker necessarily aims for styles of examination and appraisal that are analytical, abstract, universal, and objective. This model of thinking has become so entrenched in conventional academic wisdom that many educators accept it as canon". The adoption of these principals parallels themselves with the increasing reliance on a quantitative understanding of the world.

In the ‘second wave’ of critical thinking, as defined by Walters, many authors moved away from the logocentric mode of critical thinking that the ‘first wave’ privileged, especially in institutions of higher learning. Scholars began to take a more inclusive view of what constituted critical thinking, but rationality and logic are still widely accepted in many circles as the primary examples of critical thinking. Walters summarizes logicism as "the unwarranted assumption that good thinking is reducible to logical thinking".

There are three types of logical reasoning. Informally, two kinds of logical reasoning can be distinguished in addition to formal deduction, which are induction and abduction.

 e.g., X is human and all humans have a face, so X has a face.

e.g. The sum of even integers is even.

Let formula_1 then formula_2 are even by definition. formula_3, which is even; so summing two even numbers results in an even number.

 e.g., I observe sheep in a field, and they appear white from my viewing angle, so sheep are white.
 Contrast with the deductive statement:"Some sheep are white on at least one side."
Kerry S. Walters, professor emeritus of philosophy at Gettysburg College, argues that rationality demands more than just logical or traditional methods of problem solving and analysis or what he calls the "calculus of justification" but also considers "cognitive acts such as imagination, conceptual creativity, intuition and insight" (p. 63). These "functions" are focused on discovery, on more abstract processes instead of linear, rules-based approaches to problem-solving. The linear and non-sequential mind must both be engaged in the rational mind.

The ability to critically analyze an argument – to dissect structure and components, thesis and reasons – is essential. But so is the ability to be flexible and consider non-traditional alternatives and perspectives. These complementary functions are what allow for critical thinking to be a practice encompassing imagination and intuition in cooperation with traditional modes of deductive inquiry.

The list of core critical thinking skills includes observation, interpretation, analysis, inference, evaluation, explanation, and metacognition. According to Reynolds (2011), an individual or group engaged in a strong way of critical thinking gives due consideration to establish for instance:

In addition to possessing strong critical-thinking skills, one must be disposed to engage problems and decisions using those skills. Critical thinking employs not only logic but broad intellectual criteria such as clarity, credibility, accuracy, precision, relevance, depth, breadth, significance, and fairness.

Critical thinking calls for the ability to:

In sum:

"A persistent effort to examine any belief or supposed form of knowledge in the light of the evidence that supports or refutes it and the further conclusions to which it tends."

The habits of mind that characterize a person strongly disposed toward critical thinking include a desire to follow reason and evidence wherever they may lead, a systematic approach to problem solving, inquisitiveness, even-handedness, and confidence in reasoning.

According to a definition analysis by Kompf & Bond (2001), critical thinking involves problem solving, decision making, metacognition, rationality, rational thinking, reasoning, knowledge, intelligence and also a moral component such as reflective thinking. Critical thinkers therefore need to have reached a level of maturity in their development, possess a certain attitude as well as a set of taught skills.

There is a postulation by some writers that the tendencies from habits of mind should be thought as virtues to demonstrate the characteristics of a critical thinker. These intellectual virtues are ethical qualities that encourage motivation to think in particular ways towards specific circumstances. However, these virtues have also been criticized by skeptics, who argue that there is lacking evidence for this specific mental basis that are causative to critical thinking.

Edward M. Glaser proposed that the ability to think critically involves three elements:

Educational programs aimed at developing critical thinking in children and adult learners, individually or in group problem solving and decision making contexts, continue to address these same three central elements.

The Critical Thinking project at Human Science Lab, London, is involved in the scientific study of all major educational system in prevalence today to assess how the systems are working to promote or impede critical thinking.

Contemporary cognitive psychology regards human reasoning as a complex process that is both reactive and reflective. This presents a problem which is detailed as a division of a critical mind in juxtaposition to sensory data and memory.

The psychological theory disposes of the absolute nature of the rational mind, in reference to conditions, abstract problems and discursive limitations. Where the relationship between critical thinking skills and critical thinking dispositions is an empirical question, the ability to attain causal domination exists, for which Socrates was known to be largely disposed against as the practice of Sophistry. Accounting for a measure of "critical thinking dispositions" is the California Measure of Mental Motivation and the California Critical Thinking Dispositions Inventory. The Critical Thinking Toolkit is an alternative measure that examines student beliefs and attitudes about critical thinking

John Dewey is one of many educational leaders who recognized that a curriculum aimed at building thinking skills would benefit the individual learner, the community, and the entire democracy.

Critical thinking is significant in the learning process of internalization, in the construction of basic ideas, principles, and theories inherent in content. And critical thinking is significant in the learning process of application, whereby those ideas, principles, and theories are implemented effectively as they become relevant in learners' lives.

Each discipline adapts its use of critical thinking concepts and principles. The core concepts are always there, but they are embedded in subject-specific content. For students to learn content, intellectual engagement is crucial. All students must do their own thinking, their own construction of knowledge. Good teachers recognize this and therefore focus on the questions, readings, activities that stimulate the mind to take ownership of key concepts and principles underlying the subject.

Historically, the teaching of critical thinking focused only on logical procedures such as formal and informal logic. This emphasized to students that good thinking is equivalent to logical thinking. However, a second wave of critical thinking, urges educators to value conventional techniques, meanwhile expanding what it means to be a critical thinker. In 1994, Kerry Walters compiled a conglomeration of sources surpassing this logical restriction to include many different authors’ research regarding connected knowing, empathy, gender-sensitive ideals, collaboration, world views, intellectual autonomy, morality and enlightenment. These concepts invite students to incorporate their own perspectives and experiences into their thinking.

In the English and Welsh school systems, "Critical Thinking" is offered as a subject that 16- to 18-year-olds can take as an A-Level. Under the OCR exam board, students can sit two exam papers for the AS: "Credibility of Evidence" and "Assessing and Developing Argument". The full Advanced GCE is now available: in addition to the two AS units, candidates sit the two papers "Resolution of Dilemmas" and "Critical Reasoning". The A-level tests candidates on their ability to think critically about, and analyze, arguments on their deductive or inductive validity, as well as producing their own arguments. It also tests their ability to analyze certain related topics such as credibility and ethical decision-making. However, due to its comparative lack of subject content, many universities do not accept it as a main A-level for admissions. Nevertheless, the AS is often useful in developing reasoning skills, and the full Advanced GCE is useful for degree courses in politics, philosophy, history or theology, providing the skills required for critical analysis that are useful, for example, in biblical study.

There used to also be an Advanced Extension Award offered in Critical Thinking in the UK, open to any A-level student regardless of whether they have the Critical Thinking A-level. Cambridge International Examinations have an A-level in Thinking Skills.

From 2008, Assessment and Qualifications Alliance has also been offering an A-level Critical Thinking specification.
OCR exam board have also modified theirs for 2008. Many examinations for university entrance set by universities, on top of A-level examinations, also include a critical thinking component, such as the LNAT, the UKCAT, the BioMedical Admissions Test and the Thinking Skills Assessment.

In Qatar, critical thinking was offered by AL-Bairaq—an outreach, non-traditional educational program that targets high school students and focuses on a curriculum based on STEM fields. The idea behind AL-Bairaq is to offer high school students the opportunity to connect with the research environment in the Center for Advanced Materials (CAM) at Qatar University. Faculty members train and mentor the students and help develop and enhance their critical thinking, problem-solving, and teamwork skills.

In 1995, a meta-analysis of the literature on teaching effectiveness in higher education was undertaken.
The study noted concerns from higher education, politicians, and business that higher education was failing to meet society's requirements for well-educated citizens. It concluded that although faculty may aspire to develop students' thinking skills, in practice they have tended to aim at facts and concepts utilizing lowest levels of cognition, rather than developing intellect or values.

In a more recent meta-analysis, researchers reviewed 341 quasi- or true-experimental studies, all of which used some form of standardized critical thinking measure to assess the outcome variable. The authors describe the various methodological approaches and attempt to categorize the differing assessment tools, which include standardized tests (and second-source measures), tests developed by teachers, tests developed by researchers, and tests developed by teachers who also serve the role as the researcher. The results emphasized the need for exposing students to real-world problems and the importance of encouraging open dialogue within a supportive environment. Effective strategies for teaching critical thinking are thought to be possible in a wide variety of educational settings. One attempt to assess the humanities' role in teaching critical thinking and reducing belief in pseudoscientific claims was made at North Carolina State University. Some success was noted and the researchers emphasized the value of the humanities in providing the skills to evaluate current events and qualitative data in context.

Scott Lilienfeld notes that there is some evidence to suggest that basic critical thinking skills might be successfully taught to children at a younger age than previously thought.

Critical thinking is an important element of all professional fields and academic disciplines (by referencing their respective sets of permissible questions, evidence sources, criteria, etc.). Within the framework of scientific skepticism, the process of critical thinking involves the careful acquisition and interpretation of information and use of it to reach a well-justified conclusion. The concepts and principles of critical thinking can be applied to any context or case but only by reflecting upon the nature of that application. Critical thinking forms, therefore, a system of related, and overlapping, modes of thought such as anthropological thinking, sociological thinking, historical thinking, political thinking, psychological thinking, philosophical thinking, mathematical thinking, chemical thinking, biological thinking, ecological thinking, legal thinking, ethical thinking, musical thinking, thinking like a painter, sculptor, engineer, business person, etc. In other words, though critical thinking principles are universal, their application to disciplines requires a process of reflective contextualization. Psychology offerings, for example, have included courses such as Critical Thinking about the Paranormal, in which students are subjected to a series of cold readings and tested on their belief of the "psychic", who is eventually announced to be a fake.

Critical thinking is considered important in the academic fields for enabling one to analyze, evaluate, explain, and restructure thinking, thereby ensuring the act of thinking without false belief. However, even with knowledge of the methods of logical inquiry and reasoning, mistakes occur, and due to a thinker's inability to apply the methodology consistently, and because of overruling character traits such as egocentrism. Critical thinking includes identification of prejudice, bias, propaganda, self-deception, distortion, misinformation, etc. Given research in cognitive psychology, some educators believe that schools should focus on teaching their students critical thinking skills and cultivation of intellectual traits.

Critical thinking skills can be used to help nurses during the assessment process. Through the use of critical thinking, nurses can question, evaluate, and reconstruct the nursing care process by challenging the established theory and practice. Critical thinking skills can help nurses problem solve, reflect, and make a conclusive decision about the current situation they face. Critical thinking creates "new possibilities for the development of the nursing knowledge." Due to the sociocultural, environmental, and political issues that are affecting healthcare delivery, it would be helpful to embody new techniques in nursing. Nurses can also engage their critical thinking skills through the Socratic method of dialogue and reflection. This practice standard is even part of some regulatory organizations such as the College of Nurses of Ontario – Professional Standards for Continuing Competencies (2006).
It requires nurses to engage in Reflective Practice and keep records of this continued professional development for possible review by the College.

Critical thinking is also considered important for human rights education for toleration. The Declaration of Principles on Tolerance adopted by UNESCO in 1995 affirms that "education for tolerance could aim at countering factors that lead to fear and exclusion of others, and could help young people to develop capacities for independent judgement, "critical thinking" and ethical reasoning."

The advent and rising popularity of online courses have prompted some to ask if computer-mediated communication (CMC) promotes, hinders, or has no effect on the amount and quality of critical thinking in a course (relative to face-to-face communication). There is some evidence to suggest a fourth, more nuanced possibility: that CMC may promote some aspects of critical thinking but hinder others. For example, Guiller et al. (2008) found that, relative to face-to-face discourse, online discourse featured more justifications, while face-to-face discourse featured more instances of students expanding on what others had said. The increase in justifications may be due to the asynchronous nature of online discussions, while the increase in expanding comments may be due to the spontaneity of ‘real-time’ discussion. Newman et al. (1995) showed similar differential effects. They found that while CMC boasted more important statements and linking of ideas, it lacked novelty. The authors suggest that this may be due to difficulties participating in a brainstorming-style activity in an asynchronous environment. Rather, the asynchrony may promote users to put forth “considered, thought out contributions.”

Researchers assessing critical thinking in online discussion forums often employ a technique called Content Analysis, where the text of online discourse (or the transcription of face-to-face discourse) is systematically coded for different kinds of statements relating to critical thinking. For example, a statement might be coded as “Discuss ambiguities to clear them up” or “Welcoming outside knowledge” as positive indicators of critical thinking. Conversely, statements reflecting poor critical thinking may be labeled as “Sticking to prejudice or assumptions” or “Squashing attempts to bring in outside knowledge.” The frequency of these codes in CMC and face-to-face discourse can be compared to draw conclusions about the quality of critical thinking.

Searching for evidence of critical thinking in discourse has roots in a definition of critical thinking put forth by Kuhn (1991), which emphasizes the social nature of discussion and knowledge construction. There is limited research on the role of social experience in critical thinking development, but there is some evidence to suggest it is an important factor. For example, research has shown that 3- to 4-year-old children can discern, to some extent, the differential creditability and expertise of individuals. Further evidence for the impact of social experience on the development of critical thinking skills comes from work that found that 6- to 7-year-olds from China have similar levels of skepticism to 10- and 11-year-olds in the United States. If the development of critical thinking skills was solely due to maturation, it is unlikely we would see such dramatic differences across cultures.





</doc>
<doc id="9736296" url="https://en.wikipedia.org/wiki?curid=9736296" title="Linguistic performance">
Linguistic performance

The term linguistic performance was used by Noam Chomsky in 1960 to describe "the actual use of language in concrete situations". It is used to describe both the production, sometimes called " parole", as well as the comprehension of language. Performance is defined in opposition to "competence"; the latter describes the mental knowledge that a speaker or listener has of language.

Part of the motivation for the distinction between performance and competence comes from speech errors: despite having a perfect understanding of the correct forms, a speaker of a language may unintentionally produce incorrect forms. This is because performance occurs in real situations, and so is subject to many non-linguistic influences. For example, distractions or memory limitations can affect lexical retrieval (Chomsky 1965:3), and give rise to errors in both production and perception. Such non-linguistic factors are completely independent of the actual knowledge of language, and establish that speakers' knowledge of language (their competence) is distinct from their actual use of language (their performance).

Published in 1916, Ferdinand de Saussure's "Course in General Linguistics" describes language as ""a system of signs that express ideas"." de Saussure describes two components of language: "langue" and "parole". "Langue" consists of the structural relations that define a language, which includes grammar, syntax and phonology. "Parole" is the physical manifestation of signs; in particular the concrete manifestation of "langue" as speech or writing. While "langue" can be viewed strictly as a system of rules, it is not an absolute system such that "parole" must utterly conform to "langue". Drawing an analogy to chess, de Saussure compares "langue" to the rules of chess that define how the game should be played, and "parole" to the individual choices of a player given the possible moves allowed within the system of rules.

Proposed in the 1950s by Noam Chomsky, generative grammar is an analysis approach to language as a structural framework of the human mind. Through formal analysis of components such as syntax, morphology, semantics and phonology, a generative grammar seeks to model the implicit linguistic knowledge with which speakers determine grammaticality.

In transformational generative grammar theory, Chomsky distinguishes between two components of language production: competence and performance. Competence describes the mental knowledge of a language, the speaker's intrinsic understanding of sound-meaning relations as established by linguistic rules. Performance – that is the actual observed use of language – involves more factors than phonetic-semantic understanding. Performance requires extra-linguistic knowledge such as an awareness of the speaker, audience and the context, which crucially determines how speech is constructed and analyzed. It is also governed by principles of cognitive structures not considered aspects of language, such as memory, distractions, attention, and speech errors.

In 1986, Chomsky proposed a distinction similar to the competence/performance distinction, entertaining the notion of an I-Language (internal language) which is the intrinsic linguistic knowledge within a native speaker and E-Language (external language) which is the observable linguistic output of a speaker. It was I-Language that Chomsky argued should be the focus of inquiry, and not E-Language.

E-language has been used to describe the application of artificial systems, such as in calculus, set theory and with natural language viewed as sets, while performance has been used purely to describes applications of natural language. Between I-Language and competence, I-Language refers to our intrinsic faculty for language, competence is used by Chomsky as an informal, general term, or as term with reference to a specific competency such as "grammatical competence" or "pragmatic competence".

John A. Hawkins's Performance-Grammar Correspondence Hypothesis (PGCH) states that the syntactic structures of grammars are conventionalized based on whether and how much the structures are preferred in performance. Performance preference is related to structure complexity and processing, or comprehension, efficiency. Specifically, a complex structure refers to a structure containing more linguistic elements or words at the end of the structure than at the beginning. It is this structural complexity that results in decreased processing efficiency since more structure requires additional processing. This model seeks to explain word order across languages based on avoidance of unnecessary complexity in favour of increased processing efficiency. Speakers make an automatic calculation of the Immediate Constituent(IC)-to-word order ratio and produce the structure with the highest ratio. Structures with a high IC-to-word order are structures that contain the fewest words required for the listener to parse the structure into constituents which results in more efficient processing.

In head-initial structures, which includes example SVO and VSO word order, the speaker's goal is to order the sentence constituents from least to most complex.

SVO word order can be exemplified with English; consider the example sentences in (1). In (1a) three immediate constituents (ICs) are present in the verb phrase, namely VP, PP1 and PP2, and there are four words ("went, to, London, in") required to parse the VP into its constituents. Therefore, the IC-to-word ratio is 3/4=75%. In contrast, in (1b) the VP is still composed of three ICs but there are now six words that are required to determine the constituent structure of the VP ("went, in, the, late, afternoon, to"). Thus, the ratio for (1b) is 3/6 = 50%. Hawkins proposes that speakers prefer to produce (1a) since it has a higher IC-to-word ratio and this leads to faster and more efficient processing.

Hawkins supports the above analysis by providing performance data to demonstrate the preference speakers have for ordering short phrases before long phrases when producing head-initial structures. The table based on English data, below, illustrates that the short prepositional phrase (PP1) is preferentially ordered before the long PP (PP2) and that this preference increases as the size differential between the two PPs increases. For example, 60% of the sentences are ordered short (PP1) to long (PP2) when PP2 was longer than PP1 by 1 word. In contrast, 99% of the sentences are ordered short to long when PP2 is longer than PP1 by 7+ words.

English prepositional phrase orderings by relative weight 

Hawkins argues that the preference for short followed by long phrases applies to all languages that have head-initial structuring. This includes languages with VSO word order such as from Hungarian. By calculating the IC-to-word ratio for the Hungarian sentences in the same way as was done for the English sentences, 2a. emerges as having a higher ratio than 2b.

The VP and its constituents in 4. are constructed from their heads on the right. This means that the number of words used to calculate the ratio is counted from the head of the first phrase (PP in 3a. and NP in 3b.) to the verb (as indicated in bold above). The IC-to-word ratio for the VP in 3a. is 3/5=60% while the ratio for the VP in 3b. is 3/4=75%. Therefore, 3b. should be preferred by Japanese speakers since it has a higher IC-to-word ratio which leads to faster parsing of sentences by the listener.

The performance preference for long to short phrase ordering in SVO languages is supported by performance data. The table below shows that production of long to short phrases is preferred and that this preference increases as the size of the differential between the two phrases increases. For example, ordering of the longer 2ICm (where ICm is either a direct object NP with an accusative case particle or a PP constructed from the right periphery) before the shorter 1ICm is more frequent, and the frequency increases to 91% if the 2ICm is longer than the 1ICm by 9+ words.

Japanese NPo and PPm orderings by relative weight 

[[Tom Wasow]] proposes that word order arises as a result of utterance planning benefiting the speaker. He introduces the concepts of early versus late commitment, where commitment is the point in the utterance where it becomes possible to predict subsequent structure. Specifically, early commitment refers to the commitment point present earlier in the utterance and late commitment refers to the commitment point present later in the utterance. He explains that early commitment will favour the listener since early prediction of subsequent structure enables faster processing. Comparatively, late commitment will favour the speaker by postponing decision making, giving the speaker more time to plan the utterance. Wasow illustrates how utterance planning influences syntactic word order by testing early versus late commitment in [[Heavy NP shift|heavy-NP shifted]] (HNPS) sentences. The idea is to examine the patterns of HNPS to determine if the performance data show sentences that are structured to favour the speaker or the listener.

The following examples illustrate what is meant by early versus late commitment and how heavy-NP shift applies to these sentences. Wasow looked at two types of verbs:

Vt ([[transitive verbs]]): require NP objects.

In 4a. no heavy-NP shift has been applied. The NP is available early but does not provide any additional information about the sentence structure – the "to" appearing late in the sentence is an example of late commitment. In contrast, in 4b., where heavy-NP shift has shifted the NP to the right, as soon as "to" is uttered the listener knows that the VP must contain the NP and a PP. In other words, when "to" is uttered it allows the listener to predict the remaining structure of the sentence early on. Thus for transitive verbs HNPS results in early commitment and favors the listener.

Vp ([[prepositional verbs]]): can take an NP object or an immediately following PP with no NP object

No HNPS has been applied to 5a. In 5b. the listener needs to hear the word "something" in order to know that the utterance contains a PP and an NP since the object NP is optional but "something" has been shifted to later in the sentence. Thus for prepositional verbs HNPS results in late commitment and favours the speaker.

Based on the above information Wasow predicted that if sentences are constructed from the speaker's perspective then heavy-NP shift would rarely apply to sentences containing a transitive verb but would apply frequently to sentences containing a prepositional verb. The opposite prediction was made if sentences are constructed from the listener's perspective.

To test his predictions Wasow analyzed performance data (from corpora data) for the rates of occurrence of HNPS for Vt and Vp and found HNPS occurred twice as frequently in Vp than in Vt, therefore supporting the predictions made from the speaker's perspective. In contrast, he did not find evidence in support of the predictions made based on the listener's perspective. In other words, given the data above, when HNPS is applied to sentences containing a transitive verb the result favors the listener. Wasow found that HNPS applied to transitive verb sentences is rare in performance data thus supporting the speaker's perspective. Additionally, when HNPS is applied to prepositional verb structures the result favors the speaker. In his study of the performance data, Wasow found evidence of HNPS frequently applied to prepositional verb structures further supporting the speaker's perspective. Based on these findings Wasow concludes that HNPS is correlated with the speaker's preference for late commitment thereby demonstrating how speaker performance preference can influence word order.

While the dominant views of grammar are largely oriented towards competence, many, including Chomsky himself, have argued that a complete model of grammar should be able to account for performance data. But while Chomsky argues that competence should be studied first, thereby allowing further study of performance, some systems, such as constraint grammars are built with performance as a starting point (comprehension, in the case of constraint grammars While traditional models of generative grammar have had a great deal of success in describing the structure of languages, they have been less successful in describing how language is interpreted in real situations. For example, traditional grammar describes a sentence as having an "underlying structure" which is different from the "surface structure" which speakers actually produce. In a real conversation, however, a listener interprets the meaning of a sentence in real time, as the surface structure goes by. This kind of on-line processing, which accounts for phenomena such as finishing another person's sentence, and starting a sentence without knowing how it is going to finish, is not directly accounted for in traditional generative models of grammar. Several alternative grammar models exist which may be better able to capture this surface-based aspect of linguistic performance, including
Constraint Grammar, Lexical Functional Grammar, and Head-driven phrase structure grammar.

Errors in linguistic performance not only occur in children newly acquiring their native language, second language learners, those with a disability or an acquired brain injury but among competent speakers as well. Types of performance errors that will be of focus here are those that involve errors in syntax, other types of errors can occur in the phonological, semantic features of words, for further information see speech errors. Phonological and semantic errors can be due to the repetition of words, mispronunciations, limitations in verbal working memory, and length of the utterance. Slips of the tongue are most common in spoken languages and occur when the speaker either: says something they did not mean to; produces the incorrect order of sounds or words; or uses the incorrect word. Other instances of errors in linguistic performance are slips of the hand in signed languages, slips of the ear which are errors in comprehension of utterances and slips of the pen which occur while writing. Errors of linguistic performance are perceived by both the speaker and the listener and can therefore have many interpretations depending on the persons judgement and the context in which the sentence was spoken.

It is proposed that there is a close relation between the linguistic units of grammar and the psychological units of speech which implies that there is a relation between linguistic rules and the psychological processes that create utterances. Errors in performance can occur at any level of these psychological processes. Lise Menn proposes that there are five levels of processing in speech production, each with its own possible error that could occur. According to the proposed speech processing structure by Menn an error in the syntactic properties of an utterance occurs at the positional level.

Another proposal for the levels of speech processing is made by Willem J. M. Levelt to be structured as so: 
Levelt (1993) states that we as speakers are unaware of most of these levels of performance such as articulation, which includes the movement and placement of the articulators, the formulation of the utterance which includes the words selected and their pronunciation and the rules which must be followed for the utterance to be grammatical. The levels speakers are consciously aware is the intent of the message which occurs at the level of conceptualization and then again at self-monitoring which is when the speaker would become aware of any errors that may have occurred and correct themselves.

One type of slip of the tongue which cause an error in the syntax of the utterance are called transformational errors. Transformational errors are a mental operation proposed by Chomsky in his Transformational Hypothesis and it has three parts which errors in performance can occur. These transformations are applied at the level of the underlying structures and predict the ways in which an error can occur. 

Structural Analysis
errors can occur due to the application of (a) the rule misanalyzing the tense marker causing the rule to apply incorrectly, (b) the rule not being applied when it should or (c) a rule being applied when it should not.

This example from Fromkin (1980) demonstrates a rule misanalyzing the tense marker and for subject-auxiliary inversion to be incorrectly applied. The subject-auxiliary inversion is misanalyzed as to which structure it applies, applying without the verb "be" in the tense as it moves to the C position. This causes "do-support" to occur and the verb to lack tense causing the syntactic error.

The following example from Fromkin (1980) demonstrates how a rule is being applied when it should not. The subject-auxiliary inversion rule is omitted in the error utterance, causing affix-hopping to occur and putting the tense onto the verb "say" creating the syntactic error. In the target the subject-auxiliary rule and then do-support applies creating the grammatically correct structure.

This example from Fromkin (1980) shows how a rule is being applied when it should not. The subject-auxiliary inversion and do-support has applied to an idiomatic expression causing the insertion of "do" when it should not be applied in the ungrammatical utterance.

Structural Change
Errors can occur in the carrying out of rules, even though the analysis of the phrase marker is done correctly. This can occur when the analysis requires multiple rules to occur.

The following example from Fromkin (1980) shows the relative clause rule copies the determiner phrase "a boy" within the clause and this causes front attaching to the Wh-marker. Deletion is then skipped, leaving the determiner phrase in the clause in the error utterance causing it to be ungrammatical. 

Conditions errors restrict when the rule can and cannot be applied.

This last example from Fromkin (1980) shows that a rule was applied under a certain condition in which it is restricted. The subject-auxiliary inversion rule cannot apply to embedded clauses. In the case of this example it has causing for the syntactic error. 

A study of deaf Italians found that the second person singular of indicatives would extend to corresponding forms in imperatives and negative imperatives. 

The following is an example taken from Dutch data in which there is verb omission in the embedded clause of the utterance (which is not allowed in Dutch), resulting in a performance error.

A study done with Zulu speaking children with a language delay displayed errors in linguistic performance of lacking proper passive verb morphology.

The linguistic components of American Sign Language (ASL) can be broken down into four parts; the hand configuration, place of articulation, movement and other minor parameters. Hand configuration is determined by the shape of the hand, fingers and thumbs and is specific to the sign that is being used. It allows the signer to articulate what they are wanting to communicate by extending, flexing, bending or spreading the digits; the position of the thumb to the fingers; or the curvature of the hand. However, there are not an infinite amount of possible hand configurations, there are 19 classes of hand configuration primes as listed by the "Dictionary of American Sign Language". Place of articulation is the particular location that the sign is being performed known as the "signing place". The "signing place" can be the whole face or a particular part of it, the eyes, nose, cheek, ear, neck, trunk, any part of the arm, or the neutral area in front of the signers head and body. Movement is the most complex as it can be difficult to analyze. Movement is restricted to directional, rotations of the wrist, local movements of the hand and interactions of the hands. These movements can occur singularly, in sequence, or simultaneously. Minor parameters in ASL include contacting region, orientation and hand arrangement. They are subclasses of hand configuration.
Performance errors resulting in ungrammatical signs can result due to processes that change the hand configuration, place, movement or other parameter of the sign. These processes can be anticipation, preservation, or metathesis. Anticipation is caused when some characteristic of the next sign is incorporated into the sign that is presently being performed. Preservation is the opposite of anticipation where some characteristic of the preceding sign is carried over into the performance of the next sign. Metathesis occurs when two characteristics of adjacent signs are combined into one in the performance of both signs. Each of these errors will result in an incorrect sign being performed. This could result in either a different sign being performed instead of the intended one, or nonexistent signs which forms are possible and those which forms are not possible due to the structural rules. These are the main types of performance errors in sign language however on the rare occasion there is also the possibility of errors in the order of the signs performed resulting in a different meaning than what the signer intended.

Unacceptable Sentences
are ones which, although are grammatical, are not considered proper utterances. They are considered unacceptable due to the lack of our cognitive systems to process them. Speakers and listeners can be aided in the performance and processing of these sentences by eliminating time and memory constraints, increasing motivation to process these utterances and using pen and paper. In English there are three types of sentences that are grammatical but are considered unacceptable by speakers and listeners.
When a speaker makes an utterance they must translate their ideas into words, then syntactically proper phrases with proper pronunciation. The speaker must have prior world knowledge and an understanding of the grammatical rules that their language enforces. When learning a second language or with children acquiring their first language, speakers usually have this knowledge before they are able to produce them. Their speech is usually slow and deliberate, using phrases they have already mastered, and with practice their skills increase. Errors of linguistic performance are judged by the listener giving many interpretations if an utterance is well-formed or ungrammatical depending on the individual. As well the context in which an utterance is used can determine if the error would be considered or not. When comparing "Who must telephone her?" and "Who need telephone her?" the former would be considered the ungrammatical phrase. However, when comparing it to "Who want telephone her?" it would be considered the grammatical phrase. The listener may also be the speaker. When repeating sentences with errors if the error is not comprehended then it is performed. As well if the speaker does notice the error in the sentence they are supposed to repeat they are unaware of the difference between their well-formed sentence and the ungrammatical sentence.
An unacceptable utterance can also be performed due to a brain injury. Three types of brain injuries that could cause errors in performance were studied by Fromkin are dysarthria, apraxia and literal paraphasia. Dysarthria is a defect in the neuromuscular connection that involves speech movement. The speech organs involved can be paralyzed or weakened, making it difficult or impossible for the speaker to produce a target utterance. Apraxia is when there is damage to the ability to initiate speech sounds with no paralysis or weakening of the articulators. Literal paraphasia causes disorganization of linguistic properties, resulting in errors of word order of phonemes. Having a brain injury and being unable to perform proper linguistic utterances, some individuals are still able to process complex sentences and formulate syntactically well formed sentences in their mind.
Child productions when they are acquiring language are full of errors of linguistic performance. Children must go from imitating adult speech to create new phrases of their own. They will need to use their cognitive operations of the knowledge of their language they are learning to determine the rules and properties of that language. The following are examples of errors in English speaking children's productions. 

In an elicited production experiment a child, Adam, was prompted to ask questions to an Old Lady

The most commonly used measure of syntax complexity is the mean length of utterance, also known as MLU. This measure is independent from how often children talk and focuses on the complexity and development of their grammatical systems, including morphological and syntactic development. The number representing a person's MLU corresponds to the complexity of the syntax being used. In general, as the MLU increases, the syntactic complexity also increases. Typically, the average MLU corresponds to a child's age due to their increase in working memory, which allows for sentences to be of greater syntactic complexity. For example, the average MLU of a 7-year-old child is 7 words. However, children show more individual variability of syntactic performance with more complex syntax. Complex syntax have a higher number of phrases and clause levels, therefore adding more words to the overall syntactic structure. Seeing as there are more individual differences in MLU and syntactic development as children get older, MLU is particularly used to measure grammatical complexity among school-aged children. Other types of segmentation strategies for discourse are the T-unit and C-unit (communicative unit). If these two measurements are used to account for discourse, the average length of the sentence will be lower than if MLU is used alone. Both the T-units and C-units count each clause as a new unit, hence a lower number of units.

Typical MLU per age group can be found in the following table, according to Roger Brown's five stages of syntactic and morphological development:
Here are the steps for calculating MLU:


Here's an example of how to calculate MLU:

In total there are 17 morphemes in this data set. In order to find the MLU, we divide the total number of morphemes (17) by the total number of utterances (4). In this particular data set, the mean length of utterance is 17/4 = 4.25.

Clause density refers to the degree to which utterances contain dependent clauses. This density is calculated as a ratio of the total number of clauses across sentences, divide by the number of sentences in a discourse sample. For example, if the clause density is 2.0, the ratio would indicate that the sentence being analyzed has 2 clauses on average: one main clause and one subordinate clause.

Here is an example of how clause density is measured, using T-units, adapted from Silliman & Wilkinson 2007:

Indices track structures to show a more comprehensive picture of a person's syntactic complexity. Some examples of indices are Development Sentence Scoring, the Index of Productive Syntax and the Syntactic Complexity Measure.

Developmental Sentence Scoring is another method to measure syntactic performance as a clinical tool. In this indice, each consecutive utterance, or sentence, elicited from a child is scored. This is a commonly applied measurement of syntax for first and second language learners, with samples gathered from both elicited and spontaneous oral discourse. Methods for eliciting speech for these samples come in many forms, such having the participant answering questions or re-telling a story. These elicited conversations are commonly tape-recorded for playback during analysis to see how well the person can incorporate syntax among other linguistic cues. For every utterance elicited, the utterance will receive one point if it is a correct form used in adult speech. A score of 1 indicates the least complex syntactic form in the category, whereas a higher score reflects higher level grammaticality. Points are specifically awarded to an utterance based on whether or not it contains any of the eight categories outlined below.

Syntactic categories measured by developmental sentence scoring with examples:

In particular, those categories that appear the earliest in speech receive a lower score, whereas later-appearing categories receive a higher score. If an entire sentence is correct according to adult-like forms, then the utterance would receive an extra point. The eight categories above are the most commonly used structures in syntactic formation, thus structures such as possessives, articles, plurals, prepositional phrases, adverbs and descriptive adjectives were omitted and not scored. Additionally, the scoring system is arbitrary when applied to certain structures. For example, there is no indication as to why "if" would receive four points rather than five. The scores of all the utterances are totalled in the end of the analysis and then averaged to get a final score. This means that the individual's final score reflects their entire syntactic complexity level, rather than syntactic level in a specific category. The main advantage of development sentence scoring is that the final score represents the individual's general syntactic development and allows for easier tracking of changes in language development, making this tool effective for longitudinal studies.

Similar to Development Sentence Scoring, the Index of Productive Syntax evaluates the grammatical complexity of spontaneous language samples. After age 3, Index of Productive Syntax becomes more widely used than MLU to measure syntactic complexity in children. This is because at around age 3, MLU does not distinguish between children of similar language competency as well as Index of Productive Syntax does. For this reason, MLU is initially used in early childhood development to track syntactic ability, then Index of Productive Syntax is used to maintain validity. Individual utterances in a discourse sample are scored based on the presence of 60 different syntactic forms, placed more generally under four subscales: noun phrase, verb phrase, question/negation and sentence structure forms. After a sample is recorded, a corpus is then formed based on 100 utterance transcriptions with 60 different language structures being measured in each utterance. Not included in the corpus are imitations, self-repetitions and routines, which constitute language that does not represent productive language usage. In each of the four sub-scales previously mentioned, the first two unique occurrences of a form are scored. After this, occurrences of a sub-scale are not scored. However, if a child has mastered a complex syntax structure earlier than expected, they will receive extra points.

The six main tasks in standardized testing for syntax:

Some of the common standardized tests for measuring syntactic performance are the TOLD-2 Intermediate (Test of Language Development), the TOAL-2 (Test of Adolescent Language) and the CELF-R (Clinical Evaluation of Language Fundamentals, Revised Screening Test).



</doc>
<doc id="2892491" url="https://en.wikipedia.org/wiki?curid=2892491" title="Visual language">
Visual language

The visual language is a system of communication using visual elements. Speech as a means of communication cannot strictly be separated from the whole of human communicative activity which includes the visual and the term 'language' in relation to vision is an extension of its use to describe the perception, comprehension and production of visible signs.

An image which dramatizes and communicates an idea presupposes the use of a visual language. Just as people can 'verbalize' their thinking, they can 'visualize' it. A diagram, a map, and a painting are all examples of uses of visual language. Its structural units include line, shape, colour, form, motion, texture, pattern, direction, orientation, scale, angle, space and proportion.

The elements in an image represent concepts in a spatial context, rather than the linear form used for words. Speech and visual communication are parallel and often interdependent means by which humans exchange information.

Visual units in the form of lines and marks are constructed into meaningful shapes and structures or signs. Different areas of the cortex respond to different elements such as colour and form. Semir Zeki has shown the responses in the brain to the paintings of Michelangelo, Rembrandt, Vermeer, Magritte, Malevich and Picasso.

What we have in our minds in a waking state and what we imagine in dreams is very much of the same nature. Dream images might be with or without spoken words, other sounds or colours. In the waking state there is usually, in the foreground, the buzz of immediate perception, feeling, mood and as well as fleeting memory images. In a mental state between dreaming and being fully awake is a state known as 'day dreaming' or a meditative state, during which "the things we see in the sky when the clouds are drifting, the centaurs and stags, antelopes and wolves" are projected from the imagination. Rudolf Arnheim has attempted to answer the question: what does a mental image look like? In Greek philosophy, the School of Leucippus and Democritus believed that a replica of an object enters the eye and remains in the soul as a memory as a complete image. Berkeley explained that parts, for example, a leg rather than the complete body, can be brought visually to the mind. Arnheim considers the psychologist, Edward B. Titchener's account to be the breakthrough in understanding something of how the vague incomplete quality of the image is 'impressionistic' and carries meaning as well as form.

Abstract art has shown that the qualities of line and shape, proportion and colour convey meaning directly without the use of words or pictorial representation. Wassily Kandinsky showed how drawn lines and marks can be expressive without any association with a representational image. From the most ancient cultures and throughout history visual language has been used to encode meaning: "The Bronze Age Badger Stone on Ilkly Moor is covered in circles, lines, hollow cups,winged figures, a spread hand, an ancient swastika, an embryo, a shooting star? … It's a story-telling rock, a message from a world before (written) words." Richard Gregory suggests that, "Perhaps the ability to respond to absent imaginary situations," as our early ancestors did with paintings on rock, "represents an essential step towards the development of abstract thought."

The sense of sight operates selectively. Perception is not a passive recording of all that is in front of the eyes, but is a continuous judgement of scale and colour relationships, and includes making categories of forms to classify images and shapes in the world. Children of six to twelve months are to be able through experience and learning to discriminate between circles, squares and triangles. The child from this age onwards learns to classify objects, abstracting essential qualities and comparing them to other similar objects. Before objects can be perceived and identified the child must be able to classify the different shapes and sizes that a single object may appear to have when it is seen in varying surroundings and from different aspects.

The perception of a shape requires the grasping of the essential structural features, to produce a "whole" or "gestalt". The theory of the "gestalt" was proposed by Christian von Ehrenfels in 1890. He pointed out that a melody is still recognisable when played in different keys and argued that the whole is not simply the sum of its parts but a total structure. Max Wertheimer researched von Ehrenfels' idea, and in his "Theory of Form" (1923) – nicknamed "the dot essay" because it was illustrated with abstract patterns of dots and lines – he concluded that the perceiving eye tends to bring together elements that look alike (similarity groupings) and will complete an incomplete form (object hypothesis). An array of random dots tends to form configurations (constellations). All these innate abilities demonstrate how the eye and the mind are seeking pattern and simple whole shapes. When we look at more complex visual images such as paintings we can see that art has been a continuous attempt to "notate" visual information.

Thought processes are diffused and interconnected and are cognitive at a sensory level. The mind thinks at its deepest level in sense material, and the two hemispheres of the brain deal with different kinds of thought. The brain is divided into two hemispheres and a thick bundle of nerve fibres enable these two halves to communicate with each other. In most people the ability to organize and produce speech is predominantly located in the left side. Appreciating spatial perceptions depends more on the right hemisphere, although there is a left hemisphere contribution. In an attempt to understand how designers solve problems, L. Bruce Archer proposed "that the way designers (and everybody else, for that matter) form images in their mind's eye, manipulating and evaluating ideas before, during and after externalising them, constitutes a cognitive system comparable with but different from, the verbal language system. Indeed we believe that human beings have an innate capacity for cognitive modelling, and its expression through sketching, drawing, construction, acting out and so on, that is fundamental to human thought."

The visual language begins to develop in babies as the eye and brain become able to focus, and be able to recognize patterns. Children's drawings show a process of increasing perceptual awareness and range of elements to express personal experience and ideas. The development of the visual aspect of language communication in education has been referred to as graphicacy, as a parallel discipline to literacy and numeracy. The ability to think and communicate in visual terms is part of, and of equal importance in the learning process, with that of literacy and numeracy. The visual artist, as Michael Twyman has pointed out, has developed the ability to handle the visual language to communicate ideas. This includes both the understanding and conception and the production of concepts in a visual form.





</doc>
<doc id="12532423" url="https://en.wikipedia.org/wiki?curid=12532423" title="Langmaker">
Langmaker

Langmaker was a website run by Jeffrey Henning that acted as a database of conlangs, neographies, and other resources related to conlanging and conworlding. As of June 4, 2009, the site was offline. An unknown source has taken over the website, and hosts virus files.

Langmaker began as "Model Languages", a newsletter published by Henning between 1995 and 1996, in which he attempted to better publicize the hobby of conlanging and to explore various issues and questions related to conlanging. Shortly, Henning moved to a website format. He began not only discussing and commenting on conlangs but also cataloguing them in a comprehensive database, with overviews of the languages and links to their respective websites. Henning ultimately began adding neologisms; babel texts; neographies; books on languages, linguistics, and conlanging; and other general resources to his database. Langmaker has been viewed by many as an information and activity hub in the online conlanging world.

In April 2007, Langmaker was converted to wiki format, allowing its many readers to themselves make contributions to the website. Henning has since left the maintenance of the site primarily to its casual contributors and administrators, who continued to contribute up until January 4, 2008, when the site was locked.

Mark Rosenfelder writes, "Jeffrey Henning writes and posts regularly on the process of creating model languages and reviews a number of projects." As of November 11, 2015, the domain name has been bought.



</doc>
<doc id="12393342" url="https://en.wikipedia.org/wiki?curid=12393342" title="Vernacular orientation">
Vernacular orientation

Vernacular orientation refers to the status that a language is afforded by one of its mother-tongue speakers (Tiessen, 2003). This status is exhibited through the sociolinguistic behaviours of a mother-tongue speaker. A speaker who exhibits positive vernacular orientation is one who exhibits a preferred status for their mother tongue in such things as patterns of language use, language attitudes, social networks and even levels of language proficiency. Likewise, a speaker who exhibits negative vernacular orientation is one who exhibits a preferred status for a language other than their mother tongue in these areas of sociolinguistic behaviour.

An example of research into vernacular orientation as expressed in a community can be found at . This is a study on vernacular orientation in the Talysh community of the city of Sumgayit in the Republic of Azerbaijan for the purpose of gaining a greater understanding of its causes. Vernacular orientation is described in three areas of sociolinguistic behaviour: patterns of vernacular language use, vernacular language proficiency and frequency of vernacular-speaking individuals in social networks. Data was collected through personal interviews. The questionnaires for these interviews were developed using a qualitative-relational research approach. The description of vernacular orientation takes the form of a criteria-based typology of which an analysis of influential factors is ultimately made. This analysis of influential factors demonstrates the interaction between vernacular orientation as described in the typology and the contextual elements of the family, socio-economic dynamics and individual attitudes.


</doc>
<doc id="13775689" url="https://en.wikipedia.org/wiki?curid=13775689" title="Cultural emphasis">
Cultural emphasis

Cultural emphasis is an important aspect of a culture which is often reflected though language and, more specifically, vocabulary (Ottenheimer, 2006, p. 266). This means that the vocabulary people use in a culture indicates what is important to that group of people. If there are a lot of words to describe a certain topic in a specific culture, then there is a good chance that that topic is considered important to that culture.

The idea of cultural emphasis is rooted form the work of Franz Boas, who is considered to be one of the founders of American Anthropology (Ottenheimer, 2006, p. 15). Franz Boas developed and taught concepts such as cultural relativism and the "cultural unconscious", which allowed anthropologists who studied under him, like Edward Sapir and Ruth Benedict, to further study and develop ideas on language and culture (Hart, 2005, p. 179).

One way in which cultural emphasis is exemplified is a populace talks about the weather. For example, in a place where it is cold and it snows a lot, a large collection of words to describe the snow would be expected.

In a place where it is hot, a cornucopia of associated terms would be expected.

A concentration of related terms for similar phenomena suggests the importance in distinguishing between them. Furthermore, if you are not from the area, or that culture, you might not have experienced or know the difference between, for example, a dry heat or a humid heat, when the difference may have huge implications for the outcome of a particular action.




</doc>
<doc id="11716414" url="https://en.wikipedia.org/wiki?curid=11716414" title="Language preservation">
Language preservation

Language preservation is the effort to prevent languages from becoming unknown. A language is at risk of being lost when it no longer is taught to younger generations, while fluent speakers of the language (usually the elderly) die.

Language is an important part of any society, because it enables people to communicate and express themselves. When a language dies out, future generations lose a vital part of the culture that is necessary to completely understand it. This makes language a vulnerable aspect of cultural heritage, and it becomes especially important to preserve it. According to the United Nations Educational, Scientific, and Cultural Organization (UNESCO), from facts published in their "Atlas of Languages in Danger of Disappearing," there are an estimated 7,000 languages spoken worldwide today, and half of the world’s population speaks the eight most common. 

More than 3,000 languages are reportedly spoken by fewer than 10,000 people each. "Ethnologue," a reference work published by SIL International, has cataloged the world’s known living languages, and it estimates that 417 languages are on the verge of extinction. 
Language protection is protection of cultural heritage, as Karl von Habsburg, President of Blue Shield International, states. "Today, on average, we lose one language in the world every six weeks. There are approximately 6800 languages. But four percent of the population speaks 96 percent of the languages, and 96 percent of the population speaks four percent of the languages. These four percent are spoken by large language groups and are therefore not at risk. But 96 percent of the languages we know are more or less at risk. You have to treat them like extinct species."

There are different factors that can put a language in danger of becoming extinct. One is when a language is no longer being taught to the children of the community, or at least to a large number of the children. In these cases, the remaining fluent speakers of the language are generally the older members of the community, and when they pass on, the language dies out with them.

Child speakers are not enough to ensure the survival of a language however. If the children who do speak the language are relocated to another area where it is not spoken, it becomes endangered. Political and military turmoil can also endanger a language. When people are forced from their homes into new lands, they may have to learn the language of the new area to adapt, and they end up losing their language. Likewise, when a country or territory is successfully invaded, the population may be forced to learn the invader's language.

A language can also become associated with a lower social class. In this instance, parents will encourage their children to use the language used more often in society to distance themselves from the perceived lower class. Within one or two generations of this occurrence, the language can easily be lost.

When a language dies, the knowledge of and ability to understand the culture who spoke it is threatened because the teachings, customs, oral traditions and other inherited knowledge are no longer transmitted among native speakers. As each language dies, science in linguistics, anthropology, prehistory and psychology lose some diversity in data sources.

There are different ideas about the best ways to preserve a language. One way is to encourage younger generations to speak the language as they grow, so they will then teach their children the language as well. In many cases, this option is nearly impossible. There are often many factors that endanger a language, and it is impossible to control each of these factors to ensure its survival.

The internet can be used to raise awareness about the issues of language extinction and language preservation. It can be used to translate, catalog, store, and provide information and access to languages. New technologies such as podcasts can be used to preserve the spoken versions of languages, and written documents can preserve information about the native literature and linguistics of languages.

The international internet provider VeriSign estimates that 65-70% of all internet content is in English.

Using written documents to preserve information about the native literature and linguistics is also not without potential problems. Just because a language is written down, this does not mean it will survive. Written information in book or manuscript form is subject to acid issues, binding problems, environmental monitoring problems, and security concerns.

Technology can also be used to preserve the integrity of spoken versions of languages. Many of the same techniques used in recording oral history can be used to preserve spoken languages. Preservationists can use reel-to-reel audio tape recordings, along with video recordings, and new technologies like podcasts to record spoken accounts of languages. Technology is also vulnerable to new technology. Preservation efforts would fail if the technology to listen to or watch certain media such as audio tape recordings or video tapes is lost.

The Administration for Native Americans has published the "Reference Guide for Establishing Archives and Repositories," which explains why language repositories are vital to long-term language preservation efforts. The guide offers practical advice on what to preserve and why; it explains what a language repository is, how to build one, and the costs involved; and lists other resources for creating an archive and repository.




</doc>
<doc id="11517229" url="https://en.wikipedia.org/wiki?curid=11517229" title="Closed-ended question">
Closed-ended question

A closed-ended question refers to any question for which a researcher provides research participants with options from which to choose a response. Closed-ended questions are sometimes phrased as a statement which requires a response.

A closed-ended question contrasts with an open-ended question, which cannot easily be answered with specific information.

Examples of close-ended questions which may elicit a "yes" or "no" response include:

Similarly, variants of the above close-ended questions which possess specific responses are:

At the same time, there are closed-ended questions which are sometimes impossible to answer correctly with a yes or no without confusion, for example: "Have you stopped taking heroin?" (if you never took it) or "Who told you to take heroin?"; see "loaded question".

A study by the University of Cincinnati found 20 to 40 percent of Americans will provide an opinion when they do not have one because of social pressure, using context clues to select an answer they believe will please the questioner. A classic example of this phenomenon was the 1947 study of the fictional Metallic Metals Act.

Some in the field of education argue that closed-ended questions are broadly speaking "bad" questions. They are questions that are often asked to obtain a specific answer and are therefore good for testing knowledge. It is often argued that open-ended questions (i.e. questions that elicit more than a yes/no answers) are preferable because they open up discussion and enquiry.

Peter Worley argues that this is a false assumption. This is based on Worley’s central arguments that there are two different kinds of open and closed questions: grammatical and conceptual. He argues that educational practitioners should be aiming for questions that are "grammatically closed, but conceptually open". For example, in standard parlance, "Is it ever right to lie?" would be regarded as a closed question: it elicits a yes–no response. Significantly, however, it is conceptually open. Any initial yes–no answer to it can be "opened up" by the questioner ("Why do you think that?", "Could there be an instance where that's not the case?"), inviting elaboration and enquiry.

This grammatically closed but cognitively open style of questioning, Worley argues, "gives [educators] the best of both worlds: the focus and specificity of a closed question (this, after all, is why teachers use them) and the inviting, elaborating character of an open question". Closed questions, simply require "opening up" strategies to ensure that conceptually open questions can fulfil their educational potential.

Worley's structural and semantic distinction between open and closed questions is integral to his pedagogical invention "Open Questioning Mindset" (OQM). OQM refers to the development, in educators, of an open attitude towards the process of learning and the questioning at the heart of that process. It is a mind-set that is applicable to all subject areas and all pedagogical environments. Teachers who develop an Open Questioning Mindset listen openly for the cognitive content of students' contributions and looks for ways to use what is given for learning opportunities, whether right, wrong, relevant or apparently irrelevant. OQM encourages a style of pedagogy that values genuine enquiry in the classroom. It provides teachers with the tools to move beyond what Worley calls "guess what's in my head" teaching, that relies on closed and leading questions.




</doc>
<doc id="623398" url="https://en.wikipedia.org/wiki?curid=623398" title="Sublanguage">
Sublanguage

A sublanguage is a subset of a language. Sublanguages occur in natural language, computer language, and relational databases.

In informatics, natural language processing, and machine translation, a sublanguage is the language of a restricted domain, particularly a technical domain. In mathematical terms, "a subset of the sentences of a language forms a sublanguage of that language if it is closed under some operations of the language: e.g., if when two members of a subset are operated on, as by "and" or "because", the resultant is also a member of that subset" (Z. S. Harris "Language and Information", Columbia U. Press, 1988, p. 34).

The term sublanguage has also sometimes been used to denote a computer language that is a subset of another language. A sublanguage may be restricted syntactically (it accepts a subgrammar of the original language), and/or semantically (the set of possible outcomes for any given program is a subset of the possible outcomes in the original language).

For instance, ALGOL 68S was a subset of ALGOL 68 designed to make it possible to write a single-pass compiler for this "sublanguage".

SQL (Structured Query Language) statements are classified in various ways, which can be grouped into sublanguages, commonly: a data query language (DQL), a data definition language (DDL), a data control language (DCL), and a data manipulation language (DML).

In relational database theory, the term "sublanguage", first used for this purpose by E. F. Codd in 1970, refers to a computer language used to define or manipulate the structure and contents of a relational database management system (RDBMS). Typical sublanguages associated with modern RDBMS's are QBE (Query by Example) and SQL (Structured Query Language). In 1985, Codd encapsulated his thinking in twelve rules which every database must satisfy in order to be truly relational. The fifth rule is known as the "Comprehensive data sublanguage rule", and states:



</doc>
<doc id="17504079" url="https://en.wikipedia.org/wiki?curid=17504079" title="Reply">
Reply

A reply is a statement or acknowledgment made in response to an interrogative question, request or comment. Replies are communicated in a variety of ways, the most common being spoken or written, and act as a way of conveying relevant information and continuing a conversational exchange.

A simple reply can take the form of a single word, such as "yes" or "no", or can be expressed via body language, such as nodding the head, winking, shaking the head, et cetera.



</doc>
<doc id="24822996" url="https://en.wikipedia.org/wiki?curid=24822996" title="Musivisual language">
Musivisual language

In art, musivisual language is a semiotic system that is the synchronous union of music and image. The term was coined by Spanish composer Alejandro Román, and for over a century, has appeared in film and other media (television, video or multimedia). 

According to Román:

When film music and text connect, they produce meanings distinct from the separate elements. In this communication process, musical codes (melody, rhythm, harmony, sound, texture, form), in synchrony with the film (image, speech, noise...) interact. 

Román defines two levels for this language: "semiotic", i.e. the contribution of meaning of music over the image, and the specific "aesthetic" of film music, which means it has its own stylistic elements not belonging to other musical forms. These elements are determined by the cinematic form.





</doc>
<doc id="162986" url="https://en.wikipedia.org/wiki?curid=162986" title="Second language">
Second language

A person's second language, or L2, is a language that is not the native language (first language or L1) of the speaker, but is learned later (usually as a foreign language, but it can be another language used in the speaker's home country). For example, there are two official languages of Canada (English and French) and some people use both.

A speaker's dominant language, which is the language a speaker uses most or is most comfortable with, is not necessarily the speaker's first language. The second language can also be the dominant one. For example, the Canadian census defines first language for its purposes as "the first language learned in childhood and still spoken", recognizing that for some, the earliest language may be lost, a process known as language attrition. This can happen when young children move to a new language environment.

The distinction between acquiring and learning was made by Stephen Krashen (1982) as part of his Monitor Theory. According to Krashen, the "acquisition" of a language is a natural process; whereas "learning" a language is a conscious one. In the former, the student needs to partake in natural communicative situations. In the latter, error correction is present, as is the study of grammatical rules isolated from natural language. Not all educators in second language agree to this distinction; however, the study of how a second language is "learned/acquired" is referred to as "second-language acquisition" (SLA).

Research in SLA "...focuses on the developing knowledge and use of a language by children and adults who already know at least one other language... [and] a knowledge of second-language acquisition may help educational policy makers set more realistic goals for programmes for both foreign language courses and the learning of the majority language by minority language children and adults." (Spada & Lightbown, p. 115).

SLA has been influenced by both linguistic and psychological theories. One of the dominant linguistic theories hypothesizes that a "device" or "module" of sorts in the brain contains innate knowledge. Many psychological theories, on the other hand, hypothesize that cognitive mechanisms, responsible for much of human learning, process language.

Other dominant theories and points of research include 2nd language acquisition studies (which examine if L1 findings can be transferred to L2 learning), verbal behaviour (the view that constructed linguistic stimuli can create a desired speech response), morpheme studies, behaviourism, error analysis, stages and order of acquisition, structuralism (approach that looks at how the basic units of language relate to each other according to their common characteristics), 1st language acquisition studies, contrastive analysis (approach where languages were examined in terms of differences and similarities) and inter-language (which describes L2 learners’ language as a rule-governed, dynamic system) (Mitchell, Myles, 2004). 
These theories have all influenced second-language teaching and pedagogy. There are many different methods of second-language teaching, many of which stem directly from a particular theory. Common methods are the grammar-translation method, the direct method, the audio-lingual method (clearly influenced by audio-lingual research and the behaviourist approach), the Silent Way, Suggestopedia, community language learning, the Total Physical Response method, and the communicative approach (highly influenced by Krashen’s theories) (Doggett, 1994). Some of these approaches are more popular than others, and are viewed to be more effective. Most language teachers do not use one singular style, but will use a mix in their teaching. This provides a more balanced approach to teaching and helps students of a variety of learning styles succeed.

The defining difference between a first language (L1) and a second language (L2) is the age the person learned the language. For example, linguist Eric Lenneberg used "second language" to mean a language consciously acquired or used by its speaker after puberty. In most cases, people never achieve the same level of fluency and comprehension in their second languages as in their first language. These views are closely associated with the critical period hypothesis.

In acquiring an L2, Hyltenstam (1992) found that around the age of six or seven seemed to be a cut-off point for bilinguals to achieve native-like proficiency. After that age, L2 learners could get "near-native-like-ness" but their language would, while consisting of few actual errors, have enough errors to set them apart from the L1 group. The inability of some subjects to achieve native-like proficiency must be seen in relation to the "age of onset" (AO). Later, Hyltenstam & Abrahamsson (2003) modified their age cut-offs to argue that after childhood, in general, it becomes more and more difficult to acquire native-like-ness, but that there is no cut-off point in particular.<br>

As we are learning more and more about the brain, there is a hypothesis that when a child is going through puberty, that is the time that accents "start". Before a child goes through puberty, the chemical processes in the brain are more geared towards language and social communication. Whereas after puberty, the ability for learning a language without an accent has been rerouted to function in another area of the brain—most likely in the frontal lobe area promoting cognitive functions, or in the neural system of hormone allocated for reproduction and sexual organ growth.

As far as the relationship between age and eventual attainment in SLA is concerned, Krashen, Long, and Scarcella, say that people who encounter foreign language in early age, begin natural exposure to second languages and obtain better proficiency than those who learn the second language as an adult. However, when it comes to the relationship between age and rate SLA, “Adults proceed through early stages of syntactic and morphological development faster than children (where time and exposure are held constant)” (Krashen, Long, Scarcella 573). Also, “older children acquire faster than younger children do (again, in early stages of morphological and syntactic development where time and exposure are held constant)” (573). In other words, adults and older children are fast learners when it comes to the initial stage of foreign language education.

Gauthier and Genesee (2011) have done a research which mainly focuses on the second language acquisition of internationally adopted children and results show that early experiences of one language of children can affect their ability to acquire a second language, and usually children learn their second language slower and weaker even during the critical period.<br>

As for the fluency, it is better to do foreign language education at an early age, but being exposed to a foreign language since an early age causes a “weak identification” (Billiet, Maddens and Beerten 241). Such issue leads to a "double sense of national belonging," that makes one not sure of where he or she belongs to because according to Brian A. Jacob, multicultural education affects students' "relations, attitudes, and behaviors" (Jacob 364). And as children learn more and more foreign languages, children start to adapt, and get absorbed into the foreign culture that they “undertake to describe themselves in ways that engage with representations others have made” (Pratt 35). Due to such factors, learning foreign languages at an early age may incur one’s perspective of his or her native country.
Acquiring a second language can be a lifelong learning process for many. Despite persistent efforts, most learners of a second language will never become fully "native-like" in it, although with practice considerable fluency can be achieved. However, children by around the age of 5 have more or less mastered their first language with the exception of vocabulary and a few grammatical structures, and the process is relatively very fast because language is a very complex skill. Moreover, if children start to learn a second language when they are 7 years old or younger, they will also be fully fluent with their second language in a faster speed comparing to the speed of learning by adults who start to learn a second language later in their life.<br>

In the first language, children do not respond to systematic correction. Furthermore, children who have limited input still acquire the first language, which is a significant difference between input and output. Children are exposed to a language environment of errors and lack of correction but they end up having the capacity to figure out the grammatical rules. Error correction does not seem to have a direct influence on learning a second language. Instruction may affect the rate of learning, but the stages remain the same. Adolescents and adults who know the rule are faster than those who do not.

In the learning of a second language the correction of errors remains a controversial topic with many differing schools of thought. Throughout the last century much advancement has been made in research on the correction of students’ errors. In the 1950s and 60s the viewpoint of the day was that all errors must be corrected at all costs. Little thought went to students’ feelings or self-esteem in regards to this constant correction (Russell, 2009).

In the 1970s Dulay and Burt’s studies showed that learners acquire grammar forms and structures in a pre-determined, inalterable order, and that teaching or correcting styles would not change this (Russell, 2009).

In this same decade Terrell (1977) did studies that showed that there were more factors to be considered in the classroom than the cognitive processing of the students (Russell, 2009). He contested that the affective side of students and their self-esteem were equally important to the teaching process (Russell, 2009).

A few years later in the 1980s, the strict grammar and corrective approach of the 1950s became obsolete. Researchers asserted that correction was often unnecessary and that instead of furthering students’ learning it was hindering them (Russell, 2009). The main concern at this time was relieving student stress and creating a warm environment for them. Stephen Krashen was a big proponent in this hands-off approach to error correction (Russell, 2009).

The 1990s brought back the familiar idea that explicit grammar instruction and error correction was indeed useful for the SLA process. At this time, more research started to be undertaken to determine exactly which kinds of corrections are the most useful for students. In 1998, Lyster concluded that “recasts” (when the teacher repeats a student’s incorrect utterance with the correct version) are not always the most useful because students do not notice the correction (Russell, 2009). His studies in 2002 showed that students learn better when teachers help students recognize and correct their own errors (Russell, 2009). Mackey, Gas and McDonough had similar findings in 2000 and attributed the success of this method to the student’s active participation in the corrective processes.

According to Noam Chomsky, children will bridge the gap between input and output by their innate grammar because the input (utterances they hear) is so poor but all children end up having complete knowledge of grammar. Chomsky calls it the Poverty of Stimulus. And second language learners can do this by applying the rules they learn to the sentence-construction, for example. So learners in both their native and second language have knowledge that goes beyond what they have received, so that people can make correct utterances (phrases, sentences, questions, etc) that they have never learned or heard before.<br>

Bilingualism has been an advantage to today's world and being bilingual gives the opportunity to understand and communicate with people with different cultural backgrounds. However, a study done by Optiz and Degner in 2012 shows that sequential bilinguals (i.e. learn their L2 after L1) often relate themselves to the emotions more when they perceive these emotions by their first language/native language/L1, but feel less emotional when by their second language even though they know the meaning of words clearly. The emotional distinction between L1 and L2 indicates that the "effective valence" of words is processed less immediate in L2 because of the delayed vocabulary/lexical access to these two languages.<br>

Success in language learning can be measured in two ways: likelihood and quality. First language learners "will" be successful in both measurements. It is inevitable that all people will learn a first language and with few exceptions, they will be fully successful. For second language learners, success is not guaranteed. For one, learners may become fossilized or "stuck" as it were with ungrammatical items. (Fossilization occurs when language errors become a permanent feature. See Canale & Swain (1980), Johnson (1992), Selinker (1972), and Selinker and Lamendella (1978).) The difference between learners may be significant. As noted elsewhere, L2 learners rarely achieve complete "native-like" control of the second language.

For L2 pronunciation, there are two principles that haven been put forth by Levis (2005). The first is nativeness which means the speaker's ability to approximately reach the speaking pattern of the second language of speakers; and the second, understanding, refers to the speaker's ability to make themselves understood.
Being successful in learning a second language is often found to be challenging for some individuals. Research has been done to look into why some students are more successful than others. Stern (1975), Rubin (1975) and Reiss (1985) are just a few of the researchers who have dedicated time to this subject. They have worked to determine what qualities make a "good language learner" (Mollica, Neussel, 1997). Some of their common findings are that a good language learner uses positive learning strategies, is an active learner who is constantly searching for meaning. Also a good language learner demonstrates a willingness to practice and use the language in real communication. He also monitors himself and his learning, has a strong drive to communicate, and has a good ear and good listening skills (Mollica, Neussel, 1997).<br>

Özgür and Griffiths have designed an experiment in 2013 about the relationship between different motivations and second language acquisition. They looked at four types of motivations—intrinsic (inner feelings of learner), extrinsic (reward from outside), integrative (attitude towards learning), and instrumental (practical needs). According to the test results, the intrinsic part has been the main motivation for these student who learn English as their second language. However, students report themselves being strongly instrumentally motivated. In conclusion, learning a second language and being successful depend on every individual.<br>

In pedagogy and sociolinguistics, a distinction is made between second language and foreign language, the latter is being learned for use in an area where that language is originally from another country and not spoken in the native country of the speakers. And in other words, foreign language is used from the perspective of countries; the second language is used from the perspective of individuals.

For example, English in countries such as India, Pakistan, Bangladesh, the Philippines, the Nordic countries and the Netherlands is considered a second language by many of its speakers, because they learn it young and use it regularly; indeed in parts of southern Asia it is the official language of the courts, government and business. The same can be said for French in Algeria, Morocco and Tunisia, although French is not an official language in any of them. In practice, French is widely used in a variety of contexts in these countries, and signs are normally printed in both Arabic and French. A similar phenomenon exists in post-Soviet states such as Ukraine, Uzbekistan, Kyrgyzstan and Kazakhstan, where Russian can be considered a second language, and there are large Russophone communities.

However, in China (with the possible exception of Hong Kong), English must be considered a foreign language due to the lack of opportunities for use, such as historical links, media, conversation between people, and common vocabulary. Likewise, French would be considered a foreign language in Romania and Moldova, even though both French and Romanian are Romance languages, Romania's historical links to France, and all being members of la Francophonie.

Psychological studies have found that speaking two or more languages is beneficial for people's cognitive process and the differences between brains of bilinguals and single language speakers usually provides some mental benefits, according to an article in the "Daily Telegraph" in 2013. Including but not limited to these:


George H. J. Weber, a Swiss businessman and independent scholar, founder of the Andaman Association and creator of the encyclopedic andaman.org Web site, made a report in December 1997 about the number of secondary speakers of the world's leading languages. Weber used the Fischer Weltalmanach of 1986 as his primary and only source for the L2-speakers data, in preparing the data in the following table. These numbers are here compared with those referred to by Ethnologue, a popular source in the linguistics field. See below Table 1.

Collecting the number of second language speakers of every language is extremely difficult and even the best estimates contain guess work. The data below are from ethnologue.com as of June 2013.




</doc>
<doc id="25490263" url="https://en.wikipedia.org/wiki?curid=25490263" title="Speech repetition">
Speech repetition

Speech repetition is when one individual speaks the sounds they've heard another person pronounce or say. In other words, it is the saying by one individual of the spoken vocalizations made by another individual. Speech repetition requires person repeating the utterance to have the ability to map the sounds they hear from the other person's oral pronunciation to similar places and manners of articulation in their own vocal tract.

Such speech input/output imitation often occurs independently of speech comprehension; such as in speech shadowing when a person automatically says words heard in earphones, and the pathological condition of echolalia in which people reflexively repeat overheard words. This links to speech repetition of words being separate in the brain to speech perception. Speech repetition occurs in the dorsal speech processing stream while speech perception occurs in the ventral speech processing stream. Repetitions are often incorporated unawares by this route into spontaneous novel sentences immediately or after delay following storage in phonological memory.

In humans, the ability to map heard input vocalizations into motor output is highly developed due to this copying ability playing a critical role in a child's rapid expansion of their spoken vocabulary. In older children and adults it still remains important as it enables the continued learning of novel words and names and additional languages. Such repetition is also necessary for the propagation of language from generation to generation. It has also been suggested that the phonetic units out of which speech is made have been selected upon by the process of vocabulary expansion and vocabulary transmissions due to children preferentially copying words in terms of more easily imitated elementary units.

Vocal imitation happens quickly: words can be repeated within 250-300 milliseconds both in normals (during speech shadowing) and during echolalia. The imitation of speech syllables possibly happens even quicker: people begin imitating the second phone in the syllable [ao] earlier than they can identify it (out of the set [ao], [aæ] and [ai]). Indeed, "...simply executing a shift to [o] upon detection of a second vowel in [ao] takes very little longer than does interpreting and executing it as a shadowed response". Neurobiologically this suggests "...that the early phases of speech analysis yield information which is directly convertible to information required for speech production". Vocal repetition can be done immediately as in speech shadowing and echolalia. It can also be done after the pattern of pronunciation is stored in short-term memory or long-term memory. It automatically uses both auditory and where available visual information about how a word is produced.

The automatic nature of speech repetition was noted by Carl Wernicke, the late nineteenth century neurologist, who observed that "The primary speech movements, enacted before the development of consciousness, are reflexive and mimicking in nature..".

Vocal imitiation arises in development before speech comprehension and also babbling: 18-week-old infants spontaneously copy vocal expressions provided the accompanying voice matches. Imitation of vowels has been found as young as 12 weeks. It is independent of native language, language skills, word comprehension and a speaker's intelligence. Many autistic and some mentally disabled people engage in the echolalia of overheard words (often their only vocal interaction with others) without understanding what they echo. Reflex uncontrolled echoing of others words and sentences occurs in roughly half of those with Gilles de la Tourette syndrome. The ability to repeat words without comprehension also occurs in mixed transcortical aphasia where it links to the sparing of the short-term phonological store.

The ability to repeat and imitate speech sounds occurs separately to that of normal speech. Speech shadowing provides evidence of a 'privileged' input/output speech loop that is distinct to the other components of the speech system. Neurocognitive research likewise finds evidence of a direct (nonlexical) link between phonological analysis input and motor programming output.

Speech sounds can be imitatively mapped into vocal articulations in spite of vocal tract anatomy differences in size and shape due to gender, age and individual anatomical variability. Such variability is extensive making input output mapping of speech more complex than a simple mapping of vocal track movements. The shape of the mouth varies widely: dentists recognize three basic shapes of palate: trapezoid, ovoid, and triagonal; six types of malocclusion between the two jaws; nine ways teeth relate to the dental arch and a wide range of maxillary and mandible deformities. Vocal sound can also vary due to dental injury and dental caries. Other factors that do not impede the sensory motor mapping needed for vocal imitation are gross oral deformations such as hare-lips, cleft palates or amputations of the tongue tip, pipe smoking, pencil biting and teeth clinching (such as in ventriloquism). Paranasal sinuses vary between individuals 20-fold in volume, and differ in the presence and the degree of their asymmetry.

Vocal imitation occurs potentially in regard to a diverse range of phonetic units and types of vocalization. The world's languages use consonantal phones that differ in thirteen imitable vocal tract place of articulations (from the lips to the glottis). These phones can potentially be pronounced with eleven types of imitable manner of articulations (nasal stops to lateral clicks). Speech can be copied in regard to its social accent, intonation, pitch and individuality (as with entertainment impersonators). Speech can be articulated in ways which diverge considerably in speed, timbre, pitch, loudness and emotion. Speech further exists in different forms such as song, verse, scream and whisper. Intelligible speech can be produced with pragmatic intonation and in regional dialects and foreign accents. These aspects are readily copied: people asked to repeat speech-like words imitate not only phones but also accurately other pronunciation aspects such as fundamental frequency, schwa-syllable expression, voice spectra and lip kinematics, voice onset times, and regional accent.

In 1874 Carl Wernicke proposed that the ability to imitate speech plays a key role in language acquisition. This is now a widely researched issue in child development. A study of 17,000 one and two word utterances made by six children between 18 months to 25 months found that, depending upon the particular infant, between 5% and 45% of their words might be mimicked. These figures are minima since they concern only immediately heard words. Many words that may seem spontaneous are in fact delayed imitations heard days or weeks previously. At 13 months children who imitate new words (but not ones they already know) show a greater increase in noun vocabulary at four months and non noun vocabulary at eight months. A major predictor of vocabulary increase in both 20 months, 24 months, and older children between 4 and 8 years is their skill in repeating nonword phone sequences (a measure of mimicry and storage). This is also the case with children with Down's syndrome . The effect is larger than even age: in a study of 222 two-year-old children that had spoken vocabularies ranging between 3–601 words the ability to repeat nonwords accounted for 24% of the variance compared to 15% for age and 6% for gender (girls better than boys).

Imitation provides the basis for making longer sentences than children could otherwise spontaneously make on their own. Children analyze the linguistic rules, pronunciation patterns, and conversational pragmatics of speech by making monologues (often in crib talk) in which they repeat and manipulate in word play phrases and sentences previously overheard. Many proto-conversations involve children (and parents) repeating what each other has said in order to sustain social and linguistic interaction. It has been suggested that the conversion of speech sound into motor responses helps aid the vocal "alignment of interactions" by "coordinating the rhythm and melody of their speech". Repetition enables immigrant monolingual children to learn a second language by allowing them to take part in 'conversations'. Imitation related processes aids the storage of overheard words by putting them into speech based short- and long-term memory.

The ability to repeat nonwords predicts the ability to learn second-language vocabulary. A study found that adult polyglots performed better in short-term memory tasks such as repeating nonword vocalizations compared to nonpolyglots though both are otherwise similar in general intelligence, visuo-spatial short-term memory and paired-associate learning ability. Language delay in contrast links to impairments in vocal imitation.

Electrical brain stimulation research upon the human brain finds that 81% of areas that show disruption of phone identification are also those in which the imitating of oral movements is disrupted and vice versa; Brain injuries in the speech areas show a 0.9 correlation between those causing impairments to the copying of oral movements and those impairing phone production and perception.

Spoken words are sequences of motor movements organized around vocal tract gesture motor targets. Vocalization due to this is copied in terms of the motor goals that organize it rather than the exact movements with which it is produced. These vocal motor goals are auditory. According to James Abbs 'For speech motor actions, the individual articulatory movements would not appear to be controlled with regard to three- dimensional spatial targets, but rather with regard to their contribution to complex vocal tract goals such as resonance properties (e.g., shape, degree of constriction) and or aerodynamically significant variables'. Speech sounds also have duplicable higher-order characteristics such as rates and shape of modulations and rates and shape of frequency shifts. Such complex auditory goals (which often link—though not always—to internal vocal gestures) are detectable from the speech sound which they create.

Two cortical processing streams exist: a ventral one which maps sound onto meaning, and a dorsal one, that maps sound onto motor representations. The dorsal stream projects from the posterior Sylvian fissure at the temporoparietal junction, onto frontal motor areas, and is not normally involved in speech perception. 
Carl Wernicke identified a pathway between the left posterior superior temporal sulcus (a cerebral cortex region sometimes called the Wernicke's area) as a centre of the sound "images" of speech and its syllables that connected through the arcuate fasciculus with part of the inferior frontal gyrus (sometimes called the Broca's area) responsible for their articulation. This pathway is now broadly identified as the dorsal speech pathway, one of the two pathways (together with the ventral pathway) that process speech. The posterior superior temporal gyrus is specialized for the transient representation of the phonetic sequences used for vocal repetition. Part of the auditory cortex also can represent aspects of speech such as its consonantal features.

Mirror neurons have been identified that both process the perception and production of motor movements. This is done not in terms of their exact motor performance but an inference of the intended motor goals with which it is organized. Mirror neurons that both perceive and produce the motor movements of speech have been identified. Speech is mirrored constantly into its articulations since speakers cannot know in advance that a word is unfamiliar and in need of repetition—which is only learnt after the opportunity to map it into articulations has gone. Thus, speakers if they are to incorporate unfamiliar words into their spoken vocabulary
must by default map all spoken input.
Words in sign languages, unlike those in spoken ones, are made not of sequential units but of spatial configurations of subword unit arrangements, the spatial analogue of the sonic-chronological morphemes of spoken language. These words, like spoken ones, are learnt by imitation. Indeed, rare cases of compulsive sign-language echolalia exist in otherwise language-deficient deaf autistic individuals born into signing families. At least some cortical areas neurobiologically active during both sign and vocal speech, such as the auditory cortex, are associated with the act of imitation.

Birds learn their songs from those made by other birds. In several examples, birds show highly developed repetition abilities: the Sri Lankan Greater racket-tailed drongo ("Dicrurus paradiseus") copies the calls of predators and the alarm signals of other birds Albert's lyrebird ("Menura alberti") can accurately imitate the satin bowerbird ("Ptilonorhynchus violaceus"),

Research upon avian vocal motor neurons finds that they perceive their song as a series of articulatory gestures as in humans. Birds that can imitate humans, such as the Indian hill myna (Gracula religiosa), imitate human speech by mimicking the various speech formants, created by changing the shape of the human vocal tract, with different vibration frequencies of its internal tympaniform membrane. Indian hill mynahs also imitate such phonetic characteristics as voicing, fundamental frequencies, formant transitions, nasalization, and timing, through their vocal movements are made in a different way from those of the human vocal apparatus.


Apes taught language show an ability to imitate language signs with chimpanzees such as Washoe who was able to learn with his arms a vocabulary of 250 American Sign Language gestures. However, such human trained apes show no ability to imitate human speech vocalizations.



</doc>
<doc id="1854841" url="https://en.wikipedia.org/wiki?curid=1854841" title="Pivot language">
Pivot language

A pivot language, sometimes also called a bridge language, is an artificial or natural language used as an intermediary language for translation between many different languages – to translate between any pair of languages A and B, one translates A to the pivot language P, then from P to B. Using a pivot language avoids the combinatorial explosion of having translators across every combination of the supported languages, as the number of combinations of language is linear (formula_1), rather than quadratic formula_2 – one need only know the language A and the pivot language P (and someone else the language B and the pivot P), rather than needing a different translator for every possible combination of A and B.

The disadvantage of a pivot language is that each step of retranslation introduces possible mistakes and ambiguities – using a pivot language involves two steps, rather than one. For example, when Hernán Cortés communicated with Mesoamerican Indians, he would speak Spanish to Gerónimo de Aguilar, who would speak Mayan to Malintzin, who would speak Nahuatl to the locals.

English, French, Russian, and Arabic are often used as pivot languages. Interlingua has been used as a pivot language in international conferences and has been proposed as a pivot language for the European Union. Esperanto was proposed as a pivot language in the Distributed Language Translation project and has been used in this way in the Majstro Tradukvortaro at the Esperanto website Majstro.com. The Universal Networking Language is an artificial language specifically designed for use as a pivot language.

Pivot coding is also a common method of translating data for computer systems. For example, the internet protocol, XML and high level languages are pivot codings of computer data which are then often rendered into internal binary formats for particular computer systems.

Unicode was designed to be usable as a pivot coding between various major existing character encodings, though its widespread adoption as a coding in its own right has made this usage unimportant.

Current statistical machine translation (SMT) systems use parallel corpora for source (s) and target (t) languages to achieve their good results, but good parallel corpora are not available for all languages. A pivot language (p) enables the bridge between two languages, to which existing parallel corpora are entirely or partially not yet at hand.

Pivot translation can be problematic because of the potential lack of fidelity of the information forwarded in the use of different corpora. From the use of two bilingual corpora (s-p & p-t) to set up the s-t bridge, linguistic data are inevitably lost. Rule-based machine translation (RBMT) helps the system rescue this information, so that the system does not rely entirely on statistics but also on structural linguistic information.

Three basic techniques are used to employ pivot language in machine translation: (1) "triangulation", which focuses on phrase paralleling between source and pivot (s-p) and between pivot and target (p-t); (2) "transfer", which translates the whole sentence of the source language to a pivot language and then to the target language; and (3) "synthesis", which builds a corpus of its own for system training.

The triangulation method (also called "phrase table multiplication") calculates the probability of both translation correspondences and lexical weight in s-p and p-t, to try to induce a new s-t phrase table. The transfer method (also called "sentence translation strategy") simply carries a straightforward translation of s into p and then another translation of p into t without using probabilistic tests (as in triangulation). The synthetic method uses an existing corpus of s and tries to build an own synthetic corpus out of it that is used by the system to train itself. Then a bilingual s-p corpus is synthesized to enable a p-t translation.

A direct comparison between triangulation and transfer methods for SMT systems has shown that triangulation achieves much better results than transfer.

All three pivot language techniques enhance the performance of SMT systems. However, the "synthetic" technique doesn't work well with RBMT, and systems' performances are lower than expected. Hybrid SMT/RBMT systems achieve better translation quality than strict-SMT systems that rely on bad parallel corpora.

The key role of RBMT systems is that they help fill the gap left in the translation process of s-p → p-t, in the sense that these parallels are included in the SMT model for s-t.



</doc>
<doc id="33674235" url="https://en.wikipedia.org/wiki?curid=33674235" title="Language complexity">
Language complexity

Language complexity is a topic in linguistics which can be divided into several sub-topics such as phonological, morphological, syntactic, and semantic complexity.

Language complexity has been studied less than many other traditional fields of linguistics. While the consensus is turning towards recognizing that complexity is a suitable research area, a central focus has been on methodological choices. Some languages, particularly pidgins and creoles, are considered simpler than most other languages, but there is no direct ranking, and no universal method of measurement although several possibilities are now proposed within different schools of analysis.

Throughout the 19th century, differential complexity was taken for granted. The classical languages Latin and Greek, as well as Sanskrit, were considered to possess qualities which could be achieved by the rising European national languages only through an elaboration that would give them the necessary structural and lexical complexity that would meet the requirements of an advanced civilization. At the same time, languages described as 'primitive' were naturally considered to reflect the simplicity of their speakers. On the other hand, Friedrich Schlegel noted that some nations "which appear to be at the very lowest grade of intellectual culture", such as Basque, Sámi and some native American languages, possess a striking degree of elaborateness.

Darwin considered the apparent complexity of many non-Western languages as problematic for evolution theory which in his time held that less advanced people should have less complex languages. Darwin's suggestion was that simplicity and irregularities were the result of extensive language contact while "the extremely complex and regular construction of many barbarous languages" should be seen as an utmost perfection of the one and same evolutionary process.

During the 20th century, linguists and anthropologists adopted a standpoint that would reject any nationalist ideas about superiority of the languages of establishment. The first known quote that puts forward the idea that all languages are equally complex comes from Rulon S. Wells III, 1954, who attributes it to Charles F. Hockett. Within a year, the same idea found its way to Encyclopædia Britannica:
While laymen never ceased to consider certain languages as simple and others as complex, such a view was erased from official contexts. For instance, the 1971 edition of Guinness Book of World Records featured Saramaccan, a creole language, as "the world's least complex language". According to linguists, this claim was "not founded on any serious evidence", and it was removed from later editions. Apparent complexity differences in certain areas were explained with a balancing force by which the simplicity in one area would be compensated with the complexity of another; e.g. David Crystal, 1987:
In 2001 the compensation hypothesis was eventually refuted by the creolist John McWhorter who pointed out the absurdity of the idea that, as languages change, each would have to include a mechanism that calibrates it according to the complexity of all the other 6,000 or so languages around the world. He underscored that linguistics has no knowledge of any such mechanism.

Revisiting the idea of differential complexity, McWhorter argued that it is indeed creole languages, such as Saramaccan, that are structurally "much simpler than all but very few older languages". In McWhorter's notion this is not problematic in terms of the equality of creole languages because simpler structures convey logical meanings in the most straightforward manner, while increased language complexity is largely a question of features which may not add much to the functionality, or improve usefulness, of the language. Examples of such features are inalienable possessive marking, switch-reference marking, syntactic asymmetries between matrix and subordinate clauses, grammatical gender, and other secondary features which are most typically absent in creoles.

During the years following McWhorter's article, several books and dozens of articles were published on the topic. As to date, there have been research projects on language complexity, and several workshops for researchers have been organised by various universities.

At a general level, language complexity can be characterized as the number and variety of elements, and the elaborateness of their interrelational structure. This general characterisation can be broken down into sub-areas:

Measuring complexity is considered difficult, and the comparison of whole natural languages as a daunting task. On a more detailed level, it is possible to demonstrate that some structures are more complex than others. Phonology and morphology are areas where such comparisons have traditionally been made. For instance, linguistics has tools for the assessment of the phonological system of any given language. As for the study of syntactic complexity, grammatical rules have been proposed as a basis, but generative frameworks, such as Minimalist Program and Simpler Syntax, have been less successful in defining complexity and its predictions than non-formal ways of description.

Many researchers suggest that several different concepts may be needed when approaching complexity: entropy, size, description length, effective complexity, information, connectivity, irreducibility, low probability, syntactic depth etc. Research suggests that while methodological choices affect the results, even rather crude analytic tools may provide a feasible starting point for measuring grammatical complexity.

Guy (1994) illustrates the point by comparing two Santo languages he has worked on that are about as closely related as French and Spanish, Tolomako and Sakao, both spoken in the village of Port Olry, Vanuatu. Because these languages are very similar to each other, and equally distant from English, he holds that neither is inherently biased as being seen as more easy or difficult by an English speaker (see difficulty of learning languages).

Sakao has more, and more difficult, vowel distinctions than Tolomako:
In addition, it has more and more difficult consonant distinctions:
Tolomako has a simple syllable structure, maximally consonant–vowel–vowel. It is not clear if Sakao even has syllables; that is, whether trying to divide Sakao words into meaningful syllables is even possible.
With inalienably possessed nouns, Tolomako inflections are consistently regular, whereas Sakao is full of irregular nouns:

Here Tolomako "mouth" is invariably ' and "hair" invariably "," whereas Sakao "mouth" is variably ' and "hair" variably "."

With deixis, Tolomako has three degrees (here/this, there/that, yonder/yon), whereas Sakao has seven.

Tolomako has a preposition to distinguish the object of a verb from an instrument; indeed, a single preposition, "ne," is used for all relationships of space and time. Sakao, on the other hand, treats both as objects of the verb, with a transitive suffix "" that shows the verb has two objects, but letting context disambiguate which is which:

The Sakao strategy involves polysynthetic syntax, as opposed to the isolating syntax of Tolomako:
Here ' "the bow" is the instrumental of ' "to shoot", and ' "the sea" is the direct object of ' "to follow", which because they are combined into a single verb, are marked as ditransitive with the suffix "." Because ' "to shoot" has the incorporated object ' "fish", the first consonant geminates for ""; "," being part of one word, then reduces to "." And indeed, the previous example of killing a pig could be put more succinctly, but grammatically more complexly, in Sakao by incorporating the object 'pig' into the verb:

Guy asks rhetorically, "Which of the two languages spoken in Port-Olry do you think the Catholic missionaries learnt and used? Could that possibly be because it was easier than the other?"

It is generally acknowledged that, as young languages, creoles are necessarily simpler than non-creoles. Guy believes this to be untrue ; after a comparison with Antillean Creole, he writes, "I assure you that it is far, far more complex than Tolomako!", despite being based on his native language, French.




</doc>
<doc id="34384656" url="https://en.wikipedia.org/wiki?curid=34384656" title="Metafunction">
Metafunction

The term metafunction originates in systemic functional linguistics and is considered to be a property of all languages. Systemic functional linguistics is functional and semantic rather than formal and syntactic in its orientation. As a functional linguistic theory, it claims that both the emergence of grammar and the particular forms that grammars take should be explained "in terms of the functions that language evolved to serve". While languages vary in how and what they do, and what humans do with them in the contexts of human cultural practice, all languages are considered to be shaped and organised in relation to three functions, or metafunctions. Michael Halliday, the founder of systemic functional linguistics, calls these three functions the "ideational", "interpersonal", and "textual". The ideational function is further divided into the "experiential" and "logical".

Metafunctions are "systemic clusters"; that is, they are groups of semantic systems that make meanings of a related kind. The three metafunctions are mapped onto the structure of the clause. For this reason, systemic linguists analyse a clause from three perspectives. Halliday argues that the concept of metafunction is one of a small set of principles that are necessary to explain how language works; this concept of function in language is necessary to explain the organisation of the semantic system of language. Function is considered to be "a fundamental property of language itself".

According to Ruqaiya Hasan, the metafunctions in SFL "are not hierarchised; they have equal status, and each is manifested in every act of language use: in fact, an important task for grammatics is to describe how the three metafunctions are woven together into the same linguistic unit". Hasan argues that this is one way in which Halliday's account of the functions of language is different from that of Karl Bühler, for example, for whom functions of language are hierarchically ordered, with the referential function the most important of all. For Buhler, the functions were considered to operate one at a time. In SFL, the metafunctions operate simultaneously, and any utterance is a harmony of choices across all three functions.

The ideational function is language concerned with building and maintaining a theory of experience. It includes the experiential function and the logical function.

The experiential function refers to the grammatical choices that enable speakers to make meanings about the world around us and inside us:

Halliday argues that it was through this process of humans making meaning from experience that language evolved. Thus, the human species had to "make sense of the complex world in which it evolved: to classify, or group into categories, the objects and events within its awareness". These categories are not given to us through our senses; they have to be "construed". In taking this position on the active role of grammar in construing "reality", Halliday was influenced by Whorf.

Halliday describes the logical function as those systems "which set up logical–semantic relationships between one clausal unit and another" The systems which come under the logical function are and . When two clauses are combined, a speaker chooses whether to give both clauses equal status, or to make one dependent on the other. In addition, a speaker chooses some meaning relation in the process of joining or binding clauses together. Halliday argues that the meanings we make in such processes are most closely related to the experiential function. For this reason, he puts the experiential and logical functions together into the ideational function.

The interpersonal function refers to the grammatical choices that enable speakers to enact their complex and diverse interpersonal relations. This tenet of systemic functional linguistics is based on the claim that a speaker not only talks about something, but is always talking to and with others. Language not only construes experience, but simultaneously acts out "the interpersonal encounters that are essential to our survival". Halliday argues that these encounters:

The grammatical systems that relate to the interpersonal function include Mood, Modality, and Polarity.

Halliday argues that both experiential and interpersonal functions are intricately organized, but that between the two "there is comparatively very little constraint". This means that "by and large, you can put any interactional 'spin' on any representational content". What allows meanings from these two modes to freely combine is the intercession of a third, distinct mode of meaning that Halliday refers to as the textual function. The term encompasses all of the grammatical systems responsible for managing the flow of discourse. These systems "create coherent text – text that coheres within itself and with the context of situation" They are both structural (involving choices relating to the ordering of elements in the clause), and non-structural (involving choices that create cohesive ties between units that have no structural bond). The relevant grammatical systems include Theme, Given and New, as well as the systems of cohesion, such as Reference, Substitution, and Ellipsis. Halliday argues that the textual function is distinct from both the experiential and interpersonal because its object is language itself. Through the textual function, language "creates a semiotic world of its own: a parallel universe, or 'virtual reality' in modern terms".


</doc>
<doc id="33957665" url="https://en.wikipedia.org/wiki?curid=33957665" title="Uncertain plural">
Uncertain plural

An uncertain plural occurs when a writer does not know in advance whether a word should be written in the singular or plural. For English nouns, this may be demonstrated by enclosing the trailing "s" in parentheses, such as "book(s)".

In the case of articles, "they" or "their" may be used to include a single individual, when uncertain.



</doc>
<doc id="5767788" url="https://en.wikipedia.org/wiki?curid=5767788" title="Problem of religious language">
Problem of religious language

The problem of religious language considers whether it is possible to talk about God meaningfully if the traditional conceptions of God as being incorporeal, infinite, and timeless, are accepted. Because these traditional conceptions of God make it difficult to describe God, religious language has the potential to be meaningless. Theories of religious language either attempt to demonstrate that such language is meaningless, or attempt to show how religious language can still be meaningful.

Traditionally, religious language has been explained as via negativa, analogy, symbolism, or myth, each of which describes a way of talking about God in human terms. The "via negativa" is a way of referring to God according to what God is not; analogy uses human qualities as standards against which to compare divine qualities; symbolism is used non-literally to describe otherwise ineffable experiences; and a mythological interpretation of religion attempts to reveal fundamental truths behind religious stories. Alternative explanations of religious language cast it as having political, performative, or imperative functions.

Empiricist David Hume's requirement that claims about reality must be verified by evidence influenced the logical positivist movement, particularly the philosopher A. J. Ayer. The movement proposed that, for a statement to hold meaning, it must be possible to verify its truthfulness empirically – with evidence from the senses. Consequently, the logical positivists argued that religious language must be meaningless because the propositions it makes are impossible to verify. Austrian philosopher Ludwig Wittgenstein has been regarded as a logical positivist by some academics because he distinguished between things that can and cannot be spoken about; others have argued that he could not have been a logical positivist because he emphasised the importance of mysticism. British philosopher Antony Flew proposed a similar challenge based on the principle that, in so far as assertions of religious belief cannot be empirically falsified, religious statements are rendered meaningless.

The analogy of games – most commonly associated with Ludwig Wittgenstein – has been proposed as a way of establishing meaning in religious language. The theory asserts that language must be understood in terms of a game: just as each game has its own rules determining what can and cannot be done, so each context of language has its own rules determining what is and is not meaningful. Religion is classified as a possible and legitimate language game which is meaningful within its own context. Various parables have also been proposed to solve the problem of meaning in religious language. R. M. Hare used his parable of a lunatic to introduce the concept of "bliks" – unfalsifiable beliefs according to which a worldview is established – which are not necessarily meaningless. Basil Mitchell used a parable to show that faith can be logical, even if it seems unverifiable. John Hick used his parable of the Celestial City to propose his theory of eschatological verification, the view that if there is an afterlife, then religious statements will be verifiable after death.

Religious language is a philosophical problem arising from the difficulties in accurately describing God. Because God is generally conceived as incorporeal, infinite, and timeless, ordinary language cannot always apply to that entity. This makes speaking about or attributing properties to God difficult: a religious believer might simultaneously wish to describe God as good, yet also hold that God's goodness is unique and cannot be articulated by human language of goodness. This raises the problem of how (and whether) God can be meaningfully spoken about at all, which causes problems for religious belief since the ability to describe and talk about God is important in religious life. The French philosopher Simone Weil expressed this problem in her work "Waiting for God", in which she outlined her dilemma: she was simultaneously certain of God's love and conscious that she could not adequately describe him.

The medieval doctrine of divine simplicity also poses problems for religious language. This suggests that God has no accidental properties – these are properties that a being can have which do not contribute to its essence. If God has no accidental properties, he cannot be as he is traditionally conceived, because properties such as goodness are accidental. If divine simplicity is accepted, then to describe God as good would entail that goodness and God have the same definition. Such limits can also be problematic to religious believers; for example, the Bible regularly ascribes different emotions to God, ascriptions which would be implausible according to the doctrine of divine simplicity.

The theologian Sallie McFague believes that the more recent problem of religious language is based on individual experience, owing to the increased secularisation of society. She notes that human experience is of this world rather than regular encounters with the divine, which makes the experience of God uncommon and potentially unnecessary. Because of this, she argues, religious language is both idolatrous because it fails to express sufficient awe of God, and irrelevant because without adequate words it becomes meaningless.

Jewish philosopher Maimonides believed that God can only be ascribed negative attributes, a view based on two fundamental Jewish beliefs: that the existence of God must be accepted, and that it is forbidden to describe God. Maimonides believed that God is simple and so cannot be ascribed any essential attributes. He therefore argued that statements about God must be taken negatively, for example, "God lives" should be taken as "God does not lack vitality". Maimonides did not believe that God holds all of his attributes perfectly and without impairment; rather, he proposed that God lies outside of any human measures. To say that God is powerful, for example, would mean that God's power is beyond worldly power, and incomparable to any other power. In doing so, Maimonides attempted to illustrate God's indescribable nature and draw attention to the linguistic limits of describing God.

Thomas Aquinas argued that statements about God are analogous to human experience. An analogous term is partly univocal (has only one meaning) and partly equivocal (has more than one potential meaning) because an analogy is in some ways the same and in some ways different from the subject. He proposed that those godly qualities which resemble human qualities are described analogously, with reference to human terms; for example, when God is described as good, it does not mean that God is good in human terms, but that human goodness is used as a reference to describe God's goodness.

Philosopher Taede Smedes argued that religious language is symbolic. Denying any conflict between science and religion, he proposes that 'to believe' means to accept a conviction (that God exists, in the context of Christianity), which is different from 'knowing', which only occurs once something is proven. Thus, according to Smedes, we believe things that we do not know for sure. Smedes argues that, rather than being part of the world, God is so far beyond the world that there can be no common standard to which both God and the world can be compared. He argues that people can still believe in God, even though he cannot be compared to anything in the world, because belief in God is just an alternative way of viewing that world (he likens this to two people viewing a painting differently). Smedes claims that there should be no reason to look for a meaning behind our metaphors and symbols of God because the metaphors are all we have of God. He suggests that we can only talk of God "pro nobis" (for us) and not "in se" (as such) or "sine nobis" (without us). The point, he argues, is not that our concept of God should correspond with reality, but that we can only conceive of God through metaphors.

In the twentieth century, Ian Ramsey developed the theory of analogy, a development later cited in numerous works by Alister McGrath. He argued that various models of God are provided in religious writings that interact with each other: a range of analogies for salvation and the nature of God. Ramsey proposed that the models used modify and qualify each other, defining the limits of other analogies. As a result, no one analogy on its own is sufficient, but the combination of every analogy presented in Scripture gives a full and consistent depiction of God. The use of other analogies may then be used to determine if any one model of God is abused or improperly applied.

Philosopher Paul Tillich argued that religious faith is best expressed through symbolism because a symbol points to a meaning beyond itself and best expresses transcendent religious beliefs. He believed that any statement about God is symbolic and participates in the meaning of a concept. Tillich used the example of a national flag to illustrate his point: a flag points to something beyond itself, the country it represents, but also participates in the meaning of the country. He believed that symbols could unite a religious believer with a deeper dimension of himself as well as with a greater reality. Tillich believed that symbols must emerge from an individual collective unconsciousness, and can only function when they are accepted by the unconscious. He believed that symbols cannot be invented, but live and die at the appropriate times.

Louis Dupré differentiates between signs and symbols, proposing that a sign points to something while a symbol represents it. A symbol holds its own meaning: rather than merely pointing someone towards another object, it takes the place of and represents that object. He believes that a symbol has some ambiguity which does not exist with a sign. Dupré believes that a symbol may deserve respect because it contains what is signified within itself. A symbol reveals a reality beyond what is already perceived and transforms the ways the current reality is perceived. Dupré differentiates between religious and aesthetic symbols, suggesting that a religious symbol points towards something which "remains forever beyond our reach". He proposed that a religious symbol does not reveal the nature of what it signifies, but conceals it.

Langdon Brown Gilkey explained religious language and experience in terms of symbolism, identifying three characteristic features of religious symbolism which distinguish it from other language use. Firstly, religious symbolism has a double focus, referring both to something empirical and to something transcendent; Gilkey argued that the empirical manifestation points towards the transcendent being. Secondly, he believed that religious symbolism concerns fundamental questions of life, involving issues important to an individual or community. Finally, he argued that religious symbols provide standards by which life should be lived.

In the Sikh religious text the Guru Granth Sahib, religious language is used symbolically and metaphorically. In the text, Sikh Gurus repeat that the experiences they have while meditating are ineffable, incognizable, incomprehensible, and transensuous – this means that there is no object of their experience that can be conceptualised. To overcome this, the Sikh Gurus used symbolic and metaphorical language, assuming that there is a resemblance between the mystical experience of the divine (the sabad) and those experiencing it. For example, light is used to refer to the spiritual reality.

William Paden argued that religious language uses myth to present truths through stories. He argued that to those who practice a religion, myths are not mere fiction, but provide religious truths. Paden believed that a myth must explain something in the world with reference to a sacred being or force, and dismissed any myths which did not as "folktales". Using the example of creation myths, he differentiated myths from scientific hypotheses, the latter of which can be scientifically verified and do not reveal a greater truth; a myth cannot be analysed in the same way as a scientific theory.

Lutheran theologian Rudolf Bultmann proposed that the Bible contains existential content which is expressed through mythology; Bultmann sought to find the existential truths behind the veil of mythology, a task known as 'demythologising'. Bultmann distinguished between informative language and language with personal import, the latter of which commands obedience. He believed that God interacts with humans as the divine Word, perceiving a linguistic character inherent in God, which seeks to provide humans with self-understanding. Bultmann believed that the cultural embeddedness of the Bible could be overcome by demythologising the Bible, a process which he believed would allow readers to better encounter the word of God.

Christian philosopher John Hick believed that the language of the Bible should be demythologised to be compatible with naturalism. He offered a demythologised Christology, arguing that Jesus was not God incarnate, but a man with incredible experience of divine reality. To Hick, calling Jesus the Son of God was a metaphor used by Jesus' followers to describe their commitment to what Jesus represented. Hick believed that demythologising the incarnation would make sense of the variety of world religions and give them equal validity as ways to encounter God.

Islamic philosopher Carl Ernst has argued that religious language is often political, especially in the public sphere, and that its purpose is to persuade people and establish authority, as well as convey information. He explains that the modern criticisms of the West made by some sections of Islam are an ideological reaction to colonialism, which intentionally uses the same language as colonialists. Ernst argues that when it is used rhetorically, religious language cannot be taken at face value because of its political implications.

Peter Donovan argues that most religious language is not about making truth-claims; instead, it is used to achieve certain goals. He notes that language can be used in alternative ways beyond making statements of fact, such as expressing feelings or asking questions. Donovan calls many of these uses "performative", as they serve to perform a certain function within religious life. For example, the words "I promise" perform the action of promising themselves – Donovan argues that most religious language fulfils this function. Ludwig Wittgenstein also proposed that language could be performative and presented a list of the different uses of language. Wittgenstein argued that "the meaning of the language is in the use", taking the use of language to be performative. The philosopher J. L. Austin argued that religious language is not just cognitive but can perform social acts, including vows, blessings, and the naming of children. He distinguished performative statements as those that do not simply describe a state of affairs, but bring them about. Historian of religion Benjamin Ray uses the performance of rituals within religions as evidence for a performative interpretation of language. He argues that the language of rituals can perform social tasks: when a priest announces that a spiritual event has occurred, those present believe it because of the spiritual authority of the priest. He believed that the meaning of a ritual is defined by the language used by the speaker, who is defined culturally as a superhuman agent.

British philosopher R. B. Braithwaite attempted to approach religious language empirically and adopted Wittgenstein's idea of "meaning as use". He likened religious statements to moral statements because they are both non-descriptive yet still have a use and a meaning; they do not describe the world, but the believer's attitudes towards it. Braithwaite believed that the main difference between a religious and a moral statement was that religious statements are part of a linguistic system of stories, metaphors, and parables.

Professor Nathan Katz writes of the analogy of a burning building, used by the Buddha in the Lotus Sutra, which casts religious language as imperative. In the analogy, a father sees his children at the top of a burning building. He persuades them to leave, but only by promising them toys if they leave. Katz argues that the message of the parable is not that the Buddha has been telling lies; rather, he believes that the Buddha was illustrating the imperative use of language. Katz believes that religious language is an imperative and an invitation, rather than a truth-claim.

In the conclusion of his "Enquiry Concerning Human Understanding", Scottish philosopher David Hume argued that statements that make claims about reality must be verified by experience, and dismissed those that cannot be verified as meaningless. Hume regarded most religious language as unverifiable by experiment and so dismissed it.

Hume criticised the view that we cannot speak about God, and proposed that this view is no different from the skeptical view that God cannot be spoken about. He was unconvinced by Aquinas' theory of analogy and argued that God's attributes must be completely different from human attributes, making comparisons between the two impossible. Hume's scepticism influenced the logical positivist movement of the twentieth century.

The logical positivism movement originated in the Vienna Circle and was continued by British philosopher A. J. Ayer. The Vienna Circle adopted the distinction between analytic and synthetic statements: analytic statements are those whose meaning is contained within the words themselves, such as definitions, tautologies or mathematical statements, while synthetic statements make claims about reality. To determine whether a synthetic statement is meaningful, the Vienna Circle developed a verifiability theory of meaning, which proposed that for a synthetic statement to have cognitive meaning, its truthfulness must be empirically verifiable. Because claims about God cannot be empirically verified, the logical positivists argued that religious propositions are meaningless.

In 1936, Ayer wrote "Language, Truth and Logic", in which he claimed that religious language is meaningless. He put forward a strong empirical position, arguing that all knowledge must either come from observations of the world or be necessarily true, like mathematical statements. In doing so, he rejected metaphysics, which considers the reality of a world beyond the natural world and science. Because it is based on metaphysics and is therefore unverifiable, Ayer denounced religious language, as well as statements about ethics or aesthetics, as meaningless. Ayer challenged the meaningfulness of all statements about God – theistic, atheistic and agnostic – arguing that they are all equally meaningless because they all discuss the existence of a metaphysical, unverifiable being.

Austrian philosopher Ludwig Wittgenstein finished his "Tractatus Logico-Philosophicus" with the proposition that "Whereof one cannot speak, thereof one must be silent." Beverly and Brian Clack have suggested that because of this statement, Wittgenstein was taken for a positivist by many of his disciples because he made a distinction between what can and cannot be spoken about. They argue that this interpretation is inaccurate because Wittgenstein held the "mystical", which cannot be described, as important. Rather than dismissing the mystical as meaningless, as the logical positivists did, Wittgenstein believed that while the facts of the world remain the same, the perspective from which they are viewed will vary.

The falsification principle has been developed as an alternative theory by which it may be possible to distinguish between those religious statements that may potentially have meaning, and those that are meaningless. It proposes that most religious language is unfalsifiable because there is no way that it could be empirically proven false. In a landmark paper published in 1945, analytic philosopher Antony Flew argued that a meaningful statement must simultaneously assert and deny a state of affairs; for example, the statement "God loves us" both asserts that God loves us and denies that God does not love us. Flew maintained that if a religious believer could not say what circumstances would have to exist for their statements about God to be false, then they are unfalsifiable and meaningless.

Using John Wisdom's parable of the invisible gardener, Flew attempted to demonstrate that religious language is unfalsifiable. The parable tells the story of two people who discover a garden on a deserted island; one believes it is tended to by a gardener, the other believes that it formed naturally, without the existence of a gardener. The two watch out for the gardener but never find him; the non-believer consequently maintains that there is no gardener, whereas the believer rationalises the non-appearance by suggesting that the gardener is invisible and cannot be detected. Flew contended that if the believer's interpretation is accepted, nothing is left of the original gardener. He argued that religious believers tend to adopt counterpart rationalisations in response to any apparent challenge to their beliefs from empirical evidence; and these beliefs consequently suffer a "death by a thousand qualifications" as they are qualified and modified so much that they end up asserting nothing meaningful. Flew applied his principles to religious claims such as God's love for humans, arguing that if they are meaningful assertions they would deny a certain state of affairs. He argued that when faced with evidence against the existence of a loving God, such as the terminal illness of a child, theists will qualify their claims to allow for such evidence; for example they may suggest that God's love is different from human love. Such qualifications, Flew argued, make the original proposition meaningless; he questioned what God's love actually promises and what it guarantees against, and proposed that God's qualified love promises nothing and becomes worthless.

Flew continued in many subsequent publications to maintain the falsifiability criterion for meaning; but in later life retracted the specific assertion in his 1945 paper that all religious language is unfalsifiable, and so meaningless. Drawing specifically on the emerging science of molecular genetics (which had not existed at the time of his original paper), Flew eventually became convinced that the complexity this revealed in the mechanisms of biological reproduction might not be consistent with the time known to have been available for evolution on Earth to have happened; and that this potentially suggested a valid empirical test by which the assertion "that there is no creator God" might be falsified; "the latest work I have seen shows that the present physical universe gives too little time for these theories of abiogenesis to get the job done."

The analogy of a game was first proposed by Hans-Georg Gadamer in an attempt to demonstrate the epistemic unity of language. He suggested that language is like a game which everyone participates in and is played by a greater being. Gadamer believed that language makes up the fundamental structure of reality and that human language participates in a greater language; Christianity teaches this to be the divine word which created the world and was incarnate in Jesus Christ.

Ludwig Wittgenstein proposed a calculus theory of language, which maintained that all language should be analysable in a uniform way. Later in his life he rejected this theory, and instead proposed an alternative language-game analogy. He likened the differences in languages to the differences in games, arguing that just as there are many different games, each with different rules, so there are many different kinds of language. Wittgenstein argued that different forms of language have different rules which determine what makes a proposition meaningful; outside of its language-game, a proposition is meaningless. He believed that the meaning of a proposition depends on its context and the rules of that context. Wittgenstein presented a language game as a situation in which certain kinds of language are used. He provided some examples of language games: "Asking, thanking, greeting, cursing, praying".

Wittgenstein believed that religion is significant because it offers a particular way of life, rather than confirming the existence of God. He therefore believed that religious language is confessional – a confession of what someone feels and believes – rather than consisting of claims to truth. Wittgenstein believed that religious language is different from language used to describe physical objects because it occupies a different language game.

Dewi Zephaniah Phillips defended Wittgenstein's theory by arguing that although religious language games are autonomous, they should not be treated as isolated because they make statements about secular events such as birth and death. Phillips argued that because of this connection, religions can still be criticised based on human experiences of these secular events. He maintained that religion cannot be denounced as wrong because it is not empirical.

Peter Donovan criticises the language-games approach for failing to recognise that religions operate in a world containing other ideas and that many religious people make claims to truth. He notes that many religious believers not only believe their religion to be meaningful and true in its own context, but claim that it is true against all other possible beliefs; if the language games analogy is accepted, such a comparison between beliefs is impossible. Donovan proposes that debates between different religions, and the apologetics of some, demonstrates that they interact with each other and the wider world and so cannot be treated as isolated language games.

In response to Flew's falsification principle, British philosopher R. M. Hare told a parable in an attempt to demonstrate that religious language is meaningful. Hare described a lunatic who believes that all university professors want to kill him; no amount of evidence of kindly professors will dissuade him from this view. Hare called this kind of unfalsifiable conviction a "blik", and argued that it formed an unfalsifiable, yet still meaningful, worldview. He proposed that all people – religious and non-religious – hold bliks, and that they cannot be unseated by empirical evidence. Nevertheless, he maintained that a blik is meaningful because it forms the basis of a person's understanding of the world. Hare believed that some bliks are correct and others are not, though he did not propose a method of distinguishing between the two.

Basil Mitchell responded to Flew's falsification principle with his own parable. He described an underground resistance soldier who meets a stranger who claims to be leading the resistance movement. The stranger tells the soldier to keep faith in him, even if he is seen to be fighting for the other side. The soldier's faith is regularly tested as he observes the stranger fighting for both sides, but his faith remains strong. Mitchell's parable teaches that although evidence can challenge a religious belief, a believer still has reason to hold their views. He argued that although a believer will not allow anything to count decisively against his beliefs, the theist still accepts the existence of evidence which could count against religious belief.

Responding to the verification principle, John Hick used his parable of the Celestial City to describe his theory of eschatological verificationism. His parable is of two travellers, a theist and an atheist, together on a road. The theist believes that there is a Celestial City at the end of the road; the atheist believes that there is no such city. Hick's parable is an allegory of the Christian belief in an afterlife, which he argued can be verified upon death. Hick believed that eschatological verification is "unsymmetrical" because while it could be verified if it is true, it cannot be falsified if not. This is in contrast to ordinary "symmetrical" statements, which can be verified or falsified.

In his biography of Hick, David Cheetham notes a criticism of Hick's theory: waiting for eschatological verification could make religious belief provisional, preventing total commitment to faith. Cheetham argues that such criticism is misapplied because Hick's theory was not directed to religious believers but to philosophers, who argued that religion is unverifiable and therefore meaningless.

James Morris notes that Hick's eschatological verification theory has been criticised for being inconsistent with his belief in religious pluralism. Morris argues that such criticism can be overcome by modifying Hick's parable to include multiple travellers, all with different beliefs, on the road. He argues that even if some beliefs about life after death are unverifiable, Hick's belief in bodily resurrection can still be verified.




</doc>
<doc id="20324399" url="https://en.wikipedia.org/wiki?curid=20324399" title="Artificial language">
Artificial language

Artificial languages are languages of a typically very limited size which emerge either in computer simulations between artificial agents, robot interactions or controlled psychological experiments with humans. They are different from both constructed languages and formal languages in that they have been consciously devised by an individual or group but are the result of (distributed) conventionalisation processes, much like natural languages. Opposed to the idea of a central "designer", the field of artificial language evolution in which artificial languages are studied can be regarded as a sub-part of the more general cultural evolution studies.

The idea of creation of artificial language arose in 17th and 18th century as a result of gradually decreasing international role of Latin. The initial schemes were mainly aimed at the development of a rational language free from inconsistence of living language and based on classification of concepts. The material of living languages also appears later.

The lack of empirical evidence in the field of evolutionary linguistics has led many researchers to adopt computer simulations as a means to investigate the ways in which artificial agents can self-organize languages with natural-like properties. This research is based on the hypothesis that natural language is a complex adaptive system that emerges through interactions between individuals and continues to evolve in order to remain adapted to the needs and capabilities of its users. By explicitly building all assumptions into computer simulations, this strand of research strives to experimentally investigate the dynamics underlying language change as well as questions regarding the origin of language under controlled conditions.

Due to its success the paradigm has also been extended to investigate the emergence of new languages in psychological experiments with humans, leading up to the new paradigm of experimental semiotics.

Because the focus of the investigations lies on the conventionalisation dynamics and higher-level properties of the resulting languages rather than specific details of the conventions, artificially evolved languages are typically not documented or re-used outside the single experiment trial or simulation run in which they emerge. In fact, the limited size and short-lived nature of artificial languages are probably the only things that sets them apart from "natural" languages, since "all" languages are artificial insofar as they are conventional (see also Constructed language#Planned, constructed, artificial).

Artificial languages have been used in research in developmental psycholinguistics. Because researchers have a great deal of control over artificial languages, they have used these languages in statistical language acquisition studies, in which it can be helpful to control the linguistic patterns heard by infants.


Alan Reed Libert, Artificial Languages, Oxford Research Encyclopedia on Linguistics, June 2018


</doc>
<doc id="305869" url="https://en.wikipedia.org/wiki?curid=305869" title="Standard language">
Standard language

A standard language (also standard variety, standard dialect, and standard) is defined either as a language variety employed by a population for public communications, or as the variety of language that has undergone codification of grammar and usage. The term "standard language" occasionally refers to a language that includes a standardized form as one of its varieties, referring to the entirety of the language (or an ensemble of similar, standardized varieties) rather than a single, codified form. Typically, the language varieties that undergo substantive standardization are the dialects spoken and written in centers of commerce and government; which, by processes that linguistic anthropologists call "referential displacement" and that sociolinguists call "elaboration of function", acquire the social prestige associated with commerce and government. As a sociological effect of these processes, the users of the standardized varieties come to believe that the standard language is inherently superior or consider it the linguistic baseline by which to judge other varieties of language.

The standardization of a language is a continual process, because a language-in-use cannot be permanently standardized like the parts of a machine. Typically, the standardization process includes efforts to stabilize the spelling of the prestige dialect, to codify usages and particular (denotative) meanings through formal grammars and dictionaries, and to encourage public acceptance of the codifications as intrinsically correct. In that vein, a pluricentric language has interacting standard varieties; examples are English, French, and Portuguese, German, Korean, and Serbo-Croatian, Spanish and Swedish, Armenian and Mandarin Chinese; whereas monocentric languages, such as Russian and Japanese, have one standardized idiom.

In Europe, a standardized written language is sometimes identified with the German word "Schriftsprache" (written language). The term "literary language" is occasionally used as a synonym for "standard language", especially with respect to the Slavic languages, a naming convention still prevalent in the linguistic traditions of Eastern Europe. In contemporary linguistic usage, the terms "standard dialect" and "standard variety" are neutral synonyms for the term "standard language", usages which indicate that the standard is one of many dialects and varieties of a language, rather than the totality of the language, whilst minimizing the negative implication of social subordination that the standard is the only idiom worthy of the appellation "language".

The term "standard language" identifies a repertoire of broadly recognizable conventions in spoken and written communications used in a society and does not imply either a socially ideal idiom or a culturally superior form of speech. A standard language is developed from related dialects, either by social action (ethnic and cultural unification) to elevate a given dialect, such as that used in culture and in government, or by defining the norms of standard language with selected linguistic features drawn from the existing dialects. Typically, a standard language includes a relatively fixed orthography codified in grammars and normative dictionaries, and includes linguistic features drawn from an agreed-upon collection of exemplar texts from the literature, law, and religion of a society. Whether grammars and dictionaries are created by the state or by private citizens (e.g. "Webster's Dictionary"), some users regard such linguistic codifications as authoritative for correcting the spoken and written forms of the language. Consequently, the codified usage of speech and writing render the standard language as the more stable idiom of communication for a society than the purely spoken dialects; the codifications are also the bases for further linguistic development ("Ausbau"). In the practices of broadcasting and of official communications, the standard functions as a normative reference for acceptable speech and writing. It also informs the version of the language taught to non-native learners.

In those ways, the standard variety acquires social prestige and greater functional importance than nonstandard dialects, which depend upon or are heteronomous with respect to the standard idiom. Standard usage serves as the linguistic authority, as in the case of specialist terminology; moreover, the standardization of spoken forms is oriented towards the codified standard. Historically, a standard language arises in two ways: (i) in the case of Standard English, linguistic standardization occurred informally and piecemeal, without formal government intervention; (ii) in the cases of the French and Spanish languages, linguistic standardization occurred formally, directed by prescriptive language institutions, such as the Académie française and the Royal Spanish Academy, which respectively produced "Le bon français" and "El buen español".

A standard variety can be conceptualized in two ways: (i) as the sociolect of a given socio-economic stratum or (ii) as the normative codification of a dialect, an idealized abstraction. Hence, the full standardization of a language is impractical, because a standardized dialect cannot fully function as a real entity, but does function as set of linguistic norms observed to varying degrees in the course of "usus" — of how people actually speak and write the language. In practice, the language varieties identified as standard are neither uniform nor fully stabilized, especially in their spoken forms. From that perspective, the linguist Suzanne Romaine says that standard languages can be conceptually compared to the imagined communities of "nation" and "nationalism", as described by the political scientist Benedict Anderson, which indicates that linguistic standardization is the result of a society's history and sociology, and thus is not a universal phenomenon; of the approximately 7,000 contemporary spoken languages, most do not have a codified standard dialect.

Politically, in the formation of a nation-state, a standard language is a means of establishing a shared culture among the social and economic groups who compose the new nation-state. Different national standards, derived from a continuum of dialects, might be treated as discrete languages (along with heteronomous vernacular dialects), even if there are mutually intelligible varieties among them, such as the North Germanic languages of Scandinavia (Danish, Norwegian, and Swedish). Moreover, in political praxis, either a government or a neighboring population might deny the cultural status of a standard language. In response to such political interference, linguists develop a standard variety from elements of the different dialects used by a society. 

When Norway became independent from Denmark in 1814, the only written language was Danish. Different Norwegian dialects were spoken in rural districts and provincial cities, but people with higher education and upper-class urban people spoke ″Danish with a Norwegian pronunciation". Based upon the bourgeois speech of the capital Oslo (Christiania) and other major cities, several orthographic reforms, notably in 1907 and 1917, resulted in the official standard Riksmål, in 1929 renamed Bokmål ('book tongue'). The philologist Ivar Aasen (1813–1896) considered urban and upper-class Dano-Norwegian too similar to Danish, so he developed Landsmål ('country tongue'), the standard based upon the dialects of western Norway. In 1885 the Storting (parliament) declared both forms official and equal. In 1929 it was officially renamed Nynorsk (New Norwegian). 

Likewise, in Yugoslavia (1945–1992), when the Socialist Republic of Macedonia (1963–1991) developed their national language from the dialect continuum demarcated by Serbia to the north and Bulgaria to the east, their Standard Macedonian was based upon vernaculars from the west of the republic, which were the dialects most linguistically different from standard Bulgarian, the previous linguistic norm used in that region of the Balkan peninsula. Although Macedonian functions as the standard language of the Republic of North Macedonia, nonetheless, for political and cultural reasons, Bulgarians treat Macedonian as a Bulgarian dialect.

Chinese consists of hundreds of local varieties, many of which are not mutually intelligible, usually classified into seven to ten major groups, including Mandarin, Wu, Yue, Hakka and Min.
Before the 20th century, most Chinese spoke only their local variety.
For two millennia, formal writing had been done in Literary Chinese (or Classical Chinese), a style modelled on the classics and far removed from any contemporary speech.
As a practical measure, officials of the late imperial dynasties carried out the administration of the empire using a common language based on Mandarin varieties, known as "Guānhuà" (literally "speech of officials").

In the early 20th century, many Chinese intellectuals argued that the country needed a standardized language.
By the 1920s, Literary Chinese had been replaced as the written standard by written vernacular Chinese, which was based on Mandarin dialects.
In the 1930s, Standard Chinese was adopted, with its pronunciation based on the Beijing dialect, but with vocabulary also drawn from other Mandarin varieties and its syntax based on the written vernacular.
It is the official spoken language of the People's Republic of China (where it is called "Pǔtōnghuà" "common speech"), the de facto official language of the Republic of China governing Taiwan (as "Guóyǔ" "national language") and one of the official languages of Singapore (as "Huáyǔ" "Chinese language").
Standard Chinese now dominates public life, and is much more widely studied than any other variety of Chinese.

In the United Kingdom, the standard language is British English, which is based upon the language of the mediaeval court of Chancery of England and Wales. In the late-seventeenth and early eighteenth centuries, Standard English became established as the linguistic norm of the upper class, composed of the peerage and the gentry. Socially, the accent of the spoken version of the standard language then indicated that the speaker was a man or a woman possessed of a good education, and thus of high social prestige. In practise, speakers of Standard English speak the language with any accent (Australian, Canadian, American, etc.) although it usually is associated with Received Pronunciation, "the standard accent of English as spoken in the south of England."

The standard form of Modern Greek is based on the Southern dialects; these dialects are spoken mainly in the Peloponnese, the Ionian Islands, Attica, Crete and the Cyclades.

Two standardised registers of the Hindustani language have legal status in India: Standard Hindi (one of 23 co-official national languages) and Urdu (Pakistan’s official tongue), resultantly, Hindustani often called “Hindi-Urdu".

"An Caighdeán Oifigiúil" ("The Official Standard"), often shortened to "An Caighdeán", is official standard of the Irish language. It is taught in most schools in Ireland, though with strong influences from local dialects. It was first published by the translators in Dáil Éireann in the 1950s. As of September 2013, the first major revision of the Caighdeán Oifigiúil is available, both online and in print. Among the changes to be found in the revised version are, for example, various attempts to bring the recommendations of the Caighdeán closer to the spoken dialect of Gaeltacht speakers, including allowing further use of the nominative case where the genitive would historically have been found.

Standard Italian is derived from the Tuscan dialect, specifically from its Florentine variety—the Florentine influence upon early Italian literature established that dialect as base for the standard language of Italy. In particular, Italian became the language of culture for all the people of Italy, thanks to the prestige of the masterpieces of Dante Alighieri, Francesco Petrarca, Giovanni Boccaccio, Niccolò Machiavelli, and Francesco Guicciardini. It would later become the official language of all the Italian states, and after the Italian unification it became the national language of the Kingdom of Italy. Modern Standard Italian's lexicon has been deeply influenced by almost all regional languages of Italy while its received pronunciation (known as "Pronuncia Fiorentina Emendata", Amended Florentine Pronunciation) is based on the accent of Romanesco (Roman dialect); these are the reasons why Standard Italian differs significantly from the Tuscan dialect.

The standard language in the Roman Republic (509BC – 27 BC) and the Roman Empire (27 BC – AD 1453) was Classical Latin, the literary dialect spoken by upper classes of Roman society, whilst Vulgar Latin was the sociolect (colloquial language) spoken by the educated and uneducated peoples of the middle and the lower social classes of Roman society. The Latin language that Roman armies introduced to Gaul, Hispania, and Dacia was of a different grammar, syntax, and vocabulary than the Classical Latin spoken and written by the statesman Cicero.

In Brazil, actors and journalists usually adopt an unofficial, but de facto, spoken standard Portuguese, originally derived from the middle-class dialects of Rio de Janeiro and Brasilia, but that now encompasses educated urban pronunciations from the different speech communities in the southeast. In that standard, represents the phoneme when it appears at the end of a syllable (whereas in Rio de Janeiro this represents ) the rhotic consonant spelled is pronounced in the same situation (whereas in São Paulo this is usually an alveolar flap or trill). European and African dialects have differing realizations of than Brazilian dialects, with the former using and and the latter using , , or .

Four standard variants of the pluricentric Serbo-Croatian are spoken in Bosnia and Herzegovina, Croatia, Montenegro, and Serbia. They all have the same dialect basis (Štokavian). These variants do differ slightly, as is the case with other pluricentric languages, but not to a degree that would justify considering them as different languages. The differences between the variants do not hinder mutual intelligibility and do not undermine the integrity of the system as a whole. Compared to the differences between the variants of English, German, French, Spanish, or Portuguese, the distinctions between the variants of Serbo-Croatian are less significant. Serbia, Croatia, Bosnia and Herzegovina, and Montenegro in their constitution have all named the language differently.

In Somalia, Northern Somali (or North-Central Somali) forms the basis for Standard Somali, particularly the Mudug dialect of the northern Darod clan. Northern Central Somali has frequently been used by famous Somali poets as well as the political elite, and thus has the most prestige among other Somali dialects.




</doc>
<doc id="226988" url="https://en.wikipedia.org/wiki?curid=226988" title="Spoken language">
Spoken language

A spoken language is a language produced by articulate sounds, as opposed to a written language. Many languages have no written form and so are only spoken. An oral language or vocal language is a language produced with the vocal tract, as opposed to a sign language, which is produced with the hands and face. The term "spoken language" is sometimes used to mean only vocal languages, especially by linguists, making all three terms synonyms by excluding sign languages. Others refer to sign language as "spoken", especially in contrast to written transcriptions of signs.

In spoken language, much of the meaning is determined by the context. That contrasts with written language in which more of the meaning is provided directly by the text. In spoken language, the truth of a proposition is determined by common-sense reference to experience, but in written language, a greater emphasis is placed on logical and coherent argument. Similarly, spoken language tends to convey subjective information, including the relationship between the speaker and the audience, whereas written language tends to convey objective information.

The relationship between spoken language and written language is complex. Within the field of linguistics the current consensus is that speech is an innate human capability, and written language is a cultural invention. However some linguists, such as those of the Prague school, argue that written and spoken language possess distinct qualities which would argue against written language being dependent on spoken language for its existence.

Both vocal and sign languages are composed of words. In vocal languages, words are made up from a limited set of vowels and consonants, and often tone. In sign languages, words are made up from a limited set of shapes, orientations, locations movements of the hands, and often facial expressions; in both cases, the building blocks are called phonemes. In both vocal and sign languages, words are grammatically and prosodically linked into phrases, clauses, and larger units of discourse.

Hearing children acquire as their first language the language that is used around them, whether vocal, cued (if they are sighted), or signed. Deaf children can do the same with Cued Speech or sign language if either visual communication system is used around them. Vocal language are traditionally taught to them in the same way that written language must be taught to hearing children. (See oralism.) Teachers give particular emphasis on spoken language with children who speak a different primary language outside of the school. For the child it is considered important, socially and educationally, to have the opportunity to understand multiple languages.



</doc>
<doc id="373299" url="https://en.wikipedia.org/wiki?curid=373299" title="Language of mathematics">
Language of mathematics

The language of mathematics is the system used by mathematicians to communicate mathematical ideas among themselves. This language consists of a substrate of some natural language (for example English) using technical terms and grammatical conventions that are peculiar to mathematical discourse (see Mathematical jargon), supplemented by a highly specialized symbolic notation for mathematical formulas.

Like natural languages in general, discourse using the language of mathematics can employ a scala of registers. Research articles in academic journals are sources for detailed theoretical discussions about ideas concerning mathematics and its implications for society.

Here are some definitions of language:

These definitions describe language in terms of the following components:

Each of these components is also found in the language of mathematics.

Mathematical notation has assimilated symbols from many different alphabets and typefaces. It also includes symbols that are specific to mathematics, such as

Mathematical notation is central to the power of modern mathematics. Though the algebra of Al-Khwārizmī did not use such symbols, it solved equations using many more rules than are used today with symbolic notation, and had great difficulty working with multiple variables (which using symbolic notation can simply be called formula_2, etc.). Sometimes formulas cannot be understood without a written or spoken explanation, but often they are sufficient by themselves, and sometimes they are difficult to read aloud or information is lost in the translation to words, as when several parenthetical factors are involved or when a complex structure like a matrix is manipulated.

Like any other profession, mathematics also has its own brand of technical terminology. In some cases, a word in general usage has a different and specific meaning within mathematics—examples are group, ring, field, category, term, and factor. For more examples, see .

In other cases, specialist terms have been created which do not exist outside of mathematics—examples are tensor, fractal, functor. Mathematical statements have their own moderately complex taxonomy, being divided into axioms, conjectures, theorems, lemmas and corollaries. And there are stock phrases in mathematics, used with specific meanings, such as ", " and "without loss of generality". Such phrases are known as mathematical jargon.

The vocabulary of mathematics also has visual elements. Diagrams are used informally on blackboards, as well as more formally in published work. When used appropriately, diagrams display schematic information more easily. Diagrams also help visually and aid intuitive calculations. Sometimes, as in a visual proof, a diagram even serves as complete justification for a proposition. A system of diagram conventions may evolve into a mathematical notation – for example, the Penrose graphical notation for tensor products.

The mathematical notation used for formulas has its own grammar, not dependent on a specific natural language, but shared internationally by mathematicians regardless of their mother tongues. This includes the conventions that the formulas are written predominantly left to right, even when the writing system of the substrate language is right-to-left, and that the Latin alphabet is commonly used for simple variables and parameters. A formula such as
is understood by Chinese and Syrian mathematicians alike.

Such mathematical formulas can be a part of speech in a natural-language phrase, or even assume the role of a full-fledged sentence. For example, the formula above, an inequation, can be considered a sentence or an independent clause in which the greater than or equal to symbol has the role of a symbolic verb. In careful speech, this can be made clear by pronouncing "≥" as "is greater than or equal to", but in an informal context mathematicians may shorten this to "greater or equal" and yet handle this grammatically like a verb. A good example is the book title "Why does ?"; here, the equals sign has the role of an infinitive.

Mathematical formulas can be "vocalized" (spoken aloud). The vocalization system for formulas has to be learned, and is dependent on the underlying natural language. For example, when using English, the expression ""ƒ"("x")" is conventionally pronounced "eff of eks", where the insertion of the preposition "of" is not suggested by the notation per se. The expression "formula_4", on the other hand, is commonly vocalized like "dee-why-dee-eks", with complete omission of the fraction bar, in other contexts often pronounced "over". The book title "Why does ?" is said aloud as "Why does ee equal em see-squared?".

Characteristic for mathematical discourse – both formal and informal – is the use of the inclusive first person plural "we" to mean: "the audience (or reader) together with the speaker (or author)".

As is the case for spoken mathematical language, in written or printed mathematical discourse, mathematical expressions containing a symbolic verb, like formula_5, are generally treated as clauses (dependent or independent) in sentences or as complete sentences and are punctuated as such by mathematicians and theoretical physicists. In particular, this is true for "both" inline and displayed expressions. In contrast, writers in other natural sciences disciplines may try to avoid using equations within sentences and may treat displayed expressions in the same way as figures or schemes.

As an example, a mathematician might write: 

In this statement, "formula_6" (in which formula_6 is read as "ay en" or perhaps, more formally, as "the sequence ay en") and "formula_7" are treated as nouns, while "formula_8" (read: the limit of formula_18 as "n" tends to infinity equals 'big A'), "formula_9", and "formula_20" are read as independent clauses, and "formula_12" is read as "the equation formula_22 equals formula_18 plus formula_24". Moreover, the sentence ends after the displayed equation, as indicated by the period after "formula_20". In terms of typesetting conventions, broadly speaking, standard mathematical functions such as and operations such as as well as punctuation symbols including the various brackets are set in while Latin alphabet variables are set in . Matrices, vectors, and other objects made up of components are set in . (There is some disagreement as to whether the standard constants (e.g., , π, i = (–1)) or the "d" in should be italicized. Upper case Greek letters are almost always set in roman, while lower case ones are often italicized.) There are also a number of conventions for the part of the alphabet from which variable names are chosen. For example, , , , , , are usually reserved for integers, and are often used for complex numbers, while , , , α, β, γ are used for real numbers. The letters , , are frequently used for unknowns to be found or as arguments of a function, while , , are used for coefficients and , , are mostly used as names of functions. These conventions are not hard rules. Instead these suggestions are met to enhance readability and to provide an intuition for of what kind a given object is, so that one has neither to remember, nor to check the introduction of the mathematical object.

Definitions are signaled by words like "we call", "we say", or "we mean" or by statements like "An ["object"] is ["word to be defined"] if ["condition"]" (for example, "A set is closed if it contains all of its limit points."). As a special convention, the word "if" in such a definition should be interpreted as "if and only if".

Theorems have generally a title or label in bold type, and possibly identify the originator (for example, ""). This is immediately followed by the statement of the theorem, usually set in italics. The proof of a theorem is usually clearly delimited, starting with the word "Proof" while the end of the proof is indicated by a tombstone ("∎ or □") or another symbol, or by the letters Q.E.D..

Mathematics is used by mathematicians, who form a global community composed of speakers of many languages. It is also used by students of mathematics. As mathematics is a part of primary education in almost all countries, almost all educated people have some exposure to pure mathematics. There are very few cultural dependencies or barriers in modern mathematics. There are international mathematics competitions, such as the International Mathematical Olympiad, and international co-operation between professional mathematicians is commonplace.
The power of mathematics lies in economy of expression of ideas, often in service to science. Horatio Burt Williams took note of the effect of this compact form in physics:
In mathematics "per se", the brevity is profound:

Williams cites Ampère as a scientist that summarized his findings with mathematics:

The significance of mathematics lies in the logical processes of the mind have been codified by mathematics:

Williams' essay was a Gibbs Lecture prepared for scientists in general, and he was particularly concerned that biological scientists not be left behind:

Mathematics is used to communicate information about a wide range of different subjects. Here are three broad categories:


Mathematics can communicate a range of meanings that is as wide as (although different from) that of a natural language. As English mathematician R.L.E. Schwarzenberger says:

Some definitions of language, such as early versions of Charles Hockett's "design features" definition, emphasize the spoken nature of language. Mathematics would not qualify as a language under these definitions, as it is primarily a written form of communication (to see why, try reading Maxwell's equations out loud). However, these definitions would also disqualify sign languages, which are now recognized as languages in their own right, independent of spoken language.

Other linguists believe no valid comparison can be made between mathematics and language, because they are simply too different:





</doc>
<doc id="36047835" url="https://en.wikipedia.org/wiki?curid=36047835" title="Orthology (language)">
Orthology (language)

Orthology is the study of the right use of words in language. The word comes from Greek "ortho"- ("correct") and -"logy" ("science of"). This science is a place where psychology, philosophy, linguistics, and many other fields of learning come together. The most noted use of "Orthology" is for the selection of words 
for the language of Basic English by the Orthological Institute.

The book, "The Meaning of Meaning", by C.K. Ogden and I.A. Richards, is an important book dealing with orthology. The term "Orthology" comes from the book "The Grammar of Science" by Karl Pearson.


</doc>
<doc id="37208440" url="https://en.wikipedia.org/wiki?curid=37208440" title="Verbal language in dreams">
Verbal language in dreams

Verbal language in dreams is the speech—most commonly in the form of a dialogue between the dreamer him/herself and other dream characters—which forms part of the overall (mostly imagistic) dream scenario. Historically, there have been abundant references to verbal language in dreams going back millennia. Early in the twentieth century German psychiatrist Emil Kraepelin presented a large corpus of dream speech, almost all from his own dreams and virtually all deviant, without any pretense that this was representative of dream speech in general. The first systematic elicitation of verbal language in dreams from a large subject pool under methodological protocols was presented beginning in the early 1980s, along with detailed analyses as well as theoretical consideration of the implications for various dream models, from the psychoanalytic approach to more recent theories.

Traditionally, dreams have been defined predominantly in imagistic terms. Prominent dream theories of the modern era from Sigmund Freud's psychoanalytic model (1900) to the present have similarly placed emphasis of the visual aspects of dreams. Yet, even the earliest of written sources, such as the Hebrew Bible and The Odyssey make clear that dreams need not be "silent movies"; they may be "talkies" incorporating a "sound track" abounding in verbal dialogues or monologues.

A survey by Heynick of several books containing over 300 dreams, both genuine reports and dreams incorporated into works of fiction, showed that some three-quarters contained verbal dialogue or explicit reference to speech in the dream. As a specimen of a dream with dialogue as part of a famous work of fiction, Heynick cites the dream of Charles Swann, the main character in Proust's "Swann's Way" (1913; italics added):

The painter remarked to Swann [the dreamer] "that Napoleon III had eclipsed himself immediately after Odette. "They had obviously arranged it between them,"" he added; "they must have agreed to meet at the foot of the cliff, but they wouldn't say good-bye together, it might have looked odd. She is his mistress." The strange young man burst into tears. Swann endeavored to console him. "After all, she is quite right," he said to the young man, drying his eyes for him and taking off his fez to make him feel more at ease. "I've advised her to do that myself a dozen times. Why be so distressed? He was obviously the man to understand her."

In 1906 Kraepelin, a pioneer of the somatic approach to psychiatry and of the methodical classification of psychiatric disorders, published a 105-page monograph "Über Sprachstörungen im Traume" (On Speech Disorders in Dreams). As the title suggests, Kraepelin's declared aim was to analyze only deviant specimens of speech from dreams. Specimens reflecting correct speech processes were excluded from his study. To this end, Kraepelin assembled in the course of twenty years 286 specimens, the vast majority drawn from his own dreams, with no pretense to nonselectivity. He apparently drew in large measure from hypnagogic and occasionally hypnopompic dreamlets (experienced when falling asleep and waking up), which differ phenomologically from full-fledged dreams and are characterized by different neurological indices as well.

Kraepelin meticulously classified his collection of dreams according to the nature of the deviances from correct normal speech in wakefulness. Three-fifths of his specimens were grouped as disorders of word selection, including large numbers of neologisms (non-existing words, typically formed by combinations of existing words or their components); just over one-fifth as disorders of discourse (actaphasia, usually involving the incorrect choice of language dependency relations; and agrammatism, the faulty construction of complex sentences); and just under one fifth as disorders of thought. Although it was well known at the time that the speech of normal people in wakefulness is often fraught with errors, Kraepelin prized his corpus of deviant dream speech for the profound nature of many of the errors they contained, different from the common slips of the tongue made by mentally healthy people in everyday life. He likened various specimens of his dream speech corpus to the speech in waking life of patients with dementia praecox (schizophrenia), speech confusion, and aphasia. Kraeplin saw his dream experiences as affording him (a normal person) first-hand insight into these pathological processes. He further speculated on neurological concomitants involving the activities and interaction of areas of the brain—the cerebral cortex, Wernicke's coil, Broca's coil—which are different from in normal wakefulness. Although several of Kraepelin's dream speech specimens are amenable to interpretation for their latent sexual significance, he had no interest in the psychoanalytic approach and made no reference to his contemporary Freud in any of his writings.

Prior to the 1980s, therefore, no indices or standards existed that were representative of the verbal language component of the dreams of a large general population. But beginning in 1983, Heynick reported in a series of publications the results of two experiments designed to evaluate, first, the linguistic competence and, subsequently, the pragmatic competence of the dreamer in the dreaming state, using large subject pools drawn from the general population (in this case in the Netherlands) and following careful protocols designed to avoid selectivity and maximize accuracy of recall.

The term "linguistic performance," central to this experiment, derives from the transformational-generative (TG) revolution in linguistics in the second half of the twentieth century and the concomitant emergence of the field of formal psycholinguistics. The TG model of language generation assumes an ideal "linguistic competence" on the part of the speaker, theoretically enabling him or her to generate all the infinite number of well-formed sentences in the native language while generating none of the ill-formed sentences. That in actual use, i.e., linguistic performance, the speaker is limited in, among other things, the complexity or elaboration of the sentences he or she can generate, and often produces utterances which are ill-formed, is due to the limitations of the various auxiliary psychological mechanisms at the speaker's disposal, such as limited short-term memory, perceptual limitations, and defective feedback, as well as to factors such as deliberate changes in structure in mid-sentence, and unconscious interference of the Freudian type.

All the above has traditionally applied to the native speaker in the waking state. The experiment explored the linguistic performance of the native speaker in the dreaming state.

78 Dutch subjects sleeping at home recorded on special forms following precise protocol instructions a total of 566 Dutch utterances directly recalled and transcribed from 566 dreams which had been in progress prior to alarm clock awakening in the morning. (For just over 80 percent of all awakenings, there was either no dream in progress at the time of awakening or no verbal material to be reported from the dream.) The subjects reported (as part of the questionnaire form) that 60 percent of the utterances were said by him or herself in the dream scenario; 40 percent by someone else, usually addressed to the dreamer.
Word-count analysis showed the utterances in the corpus to have a wide range of length, with a mean utterance length of 7.5 words and a mean sentence length of 6.5 words. The declarative sentences in the corpus were analyzed for complexity (sentential elaboration) using in particular the number of subordinate clauses per unit as an index. The corpus when classed into three groups according to the education level of the subject-dreamers showed that those with most education had the highest sentential elaboration; those with the least, the lowest; with those with intermediate education scoring in between. (There were no standards available for absolute comparison with the sentential elaboration in spoken Dutch in waking life by the three education-level groups in the general population; however, the relative degree of sentential elaboration of the dream corpus of the three education-level groups accorded with the relative indices for written Dutch by the three educational-level groups, which were available for comparison.)

72 of the 566 utterances were marked by their respective subjects (in response to one of the questions on the form) as deviating from wakeful usage, although analysis of the same specimens by two academic linguists deemed the large majority of those marked utterances to be fully acceptable Dutch. Fewer than 5 percent of all utterances clearly deviated from correct wakeful speech. These included semantic anomalies, faulty lexical substitutions, neologisms (word-blends), non-existent proper names, language mixing, and (in two instances) syntactic errors.

The subjects in the above experiment were not asked to report the full dream scenario during which the utterances were made. Excluded, therefore, from the analysis was any consideration of "pragmatic competence," the "knowledge of condition and manner or appropriate use [of his or her linguistic ability] in conformity with various purposes," which forms part of the broader field of language psychology (rather than formal psycholinguistics).
Dream utterances "plus" their dream scenario contexts were gathered in a second experiment, again following carefully defined protocols, this time using the telephone elicitation technique. (Volunteer subjects were awakened on random nights at random hours, on average no more than once a week.) 77 dream reports from 33 subjects were tape-recorded and transcribed. These contained 92 dream speech utterances elicited verbatim from the subjects immediately after provoked awakening (to minimize deterioration in memory recall) plus an additional 113 utterances (81 in direct verbatim form, 32 in indirect form) contained in the subsequent telling of the entire dream. 40 percent of the utterances were said by the dreamer; 60 percent by someone else in the dream scenario, usually addressed to the dreamer (a reversal of the proportions in the linguistic performance experiment).

Five scorers, working independently but achieving a high degree of interscorer reliability, rated the appropriateness of each of the 205 dreams utterances to their situational context in the dream. Averaged out (and rounded off), 67% of the utterances were deemed to be "entirely suitable to the narrative"; 20% "not entirely suitable"; 7% "largely unsuitable"; and 4% "entirely unsuitable."

As an example, in the following dream all lines of dialogue were deemed by all scorers (in the first instance, by four or the five scorers) to be fully suitable to the narrative:

I was sitting in the garden and reading old magazines. [...] My son [...] took a little girlfriend along, also about five, and she started looking through the magazines. And they were getting wet, since she had such a cold that her nose ran. And I said "that she, your little girlfriend, should rinse her nose with the lotah", that's a jug used in yoga [instead of a handkerchief]. I use it myself, too. So I let her do it, but because she was so small, it didn't go right, and she started to cry. And then she walked home. So I said to my little boy, "I'll go to her mother and tell her what happened, otherwise she'll think 'what a strange lady.'" [...] Then that girl came out again, and my son had already told her mother that I had tried to help her get rid of her cold with the lotah. I simply knew that he had said that, but I didn't hear him say that literally. Then I said to that girl ""In the future, when you catch a cold, you should go to the doctor and have him write out a prescription for medicine."

In general, the dreamer as (to continue the film metaphor) "script-writer" appears to have at his or her command not just a "story grammar" capable of generating a reasonably, though not always, coherent overall scenario, but also the linguistic pragmatic competence to generate verbal dialogue which is usually, though again not always, appropriate to that scenario.

The abundance of verbal language in dreams—typically in the form of dialogue between the dreamer and other characters in the dream scenario—which generally shows a syntactic and lexical well-formedness comparable to speech in the waking state, and which in addition is, far more often than not, appropriate to the dream context, has, in Heynick's theoretical analysis, profound consequences for overall dream theories, past and present.

Freud's psychoanalytic model of the mind and, in particular, his theory of dream generation and its function posit the existence of two global modes of mental functioning. The primary process, characterized by such mechanisms as condensations, displacements, and reversals (and the absence of any sense of negation) is theoretically characteristic of the infantile mode of thinking. It is superseded in the course of ontogeny by the development of the conscious part of the mind, which in the older child and adult is governed by the secondary process, adhering to the rules of grammar and logic. The verbal language which the developing child acquires is for Freud by definition a secondary process. The primary process in psychoanalytic theory is however not banished from the mind, but is contained in the unconscious, where it continues to characterize the mode of functioning of that part of the psyche.

The unconscious with its primary process mode—along with its repressed ideational content from early childhood, such as the Oedipus complex—provides, in Freud's theory, the driving force and initial input to dreams. Freud attributed the generation of a more or less coherent dream narrative to the process of "secondary revision," which might (he vacillated on this issue) be "a contribution on the part of waking thought to the construction of dreams" rather than part of the "dream work" proper. The dream-like features which one experiences in dreams, such as condensations, displacements (symbolism), and reversals, are the manifestations of the primary-process input to the dream generation process.

Yet the dreams of Freud's own and those of his patients, which he provides in "The Interpretation of Dreams" and elsewhere, typically abound in verbal dialogue, which is always syntactically well-formed, often complex (containing subordinate clauses) and usually, though not always, semantically well-formed and appropriate to the context of the dream.

As an example, Heynick cites, among others, the "dream of Irma's injection, which Freud himself considered to be of central importance to the development of his dream theory:

I said to Irma: "If you still get pains, it's really your own fault." She replied: "If you only knew what pains I've got in my throat and stomach and abdomen—it's choking me." [...] I at once "called in" Dr. M. [who] looked quite different from usual; he was very pale, he walked with a limp and his chin was clean shaven. [...] My friend Leopold was [...] saying, "She has a dull area down on the left." He also indicated that a portion of the skin of the left shoulder was infiltrated. (I noticed this, just as he did, in spite of her dress.) [...] M. said: ""There's no doubt it's an infection, but no matter, dysentery will supervene and the toxin will be eliminated."

Virtually all the deep personal significance which Freud in his analysis attributed to his Irma dream would be lost without the verbal dialogue. Yet the utterances are fully grammatical; they contain properly embedded and conjoined clauses, two conditional phrases, a future tense, and two instances of negation. All words are proper German (in the original). Semantically most of the dialogue appears appropriate to the overall scenario, except the last which is medically absurd.

Freud was apparently confronted with the enigma of how the generation of speech, a secondary process par excellence, could be functioning at such a apparently high level of adequacy during dreaming, the input and motive power of which involves a theoretically primary process. Freud attempted to resolve this problem by introducing his "replay hypothesis" (as Heynick terms it), according to which the dialogue in dreams is merely a repeating, in the same or similar words, of speech actually said or heard by the dreamer recently in waking life, usually on the day before the dream, only slightly modified (if at all) by the above-mentioned process of "secondary revision." This theoretical regression to an infantile mimicking of speech thus denied to the dreaming process proper any substantial language functioning or (in modern terms) linguistic competence.

With a view to testing the validity of Freud's replay hypothesis, the subject-dreamers in the above-mentioned experiment on linguistic competence were asked with regard to each of their dream speech specimens whether they could recall having said the utterance prior to the dream in waking life in the same or similar words. The subjects deemed that to have been the case on the day before the dream for 8% of the utterances and for the week before the dream (including the day before the dream) for 14% of the utterances. Such low figures appear to invalidate Freud's replay hypothesis.

Although no neo-Freudians treat the phenomenon of dream speech per se, proposed revisions to the psychoanalytic model of the mind by theorists such as Gill, Holt and Noy, beginning in the late 1960s, variously included the redefining of the place of speech generation (in wakefulness, but this would also apply to speech during dreaming) whereby it would no longer necessarily be classified as a strictly secondary process. Such revisions would, Heynick points out, potentially make the phenomenon of verbal language in dreams less anomalous in the psychoanalytic scheme, and without the need to resort, as Freud did, to the replay hypothesis as an "ad hoc" expedient.

The activation-synthesis hypothesis of dream generation first proposed by Harvard University psychiatrists John Allan Hobson and Robert McCarley in the late 1970s drew upon the discovery earlier in the decade of the ponto-geniculo-occipital (PGO) impulses which during the nightly REM periods (the physiological stage of sleep which is most associated with the phenomenological experience of vivid dreaming) are fired off from the brain stem and travel to the visual areas of the cortex.

In the visual area of the cortex, according to the model, the impulses call up largely at random a series of images (=activation). The dreamer's sleeping consciousness automatically tries to integrate these images into a more or less coherent story, while, for example, linking them with fragments of memory from the past (=synthesis). (Similar mechanisms involving the PGO impulses have been incorporated into other dream models, namely Francis Crick and Graeme Mitchison's reverse learning theory and Michael Jouvet's endogenous learning hypothesis.)
The activation-synthesis hypothesis, although based on neurological data more advanced than was ever available to Freud, is reminiscent of Freud's psychoanalytic model in that the initial input to the process is primitive and chaotic, onto which some order in imposed by a cognitively more advanced process. But as for the psychological interpretation of dreams the authors of the activation-synthesis hypothesis are outspokenly anti-Freudian, in that in their model the initial input to the dreaming process (the PGO-impulses) are devoid of any depth-psychology significance, Oedipal or whatever. (If the dream acquires any significance relating to the dreamer's personality. history, and present circumstances, this occurs in the manner in which the more or less randomly evoked images are integrated into a story during the synthesis stage, analogous to what a subject may "see" in the random ink-blots of a Rorschach test.)

Be this as it may, the activation-synthesis hypothesis with its overwhelming emphasis on visual areas of the cortex is silent as to how the extensive verbal language may be generated and integrated into the dream scenario. Heynick cites as ironic a specimen dream which Hobson presents as exemplary of production by the activation-synthesis model:

I am in Willianstown, Massachusetts, talking to a colleague, Van, who is wearing a white shirt (he usually wears blue) open at the neck (he is normally necktied, and even collar-clipped) and khakis (he usually sports flannels). Casual. Van says, as if by the way, "that he attended the committee meeting that has yesterday considered my candidacy for an invited lecture series." (I know from his tone that he is going to deliver bad news.) The committee has decided against it because ""They don't feel that psychoanalysis should be confronted with laboratory data."
I allowed at how bad this idea was. "It's the wrong reason," I said. "And their timing is off, because Adolf Grünbaum is just about to publish his important new book in which he insists that that is precisely what psychoanalysis must do." Van ignores this statement, appearing never to have heard of A.G.
We go out a door (which is on the corner of the building) to behold the beautiful Williams campus. A red-brick wall extends down a green lawn to the classic white Puritan buildings.
Van says, "They chose Mary" (or seems to say that) "reflecting their priorities to attract a speaker who might help them with their fund-raising efforts." ""That's why you have such beautiful buildings," I note, "and why there is nothing in them.""

Hobson presents this specimen as an example of how dreams can sometimes reflect personal concerns—in this case relating to his academic squabbles with his colleagues due to his anti-psychoanalytic stance within the psychiatric profession. As Heynick points out, the personal significance in this dream is in fact derived almost exclusively from the verbal dialogue, without which the dream would lose all its meaning.

The psychoneirics (from Greek; psycho = mind + oneiros = dream) model of dream generation, formulated by Foulkes from the late 1970s onwards, is cited as an exemplary model (though not necessarily the only possible type of model) of how the generation of verbal language can be incorporated into the generation of the overall dream scenario. Patterned on the psycholinguistic model of speech production (in wakefulness), the psychoneirics model is basically non-neurological. It views dream generation in humans as, like speech in humans, a skillful cognitive act, in fact possibly drawing upon the selfsame cognitive abilities.

The input to the dreaming process involves (not unlike the activation-synthesis hypothesis of dreaming) the diffuse activation of memory elements. Psychoneirics focuses on the midrange dream generation processes (regardless of whether or not the dream happens to include verbal dialogue) as involving "schematic selection" and "element activation," analogous to the syntactic frame (sentence structure) selection and word selection of psycholinguistic models. (The dream-like features of dreams, such as condensations (composite images) and anomalous narrative shifts, are seen as residual flux of the dream production mechanism, which is otherwise doing a reasonably good job of developing the initial input into a coherent narrative.) Under this theoretical assumption that the human dream-generation ability and speech-generation ability (in wakefulness) derive from similar cognitive capacities, the psychoneirics model can claim to seamlessly account for the generation of verbal dialogue within dreams.

The data presented in the above experiments is considered as having implications not just for the evaluation of the various existing models of dream generation, but also for models of speech generation in general, that is in everyday life.

Dreaming is a state of consciousness different from the normal wakeful state. With regard to actions within the dream, dreaming consciousness is presumably characterized by a diminished capacity for deliberate intention on the part of the dreamer (including intention of what to say or, as scriptwriter, what to have other characters say) and a diminished attention to, or diminished ability to receive and monitor feedback from, the actions (including speech acts) as they are being carried out in the dream. The characteristics of dialogue in dreams indicate that despite the presumed diminished intention, attention and feedback on the part of the dreamer as speaker-listener and scriptwriter, the utterances generated, far more often than not, are semantically and syntactically well-formed and appropriate to the overall scenario. The implication is that the human capacity for language in general (that is, in everyday wakefulness) can largely rely on processes which, once they are triggered when the conditions match those required for their operation, can generate verbal utterances automatically, outside of awareness and without the need for intervention by the speaker except at points where some critical choice is made.

Notes
Bibliography


</doc>
<doc id="39104546" url="https://en.wikipedia.org/wiki?curid=39104546" title="Evolutionary psychology of language">
Evolutionary psychology of language

Evolutionary psychology of language is the study of the evolutionary history of language as a psychological faculty within the discipline of evolutionary psychology. It makes the assumption that language is the result of a Darwinian adaptation.

There are many competing theories of how language might have evolved, if indeed it is an evolutionary adaptation. They stem from the belief that language development could result from an adaptation, an exaptation, or a by-product. Genetics also influence the study of the evolution of language. It has been speculated that the FOXP2 gene may be what gives humans the ability to develop grammar and syntax.

In the debate surrounding the evolutionary psychology of language, three sides emerge: those who believe in language as an adaptation, those who believe it is a by-product of another adaptation, and those who believe it is an exaptation.

Scientist and psychologists Steven Pinker and Paul Bloom argue that language as a mental faculty shares many likenesses with the complex organs of the body which suggests that, like these organs, language has evolved as an adaptation, since this is the only known mechanism by which such complex organs can develop. The complexity of the mechanisms, the faculty of language and the ability to learn language provides a comparative resource between the psychological evolved traits and the physical evolved traits.

Pinker, though he mostly agrees with Noam Chomsky, a linguist and cognitive scientist, in arguing that the fact that children can learn any human language with no explicit instruction suggests that language, including most of grammar, is basically innate and that it only needs to be activated by interaction, but Pinker and Bloom argue that the organic nature of language strongly suggests that it has an adaptational origin.

Noam Chomsky spearheaded the debate on the faculty of language as a cognitive by-product, or spandrel. As a linguist, rather than an evolutionary biologist, his theoretical emphasis was on the infinite capacity of speech and speaking: there are a fixed number of words, but there is an infinite combination of the words. His analysis from this considers that the ability of our cognition to perceive infinite possibilities, or create infinite possibilities, helped give way to the extreme complexity found in our language. Both Chomsky and Gould argue that the complexity of the brain is in itself an adaptation, and language arises from such complexities.
On the issue of whether language is best seen as having evolved as an adaptation or as a by product, evolutionary biologist W. Tecumseh Fitch, following Stephen J. Gould, argues that it is unwarranted to assume that every aspect of language is an adaptation, or that language as a whole is an adaptation. He criticizes some strands of evolutionary psychology for suggesting a pan-adaptationist view of evolution, and dismisses Pinker and Bloom's question of whether "Language has evolved as an adaptation" as being misleading.
He argues instead that from a biological viewpoint the evolutionary origins of language is best conceptualized as being the probable result of a convergence of many separate adaptations into a complex system. A similar argument is made by Terrence Deacon who in "The Symbolic Species" argues that the different features of language have co-evolved with the evolution of the mind and that the ability to use symbolic communication is integrated in all other cognitive processes.

Exaptations, like adaptations, are fitness-enhancing characteristics, but, according to Stephen Jay Gould, their purposes were appropriated as the species evolved. This can be for one of two reasons: either the trait’s original function was no longer necessary so the trait took on a new purpose or a trait that does not arise for a certain purpose, but later becomes important. Typically exaptations have a specific shape and design which becomes the space for a new function. The foundation of this argument comes from the low-lying position of the larynx in humans. Other mammals have this same positioning of the larynx, but no other species has acquired language. This leads exaptationists to see an evolved modification away from its original purpose.

Research has shown that “genetic constraints” on language evolution could have caused a “specialized” and “species-specific language module. It is through this module that there are many specified “domain-specific linguistic properties,” such as syntax and agreement. Adaptationists believe that language genes “coevolved with human language itself for the purpose of communication.” This view suggests that the genes that are involved with language would only have coevolved in a very stable linguist environment. This shows that language could not have evolved in a rapidly changing environment because that type of environment would not have been stable enough for natural selection. Without natural selection, the genes would not have coevolved with the ability for language, and instead, would have come from “cultural conventions.” The adaptationist belief that genes coevolved with language also suggests that there are no “arbitrary properties of language.” This is because they would have coevolved with language through natural selection.
The Baldwin effect provides a possible explanation for how language characteristics that are learned over time could become encoded in genes. He suggested, like Darwin did, that organisms that can adapt a trait faster have a “selective advantage.” As generations pass, less environmental stimuli is needed for organisms of the species to develop that trait. Eventually no environmental stimuli are needed and it is at this point that the trait has become “genetically encoded.”

The genetic and cognitive components of language have long been under speculation, only recently have linguists been able to point out a gene that may possibly explain how language works. Evolutionary psychologists hold that the FOXP2 gene may well be associated with the evolution of human language. In the 1980s, psycholinguist Myrna Gopnik identified a dominant gene that causes language impairment in the KE family of Britain. The KE family has a mutation in the FOXP2, that makes them suffer from a speech and language disorder. It has been argued that the FOXP2 gene is the grammar gene, which is what allows humans the ability to form proper syntax and make our communication of higher quality. Children that grow up in a stable environment are able to develop highly proficient language without any instruction. Individuals with a mutation to their FOXP2 gene have trouble mastering complex sentences, and shows signs of developmental verbal dyspraxia.

This gene most likely evolved in the hominin line after the hominin and the chimpanzee lines split; this accounts for the fact that humans are the only ones able to learn and understand grammar. Humans have a unique allele of this gene, which has otherwise been closely conserved through most of mammalian evolutionary history. This unique allele seems to have first appeared between 100 and 200 thousand years ago, and it is now all but universal in humans. This suggests that speech evolved late in overall spectrum of human evolution.

There are nearly 7000 languages worldwide, with a great amount of variation thought to have evolved through cultural differentiation. There are four factors that are thought to be the reason as to why there is language variation between cultures: founder effects, drift, hybridization and adaptation. With the vast amounts of lands available different tribes began to form and to claim their territory, in order to differentiate themselves many of these groups made changes to their language and this how the evolution of languages began. There also tended to be drifts in the population a certain group would get lost and be isolated from the rest of the group, this group would lose touch with the other groups and before they knew there had been mutations in their language and a whole new language had been formed.

Hybridization also played a big role in the language evolution, one group would come in contact with another tribe and they would pick up words and sounds from each other eventually leading to the formation of a new language. Adaptation would also play a role in the evolution of language differentiation, the environment and the circumstances were constantly changing therefore the groups had to adapt to the environment and their language had to adapt to it as well, it is all about maximizing fitness.

Atkinson theorized that language may have originated in Africa since African languages have a greater variation of speech sounds than other languages. Those sounds are seen as the root for the other languages that exist across the world.

Research indicates that nonhuman animals (e.g., apes, dolphins, and songbirds) show evidence of language. Comparative studies of the sensory-motor system reveal that speech is not special to humans: nonhuman primates can discriminate between two different spoken languages. Anatomical aspects of humans, particularly the descended larynx, has been believed to be unique to humans' capacity to speak. However, further research revealed that several other mammals have a descended larynx beside humans, which indicates that a descended larynx must not be the only anatomical feature needed for speech production.
Vocal imitation is not uniquely human as well. Songbirds seem to acquire species-specific songs by imitating. Because nonhuman primates do not have a descended larynx, they lack vocal imitative capacity, which is why studies involving these primates have taught them nonverbal means of communication, e.g., sign language.

Koko and Nim Chimpsky are two apes that have successfully learned to use sign language, but not to the extent that a human being can. Nim is a chimpanzee that was taken in by a family in the 1970s and was raised as if he were a human child. Nim was able to master 150 signs, which were limited but useful. Koko was a gorilla that was taken in by a Berkley student. She was able to master 600 signs for generative communication. Koko and Nim were not able to develop speech due to the fact that they lack the larynx which is what distinguishes humans from other animals and allows them to speak.



</doc>
<doc id="39189535" url="https://en.wikipedia.org/wiki?curid=39189535" title="Terministic screen">
Terministic screen

Terministic screen is a term in the theory and criticism of rhetoric. It involves the acknowledgment of a language system that determines an individual's perception and symbolic action in the world.

Kenneth Burke develops the terministic screen in his book of essays called "Language as Symbolic Action" in 1966. He defines the concept as "a screen composed of terms through which humans perceive the world, and that direct attention away from some interpretations and toward others". Burke offers the metaphor to explain why people interpret messages differently, based on the construction of symbols, meanings, and, therefore, reality. Words convey a particular meaning, conjuring images and ideas that induce support toward beliefs or opinions. Receivers interpret the intended message through a metaphorical screen of their own vocabulary and perspective to the world. Certain terms may grab attention and lead to a particular conclusion. "Language reflects, selects, and deflects as a way of shaping the symbol systems that allow us to cope with the world". Every word chosen says something about the society we live in. The language we choose to use will be a representation of our reality, our world, our culture, and our beliefs, even without intention.

Burke describes two different types of terministic screens: scientistic and dramatistic. Scientistic begins with a definition of a term; it describes the term as what it is or what it is not, putting the term in black and white. When defining, the essential function is either attitudinal or hortatory. In other words, the focus is on expressions or commands. When terms are treated as hortatory, they are developed. Burke comments on why he uses developed rather than another word. "I say 'developed'; I do not say 'originating'. The ultimate origins of language seem to me as mysterious as the origins of the universe itself. One must view it, I feel, simply as the 'given' ". The dramatistic approach concerns action: thou shalt or thou shalt not. This screen directs the audience toward action based on interpretation of a term. Via terministic screens, the audience will be able to associate with the term or dissociate from it.

"Social constructionism is a metaphor that attempts to capture the way Burke viewed the nature of the world and the function of language therein." Symbols, terms, and language build our view of life. Social constructionism allows us to look at Burke's theory in terms we recognize and are comfortable with.

When a person says gender, most people, based on their individual beliefs, normally think of male or female. However, some could think of intersex individuals. If someone says they think of male, female, and intersex, more would be reflected about the person based on their terminology. Still others would recognize gender as different from biological sex, and say they think of man, woman, and other genders. Another example occurs within the abortion controversy. A pro-choice advocate would most likely use the word "fetus" but pro-life advocates would use the word "baby", because the term stirs more realistic and relatable images and has a bearing on the legal status. Using the word "baby" versus "fetus" defines reality differently (scientistic) and guides people to act in a certain way (dramatistic) solely based on term selection that may be unconscious.

Words are absolute for thought and action. In other words, language is needed to perform thought and action. One can not think without language, and therefore can not act without language and thought. In his "Definition of Man" Burke refers to man as the "symbol using animal" because of man's capacity to use a complex web of symbol systems (language) for meaning making. According to Burke, individuals create terministic screens consciously and unconsciously, as they perceive the world and share perspectives. Burke contends these screens set up a network of beliefs through which all ideas will be interpreted. Communication scholar Paul Stob contends that the language we use is thus not just a direct reflection of our intelligence, but also of perception and culture. David Blakesley posits that the terministic screen enables the further understanding of rhetorical perspectives. In each of these ways, the terministic screen allows for concepts to be interpreted in different ways by different people and contribute to the complexity of meaning.



</doc>
<doc id="21173" url="https://en.wikipedia.org/wiki?curid=21173" title="Natural language">
Natural language

In neuropsychology, linguistics, and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.

Though the exact definition varies between scholars, natural language can broadly be defined in contrast to artificial or constructed languages (such as computer programming languages and international auxiliary languages) and to other communication systems in nature. Examples of such communication systems include bees' waggle dance and whale song, to which researchers have found or applied the linguistic cognates of dialect and even syntax. However, classification of animal communication systems as languages is controversial.

All language varieties of world languages are natural languages, although some varieties are subject to greater degrees of published prescriptivism or language regulation than others. Thus nonstandard dialects can be viewed as a wild type in comparison with standard languages. But even an official language with a regulating academy, such as Standard French with the French Academy, is classified as a natural language (for example, in the field of natural language processing), as its prescriptive points do not make it either constructed enough to be classified as a constructed language or controlled enough to be classified as a controlled natural language.

Controlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce or eliminate both ambiguity and complexity (for instance, by cutting down on rarely used superlative or adverbial forms or irregular verbs). The purpose behind the development and implementation of a controlled natural language typically is to aid non-native speakers of a natural language in understanding it, or to ease computer processing of a natural language. An example of a widely used controlled natural language is Simplified English, which was originally developed for aerospace industry maintenance manuals.

Constructed international auxiliary languages such as Esperanto and Interlingua (even those that have native speakers) are not generally considered natural languages. Natural languages have been used to communicate and have evolved in a natural way, whereas Esperanto was designed by L. L. Zamenhof selecting elements from natural languages, not grown from natural fluctuations in vocabulary and syntax. Some natural languages have become naturally "standardized" by children's natural tendency to correct for illogical grammatical structures in their parents' speech, which can be seen in the development of pidgin languages into creole languages (as explained by Steven Pinker in "The Language Instinct"), but this is not the case in many languages, including constructed languages such as Esperanto, where strict rules are in place as an attempt to consciously remove such irregularities. The possible exception to this are true native speakers of such languages. More substantive basis for this designation is that the vocabulary, grammar, and orthography of Interlingua are natural; they have been standardized and presented by a linguistic research body, but they predated it and are not themselves considered a product of human invention. Most experts, however, consider Interlingua to be naturalistic rather than natural. Latino sine flexione, a second naturalistic auxiliary language, is also naturalistic in content but is no longer widely spoken.




</doc>
<doc id="36253964" url="https://en.wikipedia.org/wiki?curid=36253964" title="Origin of speech">
Origin of speech

The origin of speech refers to the more general problem of the origin of language in the context of the physiological development of the human speech organs such as the tongue, lips and vocal organs used to produce phonological units in all human languages.

Although related to the more general problem of the origin of language, the evolution of distinctively human speech capacities has become a distinct and in many ways separate area of scientific research. The topic is a separate one because language is not necessarily spoken: it can equally be written or signed. Speech is in this sense optional, although it is the default modality for language.
Uncontroversially, monkeys, apes and humans, like many other animals, have evolved specialised mechanisms for producing "sound" for purposes of social communication. On the other hand, no monkey or ape uses its "tongue" for such purposes. Our species' unprecedented use of the tongue, lips and other moveable parts seems to place speech in a quite separate category, making its evolutionary emergence an intriguing theoretical challenge in the eyes of many scholars.

The term "modality" means the chosen representational format for encoding and transmitting information. A striking feature of language is that it is "modality-independent." Should an impaired child be prevented from hearing or producing sound, its innate capacity to master a language may equally find expression in signing. Sign languages of the deaf are independently invented and have all the major properties of spoken language except for the modality of transmission. From this it appears that the language centres of the human brain must have evolved to function optimally irrespective of the selected modality.

This feature is extraordinary. Animal communication systems routinely combine visible with audible properties and effects, but no one is modality-independent. No vocally impaired whale, dolphin or songbird, for example, could express its song repertoire equally in visual display. Indeed, in the case of animal communication, message and modality are not capable of being disentangled. Whatever message is being conveyed stems from the intrinsic properties of the signal.

Modality independence should not be confused with the ordinary phenomenon of multimodality. Monkeys and apes rely on a repertoire of species-specific "gesture-calls" — emotionally expressive vocalisations inseparable from the visual displays which accompany them. Humans also have species-specific gesture-calls — laughs, cries, sobs and so forth — together with involuntary gestures accompanying speech. Many animal displays are polymodal in that each appears designed to exploit multiple channels simultaneously.

The human linguistic property of "modality independence" is conceptually distinct from this. It allows the speaker to encode the informational content of a message in a single channel while switching between channels as necessary. Modern city-dwellers switch effortlessly between the spoken word and writing in its various forms — handwriting, typing, e-mail and so forth. Whichever modality is chosen, it can reliably transmit the full message content without external assistance of any kind. When talking on the telephone, for example, any accompanying facial or manual gestures, however natural to the speaker, are not strictly necessary. When typing or manually signing, conversely, there's no need to add sounds. In many Australian Aboriginal cultures, a section of the population — perhaps women observing a ritual taboo — traditionally restrict themselves for extended periods to a silent (manually signed) version of their language. Then, when released from the taboo, these same individuals resume narrating stories by the fireside or in the dark, switching to pure sound without sacrifice of informational content.

Speaking is the default modality for language in all cultures. Humans' first recourse is to encode our thoughts in sound — a method which depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus.

The speech organs, everyone agrees, evolved in the first instance not for speech but for more basic bodily functions such as feeding and breathing. Nonhuman primates have broadly similar organs, but with different neural controls. Apes use their highly flexible, maneuverable tongues for eating but not for vocalizing. When an ape is not eating, fine motor control over its tongue is deactivated. "Either" it is performing gymnastics with its tongue "or" it is vocalising; it cannot perform both activities simultaneously. Since this applies to mammals in general, "Homo sapiens" is exceptional in harnessing mechanisms designed for respiration and ingestion to the radically different requirements of articulate speech.

The word "language" derives from the Latin "lingua," "tongue". Phoneticians agree that the tongue is the most important speech articulator, followed by the lips. A natural language can be viewed as a particular way of using the tongue to express thought.

The human tongue has an unusual shape. In most mammals, it is a long, flat structure contained largely within the mouth. It is attached at the rear to the hyoid bone, situated below the oral level in the pharynx. In humans, the tongue has an almost circular sagittal (midline) contour, much of it lying vertically down an extended pharynx, where it is attached to a hyoid bone in a lowered position. Partly as a result of this, the horizontal (inside-the-mouth) and vertical (down-the-throat) tubes forming the supralaryngeal vocal tract (SVT) are almost equal in length (whereas in other species, the vertical section is shorter). As we move our jaws up and down, the tongue can vary the cross-sectional area of each tube independently by about 10:1, altering formant frequencies accordingly. That the tubes are joined at a right angle permits pronunciation of the vowels [i], [u] and [a], which nonhuman primates cannot do. Even when not performed particularly accurately, in humans the articulatory gymnastics needed to distinguish these vowels yield consistent, distinctive acoustic results, illustrating the quantal nature of human speech sounds. It may not be coincidental that [i], [u] and [a] are the most common vowels in the world's languages. Human tongues are a lot shorter and thinner than other mammals and are composed of a large number of muscles, which helps shape a variety of sounds within the oral cavity. The diversity of sound production is also increased with the human’s ability to open and close the airway, allowing varying amounts of air to exit through the nose. The fine motor movements associated with the tongue and the airway, make humans more capable of producing a wide range of intricate shapes in order to produce sounds at different rates and intensities.

In humans, the lips are important for the production of stops and fricatives, in addition to vowels. Nothing, however, suggests that the lips evolved for those reasons. During primate evolution, a shift from nocturnal to diurnal activity in tarsiers, monkeys and apes (the haplorhines) brought with it an increased reliance on vision at the expense of olfaction. As a result, the snout became reduced and the rhinarium or "wet nose" was lost. The muscles of the face and lips consequently became less constrained, enabling their co-option to serve purposes of facial expression. The lips also became thicker, and the oral cavity hidden behind became smaller. "Hence", according to one major authority, "the evolution of mobile, muscular lips, so important to human speech, was the exaptive result of the evolution of diurnality and visual communication in the common ancestor of haplorhines". It is unclear whether our lips have undergone a more recent adaptation to the specific requirements of speech.

Compared with nonhuman primates, humans have significantly enhanced control of breathing, enabling exhalations to be extended and inhalations shortened as we speak. While we are speaking, intercostal and interior abdominal muscles are recruited to expand the thorax and draw air into the lungs, and subsequently to control the release of air as the lungs deflate. The muscles concerned are markedly more innervated in humans than in nonhuman primates. Evidence from fossil hominins suggests that the necessary enlargement of the vertebral canal, and therefore spinal cord dimensions, may not have occurred in "Australopithecus" or "Homo erectus" but was present in the Neanderthals and early modern humans.

The larynx or voice box is an organ in the neck housing the vocal folds, which are responsible for phonation. In humans, the larynx is "descended," it is positioned lower than in other primates.This is because the evolution of humans to an upright position shifted the head directly above the spinal cord, forcing everything else downward. The repositioning of the larynx resulted in a longer cavity called the pharynx, which is responsible for increasing the range and clarity of the sound being produced. Other primates have almost no pharynx; therefore, their vocal power is significantly lower. Our species is not unique in this respect: goats, dogs, pigs and tamarins lower the larynx temporarily, to emit loud calls. Several deer species have a permanently lowered larynx, which may be lowered still further by males during their roaring displays. Lions, jaguars, cheetahs and domestic cats also do this. However, laryngeal descent in nonhumans (according to Philip Lieberman) is not accompanied by descent of the hyoid; hence the tongue remains horizontal in the oral cavity, preventing it from acting as a pharyngeal articulator.
Despite all this, scholars remain divided as to how "special" the human vocal tract really is. It has been shown that the larynx does descend to some extent during development in chimpanzees, followed by hyoidal descent. As against this, Philip Lieberman points out that only humans have evolved permanent and substantial laryngeal descent in association with hyoidal descent, resulting in a curved tongue and two-tube vocal tract with 1:1 proportions. Uniquely in the human case, simple contact between the epiglottis and velum is no longer possible, disrupting the normal mammalian separation of the respiratory and digestive tracts during swallowing. Since this entails substantial costs — increasing the risk of choking while swallowing food — we are forced to ask what benefits might have outweighed those costs. The obvious benefit — so it is claimed — must have been speech. But this idea has been vigorously contested. One objection is that humans are in fact "not" seriously at risk of choking on food: medical statistics indicate that accidents of this kind are extremely rare. Another objection is that in the view of most scholars, speech as we know it emerged relatively late in human evolution, roughly contemporaneously with the emergence of "Homo sapiens." A development as complex as the reconfiguration of the human vocal tract would have required much more time, implying an early date of origin. This discrepancy in timescales undermines the idea that human vocal flexibility was "initially" driven by selection pressures for speech.

At least one orangutan has demonstrated the ability to control the voice box.

To lower the larynx is to increase the length of the vocal tract, in turn lowering formant frequencies so that the voice sounds "deeper" — giving an impression of greater size. John Ohala argues that the function of the lowered larynx in humans, especially males, is probably to enhance threat displays rather than speech itself. Ohala points out that if the lowered larynx were an adaptation for speech, we would expect adult human males to be better adapted in this respect than adult females, whose larynx is considerably less low. In fact, females invariably outperform males in verbal tests, falsifying this whole line of reasoning. W. Tecumseh Fitch likewise argues that this was the original selective advantage of laryngeal lowering in our species. Although (according to Fitch) the initial lowering of the larynx in humans had nothing to do with speech, the increased range of possible formant patterns was subsequently co-opted for speech. Size exaggeration remains the sole function of the extreme laryngeal descent observed in male deer. Consistent with the size exaggeration hypothesis, a second descent of the larynx occurs at puberty in humans, although only in males. In response to the objection that the larynx is descended in human females, Fitch suggests that mothers vocalizing to protect their infants would also have benefited from this ability.

Most specialists credit the Neanderthals with speech abilities not radically different from those of modern "Homo sapiens". An indirect line of argument is that their tool-making and hunting tactics would have been difficult to learn or execute without some kind of speech. A recent extraction of DNA from Neanderthal bones indicates that Neanderthals had the same version of the FOXP2 gene as modern humans. This gene, once mistakenly described as the "grammar gene", plays a role in controlling the orofacial movements which (in modern humans) are involved in speech.

During the 1970s, it was widely believed that the Neanderthals lacked modern speech capacities. It was claimed that they possessed a hyoid bone so high up in the vocal tract as to preclude the possibility of producing certain vowel sounds.

The hyoid bone is present in many mammals. It allows a wide range of tongue, pharyngeal and laryngeal movements by bracing these structures alongside each other in order to produce variation. It is now realised that its lowered position is not unique to "Homo sapiens", while its relevance to vocal flexibility may have been overstated: although men have a lower larynx, they do not produce a wider range of sounds than women or two-year-old babies. There is no evidence that the larynx position of the Neanderthals impeded the range of vowel sounds they could produce. The discovery of a modern-looking hyoid bone of a Neanderthal man in the Kebara Cave in Israel led its discoverers to argue that the Neanderthals had a descended larynx, and thus human-like speech capabilities. However, other researchers have claimed that the morphology of the hyoid is not indicative of the larynx's position. It is necessary to take into consideration the skull base, the mandible and the cervical vertebrae and a cranial reference plane.

The morphology of the outer and middle ear of Middle Pleistocene hominins from Atapuerca SH in Spain, believed to be proto-Neanderthal, suggests they had an auditory sensitivity similar to modern humans and very different from chimpanzees. They were probably able to differentiate between many different speech sounds.

The hypoglossal nerve plays an important role in controlling movements of the tongue. In 1998, one research team used the size of the hypoglossal canal in the base of fossil skulls in an attempt to estimate the relative number of nerve fibres, claiming on this basis that Middle Pleistocene hominins and Neanderthals had more fine-tuned tongue control than either australopithecines or apes. Subsequently, however, it was demonstrated that hypoglossal canal size and nerve sizes are not correlated, and it is now accepted that such evidence is uninformative about the timing of human speech evolution.

According to one influential school, the human vocal apparatus is intrinsically digital on the model of a keyboard or digital computer. If so, this is remarkable: nothing about a chimpanzee's vocal apparatus suggests a digital keyboard, notwithstanding the anatomical and physiological similarities. This poses the question as to when and how, during the course of human evolution, the transition from analog to digital structure and function occurred.

The human supralaryngeal tract is said to be digital in the sense that it is an arrangement of moveable toggles or switches, each of which, at any one time, must be in one state or another. The vocal cords, for example, are either vibrating (producing a sound) or not vibrating (in silent mode). By virtue of simple physics, the corresponding distinctive feature — in this case, "voicing" — cannot be somewhere in between. The options are limited to "off" and "on". Equally digital is the feature known as "nasalisation". At any given moment the soft palate or velum either allows or doesn't allow sound to resonate in the nasal chamber. In the case of lip and tongue positions, more than two digital states may be allowed.

The theory that speech sounds are composite entities constituted by complexes of binary phonetic features was first advanced in 1938 by the Russian linguist Roman Jakobson. A prominent early supporter of this approach was Noam Chomsky, who went on to extend it from phonology to language more generally, in particular to the study of syntax and semantics. In his 1965 book, "Aspects of the Theory of Syntax," Chomsky treated semantic concepts as combinations of binary-digital atomic elements explicitly on the model of distinctive features theory. The lexical item "bachelor", on this basis, would be expressed as [+ Human], [+ Male], [- Married].

Supporters of this approach view the vowels and consonants recognized by speakers of a particular language or dialect at a particular time as cultural entities of little scientific interest. From a natural science standpoint, the units which matter are those common to "Homo sapiens" by virtue of our biological nature. By combining the atomic elements or "features" with which all humans are innately equipped, anyone may in principle generate the entire range of vowels and consonants to be found in any of the world's languages, whether past, present or future. The distinctive features are in this sense atomic components of a universal language.

In recent years, the notion of an innate "universal grammar" underlying phonological variation has been called into question. The most comprehensive monograph ever written about speech sounds, "Sounds of the World's Languages," by Peter Ladefoged and Ian Maddieson, found virtually no basis for the postulation of some small number of fixed, discrete, universal phonetic features. Examining 305 languages, for example, they encountered vowels that were positioned basically everywhere along the articulatory and acoustic continuum. Ladefoged concludes that phonological features are not determined by human nature: "Phonological features are best regarded as artifacts that linguists have devised in order to describe linguistic systems." The controversy remains unresolved.

Self-organization characterizes systems where macroscopic structures are spontaneously formed out of local interactions between the many components of the system. In self-organized systems, global organizational properties are not to be found at the local level. In colloquial terms, self-organization is roughly captured by the idea of "bottom-up" (as opposed to "top-down") organization. Examples of self-organized systems range from ice crystals to galaxy spirals in the inorganic world, and from spots on the leopard skins to the architecture of termite nests or shape of a flock of starlings.
According to many phoneticians, the sounds of language arrange and re-arrange themselves through self-organization Speech sounds have both perceptual ("how you hear them") and articulatory ("how you produce them") properties, all with continuous values. Speakers tend to minimize effort, favoring ease of articulation over clarity. Listeners do the opposite, favoring sounds that are easy to distinguish even if difficult to pronounce. Since speakers and listeners are constantly switching roles, the syllable systems actually found in the world's languages turn out to be a compromise between acoustic distinctiveness on the one hand, and articulatory ease on the other.

How, precisely, do systems of vowels, consonants and syllables arise? Agent-based computer models take the perspective of self-organisation at the level of the speech community or population. The two main paradigms here are (1) the iterated learning model and (2) the language game model. Iterated learning focuses on transmission from generation to generation, typically with just one agent in each generation.
In the language game model, a whole population of agents simultaneously produce, perceive and learn language, inventing novel forms when the need arises.

Several models have shown how relatively simple peer-to-peer vocal interactions, such as imitation, can spontaneously self-organize a system of sounds shared by the whole population, and different in different populations. For example, models elaborated by Berrah et al., as well as de Boer, and recently reformulated using Bayesian theory, showed how a group of individuals playing imitation games can self-organize repertoires of vowel sounds which share substantial properties with human vowel systems. For example, in de Boer's model, initially vowels are generated randomly, but agents learn from each other as they interact repeatedly over time. Agent A chooses a vowel from her repertoire and produces it, inevitably with some noise. Agent B hears this vowel and chooses the closest equivalent from her own repertoire. To check whether this truly matches the original, B produces the vowel "she thinks she has heard", whereupon A refers once again to her own repertoire to find the closest equivalent. If this matches the one she initially selected, the game is successful, otherwise, it has failed. "Through repeated interactions," according to de Boer, "vowel systems emerge that are very much like the ones found in human languages."

In a different model, the phonetician Björn Lindblom was able to predict, on self-organizational grounds, the favored choices of vowel systems ranging from three to nine vowels on the basis of a principle of optimal perceptual differentiation.

Further models studied the role of self-organization in the origins of phonemic coding and combinatoriality, which is the existence of phonemes and their systematic reuse to build structured syllables. Pierre-Yves Oudeyer developed models which showed that basic neural equipment for adaptive holistic vocal imitation, coupling directly motor and perceptual representations in the brain, can generate spontaneously shared combinatorial systems of vocalizations, including phonotactic patterns, in a society of babbling individuals. These models also characterized how morphological and physiological innate constraints can interact with these self-organized mechanisms to account for both the formation of statistical regularities and diversity in vocalization systems.

The gestural theory states that speech was a relatively late development, evolving by degrees from a system that was originally gestural. Our ancestors were unable to control their vocalization at the time when gestures were used to communicate; however, as they slowly began to control their vocalizations, spoken language began to evolve.

Three types of evidence support this theory:

Research has found strong support for the idea that spoken language and signing depend on similar neural structures. Patients who used sign language, and who suffered from a left-hemisphere lesion, showed the same disorders with their sign language as vocal patients did with their oral language. Other researchers found that the same left-hemisphere brain regions were active during sign language as during the use of vocal or written language.

Humans spontaneously use hand and facial gestures when formulating ideas to be conveyed in speech. There are also, of course, many sign languages in existence, commonly associated with deaf communities; as noted above, these are equal in complexity, sophistication, and expressive power, to any oral language. The main difference is that the "phonemes" are produced on the outside of the body, articulated with hands, body, and facial expression, rather than inside the body articulated with tongue, teeth, lips, and breathing.

Many psychologists and scientists have looked into the mirror system in the brain to answer this theory as well as other behavioral theories. Evidence to support mirror neurons as a factor in the evolution of speech includes mirror neurons in primates, the success of teaching apes to communicate gesturally, and pointing/gesturing to teach young children language. Fogassi and Ferrari (2014) monitored motor cortex activity in monkeys, specifically area F5 in the Broca’s area, where mirror neurons are located. They observed changes in electrical activity in this area when the monkey executed or observed different hand actions performed by someone else. Broca’s area is a region in the frontal lobe responsible for language production and processing. The discovery of mirror neurons in this region, which fire when an action is done or observed specifically with the hand, strongly supports the belief that communication was once accomplished with gestures. The same is true when teaching young children language. When one points at a specific object or location, mirror neurons in the child fire as though they were doing the action, which results in long term learning 

Critics note that for mammals in general, sound turns out to be the best medium in which to encode information for transmission over distances at speed. Given the probability that this applied also to early humans, it's hard to see why they should have abandoned this efficient method in favor of more costly and cumbersome systems of visual gesturing — only to return to sound at a later stage.

By way of explanation, it has been proposed that at a relatively late stage in human evolution, our ancestors' hands became so much in demand for making and using tools that the competing demands of manual gesturing became a hindrance. The transition to spoken language is said to have occurred only at that point. Since humans throughout evolution have been making and using tools, however, most scholars remain unconvinced by this argument. (For a different approach to this puzzle — one setting out from considerations of signal reliability and trust — see "from pantomime to speech" below).

Little is known about the timing of language's emergence in the human species. Unlike writing, speech leaves no material trace, making it archaeologically invisible. Lacking direct linguistic evidence, specialists in human origins have resorted to the study of anatomical features and genes arguably associated with speech production. While such studies may provide information as to whether pre-modern "Homo" species had speech "capacities", it is still unknown whether they actually spoke. While they may have communicated vocally, the anatomical and genetic data lack the resolution necessary to differentiate proto-language from speech.

Using statistical methods to estimate the time required to achieve the current spread and diversity in modern languages today, Johanna Nichols — a linguist at the University of California, Berkeley — argued in 1998 that vocal languages must have begun diversifying in our species at least 100,000 years ago.

More recently — in 2012 — anthropologists Charles Perreault and Sarah Mathew used phonemic diversity to suggest a date consistent with this. "Phonemic diversity" denotes the number of perceptually distinct units of sound — consonants, vowels and tones — in a language. The current worldwide pattern of phonemic diversity potentially contains the statistical signal of the expansion of modern "Homo sapiens" out of Africa, beginning around 60-70 thousand years ago. Some scholars argue that phonemic diversity evolves slowly and can be used as a clock to calculate how long the oldest African languages would have to have been around in order to accumulate the number of phonemes they possess today. As human populations left Africa and expanded into the rest of the world, they underwent a series of bottlenecks — points at which only a very small population survived to colonise a new continent or region. Allegedly such a population crash led to a corresponding reduction in genetic, phenotypic and phonemic diversity. African languages today have some of the largest phonemic inventories in the world, while the smallest inventories are found in South America and Oceania, some of the last regions of the globe to be colonized. For example, Rotokas, a language of New Guinea, and Pirahã, spoken in South America, both have just 11 phonemes, while !Xun, a language spoken in Southern Africa has 141 phonemes.
The authors use a natural experiment — the colonization of mainland Southeast Asia on the one hand, the long-isolated Andaman Islands on the other — to estimate the rate at which phonemic diversity increases through time. Using this rate, they estimate that the world's languages date back to the Middle Stone Age in Africa, sometime between 350 thousand and 150 thousand years ago. This corresponds to the speciation event which gave rise to "Homo sapiens".

These and similar studies have however been criticized by linguists who argue that they are based on a flawed analogy between genes and phonemes, since phonemes are frequently transferred laterally between languages unlike genes, and on a flawed sampling of the world's languages, since both Oceania and the Americas also contain languages with very high numbers of phonemes, and Africa contains languages with very few. They argue that the actual distribution of phonemic diversity in the world reflects recent language contact and not deep language history - since it is well demonstrated that languages can lose or gain many phonemes over very short periods. In other words, there is no valid linguistic reason to expect genetic founder effects to influence phonemic diversity.

In 1861, historical linguist Max Müller published a list of speculative theories concerning the origins of spoken language: These theories have been grouped under the category named invention hypotheses. These hypotheses were all meant to understand how the first language could have developed and postulate that human mimicry of natural sounds were how the first words with meaning were derived.


A common concept of onomatopoeia as the first source of words is present; however, there is a glaring problem with this theory. Onomatopoeia can explain the first couple of words all derived from natural phenomenon, but there is no explanation as to how more complex words without a natural counterpart came to be. Most scholars today consider all such theories not so much wrong — they occasionally offer peripheral insights — as comically naïve and irrelevant. The problem with these theories is that they are so narrowly mechanistic. They assume that once our ancestors had stumbled upon the appropriate ingenious "mechanism" for linking sounds with meanings, language automatically evolved and changed.

From the perspective of modern science, the main obstacle to the evolution of speech-like communication in nature is not a mechanistic one. Rather, it is that symbols — arbitrary associations of sounds with corresponding meanings — are unreliable and may well be false. As the saying goes, "words are cheap". The problem of reliability was not recognised at all by Darwin, Müller or the other early evolutionist theorists.

Animal vocal signals are for the most part intrinsically reliable. When a cat purrs, the signal constitutes direct evidence of the animal's contented state. One can "trust" the signal not because the cat is inclined to be honest, but because it just can't fake that sound. Primate vocal calls may be slightly more manipulable, but they remain reliable for the same reason — because they are hard to fake. Primate social intelligence is "Machiavellian" — self-serving and unconstrained by moral scruples. Monkeys and apes often attempt to deceive one another, while at the same time constantly on guard against falling victim to deception themselves. Paradoxically, it is precisely primates' resistance to deception that blocks the evolution of their vocal communication systems along language-like lines. Language is ruled out because the best way to guard against being deceived is to ignore all signals except those that are instantly verifiable. Words automatically fail this test.

Words are easy to fake. Should they turn out to be lies, listeners will adapt by ignoring them in favor of hard-to-fake indices or cues. For language to work, then, listeners must be confident that those with whom they are on speaking terms are generally likely, to be honest. A peculiar feature of language is "displaced reference", which means reference to topics outside the currently perceptible situation. This property prevents utterances from being corroborated in the immediate "here" and "now". For this reason, language presupposes relatively high levels of mutual trust in order to become established over time as an evolutionarily stable strategy. A theory of the origins of language must, therefore, explain why humans could begin trusting cheap signals in ways that other animals apparently cannot (see signalling theory).

The "mother tongues" hypothesis was proposed in 2004 as a possible solution to this problem. W. Tecumseh Fitch suggested that the Darwinian principle of "kin selection" — the convergence of genetic interests between relatives — might be part of the answer. Fitch suggests that spoken languages were originally "mother tongues". If speech evolved initially for communication between mothers and their own biological offspring, extending later to include adult relatives as well, the interests of speakers and listeners would have tended to coincide. Fitch argues that shared genetic interests would have led to sufficient trust and cooperation for intrinsically unreliable vocal signals — spoken words — to become accepted as trustworthy and so begin evolving for the first time.

Critics of this theory point out that kin selection is not unique to humans. Ape mothers also share genes with their offspring, as do all animals, so why is it only humans who speak? Furthermore, it is difficult to believe that early humans restricted linguistic communication to genetic kin: the incest taboo must have forced men and women to interact and communicate with non-kin. So even if we accept Fitch's initial premises, the extension of the posited "mother tongue" networks from relatives to non-relatives remains unexplained.

Ib Ulbæk invokes another standard Darwinian principle — "reciprocal altruism" — to explain the unusually high levels of intentional honesty necessary for language to evolve. 'Reciprocal altruism' can be expressed as the principle that "if you scratch my back, I'll scratch yours." In linguistic terms, it would mean that "if you speak truthfully to me, I'll speak truthfully to you." Ordinary Darwinian reciprocal altruism, Ulbæk points out, is a relationship established between frequently interacting individuals. For language to prevail across an entire community, however, the necessary reciprocity would have needed to be enforced universally instead of being left to individual choice. Ulbæk concludes that for language to evolve, early society as a whole must have been subject to moral regulation.

Critics point out that this theory fails to explain when, how, why or by whom "obligatory reciprocal altruism" could possibly have been enforced. Various proposals have been offered to remedy this defect. A further criticism is that language doesn't work on the basis of reciprocal altruism anyway. Humans in conversational groups don't withhold information to all except listeners likely to offer valuable information in return. On the contrary, they seem to want to advertise to the world their access to socially relevant information, broadcasting it to anyone who will listen without thought of return.

Gossip, according to Robin Dunbar, does for group-living humans what manual grooming does for other primates — it allows individuals to service their relationships and so maintain their alliances. As humans began living in larger and larger social groups, the task of manually grooming all one's friends and acquaintances became so time-consuming as to be unaffordable. In response to this problem, humans invented "a cheap and ultra-efficient form of grooming" — "vocal grooming". To keep your allies happy, you now needed only to "groom" them with low-cost vocal sounds, servicing multiple allies simultaneously while keeping both hands free for other tasks. Vocal grooming (the production of pleasing sounds lacking syntax or combinatorial semantics) then evolved somehow into syntactical speech.

Critics of this theory point out that the very efficiency of "vocal grooming" — that words are so cheap — would have undermined its capacity to signal commitment of the kind conveyed by time-consuming and costly manual grooming. A further criticism is that the theory does nothing to explain the crucial transition from vocal grooming — the production of pleasing but meaningless sounds — to the cognitive complexities of syntactical speech.

According to another school of thought, language evolved from mimesis — the "acting out" of scenarios using vocal and gestural pantomime. Charles Darwin, who himself was skeptical, hypothesized that human speech and language is derived from gestures and mouth pantomime. This theory, further elaborated on by various authors, postulates that the genus "Homo", different from our ape ancestors, evolved a new type of cognition. Apes are capable of associational learning. They can tie a sensory cue to a motor response often trained through classical conditioning. However, in apes,  the conditioned sensory cue is necessary for a conditioned response to be observed again. The motor response will not occur without an external cue from an outside agent. A remarkable ability that humans possess is the ability to voluntarily retrieve memories without the need for a cue (e.g. conditioned stimulus). This is not an ability that has been observed in animals except language-trained apes. There is still much controversy on whether pantomime is a capability for apes, both wild and captured. For as long as utterances needed to be emotionally expressive and convincing, it was not possible to complete the transition to purely conventional signs. On this assumption, pre-linguistic gestures and vocalisations would have been required not just to disambiguate intended meanings, but also to inspire confidence in their intrinsic reliability. If contractual commitments were necessary in order to inspire community-wide trust in communicative intentions, it would follow that these had to be in place before humans could shift at last to an ultra-efficient, high-speed — digital as opposed to analog — signalling format. Vocal distinctive features (sound contrasts) are ideal for this purpose. It is therefore suggested that the establishment of contractual understandings enabled the decisive transition from mimetic gesture to fully conventionalised, digitally encoded speech.

The ritual/speech coevolution theory was originally proposed by the distinguished social anthropologist Roy Rappaport before being elaborated by anthropologists such as Chris Knight, Jerome Lewis, Nick Enfield, Camilla Power and Ian Watts. Cognitive scientist and robotics engineer Luc Steels is another prominent supporter of this general approach, as is biological anthropologist/neuroscientist Terrence Deacon.

These scholars argue that there can be no such thing as a "theory of the origins of language". This is because language is not a separate adaptation but an internal aspect of something much wider — namely, human symbolic culture as a whole. Attempts to explain language independently of this wider context have spectacularly failed, say these scientists, because they are addressing a problem with no solution. Can we imagine a historian attempting to explain the emergence of credit cards independently of the wider system of which they are a part? Using a credit card makes sense only if you have a bank account institutionally recognized within a certain kind of advanced capitalist society — one where communications technology has already been invented and fraud can be detected and prevented. In much the same way, language would not work outside a specific array of social mechanisms and institutions. For example, it would not work for an ape communicating with other apes in the wild. Not even the cleverest ape could make language work under such conditions.

Speech consists of digital contrasts whose cost is essentially zero. As pure social conventions, signals of this kind cannot evolve in a Darwinian social world — they are a theoretical impossibility. Being intrinsically unreliable, language works only if you can build up a reputation for trustworthiness within a certain kind of society — namely, one where symbolic cultural facts (sometimes called "institutional facts") can be established and maintained through collective social endorsement. In any hunter-gatherer society, the basic mechanism for establishing trust in symbolic cultural facts is collective "ritual". Therefore, the task facing researchers into the origins of language is more multidisciplinary than is usually supposed. It involves addressing the evolutionary emergence of human symbolic culture as a whole, with language an important but subsidiary component.

Critics of the theory include Noam Chomsky, who terms it the "non-existence" hypothesis — a denial of the very existence of language as an object of study for natural science. Chomsky's own theory is that language emerged in an instant and in perfect form, prompting his critics in turn to retort that only something that doesn't exist — a theoretical construct or convenient scientific fiction — could possibly emerge in such a miraculous way. The controversy remains unresolved.

The essay "The festal origin of human speech", though published in the late nineteenth century, made little impact until the American philosopher Susanne Langer re-discovered and publicised it in 1941. 
The theory sets out from the observation that primate vocal sounds are above all "emotionally" expressive. The emotions aroused are socially contagious. Because of this, an extended bout of screams, hoots or barks will tend to express not just the feelings of this or that individual but the mutually contagious ups and downs of everyone within earshot.

Turning to the ancestors of "Homo sapiens", the "festal origin" theory suggests that in the "play-excitement" preceding or following a communal hunt or other group activity, everyone might have combined their voices in a comparable way, emphasizing their mood of togetherness with such noises as rhythmic drumming and hand-clapping. Variably pitched voices would have formed conventional patterns, such that choral singing became an integral part of communal celebration.

Although this was not yet speech, according to Langer, it developed the vocal capacities from which speech would later derive. There would be conventional modes of ululating, clapping or dancing appropriate to different festive occasions, each so intimately associated with "that kind of occasion" that it would tend to collectively uphold and embody the concept of it. Anyone hearing a snatch of sound from such a song would recall the associated occasion and mood. A melodic, rhythmic sequence of syllables conventionally associated with a certain type of celebration would become, in effect, its vocal mark. On that basis, certain familiar sound sequences would become "symbolic".

In support of all this, Langer cites ethnographic reports of tribal songs consisting entirely of "rhythmic nonsense syllables". She concedes that an English equivalent such as "hey-nonny-nonny", although perhaps suggestive of certain feelings or ideas, is neither noun, verb, adjective, nor any other syntactical part of speech. So long as articulate sound served only in the capacity of "hey nonny-nonny", "hallelujah" or "alack-a-day", it cannot yet have been speech. For that to arise, according to Langer, it was necessary for such sequences to be emitted increasingly "out of context" — outside the total situation that gave rise to them. Extending a set of associations from one cognitive context to another, completely different one, is the secret of "metaphor". Langer invokes an early version of what is nowadays termed "grammaticalization" theory to show how, from, such a point of departure, syntactically complex speech might progressively have arisen.

Langer acknowledges Emile Durkheim as having proposed a strikingly similar theory back in 1912. For recent thinking along broadly similar lines, see Steven Brown on "musilanguage", Chris Knight on "ritual" and "play", Jerome Lewis on "mimicry", Steven Mithen on "Hmmmmm" Bruce Richman on "nonsense syllables" and Alison Wray on "holistic protolanguage".

Mirror neuron hypothesis (MSH) and the Motor Theory of Speech Perception

The mirror neuron hypothesis, based on a phenomenon discovered in 2008 by Rizzolatti and Fabbri, supports the motor theory of speech perception. The motor theory of speech perception was proposed in 1967 by Liberman, who believed that the motor system and language systems were closely interlinked. This would result in a more streamlined process of generating speech; both the cognition and speech formulation could occur simultaneously. Essentially, it is wasteful to have a speech decoding and speech encoding process independent of each other. This hypothesis was further supported by the discovery of motor neurons. Rizzolatti and Fabbri found that there were specific neurons in the motor cortex of macaque monkeys which were activated when seeing an action. The neurons which are activated are the same neurons in which would be required to perform the same action themselves. Mirror neurons fire when observing an action and performing an action, indicating that these neurons found in the motor cortex are necessary for understanding a visual process. The presence of mirror neurons may indicate that non-verbal, gestural communication is far more ancient than previously thought to be. Motor theory of speech perception relies on the understanding of motor representations that underlie speech gestures, such as lip movement. There is no clear understanding of speech perception currently, but it is generally accepted that the motor cortex is activated in speech perception to some capacity.

The term "musilanguage" (or "hmmmmm") refers to a pre-linguistic system of vocal communication from which (according to some scholars) "both" music "and" language later derived. The idea is that rhythmic, melodic, emotionally expressive vocal ritual helped bond coalitions and, over time, set up selection pressures for enhanced volitional control over the speech articulators. Patterns of synchronized choral chanting are imagined to have varied according to the occasion. For example, "we're setting off to find honey" might sound qualitatively different from "we're setting off to hunt" or "we're grieving over our relative's death". If social standing depended on maintaining a regular beat and harmonizing one's own voice with that of everyone else, group members would have come under pressure to demonstrate their choral skills.

Archaeologist Steven Mithen speculates that the Neanderthals possessed some such system, expressing themselves in a "language" known as "Hmmmmm", standing for Holistic, manipulative, multi-modal, musical and mimetic. In Bruce Richman's earlier version of essentially the same idea, frequent repetition of the same few songs by many voices made it easy for people to remember those sequences as whole units. Activities that a group of people were doing while they were vocalizing together — activities that were important or striking or richly emotional — came to be associated with particular sound sequences, so that each time a fragment was heard, it evoked highly specific memories. The idea is that the earliest lexical items (words) started out as abbreviated fragments of what were originally communal songs.

As group members accumulated an expanding repertoire of songs for different occasions, interpersonal call-and-response patterns evolved along one trajectory to assume linguistic form. Meanwhile, along a divergent trajectory, polyphonic singing and other kinds of music became increasingly specialized and sophisticated.

To explain the establishment of syntactical speech, Richman cites English "I wanna go home". He imagines this to have been learned in the first instance not as a combinatorial sequence of free-standing words, but as a single stuck-together combination — the melodic sound people make to express "feeling homesick". Someone might sing "I wanna go home", prompting other voices to chime in with "I need to go home", "I'd love to go home", "Let's go home" and so forth. Note that one part of the song remains constant, while another is permitted to vary. If this theory is accepted, syntactically complex speech began evolving as each chanted mantra allowed for variation at a certain point, allowing for the insertion of an element from some other song. For example, while mourning during a funeral rite, someone might want to recall a memory of collecting honey with the deceased, signaling this at an appropriate moment with a fragment of the "we're collecting honey" song. Imagine that such practices became common. Meaning-laden utterances would now have become subject to a distinctively linguistic creative principle — that of recursive embedding.

Many scholars associate the evolutionary emergence of speech with profound social, sexual, political and cultural developments. One view is that primate-style dominance needed to give way to a more cooperative and egalitarian lifestyle of the kind characteristic of modern hunter-gatherers.

According to Michael Tomasello, the key cognitive capacity distinguishing "Homo sapiens" from our ape cousins is "intersubjectivity". This entails turn-taking and role-reversal: your partner strives to read your mind, you simultaneously strive to read theirs, and each of you makes a conscious effort to assist the other in the process. The outcome is that each partner forms a representation of the other's mind in which their own can be discerned by reflection.

Tomasello argues that this kind of bi-directional cognition is central to the very possibility of linguistic communication. Drawing on his research with both children and chimpanzees, he reports that human infants, from one year old onwards, begin viewing their own mind as if from the standpoint of others. He describes this as a cognitive revolution. Chimpanzees, as they grow up, never undergo such a revolution. The explanation, according to Tomasello, is that their evolved psychology is adapted to a deeply competitive way of life. Wild-living chimpanzees from despotic social hierarchies, most interactions involving calculations of dominance and submission. An adult chimp will strive to outwit its rivals by guessing at their intentions while blocking them from reciprocating. Since bi-directional intersubjective communication is impossible under such conditions, the cognitive capacities necessary for language don't evolve.

In the scenario favoured by David Erdal and Andrew Whiten, primate-style dominance provoked equal and opposite coalitionary resistance — "counter-dominance." During the course of human evolution, increasingly effective strategies of rebellion against dominant individuals led to a compromise. While abandoning any attempt to dominate others, group members vigorously asserted their personal autonomy, maintaining their alliances to make potentially dominant individuals think twice. Within increasingly stable coalitions, according to this perspective, status began to be earned in novel ways, social rewards accruing to those perceived by their peers as especially cooperative and self-aware.

While counter-dominance, according to this evolutionary narrative, culminates in a stalemate, anthropologist Christopher Boehm extends the logic a step further. Counter-dominance tips over at last into full-scale "reverse dominance". The rebellious coalition decisively overthrows the figure of the primate alpha-male. No dominance is allowed except that of the self-organized community as a whole.

As a result of this social and political change, hunter-gatherer egalitarianism is established. As children grow up, they are motivated by those around them to reverse perspective, engaging with other minds on the model of their own. Selection pressures favor such psychological innovations as imaginative empathy, joint attention, moral judgment, project-oriented collaboration and the ability to evaluate one's own behavior from the standpoint of others. Underpinning enhanced probabilities of cultural transmission and cumulative cultural evolution, these developments culminated in the establishment of hunter-gatherer-style egalitarianism in association with intersubjective communication and cognition. It is in this social and political context that language evolves.

According to Dean Falk's "putting the baby down" theory, vocal interactions between early hominin mothers and infants sparked a sequence of events that led, eventually, to our ancestors' earliest words. The basic idea is that evolving human mothers, unlike their monkey and ape counterparts, couldn't move around and forage with their infants clinging onto their backs. Loss of fur in the human case left infants with no means of clinging on. Frequently, therefore, mothers had to put their babies down. As a result, these babies needed reassurance that they were not being abandoned. Mothers responded by developing "motherese" — an infant-directed communicative system embracing facial expressions, body language, touching, patting, caressing, laughter, tickling and emotionally expressive contact calls. The argument is that language somehow developed out of all this.

While this theory may explain a certain kind of infant-directed "protolanguage" — known today as "motherese" — it does little to solve the really difficult problem, which is the emergence among adults of syntactical speech. 

Evolutionary anthropologist Sarah Hrdy observes that only human mothers among great apes are willing to let another individual take hold of their own babies; further, we are routinely willing to let others babysit. She identifies lack of trust as the major factor preventing chimp, bonobo or gorilla mothers from doing the same: "If ape mothers insist on carrying their babies everywhere ... it is because the available alternatives are not safe enough." The fundamental problem is that ape mothers (unlike monkey mothers who may often babysit) do not have female relatives nearby. The strong implication is that, in the course of "Homo" evolution, allocare could develop because "Homo" mothers did have female kin close by — in the first place, most reliably, their own mothers. Extending the Grandmother hypothesis, Hrdy argues that evolving "Homo erectus" females necessarily relied on female kin initially; this novel situation in ape evolution of mother, infant and mother's mother as allocarer provided the evolutionary ground for the emergence of intersubjectivity. She relates this onset of "cooperative breeding in an ape" to shifts in life history and slower child development, linked to the change in brain and body size from the 2 million year mark.

Primatologist Klaus Zuberbühler uses these ideas to help explain the emergence of vocal flexibility in the human species. Co-operative breeding would have compelled infants to struggle actively to gain the attention of caregivers, not all of whom would have been directly related. A basic primate repertoire of vocal signals may have been insufficient for this social challenge. Natural selection, according to this view, would have favored babies with advanced vocal skills, beginning with babbling (which triggers positive responses in care-givers) and paving the way for the elaborate and unique speech abilities of modern humans.

These ideas might be linked to those of the renowned structural linguist Roman Jakobson, who claimed that "the sucking activities of the child are accompanied by a slight nasal murmur, the only phonation to be produced when the lips are pressed to the mother's breast ... and the mouth is full". He proposed that later in the infant's development, "this phonatory reaction to nursing is reproduced as an anticipatory signal at the mere sight of food and finally as a manifestation of a desire to eat, or more generally, as an expression of discontent and impatient longing for missing food or absent nurser, and any ungranted wish." So, the action of opening and shutting the mouth, combined with the production of a nasal sound when the lips are closed, yielded the sound sequence "Mama", which may, therefore, count as the very first word. Peter MacNeilage sympathetically discusses this theory in his major book, The "Origin of Speech", linking it with Dean Falk's "putting the baby down" theory (see above). Needless to say, other scholars have suggested completely different candidates for "Homo sapiens"' very first word.

While the biological language faculty is genetically inherited, actual languages or dialects are culturally transmitted, as are social norms, technological traditions and so forth. Biologists expect a robust co-evolutionary trajectory linking human genetic evolution with the evolution of culture. Individuals capable of rudimentary forms of protolanguage would have enjoyed enhanced access to cultural understandings, while these, conveyed in ways that young brains could readily learn, would, in turn, have become transmitted with increasing efficiency.

In some ways like beavers, as they construct their dams, humans have always engaged in niche construction, creating novel environments to which they subsequently become adapted. Selection pressures associated with prior niches tend to become relaxed as humans depend increasingly on novel environments created continuously by their own productive activities. According to Steven Pinker, language is an adaptation to "the cognitive niche". Variations on the theme of ritual/speech co-evolution — according to which speech evolved for purposes of internal communication within a ritually constructed domain — have attempted to specify more precisely when, why and how this special niche was created by human collaborative activity.

 The Swiss scholar Ferdinand de Saussure founded linguistics as a twentieth-century professional discipline. Saussure regarded a language as a rule-governed system, much like a board game such as chess. In order to understand chess, he insisted, we must ignore such external factors as the weather prevailing during a particular session or the material composition of this or that piece. The game is autonomous with respect to its material embodiments. In the same way, when studying language, it's essential to focus on its internal structure as a social institution. External matters ("e.g.", the shape of the human tongue) are irrelevant from this standpoint. Saussure regarded 'speaking' "(parole)" as individual, ancillary and more or less accidental by comparison with "language" "(langue)", which he viewed as collective, systematic and essential.

Saussure showed little interest in Darwin's theory of evolution by natural selection. Nor did he consider it worthwhile to speculate about how language might originally have evolved. Saussure's assumptions in fact cast doubt on the validity of narrowly conceived origins scenarios. His structuralist paradigm, when accepted in its original form, turns scholarly attention to a wider problem: how our species acquired the capacity to establish social institutions in general.

 In the United States, prior to and immediately following World War II, the dominant psychological paradigm was behaviourism. Within this conceptual framework, language was seen as a certain kind of behaviour — namely, verbal behavior, to be studied much like any other kind of behavior in the animal world. Rather as a laboratory rat learns how to find its way through an artificial maze, so a human child learns the verbal behavior of the society into which it is born. The phonological, grammatical and other complexities of speech are in this sense "external" phenomena, inscribed into an initially unstructured brain. Language's emergence in "Homo sapiens," from this perspective, presents no special theoretical challenge. Human behavior, whether verbal or otherwise, illustrates the malleable nature of the mammalian — and especially the human — brain.

Nativism is the theory that humans are born with certain specialized cognitive modules enabling us to acquire highly complex bodies of knowledge such as the grammar of a language.

From the mid-1950s onwards, Noam Chomsky, Jerry Fodor and others mounted what they conceptualized as a 'revolution' against behaviorism. Retrospectively, this became labelled 'the cognitive revolution'. Whereas behaviorism had denied the scientific validity of the concept of "mind", Chomsky replied that, in fact, the concept of "body" is more problematic. Behaviourists tended to view the child's brain as a "tabula rasa", initially lacking structure or cognitive content. According to B. F. Skinner, for example, richness of behavioral detail (whether verbal or non-verbal) emanated from the environment. Chomsky turned this idea on its head. The linguistic environment encountered by a young child, according to Chomsky's version of psychological nativism, is in fact hopelessly inadequate. No child could possibly acquire the complexities of grammar from such an impoverished source. Far from viewing language as wholly external, Chomsky re-conceptualized it as wholly internal. To explain how a child so rapidly and effortlessly acquires its natal language, he insisted, we must conclude that it comes into the world with the essentials of grammar already pre-installed. No other species, according to Chomsky, is genetically equipped with a language faculty — or indeed with anything remotely like one. The emergence of such a faculty in "Homo sapiens", from this standpoint, presents biological science with a major theoretical challenge.

One way to explain biological complexity is by reference to its inferred function. According to the influential philosopher John Austin, speech's primary function is active in the social world.

Speech acts, according to this body of theory, can be analyzed on three different levels: elocutionary, illocutionary and perlocutionary. An act is "locutionary" when viewed as the production of certain linguistic sounds — for example, practicing correct pronunciation in a foreign language. An act is "illocutionary" insofar as it constitutes an intervention in the world as jointly perceived or understood. Promising, marrying, divorcing, declaring, stating, authorizing, announcing and so forth are all speech acts in this "illocutionary" sense. An act is "perlocutionary" when viewed in terms of its direct psychological effect on an audience. Frightening a baby by saying 'Boo!' would be an example of a "perlocutionary" act.

For Austin, "doing things" with words means, first and foremost, deploying "illocutionary" force. The secret of this is community participation or collusion. There must be a 'correct' (conventionally agreed) procedure, and all those concerned must accept that it has been properly followed. In the case of a priest declaring a couple to be man and wife, his words will have illocutionary force only if he is properly authorized and only if the ceremony is properly conducted, using words deemed appropriate to the occasion. Austin points out that should anyone attempt to baptize a penguin, the act would be null and void. For reasons which have nothing to do with physics, chemistry or biology, baptism is inappropriate to be applied to penguins, irrespective of the verbal formulation used.

This body of theory may have implications for speculative scenarios concerning the origins of speech. "Doing things with words" presupposes shared understandings and agreements pertaining not just to language but to social conduct more generally. Apes might produce sequences of structured sound, influencing one another in that way. To deploy "illocutionary" force, however, they would need to have entered a non-physical and non-biological realm — one of shared contractual and other intangibles. This novel cognitive domain consists of what philosophers term "institutional facts" — objective facts whose existence, paradoxically, depends on communal faith or belief. Few primatologists, evolutionary psychologists or anthropologists consider that nonhuman primates are capable of the necessary levels of joint attention, sustained commitment or collaboration in pursuit of future goals.

Biosemiotics is a relatively new discipline, inspired in large part by the discovery of the genetic code in the early 1960s. Its basic assumption is that "Homo sapiens" is not alone in its reliance on codes and signs. Language and symbolic culture must have biological roots, hence semiotic principles must apply also in the animal world.

The discovery of the molecular structure of DNA apparently contradicted the idea that life could be explained, ultimately, in terms of the fundamental laws of physics. The letters of the genetic alphabet seemed to have "meaning", yet meaning is not a concept that has any place in physics. The natural science community initially solved this difficulty by invoking the concept of "information", treating information as independent of meaning. But a different solution to the puzzle was to recall that the laws of physics in themselves are never sufficient to explain natural phenomena. To explain, say, the unique physical and chemical characteristics of the planets in our solar system, scientists must work out how the laws of physics became constrained by particular sequences of events following the formation of the Sun.

According to Howard Pattee, the same principle applies to the evolution of life on earth, a process in which certain "frozen accidents" or "natural constraints" have from time to time drastically reduced the number of possible evolutionary outcomes. Codes, when they prove to be stable over evolutionary time, are constraints of this kind. The most fundamental such "frozen accident" was the emergence of DNA as a self-replicating molecule, but the history of life on earth has been characterized by a succession of comparably dramatic events, each of which can be conceptualized as the emergence of a new code. From this perspective, the evolutionary emergence of spoken language was one more event of essentially the same kind.

In 1975, the Israeli theoretical biologist Amotz Zahavi proposed a novel theory which, although controversial, has come to dominate Darwinian thinking on how signals evolve. Zahavi's "handicap principle" states that to be effective, signals must be reliable; to be reliable, the bodily investment in them must be so high as to make cheating unprofitable.

Paradoxically, if this logic is accepted, signals in nature evolve not to be efficient but, on the contrary, to be elaborate and wasteful of time and energy. A peacock's tail is the classic illustration. Zahavi's theory is that since peahens are on the look-out for male braggarts and cheats, they insist on a display of quality so costly that only a genuinely fit peacock could afford to pay. Needless to say, not all signals in the animal world are quite as elaborate as a peacock's tail. But if Zahavi is correct, all require some bodily investment — an expenditure of time and energy which "handicaps" the signaller in some way.

Animal vocalizations (according to Zahavi) are reliable because they are faithful reflections of the state of the signaller's body. To switch from an honest to a deceitful call, the animal would have to adopt a different bodily posture. Since every bodily action has its own optimal starting position, changing that position to produce a false message would interfere with the task of carrying out the action really intended. The gains made by cheating would not make up for the losses incurred by assuming an improper posture — and so the phony message turns out to be not worth its price. This may explain, in particular, why ape and monkey vocal signals have evolved to be so strikingly inflexible when compared with the varied speech sounds produced by the human tongue. The apparent inflexibility of chimpanzee vocalizations may strike the human observer as surprising until we realize that being inflexible is necessarily bound up with being perceptibly honest in the sense of "hard-to-fake".

If we accept this theory, the emergence of speech becomes theoretically impossible. Communication of this kind just cannot evolve. The problem is that words are cheap. Nothing about their acoustic features can reassure listeners that they are genuine and not fakes. Any strategy of reliance on someone else's tongue — perhaps the most flexible organ in the body — presupposes unprecedented levels of honesty and trust. To date, Darwinian thinkers have found it difficult to explain the requisite levels of community-wide cooperation and trust.

An influential standard textbook is "Animal Signals," by John Maynard Smith and David Harper. These authors divide the costs of communication into two components, (1) the investment necessary to ensure transmission of a discernible signal; (2) the investment necessary to guarantee that each signal is reliable and not a fake. The authors point out that although costs in the second category may be relatively low, they are not zero. Even in relatively relaxed, cooperative social contexts — for example, when communication is occurring between genetic kin — some investment must be made to guarantee reliability. In short, the notion of super-efficient communication — eliminating all costs except those necessary for successful transmission — is biologically unrealistic. Yet speech comes precisely into this category.
The graph shows the different signal intensities as a result of costs and benefits. If two individuals face different costs but have the same benefits, or have different benefits but the same cost, they will signal at different levels. The higher the signal represents a more reliable quality. The high-quality individual will maximize costs relative to benefits at a high signal intensities, while the low-quality individual maximizes their benefits relative to cost at low signal intensity. The high-quality individual is shown to take more risks (greater cost), which can be understood in terms of honest signals, which are expensive. The stronger you are, the more easily you can bare the cost of the signal, making you a more appealing mating partner. The low-quality individuals are less likely to be able to afford a specific signal, and will consequently be less likely to attract a female.

Cognitive linguistics views linguistic structure as arising continuously out of usage. Speakers are forever discovering new ways to convey meanings by producing sounds, and in some cases, these novel strategies become conventionalized. Between the phonological structure and semantic structure, there is no causal relationship. Instead, each novel pairing of sound and meaning involves an imaginative leap.

In their book, "Metaphors We Live By," George Lakoff and Mark Johnson helped pioneer this approach, claiming that "metaphor" is what makes human thought special. All language, they argued, is permeated with metaphor, whose use in fact "constitutes" distinctively human — that is, distinctively abstract — thought. To conceptualize things which cannot be directly perceived — intangibles such as time, life, reason, mind, society or justice — we have no choice but to set out from more concrete and directly perceptible phenomena such as motion, location, distance, size and so forth. In all cultures across the world, according to Lakoff and Johnson, people resort to such familiar metaphors as "ideas are locations, thinking is moving" and "mind is body". For example, we might express the idea of "arriving at a crucial point in our argument" by proceeding as if literally traveling from one physical location to the next.

Metaphors, by definition, are not literally true. Strictly speaking, they are fictions — from a pedantic standpoint, even falsehoods. But if we couldn't resort to metaphorical fictions, it's doubtful whether we could even form conceptual representations of such nebulous phenomena as "ideas", thoughts", "minds", and so forth.

The bearing of these ideas on current thinking on speech origins remains unclear. One suggestion is that ape communication tends to resist the metaphor for social reasons. Since they inhabit a Darwinian (as opposed to morally regulated) social world, these animals are under strong competitive pressure "not" to accept patent fictions as valid communicative currency. Ape vocal communication tends to be inflexible, marginalizing the ultra-flexible tongue, precisely because listeners treat with suspicion any signal which might prove to be a fake. Such insistence on perceptible veracity is clearly incompatible with metaphoric usage. An implication is that neither articulate speech nor distinctively human abstract thought could have begun evolving until our ancestors had become more cooperative and trusting of one another's communicative intentions.

When people converse with one another, according to the American philosopher John Searle, they're making moves, not in the real world which other species inhabit, but in a shared virtual realm peculiar to ourselves. Unlike the deployment of muscular effort to move a physical object, the deployment of illocutionary force requires no physical effort (except the movement of the tongue/mouth to produce speech) and produces no effect which any measuring device could detect. Instead, our action takes place on a quite different level — that of "social" reality. This kind of reality is in one sense hallucinatory, being a product of collective intentionality. It consists, not of "brute facts" — facts which exist anyway, irrespective of anyone's belief — but of "institutional facts", which "exist" only if you believe in them. Government, marriage, citizenship and money are examples of "institutional facts". One can distinguish between "brute" facts and "institutional" ones by applying a simple test. Suppose no one believed in the fact — would it still be true? If the answer is "yes", it's "brute". If the answer is "no", it's "institutional".

The facts of language in general and of speech, in particular, are, from this perspective, "institutional" rather than "brute". The semantic meaning of a word, for example, is whatever its users imagine it to be. To "do things with words" is to operate in a virtual world which seems real because we share it in common. In this incorporeal world, the laws of physics, chemistry, and biology do not apply. That explains why illocutionary force can be deployed without exerting muscular effort. Apes and monkeys inhabit the "brute" world. To make an impact, they must scream, bark, threaten, seduce or in other ways invest bodily effort. If they were invited to play chess, they would be unable to resist throwing their pieces at one another. Speech is not like that. A few movements of the tongue, under appropriate conditions, can be sufficient to open parliament, annul a marriage, confer a knighthood or declare war. To explain, on a Darwinian basis, how such apparent magic first began to work, we must ask how, when and why "Homo sapiens" succeeded in establishing the wider domain of institutional facts.

"Brute facts", in the terminology of speech act philosopher John Searle, are facts which are true anyway, regardless of human belief. Suppose you don't believe in gravity: jump over a cliff and you'll still fall. Natural science is the study of facts of this kind. "Institutional facts" are fictions accorded factual status within human social institutions. Monetary and commercial facts are fictions of this kind. The complexities of today's global currency system are facts only while we believe in them: suspend the belief and the facts correspondingly dissolve. Yet although institutional facts rest on human belief, that doesn't make them mere distortions or hallucinations. Take my confidence that these two five-pound banknotes in my pocket are worth ten pounds. That's not merely my subjective belief: it's an objective, indisputable fact. But now imagine a collapse of public confidence in the currency system. Suddenly, the realities in my pocket dissolve.

Scholars who doubt the scientific validity of the notion of "institutional facts" include Noam Chomsky, for whom language is not social. In Chomsky's view, language is a natural object (a component of the individual brain) and its study, therefore, a branch of natural science. In explaining the origin of language, scholars in this intellectual camp invoke non-social developments — in Chomsky's case, a random genetic mutation. Chomsky argues that language might exist inside the brain of a single mutant gorilla even if no one else believed in it, even if no one else existed apart from the mutant — and even if the gorilla in question remained unaware of its existence, never actually speaking. In the opposite philosophical camp are those who, in the tradition of Ferdinand de Saussure, argue that if no one believed in words or rules, they simply would not exist. These scholars, correspondingly, regard language as essentially institutional, concluding that linguistics should be considered a topic within social science. In explaining the evolutionary emergence of language, scholars in this intellectual camp tend to invoke profound changes in social relationships.

Criticism. Darwinian scientists today see little value in the traditional distinction between "natural" and "social" science. Darwinism in its modern form is the study of cooperation and competition in nature — a topic which is intrinsically social. Against this background, there is an increasing awareness among evolutionary linguists and Darwinian anthropologists that traditional inter-disciplinary barriers can have damaging consequences for investigations into the origins of speech.



</doc>
<doc id="15864296" url="https://en.wikipedia.org/wiki?curid=15864296" title="Fluency">
Fluency

Fluency (also called volubility and eloquency) is the property of a person or of a system that delivers information quickly and with expertise.

Language fluency is one of a variety of terms used to characterize or measure a person's language ability, often used in conjunction with accuracy and complexity. Although there are no widely agreed-upon definitions or measures of language fluency, someone is typically said to be fluent if their use of the language appears "fluid", or natural, coherent, and easy as opposed to slow, halting use. In other words, fluency is often described as the ability to produce language on demand and be understood.

Varying definitions of fluency characterize it by the language user’s automaticity, their speed and coherency of language use, or the length and rate of their speech output. Theories of automaticity postulate that more fluent language users can manage all of the components of language use without paying attention to each individual component of the act. In other words, fluency is achieved when one can access language knowledge and produce language unconsciously, or automatically. Theories that focus on speed or length and rate of speech typically expect fluent language users to produce language in real time without unusual pauses, false starts, or repetitions (recognizing that some presence of these elements are naturally part of speech). Fluency is sometimes considered to be a measure of performance rather than an indicator of more concrete language knowledge, and thus perception and understandability are often key ways that fluency is understood.

Language fluency is sometimes contrasted with accuracy (or correctness of language use, especially grammatical correctness) and complexity (or a more encompassing knowledge of vocabulary and discourse strategies). Fluency, accuracy, and complexity are distinct but interrelated components of language acquisition and proficiency.

There are four commonly discussed types of fluency: reading fluency, oral fluency, oral-reading fluency, and written or compositional fluency. These types of fluency are interrelated, but do not necessarily develop in tandem or linearly. One may develop fluency in certain type(s) and be less fluent or nonfluent in others.

In the sense of proficiency, "fluency" encompasses a number of related but separable skills:

Because an assessment of fluency is typically a measure or characterization of one's language ability, determining fluency may be a more challenging task when the speaker is acquiring a second language. It is generally thought that the later in life a learner approaches the study of a foreign language, the harder it is to acquire receptive (auditory) comprehension and fluent production (speaking) skills. For adults, once their mother tongue has already been established, the acquisition of a second language can come more slowly and less completely, ultimately affecting fluency. However, the critical period hypothesis is a hotly debated topic, with some scholars stating that adults can in fact become fluent in acquiring a second language. For instance, reading and writing skills in a foreign language can be acquired more easily even after the primary language acquisition period of youth is over.

So although it is often assumed that young children learn languages more easily than adolescents and adults, the reverse is in fact true; older learners are faster. The only exception to this rule is in pronunciation. Young children invariably learn to speak their second language with native-like pronunciation, whereas learners who start learning a language at an older age only rarely reach a native-like level.

Since childhood is a critical period, widespread opinion holds that it is easier for young children to learn a second language than it is for adults. Children can even acquire native fluency when exposed to the language on a consistent basis with rich interaction in a social setting. In addition to capacity, factors like; 1) motivation, 2) aptitude, 3) personality characteristics, 4) age of acquisition 5) first language typology 6) socio-economic status and 7) quality and context of L2 input play a role in L2 acquisitions rate and building fluency. Second language acquisition (SLA) has the ability to influence children’s cognitive growth and linguistic development.

Skill that consists of ability to produce words in target language develops until adolescence. Natural ability to acquire a new language with a deliberate effort may begin to diminish around puberty i.e. 12–14 years of age. Learning environment, comprehensible instructional materials, teacher, and the learner are indispensable elements in SLA and developing fluency in children. Extensive reading in L2 can offer twofold benefits in foreign language learning i.e. "reading to comprehend English and reading to learn English".

Paradis (2006) study on childhood language acquisition and building fluency examines how first and second language acquisition patterns are generally similar including vocabulary and morphosyntax. Phonology of first language is usually apparent in SLA and initial L1 influence can be lifelong, even for child L2 learners.

Children can acquire a second language simultaneously (learn L1 and L2 at the same time) or sequentially (learn L1 first and then L2). In the end, they develop fluency in both with one dominant language which is spoken largely by the community they live in.

According to one source, there are five stages of SLA and developing fluency:

The process of learning a second language or "L2," among older learners differs from younger learners because of their working memory. Working memory, also connected to fluency because it deals with automatic responses, is vital to language acquisition. This happens when information is stored and manipulated temporarily. During working memory, words are filtered, processed, and rehearsed, and information is stored while focusing on the next piece of interaction. These false starts, pauses or repetitions found in fluency assessments, can also be found within one's working memory as part of communication.

Those with education at or below a high school level are least likely to take language classes. It has also been found that women and young immigrants are more likely to take language classes. Further, highly educated immigrants who are searching for skilled jobs – which require interpersonal and intercultural skills that are difficult to learn – are the most affected by lower fluency in the L2.

Fluency is a speech language pathology term that means the smoothness or flow with which sounds, syllables, words and phrases are joined together when speaking quickly. "Fluency disorders" is used as a collective term for cluttering and stuttering. Both disorders have breaks in the fluidity of speech, and both have the fluency breakdown of repetition of parts of speech.

Studies in the assessment of creativity list fluency as one of the four primary elements in creative thinking, the others being flexibility, originality and elaboration. Fluency in creative thinking is seen as the ability to think of many diverse ideas quickly.




</doc>
<doc id="17524" url="https://en.wikipedia.org/wiki?curid=17524" title="Language">
Language

A language is a structured system of communication. Language, in a broader sense, is the method of communication that involves the use of – particularly human – languages. 

The scientific study of language is called linguistics. Questions concerning the philosophy of language, such as whether words can represent experience, have been debated at least since Gorgias and Plato in ancient Greece. Thinkers such as Rousseau have argued that language originated from emotions while others like Kant have held that it originated from rational and logical thought. 20th-century philosophers such as Wittgenstein argued that philosophy is really the study of language. Major figures in linguistics include Ferdinand de Saussure and Noam Chomsky.

Estimates of the number of human languages in the world vary between 5,000 and 7,000. However, any precise estimate depends on the arbitrary distinction (dichotomy) between languages and dialect. Natural languages are spoken or signed, but any language can be encoded into secondary media using auditory, visual, or tactile stimuli – for example, in writing, whistling, signing, or braille. This is because human language is modality-independent. Depending on philosophical perspectives regarding the definition of language and meaning, when used as a general concept, "language" may refer to the cognitive ability to learn and use systems of complex communication, or to describe the set of rules that makes up these systems, or the set of utterances that can be produced from those rules. All languages rely on the process of semiosis to relate signs to particular meanings. Oral, manual and tactile languages contain a phonological system that governs how symbols are used to form sequences known as words or morphemes, and a syntactic system that governs how words and morphemes are combined to form phrases and utterances.

Human language has the properties of productivity and displacement, and relies entirely on social convention and learning. Its complex structure affords a much wider range of expressions than any known system of animal communication. Language is thought to have originated when early hominins started gradually changing their primate communication systems, acquiring the ability to form a theory of other minds and a shared intentionality. This development is sometimes thought to have coincided with an increase in brain volume, and many linguists see the structures of language as having evolved to serve specific communicative and social functions. Language is processed in many different locations in the human brain, but especially in Broca's and Wernicke's areas. Humans acquire language through social interaction in early childhood, and children generally speak fluently by approximately three years old. The use of language is deeply entrenched in human culture. Therefore, in addition to its strictly communicative uses, language also has many social and cultural uses, such as signifying group identity, social stratification, as well as social grooming and entertainment.

Languages evolve and diversify over time, and the history of their evolution can be reconstructed by comparing modern languages to determine which traits their ancestral languages must have had in order for the later developmental stages to occur. A group of languages that descend from a common ancestor is known as a language family. The Indo-European family is the most widely spoken and includes languages as diverse as English, Russian and Hindi; the Sino-Tibetan family includes Mandarin and the other Chinese languages, Bodo and Tibetan; the Afro-Asiatic family includes Arabic, Somali, and Hebrew; the Bantu languages include Swahili, and Zulu, and hundreds of other languages spoken throughout Africa; and the Malayo-Polynesian languages include Indonesian, Malay, Tagalog, and hundreds of other languages spoken throughout the Pacific. The languages of the Dravidian family, spoken mostly in Southern India, include Tamil, Telugu and Kannada. Academic consensus holds that between 50% and 90% of languages spoken at the beginning of the 21st century will probably have become extinct by the year 2100.

The English word "language" derives ultimately from Proto-Indo-European "tongue, speech, language" through Latin "lingua", "language; tongue", and Old French "language". The word is sometimes used to refer to codes, ciphers, and other kinds of artificially constructed communication systems such as formally defined computer languages used for computer programming. Unlike conventional human languages, a formal language in this sense is a system of signs for encoding and decoding information. This article specifically concerns the properties of natural human language as it is studied in the discipline of linguistics.

As an object of linguistic study, "language" has two primary meanings: an abstract concept, and a specific linguistic system, e.g. "French". The Swiss linguist Ferdinand de Saussure, who defined the modern discipline of linguistics, first explicitly formulated the distinction using the French word "langage" for language as a concept, "langue" as a specific instance of a language system, and "parole" for the concrete usage of speech in a particular language.

When speaking of language as a general concept, definitions can be used which stress different aspects of the phenomenon. These definitions also entail different approaches and understandings of language, and they also inform different and often incompatible schools of linguistic theory. Debates about the nature and origin of language go back to the ancient world. Greek philosophers such as Gorgias and Plato debated the relation between words, concepts and reality. Gorgias argued that language could represent neither the objective experience nor human experience, and that communication and truth were therefore impossible. Plato maintained that communication is possible because language represents ideas and concepts that exist independently of, and prior to, language.

During the Enlightenment and its debates about human origins, it became fashionable to speculate about the origin of language. Thinkers such as Rousseau and Herder argued that language had originated in the instinctive expression of emotions, and that it was originally closer to music and poetry than to the logical expression of rational thought. Rationalist philosophers such as Kant and Descartes held the opposite view. Around the turn of the 20th century, thinkers began to wonder about the role of language in shaping our experiences of the world – asking whether language simply reflects the objective structure of the world, or whether it creates concepts that it in turn impose on our experience of the objective world. This led to the question of whether philosophical problems are really firstly linguistic problems. The resurgence of the view that language plays a significant role in the creation and circulation of concepts, and that the study of philosophy is essentially the study of language, is associated with what has been called the linguistic turn and philosophers such as Wittgenstein in 20th-century philosophy. These debates about language in relation to meaning and reference, cognition and consciousness remain active today.

One definition sees language primarily as the mental faculty that allows humans to undertake linguistic behaviour: to learn languages and to produce and understand utterances. This definition stresses the universality of language to all humans, and it emphasizes the biological basis for the human capacity for language as a unique development of the human brain. Proponents of the view that the drive to language acquisition is innate in humans argue that this is supported by the fact that all cognitively normal children raised in an environment where language is accessible will acquire language without formal instruction. Languages may even develop spontaneously in environments where people live or grow up together without a common language; for example, creole languages and spontaneously developed sign languages such as Nicaraguan Sign Language. This view, which can be traced back to the philosophers Kant and Descartes, understands language to be largely innate, for example, in Chomsky's theory of Universal Grammar, or American philosopher Jerry Fodor's extreme innatist theory. These kinds of definitions are often applied in studies of language within a cognitive science framework and in neurolinguistics.

Another definition sees language as a formal system of signs governed by grammatical rules of combination to communicate meaning. This definition stresses that human languages can be described as closed structural systems consisting of rules that relate particular signs to particular meanings. This structuralist view of language was first introduced by Ferdinand de Saussure, and his structuralism remains foundational for many approaches to language.

Some proponents of Saussure's view of language have advocated a formal approach which studies language structure by identifying its basic elements and then by presenting a formal account of the rules according to which the elements combine in order to form words and sentences. The main proponent of such a theory is Noam Chomsky, the originator of the generative theory of grammar, who has defined language as the construction of sentences that can be generated using transformational grammars. Chomsky considers these rules to be an innate feature of the human mind and to constitute the rudiments of what language is. By way of contrast, such transformational grammars are also commonly used to provide formal definitions of language are commonly used in formal logic, in formal theories of grammar, and in applied computational linguistics. In the philosophy of language, the view of linguistic meaning as residing in the logical relations between propositions and reality was developed by philosophers such as Alfred Tarski, Bertrand Russell, and other formal logicians.

Yet another definition sees language as a system of communication that enables humans to exchange verbal or symbolic utterances. This definition stresses the social functions of language and the fact that humans use it to express themselves and to manipulate objects in their environment. Functional theories of grammar explain grammatical structures by their communicative functions, and understand the grammatical structures of language to be the result of an adaptive process by which grammar was "tailored" to serve the communicative needs of its users.

This view of language is associated with the study of language in pragmatic, cognitive, and interactive frameworks, as well as in sociolinguistics and linguistic anthropology. Functionalist theories tend to study grammar as dynamic phenomena, as structures that are always in the process of changing as they are employed by their speakers. This view places importance on the study of linguistic typology, or the classification of languages according to structural features, as it can be shown that processes of grammaticalization tend to follow trajectories that are partly dependent on typology. In the philosophy of language, the view of pragmatics as being central to language and meaning is often associated with Wittgenstein's later works and with ordinary language philosophers such as J.L. Austin, Paul Grice, John Searle, and W.O. Quine.

A number of features, many of which were described by Charles Hockett and called design features set human language apart from other known systems of communication, such as those used by non-human animals.

Communication systems used by other animals such as bees or apes are closed systems that consist of a finite, usually very limited, number of possible ideas that can be expressed. In contrast, human language is open-ended and productive, meaning that it allows humans to produce a vast range of utterances from a finite set of elements, and to create new words and sentences. This is possible because human language is based on a dual code, in which a finite number of elements which are meaningless in themselves (e.g. sounds, letters or gestures) can be combined to form an infinite number of larger units of meaning (words and sentences). However, one study has demonstrated that an Australian bird, the chestnut-crowned babbler, is capable of using the same acoustic elements in different arrangements to create two functionally distinct vocalizations. Additionally, pied babblers have demonstrated the ability to generate two functionally distinct vocalisations composed of the same sound type, which can only be distinguished by the number of repeated elements.

Several species of animals have proved to be able to acquire forms of communication through social learning: for instance a bonobo named Kanzi learned to express itself using a set of symbolic lexigrams. Similarly, many species of birds and whales learn their songs by imitating other members of their species. However, while some animals may acquire large numbers of words and symbols, none have been able to learn as many different signs as are generally known by an average 4 year old human, nor have any acquired anything resembling the complex grammar of human language.

Human languages also differ from animal communication systems in that they employ grammatical and semantic categories, such as noun and verb, present and past, which may be used to express exceedingly complex meanings. Human language is also unique in having the property of recursivity: for example, a noun phrase can contain another noun phrase (as in "<nowiki>the chimpanzee]'s lips]</nowiki>") or a clause can contain another clause (as in "<nowiki>[I see [the dog is running</nowiki>"). Human language is also the only known natural communication system whose adaptability may be referred to as "modality independent". This means that it can be used not only for communication through one channel or medium, but through several. For example, spoken language uses the auditive modality, whereas sign languages and writing use the visual modality, and braille writing uses the tactile modality.

Human language is also unique in being able to refer to abstract concepts and to imagined or hypothetical events as well as events that took place in the past or may happen in the future. This ability to refer to events that are not at the same time or place as the speech event is called "displacement", and while some animal communication systems can use displacement (such as the communication of bees that can communicate the location of sources of nectar that are out of sight), the degree to which it is used in human language is also considered unique.

Theories about the origin of language differ in regard to their basic assumptions about what language is. Some theories are based on the idea that language is so complex that one cannot imagine it simply appearing from nothing in its final form, but that it must have evolved from earlier pre-linguistic systems among our pre-human ancestors. These theories can be called continuity-based theories. The opposite viewpoint is that language is such a unique human trait that it cannot be compared to anything found among non-humans and that it must therefore have appeared suddenly in the transition from pre-hominids to early man. These theories can be defined as discontinuity-based. Similarly, theories based on the generative view of language pioneered by Noam Chomsky see language mostly as an innate faculty that is largely genetically encoded, whereas functionalist theories see it as a system that is largely cultural, learned through social interaction.

Chomsky is one prominent proponent of a discontinuity-based theory of human language origins. He suggests that for scholars interested in the nature of language, "talk about the evolution of the language capacity is beside the point." Chomsky proposes that perhaps "some random mutation took place [...] and it reorganized the brain, implanting a language organ in an otherwise primate brain." Though cautioning against taking this story literally, Chomsky insists that "it may be closer to reality than many other fairy tales that are told about evolutionary processes, including language."

Continuity-based theories are held by a majority of scholars, but they vary in how they envision this development. Those who see language as being mostly innate, for example psychologist Steven Pinker, hold the precedents to be animal cognition, whereas those who see language as a socially learned tool of communication, such as psychologist Michael Tomasello, see it as having developed from animal communication in primates: either gestural or vocal communication to assist in cooperation. Other continuity-based models see language as having developed from music, a view already espoused by Rousseau, Herder, Humboldt, and Charles Darwin. A prominent proponent of this view is archaeologist Steven Mithen. Stephen Anderson states that the age of spoken languages is estimated at 60,000 to 100,000 years and that: Researchers on the evolutionary origin of language generally find it plausible to suggest that language was invented only once, and that all modern spoken languages are thus in some way related, even if that relation can no longer be recovered ... because of limitations on the methods available for reconstruction.

Because language emerged in the early prehistory of man, before the existence of any written records, its early development has left no historical traces, and it is believed that no comparable processes can be observed today. Theories that stress continuity often look at animals to see if, for example, primates display any traits that can be seen as analogous to what pre-human language must have been like. And early human fossils can be inspected for traces of physical adaptation to language use or pre-linguistic forms of symbolic behaviour. Among the signs in human fossils that may suggest linguistic abilities are: the size of the brain relative to body mass, the presence of a larynx capable of advanced sound production and the nature of tools and other manufactured artifacts.

It was mostly undisputed that pre-human australopithecines did not have communication systems significantly different from those found in great apes in general. However, a 2017 study on Ardipithecus ramidus challenges this belief. Scholarly opinions vary as to the developments since the appearance of the genus "Homo" some 2.5 million years ago. Some scholars assume the development of primitive language-like systems (proto-language) as early as "Homo habilis" (2.3 million years ago) while others place the development of primitive symbolic communication only with "Homo erectus" (1.8 million years ago) or "Homo heidelbergensis" (0.6 million years ago), and the development of language proper with Anatomically Modern "Homo sapiens" with the Upper Paleolithic revolution less than 100,000 years ago.

The study of language, linguistics, has been developing into a science since the first grammatical descriptions of particular languages in India more than 2000 years ago, after the development of the Brahmi script. Modern linguistics is a science that concerns itself with all aspects of language, examining it from all of the theoretical viewpoints described above.

The academic study of language is conducted within many different disciplinary areas and from different theoretical angles, all of which inform modern approaches to linguistics. For example, descriptive linguistics examines the grammar of single languages, theoretical linguistics develops theories on how best to conceptualize and define the nature of language based on data from the various extant human languages, sociolinguistics studies how languages are used for social purposes informing in turn the study of the social functions of language and grammatical description, neurolinguistics studies how language is processed in the human brain and allows the experimental testing of theories, computational linguistics builds on theoretical and descriptive linguistics to construct computational models of language often aimed at processing natural language or at testing linguistic hypotheses, and historical linguistics relies on grammatical and lexical descriptions of languages to trace their individual histories and reconstruct trees of language families by using the comparative method.

The formal study of language is often considered to have started in India with Pāṇini, the 5th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. However, Sumerian scribes already studied the differences between Sumerian and Akkadian grammar around 1900 BC. Subsequent grammatical traditions developed in all of the ancient cultures that adopted writing.

In the 17th century AD, the French Port-Royal Grammarians developed the idea that the grammars of all languages were a reflection of the universal basics of thought, and therefore that grammar was universal. In the 18th century, the first use of the comparative method by British philologist and expert on ancient India William Jones sparked the rise of comparative linguistics. The scientific study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt. Early in the 20th century, Ferdinand de Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them.

By introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still fundamental in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the Langue-parole distinction, distinguishing language as an abstract system ("langue"), from language as a concrete manifestation of this system ("parole").

In the 1960s, Noam Chomsky formulated the generative theory of language. According to this theory, the most basic form of language is a set of syntactic rules that is universal for all humans and which underlies the grammars of all human languages. This set of rules is called Universal Grammar; for Chomsky, describing it is the primary objective of the discipline of linguistics. Thus, he considered that the grammars of individual languages are only of importance to linguistics insofar as they allow us to deduce the universal underlying rules from which the observable linguistic variability is generated.

In opposition to the formal theories of the generative school, functional theories of language propose that since language is fundamentally a tool, its structures are best analyzed and understood by reference to their functions. Formal theories of grammar seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, while functional theories seek to define the functions performed by language and then relate them to the linguistic elements that carry them out. The framework of cognitive linguistics interprets language in terms of the concepts (which are sometimes universal, and sometimes specific to a particular language) which underlie its forms. Cognitive linguistics is primarily concerned with how the mind creates meaning through language.

Speaking is the default modality for language in all cultures. The production of spoken language depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus, the ability to acoustically decode speech sounds, and the neurological apparatus required for acquiring and producing language. The study of the genetic bases for human language is at an early stage: the only gene that has definitely been implicated in language production is FOXP2, which may cause a kind of congenital language disorder if affected by mutations.

 The brain is the coordinating center of all linguistic activity; it controls both the production of linguistic cognition and of meaning and the mechanics of speech production. Nonetheless, our knowledge of the neurological bases for language is quite limited, though it has advanced considerably with the use of modern imaging techniques. The discipline of linguistics dedicated to studying the neurological aspects of language is called neurolinguistics.

Early work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with fluency, articulation, word-finding, word repetition, and producing and comprehending complex grammatical sentences, both orally and in writing. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.

With technological advances in the late 20th century, neurolinguists have also incorporated non-invasive techniques such as functional magnetic resonance imaging (fMRI) and electrophysiology to study language processing in individuals without impairments.

Spoken language relies on human physical ability to produce sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract – the throat, the mouth, and the nose. By controlling the different parts of the speech apparatus, the airstream can be manipulated to produce different speech sounds.

The sound of speech can be analyzed into a combination of segmental and suprasegmental elements. The segmental elements are those that follow each other in sequences, which are usually represented by distinct letters in alphabetic scripts, such as the Roman script. In free flowing speech, there are no clear boundaries between one segment and the next, nor usually are there any audible pauses between words. Segments therefore are distinguished by their distinct sounds which are a result of their different articulations, and they can be either vowels or consonants. Suprasegmental phenomena encompass such elements as stress, phonation type, voice timbre, and prosody or intonation, all of which may have effects across multiple segments.

Consonants and vowel segments combine to form syllables, which in turn combine to form utterances; these can be distinguished phonetically as the space between two inhalations. Acoustically, these different segments are characterized by different formant structures, that are visible in a spectrogram of the recorded sound wave (See illustration of Spectrogram of the formant structures of three English vowels). Formants are the amplitude peaks in the frequency spectrum of a specific sound.

Vowels are those sounds that have no audible friction caused by the narrowing or obstruction of some part of the upper vocal tract. They vary in quality according to the degree of lip aperture and the placement of the tongue within the oral cavity. Vowels are called "close" when the lips are relatively closed, as in the pronunciation of the vowel (English "ee"), or "open" when the lips are relatively open, as in the vowel (English "ah"). If the tongue is located towards the back of the mouth, the quality changes, creating vowels such as (English "oo"). The quality also changes depending on whether the lips are rounded as opposed to unrounded, creating distinctions such as that between (unrounded front vowel such as English "ee") and (rounded front vowel such as German "ü").

Consonants are those sounds that have audible friction or closure at some point within the upper vocal tract. Consonant sounds vary by place of articulation, i.e. the place in the vocal tract where the airflow is obstructed, commonly at the lips, teeth, alveolar ridge, palate, velum, uvula, or glottis. Each place of articulation produces a different set of consonant sounds, which are further distinguished by manner of articulation, or the kind of friction, whether full closure, in which case the consonant is called "occlusive" or "stop", or different degrees of aperture creating "fricatives" and "approximants". Consonants can also be either "voiced or unvoiced", depending on whether the vocal cords are set in vibration by airflow during the production of the sound. Voicing is what separates English in "bus" (unvoiced sibilant) from in "buzz" (voiced sibilant).

Some speech sounds, both vowels and consonants, involve release of air flow through the nasal cavity, and these are called "nasals" or "nasalized" sounds. Other sounds are defined by the way the tongue moves within the mouth: such as the l-sounds (called "laterals", because the air flows along both sides of the tongue), and the r-sounds (called "rhotics") that are characterized by how the tongue is positioned relative to the air stream.

By using these speech organs, humans can produce hundreds of distinct sounds: some appear very often in the world's languages, whereas others are much more common in certain language families, language areas, or even specific to a single language.

When described as a system of symbolic communication, language is traditionally seen as consisting of three parts: signs, meanings, and a code connecting signs with their meanings. The study of the process of semiosis, how signs and meanings are combined, used, and interpreted is called semiotics. Signs can be composed of sounds, gestures, letters, or symbols, depending on whether the language is spoken, signed, or written, and they can be combined into complex signs, such as words and phrases. When used in communication, a sign is encoded and transmitted by a sender through a channel to a receiver who decodes it.
Some of the properties that define human language as opposed to other communication systems are: the arbitrariness of the linguistic sign, meaning that there is no predictable connection between a linguistic sign and its meaning; the duality of the linguistic system, meaning that linguistic structures are built by combining elements into larger structures that can be seen as layered, e.g. how sounds build words and words build phrases; the discreteness of the elements of language, meaning that the elements out of which linguistic signs are constructed are discrete units, e.g. sounds and words, that can be distinguished from each other and rearranged in different patterns; and the productivity of the linguistic system, meaning that the finite number of linguistic elements can be combined into a theoretically infinite number of combinations.

The rules by which signs can be combined to form words and phrases are called syntax or grammar. The meaning that is connected to individual signs, morphemes, words, phrases, and texts is called semantics. The division of language into separate but connected systems of sign and meaning goes back to the first linguistic studies of de Saussure and is now used in almost all branches of linguistics.

Languages express meaning by relating a sign form to a meaning, or its content. Sign forms must be something that can be perceived, for example, in sounds, images, or gestures, and then related to a specific meaning by social convention. Because the basic relation of meaning for most linguistic signs is based on social convention, linguistic signs can be considered arbitrary, in the sense that the convention is established socially and historically, rather than by means of a natural relation between a specific sign form and its meaning.

Thus, languages must have a vocabulary of signs related to specific meaning. The English sign "dog" denotes, for example, a member of the species "Canis familiaris". In a language, the array of arbitrary signs connected to specific meanings is called the lexicon, and a single sign connected to a meaning is called a lexeme. Not all meanings in a language are represented by single words. Often, semantic concepts are embedded in the morphology or syntax of the language in the form of grammatical categories.

All languages contain the semantic structure of predication: a structure that predicates a property, state, or action. Traditionally, semantics has been understood to be the study of how speakers and interpreters assign truth values to statements, so that meaning is understood to be the process by which a predicate can be said to be true or false about an entity, e.g. "<nowiki>[x [is y]]" or "[x [does y]]</nowiki>". Recently, this model of semantics has been complemented with more dynamic models of meaning that incorporate shared knowledge about the context in which a sign is interpreted into the production of meaning. Such models of meaning are explored in the field of pragmatics.

Depending on modality, language structure can be based on systems of sounds (speech), gestures (sign languages), or graphic or tactile symbols (writing). The ways in which languages use sounds or signs to construct meaning are studied in phonology. The study of how humans produce and perceive vocal sounds is called phonetics. In spoken language, meaning is produced when sounds become part of a system in which some sounds can contribute to expressing meaning and others do not. In any given language, only a limited number of the many distinct sounds that can be created by the human vocal apparatus contribute to constructing meaning.

Sounds as part of a linguistic system are called phonemes. Phonemes are abstract units of sound, defined as the smallest units in a language that can serve to distinguish between the meaning of a pair of minimally different words, a so-called minimal pair. In English, for example, the words "bat" and "pat" form a minimal pair, in which the distinction between and differentiates the two words, which have different meanings. However, each language contrasts sounds in different ways. For example, in a language that does not distinguish between voiced and unvoiced consonants, the sounds and (if they both occur) could be considered a single phoneme, and consequently, the two pronunciations would have the same meaning. Similarly, the English language does not distinguish phonemically between aspirated and non-aspirated pronunciations of consonants, as many other languages like Korean and Hindi do: the unaspirated in "spin" and the aspirated in "pin" are considered to be merely different ways of pronouncing the same phoneme (such variants of a single phoneme are called allophones), whereas in Mandarin Chinese, the same difference in pronunciation distinguishes between the words 'crouch' and 'eight' (the accent above the á means that the vowel is pronounced with a high tone).

All spoken languages have phonemes of at least two different categories, vowels and consonants, that can be combined to form syllables. As well as segments such as consonants and vowels, some languages also use sound in other ways to convey meaning. Many languages, for example, use stress, pitch, duration, and tone to distinguish meaning. Because these phenomena operate outside of the level of single segments, they are called suprasegmental. Some languages have only a few phonemes, for example, Rotokas and Pirahã language with 11 and 10 phonemes respectively, whereas languages like Taa may have as many as 141 phonemes. In sign languages, the equivalent to phonemes (formerly called cheremes) are defined by the basic elements of gestures, such as hand shape, orientation, location, and motion, which correspond to manners of articulation in spoken language.

Writing systems represent language using visual symbols, which may or may not correspond to the sounds of spoken language. The Latin alphabet (and those on which it is based or that have been derived from it) was originally based on the representation of single sounds, so that words were constructed from letters that generally denote a single consonant or vowel in the structure of the word. In syllabic scripts, such as the Inuktitut syllabary, each sign represents a whole syllable. In logographic scripts, each sign represents an entire word, and will generally bear no relation to the sound of that word in spoken language.

Because all languages have a very large number of words, no purely logographic scripts are known to exist. Written language represents the way spoken sounds and words follow one after another by arranging symbols according to a pattern that follows a certain direction. The direction used in a writing system is entirely arbitrary and established by convention. Some writing systems use the horizontal axis (left to right as the Latin script or right to left as the Arabic script), while others such as traditional Chinese writing use the vertical dimension (from top to bottom). A few writing systems use opposite directions for alternating lines, and others, such as the ancient Maya script, can be written in either direction and rely on graphic cues to show the reader the direction of reading.

In order to represent the sounds of the world's languages in writing, linguists have developed the International Phonetic Alphabet, designed to represent all of the discrete sounds that are known to contribute to meaning in human languages.

Grammar is the study of how meaningful elements called "morphemes" within a language can be combined into utterances. Morphemes can either be "free" or "bound". If they are free to be moved around within an utterance, they are usually called "words", and if they are bound to other words or morphemes, they are called affixes. The way in which meaningful elements can be combined within a language is governed by rules. The rules for the internal structure of words are called morphology. The rules of the internal structure of phrases and sentences are called "syntax".

Grammar can be described as a system of categories and a set of rules that determine how categories combine to form different aspects of meaning. Languages differ widely in whether they are encoded through the use of categories or lexical units. However, several categories are so common as to be nearly universal. Such universal categories include the encoding of the grammatical relations of participants and predicates by grammatically distinguishing between their relations to a predicate, the encoding of temporal and spatial relations on predicates, and a system of grammatical person governing reference to and distinction between speakers and addressees and those about whom they are speaking.

Languages organize their parts of speech into classes according to their functions and positions relative to other parts. All languages, for instance, make a basic distinction between a group of words that prototypically denotes things and concepts and a group of words that prototypically denotes actions and events. The first group, which includes English words such as "dog" and "song", are usually called nouns. The second, which includes "run" and "sing", are called verbs. Another common category is the adjective: words that describe properties or qualities of nouns, such as "red" or "big". Word classes can be "open" if new words can continuously be added to the class, or relatively "closed" if there is a fixed number of words in a class. In English, the class of pronouns is closed, whereas the class of adjectives is open, since an infinite number of adjectives can be constructed from verbs (e.g. "saddened") or nouns (e.g. with the -like suffix, as in "noun-like"). In other languages such as Korean, the situation is the opposite, and new pronouns can be constructed, whereas the number of adjectives is fixed.

Word classes also carry out differing functions in grammar. Prototypically, verbs are used to construct predicates, while nouns are used as arguments of predicates. In a sentence such as "Sally runs", the predicate is "runs", because it is the word that predicates a specific state about its argument "Sally". Some verbs such as "curse" can take two arguments, e.g. "Sally cursed John". A predicate that can only take a single argument is called "intransitive", while a predicate that can take two arguments is called "transitive".

Many other word classes exist in different languages, such as conjunctions like "and" that serve to join two sentences, articles that introduce a noun, interjections such as "wow!", or ideophones like "splash" that mimic the sound of some event. Some languages have positionals that describe the spatial position of an event or entity. Many languages have classifiers that identify countable nouns as belonging to a particular type or having a particular shape. For instance, in Japanese, the general noun classifier for humans is "nin" (人), and it is used for counting humans, whatever they are called:

For trees, it would be:

In linguistics, the study of the internal structure of complex words and the processes by which words are formed is called morphology. In most languages, it is possible to construct complex words that are built of several morphemes. For instance, the English word "unexpected" can be analyzed as being composed of the three morphemes "un-", "expect" and "-ed".

Morphemes can be classified according to whether they are independent morphemes, so-called roots, or whether they can only co-occur attached to other morphemes. These bound morphemes or affixes can be classified according to their position in relation to the root: "prefixes" precede the root, suffixes follow the root, and infixes are inserted in the middle of a root. Affixes serve to modify or elaborate the meaning of the root. Some languages change the meaning of words by changing the phonological structure of a word, for example, the English word "run", which in the past tense is "ran". This process is called "ablaut". Furthermore, morphology distinguishes between the process of inflection, which modifies or elaborates on a word, and the process of derivation, which creates a new word from an existing one. In English, the verb "sing" has the inflectional forms "singing" and "sung", which are both verbs, and the derivational form "singer", which is a noun derived from the verb with the agentive suffix "-er".

Languages differ widely in how much they rely on morphological processes of word formation. In some languages, for example, Chinese, there are no morphological processes, and all grammatical information is encoded syntactically by forming strings of single words. This type of morpho-syntax is often called isolating, or analytic, because there is almost a full correspondence between a single word and a single aspect of meaning. Most languages have words consisting of several morphemes, but they vary in the degree to which morphemes are discrete units. In many languages, notably in most Indo-European languages, single morphemes may have several distinct meanings that cannot be analyzed into smaller segments. For example, in Latin, the word "bonus", or "good", consists of the root "bon-", meaning "good", and the suffix -"us", which indicates masculine gender, singular number, and nominative case. These languages are called "fusional languages", because several meanings may be fused into a single morpheme. The opposite of fusional languages are agglutinative languages which construct words by stringing morphemes together in chains, but with each morpheme as a discrete semantic unit. An example of such a language is Turkish, where for example, the word "evlerinizden", or "from your houses", consists of the morphemes, "ev-ler-iniz-den" with the meanings "house-plural-your-from". The languages that rely on morphology to the greatest extent are traditionally called polysynthetic languages. They may express the equivalent of an entire English sentence in a single word. For example, in Persian the single word "nafahmidamesh" means "I didn't understand it" consisting of morphemes "na-fahm-id-am-esh" with the meanings, "negation.understand.past.I.it". As another example with more complexity, in the Yupik word "tuntussuqatarniksatengqiggtuq", which means "He had not yet said again that he was going to hunt reindeer", the word consists of the morphemes "tuntu-ssur-qatar-ni-ksaite-ngqiggte-uq" with the meanings, "reindeer-hunt-future-say-negation-again-third.person.singular.indicative", and except for the morpheme "tuntu" ("reindeer") none of the other morphemes can appear in isolation.

Many languages use morphology to cross-reference words within a sentence. This is sometimes called "agreement". For example, in many Indo-European languages, adjectives must cross-reference the noun they modify in terms of number, case, and gender, so that the Latin adjective "bonus", or "good", is inflected to agree with a noun that is masculine gender, singular number, and nominative case. In many polysynthetic languages, verbs cross-reference their subjects and objects. In these types of languages, a single verb may include information that would require an entire sentence in English. For example, in the Basque phrase "ikusi nauzu", or "you saw me", the past tense auxiliary verb "n-au-zu" (similar to English "do") agrees with both the subject (you) expressed by the "n"- prefix, and with the object (me) expressed by the – "zu" suffix. The sentence could be directly transliterated as "see you-did-me"

Another way in which languages convey meaning is through the order of words within a sentence. The grammatical rules for how to produce new sentences from words that are already known is called syntax. The syntactical rules of a language determine why a sentence in English such as "I love you" is meaningful, but "*love you I" is not. Syntactical rules determine how word order and sentence structure is constrained, and how those constraints contribute to meaning. For example, in English, the two sentences "the slaves were cursing the master" and "the master was cursing the slaves" mean different things, because the role of the grammatical subject is encoded by the noun being in front of the verb, and the role of object is encoded by the noun appearing after the verb. Conversely, in Latin, both "Dominus servos vituperabat" and "Servos vituperabat dominus" mean "the master was reprimanding the slaves", because "servos", or "slaves", is in the accusative case, showing that they are the grammatical object of the sentence, and "dominus", or "master", is in the nominative case, showing that he is the subject.

Latin uses morphology to express the distinction between subject and object, whereas English uses word order. Another example of how syntactic rules contribute to meaning is the rule of inverse word order in questions, which exists in many languages. This rule explains why when in English, the phrase "John is talking to Lucy" is turned into a question, it becomes "Who is John talking to?", and not "John is talking to who?". The latter example may be used as a way of placing special emphasis on "who", thereby slightly altering the meaning of the question. Syntax also includes the rules for how complex sentences are structured by grouping words together in units, called phrases, that can occupy different places in a larger syntactic structure. Sentences can be described as consisting of phrases connected in a tree structure, connecting the phrases to each other at different levels. To the right is a graphic representation of the syntactic analysis of the English sentence "the cat sat on the mat". The sentence is analyzed as being constituted by a noun phrase, a verb, and a prepositional phrase; the prepositional phrase is further divided into a preposition and a noun phrase, and the noun phrases consist of an article and a noun.

The reason sentences can be seen as being composed of phrases is because each phrase would be moved around as a single element if syntactic operations were carried out. For example, "the cat" is one phrase, and "on the mat" is another, because they would be treated as single units if a decision was made to emphasize the location by moving forward the prepositional phrase: "[And] on the mat, the cat sat". There are many different formalist and functionalist frameworks that propose theories for describing syntactic structures, based on different assumptions about what language is and how it should be described. Each of them would analyze a sentence such as this in a different manner.

Languages can be classified in relation to their grammatical types. Languages that belong to different families nonetheless often have features in common, and these shared features tend to correlate. For example, languages can be classified on the basis of their basic word order, the relative order of the verb, and its constituents in a normal indicative sentence. In English, the basic order is SVO (subject–verb–object): "The snake(S) bit(V) the man(O)", whereas for example, the corresponding sentence in the Australian language Gamilaraay would be "d̪uyugu n̪ama d̪ayn yiːy" (snake man bit), SOV. Word order type is relevant as a typological parameter, because basic word order type corresponds with other syntactic parameters, such as the relative order of nouns and adjectives, or of the use of prepositions or postpositions. Such correlations are called implicational universals. For example, most (but not all) languages that are of the SOV type have postpositions rather than prepositions, and have adjectives before nouns.

All languages structure sentences into Subject, Verb, and Object, but languages differ in the way they classify the relations between actors and actions. English uses the nominative-accusative word typology: in English transitive clauses, the subjects of both intransitive sentences ("I run") and transitive sentences ("I love you") are treated in the same way, shown here by the nominative pronoun "I". Some languages, called ergative, Gamilaraay among them, distinguish instead between Agents and Patients. In ergative languages, the single participant in an intransitive sentence, such as "I run", is treated the same as the patient in a transitive sentence, giving the equivalent of "me run". Only in transitive sentences would the equivalent of the pronoun "I" be used. In this way the semantic roles can map onto the grammatical relations in different ways, grouping an intransitive subject either with Agents (accusative type) or Patients (ergative type) or even making each of the three roles differently, which is called the tripartite type.

The shared features of languages which belong to the same typological class type may have arisen completely independently. Their co-occurrence might be due to universal laws governing the structure of natural languages, "language universals", or they might be the result of languages evolving convergent solutions to the recurring communicative problems that humans use language to solve.

While humans have the ability to learn any language, they only do so if they grow up in an environment in which language exists and is used by others. Language is therefore dependent on communities of speakers in which children learn language from their elders and peers and themselves transmit language to their own children. Languages are used by those who speak them to communicate and to solve a plethora of social tasks. Many aspects of language use can be seen to be adapted specifically to these purposes. Due to the way in which language is transmitted between generations and within communities, language perpetually changes, diversifying into new languages or converging due to language contact. The process is similar to the process of evolution, where the process of descent with modification leads to the formation of a phylogenetic tree.

However, languages differ from biological organisms in that they readily incorporate elements from other languages through the process of diffusion, as speakers of different languages come into contact. Humans also frequently speak more than one language, acquiring their first language or languages as children, or learning new languages as they grow up. Because of the increased language contact in the globalizing world, many small languages are becoming endangered as their speakers shift to other languages that afford the possibility to participate in larger and more influential speech communities.

The semantic study of meaning assumes that meaning is in a relation between signs and meanings that are firmly established through social convention. However, semantics does not study the way in which social conventions are made and affect language. Rather, when studying the way in which words and signs are used, it is often the case that words have different meanings, depending on the social context of use. An important example of this is the process called deixis, which describes the way in which certain words refer to entities through their relation between a specific point in time and space when the word is uttered. Such words are, for example, the word, "I" (which designates the person speaking), "now" (which designates the moment of speaking), and "here" (which designates the position of speaking). Signs also change their meanings over time, as the conventions governing their usage gradually change. The study of how the meaning of linguistic expressions changes depending on context is called pragmatics. Deixis is an important part of the way that we use language to point out entities in the world. Pragmatics is concerned with the ways in which language use is patterned and how these patterns contribute to meaning. For example, in all languages, linguistic expressions can be used not just to transmit information, but to perform actions. Certain actions are made only through language, but nonetheless have tangible effects, e.g. the act of "naming", which creates a new name for some entity, or the act of "pronouncing someone man and wife", which creates a social contract of marriage. These types of acts are called speech acts, although they can also be carried out through writing or hand signing.

The form of linguistic expression often does not correspond to the meaning that it actually has in a social context. For example, if at a dinner table a person asks, "Can you reach the salt?", that is, in fact, not a question about the length of the arms of the one being addressed, but a request to pass the salt across the table. This meaning is implied by the context in which it is spoken; these kinds of effects of meaning are called conversational implicatures. These social rules for which ways of using language are considered appropriate in certain situations and how utterances are to be understood in relation to their context vary between communities, and learning them is a large part of acquiring communicative competence in a language.

All healthy, normally developing human beings learn to use language. Children acquire the language or languages used around them: whichever languages they receive sufficient exposure to during childhood. The development is essentially the same for children acquiring sign or oral languages. This learning process is referred to as first-language acquisition, since unlike many other kinds of learning, it requires no direct teaching or specialized study. In "The Descent of Man", naturalist Charles Darwin called this process "an instinctive tendency to acquire an art".
First language acquisition proceeds in a fairly regular sequence, though there is a wide degree of variation in the timing of particular stages among normally developing infants. From birth, newborns respond more readily to human speech than to other sounds. Around one month of age, babies appear to be able to distinguish between different speech sounds. Around six months of age, a child will begin babbling, producing the speech sounds or handshapes of the languages used around them. Words appear around the age of 12 to 18 months; the average vocabulary of an eighteen-month-old child is around 50 words. A child's first utterances are holophrases (literally "whole-sentences"), utterances that use just one word to communicate some idea. Several months after a child begins producing words, he or she will produce two-word utterances, and within a few more months will begin to produce telegraphic speech, or short sentences that are less grammatically complex than adult speech, but that do show regular syntactic structure. From roughly the age of three to five years, a child's ability to speak or sign is refined to the point that it resembles adult language. Studies published in 2013 have indicated that unborn fetuses are capable of language acquisition to some degree.

Acquisition of second and additional languages can come at any age, through exposure in daily life or courses. Children learning a second language are more likely to achieve native-like fluency than adults, but in general, it is very rare for someone speaking a second language to pass completely for a native speaker. An important difference between first language acquisition and additional language acquisition is that the process of additional language acquisition is influenced by languages that the learner already knows.

Languages, understood as the particular set of speech norms of a particular community, are also a part of the larger culture of the community that speaks them. Languages differ not only in pronunciation, vocabulary, and grammar, but also through having different "cultures of speaking." Humans use language as a way of signalling identity with one cultural group as well as difference from others. Even among speakers of one language, several different ways of using the language exist, and each is used to signal affiliation with particular subgroups within a larger culture. Linguists and anthropologists, particularly sociolinguists, ethnolinguists, and linguistic anthropologists have specialized in studying how ways of speaking vary between speech communities.

Linguists use the term "varieties" to refer to the different ways of speaking a language. This term includes geographically or socioculturally defined dialects as well as the jargons or styles of subcultures. Linguistic anthropologists and sociologists of language define communicative style as the ways that language is used and understood within a particular culture.

Because norms for language use are shared by members of a specific group, communicative style also becomes a way of displaying and constructing group identity. Linguistic differences may become salient markers of divisions between social groups, for example, speaking a language with a particular accent may imply membership of an ethnic minority or social class, one's area of origin, or status as a second language speaker. These kinds of differences are not part of the linguistic system, but are an important part of how people use language as a social tool for constructing groups.

However, many languages also have grammatical conventions that signal the social position of the speaker in relation to others through the use of registers that are related to social hierarchies or divisions. In many languages, there are stylistic or even grammatical differences between the ways men and women speak, between age groups, or between social classes, just as some languages employ different words depending on who is listening. For example, in the Australian language Dyirbal, a married man must use a special set of words to refer to everyday items when speaking in the presence of his mother-in-law. Some cultures, for example, have elaborate systems of "social deixis", or systems of signalling social distance through linguistic means. In English, social deixis is shown mostly through distinguishing between addressing some people by first name and others by surname, and in titles such as "Mrs.", "boy", "Doctor", or "Your Honor", but in other languages, such systems may be highly complex and codified in the entire grammar and vocabulary of the language. For instance, in languages of east Asia such as Thai, Burmese, and Javanese, different words are used according to whether a speaker is addressing someone of higher or lower rank than oneself in a ranking system with animals and children ranking the lowest and gods and members of royalty as the highest.

Throughout history a number of different ways of representing language in graphic media have been invented. These are called writing systems.

The use of writing has made language even more useful to humans. It makes it possible to store large amounts of information outside of the human body and retrieve it again, and it allows communication across distances that would otherwise be impossible. Many languages conventionally employ different genres, styles, and registers in written and spoken language, and in some communities, writing traditionally takes place in an entirely different language than the one spoken. There is some evidence that the use of writing also has effects on the cognitive development of humans, perhaps because acquiring literacy generally requires explicit and formal education.

The invention of the first writing systems is roughly contemporary with the beginning of the Bronze Age in the late 4th millennium BC. The Sumerian archaic cuneiform script and the Egyptian hieroglyphs are generally considered to be the earliest writing systems, both emerging out of their ancestral proto-literate symbol systems from 3400–3200 BC with the earliest coherent texts from about 2600 BC. It is generally agreed that Sumerian writing was an independent invention; however, it is debated whether Egyptian writing was developed completely independently of Sumerian, or was a case of cultural diffusion. A similar debate exists for the Chinese script, which developed around 1200 BC. The pre-Columbian Mesoamerican writing systems (including among others Olmec and Maya scripts) are generally believed to have had independent origins.

All languages change as speakers adopt or invent new ways of speaking and pass them on to other members of their speech community. Language change happens at all levels from the phonological level to the levels of vocabulary, morphology, syntax, and discourse. Even though language change is often initially evaluated negatively by speakers of the language who often consider changes to be "decay" or a sign of slipping norms of language usage, it is natural and inevitable.

Changes may affect specific sounds or the entire phonological system. Sound change can consist of the replacement of one speech sound or phonetic feature by another, the complete loss of the affected sound, or even the introduction of a new sound in a place where there had been none. Sound changes can be "conditioned" in which case a sound is changed only if it occurs in the vicinity of certain other sounds. Sound change is usually assumed to be "regular", which means that it is expected to apply mechanically whenever its structural conditions are met, irrespective of any non-phonological factors. On the other hand, sound changes can sometimes be "sporadic", affecting only one particular word or a few words, without any seeming regularity. Sometimes a simple change triggers a chain shift in which the entire phonological system is affected. This happened in the Germanic languages when the sound change known as Grimm's law affected all the stop consonants in the system. The original consonant * became /b/ in the Germanic languages, the previous * in turn became /p/, and the previous * became /f/. The same process applied to all stop consonants and explains why Italic languages such as Latin have "p" in words like pater" and pisces", whereas Germanic languages, like English, have father" and fish".

Another example is the Great Vowel Shift in English, which is the reason that the spelling of English vowels do not correspond well to their current pronunciation. This is because the vowel shift brought the already established orthography out of synchronization with pronunciation. Another source of sound change is the erosion of words as pronunciation gradually becomes increasingly indistinct and shortens words, leaving out syllables or sounds. This kind of change caused Latin "mea domina" to eventually become the French "madame" and American English "ma'am".

Change also happens in the grammar of languages as discourse patterns such as idioms or particular constructions become grammaticalized. This frequently happens when words or morphemes erode and the grammatical system is unconsciously rearranged to compensate for the lost element. For example, in some varieties of Caribbean Spanish the final /s/ has eroded away. Since Standard Spanish uses final /s/ in the morpheme marking the second person subject "you" in verbs, the Caribbean varieties now have to express the second person using the pronoun "tú". This means that the sentence "what's your name" is "¿como te llamas?" in Standard Spanish, but in Caribbean Spanish. The simple sound change has affected both morphology and syntax. Another common cause of grammatical change is the gradual petrification of idioms into new grammatical forms, for example, the way the English "going to" construction lost its aspect of movement and in some varieties of English has almost become a full-fledged future tense (e.g. "I'm gonna").

Language change may be motivated by "language internal" factors, such as changes in pronunciation motivated by certain sounds being difficult to distinguish aurally or to produce, or through patterns of change that cause some rare types of constructions to drift towards more common types. Other causes of language change are social, such as when certain pronunciations become emblematic of membership in certain groups, such as social classes, or with ideologies, and therefore are adopted by those who wish to identify with those groups or ideas. In this way, issues of identity and politics can have profound effects on language structure.

One important source of language change is contact and resulting diffusion of linguistic traits between languages. Language contact occurs when speakers of two or more languages or varieties interact on a regular basis. Multilingualism is likely to have been the norm throughout human history and most people in the modern world are multilingual. Before the rise of the concept of the ethno-national state, monolingualism was characteristic mainly of populations inhabiting small islands. But with the ideology that made one people, one state, and one language the most desirable political arrangement, monolingualism started to spread throughout the world. Nonetheless, there are only 250 countries in the world corresponding to some 6000 languages, which means that most countries are multilingual and most languages therefore exist in close contact with other languages.

When speakers of different languages interact closely, it is typical for their languages to influence each other. Through sustained language contact over long periods, linguistic traits diffuse between languages, and languages belonging to different families may converge to become more similar. In areas where many languages are in close contact, this may lead to the formation of language areas in which unrelated languages share a number of linguistic features. A number of such language areas have been documented, among them, the Balkan language area, the Mesoamerican language area, and the Ethiopian language area. Also, larger areas such as South Asia, Europe, and Southeast Asia have sometimes been considered language areas, because of widespread diffusion of specific areal features.

Language contact may also lead to a variety of other linguistic phenomena, including language convergence, borrowing, and relexification (replacement of much of the native vocabulary with that of another language). In situations of extreme and sustained language contact, it may lead to the formation of new mixed languages that cannot be considered to belong to a single language family. One type of mixed language called pidgins occurs when adult speakers of two different languages interact on a regular basis, but in a situation where neither group learns to speak the language of the other group fluently. In such a case, they will often construct a communication form that has traits of both languages, but which has a simplified grammatical and phonological structure. The language comes to contain mostly the grammatical and phonological categories that exist in both languages. Pidgin languages are defined by not having any native speakers, but only being spoken by people who have another language as their first language. But if a Pidgin language becomes the main language of a speech community, then eventually children will grow up learning the pidgin as their first language. As the generation of child learners grow up, the pidgin will often be seen to change its structure and acquire a greater degree of complexity. This type of language is generally called a creole language. An example of such mixed languages is Tok Pisin, the official language of Papua New-Guinea, which originally arose as a Pidgin based on English and Austronesian languages; others are Kreyòl ayisyen, the French-based creole language spoken in Haiti, and Michif, a mixed language of Canada, based on the Native American language Cree and French.

"SIL Ethnologue" defines a "living language" as "one that has at least one speaker for whom it is their first language". The exact number of known living languages varies from 6,000 to 7,000, depending on the precision of one's definition of "language", and in particular, on how one defines the distinction between languages and dialects. As of 2016, "Ethnologue" cataloged 7,097 living human languages. The "Ethnologue" establishes linguistic groups based on studies of mutual intelligibility, and therefore often includes more categories than more conservative classifications. For example, the Danish language that most scholars consider a single language with several dialects is classified as two distinct languages (Danish and Jutish) by the "Ethnologue".

According to the "Ethnologue", 389 languages (nearly 6%) have more than a million speakers. These languages together account for 94% of the world's population, whereas 94% of the world's languages account for the remaining 6% of the global population. To the right is a table of the world's 10 most spoken languages with population estimates from the "Ethnologue" (2009 figures).

There is no clear distinction between a language and a dialect, notwithstanding a famous aphorism attributed to linguist Max Weinreich that "a language is a dialect with an army and navy". For example, national boundaries frequently override linguistic difference in determining whether two linguistic varieties are languages or dialects. Hakka, Cantonese and Mandarin are, for example, often classified as "dialects" of Chinese, even though they are more different from each other than Swedish is from Norwegian. Before the Yugoslav civil war, Serbo-Croatian was generally considered a single language with two normative variants, but due to sociopolitical reasons, Croatian and Serbian are now often treated as separate languages and employ different writing systems. In other words, the distinction may hinge on political considerations as much as on cultural differences, distinctive writing systems, or degree of mutual intelligibility.

The world's languages can be grouped into language families consisting of languages that can be shown to have common ancestry. Linguists recognize many hundreds of language families, although some of them can possibly be grouped into larger units as more evidence becomes available and in-depth studies are carried out. At present, there are also dozens of language isolates: languages that cannot be shown to be related to any other languages in the world. Among them are Basque, spoken in Europe, Zuni of New Mexico, Purépecha of Mexico, Ainu of Japan, Burushaski of Pakistan, and many others.

The language family of the world that has the most speakers is the Indo-European languages, spoken by 46% of the world's population. This family includes major world languages like English, Spanish, French, German, Russian, and Hindustani (Hindi/Urdu). The Indo-European family achieved prevalence first during the Eurasian Migration Period (c. 400–800 AD), and subsequently through the European colonial expansion, which brought the Indo-European languages to a politically and often numerically dominant position in the Americas and much of Africa. The Sino-Tibetan languages are spoken by 20% of the world's population and include many of the languages of East Asia, including Hakka, Mandarin Chinese, Cantonese, and hundreds of smaller languages.

Africa is home to a large number of language families, the largest of which is the Niger-Congo language family, which includes such languages as Swahili, Shona, and Yoruba. Speakers of the Niger-Congo languages account for 6.9% of the world's population. A similar number of people speak the Afroasiatic languages, which include the populous Semitic languages such as Arabic, Hebrew language, and the languages of the Sahara region, such as the Berber languages and Hausa.

The Austronesian languages are spoken by 5.5% of the world's population and stretch from Madagascar to maritime Southeast Asia all the way to Oceania. It includes such languages as Malagasy, Māori, Samoan, and many of the indigenous languages of Indonesia and Taiwan. The Austronesian languages are considered to have originated in Taiwan around 3000 BC and spread through the Oceanic region through island-hopping, based on an advanced nautical technology. Other populous language families are the Dravidian languages of South Asia (among them Kannada Tamil and Telugu), the Turkic languages of Central Asia (such as Turkish), the Austroasiatic (among them Khmer), and Tai–Kadai languages of Southeast Asia (including Thai).

The areas of the world in which there is the greatest linguistic diversity, such as the Americas, Papua New Guinea, West Africa, and South-Asia, contain hundreds of small language families. These areas together account for the majority of the world's languages, though not the majority of speakers. In the Americas, some of the largest language families include the Quechumaran, Arawak, and Tupi-Guarani families of South America, the Uto-Aztecan, Oto-Manguean, and Mayan of Mesoamerica, and the Na-Dene, Iroquoian, and Algonquian language families of North America. In Australia, most indigenous languages belong to the Pama-Nyungan family, whereas New Guinea is home to a large number of small families and isolates, as well as a number of Austronesian languages.

Language endangerment occurs when a language is at risk of falling out of use as its speakers die out or shift to speaking another language. Language loss occurs when the language has no more native speakers, and becomes a "dead language". If eventually no one speaks the language at all, it becomes an "extinct language". While languages have always gone extinct throughout human history, they have been disappearing at an accelerated rate in the 20th and 21st centuries due to the processes of globalization and neo-colonialism, where the economically powerful languages dominate other languages.

The more commonly spoken languages dominate the less commonly spoken languages, so the less commonly spoken languages eventually disappear from populations. The total number of languages in the world is not known. Estimates vary depending on many factors. The consensus is that there are between 6,000 and 7,000 languages spoken as of 2010, and that between 50–90% of those will have become extinct by the year 2100. The top 20 languages, those spoken by more than 50 million speakers each, are spoken by 50% of the world's population, whereas many of the other languages are spoken by small communities, most of them with less than 10,000 speakers.

The United Nations Educational, Scientific and Cultural Organization (UNESCO) operates with five levels of language endangerment: "safe", "vulnerable" (not spoken by children outside the home), "definitely endangered" (not spoken by children), "severely endangered" (only spoken by the oldest generations), and "critically endangered" (spoken by few members of the oldest generation, often semi-speakers). Notwithstanding claims that the world would be better off if most adopted a single common "lingua franca", such as English or Esperanto, there is a consensus that the loss of languages harms the cultural diversity of the world. It is a common belief, going back to the biblical narrative of the tower of Babel in the Old Testament, that linguistic diversity causes political conflict, but this is contradicted by the fact that many of the world's major episodes of violence have taken place in situations with low linguistic diversity, such as the Yugoslav and American Civil War, or the genocide of Rwanda, whereas many of the most stable political units have been highly multilingual.

Many projects aim to prevent or slow this loss by revitalizing endangered languages and promoting education and literacy in minority languages. Across the world, many countries have enacted specific legislation to protect and stabilize the language of indigenous speech communities. A minority of linguists have argued that language loss is a natural process that should not be counteracted, and that documenting endangered languages for posterity is sufficient.



</doc>
<doc id="42649463" url="https://en.wikipedia.org/wiki?curid=42649463" title="Passive fluency">
Passive fluency

Passive fluency is where a person can fluently read and audibly understand a language whilst not having the ability to fluently speak or write the language. Passive fluency is often brought about by being raised in one language (which becomes the person's passive language) and being schooled in another language (which becomes the person's native language).

People who are passively fluent in a language are often latent speakers who were raised in an environment where the language was spoken but did not become native speakers.

A more common term for this phenomenon is 'passive bilingualism'. Grosjean argues that there has been a monolingual bias regarding who is considered a 'bilingual', where people who do not have equal competence in all their languages are judges not speaking properly. 'Balanced bilinguals' are, in fact, very rare, and the fluency of a bilingual in his/ her languages is domain- specific: it depends on what they need the languages for. This means that speakers may not admit to their fluency in their passive language, despite the fact that there are social (extralinguistic) factors underlying their different competencies.

Karlos Cid Abasolo discusses that passive bilingualism would be a minimum requirement for the co- official status of Basque and Spanish to become a working reality. As there are currently many monolingual Spanish speakers, and no monolingual Basque speakers, there is not yet a situation where an individual fluent in Basque could speak in his/ her mother tongue, regardless of the domain, circumstance or interlocutor.


</doc>
<doc id="20324344" url="https://en.wikipedia.org/wiki?curid=20324344" title="Constructed language">
Constructed language

A constructed language (sometimes called a conlang) is a language whose phonology, grammar, and vocabulary, instead of having developed naturally, are consciously devised. Constructed languages may also be referred to as artificial languages, planned languages or invented languages and in some cases, fictional languages. Planned languages are languages that have been purposefully designed. They are the result of deliberate controlling intervention, thus of a form of language planning. 

There are many possible reasons to create a constructed language, such as to ease human communication (see international auxiliary language and code); to give fiction or an associated constructed setting an added layer of realism; for experimentation in the fields of linguistics, cognitive science, and machine learning; for artistic creation; and for language games. 

The expression "planned language" is sometimes used to indicate international auxiliary languages and other languages designed for actual use in human communication. Some prefer it to the adjective "artificial", as this term may be perceived as pejorative. Outside Esperanto culture, the term language planning means the prescriptions given to a natural language to standardize it; in this regard, even a "natural language" may be artificial in some respects, meaning some of its words have been crafted by conscious decision. Prescriptive grammars, which date to ancient times for classical languages such as Latin and Sanskrit, are rule-based codifications of natural languages, such codifications being a middle ground between naïve natural selection and development of language and its explicit construction. The term "glossopoeia" is also used to mean language construction, particularly construction of artistic languages.

Conlang speakers are rare. For example, the Hungarian census of 2011 found 8397 speakers of Esperanto, and the census of 2001 found 10 of Romanid, two each of Interlingua and Ido and one each of Idiom Neutral and Mundolinco. The Russian census of 2010 found that there were in Russia about 992 speakers of Esperanto (on place 120), nine of Ido, one of Edo and no speakers of Slovio or Interlingua.

The terms "planned", "constructed", and "artificial" are used differently in some traditions. For example, few speakers of Interlingua consider their language artificial, since they assert that it has no invented content: Interlingua's vocabulary is taken from a small set of natural languages, and its grammar is based closely on these source languages, even including some degree of irregularity; its proponents prefer to describe its vocabulary and grammar as standardized rather than artificial or constructed. Similarly, Latino sine flexione (LsF) is a simplification of Latin from which the inflections have been removed. As with Interlingua, some prefer to describe its development as "planning" rather than "constructing". Some speakers of Esperanto and Esperantidos also avoid the term "artificial language" because they deny that there is anything "unnatural" about the use of their language in human communication.

By contrast, some philosophers have argued that all human languages are conventional or artificial. François Rabelais, for instance, stated: "C'est abus de dire que nous avons une langue naturelle; les langues sont par institution arbitraires et conventions des peuples." ("It is a misuse of terms to say that we have a natural language; languages are arbitrary and conventions of peoples by institution.")

Furthermore, fictional or experimental languages can be considered "naturalistic" if they model real world languages. For example, if a naturalistic conlang is derived "a posteriori" from another language (real or constructed), it should imitate natural processes of phonological, lexical, and grammatical change. In contrast with languages such as Interlingua, naturalistic fictional languages are not usually intended for easy learning or communication. Thus, naturalistic fictional languages tend to be more difficult and complex. While Interlingua has simpler grammar, syntax, and orthography than its source languages (though more complex and irregular than Esperanto or its descendants), naturalistic fictional languages typically mimic behaviors of natural languages like irregular verbs and nouns, and complicated phonological processes.

In terms of purpose, most constructed languages can broadly be divided into:


The boundaries between these categories are by no means clear. A constructed language could easily fall into more than one of the above categories. A logical language created for aesthetic reasons would also be classifiable as an artistic language, which might be created by someone with philosophical motives intending for said conlang to be used as an auxiliary language. There are no rules, either inherent in the process of language construction or externally imposed, that would limit a constructed language to fitting only one of the above categories.

A constructed language can have native speakers if young children learn it from parents who speak it fluently. According to "Ethnologue", there are "200–2000 who speak Esperanto as a first language". A member of the Klingon Language Institute, d'Armond Speers, attempted to raise his son as a native (bilingual with English) Klingon speaker.

As soon as a constructed language has a community of fluent speakers, especially if it has numerous native speakers, it begins to evolve and hence loses its constructed status. For example, Modern Hebrew and its pronunciation norms were developed from existing traditions of Hebrew, such as Mishnaic Hebrew and Biblical Hebrew following a general Sephardic pronunciation, rather than engineered from scratch, and has undergone considerable changes since the state of Israel was founded in 1948 (Hetzron 1990:693). However, linguist Ghil'ad Zuckermann argues that Modern Hebrew, which he terms "Israeli", is a Semito-European hybrid based not only on Hebrew but also on Yiddish and other languages spoken by revivalists. Zuckermann therefore endorses the translation of the Hebrew Bible into what he calls "Israeli". Esperanto as a living spoken language has evolved significantly from the prescriptive blueprint published in 1887, so that modern editions of the "Fundamenta Krestomatio", a 1903 collection of early texts in the language, require many footnotes on the syntactic and lexical differences between early and modern Esperanto.

Proponents of constructed languages often have many reasons for using them. The famous but disputed Sapir–Whorf hypothesis is sometimes cited; this claims that the language one speaks influences the way one thinks. Thus, a "better" language should allow the speaker to think more clearly or intelligently or to encompass more points of view; this was the intention of Suzette Haden Elgin in creating Láadan, a feminist language embodied in her feminist science fiction series "Native Tongue". A constructed language could also be used to "restrict" thought, as in George Orwell's Newspeak, or to "simplify" thought, as in Toki Pona. In contrast, linguists such as Steven Pinker argue that ideas exist independently of language. For example, in the book "The Language Instinct", Pinker states that children spontaneously re-invent slang and even grammar with each generation. These linguists argue that attempts to control the range of human thought through the reform of language would fail, as concepts like "freedom" will reappear in new words if the old words vanish.

Proponents claim a particular language makes it easier to express and understand concepts in one area, and more difficult in others. An example can be taken from the way various programming languages make it easier to write certain kinds of programs and harder to write others.

Another reason cited for using a constructed language is the telescope rule, which claims that it takes less time to first learn a simple constructed language and then a natural language, than to learn only a natural language. Thus, if someone wants to learn English, some suggest learning Basic English first. Constructed languages like Esperanto and Interlingua are in fact often simpler due to the typical lack of irregular verbs and other grammatical quirks. Some studies have found that learning Esperanto helps in learning a non-constructed language later (see propaedeutic value of Esperanto).

Codes for constructed languages include the ISO 639-2 "art" for conlangs; however, some constructed languages have their own ISO 639 language codes (e.g. "eo" and "epo" for Esperanto, "jbo" for Lojban, "ia" and "ina" for Interlingua, "tlh" for Klingon and "io" and "ido" for Ido).

One constraint on a constructed language is that if it was constructed to be a natural language for use by fictional foreigners or aliens, as with Dothraki and High Valyrian in the "Game of Thrones" series, which was adapted from the "A Song of Ice and Fire" book series, the language should be easily pronounced by actors, and should fit with and incorporate any fragments of the language already invented by the book's author, and preferably also fit with any personal names of fictional speakers of the language.

An "a priori" constructed language is one whose features (including vocabulary, grammar, etc.) are not based on an existing language, and an "a posteriori" language is the opposite. This categorization, however, is not absolute, as many constructed languages may be called "a priori" when considering some linguistic factors, and at the same time "a posteriori" when considering other factors.

An "a priori" language (from Latin "a priori", "from the former") is any constructed language of which all or a number of features are not based on existing languages, but rather invented or elaborated as to work in a different way or to allude different purposes. Some "a priori" languages are designed to be international auxiliary languages that remove what could be considered an unfair learning advantage for native speakers of a source language that would otherwise exist for "a posteriori" languages. Others, known as philosophical or taxonomic languages, try to categorize their vocabulary, either to express an underlying philosophy or to make it easier to recognize new vocabulary. Finally, many artistic languages, created for either personal use or for use in a fictional medium, employ consciously constructed grammars and vocabularies, and are best understood as "a priori".




An "a posteriori" language (from Latin "a posteriori", "from the latter"), according to Louis Couturat, is any constructed language whose elements are borrowed from or based on existing languages. The term can also be extended to controlled versions of natural languages, and is most commonly used to refer to vocabulary despite other features. While most auxiliary languages are "a posteriori" due to their intended function as a medium of communication, many artistic languages are fully a posteriori in design—many for the purposes of alternate history. In distinguishing whether the language is "a priori" or "a posteriori", the prevalence and distribution of respectable traits is often the key.




Grammatical speculation dates from Classical Antiquity, appearing for instance in Plato's "Cratylus" in Hermogenes's contention that words are not inherently linked to what they refer to; that people apply "a piece of their own voice ... to the thing". Athenaeus of Naucratis, in Book III of Deipnosophistae, tells the story of two figures: Dionysius of Sicily and Alexarchus. Dionysius of Sicily created neologisms like "menandros" "virgin" (from "menei" "waiting" and "andra" "husband"), "menekratēs" "pillar" (from "menei" "it remains in one place" and "kratei" "it is strong"), and "ballantion" "javelin" (from "balletai enantion" "thrown against someone"). Incidentally, the more common Greek words for those three are "parthenos", "stulos", and "akon". Alexarchus of Macedon, the brother of King Cassander of Macedon, was the founder of the city of Ouranopolis. Athenaeus recounts a story told by Heracleides of Lembos that Alexarchus "introduced a peculiar vocabulary, referring to a rooster as a "dawn-crier," a barber as a "mortal-shaver," a drachma as "worked silver"...and a herald as an "aputēs" [from "ēputa" "loud-voiced"]. "He once wrote something...to the public authorities in Casandreia...As for what this letter says, in my opinion not even the Pythian god could make sense of it." While the mechanisms of grammar suggested by classical philosophers were designed to explain existing languages (Latin, Greek, Sanskrit), they were not used to construct new grammars. Roughly contemporary to Plato, in his descriptive grammar of Sanskrit, Pāṇini constructed a set of rules for explaining language, so that the text of his grammar may be considered a mixture of natural and constructed language.

A legend recorded in the seventh-century Irish work Auraicept na n-Éces claims that Fénius Farsaid visited Shinar after the confusion of tongues, and he and his scholars studied the various languages for ten years, taking the best features of each to create "in Bérla tóbaide" ("the selected language"), which he named "Goídelc"—the Irish language. This appears to be the first mention of the concept of a constructed language in literature.

The earliest non-natural languages were considered less "constructed" than "super-natural", mystical, or divinely inspired. The Lingua Ignota, recorded in the 12th century by St. Hildegard of Bingen is an example, and apparently the first entirely artificial language. It is a form of private mystical cant (see also language of angels). An important example from Middle-Eastern culture is Balaibalan, invented in the 16th century. Kabbalistic grammatical speculation was directed at recovering the original language spoken by Adam and Eve in Paradise, lost in the confusion of tongues. The first Christian project for an ideal language is outlined in Dante Alighieri's "De vulgari eloquentia", where he searches for the ideal Italian vernacular suited for literature. Ramon Llull's "Ars Magna" was a project of a perfect language with which the infidels could be convinced of the truth of the Christian faith. It was basically an application of combinatorics on a given set of concepts. During the Renaissance, Lullian and Kabbalistic ideas were drawn upon in a magical context, resulting in cryptographic applications. The Voynich manuscript may be an example of this.

Renaissance interest in Ancient Egypt, notably the discovery of the "Hieroglyphica" of Horapollo, and first encounters with the Chinese script directed efforts towards a perfect written language. Johannes Trithemius, in "Steganographia" and "Polygraphia", attempted to show how all languages can be reduced to one. In the 17th century, interest in magical languages was continued by the Rosicrucians and Alchemists (like John Dee and his Enochian). Jakob Boehme in 1623 spoke of a "natural language" ("Natursprache") of the senses.

Musical languages from the Renaissance were tied up with mysticism, magic and alchemy, sometimes also referred to as the language of the birds. The Solresol project of 1817 re-invented the concept in a more pragmatic context.

The 17th century saw the rise of projects for "philosophical" or "a priori" languages, such as:


These early taxonomic conlangs produced systems of hierarchical classification that were intended to result in both spoken and written expression. Leibniz had a similar purpose for his "lingua generalis" of 1678, aiming at a lexicon of characters upon which the user might perform calculations that would yield true propositions automatically, as a side-effect developing binary calculus. These projects were not only occupied with reducing or modelling grammar, but also with the arrangement of all human knowledge into "characters" or hierarchies, an idea that with the Enlightenment would ultimately lead to the "Encyclopédie". Many of these 17th−18th centuries conlangs were pasigraphies, or purely written languages with no spoken form or a spoken form that would vary greatly according to the native language of the reader.

Leibniz and the encyclopedists realized that it is impossible to organize human knowledge unequivocally in a tree diagram, and consequently to construct an "a priori" language based on such a classification of concepts. Under the entry "Charactère", D'Alembert critically reviewed the projects of philosophical languages of the preceding century. After the "Encyclopédie", projects for "a priori" languages moved more and more to the lunatic fringe. Individual authors, typically unaware of the history of the idea, continued to propose taxonomic philosophical languages until the early 20th century (e.g. Ro), but most recent engineered languages have had more modest goals; some are limited to a specific field, like mathematical formalism or calculus (e.g. Lincos and programming languages), others are designed for eliminating syntactical ambiguity (e.g., Loglan and Lojban) or maximizing conciseness (e.g., Ithkuil).

Already in the "Encyclopédie" attention began to focus on "a posteriori" auxiliary languages. Joachim Faiguet de Villeneuve in the article on "Langue" wrote a short proposition of a "laconic" or regularized grammar of French. During the 19th century, a bewildering variety of such International Auxiliary Languages (IALs) were proposed, so that Louis Couturat and Léopold Leau in "Histoire de la langue universelle" (1903) reviewed 38 projects.

The first of these that made any international impact was Volapük, proposed in 1879 by Johann Martin Schleyer; within a decade, 283 Volapükist clubs were counted all over the globe. However, disagreements between Schleyer and some prominent users of the language led to schism, and by the mid-1890s it fell into obscurity, making way for Esperanto, proposed in 1887 by L. L. Zamenhof, and its descendants. Interlingua, the most recent auxlang to gain a significant number of speakers, emerged in 1951, when the International Auxiliary Language Association published its Interlingua–English Dictionary and an accompanying . The success of Esperanto did not stop others from trying to construct new auxiliary languages, such as Leslie Jones' Eurolengo, which mixes elements of English and Spanish.

Loglan (1955) and its descendants constitute a pragmatic return to the aims of the "a priori" languages, tempered by the requirement of usability of an auxiliary language. Thus far, these modern "a priori" languages have garnered only small groups of speakers.

Robot Interaction Language (2010) is a spoken language that is optimized for communication between machines and humans. The major goals of ROILA are that it should be easily learnable by the human user, and optimized for efficient recognition by computer speech recognition algorithms.

Language can be artistic to the extent that artists use language as a source of creativity in art, poetry, calligraphy or as a metaphor to address themes as cultural diversity and the vulnerability of the individual in a globalizing world. 

Some people prefer however to take pleasure in constructing, crafting a language by a conscious decision for reasons of literary enjoyment or aesthetic reasons without any claim of usefulness. Such artistic languages begin to appear in Early Modern literature (in Pantagruel, and in Utopian contexts), but they only seem to gain notability as serious projects beginning in the 20th century. "A Princess of Mars" (1912) by Edgar Rice Burroughs was possibly the first fiction of that century to feature a constructed language. J. R. R. Tolkien developed families of related fictional languages and discussed artistic languages publicly, giving a lecture entitled "A Secret Vice" in 1931 at a congress. (Orwell's Newspeak is considered a satire of an IAL rather than an artistic language proper.)

By the beginning of the first decade of the 21st century, it had become common for science-fiction and fantasy works set in other worlds to feature constructed languages, or more commonly, an extremely limited but defined vocabulary which "suggests" the existence of a complete language, or whatever portions of the language are needed for the story, and constructed languages are a regular part of the genre, appearing in "Star Wars", "Star Trek", "Lord of the Rings" (Elvish), "Stargate SG-1", "", "Game of Thrones" (Dothraki language and Valyrian languages), "Avatar", "Dune" and the "Myst" series of computer adventure games.

The matter of whether or not a constructed language can be owned or protected by intellectual property laws, or if it would even be possible to enforce those laws, is contentious. 

In a 2015 lawsuit, CBS and Paramount Pictures challenged a fan film project called Axanar, stating the project infringed upon their intellectual property, which included the Klingon language, among other creative elements. During the controversy, Marc Okrand, the language's original designer expressed doubt as to whether Paramount's claims of ownership were valid.

David J. Peterson, a linguist who created multiple well-known constructed languages including the Valyrian languages and Dothraki, advocated a similar opinion, saying that "Theoretically, anyone can publish anything using any language I created, and, in my opinion, neither I nor anyone else should be able to do anything about it."

However, Peterson also expressed concern that the respective rights-holders—regardless of whether or not their ownership of the rights is legitimate—would be likely to sue individuals who publish material in said languages, especially if the author might profit from said material. 

Furthermore, comprehensive learning material for such constructed languages as High Valyrian and Klingon has been published and made freely accessible on the language-learning platform Duolingo—but those courses are licensed by the respective copyright holders. Because only a few such disputes have occurred thus far, the legal consensus on ownership of languages remains uncertain.

Various paper zines on constructed languages were published from the 1970s through the 1990s, such as "Glossopoeic Quarterly", "Taboo Jadoo", and "The Journal of Planned Languages".
The Conlang Mailing List was founded in 1991, and later split off an AUXLANG mailing list dedicated to international auxiliary languages. In the early to mid-1990s a few conlang-related zines were published as email or websites, such as "Vortpunoj" and "Model Languages". The Conlang mailing list has developed a community of conlangers with its own customs, such as translation challenges and translation relays, and its own terminology. Sarah Higley reports from results of her surveys that the demographics of the Conlang list are primarily men from North America and western Europe, with a smaller number from Oceania, Asia, the Middle East, and South America, with an age range from thirteen to over sixty; the number of women participating has increased over time.

More recently founded online communities include the Zompist Bulletin Board (ZBB; since 2001) and the Conlanger Bulletin Board. Discussion on these forums includes presentation of members' conlangs and feedback from other members, discussion of natural languages, whether particular conlang features have natural language precedents, and how interesting features of natural languages can be repurposed for conlangs, posting of interesting short texts as translation challenges, and meta-discussion about the philosophy of conlanging, conlangers' purposes, and whether conlanging is an art or a hobby. Another 2001 survey by Patrick Jarrett showed an average age of 30.65, with the average time since starting to invent languages 11.83 years. A more recent thread on the ZBB showed that many conlangers spend a relatively small amount of time on any one conlang, moving from one project to another; about a third spend years on developing the same language.



</doc>
<doc id="27699" url="https://en.wikipedia.org/wiki?curid=27699" title="Sign language">
Sign language

Sign languages (also known as signed languages) are languages that use the visual-manual modality to convey meaning. Sign languages are expressed through manual articulations in combination with non-manual elements. Sign languages are full-fledged natural languages with their own grammar and lexicon. Sign languages are not universal and they are not mutually intelligible with each other, although there are also striking similarities among sign languages.

Linguists consider both spoken and signed communication to be types of natural language, meaning that both emerged through an abstract, protracted aging process and evolved over time without meticulous planning. Sign language should not be confused with body language, a type of nonverbal communication.

Wherever communities of deaf people exist, sign languages have developed as handy means of communication and they form the core of local deaf cultures. Although signing is used primarily by the deaf and hard of hearing, it is also used by hearing individuals, such as those unable to physically speak, those who have trouble with spoken language due to a disability or condition (augmentative and alternative communication), or those with deaf family members, such as children of deaf adults.

It is unclear how many sign languages currently exist worldwide. Each country generally has its own native sign language, and some have more than one. The 2013 edition of "Ethnologue" lists 137 sign languages. Some sign languages have obtained some form of legal recognition, while others have no status at all.

Linguists distinguish natural sign languages from other systems that are precursors to them or derived from them, such as invented manual codes for spoken languages, home sign, "baby sign", and signs learned by non-human primates.

Groups of deaf people have used sign languages throughout history. One of the earliest written records of a sign language is from the fifth century BC, in Plato's "Cratylus", where Socrates says: "If we hadn't a voice or a tongue, and wanted to express things to one another, wouldn't we try to make signs by moving our hands, head, and the rest of our body, just as dumb people do at present?"

Until the 19th century, most of what is known about historical sign languages is limited to the manual alphabets (fingerspelling systems) that were invented to facilitate transfer of words from a spoken language to a sign language, rather than documentation of the language itself. Pedro Ponce de León (1520–1584) is said to have developed the first manual alphabet.

In 1620, Juan Pablo Bonet published (‘Reduction of letters and art for teaching mute people to speak’) in Madrid. It is considered the first modern treatise of sign language phonetics, setting out a method of oral education for deaf people and a manual alphabet.
In Britain, manual alphabets were also in use for a number of purposes, such as secret communication, public speaking, or communication by deaf people. In 1648, John Bulwer described "Master Babington", a deaf man proficient in the use of a manual alphabet, "contryved on the joynts of his fingers", whose wife could converse with him easily, even in the dark through the use of tactile signing.

In 1680, George Dalgarno published "Didascalocophus, or, The deaf and dumb mans tutor", in which he presented his own method of deaf education, including an "arthrological" alphabet, where letters are indicated by pointing to different joints of the fingers and palm of the left hand. Arthrological systems had been in use by hearing people for some time; some have speculated that they can be traced to early Ogham manual alphabets.

The vowels of this alphabet have survived in the contemporary alphabets used in British Sign Language, Auslan and New Zealand Sign Language. The earliest known printed pictures of consonants of the modern two-handed alphabet appeared in 1698 with "Digiti Lingua" (Latin for "Language" [or "Tongue"] "of the Finger"), a pamphlet by an anonymous author who was himself unable to speak. He suggested that the manual alphabet could also be used by mutes, for silence and secrecy, or purely for entertainment. Nine of its letters can be traced to earlier alphabets, and 17 letters of the modern two-handed alphabet can be found among the two sets of 26 handshapes depicted.

Charles de La Fin published a book in 1692 describing an alphabetic system where pointing to a body part represented the first letter of the part (e.g. Brow=B), and vowels were located on the fingertips as with the other British systems. He described such codes for both English and Latin.

By 1720, the British manual alphabet had found more or less its present form. Descendants of this alphabet have been used by deaf communities (or at least in classrooms) in former British colonies India, Australia, New Zealand, Uganda and South Africa, as well as the republics and provinces of the former Yugoslavia, Grand Cayman Island in the Caribbean, Indonesia, Norway, Germany and the United States.

Frenchman Charles-Michel de l'Épée published his manual alphabet in the 18th century, which has survived basically unchanged in France and North America until the present time. In 1755, Abbé de l'Épée founded the first school for deaf children in Paris; Laurent Clerc was arguably its most famous graduate. Clerc went to the United States with Thomas Hopkins Gallaudet to found the American School for the Deaf in Hartford, Connecticut, in 1817. Gallaudet's son, Edward Miner Gallaudet, founded a school for the deaf in 1857 in Washington, D.C., which in 1864 became the National Deaf-Mute College. Now called Gallaudet University, it is still the only liberal arts university for deaf people in the world.

International Sign, formerly known as Gestuno, is used mainly at international deaf events such as the Deaflympics and meetings of the World Federation of the Deaf. While recent studies claim that International Sign is a kind of a pidgin, they conclude that it is more complex than a typical pidgin and indeed is more like a full sign language.
While the more commonly used term is International Sign, it is sometimes referred to as Gestuno, or International Sign Pidgin and International Gesture (IG). International Sign is a term used by the World Federation of the Deaf and other international organisations.

In linguistic terms, sign languages are as rich and complex as any spoken language, despite the common misconception that they are not "real languages". Professional linguists have studied many sign languages and found that they exhibit the fundamental properties that exist in all languages.

Sign languages are not mime—in other words, signs are conventional, often arbitrary and do not necessarily have a visual relationship to their referent, much as most spoken language is not onomatopoeic. While iconicity is more systematic and widespread in sign languages than in spoken ones, the difference is not categorical. The visual modality allows the human preference for close connections between form and meaning, present but suppressed in spoken languages, to be more fully expressed. This does not mean that sign languages are a visual rendition of a spoken language. They have complex grammars of their own and can be used to discuss any topic, from the simple and concrete to the lofty and abstract.

Sign languages, like spoken languages, organize elementary, meaningless units called phonemes into meaningful semantic units. (These were once called cheremes, from the Greek word for "hand", in the case of sign languages, by analogy to the phonemes, from Greek for "voice", of spoken languages, but now also called phonemes, since the function is the same.) This is often called duality of patterning. As in spoken languages, these meaningless units are represented as (combinations of) features, although crude distinctions are often also made in terms of handshape (or "handform"), orientation, location (or "place of articulation"), movement, and non-manual expression. More generally, both sign and spoken languages share the characteristics that linguists have found in all natural human languages, such as transitoriness, semanticity, arbitrariness, productivity, and cultural transmission.

Common linguistic features of many sign languages are the occurrence of classifiers constructions, a high degree of inflection by means of changes of movement, and a topic-comment syntax. More than spoken languages, sign languages can convey meaning by simultaneous means, e.g. by the use of space, two manual articulators, and the signer's face and body. Though there is still much discussion on the topic of iconicity in sign languages, classifiers are generally considered to be highly iconic, as these complex constructions "function as predicates that may express any or all of the following: motion, position, stative-descriptive, or handling information". It needs to be noted that the term classifier is not used by everyone working on these constructions. Across the field of sign language linguistics the same constructions are also referred with other terms.

Today, linguists study sign languages as true languages, part of the field of linguistics. However, the category "sign languages" was not added to the "Linguistic Bibliography / Bibliographie Linguistique" until the 1988 volume, when it appeared with 39 entries.

There is a common misconception that sign languages are somehow dependent on spoken languages: that they are spoken language expressed in signs, or that they were invented by hearing people. Similarities in language processing in the brain between signed and spoken languages further perpetuated this misconception. Hearing teachers in deaf schools, such as Charles-Michel de l'Épée or Thomas Hopkins Gallaudet, are often incorrectly referred to as "inventors" of sign language. Instead, sign languages, like all natural languages, are developed by the people who use them, in this case, deaf people, who may have little or no knowledge of any spoken language.

As a sign language develops, it sometimes borrows elements from spoken languages, just as all languages borrow from other languages that they are in contact with. Sign languages vary in how and how much they borrow from spoken languages. In many sign languages, a manual alphabet (fingerspelling) may be used in signed communication to borrow a word from a spoken language, by spelling out the letters. This is most commonly used for proper names of people and places; it is also used in some languages for concepts for which no sign is available at that moment, particularly if the people involved are to some extent bilingual in the spoken language. Fingerspelling can sometimes be a source of new signs, such as initialized signs, in which the handshape represents the first letter of a spoken word with the same meaning.

On the whole, though, sign languages are independent of spoken languages and follow their own paths of development. For example, British Sign Language (BSL) and American Sign Language (ASL) are quite different and mutually unintelligible, even though the hearing people of the United Kingdom and the United States share the same spoken language. The grammars of sign languages do not usually resemble those of spoken languages used in the same geographical area; in fact, in terms of syntax, ASL shares more with spoken Japanese than it does with English.

Similarly, countries which use a single spoken language throughout may have two or more sign languages, or an area that contains more than one spoken language might use only one sign language. South Africa, which has 11 official spoken languages and a similar number of other widely used spoken languages, is a good example of this. It has only one sign language with two variants due to its history of having two major educational institutions for the deaf which have served different geographic areas of the country.

Sign languages exploit the unique features of the visual medium (sight), but may also exploit tactile features (tactile sign languages). Spoken language is by and large linear; only one sound can be made or received at a time. Sign language, on the other hand, is visual and, hence, can use a simultaneous expression, although this is limited articulatorily and linguistically. Visual perception allows processing of simultaneous information.

One way in which many sign languages take advantage of the spatial nature of the language is through the use of classifiers. Classifiers allow a signer to spatially show a referent's type, size, shape, movement, or extent.

The large focus on the possibility of simultaneity in sign languages in contrast to spoken languages is sometimes exaggerated, though. The use of two manual articulators is subject to motor constraints, resulting in a large extent of symmetry or signing with one articulator only. Further, sign languages, just like spoken languages, depend on linear sequencing of signs to form sentences; the greater use of simultaneity is mostly seen in the morphology (internal structure of individual signs).

Sign languages convey much of their prosody through non-manual elements. Postures or movements of the body, head, eyebrows, eyes, cheeks, and mouth are used in various combinations to show several categories of information, including lexical distinction, grammatical structure, adjectival or adverbial content, and discourse functions.

At the lexical level, signs can be lexically specified for non-manual elements in addition to the manual articulation. For instance, facial expressions may accompany verbs of emotion, as in the sign for "angry" in Czech Sign Language. Non-manual elements may also be lexically contrastive. For example, in ASL (American Sign Language), facial components distinguish some signs from other signs. An example is the sign translated as "not yet", which requires that the tongue touch the lower lip and that the head rotate from side to side, in addition to the manual part of the sign. Without these features the sign would be interpreted as "late". Mouthings, which are (parts of) spoken words accompanying lexical signs, can also be contrastive, as in the manually identical signs for "doctor" and "battery" in Sign Language of the Netherlands.

While the content of a signed sentence is produced manually, many grammatical functions are produced non-manually (i.e., with the face and the torso). Such functions include questions, negation, relative clauses and topicalization. ASL and BSL use similar non-manual marking for yes/no questions, for example. They are shown through raised eyebrows and a forward head tilt.

Some adjectival and adverbial information is conveyed through non-manual elements, but what these elements are varies from language to language. For instance, in ASL a slightly open mouth with the tongue relaxed and visible in the corner of the mouth means 'carelessly', but a similar non-manual in BSL means 'boring' or 'unpleasant'.

Discourse functions such as turn taking are largely regulated through head movement and eye gaze. Since the addressee in a signed conversation must be watching the signer, a signer can avoid letting the other person have a turn by not looking at them, or can indicate that the other person may have a turn by making eye contact.

Iconicity is similarity or analogy between the form of a sign (linguistic or otherwise) and its meaning, as opposed to arbitrariness. The first studies on iconicity in ASL were published in the late 1970s, and early 1980s. Many early sign language linguists rejected the notion that iconicity was an important aspect of the language. Though they recognized that certain aspects of the language seemed iconic, they considered this to be merely extralinguistic, a property which did not influence the language. However, mimetic aspects of sign language (signs that imitate, mimic, or represent) are found in abundance across a wide variety of sign languages. For example, deaf children learning sign language try to express something but do not know the associated sign, they will often invent an iconic sign that displays mimetic properties. Though it never disappears from a particular sign language, iconicity is gradually weakened as forms of sign languages become more customary and are subsequently grammaticized. As a form becomes more conventional, it becomes disseminated in a methodical way phonologically to the rest of the sign language community. Frishberg (1975) wrote a very influential paper addressing the relationship between arbitrariness and iconicity in ASL. She concluded that though originally present in many signs, iconicity is degraded over time through the application of grammatical processes. In other words, over time, the natural processes of regularization in the language obscures any iconically motivated features of the sign.

In 1978, Psychologist Roger Brown was one of the first to suggest that the properties of ASL give it a clear advantage in terms of learning and memory. In his study, Brown found that when a group of six hearing children were taught signs that had high levels of iconic mapping they were significantly more likely to recall the signs in a later memory task than another group of six children that were taught signs that had little or no iconic properties. In contrast to Brown, linguists Elissa Newport and Richard Meier found that iconicity "appears to have virtually no impact on the acquisition of American Sign Language".

A central task for the pioneers of sign language linguistics was trying to prove that ASL was a real language and not merely a collection of gestures or "English on the hands." One of the prevailing beliefs at this time was that 'real languages' must consist of an arbitrary relationship between form and meaning. Thus, if ASL consisted of signs that had iconic form-meaning relationship, it could not be considered a real language. As a result, iconicity as a whole was largely neglected in research of sign languages.

The cognitive linguistics perspective rejects a more traditional definition of iconicity as a relationship between linguistic form and a concrete, real-world referent. Rather it is a set of selected correspondences between the form and meaning of a sign. In this view, iconicity is grounded in a language user's mental representation ("construal" in cognitive grammar). It is defined as a fully grammatical and central aspect of a sign language rather than a peripheral phenomenon.

The cognitive linguistics perspective allows for some signs to be fully iconic or partially iconic given the number of correspondences between the possible parameters of form and meaning. In this way, the Israeli Sign Language (ISL) sign for "ask" has parts of its form that are iconic ("movement away from the mouth" means "something coming from the mouth"), and parts that are arbitrary (the handshape, and the orientation).

Many signs have metaphoric mappings as well as iconic or metonymic ones. For these signs there are three way correspondences between a form, a concrete source and an abstract target meaning. The ASL sign LEARN has this three way correspondence. The abstract target meaning is "learning". The concrete source is putting objects into the head from books. The form is a grasping hand moving from an open palm to the forehead. The iconic correspondence is between form and concrete source. The metaphorical correspondence is between concrete source and abstract target meaning. Because the concrete source is connected to two correspondences linguistics refer to metaphorical signs as "double mapped".
Although sign languages have emerged naturally in deaf communities alongside or among spoken languages, they are unrelated to spoken languages and have different grammatical structures at their core.

Sign languages may be classified by how they arise.

In non-signing communities, home sign is not a full language, but closer to a pidgin. Home sign is amorphous and generally idiosyncratic to a particular family, where a deaf child does not have contact with other deaf children and is not educated in sign. Such systems are not generally passed on from one generation to the next. Where they are passed on, creolization would be expected to occur, resulting in a full language. However, home sign may also be closer to full language in communities where the hearing population has a gestural mode of language; examples include various Australian Aboriginal sign languages and gestural systems across West Africa, such as Mofu-Gudur in Cameroon.

A village sign language is a local indigenous language that typically arises over several generations in a relatively insular community with a high incidence of deafness, and is used both by the deaf and by a significant portion of the hearing community, who have deaf family and friends. The most famous of these is probably the extinct Martha's Vineyard Sign Language of the US, but there are also numerous village languages scattered throughout Africa, Asia, and America.

Deaf-community sign languages, on the other hand, arise where deaf people come together to form their own communities. These include school sign, such as Nicaraguan Sign Language, which develop in the student bodies of deaf schools which do not use sign as a language of instruction, as well as community languages such as Bamako Sign Language, which arise where generally uneducated deaf people congregate in urban centers for employment. At first, Deaf-community sign languages are not generally known by the hearing population, in many cases not even by close family members. However, they may grow, in some cases becoming a language of instruction and receiving official recognition, as in the case of ASL.

Both contrast with speech-taboo languages such as the various Aboriginal Australian sign languages, which are developed by the hearing community and only used secondarily by the deaf. It is doubtful whether most of these are languages in their own right, rather than manual codes of spoken languages, though a few such as Yolngu Sign Language are independent of any particular spoken language. Hearing people may also develop sign to communicate with speakers of other languages, as in Plains Indian Sign Language; this was a contact signing system or pidgin that was evidently not used by deaf people in the Plains nations, though it presumably influenced home sign.

Language contact and creolization is common in the development of sign languages, making clear family classifications difficult – it is often unclear whether lexical similarity is due to borrowing or a common parent language, or whether there was one or several parent languages, such as several village languages merging into a Deaf-community language. Contact occurs between sign languages, between sign and spoken languages (contact sign, a kind of pidgin), and between sign languages and gestural systems used by the broader community. One author has speculated that Adamorobe Sign Language, a village sign language of Ghana, may be related to the "gestural trade jargon used in the markets throughout West Africa", in vocabulary and areal features including prosody and phonetics.

The only comprehensive classification along these lines going beyond a simple listing of languages dates back to 1991. The classification is based on the 69 sign languages from the 1988 edition of Ethnologue that were known at the time of the 1989 conference on sign languages in Montreal and 11 more languages the author added after the conference.

In his classification, the author distinguishes between primary and auxiliary sign languages as well as between single languages and names that are thought to refer to more than one language. The prototype-A class of languages includes all those sign languages that seemingly cannot be derived from any other language. Prototype-R languages are languages that are remotely modelled on a prototype-A language (in many cases thought to have been French Sign Language) by a process Kroeber (1940) called "stimulus diffusion". The families of BSL, DGS, JSL, LSF (and possibly LSG) were the products of creolization and relexification of prototype languages. Creolization is seen as enriching overt morphology in sign languages, as compared to reducing overt morphology in spoken languages.

Linguistic typology (going back to Edward Sapir) is based on word structure and distinguishes morphological classes such as agglutinating/concatenating, inflectional, polysynthetic, incorporating, and isolating ones.

Sign languages vary in word-order typology. For example, Austrian Sign Language, Japanese Sign Language and Indo-Pakistani Sign Language are Subject-object-verb while ASL is Subject-verb-object. Influence from the surrounding spoken languages is not improbable.

Sign languages tend to be incorporating classifier languages, where a classifier handshape representing the object is incorporated into those transitive verbs which allow such modification. For a similar group of intransitive verbs (especially motion verbs), it is the subject which is incorporated. Only in a very few sign languages (for instance Japanese Sign Language) are agents ever incorporated. in this way, since subjects of intransitives are treated similarly to objects of transitives, incorporation in sign languages can be said to follow an ergative pattern.

Brentari classifies sign languages as a whole group determined by the medium of communication (visual instead of auditory) as one group with the features monosyllabic and polymorphemic. That means, that one syllable (i.e. one word, one sign) can express several morphemes, e.g., subject and object of a verb determine the direction of the verb's movement (inflection).

Another aspect of typology that has been studied in sign languages is their systems for cardinal numbers. Typologically significant differences have been found between sign languages.

Children who are exposed to a sign language from birth will acquire it, just as hearing children acquire their native spoken language.

The Critical Period hypothesis suggests that language, spoken or signed, is more easily acquired as a child at a young age versus an adult because of the plasticity of the child's brain. In a study done at the University of McGill, they found that American Sign Language users who acquired the language natively (from birth) performed better when asked to copy videos of ASL sentences than ASL users who acquired the language later in life. They also found that there are differences in the grammatical morphology of ASL sentences between the two groups, all suggesting that there is a very important critical period in learning signed languages.

The acquisition of non-manual features follows an interesting pattern: When a word that always has a particular non-manual feature associated with it (such as a wh- question word) is learned, the non-manual aspects are attached to the word but don't have the flexibility associated with adult use. At a certain point, the non-manual features are dropped and the word is produced with no facial expression. After a few months, the non-manuals reappear, this time being used the way adult signers would use them.

Sign languages do not have a traditional or formal written form. Many deaf people do not see a need to write their own language.

Several ways to represent sign languages in written form have been developed.


So far, there is no consensus regarding the written form of sign language. Except for SignWriting, none are widely used. Maria Galea writes that SignWriting "is becoming widespread, uncontainable and untraceable. In the same way that works written in and about a well developed writing system such as the Latin script, the time has arrived where SW is so widespread, that it is impossible in the same way to list all works that have been produced using this writing system and that have been written about this writing system." In 2015, the Federal University of Santa Catarina accepted a dissertation written in Brazilian Sign Language using Sutton SignWriting for a master's degree in linguistics. The dissertation "The Writing of Grammatical Non-Manual Expressions in Sentences in LIBRAS Using the SignWriting System" by João Paulo Ampessan states that "the data indicate the need for [non-manual expressions] usage in writing sign language".

For a native signer, sign perception influences how the mind makes sense of their visual language experience. For example, a handshape may vary based on the other signs made before or after it, but these variations are arranged in perceptual categories during its development. The mind detects handshape contrasts but groups similar handshapes together in one category. Different handshapes are stored in other categories. The mind ignores some of the similarities between different perceptual categories, at the same time preserving the visual information within each perceptual category of handshape variation.

When Deaf people constitute a relatively small proportion of the general population, Deaf communities often develop that are distinct from the surrounding hearing community.
These Deaf communities are very widespread in the world, associated especially with sign languages used in urban areas and throughout a nation, and the cultures they have developed are very rich.

One example of sign language variation in the Deaf community is Black ASL. This sign language was developed in the Black Deaf community as a variant during the American era of segregation and racism, where young Black Deaf students were forced to attend separate schools than their white Deaf peers.

On occasion, where the prevalence of deaf people is high enough, a deaf sign language has been taken up by an entire local community, forming what is sometimes called a "village sign language" or "shared signing community". Typically this happens in small, tightly integrated communities with a closed gene pool. Famous examples include:
In such communities deaf people are generally well integrated in the general community and not socially disadvantaged, 
so much so that it is difficult to speak of a separate "Deaf" community.

Many Australian Aboriginal sign languages arose in a context of extensive speech taboos, such as during mourning and initiation rites. They are or were especially highly developed among the Warlpiri, Warumungu, Dieri, Kaytetye, Arrernte, and Warlmanpa, and are based on their respective spoken languages.

A pidgin sign language arose among tribes of American Indians in the Great Plains region of North America (see Plains Indian Sign Language). It was used by hearing people to communicate among tribes with different spoken languages, as well as by deaf people. There are especially users today among the Crow, Cheyenne, and Arapaho. Unlike Australian Aboriginal sign languages, it shares the spatial grammar of deaf sign languages.
In the 1500s, a Spanish expeditionary, Cabeza de Vaca, observed natives in the western part of modern-day Florida using sign language, and in the mid-16th century Coronado mentioned that communication with the Tonkawa using signs was possible without a translator. Whether or not these gesture systems reached the stage at which they could properly be called languages is still up for debate. There are estimates indicating that as many as 2% of Native Americans are seriously or completely deaf, a rate more than twice the national average.

Signs may also be used by hearing people for manual communication in secret situations, such as hunting, in noisy environments, underwater, through windows or at a distance.

Some sign languages have obtained some form of legal recognition, while others have no status at all. Sarah Batterbury has argued that sign languages should be recognized and supported not merely as an accommodation for the disabled, but as the communication medium of language communities.

One of the first demonstrations of the ability for telecommunications to help sign language users communicate with each other occurred when AT&T's videophone (trademarked as the "Picturephone") was introduced to the public at the 1964 New York World's Fair – two deaf users were able to freely communicate with each other between the fair and another city. However, video communication did not become widely available until sufficient bandwidth for the high volume of video data became available in the early 2000s.

The Internet now allows deaf people to talk via a video link, either with a special-purpose videophone designed for use with sign language or with "off-the-shelf" video services designed for use with broadband and an ordinary computer webcam. The special videophones that are designed for sign language communication may provide better quality than 'off-the-shelf' services and may use data compression methods specifically designed to maximize the intelligibility of sign languages. Some advanced equipment enables a person to remotely control the other person's video camera, in order to zoom in and out or to point the camera better to understand the signing.

In order to facilitate communication between deaf and hearing people, sign language interpreters are often used. Such activities involve considerable effort on the part of the interpreter, since sign languages are distinct natural languages with their own syntax, different from any spoken language.

The interpretation flow is normally between a sign language and a spoken language that are customarily used in the same country, such as French Sign Language (LSF) and spoken French in France, Spanish Sign Language (LSE) to spoken Spanish in Spain, British Sign Language (BSL) and spoken English in the U.K., and American Sign Language (ASL) and spoken English in the US and most of anglophone Canada (since BSL and ASL are distinct sign languages both used in English-speaking countries), etc. Sign language interpreters who can translate between signed and spoken languages that are not normally paired (such as between LSE and English), are also available, albeit less frequently.

With recent developments in artificial intelligence in computer science, some recent deep learning based machine translation algorithms have been developed which automatically translate short videos containing sign language sentences (often simple sentence consists of only one clause) directly to written language. 

Interpreters may be physically present with both parties to the conversation but, since the technological advancements in the early 2000s, provision of interpreters in remote locations has become available. In video remote interpreting (VRI), the two clients (a sign language user and a hearing person who wish to communicate with each other) are in one location, and the interpreter is in another. The interpreter communicates with the sign language user via a video telecommunications link, and with the hearing person by an audio link. VRI can be used for situations in which no on-site interpreters are available.

However, VRI cannot be used for situations in which all parties are speaking via telephone alone. With video relay service (VRS), the sign language user, the interpreter, and the hearing person are in three separate locations, thus allowing the two clients to talk to each other on the phone through the interpreter.
Sign language is sometimes provided for television programmes. The signer usually appears in the bottom corner of the screen, with the programme being broadcast full size or slightly shrunk away from that corner. Typically for press conferences such as those given by the Mayor of New York City, the signer appears to stage left or right of the public official to allow both the speaker and signer to be in frame at the same time.

Paddy Ladd initiated deaf programming on British television in the 1980s and is credited with getting sign language on television and enabling deaf children to be educated in sign.

In traditional analogue broadcasting, many programmes are repeated, often in the early hours of the morning, with the signer present rather than have them appear at the main broadcast time. This is due to the distraction they cause to those not wishing to see the signer. On the BBC, many programmes that broadcast late at night or early in the morning are signed. Some emerging television technologies allow the viewer to turn the signer on and off in a similar manner to subtitles and closed captioning.

Legal requirements covering sign language on television vary from country to country. In the United Kingdom, the Broadcasting Act 1996 addressed the requirements for blind and deaf viewers, but has since been replaced by the Communications Act 2003.

As with any spoken language, sign languages are also vulnerable to becoming endangered. For example, a sign language used by a small community may be endangered and even abandoned as users shift to a sign language used by a larger community, as has happened with Hawai'i Sign Language, which is almost extinct except for a few elderly signers. Even national sign languages can be endangered; for example, New Zealand Sign Language is losing users. Methods are being developed to assess the language vitality of sign languages.

There are a number of communication systems that are similar in some respects to sign languages, while not having all the characteristics of a full sign language, particularly its grammatical structure. Many of these are either precursors to natural sign languages or are derived from them.

When Deaf and Hearing people interact, signing systems may be developed that use signs drawn from a natural sign language but used according to the grammar of the spoken language. In particular, when people devise one-for-one sign-for-word correspondences between spoken words (or even morphemes) and signs that represent them, the system that results is a manual code for a spoken language, rather than a natural sign language. Such systems may be invented in an attempt to help teach Deaf children the spoken language, and generally are not used outside an educational context.

Some hearing parents teach signs to young hearing children. Since the muscles in babies' hands grow and develop quicker than their mouths, signs are seen as a beneficial option for better communication. Babies can usually produce signs before they can speak. This reduces the confusion between parents when trying to figure out what their child wants. When the child begins to speak, signing is usually abandoned, so the child does not progress to acquiring the grammar of the sign language.

This is in contrast to hearing children who grow up with Deaf parents, who generally acquire the full sign language natively, the same as Deaf children of Deaf parents.

Informal, rudimentary sign systems are sometimes developed within a single family. For instance, when hearing parents with no sign language skills have a deaf child, the child may develop a system of signs naturally, unless repressed by the parents. The term for these mini-languages is home sign (sometimes "kitchen sign").

Home sign arises due to the absence of any other way to communicate. Within the span of a single lifetime and without the support or feedback of a community, the child naturally invents signs to help meet his or her communication needs, and may even develop a few grammatical rules for combining short sequences of signs. Still, this kind of system is inadequate for the intellectual development of a child and it comes nowhere near meeting the standards linguists use to describe a complete language. No type of home sign is recognized as a full language.

There have been several notable examples of scientists teaching signs to non-human primates in order to communicate with humans, such as 
chimpanzees, gorillas and 
orangutans. However, linguists generally point out that this does not constitute knowledge of a human "language" as a complete system, rather than simply signs / words. Notable examples of animals who have learned signs include:

One theory of the evolution of human language states that it developed first as a gestural system, which later shifted to speech. An important question for this gestural theory is what caused the shift to vocalization.




"Note: the articles for specific sign languages (e.g. ASL or BSL) may contain further external links, e.g. for learning those languages."


</doc>
<doc id="15220" url="https://en.wikipedia.org/wiki?curid=15220" title="Imprecise language">
Imprecise language

Often, informal, spoken language, "everyday language" is less precise than any more formal or academic languages.

Language might be said to be imprecise because it exhibits one or more of the following features:

While imprecise language is not desirable in various scientific fields, it may be helpful, illustrative or discussion-stimulative in other contexts. Imprecision in a discourse may or may not be the intention of the author(s) or speaker(s). The role of imprecision may depend on audience, end goal, extended context and subject matter. Relevant players and real stakes will also bear on truth-grounds of statements.


</doc>
<doc id="32017750" url="https://en.wikipedia.org/wiki?curid=32017750" title="Languaculture">
Languaculture

Languaculture is a term meaning that a language includes not only elements such as grammar and vocabulary, but also past knowledge, local and cultural information, habits and behaviours. The term was created by the American anthropologist Michael Agar.

Agar used the term "languaculture" for the first time in his book "Language Shock: Understanding the culture of conversation". Languaculture is a supposed improvement on the term "linguaculture" coined by the American linguistic anthropologist Paul Friedrich. Agar explains the change stating that "language" is a more commonly used word in English. It seems that "linguaculture" is getting more common again (cf. Risager 2012).

When Agar talks about languaculture, he defines it as the necessary tie between language and culture. He underlines that languages and cultures are always closely related and it is not possible to distinguish languages from cultures. Therefore, you cannot really know a language if you do not know also the culture expressed by that language.

The notion of culture and its understanding involve the link between two different languacultures that Agar define LC1 (source languaculture) and LC2 (target languaculture).

The learning of target languaculture is driven by "rich points". We realize that a culture is different from ours when we face some behaviours which we do not understand. Rich points are those surprises, those departures from an outsider's expectations that signal a difference between source languaculture and target languaculture. They are the moments of incomprehension, when you suddenly do not know what is happening. In this situation different reactions are possible. You can ignore the rich point and hope that the next part makes sense. You can perceive it as evidence that the person who produced it has some lacks. Or you can wonder why you do not understand and if maybe some other languaculture comes into play. Therefore, rich points belong to daily life and not only to language. Agar highlights that the term "rich" has the positive connotations of thickness, wealth and abundance. The largest rich point is the total incomprehension due to huge differences between source languaculture and target languaculture. In this case we are facing a ‘culture shock’ that causes a deep bewilderment. The smallest rich point can occur among different groups of the same community.
The existence of rich points comes from the fact that every statement implicitly refers to various elements that are taken for granted in a certain culture and do not match the elements of another culture (cultural implicitness).

According to Agar, culture is a construction, a translation between source languaculture and target languaculture. Like a translation, it makes no sense to talk about the culture of X without saying the culture of X for Y, taking into account the standpoint from which it is observed. For this reason culture is relational.
Moreover, culture is always plural. No person or group can be described, explained or generalized completely with a single cultural label.



</doc>
<doc id="604771" url="https://en.wikipedia.org/wiki?curid=604771" title="Ergative–absolutive language">
Ergative–absolutive language

Ergative–absolutive languages, or ergative languages, are languages that share a certain distinctive pattern relating to the subjects (technically, arguments) of verbs. Examples are Basque, Georgian, Mayan, Tibetan, a few Indo-European languages (such as the Kurdish languages and Hindi) and, to some degree, the Semitic modern Aramaic languages. 

In an ergative language, the single argument ("subject") of an intransitive verb behaves like the object of a transitive verb, and differently from the agent of a transitive verb.

That is in contrast to nominative–accusative languages, such as English and most other Indo-European languages, where the single argument of an intransitive verb ("She" in the sentence "She walks.") behaves grammatically like the agent of a transitive verb ("She" in the sentence "She finds it.") but differently from the object of a transitive verb ("her" in the sentence "He likes her."). In ergative–absolutive languages with grammatical case, the case used for the single argument of an intransitive verb and the object of a transitive verb is the absolutive, and the case used for the agent of a transitive verb is the ergative. In nominative–accusative languages, the case for the single argument of an intransitive verb and the agent of a transitive verb is the nominative while the case for the direct object of a transitive verb is the accusative. 

There is a variant group, the ergative–accusative languages, otherwise known as split ergative languages, (such as Dyirbal), which functions ergatively with respect to nouns but is nominative-accusative with pronouns.

Several scholars have hypothesized that Proto-Indo-European was an ergative language. However, this hypothesis is disputed.

An ergative language maintains a syntactic or morphological equivalence (such as the same word order or grammatical case) for the object of a transitive verb and the single core argument of an intransitive verb, while treating the agent of a transitive verb differently.

This contrasts with nominative–accusative languages such as English, where the single argument of an intransitive verb and the agent of a transitive verb (both called the subject) are treated alike and kept distinct from the object of a transitive verb.

These different arguments are usually symbolized as follows:

The relationship between ergative and accusative systems can be schematically represented as the following:

"See morphosyntactic alignment for a more technical explanation and a comparison with nominative–accusative languages."

Note that the word "subject", as it is typically defined in grammars of nominative–accusative languages, has a different application when referring to ergative–absolutive languages, or when discussing morphosyntactic alignment in general.

Ergative languages tend to be either verb-final or verb-initial; there are few, if any, ergative SVO-languages.

Ergativity can be found in both morphological and syntactic behavior.

If the language has morphological case, then the verb arguments are marked thus:

If there is no case marking, ergativity can be marked through other means, such as in verbal morphology. For instance, Abkhaz and most Mayan languages have no morphological ergative case, but they have a verbal agreement structure that is ergative. In languages with ergative–absolutive agreement systems, the absolutive form is usually the most unmarked form of a word (exceptions include Nias and Tlapanec).

The following examples from Basque demonstrate an ergative–absolutive case marking system:

Here "-Ø" represents a zero morpheme, as the absolutive case is unmarked in Basque. The forms for the ergative are "-k" after a vowel, and "-ek" after a consonant. It is a further rule in Basque grammar that in most cases a noun phrase must be closed by a determiner. The default determiner (commonly called the article, which is suffixed to common nouns and usually translatable by "the" in English) is "-a" in the singular and "-ak" in the plural, the plural being marked only on the determiner and never the noun. For common nouns, this default determiner is fused with the ergative case marker. Thus one obtains the following forms for "gizon" ("man" in English): gizon-a (man-the.sing.abs), gizon-ak (man-the.pl.abs), gizon-ak (man-the.sing.erg), gizon-ek (man-the.pl.erg). Note that when fused with the article, the absolutive plural is homophonous with the ergative singular. See Basque grammar for details.

In contrast, Japanese is a nominative–accusative language:

In this language, the argument of the intransitive and agent of the transitive sentence are marked with the same nominative case particle "ga", while the object of the transitive sentence is marked with the accusative case o.

If one sets: A = agent of a transitive verb; S = argument of an intransitive verb; O = object of a transitive verb, then we can contrast normal nominative–accusative English with a hypothetical ergative English:

Accusative English:

Hypothetical ergative English:

A number of languages have both ergative and accusative morphology. A typical example is a language that has nominative–accusative marking on verbs and ergative–absolutive case marking on nouns.

Georgian also has an ergative alignment, but the agent is only marked with the ergative case in the perfective aspect (also known as the "aorist screeve"). Compare:

However, there are some intransitive verbs in Georgian that behave like transitive verbs, and therefore employ the ergative case in the past tense. Consider:

Although the verb "sneeze" is clearly intransitive, it is conjugated like a transitive verb. In Georgian there are a few verbs like these, and there has not been a clear-cut explanation as to why these verbs have evolved this way. One explanation is that verbs such as "sneeze" used to have a direct object (the object being "nose" in the case of "sneeze") and over time lost these objects, yet kept their transitive behavior.

Ergativity may be manifested through syntax, such as saying “Arrived I” for “I arrived”, in addition to morphology. Syntactic ergativity is quite rare, and while all languages that exhibit it also feature morphological ergativity, few morphologically ergative languages have ergative syntax. As with morphology, syntactic ergativity can be placed on a continuum, whereby certain syntactic operations may pattern accusatively and others ergatively. The degree of syntactic ergativity is then dependent on the number of syntactic operations that treat the subject like the object. Syntactic ergativity is also referred to as inter-clausal ergativity, as it typically appears in the relation of two clauses.

Syntactic ergativity may appear in:

Example of syntactic ergativity in the "conjunction reduction" construction (coordinated clauses) in Dyirbal in contrast with English conjunction reduction. (The subscript (i) indicates coreference.)

English (SVO word order):

Dyirbal (OSV word order):

The term "ergative–absolutive" is considered unsatisfactory by some, since there are very few languages without any patterns that exhibit nominative–accusative alignment. Instead they posit that one should only speak of "ergative–absolutive systems", which languages employ to different degrees.

Many languages classified as ergative in fact show split ergativity, whereby syntactic and/or morphological ergative patterns are conditioned by the grammatical context, typically person or the tense/aspect of the verb. Basque is unusual in having an almost fully ergative system in case-marking and verbal agreement, though it shows thoroughly nominative–accusative syntactic alignment.

In Urdu and Hindi, the ergative case is marked on agents in the preterite and perfect tenses for transitive and ditransitive verbs, while in other situations agents appear in the nominative case.

In the Northern Kurdish language Kurmanji, the ergative case is marked on agents and verbs of transitive verbs in past tenses, for the events actually occurred in the past. Present, future and "future in the past" tenses show no ergative mark neither for agents nor the verbs. For example:

but:

In sentences (1) to (4), there is no ergativity (transitive and intransitive verbs alike). In sentences (6) and (8), the ergative case is marked on agents and verbs. 

In Dyirbal, pronouns are morphologically nominative–accusative when the agent is first or second person, but ergative when the agent is a third person.

Many languages with ergative marking display what is known as "optional ergativity", where the ergative marking is not always expressed in all situations. McGregor (2010) gives a range of contexts when we often see optional ergativity, and argues that the choice is often not truly "optional" but is affected by semantics and pragmatics. Note that unlike split ergativity, which occurs regularly but in limited locations, optional ergativity can occur in a range of environments, but may not be used in a way that appears regular or consistent.

Optional ergativity may be motivated by:

Languages from Australia, New Guinea and Tibet have been shown to have optional ergativity.

Prototypical ergative languages are, for the most part, restricted to specific regions of world: the Mesopotamia (Kurdish, and some extinct languages), Caucasus, the Americas, the Tibetan Plateau, and Australia and parts of New Guinea.

Some specific languages and language families are the following:

Americas

Africa

Asia

Australian
Certain Australian Aboriginal languages (e.g., Wangkumara) possess an intransitive case and an accusative case along with an ergative case, and lack an absolutive case; such languages are called tripartite languages or ergative–accusative languages.

Papua


Europe

Caucasus and Near East

Some languages have limited ergativity


Sign languages (for example, Nepali Sign Language) should also generally be considered ergative in the patterning of actant incorporation in verbs. In sign languages that have been studied, classifier handshapes are incorporated into verbs, indicating the subject of intransitive verbs when incorporated, and the object of transitive verbs. (If we follow the "semantic phonology" model proposed by William Stokoe (1991) this ergative-absolutive patterning also works at the level of the lexicon: thus in Nepali Sign Language the sign for TEA has the motion for the verb DRINK with a manual alphabet handshape च /ca/ (standing for the first letter of the Nepali word TEA चिया /chiya:/) being incorporated as the object.)

English has derivational morphology that parallels ergativity in that it operates on intransitive verbs and objects of transitive verbs. With certain intransitive verbs, adding the suffix "-ee" to the verb produces a label for the person performing the action:

However, with a transitive verb, adding "-ee" does not produce a label for the person doing the action. Instead, it gives us a label for the person to whom the action is done:

Etymologically, the sense in which "-ee" denotes the object of a transitive verb is the original one, arising from French past participles in "-é". This is still the prevalent sense in British English: the intransitive uses are all 19th-century American coinages and all except "escapee" are still marked as "chiefly U.S." by the "Oxford English Dictionary".

English also has a number of so-called ergative verbs, where the object of the verb when transitive is equivalent to the subject of the verb when intransitive.

When English nominalizes a clause, the underlying subject of an intransitive verb and the underlying object of a transitive verb are both marked with the possessive case or with the preposition "of" (the choice depends on the type and length of the noun: pronouns and short nouns are typically marked with the possessive, while long and complex NPs are marked with "of"). The underlying subject of a transitive is marked differently (typically with "by" as in a passive construction):





</doc>
<doc id="47665171" url="https://en.wikipedia.org/wiki?curid=47665171" title="Feminist language reform">
Feminist language reform

Feminist language reform or feminist language planning refers to the effort, often of political and grassroots movements, to change how language is used to gender people, activities and ideas on an individual and societal level. This initiative has been adopted in countries such as Sweden, Switzerland and Australia, and has been tentatively linked to higher gender equality.

Linguistic activism and feminist authorship stemming from second wave feminism in the 1960s and 70s began to draw attention to gender bias in language, including "the uncovering of the gendered nature of many linguistic rules and norms". Scholarship such as Dennis Baron's "Grammar and Gender" and Anne Bodine's "Androcentrism in Prescriptive Grammar" uncovered historical male regulation to promote male-centric language such as the use of "he" as a generic pronoun.

Exposition and analysis of sexism in language through a grassroots feminist linguistics movement continued throughout the 80's and 90's, including study across languages and speech communities such as Germany and France. Study and documentation of gendered language has since spread to cover over 30 languages.

Feminist language planning has more recently been instituted centrally in countries such as Sweden, Switzerland and Australia, with mixed results.

Sweden have made strides towards shifting their language to fit a less misogynistic society. In the Swedish language, there has never been a word for the female genitalia or even a translation of the word “vagina”, even though the word "snopp" translates to “penis” and has been used as such since the 1960s. Through history, there have been many slang terms used for the woman's genitalia, including words such as "fitta" translated to “cunt”, "där nere" translated to “down-there”, and even "mus" translated to “mouse”. In the 1990s, Swedish media started to bring the absence of such a word to light. It wasn't until the early 2000s did the feminists and activists start using the word "snippa" to be identified with the female genitalia. "Snippa"’s origins can be traced back to many different Swedish dialects. Its popular definition “refers to something small and/or narrow, for example a small pike or a narrow boat”. In regards to genitalia, “it might have been used to refer to female genitalia of cows and pigs in the early twentieth century”. Since the popularization of using the word "Snippa," the Swedish Academy added the word to the 2006 Swedish Language Dictionary.

Some language reformers directly work with identifying and changing sexist undertones and patriarchal vocabulary through a method called “linguistic disruption”. An example: In the United States, the word “herstory” became popularized “to refer to history which is not only about men”. 

Sweden has also shown efforts in language planning regarding changing misogynistic undertones in their vocabulary. The Swedish Association for Sexuality Education has promoted the word "slidkrans" to replace the word for “hymen”, "mödomshinna." “The new word, "slidkrans", is made up of the two parts "slid", translating to “vaginal” and "krans", translating to “garland”. It lacks the connotations of the ideology of virginity and honour attached to mödomshinna.”
The gender-neutral pronoun "hen" was originally promoted by feminists and the LGBT community. Controversial at the outset, it has gained wide acceptance in Sweden, is used in schools, and recently was added to dictionaries.

Australia has been identified as a nation that officially promotes the feminist influence to its public bureaucracy by implementing feminist language reform across many institutions. Since this planned social shift, Australia has seen changes in political and government leadership that aim to interfere with this reform, such as a shift towards a conservative-leaning government. There are shifts that come from such movements that support them as well, such as the gender-neutral pronoun “they” being more widely accepted.

The ongoing feminist movement acknowledges language as a “powerful instrument of patriarchy”. The goals set for linguistic reform aim to achieve linguistic equality of the sexes. A study of Australian newspapers from 1992 and 1996 found that the word “chairman” was used to describe all people holding the position, including women. This is an example of a linguistic issue that feminists seek to reform. Occupational nomenclature reflects gender bias when “professional nomenclature used in employment-related contexts displays bias in favour of men leading to women's invisibility in this area.” The invisibility of women is a linguistic feminist issue because when encountering sentences predominantly using male pronouns, listeners are more likely to think of men before women and therefore women get overlooked. Positions are gendered to be male and the “continuing, frequent use reflects the fact that far more men than women continue to occupy this position.” This study further investigated and found instances of female professionals being specified as women while men would just be titled with the profession itself, for example “female judge,” “woman engineer,” and “woman politician.”

Switzerland has attempted to implement feminist language reform both formally and informally. However, changes in Switzerland have proven to be complicated due to the fact that Switzerland is a multilingual country (with the major languages being German, French, and Italian). The Bulletin Suisse de Linguistique Appliquée (Swiss Bulletin of Applied Linguistics) addressed this issue in 2000 when it created a special issue dedicated to the feminization of language in Switzerland. The bulletin attempted to critique language in Switzerland by creating a composite image of all the languages in Switzerland and how they interact with gender.

The most commonly spoken language in Switzerland is German. German is a gendered language. This has concerned some language activists due to the fact that many important societal positions such as judge and professor possess the gender of male and are often referred to as he/him. Activists worry that the gendering of those words discourages women from entering those fields. This facet of the German language is particularly important in Switzerland because it was historically used as a justification to restrict women's right to vote and pass the bar.

Various attempts to implement feminist language reform have been undertaken in German-speaking Switzerland. The government and other organizations have attempted to implement language feminization in the realms of policy making, teaching, advertising, etc. Language feminization refers to when in writing or talking traditional male words are feminized by either using the feminine variant of the word or adding a feminine suffix. However, these attempts have had only limited success. For example, private Swiss radio and television broadcasts still generally use the generic-masculine form of words.

The second most commonly spoken language in Switzerland is French which is also a gendered language. The French language raises similar concerns to that of the German language. This is because many nouns (especially those of professions) are gendered. To address these concerns, the Swiss government has created a guide on the non-sexist use of the French language. However, these attempts at change have been met with little success. This is due to the fact that Switzerland has limited influence over the French language. Meanwhile, France and specifically the government backed Académie française (French Academy) (the French council for matters relating to the French language) has resisted feminist language reform.

The main focus of feminist language reform is to acknowledge the often unconscious ways that language both silences and emphasizes gender in negative ways. In some languages it is clear with gendered nouns how some words are gendered to associate those words with maleness of femaleness. Feminist Philosophers argue that English, a non gendered language, still has the need for Language Reform.

Previous language reform attempts to avoid sexist words or phrases were addressed in a symptomatic manner. Often in the workplace, employees were given pamphlets with lists of words to avoid or preferred words to use. Many modern day feminists argue that this is ineffective because it does not address the root of the problem or make the large scale changes to the language that they feel are necessary.

A major part of the theory focuses on when words or phrases make one gender, typically women, subjugated or invisible compared to the other. The most popular examples are the pronoun “he” or the word “man”. Feminist language philosophers argue that these words participate in making women invisible by having them being used to refer to men and also women. The fact that the pronouns or words for the male gender can be also used to refer to the female gender shows how maleness is dominant and femaleness is subjugated.

Feminist language theory also focuses on when words or phrases emphasize a break in gender norms. Clear examples of this are words like Lady Doctor or Manageress. These are positions of power that are typically held by men. Therefore, when a woman holds them, they need a new title to emphasize their break of social norm. It also goes both ways, with terms like male nurse referring to a man in a typically feminine role. Feminist language reform seeks to remove words like this because they help to sustain unhealthy gender norms.

Some modern feminists, like Sergio Bolaños Cuellar, argue that feminist language reforms needs to reverse the generic masculine forms and create a generic feminine form with words like he or man being replaced with she or woman.

Cases of feminist language planning have taken a largely sociolinguistic approach in which the goal is to enact social change through the reform of language and language use. This approach to language planning is divided into four stages:





</doc>
<doc id="25429108" url="https://en.wikipedia.org/wiki?curid=25429108" title="Angelic tongues">
Angelic tongues

Angelic tongues are the languages believed by some religious traditions to be used by angels. It usually refers to sung praise in Second Temple period Jewish materials.

Songs of the Sabbath Sacrifice is the principal source for angelic tongues at Qumran. The texts are fragmentary but appear to relate to praise tongues:

It is not clear whether the angelic tongues are coherent, intelligible to man. However, since Songs of the Sabbath Sacrifice is itself related to sung praise at the Qumran community, there is not a parallel with coherent angelic praise tongues in Testament of Job.

The pseudepigraphical Testament of Job (ca.100 BCE–100CE) contains a conclusion which is believed to relate to the compiling of the hymnbook used by a Therapeutae community. Job gives one of his daughters "a cord" (a stringed instrument of some kind?) 
Job’s other daughters likewise took on “the dialect of archons”, “the dialect of those and the “dialect of the cherubim” (T. Job 49:1-50:3). The “cherubim” are also mentioned Songs of the Sabbath Sacrifice as blessing God (4Q403 1 2, 15, cf. 4Q405 20 2, 3). 

There is parallel description of sung prophecy among the Therapeutae in Alexandria by Philo, but no mention there of angelic tongues.

A possible reference to Jewish practices of angelic tongues is 1Co13:1 "If I speak in the tongues of men and of angels, but have not love, I am a noisy gong or a clanging cymbal." The distinction "of men" and "of angels" may suggests that a distinction was known to the Corinthians. If a distinction is intended then 1Co14:10 "There are doubtless many different languages in the world, and none is without meaning" may imply that "tongues of men" were intelligible, whereas 1Co14:2 For one who speaks in a tongue speaks not to men but to God; for no one understands him, but he utters mysteries in the Spirit." refers to angelic tongues. The problem with this is that the "angelic" tongues documented at Qumran and among the Therapeutae appear to be inspired, but coherent and intelligible, sung praise. Against this is the view of Dunn that "It is evident then that Paul thinks of glossolalia as language". 


</doc>
<doc id="1060739" url="https://en.wikipedia.org/wiki?curid=1060739" title="Counterword">
Counterword

A counterword (also spelled counter word and counter-word) is a word such as "so" that is frequently used to answer ("counter") in a reflex-like manner and that has due to this frequent use quickly taken on a new, much less specific or much looser meaning or is even almost meaningless or performs a completely new function. The word "so", for example, is frequently used to begin an answer in the sense of "Well..." or to function as an indirect way of saying "Before answering that, I'd like to..." or even instead of saying "On the contrary..." or "No, I...".

In a more general sense, the term is used for such words also when they are not used as a reflex-like answer and even for any widely used words that (due to a similar change) now have a broad and vague range of meanings in many very different situations (e.g. case, awfully, fix, job, payoff).

Since such change due to very frequent use occurs much more rapidly than the change in meaning all words go through, and since such words are even sometimes still simultaneously used in their original sense, the new usage is often considered incorrect by some speakers. Other examples include "nice", "terrific", "terrible", "awful", "tremendous", "swell", "hopefully" and "very fine" (degrading the meaning of "fine" to "OK").

The "Oxford English Dictionary" does not support this and defines counter-word as "countersign", noting that its usage is military and obsolete with a single quotation from 1678.


</doc>
<doc id="191445" url="https://en.wikipedia.org/wiki?curid=191445" title="Vocabulary">
Vocabulary

A vocabulary is a set of familiar words within a person's language. A vocabulary, usually developed with age, serves as a useful and fundamental tool for communication and acquiring knowledge. Acquiring an extensive vocabulary is one of the largest challenges in learning a second language.

Vocabulary is commonly defined as "all the words known and used by a particular person".

The first major change distinction that must be made when evaluating word knowledge is whether the knowledge is productive (also called achieve) or receptive (also called receive); even within those opposing categories, there is often no clear distinction. Words that are generally understood when heard or read or seen constitute a person's receptive vocabulary. These words may range from well-known to barely known (see degree of knowledge below). A person's receptive vocabulary is usually the larger of the two. For example, although a young child may not yet be able to speak, write, or sign, he or she may be able to follow simple commands and appear to understand a good portion of the language to which they are exposed. In this case, the child's receptive vocabulary is likely tens, if not hundreds of words, but his or her active vocabulary is zero. When that child learns to speak or sign, however, the child's active vocabulary begins to increase. It is also possible for the productive vocabulary to be larger than the receptive vocabulary, for example in a second-language learner who has learned words through study rather than exposure, and can produce them, but has difficulty recognizing them in conversation.

Productive vocabulary, therefore, generally refers to words that can be produced within an appropriate context and match the intended meaning of the speaker or signer. As with receptive vocabulary, however, there are many degrees at which a particular word may be considered part of an active vocabulary. Knowing how to pronounce, sign, or write a word does not necessarily mean that the word that has been used correctly or accurately reflects the intended message; but it does reflect a minimal amount of productive knowledge.

Within the receptive–productive distinction lies a range of abilities that are often referred to as "degree of knowledge". This simply indicates that a word gradually enters a person's vocabulary over a period of time as more aspects of word knowledge are learnt. Roughly, these stages could be described as:


The differing degrees of word knowledge imply a greater "depth of knowledge", but the process is more complex than that. There are many facets to knowing a word, some of which are not hierarchical so their acquisition does not necessarily follow a linear progression suggested by "degree of knowledge". Several frameworks of word knowledge have been proposed to better operationalise this concept. One such framework includes nine facets:


Words can be defined in various ways, and estimates of vocabulary size differ depending on the definition used. The most common definition is that of a lemma (the uninflected or dictionary form; this includes "walk", but not "walks, walked or walking"). Most of the time lemmas do not include proper nouns (names of people, places, companies, etc). Another definition often used in research of vocabulary size is that of word family. These are all the words that can be derived from a ground word (e.g., the words "effortless, effortlessly, effortful, effortfully" are all part of the word family "effort"). Estimates of vocabulary size range from as high as 200 thousand to as low as 10 thousand, depending on the definition used. 

"Listed in order of most ample to most limited:"

A literate person's vocabulary is all the words they can recognize when reading. This is generally the largest type of vocabulary simply because a reader tends to be exposed to more words by reading than by listening.

A person's listening vocabulary is all the words they can recognize when listening to speech. People may still understand words they were not exposed to before using cues such as tone, gestures, the topic of discussion and the social context of the conversation.

A person's speaking vocabulary is all the words they use in speech. It is likely to be a subset of the listening vocabulary. Due to the spontaneous nature of speech, words are often misused. This misuse, though slight and unintentional, may be compensated by facial expressions and tone of voice.

Words are used in various forms of writing from formal essays to social media feeds. Many written words do not commonly appear in speech. Writers generally use a limited set of words when communicating. For example, if there are a number of synonyms, a writer may have a preference as to which of them to use, and they are unlikely to use technical vocabulary relating to a subject in which they have no knowledge or interest.

The American philosopher Richard Rorty characterized a person's "final vocabulary" as follows:

All human beings carry about a set of words which they employ to justify their actions, their beliefs, and their lives. These are the words in which we formulate praise of our friends and contempt for our enemies, our long-term projects, our deepest self-doubts and our highest hopes… I shall call these words a person's “final vocabulary”. Those words are as far as he can go with language; beyond them is only helpless passivity or a resort to force. ("Contingency, Irony, and Solidarity" p. 73)

Focal vocabulary is a specialized set of terms and distinctions that is particularly important to a certain group: those with a particular focus of experience or activity. A lexicon, or vocabulary, is a language's dictionary: its set of names for things, events, and ideas. Some linguists believe that lexicon influences people's perception of things, the Sapir–Whorf hypothesis. For example, the Nuer of Sudan have an elaborate vocabulary to describe cattle. The Nuer have dozens of names for cattle because of the cattle's particular histories, economies, and environments. This kind of comparison has elicited some linguistic controversy, as with the number of "Eskimo words for snow". English speakers with relevant specialised knowledge can also display elaborate and precise vocabularies for snow and cattle when the need arises.

During its infancy, a child instinctively builds a vocabulary. Infants imitate words that they hear and then associate those words with objects and actions. This is the listening vocabulary. The speaking vocabulary follows, as a child's thoughts become more reliant on his/her ability to self-express without relying on gestures or babbling. Once the reading and writing vocabularies start to develop, through questions and education, the child starts to discover the anomalies and irregularities of language.

In first grade, a child who can read learns about twice as many words as one who cannot. Generally, this gap does not narrow later. This results in a wide range of vocabulary by age five or six, when an English-speaking child will have learned about 1500 words.

Vocabulary grows throughout our entire life. Between the ages of 20 and 60, people learn some 6,000 more lemmas, or one every other day. An average 20-year-old knows 42,000 words coming from 11,100 word families; an average 60-year-old knows 48,200 lemmas coming from 13,400 word families. People expand their vocabularies by e.g. reading, playing word games, and participating in vocabulary-related programs. Exposure to traditional print media teaches correct spelling and vocabulary, while exposure to text messaging leads to more relaxed word acceptability constraints.


Estimating average vocabulary size poses various difficulties and limitations due to the different definitions and methods employed such as what is the word, what is to know a word, what sample dictionaries were used, how tests were conducted, and so on. Native speakers' vocabularies also vary widely within a language, and are dependent on the level of the speaker's education.

As a result estimates vary from as little as 10,000 to as many as over 50,000 for young adult native speakers of English.

One most recent 2016 study shows that 20-year-old English native speakers recognize on average 42,000 lemmas, ranging from 27,100 for the lowest 5% of the population to 51,700 lemmas for the highest 5%. These lemmas come from 6,100 word families in the lowest 5% of the population and 14,900 word families in the highest 5%. 60-year-olds know on average 6,000 lemmas more.
According to another, earlier 1995 study junior-high students would be able to recognize the meanings of about 10,000–12,000 words, whereas for college students this number grows up to about 12,000–17,000 and for elderly adults up to about 17,000 or more.

For native speakers of German average absolute vocabulary sizes range from 5,900 lemmas in first grade to 73,000 for adults.

The knowledge of the 3000 most frequent English word families or the 5000 most frequent words provides 95% vocabulary coverage of spoken discourse.
For minimal reading comprehension a threshold of 3,000 word families (5,000 lexical items) was suggested and for reading for pleasure 5,000 word families (8,000 lexical items) are required. An "optimal" threshold of 8,000 word families yields the coverage of 98% (including proper nouns).

Learning vocabulary is one of the first steps in learning a second language, but a learner never finishes vocabulary acquisition. Whether in one's native language or a second language, the acquisition of new vocabulary is an ongoing process. There are many techniques that help one acquire new vocabulary.

Although memorization can be seen as tedious or boring, associating one word in the native language with the corresponding word in the second language until memorized is considered one of the best methods of vocabulary acquisition. By the time students reach adulthood, they generally have gathered a number of personalized memorization methods. Although many argue that memorization does not typically require the complex cognitive processing that increases retention (Sagarra and Alba, 2006), it does typically require a large amount of repetition, and spaced repetition with flashcards is an established method for memorization, particularly used for vocabulary acquisition in computer-assisted language learning. Other methods typically require more time and longer to recall.

Some words cannot be easily linked through association or other methods. When a word in the second language is phonologically or visually similar to a word in the native language, one often assumes they also share similar meanings. Though this is frequently the case, it is not always true. When faced with a false friend, memorization and repetition are the keys to mastery. If a second language learner relies solely on word associations to learn new vocabulary, that person will have a very difficult time mastering false friends. When large amounts of vocabulary must be acquired in a limited amount of time, when the learner needs to recall information quickly, when words represent abstract concepts or are difficult to picture in a mental image, or when discriminating between false friends, rote memorization is the method to use. A neural network model of novel word learning across orthographies, accounting for L1-specific memorization abilities of L2-learners has recently been introduced (Hadzibeganovic and Cannas, 2009).

One useful method of building vocabulary in a second language is the keyword method. If time is available or one wants to emphasize a few key words, one can create mnemonic devices or word associations. Although these strategies tend to take longer to implement and may take longer in recollection, they create new or unusual connections that can increase retention. The keyword method requires deeper cognitive processing, thus increasing the likelihood of retention (Sagarra and Alba, 2006). This method uses fits within Paivio's (1986) dual coding theory because it uses both verbal and image memory systems. However, this method is best for words that represent concrete and imageable things. Abstract concepts or words that do not bring a distinct image to mind are difficult to associate. In addition, studies have shown that associative vocabulary learning is more successful with younger students (Sagarra and Alba, 2006). Older students tend to rely less on creating word associations to remember vocabulary.

Several word lists have been developed to provide people with a limited vocabulary either for the purpose of rapid language proficiency or for effective communication. These include Basic English (850 words), Special English (1,500 words), General Service List (2,000 words), and Academic Word List. Some learner's dictionaries have developed defining vocabularies which contain only most common and basic words. As a result word definitions in such dictionaries can be understood even by learners with a limited vocabulary. Some publishers produce dictionaries based on word frequency or thematic groups.

The Swadesh list was made for investigation in linguistics.





</doc>
<doc id="27724306" url="https://en.wikipedia.org/wiki?curid=27724306" title="Nonsense word">
Nonsense word

A nonsense word, unlike a sememe, may have no definition. Nonsense words can be classified depending on their orthographic and phonetic similarity with (meaningful) words. If it can be pronounced according to a language's phonotactics, it is a pseudoword. Nonsense words are used in literature for poetic or humorous effect. Proper names of real or fictional entities are sometimes nonsense words.

A stunt word is a nonsense word used for a special effect, or to attract attention, as part of a performance. Such words are a feature of the work of Dr. Seuss ("Sometimes I am quite certain there's a Jertain in the curtain").

The ability to infer the (hypothetical) meaning of a nonsense word from context is used to test for brain damage.


</doc>
<doc id="8128" url="https://en.wikipedia.org/wiki?curid=8128" title="Dialect">
Dialect

The term dialect (from Latin , , from the Ancient Greek word , , "discourse", from , , "through" and , , "I speak") is used in two distinct ways to refer to two different types of linguistic phenomena:



Features that distinguish dialects from each other can be found in lexicon (vocabulary) and grammar, as well as in pronunciation (phonology, including prosody). Where the salient distinctions are only or mostly to be observed in pronunciation, the more specific term "accent" may be used instead of "dialect". Other types of speech varieties include jargons, which are characterized by differences in lexicon; slang; patois; pidgins; and argots. The particular speech patterns used by an individual are termed an idiolect.

A "standard dialect" (also known as a "standardized dialect" or "standard language") is a dialect that is supported by institutions. Such institutional support may include government recognition or designation; presentation as being the "correct" form of a language in schools; published grammars, dictionaries, and textbooks that set forth a normative spoken and written form; and an extensive formal literature that employs that variety (prose, poetry, non-fiction, etc.). There may be multiple standard dialects associated with a single language. For example, Standard American English, Standard British English, Standard Canadian English, Standard Indian English, Standard Australian English, and Standard Philippine English may all be said to be standard dialects of the English language.

A nonstandard dialect, like a standard dialect, has a complete grammar and vocabulary, but is usually not the beneficiary of institutional support. Examples of a nonstandard English dialect are Southern American English, Western Australian English, New York English, New England English, Mid-Atlantic American or Philadelphia / Baltimore English, Scouse, Brummie, Cockney, and Tyke. The Dialect Test was designed by Joseph Wright to compare different English dialects with each other.

There is no universally accepted criterion for distinguishing two different languages from two dialects (i.e. varieties) of the same language. A number of rough measures exist, sometimes leading to contradictory results. The distinction (dichotomy) between dialect and language is therefore subjective (arbitrary) and depends upon the user's preferred frame of reference. For example, there has been discussion about whether or not the Limón Creole English should be considered "a kind" of English or a different language. This creole is spoken in the Caribbean coast of Costa Rica (Central America) by descendants of Jamaican people. The position that Costa Rican linguists support depends upon which University they represent. Another example is Scanian, which even, for a time, had its own ISO code.

One criterion, which is often considered to be purely linguistic, is that of mutual intelligibility: two varieties are said to be dialects of the same language if being a speaker of one variety confers sufficient knowledge to understand and be understood by a speaker of the other; otherwise, they are said to be different languages. However, this definition cannot consistently delimit languages in the case of a dialect continuum (or dialect chain), containing a sequence of varieties, each mutually intelligible with the next, but where widely separated varieties may not be mutually intelligible.
Further problems with this criterion are that mutual intelligibility occurs in varying degrees, and that it is difficult to distinguish from prior familiarity with the other variety. Reported mutual intelligibility may also be affected by speakers' attitudes to the other speech community.

Another occasionally used criterion for discriminating dialects from languages is the sociolinguistic notion of linguistic authority. According to this definition, two varieties are considered dialects of the same language if (under at least some circumstances) they would defer to the same authority regarding some questions about their language. For instance, to learn the name of a new invention, or an obscure foreign species of plant, speakers of Westphalian and East Franconian German might each consult a German dictionary or ask a German-speaking expert in the subject.
Thus these varieties are said to be dependent on, or heteronomous with respect to, Standard German, which is said to be autonomous.
In contrast, speakers in the Netherlands of Low Saxon varieties similar to Westphalian would instead consult a dictionary of Standard Dutch.
Similarly, although Yiddish is classified by linguists as a language in the Middle High German group of languages, a Yiddish speaker would consult a different dictionary in such a case.

Within this framework, W. A. Stewart defined a "language" as an autonomous variety together with all the varieties that are heteronomous with respect to it, noting that an essentially equivalent definition had been stated by Charles A. Ferguson and John J. Gumperz in 1960.
Similarly, a heteronomous variety may be considered a "dialect" of a language defined in this way.
In these terms, Danish and Norwegian, though mutually intelligible to a large degree, are considered separate languages.
In the framework of Heinz Kloss, these are described as languages by "ausbau" (development) rather than by "abstand" (separation).

In other situations, a closely related group of varieties possess considerable (though incomplete) mutual intelligibility, but none dominates the others.
To describe this situation, the editors of the "Handbook of African Languages" introduced the term "dialect cluster" as a classificatory unit at the same level as a language.
A similar situation, but with a greater degree of mutual unintelligibility, has been termed a "language cluster".

In many societies, however, a particular dialect, often the sociolect of the elite class, comes to be identified as the "standard" or "proper" version of a language by those seeking to make a social distinction and is contrasted with other varieties. As a result of this, in some contexts, the term "dialect" refers specifically to varieties with low social status. In this secondary sense of "dialect", language varieties are often called "dialects" rather than "languages":

The status of "language" is not solely determined by linguistic criteria, but it is also the result of a historical and political development. Romansh came to be a written language, and therefore it is recognized as a language, even though it is very close to the Lombardic alpine dialects. An opposite example is the case of Chinese, whose variations such as Mandarin and Cantonese are often called dialects and not languages in China, despite their mutual unintelligibility.

Modern nationalism, as developed especially since the French Revolution, has made the distinction between "language" and "dialect" an issue of great political importance. A group speaking a separate "language" is often seen as having a greater claim to being a separate "people", and thus to be more deserving of its own independent state, while a group speaking a "dialect" tends to be seen not as "a people" in its own right, but as a sub-group, part of a bigger people, which must content itself with regional autonomy. The distinction between language and dialect is thus inevitably made at least as much on a political basis as on a linguistic one, and can lead to great political controversy or even armed conflict.

The Yiddish linguist Max Weinreich published the expression, "A shprakh iz a dialekt mit an armey un flot" (: "A language is a dialect with an army and navy") in "YIVO Bleter" 25.1, 1945, p. 13. The significance of the political factors in any attempt at answering the question "what is a language?" is great enough to cast doubt on whether any strictly linguistic definition, without a socio-cultural approach, is possible. This is illustrated by the frequency with which the army-navy aphorism is cited.

By the definition most commonly used by linguists, any linguistic variety can be considered a "dialect" of "some" language—"everybody speaks a dialect". According to that interpretation, the criteria above merely serve to distinguish whether two varieties are dialects of the "same" language or dialects of "different" languages.

The terms "language" and "dialect" are not necessarily mutually exclusive, although it is often perceived to be. Thus there is nothing contradictory in the statement "the "language" of the Pennsylvania Dutch is a dialect of German".

There are various terms that linguists may use to avoid taking a position on whether the speech of a community is an independent language in its own right or a dialect of another language. Perhaps the most common is "variety"; "lect" is another. A more general term is "languoid", which does not distinguish between dialects, languages, and groups of languages, whether genealogically related or not.

John Lyons writes that "Many linguists [...] subsume differences of accent under differences of dialect." In general, "accent" refers to variations in pronunciation, while "dialect" also encompasses specific variations in grammar and vocabulary.

When talking about the German language, the term German dialects is only used for the traditional regional varieties. That allows them to be distinguished from the regional varieties of modern standard German.

The German dialects show a wide spectrum of variation. Some of them are not mutually intelligible. German dialectology traditionally names the major dialect groups after Germanic tribes from which they were assumed to have descended.

The extent to which the dialects are spoken varies according to a number of factors: In Northern Germany, dialects are less common than in the South. In cities, dialects are less common than in the countryside. In a public environment, dialects are less common than in a familiar environment.

The situation in Switzerland and Liechtenstein is different from the rest of the German-speaking countries. The Swiss German dialects are the default everyday language in virtually every situation, whereas standard German is only spoken in education, partially in media, and with foreigners not possessing knowledge of Swiss German. Most Swiss German speakers perceive standard German to be a foreign language.

The Low German and Low Franconian varieties spoken in Germany are often counted among the German dialects. This reflects the modern situation where they are roofed by standard German. This is different from the situation in the Middle Ages when Low German had strong tendencies towards an ausbau language.

The Frisian languages spoken in Germany are excluded from the German dialects.

Italy is an often quoted example of a country where the second definition of the word "dialect" ("dialetto") is most prevalent. Italy is in fact home to a vast array of languages, most of which are Latin-based, lack mutual intelligibility with one another, and have their own local varieties; twelve of them (Albanian, Catalan, German, Greek, Slovene, Croatian, French, Franco-Provençal, Friulian, Ladin, Occitan and Sardinian) underwent Italianization to a varying degree (ranging from the critically endangered state displayed by Sardinian and Greek to the vigorous promotion of South Tyrolean), but are officially recognized as minority languages ("minoranze linguistiche storiche") nowadays, in light of their distinctive historical development. Yet, most of the regional languages spoken across the peninsula are often colloquially referred to in non-linguistic circles as Italian "dialetti", since even the prestigious Neapolitan, Sicilian and Venetian have had vulgar Tuscan as their reference language since the Middle Ages. However, only a few of them are actually derived from modern Italian, the majority being rather evolved from Vulgar Latin separately and individually from one another and independently of Italian, long prior to the popular diffusion of the latter throughout what is now Italy.

During the Risorgimento, Italian still existed mainly as a literary language, and only 2.5% of Italy's population could speak Italian. Proponents of Italian republicanism and Italian nationalism, like the Lombard Alessandro Manzoni, stressed the importance of establishing a uniform national language in order to better create an Italian national identity. With the unification of Italy in the 1860s, Italian became the official national language of the new Italian state, while the other ones became to be institutionally regarded as subordinate dialects to Italian, and negatively associated with a lack of education.

In the early 20th century, the vast conscription of Italian men from all throughout Italy during World War I is credited with facilitating the diffusion of Italian among less educated Italian men, as these men from various regions with various regional languages were forced to communicate with each other in a common tongue while serving in the Italian military. With the popular spread of standard Italian out of the intellectual circles, because of the mass-media and the establishment of public education, Italians from all regions were increasingly exposed to Italian. While "dialect" levelling has increased the number of Italian speakers and decreased the number of speakers of other languages native to Italy, Italians in different regions have developed variations of standard Italian specific to their region. These variations of standard Italian, known as regional Italian, would thus more appropriately be called "dialects" in accordance with the first linguistic definition of "dialect", as they are in fact derived partially or mostly from standard Italian, with some degree of influence from the local or regional native languages and accents.

The most widely spoken languages of Italy fall within a family of which even Italian is part, the Italo-Dalmatian group. This wide category includes:

Modern Italian is heavily based on the Florentine dialect of Tuscan. The Tuscan-based language that would eventually become modern Italian had been used in poetry and literature since at least the 12th century, and it first spread outside the Tuscan linguistic borders through the works of the so-called "tre corone" ("three crowns"): Dante Alighieri, Petrarch, and Giovanni Boccaccio. Florentine thus gradually rose to prominence as the "volgare" of the literate and upper class in Italy, and it spread throughout the peninsula and Sicily as the "lingua franca" among the Italian educated class as well as Italian travelling merchants. The economic prowess and cultural and artistic importance of Tuscany in the Late Middle Ages and the Renaissance further encouraged the diffusion of the Florentine-Tuscan Italian throughout Italy and among the educated and powerful, though local and regional languages remained the main languages of the common people.

Aside from the Italo-Dalmatian languages, the second most widespread family in Italy is the Gallo-Italic group, spanning throghout much of Northern Italy's languages and dialects (such as Piedmontese, Emilian-Romagnol, Ligurian, Lombard, Venetian, Sicily's and Basilicata's Gallo-Italic in southern Italy, etc.).

Finally, other languages from a number of different families follow the last two major groups: the Gallo-Romance languages (French, Occitan and its Vivaro-Alpine dialect, Franco-Provençal); the Rhaeto-Romance languages (Friulian and Ladin); the Ibero-Romance languages (Sardinia's Algherese); the Germanic Cimbrian, Southern Bavarian, Walser German and the Mòcheno language; the Albanian Arbëresh language; the Hellenic Griko language and Calabrian Greek; the Serbo-Croatian Slavomolisano dialect; and the various Slovene languages, including the Gail Valley dialect and Istrian dialect. The language indigenous to Sardinia, while being Romance in nature, is considered to be a specific linguistic family of its own, separate from all the other Neo-Latin groups; it is often subdivided into the Centro-Southern and Centro-Northern dialects.

Though mostly mutually unintelligible, the exact degree to which all the Italian languages are mutually unintelligible varies, often correlating with geographical distance or geographical barriers between the languages; some regional Italian languages that are closer in geographical proximity to each other or closer to each other on the dialect continuum are more or less mutually intelligible. For instance, a speaker of purely Eastern Lombard, a language in Northern Italy's Lombardy region that includes the Bergamasque dialect, would have severely limited mutual intelligibility with a purely Italian speaker and would be nearly completely unintelligible to a Sicilian-speaking individual. Due to Eastern Lombard's status as a Gallo-Italic language, an Eastern Lombard speaker may, in fact, have more mutual intelligibility with an Occitan, Catalan, or French speaker than with an Italian or Sicilian speaker. Meanwhile, a Sicilian-speaking person would have a greater degree of mutual intelligibility with a speaker of the more closely related Neapolitan language, but far less mutual intelligibility with a person speaking Sicilian Gallo-Italic, a language that developed in isolated Lombard emigrant communities on the same island as the Sicilian language.

Today, the majority of Italian nationals are able to speak Italian, though many Italians still speak their regional language regularly or as their primary day-to-day language, especially at home with family or when communicating with Italians from the same town or region.

The classification of speech varieties as dialects or languages and their relationship to other varieties of speech can be controversial and the verdicts inconsistent. Serbo-Croatian illustrates this point. Serbo-Croatian has two major formal variants (Serbian and Croatian). Both are based on the "Shtokavian" dialect and therefore mutually intelligible with differences found mostly in their respective local vocabularies and minor grammatical differences. Certain dialects of Serbia ("Torlakian") and Croatia ("Kajkavian" and "Chakavian"), however, are not mutually intelligible even though they are usually subsumed under Serbo-Croatian. How these dialects should be classified in relation to Shtokavian remains a matter of dispute.

Macedonian, although largely mutually intelligible with Bulgarian and certain dialects of Serbo-Croatian (Torlakian), is considered by Bulgarian linguists to be a Bulgarian dialect, in contrast with the contemporary international view and the view in North Macedonia, which regards it as a language in its own right. Nevertheless, before the establishment of a literary standard of Macedonian in 1944, in most sources in and out of Bulgaria before the Second World War, the southern Slavonic dialect continuum covering the area of today's North Macedonia were referred to as Bulgarian dialects.

In Lebanon, a part of the Christian population considers "Lebanese" to be in some sense a distinct language from Arabic and not merely a dialect. During the civil war Christians often used Lebanese Arabic officially, and sporadically used the Latin script to write Lebanese, thus further distinguishing it from Arabic. All Lebanese laws are written in the standard literary form of Arabic, though parliamentary debate may be conducted in Lebanese Arabic.

In Tunisia, Algeria, and Morocco, the Darijas (spoken North African languages) are sometimes considered more different from other Arabic dialects. Officially, North African countries prefer to give preference to the Literary Arabic and conduct much of their political and religious life in it (adherence to Islam), and refrain from declaring each country's specific variety to be a separate language, because Literary Arabic is the liturgical language of Islam and the language of the Islamic sacred book, the Qur'an. Although, especially since the 1960s, the Darijas are occupying an increasing use and influence in the cultural life of these countries. Examples of cultural elements where Darijas' use became dominant include: theatre, film, music, television, advertisement, social media, folk-tale books and companies' names.

The Modern Ukrainian language has been in common use since the late 17th century, associated with the establishment of the Cossack Hetmanate. In the 19th century, the Tsarist Government of the Russian Empire claimed that Ukrainian was merely a dialect of Russian and not a language on its own. According to these claims, the differences were few and caused by the conquest of western Ukraine by the Polish-Lithuanian Commonwealth. However, in reality the dialects in Ukraine were developing independently from the dialects in the modern Russia for several centuries, and as a result they differed substantially.

Following the signing of the Brest-Litovsk Treaty, the German Empire briefly gained control over Ukraine during World War I, but was eventually defeated by the Entente, with major involvement by Russian Bolsheviks. After Bolsheviks managed to conquer the rest of Ukraine from the Ukrainian People's Republic and the Whites, Ukraine became part of the USSR, whence a process of Ukrainization was begun, with encouragement from Moscow. However, in the late 1920s - early 1930s, the process started to reverse. Witnessing the Ukrainian cultural revival spurred by the Ukrainization in the early 1920s, and fearing that it might lead to an independence movement, Moscow started to remove from power and in some cases physically eliminate the public proponents of Ukrainization. The appointment of Pavel Postyshev as the secretary of the Communist Party of Ukraine marked the end of Ukrainization, and the opposite process of Russification started. After World War II, citing Ukrainian collaboration with Nazi Germany in an attempt to gain independence as the reason, Moscow changed its policy towards increasing repression of the Ukrainian language.

Today the boundaries of the Ukrainian language to the Russian language are still not drawn clearly, with an intermediate dialect between them, called Surzhyk, developing in Ukraine, also known as balachka in Ukrainian ethnic territories controlled by Russia.

There have been cases of a variety of speech being deliberately reclassified to serve political purposes. One example is Moldovan. In 1996, the Moldovan parliament, citing fears of "Romanian expansionism", rejected a proposal from President Mircea Snegur to change the name of the language to Romanian, and in 2003 a Moldovan–Romanian dictionary was published, purporting to show that the two countries speak different languages. Linguists of the Romanian Academy reacted by declaring that all the Moldovan words were also Romanian words; while in Moldova, the head of the Academy of Sciences of Moldova, Ion Bărbuţă, described the dictionary as a politically motivated "absurdity".

Unlike languages that use alphabets to indicate their pronunciation, Chinese characters have developed from logograms that do not always give hints to their pronunciation. Although the written characters have remained relatively consistent for the last two thousand years, the pronunciation and grammar in different regions have developed to an extent that the varieties of the spoken language are often mutually unintelligible. As a series of migration to the south throughout the history, the regional languages of the south, including Gan, Xiang, Wu, Min, Yue and Hakka often show traces of Old Chinese or Middle Chinese. From the Ming dynasty onward, Beijing has been the capital of China and the dialect spoken in Beijing has had the most prestige among other varieties. With the founding of the Republic of China, Standard Mandarin was designated as the official language, based on the spoken language of Beijing. Since then, other spoken varieties are regarded as "fangyan" (regional speech). Cantonese is still the most commonly-used language in Guangzhou, Hong Kong, Macau and among some overseas Chinese communities, whereas Hokkien has been accepted in Taiwan as an important local language alongside Mandarin.

One language, Interlingua, was developed so that the languages of Western civilization would act as its dialects. Drawing from such concepts as the international scientific vocabulary and Standard Average European, linguists developed a theory that the modern Western languages were actually dialects of a hidden or latent language. Researchers at the International Auxiliary Language Association extracted words and affixes that they considered to be part of Interlingua's vocabulary. In theory, speakers of the Western languages would understand written or spoken Interlingua immediately, without prior study, since their own languages were its dialects. This has often turned out to be true, especially, but not solely, for speakers of the Romance languages and educated speakers of English. Interlingua has also been found to assist in the learning of other languages. In one study, Swedish high school students learning Interlingua were able to translate passages from Spanish, Portuguese, and Italian that students of those languages found too difficult to understand. The vocabulary of Interlingua extends beyond the Western language families.




</doc>
<doc id="18894210" url="https://en.wikipedia.org/wiki?curid=18894210" title="Open-ended question">
Open-ended question

An open-ended question is a question that cannot be answered with a "yes" or "no" response, or with a static response. Open-ended questions are phrased as a statement which requires a response. The response can be compared to information that is already known to the questioner.

Examples of open-ended questions:

The received wisdom in education is that open questions are broadly speaking 'good' questions. They invite students to give longer responses that demonstrate their understanding. They are preferable to closed questions (i.e. one that demands a yes/no answer) because they are better for discussions or enquiries, whereas closed questions are only good for testing. 

Peter Worley argues that this is a false assumption. This is based on Worley's central arguments that there are two different kinds of open and closed questions: grammatical and conceptual. He argues that educational practitioners should be aiming for questions that are "grammatically closed, but conceptually open". For example, in standard parlance, 'is it ever right to lie?' would be regarded as a closed question: it elicits a yes/no response. Significantly, however, it is conceptually open. Any initial yes/no answer to it can be 'opened up' by the questioner ('why do you think that?,' 'Could there be an instance where that's not the case?), inviting elaboration and enquiry. 

This grammatically closed but cognitively open style of questioning, Worley argues, "gives [educators] the best of both worlds: the focus and specificity of a closed question (this, after all, is why teachers use them) and the inviting, elaborating character of an open question". Closed questions, simply require 'opening up' strategies to ensure that conceptually open questions can fulfil their educational potential. 

Worley's structural and semantic distinction between open and closed questions is integral to his pedagogical invention 'Open Questioning Mindset', or OQM. OQM refers to the development, in educators, of an open attitude towards the process of learning and the questioning at the heart of that process. It is a mind-set that is applicable to all subject areas and all pedagogical environments. Teachers who develop an Open Questioning Mindset listen openly for the cognitive content of student's contributions and looks for ways to use what is given for learning opportunities, whether right, wrong, relevant or apparently irrelevant. OQM encourages a style of pedagogy that values genuine enquiry in the classroom. It provides teachers with the tools to move beyond what Worley calls 'guess what's in my head' teaching, that relies on closed and leading questions.



</doc>
<doc id="53111373" url="https://en.wikipedia.org/wiki?curid=53111373" title="Multiethnolect">
Multiethnolect

Multiethnolect is a term originally coined by Clyne (2000) and Quist (2000). It is used by a number of linguists to define an emerging, distinct variety of language found in young, working-class urban neighbourhoods, across Scandinavia, the Netherlands, Belgium, Germany and Great Britain. Multiethnolects appear to be less homogeneous than either dialects or sociolects and are assumed to be context-bound and transient, to the extent that they are ‘youth languages'. Wiese (2006) uses the term German Kiezdeutsch, meaning ‘neighbourhood German’, to refer to multiethnic youth language in Germany. Cheshire et al. (2011) claim that the term "Jafaican", which refers to youth language in multiethnic parts of London, a name that has close associations with hip-hop, is a type of multiethnolect. Kotsinas (1988) uses the term "rinkebysvenska" (named after one such district, Rinkeby) to refer to the Swedish characteristics of multiethnolects that are spoken in districts of Stockholm. Multientholects are considered to be a type of Labovian "vernacular" that many older people claim that young people in London today sound as if they are "talking black"."The reasons for the emergence of European multiethnolects at this point in history is presumably linked to specific types of community formation in urban areas which have seen very large-scale immigration from developing countries. People of different language backgrounds have settled in already quite underprivileged neighbourhoods, and economic deprivation has led to the maintenance of close kin and neighbourhood ties. Castells (2000) writes of prosperous metropolises containing communities such as these: ‘It is this distinctive feature of being globally connected and locally disconnected, physically and socially, that makes mega-cities a new urban form’."Cheshire, Nortier, and Adger state that 'a defining characteristic is that [multiethnolects] are used by (usually monolingual) young people from non-immigrant backgrounds as well as by their bilingual peers'.




</doc>
<doc id="29584019" url="https://en.wikipedia.org/wiki?curid=29584019" title="Word family">
Word family

A word family is the base form of a word plus its inflected forms and derived forms made with suffixes and prefixes plus its cognates, i.e. all words that have a common etymological origin, some of which even native speakers don't recognize as being related (e.g. "wrought (iron)" and "work(ed)"). In the English language, inflectional affixes include third person -"s", verbal "-ed" and "-ing", plural -"s", possessive -"s", comparative -"er" and superlative -"est". Derivational affixes include -"able, -er, -ish, -less, -ly, -ness, -th, -y, non-, un-, -al, -ation, -ess, -ful, -ism, -ist, -ity, -ize/-ise, -ment, in-". The idea is that a base word and its inflected forms support the same core meaning, and can be considered learned words if a learner knows both the base word and the affix.
Bauer and Nation proposed seven levels of affixes based on their frequency in English. It has been shown that word families can assist with deriving related words via affixes, along with decreasing the time needed to derive and recognize such words.

There are several studies that suggest that knowledge of root words and their derivatives can assist with learning or even deducing the meaning of other members of a word family. A study from Carlisle and Katz (2006) comparing separate English word families varying in size, frequency, and affirmation and negation suggests that “accuracy of reading derived words by 4th and 6th graders is related to measures of familiarity, ... base word frequencies, family size, average family frequency, and word length”. It was found that families that were either larger or more frequent (i.e. word families that had more words or were more common) were more quickly read. Nagy et al. (1989) found that morphologically related families had an increase of reaction time of up to 7 ms compared to those without a morphological relation. Nagy et al. (1993) summarizes how knowledge of the meanings of common English suffixes underwent significant development between fourth grade and high school.

There have also been studies on non-native English speakers and learners on their knowledge and understanding of word families. A study of nonnative-English-speaking college students showed that non-native English speakers knew at least some of the four word forms studied (nouns, verbs, adjectives, and adverbs). Out of these four, word families derived from nouns and verbs were found to be the most well-known. Results showed that in regards to these word forms, ESL students knew the least, MA-ELT (English Language Teaching) students knew more, and native speakers knew the most. In addition, a study of Japanese students learning English showed poor knowledge of the affixes studied, showing a division between their knowledge of a word's meaning and a derivative form of a separate word (e.g. "stimulate" versus "similar", "disclose" and "far"). To conclude their study, Schmitt and Zimmerman have provided the following for those teaching word families as a guideline:



</doc>
<doc id="6286841" url="https://en.wikipedia.org/wiki?curid=6286841" title="Li Europan lingues">
Li Europan lingues

Li Europan lingues () is a quotation in Occidental (Interlingue), an international auxiliary language devised by Edgar von Wahl in 
1922. It is used in some HTML templates as a fill-in or placeholder text.

When used as placeholder text, "Li Europan lingues" is usually one or two paragraphs and reads as follows:


The original text of Li Europan lingues comes from an article written in 1933 for the journal Cosmoglotta entitled "Occidental es inevitabil" (Occidental is inevitable), in which S.W. Beer from the universities of London and Cambridge wrote a letter explaining that he supported the language for practical reasons because he believed it would inevitably become Europe's lingua franca.

The text of "Li Europan lingues" is found in many HTML templates and through copying and uploading of templates this phrase seems to have found its use in many websites.

Don Harlow posted a message to the Auxlang List on 5 August 2006, mentioning its appearance in the "CSS Cookbook" from O'Reilly by Christopher Schmitt, and in templates of webpages which implement CSS.




</doc>
<doc id="1187874" url="https://en.wikipedia.org/wiki?curid=1187874" title="Speech balloon">
Speech balloon

Speech balloons (also speech bubbles, dialogue balloons or word balloons) are a graphic convention used most commonly in comic books, comics and cartoons to allow words (and much less often, pictures) to be understood as representing the speech or thoughts of a given character in the comic. There is often a formal distinction between the balloon that indicates thoughts and the one that indicates words spoken aloud: the balloon that conveys thoughts is often referred to as a thought bubble.

One of the earliest antecedents to the modern speech bubble were the "speech scrolls", wispy lines that connected first-person speech to the mouths of the speakers in Mesoamerican art between 600 and 900 AD.
Earlier, paintings, depicting stories in subsequent frames, using descriptive text resembling bubbles-text, were used in murals, one such example witten in Greek, dating to the 2nd century, found in Capitolias, today in Jordan.

In Western graphic art, labels that reveal what a pictured figure is saying have appeared since at least the 13th century. These were in common European use by the early 16th century. Word balloons (also known as "banderoles") began appearing in 18th-century printed broadsides, and political cartoons from the American Revolution (including some published by Benjamin Franklin) often used them – as did cartoonist James Gillray on the other side of the Atlantic. They later fell out of fashion, but by 1904 had regained their popularity, although they were still considered novel enough to require explanation. With the development of the comics industry in the 20th century, the appearance of speech balloons has become increasingly standardized, though the formal conventions that have evolved in different cultures (USA as opposed to Japan, for example), can be quite distinct.

Richard F. Outcault's Yellow Kid is generally credited as the first American comic strip character. His words initially appeared on his yellow shirt, but word balloons very much like those in use today were added almost immediately, as early as 1896. By the start of the 20th century, word balloons were ubiquitous; since that time, few American comic strips and comic books have relied on captions, notably Hal Foster's "Prince Valiant" and the early "Tarzan" comic strip in the 1930s. In Europe, where text comics were more common, speech balloons slowly caught on, with well-known examples being Alain Saint-Ogan's "Zig et Puce" (1925), Hergé's "The Adventures of Tintin" (1929) and Rob-Vel's "Spirou" (1938).

The most common is the speech bubble. It comes in two forms for two circumstances: an in-panel character and an off-panel character. An in-panel character (one who is fully or mostly visible in the panel of the strip of comic that the reader is viewing) uses a bubble with a pointer, called a tail, directed towards the speaker.

When one character has multiple balloons within a panel, often only the balloon nearest to the speaker's head has a tail, and the others are connected to it in sequence by narrow bands. This style is often used in "Mad Magazine", due to its "call-and-response" dialogue-based humor.

An off-panel character (the comic book equivalent of being "off screen") has several options, some of them rather unconventional. The first is a standard speech bubble with a tail pointing toward the speaker's position. The second option, which originated in manga, has the tail pointing "into" the bubble, instead of out. (This tail is still pointing towards the speaker.) The third option replaces the tail with a sort of bottleneck that connects with the side of the panel. It can be seen in the works of Marjane Satrapi (author of "Persepolis").

In American comics, a bubble without a tail means that the speaker is not merely outside the reader's field of view but invisible to the viewpoint character, often as an unspecified member of a crowd.

Characters distant (in space or time) from the scene of the panel can still speak, in squared bubbles without a tail; this usage, equivalent to voice-over in film, is not uncommon in American comics for dramatic contrast. In contrast to captions, the corners of such balloons never coincide with those of the panel; for further distinction they often have a double outline, a different background color, or quotation marks.

Thought bubbles come in two forms: the chain thought bubble and the "fuzzy" bubble.

The chain thought bubble is the almost universal symbol for thinking in cartoons. It consists of a large, cloud-like bubble containing the text of the thought, with a chain of increasingly smaller circular bubbles leading to the character. Some artists use an elliptical bubble instead of a cloud-shaped one.

Often animal characters like Snoopy and Garfield "talk" using thought bubbles. Thought bubbles may also be used in circumstances when a character is gagged or otherwise unable to speak.

Another, less conventional thought bubble has emerged: the "fuzzy" thought bubble. Used in manga (by such artists as Ken Akamatsu), the fuzzy bubble is roughly circular in shape (generally), but the edge of the bubble is not a line but a collection of spikes close to each other, creating the impression of fuzziness. Fuzzy thought bubbles do not use tails, and are placed near the character who is thinking. This has the advantage of reflecting the TV equivalent effect: something said with an echo.

Writers and artists can refuse to use thought bubbles, expressing the action through spoken dialogue and drawing; they are sometimes seen as an inefficient method of expressing thought because they are attached directly to the head of the thinker, unlike methods such as caption boxes, which can be used both as an expression of thought and narration while existing in an entirely different panel from the character thinking. However, they are restricted to the current viewpoint character. An example is Alan Moore and David Lloyd's "V for Vendetta", wherein during one chapter, a monologue expressed in captions serves not only to express the thoughts of a character but also the mood, status and actions of three others.

The shape of a speech balloon can be used to convey further information. Common ones include the following:

Captions are generally used for narration purposes, such as showing location and time, or conveying editorial commentary. They are generally rectangular and positioned near the edge of the panel. Often they are also colored to indicate the difference between themselves and the word balloons used by the characters, which are almost always white. Increasingly in modern comics, captions are frequently used to convey an internal monologue or typical speech.

Some characters and strips use highly unconventional methods of communication. Perhaps the most notable is the Yellow Kid, an early American comic strip. His (but not the other characters') words would appear on his large, smock-like shirt. A short-run American animated TV series of the early 1980s used this same concept, but with changing phrases on the "T-shirts" worn by the animal-based characters, depending on the characters' thoughts. 

Also noteworthy are the many variations on the form created by Dave Sim for his comic "Cerebus the Aardvark". Depending on the shape, size, and position of the bubble, as well as the texture and shape of the letters within it, Sim could convey large amounts of information about the speaker. This included separate bubbles for different states of mind (drunkenness, etc.), for echoes, and a special class of bubbles for one single floating apparition.

An early pioneer in experimenting with many different types of speech balloons and lettering for different types of speech was Walt Kelly, in his "Pogo" strip. Deacon Mushrat speaks in blackletter, P.T. Bridgeport speaks in circus posters, Sarcophagus MacAbre speaks in condolence cards, "Mr. Pig" (a take on Nikita Khrushchev) speaks in faux Cyrillic, etc.

In the famous French comic series "Asterix", Goscinny and Uderzo use bubbles without tails to indicate a distant or unseen speaker. They have also experimented with using different types of lettering for characters of different nationalities to indicate they speak a different language that Asterix may not understand; Goths speak in blackletter, Greeks in angular lettering (though always understood by the Gaulish main characters, so it is more of an accent than a language), Norse with "Nørdic åccents", Egyptians in faux hieroglyphs (depictive illustrations and rebuses), etc. Another experiment with speech bubbles was exclusive to one book, "Asterix and the Roman Agent". The agent in question is a vile manipulator who creates discord in a group of people with a single innocent-sounding comment. His victims start quarreling and ultimately fighting each other while speaking in green-colored speech bubbles.

Font variation is a common tactic in comics. "The Sandman" series, written by Neil Gaiman and lettered by Todd Klein, features many characters whose speech bubbles are written with a font that is exclusive to them. For examples, the main character, the gloomy Dream, speaks in wavy-edged bubbles, completely black, with similarly wavy white lettering. His sister, the scatterbrained and whimsical Delirium speaks in bubbles in a many-colored explosive background with uneven lettering, and the irreverent raven Matthew speaks in a shaky angular kind of bubble with scratchy lettering. Other characters, such as John Dee, have special shapes of bubbles for their own.

In "Mad"s recurring "Monroe" comic strip, certain words are written larger or in unusual fonts for emphasis.

In manga, there is a tendency to include the speech necessary for the storyline in balloons, while small scribbles outside the balloons add side comments, often used for irony or to show that they're said in a much smaller voice. Satsuki Yotsuba in the manga series "Negima" is notable because she speaks almost "entirely" in side scribble.

Speech bubbles are used not only to include a character's words, but also emotions, voice inflections and unspecified language.

One of the universal emblems of the art of comics is the use of a single punctuation mark to depict a character's emotions, much more efficiently than any possible sentence. A speech bubble with a single big question mark (?) (often drawn by hand, not counted as part of the lettering) denotes confusion or ignorance. An exclamation mark (!) indicates surprise or terror. This device is broadly used in the European comic tradition, the Belgian artist Hergé's "The Adventures of Tintin" series being a good example. Sometimes, the punctuation marks stand alone above the character's head, with no bubble needed.

In manga, the ellipsis (i.e. three dots) is also used to express silence in a much more significant way than the mere absence of bubbles. This is specially seen when a character is supposed to say something, to indicate a stunned silence or when a sarcastic comment is expected by the reader. The ellipsis, along with the big drop of sweat on the character's temple—usually depicting shame, confusion, or embarrassment caused by other people's actions—is one of the Japanese graphic symbols that have taken root in comics all around the world, although they are still rare in Western tradition. Japanese even has a sound effect for "deafening silence", .

In many comic books, words that would be foreign to the narration but are displayed in translation for the reader are surrounded by angle brackets or chevrons .

Gilbert Hernandez's series about Palomar is written in English, but supposed to take place mainly in a Hispanic country. Thus, what is supposed to be representations of Spanish speech is written without brackets, but occasional actual English speech is written within brackets, to indicate that it is unintelligible to the main Hispanophone characters in the series.

Some comics will have the actual foreign language in the speech balloon, with the translation as a footnote; this is done with Latin aphorisms in "Asterix". In the webcomic "Stand Still, Stay Silent", in which characters may speak up to five different languages in the same scene, most dialogue is unmarked (languages mostly being inferred by who is speaking and to whom), but miniature flags indicate the language being spoken where this is relevant.

Another convention is to put the foreign speech in a distinctive lettering style; for example, "Asterix"<nowiki>'</nowiki>s Goths speak in blackletter.

Since the Japanese language uses two writing directionalities (vertical, which is the traditional direction; and horizontal, as most other languages), manga has a convention of representing translated foreign speech as horizontal text.

It is a convention in American comics that the sound of a snore is represented as a series of Z's, dating back at least to Rudolph Dirks' early 20th-century strip "The Katzenjammer Kids". This practice has even been reduced to a single letter Z, so that a speech bubble with this letter standing all alone (again, drawn by hand rather than a font type) means the character is sleeping in most humorous comics. This can be seen, for instance, in Charles Schulz's "Peanuts" comic strips.

Originally, the resemblance between the 'z' sound and that of a snore seemed exclusive to the English language, but the spread of American comics has made it a frequent feature in other countries. An exception to this is in Japanese manga, where the usual symbol for sleep is a large bubble coming out of the character's nose.

Singing characters usually have musical notes drawn into their word balloons. Archie Comics' Melody Valentine, a character in their "Josie and the Pussycats" comic, has musical notes drawn into her word balloons at all times, to convey that she speaks in a sing-song voice.

The above-mentioned Albert Uderzo in the "Asterix" series decorates speech bubbles with beautiful flowers depicting an extremely soft, sweet voice (usually preceding a violent outburst by the same character).

A stormy cloud with a rough lightning sticking out of it, either in a bubble or just floating above the character's head as a modified 'cloudy' thought bubble, depicts anger, not always verbally expressed.

Light bulbs are sometimes used when the character comes up with an idea or solution to a problem.

In the Western world, it is common to replace profanity with a string of nonsense symbols (&%$@*$#), sometimes called grawlixes. In comics that are usually addressed to children or teenagers, bad language is censored by replacing it with more or less elaborate drawings and expressionistic symbols. For example, instead of calling someone a swine, a pig is drawn in the speech bubble.

One example is the Spanish "Mortadelo" series, created by Francisco Ibáñez. Although not specifically addressed to children, "Mortadelo" was born during Francisco Franco's dictatorship, when censorship was the order of the day and the slightest attempt of rough language was prohibited. When Ibáñez's characters are angry, donkey heads, lightning, lavatories, billy goats and even faux Japanese characters are often seen in their bubbles.

When "Mortadelo" was portrayed on film by Spanish director Javier Fesser in 2003, one of the critiques made to his otherwise successful adaptation was the character's use of words that never appeared in the comics. Fesser claimed: "When you see a bubble speech containing a lightning falling on a pig, what do you imagine the character's saying?"

In order for comic strip and graphic novel dialogue to make sense, it has to be read in order. Thus, conventions have evolved in the order in which the communication bubbles are read. The individual bubbles are read in the order of the language. For example, in English, the bubbles are read from left to right in a panel, while in Japanese, it is the other way around. Sometimes the bubbles are "stacked", with two characters having multiple bubbles, one above the other. Such stacks are read from the top down. Poor use of speech balloons can unintentionally make the proper reading order ambiguous, confusing the reader.

Traditionally, a cartoonist or occupational letterer would draw in all the individual letters in the balloons and sound effects by hand. A modern alternative, used by most comics today and universal in English-translated manga, is to letter with computer programs. The fonts used usually emulate the style of hand-lettering.

Traditionally, most mainstream comic books are lettered entirely in upper-case, with a few exceptions:

When hand-lettering, upper-case lettering saves time and effort because it requires drawing only three guidelines, while mixed-case lettering requires five.

In a few comics, uppercase and lowercase are used as in ordinary writing. Since the mid-1980s, mixed case lettering has gradually become more widely used in mainstream books. Some comics, such as "Pearls Before Swine", also use lowercase speech to mark a distinctive accent (in this case, the male crocodiles’ accented speech, opposed to all other characters who use standard uppercase speech).

From 2002 to 2004, Marvel Comics experimented with mixed-case lettering in all its books. Most mainstream titles have since returned to traditional all upper-case lettering.

In many comics, although the lettering is entirely in capital letters, serif versions of "I" are used exclusively where a capital I would appear in normal print text, and a sanserif (i.e., a simple vertical line) is used in all other places. This reduces confusion with the number one, and also serves to indicate when the personal pronoun "I" is meant. This lettering convention can be seen in computer fonts such as Blambot's "DigitalStrip.ttf" and "AnimeAce.ttf" fonts, which make no other distinction between lower- and uppercase letters.

In several occasions, comics artists have used balloons (or similar narrative devices) as if they have true substance, usually for humorous meta-like purposes. In "Peanuts", for example, the notes played by Schroeder occasionally take substance and are used in various ways, including Christmas decorations or perches for birds. Sometimes balloons can be influenced by the strip's environment: in the Italian strip "Sturmtruppen" they freeze and crack when the temperature is very low, or an Archie comic strip where two men from Alaska remarked on how cold it was, by saying the speech balloons froze as they said them, and the words had to be thawed out to be heard.

In the Flemish "Suske en Wiske" series, on one occasion a thought bubble full of mathematical formulas is cut open with scissors and its contents emptied in a bag, to be saved for later (in a manner not unlike the pensieve in the Harry Potter series). In the same series, speech balloons are occasionally even held and blown up to function as actual balloons or the words of the speech bubble are occasionally shown coming out the side of the speech bubble, to signify that the speaker is moving so fast that their words can't keep up with them, i.e. at supersonic speed.

In the novel "Who Censored Roger Rabbit?", the last words of a murdered Toon (cartoon character) are found under his body in the form of a speech balloon.

Many digital artists generate speech balloons with general-purpose illustration software. Products like Comic Book Creator for Microsoft Windows, Comic Life for Mac OS X and Windows target the non-professional end of the market.




</doc>
<doc id="10956960" url="https://en.wikipedia.org/wiki?curid=10956960" title="Information and media literacy">
Information and media literacy

Information and media literacy (IML) enables people to show and make informed judgments as users of information and media, as well as to become skillful creators and producers of information and media messages in their own right.

Prior to the 1990s, the primary focus of information literacy was research skills. Media literacy, a study that emerged around the 1970s, traditionally focuses on the analysis and the delivery of information through various forms of media. Nowadays, the study of information literacy has been extended to include the study of media literacy in many countries like the UK, Australia and New Zealand. The term "information and media literacy" is used by UNESCO to differentiate the combined study from the existing study of information literacy. It is also referred to as information and communication technologies (ICT) in the United States. Educators such as Gregory Ulmer have also defined the field as electracy.

IML is a combination of information literacy and media literacy. The purpose of being information and media literate is to engage in a digital society; one needs to be able to understand, inquire, create, communicate and think critically. It is important to effectively access, organize, analyze, evaluate, and create messages in a variety of forms. The transformative nature of IML includes creative works and creating new knowledge; to publish and collaborate responsibly requires ethical, cultural and social understanding.

The IML learning capacities prepare students to be 21st century literate. According to Jeff Wilhelm (2000), "technology has everything to do with literacy. And being able to use the latest electronic technologies has everything to do with being literate." He supports his argument with J. David Bolter's statement that "if our students are not reading and composing with various electronic technologies, then they are illiterate. They are not just unprepared for the future; they are illiterate right now, in our current time and context".

Wilhelm's statement is supported by the 2005 Wired World Phase II (YCWW II) survey conducted by the Media Awareness Network of Canada on 5000 Grade 4 – 11 students. The key findings of the survey were:

Marc Prensky (2001) uses the term "digital native" to describe people who have been brought up in a digital world. The Internet has been a pervasive element of young people's home lives. 94% of kids reported that they had Internet access at home, and a significant majority (61%) had a high-speed connection.

By the time kids reach Grade 11, half of them (51 percent) have their own Internet-connected computer, separate and apart from the family computer. The survey also showed that young Canadians are now among the most wired in the world. Contrary to the earlier stereotype of the isolated and awkward computer nerd, today's wired kid is a social kid.

In general, many students are better networked through the use of technology than most teachers and parents, who may not understand the abilities of technology. Students are no longer limited to desktop computera. They may use mobile technologies to graph mathematical problems, research a question for social studies, text message an expert for information, or send homework to a drop box. Students are accessing information by using MSN, personal Web pages, Weblogs and social networking sites.

Many teachers continue the tradition of teaching of the past 50 years. Traditionally teachers have been the experts sharing their knowledge with children. Technology, and the learning tools it provides access to, forces us to change to being facilitators of learning. We have to change the stereotype of teacher as the expert who delivers information, and students as consumers of information, in order to meet the needs of digital students. Teachers not only need to learn to speak digital, but also to embrace the language of digital natives.

Language is generally defined as a system used to communicate in which symbols convey information. Digital natives can communicate fluently with digital devices and convey information in a way that was impossible without digital devices. People born prior to 1988 are sometimes referred to as "digital immigrants." They experience difficulty programming simple devices like a VCR. Digital immigrants do not start pushing buttons to make things work.

Learning a language is best done early in a child's development.

In acquiring a second language, Hyltenstam (1992) found that around the age of 6 and 7 seemed to be a cut-off point for bilinguals to achieve native-like proficiency. After that age, second language learners could get near-native-like-ness but their language would, while consisting of very few actual errors, have enough errors that would set them apart from the first language group. Although more recent research suggests this impact still exists up to 10 years of age.

Kindergarten and grades 1 and 2 are critical to student success as digital natives because not all students have a "digital"-rich childhood. Students learning technological skills before Grade 3 can become equivalently bilingual. "Language-minority students who cannot read and write proficiently in English cannot participate fully in American schools, workplaces, or society. They face limited job opportunities and earning power." Speaking "digital" is as important as being literate in order to participate fully in North American society and opportunities.

Many students are considered illiterate in media and information for various reasons. They may not see the value of media and information literacy in the 21st-century classroom. Others are not aware of the emergence of the new form of information. Educators need to introduce IML to these students to help them become media and information literate. Very little changes will be made if the educators are not supporting information and media literacy in their own classrooms.

Performance standards, the foundation to support them, and tools to implement them are readily available. Success will come when there is full implementation and equitable access are established. Shared vision and goals that focus on strategic actions with measurable results are also necessary.

When the staff and community, working together, identify and clarify their values, beliefs, assumptions, and perceptions about what they want children to know and be able to do, an important next step will be to discover which of these values and expectations will be achieved. Using the capacity tools to assess IML will allow students, staff and the community to reflect on how well students are meeting learning needs as related to technology.

The IML Performance standards allow data collection and analysis to evidence that student-learning needs are being met. After assessing student IML, three questions can be asked:


Teachers can use classroom assessment for learning to identify areas that might need increased focus and support. Students can use classroom assessment to set learning goals for themselves.

This integration of technology across the curriculum is a positive shift from computers being viewed as boxes to be learned to computers being used as technical communication tools. In addition, recent learning pedagogy recognizes the inclusion for students to be creators of knowledge through technology. International Society for Technology in Education (ISTE) has been developing a standard IML curriculum for the US and other countries by implementing the National Educational Technology Standards.

In the UK, IML has been promoted among educators through an information literacy website developed by several organizations that have been involved in the field.

IML is included in the Partnership for the 21st Century program sponsored by the US Department of Education. Special mandates have been provided to Arizona, Iowa, Kansas, Maine, New Jersey, Massachusetts, North Carolina, South Dakota, West Virginia and Wisconsin. Individual school districts, such as the Clarkstown Central School District, have also developed their own information literacy curriculum. ISTE has also produced the National Educational Technology Standards for Students, Teachers and Administrators.

In British Columbia, Canada, the Ministry of Education has de-listed the Information Technology K to 7 IRP as a stand-alone course. It is still expected that all the prescribed learning outcomes continue to be integrated.

This integration of technology across the curriculum is a positive shift from computers being viewed as boxes to be learned to computers being used as technical communication tools. In addition, recent learning pedagogy recognizes the inclusion for students to be creators of knowledge through technology. Unfortunately, there has been no clear direction to implement IML.

The BC Ministry of Education published the Information and Communications Technology Integration Performance Standards, Grades 5 to 10 ICTI in 2005. These standards provide performance standards expectations for Grade 5 to 10; however, they do not provide guidance for other grades, and the expectation for a Grade 5 and Grade 10 student are the same.

In the Arab region, media and information literacy was largely ignored up until 2011, when the Media Studies Program at the American University of Beirut, the Open Society Foundations and the Arab-US Association for Communication Educators (AUSACE) launched a regional conference themed "New Directions: Digital and Media Literacy". The conference attracted significant attention from Arab universities and scholars, who discussed obstacles and needs to advance media literacy in the Arab region, including developing curricula in Arabic, training faculty and promoting the field. 

Following up on that recommendation, the Media Studies Program at AUB and the Open Society Foundations in collaboration with the Salzburg Academy on Media and Global Change launched in 2013 the first regional initiative to develop, vitalize, and advance media literacy education in the Arab region. The Media and Digital Literacy Academy of Beirut (MDLAB) offered an annual two-week summer training program in addition to working year-round to develop media literacy curricula and programs. The academy is conducted in Arabic and English and brings pioneering international instructors and professionals to teach advanced digital and media literacy concepts to young Arab academics and graduate students from various fields. MDLAB hopes that the participating Arab academics will carry what they learned to their countries and institutions and offers free curricular material in Arabic and English, including media literacy syllabi, lectures, exercises, lesson plans, and multi-media material, to assist and encourage the integration of digital and media literacy into Arab university and school curricula. 

In recognition of MDLAB's accomplishments in advancing media literacy education in the Arab region, the founder of MDLAB received the 2015 UNESCO-UNAOC International Media and Information Literacy Award. 

Prior to 2013, only two Arab universities offered media literacy courses: the American University of Beirut (AUB) and the American University of Sharjah (AUS). Three years after the launch of MDLAB, over two dozen Arab universities incorporated media literacy education into their curricula, both as stand-alone courses or as modules injected into their existing media courses. Among the universities who have full-fledged media literacy courses (as of 2015) are Lebanese American University (Lebanon), Birzeit University (Palestine), University of Balamand (Lebanon), Damascus University (Syria), Rafik Hariri University (Lebanon), Notre Dame University (Lebanon), Ahram Canadian University (Egypt), American University of Beirut (Lebanon), American University of Sharjah (UAE), and Al Azm University (Lebanon). The first Arab school to adopt media literacy as part of its strategic plan is the International College (IC) in Lebanon. Efforts to introduce media literacy to the region's other universities and schools continues with the help of other international organizations, such as UNESCO, UNAOC, AREACORE, DAAD, and OSF.

In Singapore and Hong Kong, information literacy or information technology was listed as a formal curriculum.

One barrier to learning to read is the lack of books, while a barrier to learning IML is the lack of technology access. Highlighting the value of IML helps to identify existing barriers within school infrastructure, staff development, and support systems. While there is a continued need to work on the foundations to provide a sustainable and equitable access, the biggest obstacle is school climate.

Marc Prensky identifies one barrier as teachers viewing digital devices as distractions: "Let's admit the real reason that we ban cell phones is that, given the opportunity to use them, students would vote with their attention, just as adults would 'vote with their feet' by leaving the room when a presentation is not compelling."

The mindset of banning new technology, and fearing the bad things that can happen, can affect educational decisions. The decision to ban digital devices impacts students for the rest of their lives.

Any tool that is used poorly or incorrectly can be unsafe. Safety lessons are mandatory in industrial technology and science. Yet safety or ethical lessons are not mandatory to use technology.

Not all decisions in schools are measured by common ground beliefs. One school district in Ontario banned digital devices from their schools. Local schools have been looking at doing the same. These kinds of reactions are often about immediate actions and not about teaching, learning or creating solutions. Many barriers to IML exist.



</doc>
<doc id="22760983" url="https://en.wikipedia.org/wiki?curid=22760983" title="Linguistics">
Linguistics

Linguistics is the scientific study of language. It involves analysing language form, language meaning, and language in context. Linguists traditionally analyse human language by observing an interplay between sound and meaning. Linguistics also deals with the social, cultural, historical and political factors that influence language, through which linguistic and language-based context is often determined. Research on language through the sub-branches of historical and evolutionary linguistics also focuses on how languages change and grow, particularly over an extended period of time.

The earliest activities in the documentation and description of language have been attributed to the 6th-century-BC Indian grammarian Pāṇini who wrote a formal description of the Sanskrit language in his "".

Related areas of study include the disciplines of semiotics (the study of direct and indirect language through signs and symbols), literary criticism (the historical and ideological analysis of literature, cinema, art, or published material), translation (the conversion and documentation of meaning in written/spoken text from one language or dialect onto another), and speech-language pathology (a corrective method to cure phonetic disabilities and dis-functions at the cognitive level).

Historical linguistics is the study of language change over time particularly with regards to a specific language or group of languages. Historical linguistics was among the first sub-disciplines to emerge in linguistics, and was the most widely practised form of linguistics in the late 19th century. There was a shift of focus in the early twentieth century to the synchronic approach (the systemic study of the current stage in languages), but historical research remained a field of linguistic inquiry. Subfields include language change and grammaticalisation studies.

Western modern historical linguistics dates from the late 18th century. It grew out of the earlier discipline of philology, the study of ancient texts and documents dating back to antiquity.

At first, historical linguistics served as the cornerstone of comparative linguistics primarily as a tool for linguistic reconstruction. Scholars were concerned chiefly with establishing language families and reconstructing prehistoric proto-languages, using the comparative method and internal reconstruction. The focus was initially on the well-known Indo-European languages, many of which had long written histories; the scholars also studied the Uralic languages, another European language family for which less early written material exists. Since then, there has been significant comparative linguistic work expanding outside of European languages as well, such as on the Austronesian languages and various families of Native American languages, among many others. Comparative linguistics is now, however, only a part of a more broadly conceived discipline of historical linguistics. For the Indo-European languages, comparative study is now a highly specialized field. Most research is being carried out on the subsequent development of these languages, in particular, the development of the modern standard varieties.

Some scholars have undertaken studies attempting to establish super-families, linking, for example, Indo-European, Uralic, and other families into Nostratic. These attempts have not been accepted widely. The information necessary to establish relatedness becomes less available as the time depth is increased. The time-depth of linguistic methods is limited due to chance word resemblances and variations between language groups, but a limit of around 10,000 years is often assumed. The dating of the various proto-languages is also difficult; several methods are available for dating, but only approximate results can be obtained.

Syntax and morphology are branches of linguistics concerned with the order and structure of meaningful linguistic units such as words and morphemes. Syntacticians study the rules and constraints that govern how speakers of a language can organize words into sentences. Morphologists study similar rules for the order of morphemes—sub-word units such as prefixes and suffixes—and how they may be combined to form words.

While words, along with clitics, are generally accepted as being the smallest units of syntax, in most languages, if not all, many words can be related to other words by rules that collectively describe the grammar for that language. For example, English speakers recognize that the words "dog" and "dogs" are closely related, differentiated only by the plurality morpheme "-s", only found bound to noun phrases. Speakers of English, a fusional language, recognize these relations from their innate knowledge of English's rules of word formation. They infer intuitively that "dog" is to "dogs" as "cat" is to "cats"; and, in similar fashion, "dog" is to "dog catcher" as "dish" is to "dishwasher". By contrast, Classical Chinese has very little morphology, using almost exclusively unbound morphemes ("free" morphemes) and depending on word order to convey meaning. (Most words in modern Standard Chinese ["Mandarin"], however, are compounds and most roots are bound.) These are understood as grammars that represent the morphology of the language. The rules understood by a speaker reflect specific patterns or regularities in the way words are formed from smaller units in the language they are using, and how those smaller units interact in speech. In this way, morphology is the branch of linguistics that studies patterns of word formation within and across languages and attempts to formulate rules that model the knowledge of the speakers of those languages.

Phonological and orthographic modifications between a base word and its origin may be partial to literacy skills. Studies have indicated that the presence of modification in phonology and orthography makes morphologically complex words harder to understand and that the absence of modification between a base word and its origin makes morphologically complex words easier to understand. Morphologically complex words are easier to comprehend when they include a base word.

Polysynthetic languages, such as Chukchi, have words composed of many morphemes. The Chukchi word "təmeyŋəlevtpəγtərkən", for example, meaning "I have a fierce headache", is composed of eight morphemes "t-ə-meyŋ-ə-levt-pəγt-ə-rkən" that may be glossed. The morphology of such languages allows for each consonant and vowel to be understood as morphemes, while the grammar of the language indicates the usage and understanding of each morpheme.

The discipline that deals specifically with the sound changes occurring within morphemes is morphophonology.

Semantics and pragmatics are branches of linguistics concerned with meaning. These subfields have traditionally been divided by the role of linguistic and social context in the determination of meaning. Semantics in this conception is concerned with core meanings and pragmatics concerned with meaning in context. Pragmatics encompasses speech act theory, conversational implicature, talk in interaction and other approaches to language behavior in philosophy, sociology, linguistics and anthropology. Unlike semantics, which examines meaning that is conventional or "coded" in a given language, pragmatics studies how the transmission of meaning depends not only on structural and linguistic knowledge (grammar, lexicon, etc.) of the speaker and listener but also on the context of the utterance, any pre-existing knowledge about those involved, the inferred intent of the speaker, and other factors. In that respect, pragmatics explains how language users are able to overcome apparent ambiguity since meaning relies on the manner, place, time, etc. of an utterance.

Phonetics and phonology are branches of linguistics concerned with sounds (or the equivalent aspects of sign languages). Phonetics is largely concerned with the physical aspects of sounds such as their acoustics, production, and perception. Phonology is concerned with the linguistic abstractions and categorizations of sounds.

Languages exist on a wide continuum of conventionalization with blurry divisions between concepts such as dialects and languages. Languages can undergo internal changes which lead to the development of subvarieties such as linguistic registers, accents, and dialects. Similarly, languages can undergo changes caused by contact with speakers of other languages, and new language varieties may be born from these contact situations through the process of language genesis.

Contact varieties such as pidgins and creoles are language varieties that often arise in situations of sustained contact between communities that speak different languages. Pidgins are language varieties with limited conventionalization where ideas are conveyed through simplified grammars that may grow more complex as linguistic contact continues. Creole languages are language varieties similar to pidgins but with greater conventionalization and stability. As children grow up in contact situations, they may learn a local pidgin as their native language. Through this process of acquisition and transmission, new grammatical features and lexical items are created and introduced to fill gaps in the pidgin eventually developing into a complete language.

Not all language contact situations result in the development of a pidgin or creole, and researchers have studied the features of contact situations that make contact varieties more likely to develop. Often these varieties arise in situations of colonization and enslavement, where power imbalances prevent the contact groups from learning the other's language but sustained contact is nevertheless maintained. The subjugated language in the power relationship is the substrate language, while the dominant language serves as the superstrate. Often the words and lexicon of a contact variety come from the superstrate, making it the lexifier, while grammatical structures come from the substrate, but this is not always the case.

A dialect is a variety of language that is characteristic of a particular group among the language's speakers. The group of people who are the speakers of a dialect are usually bound to each other by social identity. This is what differentiates a dialect from a register or a discourse, where in the latter case, cultural identity does not always play a role. Dialects are speech varieties that have their own grammatical and phonological rules, linguistic features, and stylistic aspects, but have not been given an official status as a language. Dialects often move on to gain the status of a language due to political and social reasons. Other times, dialects remain marginalized, particularly when they are associated with marginalized social groups. Differentiation amongst dialects (and subsequently, languages) is based upon the use of grammatical rules, syntactic rules, and stylistic features, though not always on lexical use or vocabulary. The popular saying that "a language is a dialect with an army and navy" is attributed as a definition formulated by Max Weinreich.

"We may as individuals be rather fond of our own dialect. This should not make us think, though, that it is actually any better than any other dialect. Dialects are not good or bad, nice or nasty, right or wrong – they are just different from one another, and it is the mark of a civilised society that it tolerates different dialects just as it tolerates different races, religions and sexes."

When a dialect is documented sufficiently through the linguistic description of its grammar, which has emerged through the consensual laws from within its community, it gains political and national recognition through a country or region's policies. That is the stage when a language is considered a standard variety, one whose grammatical laws have now stabilised from within the consent of speech community participants, after sufficient evolution, improvisation, correction, and growth. The English language, besides perhaps the French language, may be examples of languages that have arrived at a stage where they are said to have become standard varieties.

As constructed popularly through the Sapir–Whorf hypothesis, relativists believe that the structure of a particular language is capable of influencing the cognitive patterns through which a person shapes his or her world view. Universalists believe that there are commonalities between human perception as there is in the human capacity for language, while relativists believe that this varies from language to language and person to person. While the Sapir–Whorf hypothesis is an elaboration of this idea expressed through the writings of American linguists Edward Sapir and Benjamin Lee Whorf, it was Sapir's student Harry Hoijer who termed it thus. The 20th century German linguist Leo Weisgerber also wrote extensively about the theory of relativity. Relativists argue for the case of differentiation at the level of cognition and in semantic domains. The emergence of cognitive linguistics in the 1980s also revived an interest in linguistic relativity. Thinkers like George Lakoff have argued that language reflects different cultural metaphors, while the French philosopher of language Jacques Derrida's writings, especially aboutdeconstruction, have been seen to be closely associated with the relativist movement in linguistics, for which he was heavily criticized in the media at the time of his death.

Linguistic structures are pairings of meaning and form. Any particular pairing of meaning and form is a Saussurean sign. For instance, the meaning "cat" is represented worldwide with a wide variety of different sound patterns (in oral languages), movements of the hands and face (in sign languages), and written symbols (in written languages). Linguistic patterns have proven their importance for the knowledge engineering field especially with the ever-increasing amount of available data.

Linguists focusing on structure attempt to understand the rules regarding language use that native speakers know (not always consciously). All linguistic structures can be broken down into component parts that are combined according to (sub)conscious rules, over multiple levels of analysis. For instance, consider the structure of the word "tenth" on two different levels of analysis. On the level of internal word structure (known as morphology), the word "tenth" is made up of one linguistic form indicating a number and another form indicating ordinality. The rule governing the combination of these forms ensures that the ordinality marker "th" follows the number "ten." On the level of sound structure (known as phonology), structural analysis shows that the "n" sound in "tenth" is made differently from the "n" sound in "ten" spoken alone. Although most speakers of English are consciously aware of the rules governing internal structure of the word pieces of "tenth", they are less often aware of the rule governing its sound structure. Linguists focused on structure find and analyze rules such as these, which govern how native speakers use language.

Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Modern frameworks that deal with the principles of grammar include structural and functional linguistics, and generative linguistics.

Sub-fields that focus on a grammatical study of language include the following.

Discourse is language as social practice (Baynham, 1995) and is a multilayered concept. As a social practice, discourse embodies different ideologies through written and spoken texts. Discourse analysis can examine or expose these ideologies. Discourse influences genre, which is chosen in response to different situations and finally, at micro level, discourse influences language as text (spoken or written) at the phonological or lexico-grammatical level. Grammar and discourse are linked as parts of a system. A particular discourse becomes a language variety when it is used in this way for a particular purpose, and is referred to as a register. There may be certain lexical additions (new words) that are brought into play because of the expertise of the community of people within a certain domain of specialization. Registers and discourses therefore differentiate themselves through the use of vocabulary, and at times through the use of style too. People in the medical fraternity, for example, may use some medical terminology in their communication that is specialized to the field of medicine. This is often referred to as being part of the "medical discourse", and so on.

The lexicon is a catalogue of words and terms that are stored in a speaker's mind. The lexicon consists of words and bound morphemes, which are parts of words that can't stand alone, like affixes. In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included. Lexicography, closely linked with the domain of semantics, is the science of mapping the words into an encyclopedia or a dictionary. The creation and addition of new words (into the lexicon) is called coining or neologization, and the new words are called neologisms.

It is often believed that a speaker's capacity for language lies in the quantity of words stored in the lexicon. However, this is often considered a myth by linguists. The capacity for the use of language is considered by many linguists to lie primarily in the domain of grammar, and to be linked with competence, rather than with the growth of vocabulary. Even a very small lexicon is theoretically capable of producing an infinite number of sentences.

Stylistics also involves the study of written, signed, or spoken discourse through varying speech communities, genres, and editorial or narrative formats in the mass media. It involves the study and interpretation of texts for aspects of their linguistic and tonal style. Stylistic analysis entails the analysis of description of particular dialects and registers used by speech communities. Stylistic features include rhetoric, diction, stress, satire, irony, dialogue, and other forms of phonetic variations. Stylistic analysis can also include the study of language in canonical works of literature, popular fiction, news, advertisements, and other forms of communication in popular culture as well. It is usually seen as a variation in communication that changes from speaker to speaker and community to community. In short, Stylistics is the interpretation of text.

In the 1960s, Jacques Derrida, for instance, further distinguished between speech and writing, by proposing that written language be studied as a linguistic medium of communication in itself. Palaeography is therefore the discipline that studies the evolution of written scripts (as signs and symbols) in language. The formal study of language also led to the growth of fields like psycholinguistics, which explores the representation and function of language in the mind; neurolinguistics, which studies language processing in the brain; biolinguistics, which studies the biology and evolution of language; and language acquisition, which investigates how children and adults acquire the knowledge of one or more languages.

A semiotic tradition of linguistic research considers language as arising from the interaction of a semantic system and a sign system. The organisation of linguistic levels is considered computational. Linguistics is essentially seen as relating to social and cultural studies because different languages are shaped in social interaction by the speech community. Frameworks representing the humanistic view of language include structural linguistics, among others. 

Structural analysis means dissecting each linguistic level: phonetic, morphological, syntactic, and discourse, to the smallest units which are then reconnected with structures within a hierarchy of structures and layers. Functional analysis adds to structural analysis the assignment of semantic and other functional roles that each unit may have, including semantic and pragmatic. 

Functional linguistics, or functional grammar, is a branch of structural linguistics. In the humanistic reference, the terms structuralism and functionalism are related to their meaning in other human sciences. The difference between formal and functional structuralism lies in the way that the two approaches explain why languages have the properties they have. Functional explanation entails the idea that language is a tool for communication, or that communication is the primary function of language. Linguistic forms are consequently explained by an appeal to their functional value, or usefulness. Other structuralist approaches take the perspective that form follows from the inner mechanisms of the binary or multilayered language system.

Other linguistics frameworks take as their starting point the notion that language is a biological phenomenon in humans. Generative Grammar is the study of an innate linguistic structure. In contrast to structural linguistics, Generative Grammar rejects the notion that meaning and social interaction affect language. Instead, all human languages are based on a crystallised structure which may have been caused by a mutation exclusively in humans. The study of linguistics is considered as the study of this hypothesised structure. 

Cognitive Linguistics, in contrast, rejects the notion of innate grammar, and studies the impact of cognitive constraints and biases on human language. Objects of study include frames, idealised cognitive models, and memes. A closely related approach is evolutionary linguistics which includes the study of linguistic units as cultural replicators. It is possible to study how language replicates and adapts to the mind of the individual or the speech community.

The generative versus evolutionary approach are sometimes called formalism and functionalism, respectively. This reference is however different from the use of the terms in human sciences.

Linguistics is primarily descriptive. Linguists describe and explain features of language without making subjective judgments on whether a particular feature or usage is "good" or "bad". This is analogous to practice in other sciences: a zoologist studies the animal kingdom without making subjective judgments on whether a particular species is "better" or "worse" than another.

Prescription, on the other hand, is an attempt to promote particular linguistic usages over others, often favouring a particular dialect or "acrolect". This may have the aim of establishing a linguistic standard, which can aid communication over large geographical areas. It may also, however, be an attempt by speakers of one language or dialect to exert influence over speakers of other languages or dialects (see Linguistic imperialism). An extreme version of prescriptivism can be found among censors, who attempt to eradicate words and structures that they consider to be destructive to society. Prescription, however, may be practised appropriately in language instruction, like in ELT, where certain fundamental grammatical rules and lexical items need to be introduced to a second-language speaker who is attempting to acquire the language.

The objective of describing languages is often to uncover cultural knowledge about communities. The use of anthropological methods of investigation on linguistic sources leads to the discovery of certain cultural traits among a speech community through its linguistic features. It is also widely used as a tool in language documentation, with an endeavour to curate endangered languages. However, linguistic inquiry now uses the anthropological method to understand cognitive, historical, sociolinguistic and historical processes that languages undergo as they change and evolve, as well as general anthropological inquiry uses the linguistic method to excavate into culture. In all aspects, anthropological inquiry usually uncovers the different variations and relativities that underlie the usage of language.

Most contemporary linguists work under the assumption that spoken data and signed data are more fundamental than written data. This is because

Nonetheless, linguists agree that the study of written language can be worthwhile and valuable. For research that relies on corpus linguistics and computational linguistics, written language is often much more convenient for processing large amounts of linguistic data. Large corpora of spoken language are difficult to create and hard to find, and are typically transcribed and written. In addition, linguists have turned to text-based discourse occurring in various formats of computer-mediated communication as a viable site for linguistic inquiry.

The study of writing systems themselves, graphemics, is, in any case, considered a branch of linguistics.

Before the 20th century, linguists analysed language on a diachronic plane, which was historical in focus. This meant that they would compare linguistic features and try to analyse language from the point of view of how it had changed between then and later. However, with Saussurean linguistics in the 20th century, the focus shifted to a more synchronic approach, where the study was more geared towards analysis and comparison between different language variations, which existed at the same given point of time.

At another level, the syntagmatic plane of linguistic analysis entails the comparison between the way words are sequenced, within the syntax of a sentence. For example, the article "the" is followed by a noun, because of the syntagmatic relation between the words. The paradigmatic plane on the other hand, focuses on an analysis that is based on the paradigms or concepts that are embedded in a given text. In this case, words of the same type or class may be replaced in the text with each other to achieve the same conceptual understanding.

Before the 20th century, the term "philology", first attested in 1716, was commonly used to refer to the study of language, which was then predominantly historical in focus. Since Ferdinand de Saussure's insistence on the importance of synchronic analysis, however, this focus has shifted and the term "philology" is now generally used for the "study of a language's grammar, history, and literary tradition", especially in the United States (where philology has never been very popularly considered as the "science of language").

Although the term "linguist" in the sense of "a student of language" dates from 1641, the term "linguistics" is first attested in 1847. It is now the usual term in English for the scientific study of language, though "linguistic science" is sometimes used.

Linguistics is a multi-disciplinary field of research that combines tools from natural sciences, social sciences, and the humanities. Many linguists, such as David Crystal, conceptualize the field as being primarily scientific. The term "linguist" applies to someone who studies language or is a researcher within the field, or to someone who uses the tools of the discipline to describe and analyse specific languages.

The formal study of language began in India with Pāṇini, the 6th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. Pāṇini's systematic classification of the sounds of Sanskrit into consonants and vowels, and word classes, such as nouns and verbs, was the first known instance of its kind. In the Middle East, Sibawayh, a non-Arab, made a detailed description of Arabic in AD 760 in his monumental work, "Al-kitab fi al-nahw" (, "The Book on Grammar"), the first known author to distinguish between sounds and phonemes (sounds as units of a linguistic system). Western interest in the study of languages began somewhat later than in the East, but the grammarians of the classical languages did not use the same methods or reach the same conclusions as their contemporaries in the Indic world. Early interest in language in the West was a part of philosophy, not of grammatical description. The first insights into semantic theory were made by Plato in his "Cratylus" dialogue, where he argues that words denote concepts that are eternal and exist in the world of ideas. This work is the first to use the word etymology to describe the history of a word's meaning. Around 280 BC, one of Alexander the Great's successors founded a university (see Musaeum) in Alexandria, where a school of philologists studied the ancient texts in and taught Greek to speakers of other languages. While this school was the first to use the word "grammar" in its modern sense, Plato had used the word in its original meaning as "téchnē grammatikḗ" (), the "art of writing", which is also the title of one of the most important works of the Alexandrine school by Dionysius Thrax. Throughout the Middle Ages, the study of language was subsumed under the topic of philology, the study of ancient languages and texts, practised by such educators as Roger Ascham, Wolfgang Ratke, and John Amos Comenius.

In the 18th century, the first use of the comparative method by William Jones sparked the rise of comparative linguistics. Bloomfield attributes "the first great scientific linguistic work of the world" to Jacob Grimm, who wrote "Deutsche Grammatik". It was soon followed by other authors writing similar comparative studies on other language groups of Europe. The study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt, of whom Bloomfield asserts:

This study received its foundation at the hands of the Prussian statesman and scholar Wilhelm von Humboldt (1767–1835), especially in the first volume of his work on Kavi, the literary language of Java, entitled "Über die Verschiedenheit des menschlichen Sprachbaues und ihren Einfluß auf die geistige Entwickelung des Menschengeschlechts" ("On the Variety of the Structure of Human Language and its Influence upon the Mental Development of the Human Race").

There was a shift of focus from historical and comparative linguistics to synchronic analysis in early 20th century. Structural analysis was improved by Leonard Bloomfield, Louis Hjelmslev; and Zellig Harris who also developed methods of discourse analysis. Functional analysis was developed by the Prague linguistic circle and André Martinet. As sound recording devices became commonplace in the 1960s, dialectal recordings were made and archived, and the audio-lingual method provided a technological solution to foreign language learning. The 1960s also saw a new rise of comparative linguistics: the study of language universals in linguistic typology. Towards the end of the century the field of linguistics became divided into further areas of interest with the advent of language technology and digitalised corpora.

Ecolinguistics explores the role of language in the life-sustaining interactions of humans, other species and the physical environment. The first aim is to develop linguistic theories which see humans not only as part of society, but also as part of the larger ecosystems that life depends on. The second aim is to show how linguistics can be used to address key ecological issues, from climate change and biodiversity loss to environmental justice.

Sociolinguistics is the study of how language is shaped by social factors. This sub-discipline focuses on the synchronic approach of linguistics, and looks at how a language in general, or a set of languages, display variation and varieties at a given point in time. The study of language variation and the different varieties of language through dialects, registers, and idiolects can be tackled through a study of style, as well as through analysis of discourse. Sociolinguists research both style and discourse in language, as well as the theoretical factors that are at play between language and society.

Developmental linguistics is the study of the development of linguistic ability in individuals, particularly the acquisition of language in childhood. Some of the questions that developmental linguistics looks into is how children acquire different languages, how adults can acquire a second language, and what the process of language acquisition is.

Neurolinguistics is the study of the structures in the human brain that underlie grammar and communication. Researchers are drawn to the field from a variety of backgrounds, bringing along a variety of experimental techniques as well as widely varying theoretical perspectives. Much work in neurolinguistics is informed by models in psycholinguistics and theoretical linguistics, and is focused on investigating how the brain can implement the processes that theoretical and psycholinguistics propose are necessary in producing and comprehending language. Neurolinguists study the physiological mechanisms by which the brain processes information related to language, and evaluate linguistic and psycholinguistic theories, using aphasiology, brain imaging, electrophysiology, and computer modelling. Amongst the structures of the brain involved in the mechanisms of neurolinguistics, the cerebellum which contains the highest numbers of neurons has a major role in terms of predictions required to produce language.

Linguists are largely concerned with finding and describing the generalities and varieties both within particular languages and among all languages. Applied linguistics takes the results of those findings and "applies" them to other areas. Linguistic research is commonly applied to areas such as language education, lexicography, translation, language planning, which involves governmental policy implementation related to language use, and natural language processing. "Applied linguistics" has been argued to be something of a misnomer. Applied linguists actually focus on making sense of and engineering solutions for real-world linguistic problems, and not literally "applying" existing technical knowledge from linguistics. Moreover, they commonly apply technical knowledge from multiple sources, such as sociology (e.g., conversation analysis) and anthropology. (Constructed language fits under Applied linguistics.)

Today, computers are widely used in many areas of applied linguistics. Speech synthesis and speech recognition use phonetic and phonemic knowledge to provide voice interfaces to computers. Applications of computational linguistics in machine translation, computer-assisted translation, and natural language processing are areas of applied linguistics that have come to the forefront. Their influence has had an effect on theories of syntax and semantics, as modelling syntactic and semantic theories on computers constraints.

Linguistic analysis is a sub-discipline of applied linguistics used by many governments to verify the claimed nationality of people seeking asylum who do not hold the necessary documentation to prove their claim. This often takes the form of an interview by personnel in an immigration department. Depending on the country, this interview is conducted either in the asylum seeker's native language through an interpreter or in an international "lingua franca" like English. Australia uses the former method, while Germany employs the latter; the Netherlands uses either method depending on the languages involved. Tape recordings of the interview then undergo language analysis, which can be done either by private contractors or within a department of the government. In this analysis, linguistic features of the asylum seeker are used by analysts to make a determination about the speaker's nationality. The reported findings of the linguistic analysis can play a critical role in the government's decision on the refugee status of the asylum seeker.

Semiotics is the study of sign processes (semiosis), or signification and communication, signs, and symbols, both individually and grouped into sign systems, including the study of how meaning is constructed and understood. Semioticians often do not restrict themselves to linguistic communication when studying the use of signs but extend the meaning of "sign" to cover all kinds of cultural symbols. Nonetheless, semiotic disciplines closely related to linguistics are literary studies, discourse analysis, text linguistics, and philosophy of language. Semiotics, within the linguistics paradigm, is the study of the relationship between language and culture. Historically, Edward Sapir and Ferdinand De Saussure's structuralist theories influenced the study of signs extensively until the late part of the 20th century, but later, post-modern and post-structural thought, through language philosophers including Jacques Derrida, Mikhail Bakhtin, Michel Foucault, and others, have also been a considerable influence on the discipline in the late part of the 20th century and early 21st century. These theories emphasize the role of language variation, and the idea of subjective usage, depending on external elements like social and cultural factors, rather than merely on the interplay of formal elements.

Language documentation combines anthropological inquiry (into the history and culture of language) with linguistic inquiry, in order to describe languages and their grammars. Lexicography involves the documentation of words that form a vocabulary. Such a documentation of a linguistic vocabulary from a particular language is usually compiled in a dictionary. Computational linguistics is concerned with the statistical or rule-based modeling of natural language from a computational perspective. Specific knowledge of language is applied by speakers during the act of translation and interpretation, as well as in language education – the teaching of a second or foreign language. Policy makers work with governments to implement new plans in education and teaching which are based on linguistic research.

Since the inception of the discipline of linguistics, linguists have been concerned with describing and analysing previously undocumented languages. Starting with Franz Boas in the early 1900s, this became the main focus of American linguistics until the rise of formal structural linguistics in the mid-20th century. This focus on language documentation was partly motivated by a concern to document the rapidly disappearing languages of indigenous peoples. The ethnographic dimension of the Boasian approach to language description played a role in the development of disciplines such as sociolinguistics, anthropological linguistics, and linguistic anthropology, which investigate the relations between language, culture, and society.

The emphasis on linguistic description and documentation has also gained prominence outside North America, with the documentation of rapidly dying indigenous languages becoming a primary focus in many university programmes in linguistics. Language description is a work-intensive endeavour, usually requiring years of field work in the language concerned, so as to equip the linguist to write a sufficiently accurate reference grammar. Further, the task of documentation requires the linguist to collect a substantial corpus in the language in question, consisting of texts and recordings, both sound and video, which can be stored in an accessible format within open repositories, and used for further research.

The sub-field of translation includes the translation of written and spoken texts across media, from digital to print and spoken. To translate literally means to transmute the meaning from one language into another. Translators are often employed by organizations such as travel agencies and governmental embassies to facilitate communication between two speakers who do not know each other's language. Translators are also employed to work within computational linguistics setups like Google Translate, which is an automated program to translate words and phrases between any two or more given languages. Translation is also conducted by publishing houses, which convert works of writing from one language to another in order to reach varied audiences. Academic translators specialize in or are familiar with various other disciplines such as technology, science, law, economics, etc.

Biolinguistics is the study of the biology and evolution of language. It is a highly interdisciplinary field, including linguists, biologists, neuroscientists, psychologists, mathematicians, and others. By shifting the focus of investigation in linguistics to a comprehensive scheme that embraces the natural sciences, it seeks to yield a framework by which the fundamentals of the faculty of language are understood.

Clinical linguistics is the application of linguistic theory to the field of speech-language pathology. Speech language pathologists work on corrective measures to treat communication and swallowing disorders.

Chaika (1990) showed that people with schizophrenia who display speech disorders like rhyming inappropriately have attentional dysfunction, as when a patient was shown a color chip and then asked to identify it, responded "looks like clay. Sounds like gray. Take you for a roll in the hay. Heyday, May Day." The color chip was actually clay-colored, so his first response was correct.'

However, most people suppress or ignore words which rhyme with what they've said unless they are deliberately producing a pun, poem or rap. Even then, the speaker shows connection between words chosen for rhyme and an overall meaning in discourse. People with schizophrenia with speech dysfunction show no such relation between rhyme and reason. Some even produce stretches of gibberish combined with recognizable words.

Computational linguistics is the study of linguistic issues in a way that is "computationally responsible", i.e., taking careful note of computational consideration of algorithmic specification and computational complexity, so that the linguistic theories devised can be shown to exhibit certain desirable computational properties and their implementations. Computational linguists also work on computer language and software development.

Evolutionary linguistics is the interdisciplinary study of the emergence of the language faculty through human evolution, and also the application of evolutionary theory to the study of cultural evolution among different languages. It is also a study of the dispersal of various languages across the globe, through movements among ancient communities.

Forensic linguistics is the application of linguistic analysis to forensics. Forensic analysis investigates the style, language, lexical use, and other linguistic and grammatical features used in the legal context to provide evidence in courts of law. Forensic linguists have also used their expertise in the framework of criminal cases.



</doc>
