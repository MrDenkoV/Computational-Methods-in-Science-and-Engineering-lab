<doc id="4699587" url="https://en.wikipedia.org/wiki?curid=4699587" title="Fish">
Fish

Fish are gill-bearing aquatic craniate animals that lack limbs with digits. They form a sister group to the tunicates, together forming the olfactores. Included in this definition are the living hagfish, lampreys, and cartilaginous and bony fish as well as various extinct related groups. Tetrapods emerged within lobe-finned fishes, so cladistically they are fish as well. However, traditionally fish are rendered paraphyletic by excluding the tetrapods (i.e., the amphibians, reptiles, birds and mammals which all descended from within the same ancestry). Because in this manner the term "fish" is defined negatively as a paraphyletic group, it is not considered a formal taxonomic grouping in systematic biology, unless it is used in the cladistic sense, including tetrapods. The traditional term pisces (also ichthyes) is considered a typological, but not a phylogenetic classification.

The earliest organisms that can be classified as fish were soft-bodied chordates that first appeared during the Cambrian period. Although they lacked a true spine, they possessed notochords which allowed them to be more agile than their invertebrate counterparts. Fish would continue to evolve through the Paleozoic era, diversifying into a wide variety of forms. Many fish of the Paleozoic developed external armor that protected them from predators. The first fish with jaws appeared in the Silurian period, after which many (such as sharks) became formidable marine predators rather than just the prey of arthropods.

Most fish are ectothermic ("cold-blooded"), allowing their body temperatures to vary as ambient temperatures change, though some of the large active swimmers like white shark and tuna can hold a higher core temperature.

Fish can communicate in their underwater environments through the use of acoustic communication. Acoustic communication in fish involves the transmission of acoustic signals from one individual of a species to another. The production of sounds as a means of communication among fish is most often used in the context of feeding, aggression or courtship behaviour. The sounds emitted by fish can vary depending on the species and stimulus involved. They can produce either stridulatory sounds by moving components of the skeletal system, or can produce non-stridulatory sounds by manipulating specialized organs such as the swimbladder.

Fish are abundant in most bodies of water. They can be found in nearly all aquatic environments, from high mountain streams (e.g., char and gudgeon) to the abyssal and even hadal depths of the deepest oceans (e.g., cusk-eels and snailfish), although no species has yet been documented in the deepest 25% of the ocean. With 34,300 described species, fish exhibit greater species diversity than any other group of vertebrates.

Fish are an important resource for humans worldwide, especially as food. Commercial and subsistence fishers hunt fish in wild fisheries (see fishing) or farm them in ponds or in cages in the ocean (see aquaculture). They are also caught by recreational fishers, kept as pets, raised by fishkeepers, and exhibited in public aquaria. Fish have had a role in culture through the ages, serving as deities, religious symbols, and as the subjects of art, books and movies.

Fish, as vertebrata, developed as sister of the tunicata. As the tetrapods emerged deep within the fishes group, as sister of the lungfish, characteristics of fish are typically shared by tetrapods, including having vertebrae and a cranium.
Early fish from the fossil record are represented by a group of small, jawless, armored fish known as ostracoderms. Jawless fish lineages are mostly extinct. An extant clade, the lampreys may approximate ancient pre-jawed fish. The first jaws are found in Placodermi fossils. They lacked distinct teeth, having instead the oral surfaces of their jaw plates modified to serve the various purposes of teeth. The diversity of jawed vertebrates may indicate the evolutionary advantage of a jawed mouth. It is unclear if the advantage of a hinged jaw is greater biting force, improved respiration, or a combination of factors.

Fish may have evolved from a creature similar to a coral-like sea squirt, whose larvae resemble primitive fish in important ways. The first ancestors of fish may have kept the larval form into adulthood (as some sea squirts do today), although perhaps the reverse is the case.

Fish are a paraphyletic group: that is, any clade containing all fish also contains the tetrapods, which are not fish. For this reason, groups such as the class "Pisces" seen in older reference works are no longer used in formal classifications.
Traditional classification divides fish into three extant classes, and with extinct forms sometimes classified within the tree, sometimes as their own classes:

The above scheme is the one most commonly encountered in non-specialist and general works. Many of the above groups are paraphyletic, in that they have given rise to successive groups: Agnathans are ancestral to Chondrichthyes, who again have given rise to Acanthodiians, the ancestors of Osteichthyes. With the arrival of phylogenetic nomenclature, the fishes has been split up into a more detailed scheme, with the following major groups:

† – indicates extinct taxonSome palaeontologists contend that because Conodonta are chordates, they are primitive fish. For a fuller treatment of this taxonomy, see the vertebrate article.

The position of hagfish in the phylum Chordata is not settled. Phylogenetic research in 1998 and 1999 supported the idea that the hagfish and the lampreys form a natural group, the Cyclostomata, that is a sister group of the Gnathostomata.

The various fish groups account for more than half of vertebrate species. There are almost 28,000 known extant species, of which almost 27,000 are bony fish, with 970 sharks, rays, and chimeras and about 108 hagfish and lampreys. A third of these species fall within the nine largest families; from largest to smallest, these families are Cyprinidae, Gobiidae, Cichlidae, Characidae, Loricariidae, Balitoridae, Serranidae, Labridae, and Scorpaenidae. About 64 families are monotypic, containing only one species. The final total of extant species may grow to exceed 32,500.

The term "fish" most precisely describes any non-tetrapod craniate (i.e. an animal with a skull and in most cases a backbone) that has gills throughout life and whose limbs, if any, are in the shape of fins. Unlike groupings such as birds or mammals, fish are not a single clade but a paraphyletic collection of taxa, including hagfishes, lampreys, sharks and rays, ray-finned fish, coelacanths, and lungfish. Indeed, lungfish and coelacanths are closer relatives of tetrapods (such as mammals, birds, amphibians, etc.) than of other fish such as ray-finned fish or sharks, so the last common ancestor of all fish is also an ancestor to tetrapods. As paraphyletic groups are no longer recognised in modern systematic biology, the use of the term "fish" as a biological group must be avoided.

Many types of aquatic animals commonly referred to as "fish" are not fish in the sense given above; examples include shellfish, cuttlefish, starfish, crayfish and jellyfish. In earlier times, even biologists did not make a distinction – sixteenth century natural historians classified also seals, whales, amphibians, crocodiles, even hippopotamuses, as well as a host of aquatic invertebrates, as fish. However, according to the definition above, all mammals, including cetaceans like whales and dolphins, are not fish. In some contexts, especially in aquaculture, the true fish are referred to as finfish (or fin fish) to distinguish them from these other animals.

A typical fish is ectothermic, has a streamlined body for rapid swimming, extracts oxygen from water using gills or uses an accessory breathing organ to breathe atmospheric oxygen, has two sets of paired fins, usually one or two (rarely three) dorsal fins, an anal fin, and a tail fin, has jaws, has skin that is usually covered with scales, and lays eggs.

Each criterion has exceptions. Tuna, swordfish, and some species of sharks show some warm-blooded adaptations – they can heat their bodies significantly above ambient water temperature. Streamlining and swimming performance varies from fish such as tuna, salmon, and jacks that can cover 10–20 body-lengths per second to species such as eels and rays that swim no more than 0.5 body-lengths per second. Many groups of freshwater fish extract oxygen from the air as well as from the water using a variety of different structures. Lungfish have paired lungs similar to those of tetrapods, gouramis have a structure called the labyrinth organ that performs a similar function, while many catfish, such as "Corydoras" extract oxygen via the intestine or stomach. Body shape and the arrangement of the fins is highly variable, covering such seemingly un-fishlike forms as seahorses, pufferfish, anglerfish, and gulpers. Similarly, the surface of the skin may be naked (as in moray eels), or covered with scales of a variety of different types usually defined as placoid (typical of sharks and rays), cosmoid (fossil lungfish and coelacanths), ganoid (various fossil fish but also living gars and bichirs), cycloid, and ctenoid (these last two are found on most bony fish). There are even fish that live mostly on land or lay their eggs on land near water. Mudskippers feed and interact with one another on mudflats and go underwater to hide in their burrows. A single, undescribed species of "Phreatobius", has been called a true "land fish" as this worm-like catfish strictly lives among waterlogged leaf litter. Many species live in underground lakes, underground rivers or aquifers and are popularly known as cavefish.

Fish range in size from the huge whale shark to the tiny stout infantfish.

Fish species diversity is roughly divided equally between marine (oceanic) and freshwater ecosystems. Coral reefs in the Indo-Pacific constitute the center of diversity for marine fishes, whereas continental freshwater fishes are most diverse in large river basins of tropical rainforests, especially the Amazon, Congo, and Mekong basins. More than 5,600 fish species inhabit Neotropical freshwaters alone, such that Neotropical fishes represent about 10% of all vertebrate species on the Earth. Exceptionally rich sites in the Amazon basin, such as Cantão State Park, can contain more freshwater fish species than occur in all of Europe.

Most fish exchange gases using gills on either side of the pharynx. Gills consist of threadlike structures called filaments. Each filament contains a capillary network that provides a large surface area for exchanging oxygen and carbon dioxide. Fish exchange gases by pulling oxygen-rich water through their mouths and pumping it over their gills. In some fish, capillary blood flows in the opposite direction to the water, causing countercurrent exchange. The gills push the oxygen-poor water out through openings in the sides of the pharynx. Some fish, like sharks and lampreys, possess multiple gill openings. However, bony fish have a single gill opening on each side. This opening is hidden beneath a protective bony cover called an operculum.

Juvenile bichirs have external gills, a very primitive feature that they share with larval amphibians.

Fish from multiple groups can live out of the water for extended periods. Amphibious fish such as the mudskipper can live and move about on land for up to several days, or live in stagnant or otherwise oxygen depleted water. Many such fish can breathe air via a variety of mechanisms. The skin of anguillid eels may absorb oxygen directly. The buccal cavity of the electric eel may breathe air. Catfish of the families Loricariidae, Callichthyidae, and Scoloplacidae absorb air through their digestive tracts. Lungfish, with the exception of the Australian lungfish, and bichirs have paired lungs similar to those of tetrapods and must surface to gulp fresh air through the mouth and pass spent air out through the gills. Gar and bowfin have a vascularized swim bladder that functions in the same way. Loaches, trahiras, and many catfish breathe by passing air through the gut. Mudskippers breathe by absorbing oxygen across the skin (similar to frogs). A number of fish have evolved so-called accessory breathing organs that extract oxygen from the air. Labyrinth fish (such as gouramis and bettas) have a labyrinth organ above the gills that performs this function. A few other fish have structures resembling labyrinth organs in form and function, most notably snakeheads, pikeheads, and the Clariidae catfish family.

Breathing air is primarily of use to fish that inhabit shallow, seasonally variable waters where the water's oxygen concentration may seasonally decline. Fish dependent solely on dissolved oxygen, such as perch and cichlids, quickly suffocate, while air-breathers survive for much longer, in some cases in water that is little more than wet mud. At the most extreme, some air-breathing fish are able to survive in damp burrows for weeks without water, entering a state of aestivation (summertime hibernation) until water returns.

Air breathing fish can be divided into obligate air breathers and facultative air breathers. Obligate air breathers, such as the African lungfish, "must" breathe air periodically or they suffocate. Facultative air breathers, such as the catfish "Hypostomus plecostomus", only breathe air if they need to and will otherwise rely on their gills for oxygen. Most air breathing fish are facultative air breathers that avoid the energetic cost of rising to the surface and the fitness cost of exposure to surface predators.

Fish have a closed-loop circulatory system. The heart pumps the blood in a single loop throughout the body. In most fish, the heart consists of four parts, including two chambers and an entrance and exit. The first part is the sinus venosus, a thin-walled sac that collects blood from the fish's veins before allowing it to flow to the second part, the atrium, which is a large muscular chamber. The atrium serves as a one-way antechamber, sends blood to the third part, ventricle. The ventricle is another thick-walled, muscular chamber and it pumps the blood, first to the fourth part, bulbus arteriosus, a large tube, and then out of the heart. The bulbus arteriosus connects to the aorta, through which blood flows to the gills for oxygenation.

Jaws allow fish to eat a wide variety of food, including plants and other organisms. Fish ingest food through the mouth and break it down in the esophagus. In the stomach, food is further digested and, in many fish, processed in finger-shaped pouches called pyloric caeca, which secrete digestive enzymes and absorb nutrients. Organs such as the liver and pancreas add enzymes and various chemicals as the food moves through the digestive tract. The intestine completes the process of digestion and nutrient absorption.

As with many aquatic animals, most fish release their nitrogenous wastes as ammonia. Some of the wastes diffuse through the gills. Blood wastes are filtered by the kidneys.

Saltwater fish tend to lose water because of osmosis. Their kidneys return water to the body. The reverse happens in freshwater fish: they tend to gain water osmotically. Their kidneys produce dilute urine for excretion. Some fish have specially adapted kidneys that vary in function, allowing them to move from freshwater to saltwater.

The scales of fish originate from the mesoderm (skin); they may be similar in structure to teeth.

Fish typically have quite small brains relative to body size compared with other vertebrates, typically one-fifteenth the brain mass of a similarly sized bird or mammal. However, some fish have relatively large brains, most notably mormyrids and sharks, which have brains about as massive relative to body weight as birds and marsupials.

Fish brains are divided into several regions. At the front are the olfactory lobes, a pair of structures that receive and process signals from the nostrils via the two olfactory nerves. The olfactory lobes are very large in fish that hunt primarily by smell, such as hagfish, sharks, and catfish. Behind the olfactory lobes is the two-lobed telencephalon, the structural equivalent to the cerebrum in higher vertebrates. In fish the telencephalon is concerned mostly with olfaction. Together these structures form the forebrain.

Connecting the forebrain to the midbrain is the diencephalon (in the diagram, this structure is below the optic lobes and consequently not visible). The diencephalon performs functions associated with hormones and homeostasis. The pineal body lies just above the diencephalon. This structure detects light, maintains circadian rhythms, and controls color changes.

The midbrain (or mesencephalon) contains the two optic lobes. These are very large in species that hunt by sight, such as rainbow trout and cichlids.

The hindbrain (or metencephalon) is particularly involved in swimming and balance. The cerebellum is a single-lobed structure that is typically the biggest part of the brain. Hagfish and lampreys have relatively small cerebellae, while the mormyrid cerebellum is massive and apparently involved in their electrical sense.

The brain stem (or myelencephalon) is the brain's posterior. As well as controlling some muscles and body organs, in bony fish at least, the brain stem governs respiration and osmoregulation.

Most fish possess highly developed sense organs. Nearly all daylight fish have color vision that is at least as good as a human's (see vision in fishes). Many fish also have chemoreceptors that are responsible for extraordinary senses of taste and smell. Although they have ears, many fish may not hear very well. Most fish have sensitive receptors that form the lateral line system, which detects gentle currents and vibrations, and senses the motion of nearby fish and prey. Some fish, such as catfish and sharks, have the Ampullae of Lorenzini, organs that detect weak electric currents on the order of millivolt. Other fish, like the South American electric fishes Gymnotiformes, can produce weak electric currents, which they use in navigation and social communication.

Fish orient themselves using landmarks and may use mental maps based on multiple landmarks or symbols. Fish behavior in mazes reveals that they possess spatial memory and visual discrimination.

Vision is an important sensory system for most species of fish. Fish eyes are similar to those of terrestrial vertebrates like birds and mammals, but have a more spherical lens. Their retinas generally have both rods and cones (for scotopic and photopic vision), and most species have colour vision. Some fish can see ultraviolet and some can see polarized light. Amongst jawless fish, the lamprey has well-developed eyes, while the hagfish has only primitive eyespots. Fish vision shows adaptation to their visual environment, for example deep sea fishes have eyes suited to the dark environment.

Hearing is an important sensory system for most species of fish. Fish sense sound using their lateral lines and their ears.

New research has expanded preconceptions about the cognitive capacities of fish. For example, manta rays have exhibited behavior linked to self-awareness in mirror test cases. Placed in front of a mirror, individual rays engaged in contingency testing, that is, repetitive behavior aiming to check whether their reflection's behavior mimics their body movement.

Wrasses have also passed the mirror test in a 2018 scientific study.

Cases of tool use have also been noticed, notably in the Choerodon family, in archerfish and Atlantic cod.

Experiments done by William Tavolga provide evidence that fish have pain and fear responses. For instance, in Tavolga's experiments, toadfish grunted when electrically shocked and over time they came to grunt at the mere sight of an electrode.

In 2003, Scottish scientists at the University of Edinburgh and the Roslin Institute concluded that rainbow trout exhibit behaviors often associated with pain in other animals. Bee venom and acetic acid injected into the lips resulted in fish rocking their bodies and rubbing their lips along the sides and floors of their tanks, which the researchers concluded were attempts to relieve pain, similar to what mammals would do. Neurons fired in a pattern resembling human neuronal patterns.

Professor James D. Rose of the University of Wyoming claimed the study was flawed since it did not provide proof that fish possess "conscious awareness, particularly a kind of awareness that is meaningfully like ours". Rose argues that since fish brains are so different from human brains, fish are probably not conscious in the manner humans are, so that reactions similar to human reactions to pain instead have other causes. Rose had published a study a year earlier arguing that fish cannot feel pain because their brains lack a neocortex. However, animal behaviorist Temple Grandin argues that fish could still have consciousness without a neocortex because "different species can use different brain structures and systems to handle the same functions."

Animal welfare advocates raise concerns about the possible suffering of fish caused by angling. Some countries, such as Germany have banned specific types of fishing, and the British RSPCA now formally prosecutes individuals who are cruel to fish.

In 2019, scientists have shown that members of the monogamous species Amatitlania siquia exhibit pessimistic behavior when they are prevented from being with their partner.

Most fish move by alternately contracting paired sets of muscles on either side of the backbone. These contractions form S-shaped curves that move down the body. As each curve reaches the back fin, backward force is applied to the water, and in conjunction with the fins, moves the fish forward. The fish's fins function like an airplane's flaps. Fins also increase the tail's surface area, increasing speed. The streamlined body of the fish decreases the amount of friction from the water. Since body tissue is denser than water, fish must compensate for the difference or they will sink. Many bony fish have an internal organ called a swim bladder that adjusts their buoyancy through manipulation of gases.

Although most fish are exclusively ectothermic, there are exceptions. The only known bony fishes (infraclass Teleostei) that exhibit endothermy are in the suborder Scombroidei – which includes the billfishes, tunas, and the butterfly kingfish, a basal species of mackerel – and also the opah. The opah, a lampriform, was demonstrated in 2015 to utilize "whole-body endothermy", generating heat with its swimming muscles to warm its body while countercurrent exchange (as in respiration) minimizes heat loss. It is able to actively hunt prey such as squid and swim for long distances due to the ability to warm its entire body, including its heart, which is a trait typically found in only mammals and birds (in the form of homeothermy). In the cartilaginous fishes (class Chondrichthyes), sharks of the families Lamnidae (porbeagle, mackerel, salmon, and great white sharks) and Alopiidae (thresher sharks) exhibit endothermy. The degree of endothermy varies from the billfishes, which warm only their eyes and brain, to the bluefin tuna and the porbeagle shark, which maintain body temperatures in excess of above ambient water temperatures.

Endothermy, though metabolically costly, is thought to provide advantages such as increased muscle strength, higher rates of central nervous system processing, and higher rates of digestion.

Fish reproductive organs include testicles and ovaries. In most species, gonads are paired organs of similar size, which can be partially or totally fused. There may also be a range of secondary organs that increase reproductive fitness.

In terms of spermatogonia distribution, the structure of teleosts testes has two types: in the most common, spermatogonia occur all along the seminiferous tubules, while in atherinomorph fish they are confined to the distal portion of these structures. Fish can present cystic or semi-cystic spermatogenesis in relation to the release phase of germ cells in cysts to the seminiferous tubules lumen.

Fish ovaries may be of three types: gymnovarian, secondary gymnovarian or cystovarian. In the first type, the oocytes are released directly into the coelomic cavity and then enter the ostium, then through the oviduct and are eliminated. Secondary gymnovarian ovaries shed ova into the coelom from which they go directly into the oviduct. In the third type, the oocytes are conveyed to the exterior through the oviduct. Gymnovaries are the primitive condition found in lungfish, sturgeon, and bowfin. Cystovaries characterize most teleosts, where the ovary lumen has continuity with the oviduct. Secondary gymnovaries are found in salmonids and a few other teleosts.

Oogonia development in teleosts fish varies according to the group, and the determination of oogenesis dynamics allows the understanding of maturation and fertilization processes. Changes in the nucleus, ooplasm, and the surrounding layers characterize the oocyte maturation process.

Postovulatory follicles are structures formed after oocyte release; they do not have endocrine function, present a wide irregular lumen, and are rapidly reabsorbed in a process involving the apoptosis of follicular cells. A degenerative process called follicular atresia reabsorbs vitellogenic oocytes not spawned. This process can also occur, but less frequently, in oocytes in other development stages.

Some fish, like the California sheephead, are hermaphrodites, having both testes and ovaries either at different phases in their life cycle or, as in hamlets, have them simultaneously.

Over 97% of all known fish are oviparous, that is, the eggs develop outside the mother's body. Examples of oviparous fish include salmon, goldfish, cichlids, tuna, and eels. In the majority of these species, fertilisation takes place outside the mother's body, with the male and female fish shedding their gametes into the surrounding water. However, a few oviparous fish practice internal fertilization, with the male using some sort of intromittent organ to deliver sperm into the genital opening of the female, most notably the oviparous sharks, such as the horn shark, and oviparous rays, such as skates. In these cases, the male is equipped with a pair of modified pelvic fins known as claspers.

Marine fish can produce high numbers of eggs which are often released into the open water column. The eggs have an average diameter of .

The newly hatched young of oviparous fish are called larvae. They are usually poorly formed, carry a large yolk sac (for nourishment), and are very different in appearance from juvenile and adult specimens. The larval period in oviparous fish is relatively short (usually only several weeks), and larvae rapidly grow and change appearance and structure (a process termed metamorphosis) to become juveniles. During this transition larvae must switch from their yolk sac to feeding on zooplankton prey, a process which depends on typically inadequate zooplankton density, starving many larvae.

In ovoviviparous fish the eggs develop inside the mother's body after internal fertilization but receive little or no nourishment directly from the mother, depending instead on the yolk. Each embryo develops in its own egg. Familiar examples of ovoviviparous fish include guppies, angel sharks, and coelacanths.

Some species of fish are viviparous. In such species the mother retains the eggs and nourishes the embryos. Typically, viviparous fish have a structure analogous to the placenta seen in mammals connecting the mother's blood supply with that of the embryo. Examples of viviparous fish include the surf-perches, splitfins, and lemon shark. Some viviparous fish exhibit oophagy, in which the developing embryos eat other eggs produced by the mother. This has been observed primarily among sharks, such as the shortfin mako and porbeagle, but is known for a few bony fish as well, such as the halfbeak "Nomorhamphus ebrardtii". Intrauterine cannibalism is an even more unusual mode of vivipary, in which the largest embryos eat weaker and smaller siblings. This behavior is also most commonly found among sharks, such as the grey nurse shark, but has also been reported for "Nomorhamphus ebrardtii".

Aquarists commonly refer to ovoviviparous and viviparous fish as livebearers.

Acoustic communication in fish involves the transmission of acoustic signals from one individual of a species to another. The production of sounds as a means of communication among fish is most often used in the context of feeding, aggression or courtship behaviour.
The sounds emitted can vary depending on the species and stimulus involved. Fish can produce either stridulatory sounds by moving components of the skeletal system, or can produce non-stridulatory sounds by manipulating specialized organs such as the swimbladder.

There are some species of fish that can produce sounds by rubbing or grinding their bones together. These noises produced by bone-on-bone interactions are known as 'stridulatory sounds'.

An example of this is seen in "Haemulon flavolineatum", a species commonly referred to as the 'French grunt fish', as it produces a grunting noise by grinding its teeth together.
This behaviour is most pronounced when the "H. flavolineatum" is in distress situations. The grunts produced by this species of fishes generate a frequency of approximately 700 Hz, and last approximately 47 milliseconds. The "H. flavolineatum" does not emit sounds with frequencies greater than 1000 Hz, and does not detect sounds that have frequencies greater than 1050 Hz.

In a study conducted by Oliveira et al. (2014), the longsnout seahorse, "Hippocampus reidi", was recorded producing two different categories of sounds; ‘clicks’ and ‘growls’. The sounds emitted by the "H. reidi" are accomplished by rubbing their coronet bone across the grooved section of their neurocranium.
‘Clicking’ sounds were found to be primarily produced during courtship and feeding, and the frequencies of clicks were within the range of 50 Hz-800 Hz. The frequencies were noted to be on the higher end of the range during spawning periods, when the female and male fishes were less than fifteen centimeters apart. Growl sounds were produced when the "H. reidi" encountered stressful situations, such as handling by researchers. The ‘growl’ sounds consist of a series of sound pulses and are emitted simultaneously with body vibrations.

Some fish species create noise by engaging specialized muscles that contract and cause swimbladder vibrations.

Oyster toadfish produce loud grunting sounds by contracting muscles located along the sides of their swim bladder, known as sonic muscles
Female and male toadfishes emit short-duration grunts, often as a fright response. In addition to short-duration grunts, male toadfishes produce “boat whistle calls”. These calls are longer in duration, lower in frequency, and are primarily used to attract mates.
The sounds emitted by the "O. tao" have frequency range of 140 Hz to 260 Hz. The frequencies of the calls depend on the rate at which the sonic muscles contract.

The red drum, "Sciaenops ocellatus", produces drumming sounds by vibrating its swimbladder. Vibrations are caused by the rapid contraction of sonic muscles that surround the dorsal aspect of the swimbladder. These vibrations result in repeated sounds with frequencies that range from 100 to >200 Hz. The "S. Ocellatus" can produce different calls depending on the stimuli involved. The sounds created in courtship situations are different from those made during distressing events such as predatorial attacks. Unlike the males of the "S. Ocellatus" species, the females of this species don't produce sounds and lack sound-producing (sonic) muscles.

Like other animals, fish suffer from diseases and parasites. To prevent disease they have a variety of defenses. "Non-specific" defenses include the skin and scales, as well as the mucus layer secreted by the epidermis that traps and inhibits the growth of microorganisms. If pathogens breach these defenses, fish can develop an inflammatory response that increases blood flow to the infected region and delivers white blood cells that attempt to destroy pathogens. Specific defenses respond to particular pathogens recognised by the fish's body, i.e., an immune response. In recent years, vaccines have become widely used in aquaculture and also with ornamental fish, for example furunculosis vaccines in farmed salmon and koi herpes virus in koi.

Some species use cleaner fish to remove external parasites. The best known of these are the Bluestreak cleaner wrasses of the genus "Labroides" found on coral reefs in the Indian and Pacific oceans. These small fish maintain so-called "cleaning stations" where other fish congregate and perform specific movements to attract the attention of the cleaners. Cleaning behaviors have been observed in a number of fish groups, including an interesting case between two cichlids of the same genus, "Etroplus maculatus", the cleaner, and the much larger "Etroplus suratensis".

Immune organs vary by type of fish.
In the jawless fish (lampreys and hagfish), true lymphoid organs are absent. These fish rely on regions of lymphoid tissue within other organs to produce immune cells. For example, erythrocytes, macrophages and plasma cells are produced in the anterior kidney (or pronephros) and some areas of the gut (where granulocytes mature.) They resemble primitive bone marrow in hagfish.
Cartilaginous fish (sharks and rays) have a more advanced immune system. They have three specialized organs that are unique to Chondrichthyes; the epigonal organs (lymphoid tissue similar to mammalian bone) that surround the gonads, the Leydig's organ within the walls of their esophagus, and a spiral valve in their intestine. These organs house typical immune cells (granulocytes, lymphocytes and plasma cells). They also possess an identifiable thymus and a well-developed spleen (their most important immune organ) where various lymphocytes, plasma cells and macrophages develop and are stored.
Chondrostean fish (sturgeons, paddlefish, and bichirs) possess a major site for the production of granulocytes within a mass that is associated with the meninges (membranes surrounding the central nervous system.) Their heart is frequently covered with tissue that contains lymphocytes, reticular cells and a small number of macrophages. The chondrostean kidney is an important hemopoietic organ; where erythrocytes, granulocytes, lymphocytes and macrophages develop.

Like chondrostean fish, the major immune tissues of bony fish (or teleostei) include the kidney (especially the anterior kidney), which houses many different immune cells. In addition, teleost fish possess a thymus, spleen and scattered immune areas within mucosal tissues (e.g. in the skin, gills, gut and gonads). Much like the mammalian immune system, teleost erythrocytes, neutrophils and granulocytes are believed to reside in the spleen whereas lymphocytes are the major cell type found in the thymus. In 2006, a lymphatic system similar to that in mammals was described in one species of teleost fish, the zebrafish. Although not confirmed as yet, this system presumably will be where naive (unstimulated) T cells accumulate while waiting to encounter an antigen.

B and T lymphocytes bearing immunoglobulins and T cell receptors, respectively, are found in all jawed fishes. Indeed, the adaptive immune system as a whole evolved in an ancestor of all jawed vertebrate.

The 2006 IUCN Red List names 1,173 fish species that are threatened with extinction. Included are species such as Atlantic cod, Devil's Hole pupfish, coelacanths, and great white sharks. Because fish live underwater they are more difficult to study than terrestrial animals and plants, and information about fish populations is often lacking. However, freshwater fish seem particularly threatened because they often live in relatively small water bodies. For example, the Devil's Hole pupfish occupies only a single pool.

Overfishing is a major threat to edible fish such as cod and tuna. Overfishing eventually causes population (known as stock) collapse because the survivors cannot produce enough young to replace those removed. Such commercial extinction does not mean that the species is extinct, merely that it can no longer sustain a fishery.

One well-studied example of fishery collapse is the Pacific sardine "Sadinops sagax caerulues" fishery off the California coast. From a 1937 peak of the catch steadily declined to only in 1968, after which the fishery was no longer economically viable.

The main tension between fisheries science and the fishing industry is that the two groups have different views on the resiliency of fisheries to intensive fishing. In places such as Scotland, Newfoundland, and Alaska the fishing industry is a major employer, so governments are predisposed to support it. On the other hand, scientists and conservationists push for stringent protection, warning that many stocks could be wiped out within fifty years.

A key stress on both freshwater and marine ecosystems is habitat degradation including water pollution, the building of dams, removal of water for use by humans, and the introduction of exotic species. An example of a fish that has become endangered because of habitat change is the pallid sturgeon, a North American freshwater fish that lives in rivers damaged by human activity.

Introduction of non-native species has occurred in many habitats. One of the best studied examples is the introduction of Nile perch into Lake Victoria in the 1960s. Nile perch gradually exterminated the lake's 500 endemic cichlid species. Some of them survive now in captive breeding programmes, but others are probably extinct. Carp, snakeheads, tilapia, European perch, brown trout, rainbow trout, and sea lampreys are other examples of fish that have caused problems by being introduced into alien environments.

Throughout history, humans have utilized fish as a food source. Historically and today, most fish protein has come by means of catching wild fish. However, aquaculture, or fish farming, which has been practiced since about 3,500 BCE. in China, is becoming increasingly important in many nations. Overall, about one-sixth of the world's protein is estimated to be provided by fish. That proportion is considerably elevated in some developing nations and regions heavily dependent on the sea. In a similar manner, fish have been tied to trade.

Catching fish for the purpose of food or sport is known as fishing, while the organized effort by humans to catch fish is called a fishery. Fisheries are a huge global business and provide income for millions of people. The annual yield from all fisheries worldwide is about 154 million tons, with popular species including herring, cod, anchovy, tuna, flounder, and salmon. However, the term fishery is broadly applied, and includes more organisms than just fish, such as mollusks and crustaceans, which are often called "fish" when used as food.

Fish have been recognized as a source of beauty for almost as long as used for food, appearing in cave art, being raised as ornamental fish in ponds, and displayed in aquariums in homes, offices, or public settings.

"Recreational fishing" is fishing for pleasure or competition; it can be contrasted with commercial fishing, which is fishing for profit. The most common form of recreational fishing is done with a rod, reel, line, hooks and any one of a wide range of baits. Angling is a method of fishing, specifically the practice of catching fish by means of an "angle" (hook). Anglers must select the right hook, cast accurately, and retrieve at the right speed while considering water and weather conditions, species, fish response, time of the day, and other factors.

Fish themes have symbolic significance in many religions. In ancient Mesopotamia, fish offerings were made to the gods from the very earliest times. Fish were also a major symbol of Enki, the god of water. Fish frequently appear as filling motifs in cylinder seals from the Old Babylonian ( 1830 BC – 1531 BC) and Neo-Assyrian (911–609 BC) periods. Starting during the Kassite Period ( 1600 BC – 1155 BC) and lasting until the early Persian Period (550–30 BC), healers and exorcists dressed in ritual garb resembling the bodies of fish. During the Seleucid Period (312–63 BC), the legendary Babylonian culture hero Oannes, described by Berossus, was said to have dressed in the skin of a fish. Fish were sacred to the Syrian goddess Atargatis and, during her festivals, only her priests were permitted to eat them.

In the Book of Jonah, a work of Jewish literature probably written in the fourth century BC, the central figure, a prophet named Jonah, is swallowed by a giant fish after being thrown overboard by the crew of the ship he is travelling on. The fish later vomits Jonah out on shore after three days. This book was later included as part of the Hebrew Bible, or Christian Old Testament, and a version of the story it contains is summarized in Surah 37:139-148 of the Quran. Early Christians used the "ichthys", a symbol of a fish, to represent Jesus, because the Greek word for fish, ΙΧΘΥΣ Ichthys, could be used as an acronym for "Ίησοῦς Χριστός, Θεοῦ Υἱός, Σωτήρ" (Iesous Christos, Theou Huios, Soter), meaning "Jesus Christ, Son of God, Saviour". The gospels also refer to "fishers of men" and feeding the multitude. In the dhamma of Buddhism, the fish symbolize happiness as they have complete freedom of movement in the water. Often drawn in the form of carp which are regarded in the Orient as sacred on account of their elegant beauty, size and life-span.

Among the deities said to take the form of a fish are Ika-Roa of the Polynesians, Dagon of various ancient Semitic peoples, the shark-gods of Hawaii and Matsya of the Hindus. The astrological symbol Pisces is based on a constellation of the same name, but there is also a second fish constellation in the night sky, Piscis Austrinus.

Fish feature prominently in art and literature, in movies such as "Finding Nemo" and books such as "The Old Man and the Sea". Large fish, particularly sharks, have frequently been the subject of horror movies and thrillers, most notably the novel "Jaws", which spawned a series of films of the same name that in turn inspired similar films or parodies such as "Shark Tale" and "Snakehead Terror". Piranhas are shown in a similar light to sharks in films such as "Piranha"; however, contrary to popular belief, the red-bellied piranha is actually a generally timid scavenger species that is unlikely to harm humans. Legends of half-human, half-fish mermaids have featured in folklore, including the stories of Hans Christian Andersen.

Though often used interchangeably, in biology these words have different meanings. "Fish" is used as a singular noun, or as a plural to describe multiple individuals from a single species. "Fishes" is used to describe different species or species groups. Thus a pond that contained a single species might be said to contain 120 fish. But if the pond contained a total of 120 fish from three different species, it would be said to contain three fishes. The distinction is similar to that between people and peoples.


A random assemblage of fish merely using some localised resource such as food or nesting sites is known simply as an "aggregation". When fish come together in an interactive, social grouping, then they may be forming either a "shoal" or a "school" depending on the degree of organisation. A "shoal" is a loosely organised group where each fish swims and forages independently but is attracted to other members of the group and adjusts its behaviour, such as swimming speed, so that it remains close to the other members of the group. "Schools" of fish are much more tightly organised, synchronising their swimming so that all fish move at the same speed and in the same direction. Shoaling and schooling behaviour is believed to provide a variety of advantages.

Examples:

While the words "school" and "shoal" have different meanings within biology, the distinctions are often ignored by non-specialists who treat the words as synonyms. Thus speakers of British English commonly use "shoal" to describe any grouping of fish, and speakers of American English commonly use "school" just as loosely.




</doc>
<doc id="35843159" url="https://en.wikipedia.org/wiki?curid=35843159" title="Outline of fish">
Outline of fish

The following outline is provided as an overview of and topical guide to fish:

Fish – any member of a paraphyletic group of organisms that consist of all gill-bearing aquatic craniate animals that lack limbs with digits. Included in this definition are the living hagfish, lampreys, and cartilaginous and bony fish, as well as various extinct related groups. Most fish are ectothermic ("cold-blooded"), allowing their body temperatures to vary as ambient temperatures change, though some of the large active swimmers like white shark and tuna can hold a higher core temperature. Fish are abundant in most bodies of water. They can be found in nearly all aquatic environments, from high mountain streams (e.g., char and gudgeon) to the abyssal and even hadal depths of the deepest oceans (e.g., cusk-eel and snailfish). At 32,000 species, fish exhibit greater species diversity than any other group of vertebrates.

Fish can be described as all of the following:





Fish anatomy

Fish reproduction

Fish locomotion




Fish conservation







</doc>
<doc id="9727640" url="https://en.wikipedia.org/wiki?curid=9727640" title="Blind fish">
Blind fish

A blind fish is a fish without functional eyes. Most blind fish species are found in dark habitats such as the deep ocean, deep river channels and underground.



 




</doc>
<doc id="60622389" url="https://en.wikipedia.org/wiki?curid=60622389" title="Billfish in the Indian Ocean">
Billfish in the Indian Ocean

Of the twelve species of billfish, there are six species of Billfish in the Indian Ocean.

The term billfish refers to the large fishes of the family Istiophoridae, comprising marlin and sailfish, and of the family Xiphiidae, comprising swordfish. Billfish are epipelagic and highly migratory fishes found throughout the world's oceans typically inhabiting the coastal and offshore waters of tropical and temperate oceans. They are normally found in the upper 200 metres of water layers above the thermocline but may occur in  depths up to 800 metres. They migrate into temperate or cold waters for feeding and back to tropical waters for spawning.

They are “characterized by the prolongation of the upper jaw, much beyond the lower jaw into a long  rostrum which is flat and swordlike (swordfish) or rounded and spearlike (sailfishes, spearfishes and marlins).” All billfish species are dioecious with the females attaining larger sizes than males. Spawning occurs in tropical waters with eggs hatching into larvae which feed on planktons.

Billfishes are active and voracious apex predators using their long bill to attack and stun their prey by moving their heads in a sideways motion, knocking their prey unconscious and making it easier to catch. They  feed on a wide variety of tuna-like fishes,  cephalopods and crustaceans.

Of the twelve species of billfish, the following six species are found in the Indian Ocean which comprises the FAO major fishing areas 51 (Western Indian Ocean) and 57 (Eastern Indian Ocean). 

These six billfish species are not usually targeted by industrial and artisanal fisheries operating throughout the Indian Ocean but are caught and retained as a by-product. These species are important for localised small-scale and artisanal fisheries and recreational fishing.

Black marlin ("Makaira indica") inhabits the tropical and subtropical waters of the Indian Ocean between latitudes 25 N and 45 S. "M indica" is largely considered to be a bycatch  of industrial and artisanal fisheries and since the 1990s catches in the Indian Ocean have increased steadily, from 2,800 t in 1991 to over 10,000 t in 2004. In recent years catches have further increased sharply from around 15,000 t in 2013 to over 22,000 t in 2016 and 2017. Gillnets account for around 50% of total catches, followed by longlines (17%), with remaining catches by troll and handlines. Based on 2013-2017 catches, the main fleets are operated by India (27%), Iran (26%), Sri Lanka (18%) and Indonesia (14%).

The Indo-Pacific blue marlin ("Makaira mazara") is found primarily in tropical and subtropical waters of the Indian Ocean between latitudes 25 N and 40 to 45°S of the southwestern region and 35°S in the southeastern region.

"M mazara" is largely a non-target specie of industrial and artisanal fisheries. Western Indian Ocean is the main fishing area operated by longliners which account for 70% of total catches followed by gillnets (24%) with remaining catches by troll and handlines. Catches until the late 1970s were around 3,000 t to 4,000 t and steadily increased to over 10,000 t by the early 1990s and over 17,000 t by 2016. The main fleets, based on 2013-2017 catches, are operated by Taiwan,China (34%), Indonesia (31%), Pakistan (12%), Iran (9%) and Sri Lanka (6%).

Striped marlin ("Tetrapturus audax") is found in the Indian Ocean mainly in the tropical, subtropical and temperate waters between latitudes 25 N and as far south as 45°S in the southwestern region and 35°S in the southeastern region. "T audax" is considered to be a non-target specie of industrial fisheries with catch trends ranging from 2000 t to 8000 t per year. Most of the catch is taken in the north-west Indian Ocean, although catches dropped markedly between 2007-2011 due to piracy. Based on 2013-2017 catches, longlines account for around 66% of total catches followed by gillnets (27%). These catches were operated by fleets from Indonesia (37%), Taiwan, China (19%), Iran (16%) and Pakistan (8%).

Shortbill spearfish ("Tetrapturus angustirostris") is an oceanic pelagic fish which does not generally occur in coastal or enclosed waters but is found well offshore. It is found between latitudes 20°N to 35°- 45°S in the Indian Ocean.

The Indo-Pacific sailfish ("Istiophorus platypterus") is usually found above the thermocline between latitudes 20°N and 45°S in the western Indian Ocean and 35°S in the eastern Indian Ocean. The main fishing area is the north-west Indian Ocean around the Arabian Sea. Catches increased sharply from around 5,000 t in the mid-1990s to nearly 30,000 t as from 2011 with the development fisheries in Sri Lanka and Iran.

The Swordfish ("Xiphias gladius") is a large oceanic apex predator inhabiting all the world's oceans. It is found in the entire Indian Ocean down to latitude 45°S. Before the 1990s "X gladius" was mainly a non-targeted catch of industrial longline fisheries; but after 1990 catches increased from around 8,000 t to 36,000 t in 1998 with longliner fleets changing their targets from tunas to swordfish. Annual catches decreased during the mid 2000s in response to piracy threat in the area off Somalia but has improved since 2012 with the control of the threat.

Commercial fishing in the Indian Ocean started in the early 1950s with Japanese fishing yellowfin and bigeye tuna using longlines and they were followed in the late 1960s by fishing fleet from Taiwan, China and The Republic of Korea. Since 1980 European fleet from Spain, Portugal, France and UK started fishing tuna in the Indian Ocean using purse seine. Up to the early 1980s over 90% of billfishes were taken as incidental bycatch by long line vessels. This has decreased to between 50% to 70%  during the past 20 years as billfish catches from offshore gillnet fisheries have become more important for fleets from  Iran and Sri Lanka.

Total billfish catch which averaged 10,000Mt in the 1960s and 1970s increased to 75,000Mt  by the 2000s. The average catch for 2013-2017 was 113,000Mt with swordfish and Indo-Pacific sailfish accounting for around two thirds of total catches followed by black marlin, blue marlin and striped marlin. In the last few years, 75% of all billfish catches were recorded by five countries comprising Indonesia, Iran, India, Sri Lanka and Taiwan, China. The methods used for commercial fishing of billfish vary greatly by country with Indonesia, Taiwan China and Spain having mainly longline fleets whereas Iran and Pakistan primarily use gillnets.

Billfish are amongst the most prized fish in recreational fishing in the Indian Ocean, along with dorados and tunas. Sport fisheries mainly target Indo-Pacific sailfish, black and blue marlins. Main sport fisheries are located in West Australia, Kenya, Mauritius, La Reunion Island, Seychelles and  South Africa.

Sports fishermen go on private fishing boats and while at sea, they tend to follow school of seagulls, as these birds usually dive-hunt for small surface fish. Live baits such as bonitos are used as well as plastic lures mirroring the effects of swimming squids. The fishing trip can take hours usually ending around midday.

Tag and release is commonly used, as it positively impacts conservation and provides important information for the study of the specie. There has been over 55,000 recorded tags in East African Indian Ocean waters, mainly compromising of sailfish and marlins.

Global assessment of the status of billfish indicate the most of the stocks are experiencing high mortality rates with some species being overfished.  For the last two decades Indian Ocean billfish stocks have been subject to particularly intense fisheries pressure as a result of the increase in longline vessels up to the early 1980s which accounted for over 90% of the total billfish non-targeted catch. Presently all stocks have either been overfished or have been experiencing overfishing with swordfish being the exception. The declining billfish  stocks is as a result of the growing demand for their meat in the global market as well as for artisanal fisheries primarily in Sri Lanka and Indonesia.

A reduction in fishing pressure was noted in the 2000s attributed to the threats from pirates operating around the horn of Africa off the coast of Somalia. However, with the reduced threat of piracy in recent years, pressure on billfish stocks appears to be increasing again.

The rapid warming over the western Indian Ocean noted during the past six decades has led to a decrease of up to 20% in phytoplankton in this region.> The reduction in phytoplankton may add further pressure on the fish stock levels including billfish with the risk of decline to a point of no return.

Regional Fisheries Management Organisations (RFMOs) were established to manage and conserve tuna and billfish stocks due to their transnational distributions and widespread economic importance. The billfish population which is exploited or taken as bycatch by multinational fisheries is becoming difficult to regulate due to the non availability of accurate catch data which may not be collected or is aggregated with other species.

The Indian Ocean Tuna Commission (IOTC) was established as an intergovernmental RFMO in 1993 within the framework of  the Food and Agriculture Organisation (FAO) of the United Nations for the management of tunas and tuna like species in the Indian Ocean and adjacent seas under the “Agreement for the Establishment of the Indian Ocean Tuna Commission”. The principal objective of the IOTC as stated in Article V, para 1 is to ensure “…. through appropriate management, the conservation and optimum utilization of stocks covered by this Agreement and encouraging sustainable development of fisheries based on such stocks.”

Under this mandate the IOTC reviews the conditions and trends of tuna and billfishes stocks in the Indian Ocean, performs research on these stocks and adopts conservation and management measures.

Membership of the IOTC is open to coastal states of the Indian Ocean or states whose vessels engage in fishing in the Indian Ocean for stocks covered by the Agreement. There are presently 33 states who are members of the IOTC.

With the exception of swordfish, which is a target of commercial fisheries, the status of the other species of billfishes in the Indian Ocean is uncertain with their assessment and management becoming complicated and at times ineffective.

The billfish assessment carried out by the IOTC in 2017 established that the swordfish stock was not overfished and not subject to overfishing. The other four billfish species (black marlin, blue marlin, striped marlin and Indo-Pacific marlin) were either overfished or have been experiencing overfishing.

At its 22nd session held on 21–25 May 2018 the IOTC adopted Resolution 18/05 “On Management Measures for the conservation of the Billfishes: Striped Marlin, Black Marlin, Blue Marlin And Indo-Pacific Sailfish”. The Resolution set catch limits for these four billfishes in order not to exceed their Maximum Sustainable Yield (MSY). The IOTC will review the catch levels and would consider implementing additional conservation and management measures in the event that the catch levels were to exceed the set limits in any two consecutive years as from 2020 onwards.

As a result of the declining billfish population sport fishermen and conservationists have established tag and release program that tag, release and report the billfish catches. The use of tags and data gathered from recaptured billfish provides valuable scientific data on their growth rates, migratory patterns and stock assessment. The African Billfish Foundation was established in 1991 as a nonprofit organisation for conservation and research of the billfish species in the Indian Ocean.  It has an active tag and release program with an excess of 55,000 fish tagged. Additionally, the government of Seychelles is encouraging the use of small-scale fisheries in order to sustain the continuous increasing demand for ocean products

Although having strong measures and awareness campaigns, the billfish population is still under threat, with the blue marlin classified as vulnerable by the IUCN Red List.


</doc>
<doc id="62821224" url="https://en.wikipedia.org/wiki?curid=62821224" title="Phenotype modification">
Phenotype modification

Phenotype modification is the process of experimentally altering an organism's phenotype to investigate the impact of phenotype on the fitness.

Phenotype modification has been used to assess the impact of parasite mechanical presence on fish host behaviour.


</doc>
<doc id="63505097" url="https://en.wikipedia.org/wiki?curid=63505097" title="World Day for the End of Fishing">
World Day for the End of Fishing

The World Day for The End of Fishing (WoDEF) is an international campaign launched by animal rights activists demanding the end of fishing practices. It takes place the fourth Saturday of March every year.

The campaign was born in Switzerland and France in 2016 and took an international turn in 2017. It was first launched by the association "Pour L'Égalité Animale" (PEA).
Over the years and around the world lots of actions were organized for the WoDEF : street protests, sit-ins, screenings, conferences, fish counts, workshops, exhibitions, etc. in order to bring awareness to fish pain, sentience and fish intelligence.

In 2017, for its first edition, the event occurred in many cities around the world : in Lorient, Paris, Valence, Lyon, Lille, Montpellier, Saint Malo, Rennes (France), Geneva, Lausanne (Swiss), Brussells, Namur, Charleroi (Belgium), Montreal, Toronto (Canada), Stuttgart, Vogelsberg, Siegen, Hannover, Göttingen, Hamburg, Berlin (Germany), Lisbon (Portugal), Tel Aviv, Haifa (Israel), Melbourne (Australia), San Diego and Monterey Bay (United States).

In 2018, for its second edition, the event occurend once again internationally : in Canada, Australia, Belgium, Swiss, Peru, Sweden, United States, Germany, Japan, Brazil, France, Denmark, Mexico, United Kingdom and Panama.

Also, in 2018, an open letter signed by dozens of philosophers and scientists, including , Peter Singer, , Sue Donaldson, Will Kymlicka and Élise Desaulniers, was published in "Le Nouveau Magazine Littéraire" in order to highlight the WoDEF.

In 2019, for its third edition, activists from pierced their cheeks with fish hooks in a street protest for the WoDEF.

The organizers of the Word Day for the End of Fishing demand the abolition of all kinds of fishing practices for fish, crustaceans and cephalopods : the end of aquaculture, industrial or wild fishing, the use of marine animals as domestic pets, in scientific experiments and for entertainment, in antispeciesist perspective.

Protesters affirm that fish can feel pain and that it is unecessary to consume them. 



</doc>
<doc id="10772350" url="https://en.wikipedia.org/wiki?curid=10772350" title="History">
History

History (from Greek , "historia", meaning 'inquiry; knowledge acquired by investigation') is the past as it is described in written documents, and the study thereof. Events occurring before written records are considered prehistory. "History" is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Scholars who write about history are called historians.

History also includes the academic discipline which uses a narrative to examine and analyse a sequence of past events, and objectively determine the patterns of cause and effect that determine them. Historians sometimes debate the nature of history and its usefulness by discussing the study of the discipline as an end in itself and as a way of providing "perspective" on the problems of the present.

Stories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends, because they do not show the "disinterested investigation" required of the discipline of history. Herodotus, a 5th-century BC Greek historian is often considered within the Western tradition to be the "father of history", or by some the "father of lies", and, along with his contemporary Thucydides, helped form the foundations for the modern study of human history. Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In East Asia, a state chronicle, the Spring and Autumn Annals, was known to be compiled from as early as 722 BC although only 2nd-century BC texts have survived.

Ancient influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and the study of certain topical or thematical elements of historical investigation. Often history is taught as part of primary and secondary education, and the academic study of history is a major discipline in university studies.

The word "history" comes from the Ancient Greek ἱστορία ("historía"), meaning 'inquiry', 'knowledge from inquiry', or 'judge'. It was in that sense that Aristotle used the word in his "History of Animals." The ancestor word is attested early on in Homeric Hymns, Heraclitus, the Athenian ephebes' oath, and in Boiotic inscriptions (in a legal sense, either 'judge' or 'witness', or similar). The Greek word was borrowed into Classical Latin as "historia", meaning "investigation, inquiry, research, account, description, written account of past events, writing of history, historical narrative, recorded knowledge of past events, story, narrative". "History" was borrowed from Latin (possibly via Old Irish or Old Welsh) into Old English as "stær" ('history, narrative, story'), but this word fell out of use in the late Old English period. Meanwhile, as Latin became Old French (and Anglo-Norman), "historia" developed into forms such as "istorie", "estoire", and "historie", with new developments in the meaning: "account of the events of a person's life (beginning of the 12th century), chronicle, account of events as relevant to a group of people or people in general (1155), dramatic or pictorial representation of historical events (c. 1240), body of knowledge relative to human evolution, science (c. 1265), narrative of real or imaginary events, story (c. 1462)".

It was from Anglo-Norman that "history" was borrowed into Middle English, and this time the loan stuck. It appears in the 13th-century "Ancrene Wisse", but seems to have become a common word in the late 14th century, with an early attestation appearing in John Gower's "Confessio Amantis" of the 1390s (VI.1383): "I finde in a bok compiled | To this matiere an old histoire, | The which comth nou to mi memoire". In Middle English, the meaning of "history" was "story" in general. The restriction to the meaning "the branch of knowledge that deals with past events; the formal record or study of past events, esp. human affairs" arose in the mid-15th century. With the Renaissance, older senses of the word were revived, and it was in the Greek sense that Francis Bacon used the term in the late 16th century, when he wrote about "Natural History". For him, "historia" was "the knowledge of objects determined by space and time", that sort of knowledge provided by memory (while science was provided by reason, and poetry was provided by fantasy).

In an expression of the linguistic synthetic vs. analytic/isolating dichotomy, English like Chinese (史 vs. 诌) now designates separate words for human history and storytelling in general. In modern German, French, and most Germanic and Romance languages, which are solidly synthetic and highly inflected, the same word is still used to mean both 'history' and 'story'. "Historian" in the sense of a "researcher of history" is attested from 1531. In all European languages, the substantive "history" is still used to mean both "what happened with men", and "the scholarly study of the happened", the latter sense sometimes distinguished with a capital letter, or the word "historiography". The adjective "historical" is attested from 1661, and "historic" from 1669.

Historians write in the context of their own time, and with due regard to the current dominant ideas of how to interpret the past, and sometimes write to provide lessons for their own society. In the words of Benedetto Croce, "All history is contemporary history". History is facilitated by the formation of a "true discourse of past" through the production of narrative and analysis of past events relating to the human race. The modern discipline of history is dedicated to the institutional production of this discourse.

All events that are remembered and preserved in some authentic form constitute the historical record. The task of historical discourse is to identify the sources which can most usefully contribute to the production of accurate accounts of past. Therefore, the constitution of the historian's archive is a result of circumscribing a more general archive by invalidating the usage of certain texts and documents (by falsifying their claims to represent the "true past").

The study of history has sometimes been classified as part of the humanities and at other times as part of the social sciences. It can also be seen as a bridge between those two broad areas, incorporating methodologies from both. Some individual historians strongly support one or the other classification. In the 20th century, French historian Fernand Braudel revolutionized the study of history, by using such outside disciplines as economics, anthropology, and geography in the study of global history.

Traditionally, historians have recorded events of the past, either in writing or by passing on an oral tradition, and have attempted to answer historical questions through the study of written documents and oral accounts. From the beginning, historians have also used such sources as monuments, inscriptions, and pictures. In general, the sources of historical knowledge can be separated into three categories: what is written, what is said, and what is physically preserved, and historians often consult all three. But writing is the marker that separates history from what comes before.

Archaeology is a discipline that is especially helpful in dealing with buried sites and objects, which, once unearthed, contribute to the study of history. But archaeology rarely stands alone. It uses narrative sources to complement its discoveries. However, archaeology is constituted by a range of methodologies and approaches which are independent from history; that is to say, archaeology does not "fill the gaps" within textual sources. Indeed, "historical archaeology" is a specific branch of archaeology, often contrasting its conclusions against those of contemporary textual sources. For example, Mark Leone, the excavator and interpreter of historical Annapolis, Maryland, USA; has sought to understand the contradiction between textual documents and the material record, demonstrating the possession of slaves and the inequalities of wealth apparent via the study of the total historical environment, despite the ideology of "liberty" inherent in written documents at this time.

There are varieties of ways in which history can be organized, including chronologically, culturally, territorially, and thematically. These divisions are not mutually exclusive, and significant overlaps are often present, as in "The International Women's Movement in an Age of Transition, 1830–1975." It is possible for historians to concern themselves with both the very specific and the very general, although the modern trend has been toward specialization. The area called Big History resists this specialization, and searches for universal patterns or trends. History has often been studied with some practical or theoretical aim, but also may be studied out of simple intellectual curiosity.

The history of the world is the memory of the past experience of "Homo sapiens sapiens" around the world, as that experience has been preserved, largely in written records. By "prehistory", historians mean the recovery of knowledge of the past in an area where no written records exist, or where the writing of a culture is not understood. By studying painting, drawings, carvings, and other artifacts, some information can be recovered even in the absence of a written record. Since the 20th century, the study of prehistory is considered essential to avoid history's implicit exclusion of certain civilizations, such as those of Sub-Saharan Africa and pre-Columbian America. Historians in the West have been criticized for focusing disproportionately on the Western world. In 1961, British historian E. H. Carr wrote:

This definition includes within the scope of history the strong interests of peoples, such as Indigenous Australians and New Zealand Māori in the past, and the oral records maintained and transmitted to succeeding generations, even before their contact with European civilization.

Historiography has a number of related meanings. Firstly, it can refer to how history has been produced: the story of the development of methodology and practices (for example, the move from short-term biographical narrative towards long-term thematic analysis). Secondly, it can refer to what has been produced: a specific body of historical writing (for example, "medieval historiography during the 1960s" means "Works of medieval history written during the 1960s"). Thirdly, it may refer to why history is produced: the Philosophy of history. As a meta-level analysis of descriptions of the past, this third conception can relate to the first two in that the analysis usually focuses on the narratives, interpretations, world view, use of evidence, or method of presentation of other historians. Professional historians also debate the question of whether history can be taught as a single coherent narrative or a series of competing narratives.

The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history.

Herodotus of Halicarnassus (484 BC – ca.425 BC) has generally been acclaimed as the "father of history". However, his contemporary Thucydides (c. 460 BC – c. 400 BC) is credited with having first approached history with a well-developed historical method in his work the "History of the Peloponnesian War". Thucydides, unlike Herodotus, regarded history as being the product of the choices and actions of human beings, and looked at cause and effect, rather than as the result of divine intervention (though Herodotus was not wholly committed to this idea himself). In his historical method, Thucydides emphasized chronology, a nominally neutral point of view, and that the human world was the result of the actions of human beings. Greek historians also viewed history as cyclical, with events regularly recurring.

There were historical traditions and sophisticated use of historical method in ancient and medieval China. The groundwork for professional historiography in East Asia was established by the Han dynasty court historian known as Sima Qian (145–90 BC), author of the "Records of the Grand Historian" ("Shiji"). For the quality of his written work, Sima Qian is posthumously known as the Father of Chinese historiography. Chinese historians of subsequent dynastic periods in China used his "Shiji" as the official format for historical texts, as well as for biographical literature.

Saint Augustine was influential in Christian and Western thought at the beginning of the medieval period. Through the Medieval and Renaissance periods, history was often studied through a sacred or religious perspective. Around 1800, German philosopher and historian Georg Wilhelm Friedrich Hegel brought philosophy and a more secular approach in historical study.

In the preface to his book, the "Muqaddimah" (1377), the Arab historian and early sociologist, Ibn Khaldun, warned of seven mistakes that he thought that historians regularly committed. In this criticism, he approached the past as strange and in need of interpretation. The originality of Ibn Khaldun was to claim that the cultural difference of another age must govern the evaluation of relevant historical material, to distinguish the principles according to which it might be possible to attempt the evaluation, and lastly, to feel the need for experience, in addition to rational principles, in order to assess a culture of the past. Ibn Khaldun often criticized "idle superstition and uncritical acceptance of historical data." As a result, he introduced a scientific method to the study of history, and he often referred to it as his "new science". His historical method also laid the groundwork for the observation of the role of state, communication, propaganda and systematic bias in history, and he is thus considered to be the "father of historiography" or the "father of the philosophy of history".

In the West, historians developed modern methods of historiography in the 17th and 18th centuries, especially in France and Germany. In 1851, Herbert Spencer summarized these methods: 

By the "rich ore" Spencer meant scientific theory of history. Meanwhile, Henry Thomas Buckle expressed a dream of history becoming one day science: 

Contrary to Buckle's dream, the 19th-century historian with greatest influence on methods became Leopold von Ranke in Germany. He limited history to “what really happened” and by this directed the field further away from science. For Ranke, historical data should be collected carefully, examined objectively and put together with critical rigor. But these procedures “are merely the prerequisites and preliminaries of science. The heart of science is searching out order and regularity in the data being examined and in formulating generalizations or laws about them.”

In the 20th century, academic historians focused less on epic nationalistic narratives, which often tended to glorify the nation or great men, to more objective and complex analyses of social and intellectual forces. A major trend of historical methodology in the 20th century was a tendency to treat history more as a social science rather than as an art, which traditionally had been the case. Some of the leading advocates of history as a social science were a diverse collection of scholars which included Fernand Braudel, E. H. Carr, Fritz Fischer, Emmanuel Le Roy Ladurie, Hans-Ulrich Wehler, Bruce Trigger, Marc Bloch, Karl Dietrich Bracher, Peter Gay, Robert Fogel, Lucien Febvre and Lawrence Stone. Many of the advocates of history as a social science were or are noted for their multi-disciplinary approach. Braudel combined history with geography, Bracher history with political science, Fogel history with economics, Gay history with psychology, Trigger history with archaeology while Wehler, Bloch, Fischer, Stone, Febvre and Le Roy Ladurie have in varying and differing ways amalgamated history with sociology, geography, anthropology, and economics. Nevertheless, these multidisciplinary approaches failed to produce a theory of history. So far only one theory of history came from the pen of a professional Historian. Whatever other theories of history we have, they were written by experts from other fields (for example, Marxian theory of history). More recently, the field of digital history has begun to address ways of using computer technology to pose new questions to historical data and generate digital scholarship.

In sincere opposition to the claims of history as a social science, historians such as Hugh Trevor-Roper, John Lukacs, Donald Creighton, Gertrude Himmelfarb and Gerhard Ritter argued that the key to the historians' work was the power of the imagination, and hence contended that history should be understood as an art. French historians associated with the Annales School introduced quantitative history, using raw data to track the lives of typical individuals, and were prominent in the establishment of cultural history (cf. "histoire des mentalités"). Intellectual historians such as Herbert Butterfield, Ernst Nolte and George Mosse have argued for the significance of ideas in history. American historians, motivated by the civil rights era, focused on formerly overlooked ethnic, racial, and socio-economic groups. Another genre of social history to emerge in the post-WWII era was "Alltagsgeschichte" (History of Everyday Life). Scholars such as Martin Broszat, Ian Kershaw and Detlev Peukert sought to examine what everyday life was like for ordinary people in 20th-century Germany, especially in the Nazi period.

Marxist historians such as Eric Hobsbawm, E. P. Thompson, Rodney Hilton, Georges Lefebvre, Eugene Genovese, Isaac Deutscher, C. L. R. James, Timothy Mason, Herbert Aptheker, Arno J. Mayer and Christopher Hill have sought to validate Karl Marx's theories by analyzing history from a Marxist perspective. In response to the Marxist interpretation of history, historians such as François Furet, Richard Pipes, J. C. D. Clark, Roland Mousnier, Henry Ashby Turner and Robert Conquest have offered anti-Marxist interpretations of history. Feminist historians such as Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese, and Lynn Hunt have argued for the importance of studying the experience of women in the past. In recent years, postmodernists have challenged the validity and need for the study of history on the basis that all history is based on the personal interpretation of sources. In his 1997 book "In Defence of History", Richard J. Evans defended the worth of history. Another defence of history from post-modernist criticism was the Australian historian Keith Windschuttle's 1994 book, "The Killing of History".

The Marxist theory of historical materialism theorises that society is fundamentally determined by the "material conditions" at any given time – in other words, the relationships which people have with each other in order to fulfill basic needs such as feeding, clothing and housing themselves and their families. Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe. Marxist historiography was once orthodoxy in the Soviet Union, but since the collapse of communism there in 1991, Mikhail Krom says it has been reduced to the margins of scholarship.

Historical study often focuses on events and developments that occur in particular blocks of time. Historians give these periods of time names in order to allow "organising ideas and classificatory generalisations" to be used by historians. The names given to a period can vary with geographical location, as can the dates of the beginning and end of a particular period. Centuries and decades are commonly used periods and the time they represent depends on the dating system used. Most periods are constructed retrospectively and so reflect value judgments made about the past. The way periods are constructed and the names given to them can affect the way they are viewed and studied.

The field of history generally leaves prehistory to the archaeologists, who have entirely different sets of tools and theories. The usual method for periodisation of the distant prehistoric past, in archaeology is to rely on changes in material culture and technology, such as the Stone Age, Bronze Age and Iron Age and their sub-divisions also based on different styles of material remains. Here prehistory is divided into a series of "chapters" so that periods in history could unfold not only in a relative chronology but also narrative chronology. This narrative content could be in the form of functional-economic interpretation. There are periodisation, however, that do not have this narrative aspect, relying largely on relative chronology and, thus, devoid of any specific meaning.

Despite the development over recent decades of the ability through radiocarbon dating and other scientific methods to give actual dates for many sites or artefacts, these long-established schemes seem likely to remain in use. In many cases neighbouring cultures with writing have left some history of cultures without it, which may be used. Periodisation, however, is not viewed as a perfect framework with one account explaining that "cultural changes do not conveniently start and stop (combinedly) at periodisation boundaries" and that different trajectories of change are also needed to be studied in their own right before they get intertwined with cultural phenomena.

Particular geographical locations can form the basis of historical study, for example, continents, countries, and cities. Understanding why historic events took place is important. To do this, historians often turn to geography. According to Jules Michelet in his book "Histoire de France" (1833), "without geographical basis, the people, the makers of history, seem to be walking on air." Weather patterns, the water supply, and the landscape of a place all affect the lives of the people who live there. For example, to explain why the ancient Egyptians developed a successful civilization, studying the geography of Egypt is essential. Egyptian civilization was built on the banks of the Nile River, which flooded each year, depositing soil on its banks. The rich soil could help farmers grow enough crops to feed the people in the cities. That meant everyone did not have to farm, so some people could perform other jobs that helped develop the civilization. There is also the case of climate, which historians like Ellsworth Huntington and Allen Semple, cited as a crucial influence on the course of history and racial temperament.


Military history concerns warfare, strategies, battles, weapons, and the psychology of combat. The "new military history" since the 1970s has been concerned with soldiers more than generals, with psychology more than tactics, and with the broader impact of warfare on society and culture.

The history of religion has been a main theme for both secular and religious historians for centuries, and continues to be taught in seminaries and academe. Leading journals include "Church History", "The Catholic Historical Review", and "History of Religions". Topics range widely from political and cultural and artistic dimensions, to theology and liturgy. This subject studies religions from all regions and areas of the world where humans have lived.

"Social history", sometimes called the "new social history", is the field that includes history of ordinary people and their strategies and institutions for coping with life. In its "golden age" it was a major growth field in the 1960s and 1970s among scholars, and still is well represented in history departments. In two decades from 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%. In the history departments of British universities in 2007, of the 5723 faculty members, 1644 (29%) identified themselves with social history while political history came next with 1425 (25%).
The "old" social history before the 1960s was a hodgepodge of topics without a central theme, and it often included political movements, like Populism, that were "social" in the sense of being outside the elite system. Social history was contrasted with political history, intellectual history and the history of great men. English historian G. M. Trevelyan saw it as the bridging point between economic and political history, reflecting that, "Without social history, economic history is barren and political history unintelligible." While the field has often been viewed negatively as history with the politics left out, it has also been defended as "history with the people put back in."

The chief subfields of social history include:
Smaller specialties include:

Cultural history replaced social history as the dominant form in the 1980s and 1990s. It typically combines the approaches of anthropology and history to look at language, popular cultural traditions and cultural interpretations of historical experience. It examines the records and narrative descriptions of past knowledge, customs, and arts of a group of people. How peoples constructed their memory of the past is a major topic.
Cultural history includes the study of art in society as well is the study of images and human visual production (iconography).

Diplomatic history focuses on the relationships between nations, primarily regarding diplomacy and the causes of wars. More recently it looks at the causes of peace and human rights. It typically presents the viewpoints of the foreign office, and long-term strategic values, as the driving force of continuity and change in history. This type of "political history" is the study of the conduct of international relations between states or across state boundaries over time. Historian Muriel Chamberlain notes that after the First World War, "diplomatic history replaced constitutional history as the flagship of historical investigation, at once the most important, most exact and most sophisticated of historical studies." She adds that after 1945, the trend reversed, allowing social history to replace it.

Although economic history has been well established since the late 19th century, in recent years academic studies have shifted more and more toward economics departments and away from traditional history departments. Business history deals with the history of individual business organizations, business methods, government regulation, labour relations, and impact on society. It also includes biographies of individual companies, executives, and entrepreneurs. It is related to economic history; Business history is most often taught in business schools.

Environmental history is a new field that emerged in the 1980s to look at the history of the environment, especially in the long run, and the impact of human activities upon it.

World history is the study of major civilizations over the last 3000 years or so. World history is primarily a teaching field, rather than a research field. It gained popularity in the United States, Japan and other countries after the 1980s with the realization that students need a broader exposure to the world as globalization proceeds.

It has led to highly controversial interpretations by Oswald Spengler and Arnold J. Toynbee, among others.

The World History Association publishes the "Journal of World History" every quarter since 1990. The H-World discussion list serves as a network of communication among practitioners of world history, with discussions among scholars, announcements, syllabi, bibliographies and book reviews.

A people's history is a type of historical work which attempts to account for historical events from the perspective of common people. A people's history is the history of the world that is the story of mass movements and of the outsiders. Individuals or groups not included in the past in other type of writing about history are the primary focus, which includes the disenfranchised, the oppressed, the poor, the nonconformists, and the otherwise forgotten people. The authors are typically on the left and have a socialist model in mind, as in the approach of the History Workshop movement in Britain in the 1960s.

Intellectual history and the history of ideas emerged in the mid-20th century, with the focus on the intellectuals and their books on the one hand, and on the other the study of ideas as disembodied objects with a career of their own.

Gender history is a sub-field of History and Gender studies, which looks at the past from the perspective of gender. It is in many ways, an outgrowth of women's history. Despite its relatively short life, Gender History (and its forerunner Women's History) has had a rather significant effect on the general study of history. Since the 1960s, when the initially small field first achieved a measure of acceptance, it has gone through a number of different phases, each with its own challenges and outcomes. Although some of the changes to the study of history have been quite obvious, such as increased numbers of books on famous women or simply the admission of greater numbers of women into the historical profession, other influences are more subtle.

Public history describes the broad range of activities undertaken by people with some training in the discipline of history who are generally working outside of specialized academic settings. Public history practice has quite deep roots in the areas of historic preservation, archival science, oral history, museum curatorship, and other related fields. The term itself began to be used in the U.S. and Canada in the late 1970s, and the field has become increasingly professionalized since that time. Some of the most common settings for public history are museums, historic homes and historic sites, parks, battlefields, archives, film and television companies, and all levels of government.

Professional and amateur historians discover, collect, organize, and present information about past events. They discover this information through archaeological evidence, written primary sources from the past and other various means such as place names. In lists of historians, historians can be grouped by order of the historical period in which they were writing, which is not necessarily the same as the period in which they specialized. Chroniclers and annalists, though they are not historians in the true sense, are also frequently included.

Since the 20th century, Western historians have disavowed the aspiration to provide the "judgement of history." The goals of historical judgements or interpretations are separate to those of legal judgements, that need to be formulated quickly after the events and be final. A related issue to that of the judgement of history is that of collective memory.

Pseudohistory is a term applied to texts which purport to be historical in nature but which depart from standard historiographical conventions in a way which undermines their conclusions.
It is closely related to deceptive historical revisionism. Works which draw controversial conclusions from new, speculative, or disputed historical evidence, particularly in the fields of national, political, military, and religious affairs, are often rejected as pseudohistory.

A major intellectual battle took place in Britain in the early twentieth century regarding the place of history teaching in the universities. At Oxford and Cambridge, scholarship was downplayed. Professor Charles Harding Firth, Oxford's Regius Professor of history in 1904 ridiculed the system as best suited to produce superficial journalists. The Oxford tutors, who had more votes than the professors, fought back in defence of their system saying that it successfully produced Britain's outstanding statesmen, administrators, prelates, and diplomats, and that mission was as valuable as training scholars. The tutors dominated the debate until after the Second World War. It forced aspiring young scholars to teach at outlying schools, such as Manchester University, where Thomas Frederick Tout was professionalizing the History undergraduate programme by introducing the study of original sources and requiring the writing of a thesis.

In the United States, scholarship was concentrated at the major PhD-producing universities, while the large number of other colleges and universities focused on undergraduate teaching. A tendency in the 21st century was for the latter schools to increasingly demand scholarly productivity of their younger tenure-track faculty. Furthermore, universities have increasingly relied on inexpensive part-time adjuncts to do most of the classroom teaching.

From the origins of national school systems in the 19th century, the teaching of history to promote national sentiment has been a high priority. In the United States after World War I, a strong movement emerged at the university level to teach courses in Western Civilization, so as to give students a common heritage with Europe. In the U.S. after 1980, attention increasingly moved toward teaching world history or requiring students to take courses in non-western cultures, to prepare students for life in a globalized economy.

At the university level, historians debate the question of whether history belongs more to social science or to the humanities. Many view the field from both perspectives.

The teaching of history in French schools was influenced by the "Nouvelle histoire" as disseminated after the 1960s by "Cahiers pédagogiques and Enseignement" and other journals for teachers. Also influential was the Institut national de recherche et de documentation pédagogique, (INRDP). Joseph Leif, the Inspector-general of teacher training, said pupils children should learn about historians' approaches as well as facts and dates. Louis François, Dean of the History/Geography group in the Inspectorate of National Education advised that teachers should provide historic documents and promote "active methods" which would give pupils "the immense happiness of discovery." Proponents said it was a reaction against the memorization of names and dates that characterized teaching and left the students bored. Traditionalists protested loudly it was a postmodern innovation that threatened to leave the youth ignorant of French patriotism and national identity.

In several countries history textbooks are tools to foster nationalism and patriotism, and give students the official narrative about national enemies.

In many countries, history textbooks are sponsored by the national government and are written to put the national heritage in the most favourable light. For example, in Japan, mention of the Nanking Massacre has been removed from textbooks and the entire Second World War is given cursory treatment. Other countries have complained. It was standard policy in communist countries to present only a rigid Marxist historiography.

In the United States, the history of the Southern states, slavery and the American Civil War are controversial topics. McGraw-Hill Education for example, was criticised for describing Africans brought to American plantations as "workers" instead of slaves in a textbook.

Academic historians have often fought against the politicization of the textbooks, sometimes with success.

In 21st-century Germany, the history curriculum is controlled by the 16 states, and is characterized not by superpatriotism but rather by an "almost pacifistic and deliberately unpatriotic undertone" and reflects "principles formulated by international organizations such as UNESCO or the Council of Europe, thus oriented towards human rights, democracy and peace." The result is that "German textbooks usually downplay national pride and ambitions and aim to develop an understanding of citizenship centered on democracy, progress, human rights, peace, tolerance and Europeanness."






</doc>
<doc id="11325429" url="https://en.wikipedia.org/wiki?curid=11325429" title="Historical figure">
Historical figure

A historical figure is a famous person in history, such as Catherine the Great, Abraham Lincoln, George Washington, or Napoleon.

The significance of such figures in human progress has been debated. Some think they play a crucial role, while others say they have little impact on the broad currents of thought and social change. The concept is generally used in the sense that the person really existed in the past, as opposed to being legendary. However, the legends that can grow up around historical figures may be hard to distinguish from fact. Sources are often incomplete and may be inaccurate, particularly those from early periods of history. Without a body of personal documents, the more subtle aspects of personality of a historical figure can only be deduced. With historical figures who were also religious figures attempts to separate fact from belief may be controversial.

In education, presenting information as if it were being told by a historical figure may give it greater impact. Since classical times, students have been asked to put themselves in the place of a historical figure as a way of bringing history to life. Historical figures are often represented in fiction, where fact and fancy are combined. In earlier traditions, before the rise of a critical historical tradition, authors took less care to be as accurate when describing what they knew of historical figures and their actions, interpolating imaginary elements intended to serve a moral purpose to events: such is the Monk of St. Gall's anecdotal account of Charlemagne, "De Carolo Magno". More recently there has been a tendency once again for authors to freely depart from the "facts" when they conflict with their creative goals.

The significance of historical figures has long been the subject of debate by philosophers. Hegel (1770–1831) considered that "world-historical figures" played a pivotal role in human progress, but felt that they were bound to emerge when change was needed. Thomas Carlyle (1795–1881) saw the study of figures such as Muhammad, William Shakespeare and Oliver Cromwell as key to understanding history. Herbert Spencer (1820–1903), an early believer in evolution and in the universality of natural law, felt that historical individuals were of little importance.

The German philosopher Hegel defined the concept of the world-historical figure, who embodied the ruthless advance of Immanuel Kant's World Spirit, often overthrowing outdated structures and ideas. To him, Napoleon was such a figure.
Hegel proposed that a world-historical figure essentially posed a challenge, or thesis, and this would generate an antithesis, or opposing force. Eventually a synthesis would resolve the conflict.
Hegel viewed Julius Caesar as a world historical figure, who appeared at a stage when Rome had grown to the point it could no longer continue as a republican city state but had to become an empire. Caesar failed in his bid to make himself an emperor, and was assassinated, but the empire came into existence soon afterward, and Caesar's name has become synonymous with "emperor" in forms such as "kaiser" or "czar".

Søren Kierkegaard, in his early essay "The Concept of Irony", generally agrees with Hegel's views, such as his characterization of Socrates as a world-historical figure who acted as a destructive force on Greek received views of morality.
In Hegel's view, Socrates broke down social harmony by questioning the meaning of concepts like "justice" and "virtue".
Eventually, the Athenians quite rightly *(Name source) condemned Socrates to death. But they could not stop the evolution of thought that Socrates had begun, which would lead to the concept of individual conscience.
Hegel said of world-historical figures,
However, Hegel, Thomas Carlyle and others noted that the great historical figures were just representative men, expressions of the material forces of history. Essentially they have little choice about what they do. This is in conflict with the views of George Bancroft or Ralph Waldo Emerson, who praised self-reliance and individualism, and in conflict with Karl Marx and Friedrich Engels, who also felt that individuals can determine their destiny.
Engels found that Hegel's system contained an "internal and incurable contradiction", resting as it does on both dialectical relativism and idealistic absolutism.

The Scottish philosopher and evolutionist Herbert Spencer, who was highly influential in the latter half of the nineteenth century, felt that historical figures were relatively unimportant.
He wrote to a friend, "I ignore utterly the personal element in history, and, indeed, show little respect for history altogether as it is ordinarily conceived."
He wrote, "The births, deaths, and marriages of kings, and other like historic trivialities, are committed to memory, not because of any direct benefits that can possibly result from knowing them: but because society considers them parts of a good education."
In his essay "What Knowledge Is of Most Worth?" he wrote:
Taken to an extreme, one may consider that what Hegel calls the "world spirit" and T. S. Eliot calls "those vast impersonal forces" hold us in their grip. What happens is predetermined. 
Both Hegel and Marx advocated historical inevitability in contrast to the doctrine of contingency, allowing for alternative outcomes, that was advocated by Friedrich Nietzsche, Michel Foucault and others.
However, Marx argued against the use of the "historical inevitability" argument when used to explain the destruction of early communes in Russia.
As an orthodox Marxist, Vladimir Lenin accepted the laws of history that Marx had discovered, including the historical inevitability of capitalism followed by a transition to socialism.
Despite this, Lenin also believed the transition could be effected faster by voluntary action.

In 1936 Karl Popper published an influential paper on "The Poverty of Historicism", published as a book in 1957, that attacked the doctrine of historical inevitability.
The historian Isaiah Berlin, author of "Historical Inevitability", also argued forcibly against this view, going as far as to say that some choices are entirely free and cannot be predicted scientifically.
Berlin presented his views in a 1953 lecture at the London School of Economics, published soon afterwards. When speaking he referred to Ludwig Wittgenstein's views, but the published version speaks approvingly of Karl Popper, which caused a stir among academics.

Thomas Carlyle has espoused the "heroic view" of history, famously saying in his essay on the Norse god Odin in his book "On heroes, hero-worship, & the heroic in history" that "No great man lives in vain. The History of the world is but the Biography of great men ... We do not now call our great men Gods, nor admire "without" limit; ah no, "with" limit enough! But if we have no great men, or do not admire at all,— that were a still worse case."
Carlyle's historical philosophy was based on the "Great Man theory", saying, "Universal History, the history of what man has accomplished in the world ... [is] at bottom the History of the Great Men who have worked here."
An extreme believer in individuality, he also believed that the masses of people should let themselves be guided by the great leaders of men.
Talking of poets he said,

More recently, in his 1943 book "The Hero in History", the pragmatist scholar Sidney Hook asserts:

Hook recognizes the relevance of the environment within which the "great man" or "hero" acted, but asserts that this can provide the backdrop but never the plot of the "dramas of human history". and distinguish life and species

There have been rankings of the significance of major historical figures. For example, Cesar A. Hidalgo and colleagues at the MIT Media Lab has calculated the memorability of historical figures using data such as the number of language editions for which there are articles for each person, the pageviews received, and other factors. These lists are available at MIT's Pantheon project.

It is sometimes hard to discern whether apparently historical figures from the earliest periods did in fact exist, due to the lack of records. Even with more recent personages, stories or anecdotes about the person often accumulate that have no basis in fact. Although the external aspects of a historical figure may be well documented, their inner nature can only be a subject of speculation. With religious figures, often the subjects of voluminous literature, separating "fact" from "belief" can be difficult if not impossible.

With older texts it can be difficult to be sure whether a person in the text is, in fact, a historical figure. "Wisdom literature" from early middle-eastern cultures (such as the Book of Job), mainly consist of verbal expositions or discussions that must be considered the work of the author, rather than the character supposedly speaking. It may still be possible to identify a figure in such texts with a historical figure known from some other context, and the text may be taken as informative about this figure, even if not verified by an independent source. On the other hand, a text may include realistic settings and references to historical people, while the central character may or may not be a historical figure.

Napoleon spoke of history as being a fable which had been agreed upon:– "la fable convenue qu'on appellera l'histoire". Great figures of the past have stories told about them which grow in the telling, and so become myths and legends which may dominate or displace the more prosaic historical facts about them. For example, some ancient chroniclers said that the Emperor Nero fiddled while Rome burned, but Tacitus disputed this by saying the stories were just malicious rumours. Similarly, there is no good evidence that Marie-Antoinette ever said "let them eat cake", or that Lady Godiva rode naked through the streets of Coventry.

Thomas Carlyle pointed out that even to the person living it, every life "remains in so many points unintelligible". The historian must struggle when writing biographies, "the very facts of which, to say nothing of the purport of them, we know not, and cannot know!"
Some psychologists have sought to understand the personalities of historical figures through clues about the way in which they were raised. However, this theoretical psychoanalytic approach is not supported empirically. An alternative approach, favored by psychobiographers such as William Runyan, is to explain the personality of the historical figure in terms of their life history. This approach has the advantage of recognizing that personality may evolve over time in response to events.

With historical religious figures, fact and belief may be difficult to disentangle.
There are cultural differences in the treatment of historical figures. Thus the Chinese can recognise that Mencius or Confucius were historical individuals, while also endowing them with sanctity. In Indian Hinduism, on the other hand, figures such as Krishna or Rama are almost always seen as embodiments of gods rather than as historical people. The Nirvana Sutra states: "Do not rely on the man but on the Dharma." A teacher such as Gautama Buddha is thus treated almost exclusively as a lesser god rather than a historical figure.

E. P. Sanders, author of "The Historical Figure of Jesus", called Jesus of Nazareth "one of the most important figures in human history". Various writers have struggled to present "historical" views of Jesus, as opposed to views distorted by belief. When writing about this subject, a historian who relies only on sources other than the New Testament may be criticized for implying that it is not a sufficient source of information about the subject.

The theologian Martin Kähler is known for his work "Der sogenannte historische Jesus und der geschichtliche, biblische Christus" (The so-called historical Jesus, and the historic, biblical Christ). He clearly distinguished between "the Jesus of history" and "the Christ of faith". Some historians openly admit bias, which may anyway be unavoidable. Paul Hollenback says he writes about the historical Jesus, "...in order to overthrow, not simply correct, the mistake called Christianity." Another historian who has written about Jesus, Frederick Gaiser, says, "historical investigation is part and parcel of biblical faith."

A historical figure may be interpreted to support political aims.
In France in the first half of the seventeenth century, there was an outpouring of writing about Joan of Arc, including seven biographies, three plays and an epic poem. Joan had become a symbol of national pride and the Catholic faith, helping unite a country that had been divided by the recent wars of religion. The reality of the historical Joan was subordinated to the need for a symbol of feminine strength, Christian virtue and resistance to the English.
George Bernard Shaw, introducing his 1923 play "Saint Joan", discussed representations of Joan by other authors.
He felt that William Shakespeare's depiction in "Henry VI, Part 1" was constrained from making her a "beautiful and romantic figure" by political considerations. Voltaire's version in his poem "La Pucelle d'Orléans" was also flawed by Voltaire's biases and Friedrich Schiller's play "Die Jungfrau von Orleans" "is not about Joan at all, and can hardly be said to pretend to be."

A historical figure may be used to validate a politician's claim to authority,
where the modern leader shapes and exploits the ideas associated with the historical figure,
which they are presumed to have inherited.
Thus Jesse Jackson has frequently evoked the spirit of Martin Luther King, Jr..
Fidel Castro often presented himself as following the path defined by José Martí.
Hugo Chávez of Venezuela has frequently identified himself with the historical figure Simón Bolívar, the liberator of South America from Spanish rule.

Hegel believed in the role of the state in guaranteeing individual liberties, and his views were therefore rejected by the German National Socialists, who considered him dangerously liberal and perhaps a proto-Marxist. On the other hand, Adolf Hitler identified himself as a Hegelian world historical figure, and justified his actions on this basis.

Plato used historical figures in his writing, but only to illustrate his points. Xenophon used Cyrus the Great in the same way.
When Plato apparently quotes Socrates in "The Republic", it is only to add dramatic effect to the presentation of his own thought. For this reason, Plato's writings on Socrates tell us little, at least directly, about Socrates. The historical figure is used only as a device for communicating Plato's ideas. In classical Rome, students of rhetoric had to master the "suasoria" — a form of declamation in which they wrote the soliloquy of a historical figure who was debating a critical course of action. For example, the poet Juvenal wrote a speech for the dictator Sulla, in which he was counselled to retire. The poet Ovid enjoyed this exercise more than the other final challenge — the "controversia".

The German philosopher Friedrich Nietzsche wrote an influential essay "On the Uses and Disadvantages of History for Life". He said "the unhistorical and historical are necessary in equal measure for the health of an individual, of a people and of a culture."
Nietzsche identifies three approaches to history, each with dangers.
The monumental approach describes the glories of the past, often focusing on heroic figures like Elizabeth I of England or Louis Pasteur. 
By treating these figures as models, the student is tempted to consider that there can be nobody of such stature today.
The antiquarian view examines the past in minute and reverent detail, turning its back on the present.
The critical approach challenges traditional views, even though they may be valid.

Historical figures may today be simulated as animated pedagogical agents to teach history and foreign culture. An example is Freudbot, which acted the part of Sigmund Freud for psychology students. When a variety of simulated character types were tried as educational agents, students rated historical figures as the most engaging.
There are gender differences in the perception of historical figures. When modern US schoolchildren were asked to roleplay or illustrate historical stereotypes, boys tended to focus upon male figures exclusively while girls showed more varied family groupings.

Using historical figures in marketing communicationsn and in branding is a new area of marketing research but historical figures’ names were used to promote products as early as in the Middle Ages. 

Historical figure brand is using famous historical person in branding, for instance Mozartkugel, Chopin (vodka) or Café Einstein https://www.cafeeinstein.com/. 

Historical figure is a person who lived in the past and whose deeds exerted a significant impact on other people’s lives and consciousness. These figures are attributed with certain features that are a compilation of the actual values they proclaimed and the manner they were perceived by others. This perception evolves and subsequent generations read the biography of a given historical figure in their own way through their own knowledge and experience. In order to determine the popularity of the commercialisation of historical figures, a study was conducted at the beginning of 2014 on the number of trademark protection applications filed with the Patent Office of the Republic of Poland as a measure of entrepreneurs’ interest in this activity. The names of 300 most prominent Polish historical figures were considered. The study showed that over 21% of the names analysed were recorded in the trademark register. 1,033 trademark protection applications were filed for 64 names out of the 300 historical figures investigated [Aldona Lipka, 2015,]. The greatest number of trademark protection applications were recorded for Mieszko (295), followed by Nicolaus Copernicus (250), John III Sobieski (94) and Chopin (81).

There is a huge body of historical fiction, where the text includes both imaginary and factual elements. In early English literature, Robin Hood was a fictional character, but the historical King Richard I of England also appears.
William Shakespeare wrote plays about people who were historical figures in his day, such as "Julius Caesar". He did not present these people as pure history, but dramatised their lives as a commentary about the people and politics of his own time.
Napoleon figured in Victor Hugo's 1862 classic "Les Misérables".
There are many more examples.

The compiler of a survey of historical novels in the 1920s claimed that the "appearance of reality ... is the great charm of the historical novel." He went on to assert, regarding novels about periods of which little is known, that "the danger is that the very elements which add to our interest in the tale as such will go far to mislead us in our conception of the period dealt with".
Traditionally the treatment of historical figures in fiction was realistic in style and respectful of fact. A historical novel would be true to the facts known about the period in which the novel is set, a biographical novel would follow the facts that are known about the protagonist's life, and a "roman à clef" would try to give an accurate interpretation of what is known about a public figure's private life. In each genre, the novelist would avoid introducing any elements that were clearly in conflict with the facts.

A writer may be handicapped by his readers' preconceptions about a historical person, which may or may not be accurate, and the facts about the historical person may also conflict with the novelist's plot requirements.
According to the Marxist philosopher György Lukács in his 1937 book on "The Historical Novel", "The 'world-historical individual' can only figure as a minor character in the [historical] novel because of the complexity and intricacy of the whole social-historical process."
As Jacobs observes, the "realist aesthetic" of the historical novel "assumes that a recognizable historical figure in fiction must not 'do things' its model did not do in real life; it follows that historical figures can be used only in very limited ways."
The author of a traditional historical novel should therefore focus more on the people who have been lost to history.
A novelist such as Sir Walter Scott or Leo Tolstoy ("War and Peace") would describe historical events accurately. They would give rein to their imagination only in scenes that were not significant historically, when interactions with fictional characters could safely be introduced.

More recently, however, starting with works such as "The Confessions of Nat Turner", and "Sophie's Choice" by William Styron,
the novelist has felt more free to introduce much larger amounts of purely imaginary detail about historical people.
E. L. Doctorow illustrates this different attitude when discussing his book "Ragtime": "Certain details were so delicious that I was scrupulous about getting them right. Others ... demanded to be mythologized." This reflects a changing attitude about the distinction between "fact" and "truth", expressed by Ursule Molinaro when he makes his Cassandra say, "I've come as close to the truth as facts would let me ... facts oppress the truth, which can breathe freely only in poetry & art."

Many films have depicted historical figures. Often the way in which the films interpret these figures and their times reflects the social and cultural values of the period in which the film was made.
Historical figures are familiar to the general reader and so may be used in speculative fiction so that readers marvel at their appearance in novel settings or with a fresh perspective.
For example, the time traveler The Doctor has encountered numerous historical figures such as Marco Polo and Queen Elizabeth I in his adventures.
They appeared most frequently when the television series first started, as it was directed at children and the use of historical figures in historical settings was intended to be educational.



</doc>
<doc id="53372308" url="https://en.wikipedia.org/wiki?curid=53372308" title="National memory">
National memory

National memory is a form of collective memory defined by shared experiences and culture. It is an integral part to national identity.

It represents one specific form of cultural memory, which makes an essential contribution to national group cohesion. Historically national communities have drawn upon commemorative ceremonies and monuments, myths and rituals, glorified individuals, objects, and events in their own history to produce a common narrative.

According to Lorraine Ryan, national memory is based on the public's reception of national historic narratives and the ability of people to affirm the legitimacy of these narratives.
National memory typically consists of a shared interpretation of a nation's past. Such interpretations can vary and sometimes compete. They can get challenged and augmented by a range of interest groups, fighting to have their histories acknowledged, documented and commemorated and reshape national stories. Often national memory is adjusted to offer a politicized vision of the past to make a political position appear consistent with national identity. Furthermore, it profoundly affects how historical facts are perceived and recorded and may circumvent or appropriate facts. A repertoire of discursive strategies functions to emotionalize national narrative and nationalize personal pasts.

National memory has been used calculatedly by governments for dynastic, political, religious and cultural purposes since as early as the sixteenth century.

Marketing of memory by the culture industry and its instrumentalisation for political purposes can both be seen as serious threats to the objective understanding of a nation's past.

Lorraine Ryan notes that individual memory both shapes and is shaped by national memory, and that there is a competition between the dominant and individual memories of a nation.

Hyung Park states that the nation is continuously revived, re-imagined, reconstituted, through shared memories among its citizens.

National memories may also conflict with the other nations' collective memory.

Reports that are narrated in terms of national memory characterize the past in ways that merge the past, the present and the future into "a single ongoing tale".

Pierre Nora argues that a "democratisation of history" allows for emancipatory versions of the past to surface:

However, national history being passed on by the culture industry, such as by historical films, can be seen as serious threats to the objective understanding of a nation's past.

Nations' memories can be shared across nations via media such as the Internet.

National memory can be a force of cohesion as well as division and conflict. It can foster constructive national reforms, international communities and agreements, dialogue as well as deepen problematic courses and rhetoric.

Identity crisis can occur due to bad memories (such as national crimes) or the absence of a belief in a shared past.

Often new developments, processes, problems and events are made sense of and contextualized by drawing from national memory.

Critical history or historic memory cuts from national memory's tradition centric to national heritage and orients itself towards a specialized study of history in a more sociological manner.

It has been proposed that the unthinkable ought not to be unmasked but that instead what made it thinkable should be reconstructed and that the difficulty of discussing the non-places or the bad places of national memory make it necessary to include forgetfulness and amnesia in the concept.

National memory may lead to questioning the nation as it is as well as its identity and imply a societal negotiation of what the country wishes to be as a nation. To understand the links between memory, forgetfulness, identity and the imaginary construction of the nation analysis of the discourse in the places of memory is fundamental as in all writings of national history an image of the nation is being restructured.



</doc>
<doc id="21014942" url="https://en.wikipedia.org/wiki?curid=21014942" title="Glossary of history">
Glossary of history

This glossary of history is a list of definitions of terms and concepts relevant to the study of history and its related fields and sub-disciplines, including both prehistory and the period of human history.




</doc>
<doc id="6830515" url="https://en.wikipedia.org/wiki?curid=6830515" title="Index of history articles">
Index of history articles

History is the study of the past. When used as the name of a field of study, "history" refers to the study and interpretation of the record of humans, families, and societies as preserved primarily through written sources. This is a list of history topics covered on English Wikipedia:





























</doc>
<doc id="1827281" url="https://en.wikipedia.org/wiki?curid=1827281" title="List of historical classifications">
List of historical classifications

Historical classification groups the various history topics into different categories according to subject matter as shown below.






See also Periodization.






Although there is arguably some intrinsic bias in history studies (with national bias perhaps being the most significant), history can also be studied from ideological perspectives, which practitioners feel are often ignored, such as:

A form of historical speculation known commonly as counterfactual history has also been adopted by some historians as a means of assessing and exploring the possible outcomes if certain events had not occurred or had occurred in a different way. This is somewhat similar to the alternate history genre in fiction.

Lists of false or dubious historical resources and historical myths that were once popular and widespread, or have become so, have also been prepared.


</doc>
<doc id="26334944" url="https://en.wikipedia.org/wiki?curid=26334944" title="Auxiliary sciences of history">
Auxiliary sciences of history

Auxiliary (or ancillary) sciences of history are scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Many of these areas of study, classification and analysis were originally developed between the 16th and 19th centuries by antiquaries, and would then have been regarded as falling under the broad heading of antiquarianism. "History" was at that time regarded as a largely literary skill. However, with the spread of the principles of empirical source-based history championed by the Göttingen School of History in the late 18th century and later by Leopold von Ranke from the mid-19th century onwards, they have been increasingly regarded as falling within the skill-set of the trained historian.

Auxiliary sciences of history include, but are not limited to:



</doc>
<doc id="60209193" url="https://en.wikipedia.org/wiki?curid=60209193" title="Historical significance">
Historical significance

Historical significance is a historiographical concept that defines and influences the social remembrance of past events. 

Historians consider knowledge of dates and events the primary content of history, or "first-order knowledge". They class historical significance as an aspect of the study of primary content, deeming it secondary or "second-order knowledge". However, the way dates and events are chosen and described is often used to assign significance, without acknowledging bias. As Winston Churchill and Michel Foucault have said: "History is written by the victors". 

A central topic to the study of history, historical significance defines what is remembered about the past, through its reflections in various objects of contemporary culture (images on stamps and banknotes, street names, etc.) Examining it can be an effective tool for guiding students to understand how cultural background affects perception of history and the preferences they have.

Historical significance is typically assessed by judging an event against pre-defined criteria. For example, UNESCO includes any site as a world heritage site, provided it "bear[s] a unique or at least exceptional testimony to a cultural tradition or to a civilization". However, these criteria are often subjective and perception-biased "or" unavailable. Thus the alluding of any event as historically significant or non-significant remains open to challenge.

Numerous criteria for assessing historical significance have been proposed.


</doc>
<doc id="3402732" url="https://en.wikipedia.org/wiki?curid=3402732" title="Outline of history">
Outline of history

The following outline is provided as an overview of and topical guide to history:

History – discovery, collection, organization, and presentation of information about past events. History can also mean the period of time after writing was invented (the beginning of recorded history).

History can be described as all of the following:



Auxiliary sciences of history – scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Auxiliary sciences of history include, but are not limited to:


History by period





Regional history


Era

















</doc>
<doc id="2118843" url="https://en.wikipedia.org/wiki?curid=2118843" title="Progress">
Progress

Progress is the movement towards a refined, improved, or otherwise desired state. In the context of progressivism, it refers to the proposition that advancements in technology, science, and social organization have resulted, and by extension will continue to result, in an improved human condition; the latter may happen as a result of direct human action, as in social enterprise or through activism, or as a natural part of sociocultural evolution.

The concept of progress was introduced in the early 19th-century social theories, especially social evolution as described by Auguste Comte and Herbert Spencer. It was present in the Enlightenment's philosophies of history. As a goal, social progress has been advocated by varying realms of political ideologies with different theories on how it is to be achieved.

Specific indicators for measuring progress can range from economic data, technical innovations, change in the political or legal system, and questions bearing on individual life chances, such as life expectancy and risk of disease and disability.

GDP growth has become a key orientation for politics and is often taken as a key figure to evaluate a politician's performance. However, GDP has a number of flaws that make it a bad measure of progress, especially for developed countries. For example, environmental damage is not taken into account nor is the sustainability of economic activity. Wikiprogress has been set up to share information on evaluating societal progress. It aims to facilitate the exchange of ideas, initiatives and knowledge. HumanProgress.org is another online resource that seeks to compile data on different measures of societal progress.

Our World in Data is a scientific online publication, based at the University of Oxford, that studies how to make progress against large global problems such as poverty, disease, hunger, climate change, war, existential risks, and inequality.
The mission of Our World in Data is to present "research and data to make progress against the world’s largest problems".
The Social Progress Index is a tool developed by the International Organization Imperative Social Progress, which measures the extent to which countries cover social and environmental needs of its citizenry. There are fifty-two indicators in three areas or dimensions: Basic Human Needs, and Foundations of Wellbeing and Opportunities which show the relative performance of nations.

Indices that can be used to measure progress include:

Scientific progress is the idea that the scientific community learns more over time, which causes a body of scientific knowledge to accumulate. The chemists in the 19th century knew less about chemistry than the chemists in the 20th century, and they in turn knew less than the chemists in the 21st century. Looking forward, today's chemists reasonably expect that chemists in future centuries will know more than they do. 

This process differs from non-science fields, such as human languages or history: the people who spoke a now-extinct language, or who lived through a historical time period, can be said to have known different things from the scholars who studied it later, but they cannot be said to know less about their lives than the modern scholars. Some valid knowledge is lost through the passage of time, and other knowledge is gained, with the result that the non-science fields do not make scientific progress towards understanding their subject areas.

From the 18th century through late 20th century, the history of science, especially of the physical and biological sciences, was often presented as a progressive accumulation of knowledge, in which true theories replaced false beliefs. Some more recent historical interpretations, such as those of Thomas Kuhn, tend to portray the history of science in terms of competing paradigms or conceptual systems in a wider matrix of intellectual, cultural, economic and political trends. These interpretations, however, have met with opposition for they also portray the history of science as an incoherent system of incommensurable paradigms, not leading to any scientific progress, but only to the illusion of progress.

Aspects of social progress, as described by Condorcet, have included the disappearance of slavery, the rise of literacy, the lessening of inequalities between the sexes, reforms of harsh prisons and the decline of poverty. The social progress of a society can be measured based on factors such as its ability to address fundamental human needs, help citizens improve their quality of life, and provide opportunities for citizens to succeed.

Social progress is often improved by increases in GDP, although other factors are also relevant. An imbalance between economic and social progress hinders further economic progress, and can lead to political instability.

How progress improved the status of women in traditional society was a major theme of historians starting in the Enlightenment and continuing to today. British theorists William Robertson (1721–1793) and Edmund Burke (1729–1797), along with many of their contemporaries, remained committed to Christian- and republican-based conceptions of virtue, while working within a new Enlightenment paradigm. The political agenda related beauty, taste, and morality to the imperatives and needs of modern societies of a high level of sophistication and differentiation. Two themes in the work of Robertson and Burke—the nature of women in 'savage' and 'civilized' societies and 'beauty in distress'—reveals how long-held convictions about the character of women, especially with regard to their capacity and right to appear in the public domain, were modified and adjusted to the idea of progress and became central to modern European civilization.

Classics experts have examined the status of women in the ancient world, concluding that in the Roman Empire, with its superior social organization, internal peace, and rule of law, allowed women to enjoy a somewhat better standing than in ancient Greece, where women were distinctly inferior. The inferior status of women in traditional China has raised the issue of whether the idea of progress requires a thoroughgoing reject of traditionalism—a belief held by many Chinese reformers in the early 20th century.

Historians Leo Marx and Bruce Mazlish asking, "Should we in fact abandon the idea of progress as a view of the past," answer that there is no doubt "that the status of women has improved markedly" in cultures that have adopted the Enlightenment idea of progress.

Modernization was promoted by classical liberals in the 19th and 20th centuries, who called for the rapid modernization of the economy and society to remove the traditional hindrances to free markets and free movements of people. During the Enlightenment in Europe social commentators and philosophers began to realize that people "themselves" could change society and change their way of life. Instead of being made completely by gods, there was increasing room for the idea that people themselves "made their own society"—and not only that, as Giambattista Vico argued, "because" people made their own society, they could also fully comprehend it. This gave rise to new sciences, or proto-sciences, which claimed to provide new scientific knowledge about what society was like, and how one may change it for the better.

In turn, this gave rise to progressive opinion, in contrast with conservational opinion. The social conservationists were skeptical about panaceas for social ills. According to conservatives, attempts to radically remake society normally make things worse. Edmund Burke was the leading exponent of this, although later-day liberals like Hayek have espoused similar views. They argue that society changes organically and naturally, and that grand plans for the remaking of society, like the French Revolution, National Socialism and Communism hurt society by removing the traditional constraints on the exercise of power.

The scientific advances of the 16th and 17th centuries provided a basis for Francis Bacon's book the New Atlantis. In the 17th century, Bernard le Bovier de Fontenelle described progress with respect to arts and the sciences, saying that each age has the advantage of not having to rediscover what was accomplished in preceding ages. The epistemology of John Locke provided further support and was popularized by the Encyclopedists Diderot, Holbach, and Condorcet. Locke had a powerful influence on the American Founding Fathers. The first complete statement of progress is that of Turgot, in his "A Philosophical Review of the Successive Advances of the Human Mind" (1750). For Turgot, progress covers not only the arts and sciences but, on their base, the whole of culture—manner, mores, institutions, legal codes, economy, and society. Condorcet predicted the disappearance of slavery, the rise of literacy, the lessening of inequalities between the sexes, reforms of harsh prisons and the decline of poverty.

John Stuart Mill's (1806–1873) ethical and political thought demonstrated faith in the power of ideas and of intellectual education for improving human nature or behavior. For those who do not share this faith the idea of progress becomes questionable.

Alfred Marshall (1842–1924), a British economist of the early 20th century, was a proponent of classical liberalism. In his highly influential "Principles of Economics" (1890), he was deeply interested in human progress and in what is now called "sustainable development." For Marshall, the importance of wealth lay in its ability to promote the physical, mental, and moral health of the general population. After World War II, the modernization and development programs undertaken in the Third World were typically based on the idea of progress.

In Russia the notion of progress was first imported from the West by Peter the Great (1672–1725). An absolute ruler, he used the concept to modernize Russia and to legitimize his monarchy (unlike its usage in Western Europe, where it was primarily associated with political opposition). By the early 19th century, the notion of progress was being taken up by Russian intellectuals and was no longer accepted as legitimate by the tsars. Four schools of thought on progress emerged in 19th-century Russia: conservative (reactionary), religious, liberal, and socialist—the latter winning out in the form of Bolshevist materialism.

The intellectual leaders of the American Revolution, such as Benjamin Franklin, Thomas Paine, Thomas Jefferson and John Adams, were immersed in Enlightenment thought and believed the idea of progress meant that they could reorganize the political system to the benefit of the human condition; both for Americans and also, as Jefferson put it, for an "Empire of Liberty" that would benefit all mankind. In particular, Adams wrote “I must study politics and war, that our sons may have liberty to study mathematics and philosophy. Our sons ought to study mathematics and philosophy, geography, natural history and naval architecture, navigation, commerce and agriculture in order to give their children a right to study painting, poetry, music, architecture, statuary, tapestry and porcelain.”

Juan Bautista Alberdi (1810–1884) was one of the most influential political theorists in Argentina. Economic liberalism was the key to his idea of progress. He promoted faith in progress, while chiding fellow Latin Americans for blind copying of American and European models. He hoped for progress through promotion of immigration, education, and a moderate type of federalism and republicanism that might serve as a transition in Argentina to true democracy.

In Mexico, José María Luis Mora (1794–1850) was a leader of classical liberalism in the first generation after independence, leading the battle against the conservative trinity of the army, the church, and the "hacendados". He envisioned progress as both a process of human development by the search for philosophical truth and as the introduction of an era of material prosperity by technological advancement. His plan for Mexican reform demanded a republican government bolstered by widespread popular education free of clerical control, confiscation and sale of ecclesiastical lands as a means of redistributing income and clearing government debts, and effective control of a reduced military force by the government. Mora also demanded the establishment of legal equality between native Mexicans and foreign residents. His program, untried in his lifetime, became the key element in the Mexican Constitution of 1857.

In Italy, the idea that progress in science and technology would lead to solutions for human ills was connected to the nationalism that united the country in 1860. The Piedmontese Prime Minister Camillo Cavour envisaged the railways as a major factor in the modernization and unification of the Italian peninsula. The new Kingdom of Italy, formed in 1861, worked to speed up the processes of modernization and industrialization that had begun in the north, but were slow to arrive in the Papal States and central Italy, and were nowhere in sight in the "Mezzogiorno" (that is, Southern Italy, Sicily, and Sardinia). The government sought to combat the backwardness of the poorer regions in the south and work towards augmenting the size and quality of the newly created Italian army so that it could compete on an equal footing with the powerful nations of Europe. In the same period, the government was legislating in favour of public education to fight the great problem of illiteracy, upgrade the teaching classes, improve existing schools, and procure the funds needed for social hygiene and care of the body as factors in the physical and moral regeneration of the race.

In China, in the 20th century the Kuomintang or Nationalist party, which ruled from the 1920s to the 1940s, advocated progress. The Communists under Mao Zedong adopted western models and their ruinous projects caused mass famines. After Mao's death, however, the new regime led by Deng Xiaoping (1904–1997) and his successors aggressively promoted modernization of the economy using capitalist models and imported western technology. This was termed the "Opening of China" in the west, and more broadly encompasses Chinese economic reform.

Among environmentalists, there is a continuum between two opposing poles. The one pole is optimistic, progressive, and business-oriented, and endorses the classic idea of progress. For example, bright green environmentalism endorses the idea that new designs, social innovations and green technologies can solve critical environmental challenges. The other is pessimistic in respect of technological solutions, warning of impending global crisis (through climate change or peak oil, for example) and tends to reject the very idea of modernity and the myth of progress that is so central to modernization thinking. Similarly, Kirkpatrick Sale, wrote about progress as a myth benefiting the few, and a pending environmental doomsday for everyone. An example is the philosophy of Deep Ecology.

Sociologist Robert Nisbet said that "No single idea has been more important than ... the Idea of Progress in Western civilization for three thousand years", and defines five "crucial premises" of the idea of progress:

Sociologist P. A. Sorokin said, "The ancient Chinese, Babylonian, Hindu, Greek, Roman, and most of the medieval thinkers supporting theories of rhythmical, cyclical or trendless movements of social processes were much nearer to reality than the present proponents of the linear view". Unlike Confucianism and to a certain extent Taoism, that both search for an ideal past, the Judeo-Christian-Islamic tradition believes in the fulfillment of history, which was translated into the idea of progress in the modern age. Therefore, Chinese proponents of modernization have looked to western models. According to Thompson, the late Qing dynasty reformer, Kang Youwei, believed he had found a model for reform and "modernisation" in the Ancient Chinese Classics.

Philosopher Karl Popper said that progress was not fully adequate as a scientific explanation of social phenomena.
More recently, Kirkpatrick Sale, a self-proclaimed neo-luddite author, wrote exclusively about progress as a myth, in an essay entitled "Five Facets of a Myth".

Iggers (1965) says that proponents of progress underestimated the extent of man's destructiveness and irrationality, while critics misunderstand the role of rationality and morality in human behavior.

In 1946, psychoanalyst Charles Baudouin claimed modernity has retained the "corollary" of the progress myth, the idea that the present is superior to the past, while at the same time insisting that it is free of the myth:
A cyclical theory of history was adopted by Oswald Spengler (1880–1936), a German historian who wrote "The Decline of the West" in 1920. World War I, World War II, and the rise of totalitarianism demonstrated that progress was not automatic and that technological improvement did not necessarily guarantee democracy and moral advancement. British historian Arnold J. Toynbee (1889–1975) felt that Christianity would help modern civilization overcome its challenges.

The Jeffersonians said that history is not exhausted but that man may begin again in a new world. Besides rejecting the lessons of the past, they Americanized the idea of progress by democratizing and vulgarizing it to include the welfare of the common man as a form of republicanism. As Romantics deeply concerned with the past, collecting source materials and founding historical societies, the Founding Fathers were animated by clear principles. They saw man in control of his destiny, saw virtue as a distinguishing characteristic of a republic, and were concerned with happiness, progress, and prosperity. Thomas Paine, combining the spirit of rationalism and romanticism, pictured a time when America's innocence would sound like a romance, and concluded that the fall of America could mark the end of 'the noblest work of human wisdom.'

Historian J. B. Bury wrote in 1920:

In the postmodernist thought steadily gaining ground from the 1980s, the grandiose claims of the modernizers are steadily eroded, and the very concept of social progress is again questioned and scrutinized. In the new vision, radical modernizers like Joseph Stalin and Mao Zedong appear as totalitarian despots, whose vision of social progress is held to be totally deformed. Postmodernists question the validity of 19th-century and 20th-century notions of progress—both on the capitalist and the Marxist side of the spectrum. They argue that both capitalism and Marxism over-emphasize technological achievements and material prosperity while ignoring the value of inner happiness and peace of mind. Postmodernism posits that both dystopia and utopia are one and the same, overarching grand narratives with impossible conclusions.
Some 20th-century authors refer to the "Myth of Progress" to refer to the idea that the human condition will inevitably improve. In 1932, English physician Montague David Eder wrote: "The myth of progress states that civilization has moved, is moving, and will move in a desirable direction. Progress is inevitable... Philosophers, men of science and politicians have accepted the idea of the inevitability of progress." Eder argues that the advancement of civilization is leading to greater unhappiness and loss of control in the environment. The strongest critics of the idea of progress complain that it remains a dominant idea in the 21st century, and shows no sign of diminished influence. As one fierce critic, British historian John Gray (b. 1948), concludes:
Recently the idea of progress has been generalized to psychology, being related with the concept of a goal, that is, progress is understood as "what counts as a means of advancing towards the end result of a given defined goal."

Historian J. B. Bury said that thought in ancient Greece was dominated by the theory of world-cycles or the doctrine of eternal return, and was steeped in a belief parallel to the Judaic "fall of man," but rather from a preceding "Golden Age" of innocence and simplicity. Time was generally regarded as the enemy of humanity which depreciates the value of the world. He credits the Epicureans with having had a potential for leading to the foundation of a theory of progress through their materialistic acceptance of the atomism of Democritus as the explanation for a world without an intervening deity.
Robert Nisbet and Gertrude Himmelfarb have attributed a notion of progress to other Greeks. Xenophanes said "The gods did not reveal to men all things in the beginning, but men through their own search find in the course of time that which is better." Plato's Book III of "The Laws" depicts humanity's progress from a state of nature to the higher levels of culture, economy, and polity. Plato's "The Statesman" also outlines a historical account of the progress of mankind.

During the Medieval period, science was to a large extent based on Scholastic (a method of thinking and learning from the Middle Ages) interpretations of Aristotle's work. The Renaissance of the 15th, 16th and 17th Centuries changed the mindset in Europe towards an empirical view, based on a pantheistic interpretation of Plato. This induced a revolution in curiosity about nature in general and scientific advance, which opened the gates for technical and economic advance. Furthermore, the individual potential was seen as a never-ending quest for being God-like, paving the way for a view of Man based on unlimited perfection and progress.

In the Enlightenment, French historian and philosopher Voltaire (1694–1778) was a major proponent of progress. At first Voltaire's thought was informed by the idea of progress coupled with rationalism. His subsequent notion of the historical idea of progress saw science and reason as the driving forces behind societal advancement.

Immanuel Kant (1724–1804) argued that progress is neither automatic nor continuous and does not measure knowledge or wealth, but is a painful and largely inadvertent passage from barbarism through civilization toward enlightened culture and the abolition of war. Kant called for education, with the education of humankind seen as a slow process whereby world history propels mankind toward peace through war, international commerce, and enlightened self-interest.

Scottish theorist Adam Ferguson (1723–1816) defined human progress as the working out of a divine plan, though he rejected predestination. The difficulties and dangers of life provided the necessary stimuli for human development, while the uniquely human ability to evaluate led to ambition and the conscious striving for excellence. But he never adequately analyzed the competitive and aggressive consequences stemming from his emphasis on ambition even though he envisioned man's lot as a perpetual striving with no earthly culmination. Man found his happiness only in effort.

Some scholars consider the idea of progress that was affirmed with the Enlightenment, as a secularization of ideas from early Christianity, and a reworking of ideas from ancient Greece.

In the 19th century, Romantic critics charged that progress did not automatically better the human condition, and in some ways could make it worse. Thomas Malthus (1766–1834) reacted against the concept of progress as set forth by William Godwin and Condorcet because he believed that inequality of conditions is "the best (state) calculated to develop the energies and faculties of man". He said, "Had population and food increased in the same ratio, it is probable that man might never have emerged from the savage state". He argued that man's capacity for improvement has been demonstrated by the growth of his intellect, a form of progress which offsets the distresses engendered by the law of population.

German philosopher Friedrich Nietzsche (1844–1900) criticized the idea of progress as the 'weakling's doctrines of optimism,' and advocated undermining concepts such as faith in progress, to allow the strong individual to stand above the plebeian masses. An important part of his thinking consists of the attempt to use the classical model of 'eternal recurrence of the same' to dislodge the idea of progress.

Iggers (1965) argues there was general agreement in the late 19th century that the steady accumulation of knowledge and the progressive replacement of conjectural, that is, theological or metaphysical, notions by scientific ones was what created progress. Most scholars concluded this growth of scientific knowledge and methods led to the growth of industry and the transformation of warlike societies into industrial and pacific ones. They agreed as well that there had been a systematic decline of coercion in government, and an increasing role of liberty and of rule by consent. There was more emphasis on impersonal social and historical forces; progress was increasingly seen as the result of an inner logic of society.

Marx developed a theory of historical materialism. He describes the mid-19th-century condition in "The Communist Manifesto" as follows:

Furthermore, Marx described the process of social progress, which in his opinion is based on the interaction between the productive forces and the relations of production:

Capitalism is thought by Marx as a process of continual change, in which the growth of markets dissolve all fixities in human life, and Marx admits that capitalism is progressive and non-reactionary. Marxism further states that capitalism, in its quest for higher profits and new markets, will inevitably sow the seeds of its own destruction. Marxists believe that, in the future, capitalism will be replaced by socialism and eventually communism.

Many advocates of capitalism such as Schumpeter agreed with Marx's analysis of capitalism as a process of continual change through creative destruction, but, unlike Marx, believed and hoped that capitalism could essentially go on forever.

Thus, by the beginning of the 20th century, two opposing schools of thought—Marxism and liberalism—believed in the possibility and the desirability of continual change and improvement. Marxists strongly opposed capitalism and the liberals strongly supported it, but the one concept they could both agree on was progress, which affirms the power of human beings to make, improve and reshape their society, with the aid of scientific knowledge, technology and practical experimentation. "Modernity" denotes cultures that embrace that concept of progress. (This is not the same as modernism, which was the artistic and philosophical response to modernity, some of which embraced technology while rejecting individualism, but more of which rejected modernity entirely.)



</doc>
<doc id="7370562" url="https://en.wikipedia.org/wiki?curid=7370562" title="Biography">
Biography

A biography, or simply bio, is a detailed description of a person's life. It involves more than just the basic facts like education, work, relationships, and death; it portrays a person's experience of these life events. Unlike a profile or curriculum vitae (résumé), a biography presents a subject's life story, highlighting various aspects of his or her life, including intimate details of experience, and may include an analysis of the subject's personality.

Biographical works are usually non-fiction, but fiction can also be used to portray a person's life. One in-depth form of biographical coverage is called legacy writing. Works in diverse media, from literature to film, form the genre known as biography.

An authorized biography is written with the permission, cooperation, and at times, participation of a subject or a subject's heirs. An autobiography is written by the person himself or herself, sometimes with the assistance of a collaborator or ghostwriter.

At first, biographical writings were regarded merely as a subsection of history with a focus on a particular individual of historical importance. The independent genre of biography as distinct from general history writing, began to emerge in the 18th century and reached its contemporary form at the turn of the 20th century.

One of the earliest biographers was Cornelius Nepos, who published his work "Excellentium Imperatorum Vitae" ("Lives of outstanding generals") in 44 BC. Longer and more extensive biographies were written in Greek by Plutarch, in his "Parallel Lives", published about 80 A.D. In this work famous Greeks are paired with famous Romans, for example the orators Demosthenes and Cicero, or the generals Alexander the Great and Julius Caesar; some fifty biographies from the work survive. Another well-known collection of ancient biographies is "De vita Caesarum" ("On the Lives of the Caesars") by Suetonius, written about AD 121 in the time of the emperor Hadrian.

In the early Middle Ages (AD 400 to 1450), there was a decline in awareness of the classical culture in Europe. During this time, the only repositories of knowledge and records of the early history in Europe were those of the Roman Catholic Church. Hermits, monks, and priests used this historic period to write biographies. Their subjects were usually restricted to the church fathers, martyrs, popes, and saints. Their works were meant to be inspirational to the people and vehicles for conversion to Christianity (see Hagiography). One significant secular example of a biography from this period is the life of Charlemagne by his courtier Einhard.

In Medieval Islamic Civilization (c. AD 750 to 1258), similar traditional Muslim biographies of Muhammad and other important figures in the early history of Islam began to be written, beginning the Prophetic biography tradition. Early biographical dictionaries were published as compendia of famous Islamic personalities from the 9th century onwards. They contained more social data for a large segment of the population than other works of that period. The earliest biographical dictionaries initially focused on the lives of the prophets of Islam and their companions, with one of these early examples being "The Book of The Major Classes" by Ibn Sa'd al-Baghdadi. And then began the documentation of the lives of many other historical figures (from rulers to scholars) who lived in the medieval Islamic world.
By the late Middle Ages, biographies became less church-oriented in Europe as biographies of kings, knights, and tyrants began to appear. The most famous of such biographies was "Le Morte d'Arthur" by Sir Thomas Malory. The book was an account of the life of the fabled King Arthur and his Knights of the Round Table. Following Malory, the new emphasis on humanism during the Renaissance promoted a focus on secular subjects, such as artists and poets, and encouraged writing in the vernacular.

Giorgio Vasari's "Lives of the Artists" (1550) was the landmark biography focusing on secular lives. Vasari made celebrities of his subjects, as the "Lives" became an early "bestseller". Two other developments are noteworthy: the development of the printing press in the 15th century and the gradual increase in literacy.

Biographies in the English language began appearing during the reign of Henry VIII. John Foxe's "Actes and Monuments" (1563), better known as "Foxe's Book of Martyrs", was essentially the first dictionary of the biography in Europe, followed by Thomas Fuller's "The History of the Worthies of England" (1662), with a distinct focus on public life.

Influential in shaping popular conceptions of pirates, "A General History of the Pyrates" (1724), by Charles Johnson, is the prime source for the biographies of many well-known pirates.

A notable early collection of biographies of eminent men and women in the United Kingdom was "Biographia Britannica" (1747-1766) edited by William Oldys.

The American biography followed the English model, incorporating Thomas Carlyle's view that biography was a part of history. Carlyle asserted that the lives of great human beings were essential to understanding society and its institutions. While the historical impulse would remain a strong element in early American biography, American writers carved out a distinct approach. What emerged was a rather didactic form of biography, which sought to shape the individual character of a reader in the process of defining national character.

The first modern biography, and a work which exerted considerable influence on the evolution of the genre, was James Boswell's "The Life of Samuel Johnson", a biography of lexicographer and man-of-letters Samuel Johnson published in 1791. While Boswell's personal acquaintance with his subject only began in 1763, when Johnson was 54 years old, Boswell covered the entirety of Johnson's life by means of additional research. Itself an important stage in the development of the modern genre of biography, it has been claimed to be the greatest biography written in the English language. Boswell's work was unique in its level of research, which involved archival study, eye-witness accounts and interviews, its robust and attractive narrative, and its honest depiction of all aspects of Johnson's life and character - a formula which serves as the basis of biographical literature to this day.

Biographical writing generally stagnated during the 19th century - in many cases there was a reversal to the more familiar hagiographical method of eulogizing the dead, similar to the biographies of saints produced in Medieval times. A distinction between mass biography and literary biography began to form by the middle of the century, reflecting a breach between high culture and middle-class culture. However, the number of biographies in print experienced a rapid growth, thanks to an expanding reading public. This revolution in publishing made books available to a larger audience of readers. In addition, affordable paperback editions of popular biographies were published for the first time. Periodicals began publishing a sequence of biographical sketches.

Autobiographies became more popular, as with the rise of education and cheap printing, modern concepts of fame and celebrity began to develop. Autobiographies were written by authors, such as Charles Dickens (who incorporated autobiographical elements in his novels) and Anthony Trollope, (his "Autobiography" appeared posthumously, quickly becoming a bestseller in London), philosophers, such as John Stuart Mill, churchmen – John Henry Newman – and entertainers – P. T. Barnum.

The sciences of psychology and sociology were ascendant at the turn of the 20th century and would heavily influence the new century's biographies. The demise of the "great man" theory of history was indicative of the emerging mindset. Human behavior would be explained through Darwinian theories. "Sociological" biographies conceived of their subjects' actions as the result of the environment, and tended to downplay individuality. The development of psychoanalysis led to a more penetrating and comprehensive understanding of the biographical subject, and induced biographers to give more emphasis to childhood and adolescence. Clearly these psychological ideas were changing the way biographies were written, as a culture of autobiography developed, in which the telling of one's own story became a form of therapy. The conventional concept of heroes and narratives of success disappeared in the obsession with psychological explorations of personality.
British critic Lytton Strachey revolutionized the art of biographical writing with his 1918 work "Eminent Victorians", consisting of biographies of four leading figures from the Victorian era: Cardinal Manning, Florence Nightingale, Thomas Arnold, and General Gordon. Strachey set out to breathe life into the Victorian era for future generations to read. Up until this point, as Strachey remarked in the preface, Victorian biographies had been "as familiar as the "cortège" of the undertaker", and wore the same air of "slow, funereal barbarism." Strachey defied the tradition of "two fat volumes... of undigested masses of material" and took aim at the four iconic figures. His narrative demolished the myths that had built up around these cherished national heroes, whom he regarded as no better than a "set of mouth bungled hypocrites". The book achieved worldwide fame due to its irreverent and witty style, its concise and factually accurate nature, and its artistic prose.

In the 1920s and '30s, biographical writers sought to capitalize on Strachey's popularity by imitating his style. This new school featured iconoclasts, scientific analysts, and fictional biographers and included Gamaliel Bradford, André Maurois, and Emil Ludwig, among others. Robert Graves ("I, Claudius", 1934) stood out among those following Strachey's model of "debunking biographies." The trend in literary biography was accompanied in popular biography by a sort of "celebrity voyeurism", in the early decades of the century. This latter form's appeal to readers was based on curiosity more than morality or patriotism. By World War I, cheap hard-cover reprints had become popular. The decades of the 1920s witnessed a biographical "boom."

The feminist scholar Carolyn Heilbrun observed that women's biographies and autobiographies began to change character during the second wave of feminist activism. She cited Nancy Milford's 1970 biography "Zelda", as the "beginning of a new period of women's biography, because "[only] in 1970 were we ready to read not that Zelda had destroyed Fitzgerald, but Fitzgerald her: he had usurped her narrative." Heilbrun named 1973 as the turning point in women's autobiography, with the publication of May Sarton's "Journal of a Solitude," for that was the first instance where a woman told her life story, not as finding "beauty even in pain" and transforming "rage into spiritual acceptance," but acknowledging what had previously been forbidden to women: their pain, their rage, and their "open admission of the desire for power and control over one's life."

In recent years, multimedia biography has become more popular than traditional literary forms. Along with documentary biographical films, Hollywood produced numerous commercial films based on the lives of famous people. The popularity of these forms of biography have led to the proliferation of TV channels dedicated to biography, including A&E, The Biography Channel, and The History Channel.

CD-ROM and online biographies have also appeared. Unlike books and films, they often do not tell a chronological narrative: instead they are archives of many discrete media elements related to an individual person, including video clips, photographs, and text articles. Biography-Portraits were created in 2001, by the German artist Ralph Ueltzhoeffer. Media scholar Lev Manovich says that such archives exemplify the database form, allowing users to navigate the materials in many ways. General "life writing" techniques are a subject of scholarly study.

In recent years, debates have arisen as to whether all biographies are fiction, especially when authors are writing about figures from the past. President of Wolfson College at Oxford University, Hermione Lee argues that all history is seen through a perspective that is the product of our contemporary society and as a result biographical truths are constantly shifting. So the history biographers write about will not be the way that it happened; it will be the way they remembered it. Debates have also arisen concerning the importance of space in life-writing.

Daniel R. Meister in 2017 argues that:

Biographical research is defined by Miller as a research method that collects and analyses a person's whole life, or portion of a life, through the in-depth and unstructured interview, or sometimes reinforced by semi-structured interview or personal documents. It is a way of viewing social life in procedural terms, rather than static terms. The information can come from "oral history, personal narrative, biography and autobiography” or "diaries, letters, memoranda and other materials". The central aim of biographical research is to produce rich descriptions of persons or "conceptualise structural types of actions", which means to "understand the action logics or how persons and structures are interlinked". This method can be used to understand an individual's life within its social context or understand the cultural phenomena.

There are many largely unacknowledged pitfalls to writing good biographies, and these largely concern the relation between firstly the individual and the context, and, secondly, the private and public. Paul James writes:
Several countries offer an annual prize for writing a biography such as the:





</doc>
<doc id="51404262" url="https://en.wikipedia.org/wiki?curid=51404262" title="History of Asmara">
History of Asmara

Eritrea's capital of Asmara, 100km inland from its coast, has occupied a space in the highlands of Eritrea for hundreds of years. During the last century alone, it has been a staging ground for Italian initiatives in East Africa, collateral of Britain’s colonial politics, and victim of the expansion of Ethiopia’s empire until 1993 when it finally became the new and sovereign state of Eritrea. Only the large cluster of buildings built in the style of the Avant-Garde betrays the multifaceted history of this strange city and its urban core almost entirely unchanged for over seventy years.

Asmara – developed as a European city – is, today, an Eritrean city. Even though city planners, architects, and engineers were largely European, and members of the indigenous population were largely used as construction workers, Asmarinos still identify with their city’s legacy.

The name "Asmara" derives from 'Arbate Asmara', the name given to a village on today's site of Asmara. The name means “the women have united the four villages” and relates to a foundational story in which women forced the men of four villages to consolidate their villages into one.

Asmara was the perfect place to build a settlement; it had fertile soil, a mild climate because of its location on the plateau, and regular precipitation. While remains of prehistoric peoples have been found near Asmara; the ancient Aksum Empire ruled closeby; Islam had made inroads in the Horn of Africa from the region; and ruins of small villages found around present-day Asmara indicate the existence of peoples who had settled in the area long before the development of a city in the region, it can be said that the history of Asmara began around four hundred years before the effects of European colonization began to take hold. According to oral traditions, there were once four villages on Asmara’s plains. Attacks by wild animals and raids by other indigenous groups prompted the women of the neighboring villages to come together to discuss solutions that would help to ensure the protection of their families, properties, and belongings. The women decided that they would not serve lunch to the men of their villages until they agreed to consolidate the four villages into one. The men fulfilled the women’s wishes and built one united village, which they named Arbate Asmara. Eritreans today enjoy relaying this tale about the founding of their city, as it serves as an example of how Eritrean people have been able to come together and persevere even under the harshest conditions.

From this point on, the new village of 150 inhabitants lay in the medieval Medri Bahri Kingdom, until, after a short period when it was ruled by the Ottomans and later the Egyptians, it fell into the hands of Ethiopian Emperor Yohannes IV in the middle of the 19th century, who chose his trusted General Ras Alula to be governor of the newly-occupied kingdom. Alula declared Asmara the capital of the province, and within just a few years had increased the population of the small village to more than 5,000 inhabitants. A buoyant weekly market drew traders and builders from all over the surrounding region. Political and military conflicts between the local warlords of the Hamasien Plateau compelled Alula to station 12,000 soldiers in Asmara, so that the small city soon took on the appearance of a military camp. During the 1870s, there were at least two battles between Alula and these warlords, both of which essentially destroyed the city. Asmara’s weekly market, however, continued to take place. Emperor Yohannes IV called Alula and his troops up to Addis Abeba in 1889 to support him in his fight against the Mahdi uprisings.

On August 3, 1889, Italian troops took advantage of Alula’s absence, the power vacuum left as a result of Emperor Yohannes’ death, and the havoc wreaked by three years of famine and took over the small city. They built their fort on a hill in the village of Beit Mekae, forcing the inhabitants of this village to resettle elsewhere. At the time of General Baldisera’s arrival, the small town had 3000 inhabitants and consisted of traditional mud huts, Agdos and Hidmos. Above all, the military fort at Beit Mekae was a strategic point when it came to controlling the local population; it also served as a military strongpoint from which local uprisings, which were largely directed against the government’s power of eminent domain, could be suppressed. Eritrea officially became an Italian colony in 1890 and Massawa – already the seat of the colonial administration since 1885 – was declared the capital. At this time, because of the danger of insurrection in the highlands, Asmara was not even in the running to become the capital of the  new colony. Furthermore, Bizzon explains that Asmara was not developed enough to serve as the capital: “Politically, militarily, and even logistically, the distance between Massawa and Asmara is greater than the distance between Italy and Massawa.” The lack of an infrastructural link to Massawa prevented Asmara from developing quickly.

That, despite all of this, Asmara rose to significance has to do with the goals of Italian colonization. The interest in the occupation of Eritrea lay, on the one hand, in the settlement of Italian citizens who were able to strengthen the economy of their motherland through development of the colony, and on the other hand, to use Eritrea as a military base from which the expansion of the colony could take place. The Ethiopian Empire was one of the last uncolonized regions of Africa, and the vision of a strong, modern Italian nation after the successes of the Risorgimento, would require the acquisition of colonies if the country to become a European superpower, and to affirm the autonomy of the young kingdom. Colonial politics brought about the government’s claim to eminent domain as well as the realization of many noteworthy infrastructure projects. As a result of the takeover of Asmara by foreign militaries and because of the employment opportunities within infrastructure projects throughout Eritrea, the population of Asmara itself shrunk from 5000 to 800 inhabitants during the first five years of the Italian occupation. However, it remained an important market town on the Hamasien Plateau. The construction of the Massawa-Asmara-Railroad as well as a funicular railroad built along the same stretch that could transport heavy loads, finally allowed the Italians to use Asmara as a new base in the highlands.

With the arrival of the first governor, Ferdinando Martini, in 1897 and after advances in the invasion of the highlands, Asmara was declared the capital of the colony of Eritrea. Asmara offered a strategic military position in the highlands opposite Massawa, fortifications at the residences of Alula and Baldissera, and an increase in the quality and quantity of transportation connections in the region. The potential that the city could become a cultural-historical hub did not play any role in the decision to make Asmara into the capital. Only a few modern buildings like the Commissariato, a small jail, the troop commanders’ Villa, a club for Italian officers, and several houses were built before 1900.

These buildings surrounded the old post office building in the city neighborhood known as Mai Bela, remembered for its narrow streets and short blocks, symbols of the beginnings of modern Asmara. In 1900, a fire destroyed large parts of neighborhoods inhabited mostly by indigenous people, which would have given colonial city planners the chance to reorganize these areas in accordance with European criteria had they taken the opportunity. Early urban development as well as the construction of two fortresses in addition to Fort Baldisera took place along the East-West line, which later became the city’s central avenue, today called Harnet Avenue.

The first city plan for Asmara was introduced under Martini in 1902. At the center of the plan was a grid system, even though the topography of the city was not ideal for such a road system, as the following report describes:“I say the ‘flat ground’ of the city, but instead it is anything else but flat. Light undulations, little hills, mounds, bumps, dips and little valleys in the ground bring a truly picturesque variety to the buildings and to the streets, which therefore never appear similar to one another. They offer suitable areas and remarkable positions in which to make public buildings prominent. In short, it is a ground so made that it would be the torment of a builder from Turin, the horror of an engineer from America, but would be dreamt of by an architect who is truly artistic, a lover of the picturesque, and an enemy of uniformity.”Nevertheless, Italian planners chose to implement a grid system for Asmara’s roads. It would be a symbol of how the “civilized” brought order to the organically developed neighborhoods of the natives, and would, in this way, highlight the so-called superiority of the colonizers. 

Above all, the first city plans should be seen as an expression of the power relations between two groups. The plan from 1902 had already divided the city into three zones: a zone for Italians which included the city center, a zone centered on the traditional market meant for other European groups like Greeks and Jews, and an unplanned zone for indigenous folks, located outside of the northern city border. A fourth zone for industry was envisaged in the next plan of 1908. Additional plans by planner Cavagniari, which were decided upon in 1913 and 1916 respectively, strengthened the principle of racial separation, which then became increasingly more rigid during the beginning of the fascist era in the 1920s.

The plans saw Asmara first and foremost as a new home for qualified skilled laborers from Italy. The spatial model of the garden city would be applied to the new European quarter, and would include broad, tree-lined boulevards, residential streets, and houses situated behind Bougainvillea hedges, built so that they did not crowd one another. Similarly, Asmara became an Italian city, prompting Eritrean journalist Emanuel Sahle to say: “Now Eritrean mothers who travel to Rome to see their relatives there wonder at how the citizens of Rome dared imitate ‘our dear and beautiful Asmera [sic].’” The fact that Eritreans could express appropriation and affection in this way may sound astonishing, as Asmara was not only predominantly Italian, but Eritreans themselves could only make limited use of the capital.

The institutionalized racial separation central to Italian colonial politics was not designed to account for the indigenous population, who would be displaced as the expansion of the industrial zone reached the unplanned quarters to the north. Eritreans who owned property in the city center, or the European quarter, were forced to move out and sell their land. In 1908, the Governor issued a decree that, in the interest of promoting and maintaining “public order”, declared that the first zone was meant only for Italians and citizens from other European countries, whereas the sudditi coloniali, or the “colonial subjects”, were only allowed in the indigenous quarter and the mixed Market zone. Only the Askari, indigenous soldiers who served in colonial troops, were allowed to build their huts in areas near the Italian residences, where conditions were better than those endured by their compatriots. The untypical proximity of the Askari to the Italians illustrates the deep trust of the Italian administration in the indigenous troops. In 1926, colonial minister, Luigi Federzoni, described the Eritrean regiments as “the most solid, effective and safe pillar of our military might in all our African colonies.” This relationship lasted throughout the Italian occupation of Eritrea, and is more than likely an important factor in determining the good-naturedness of Eritreans toward Italians today.

The first plans led to a phase of intensive building, which ultimately resulted in an increase of Asmara’s population from between 800 and 1,900 people to more than 8,500 people in 1905, of which around 1,500 were white. This dramatic population increase exacerbated both a growth in trade and a rise in employment opportunities, as well as an upsurge in the number of people moving from rural areas to cities. In 1910, the city’s population had grown to around 37,000 people. During this period of growth, many important buildings cropped up, including the Palast des Gouverneurs (today the residence of the president), the first Italian school, the cathedral, the theater, and the high court, all of which are located on the centralized Corso del Re, later known as the Viale Mussolini, and even later as Harnet Avenue. Asmara’s road system and the important connections between Asmara and the cities of Keren and Decamhare were also built during this time period, along with the Massawa-Asmara-Railroad Line, which was finally finished in 1911. 

With Mussolini’s rise to power in 1922, Asmara began a new chapter in its history. During the fascist occupation, the construction that took place in the city was by far the most formative. With Mussolini’s efforts put toward building an Italian empire, the Eritrean capital became increasingly important in the years that followed, and in the 1930s, it became one of the most important sites in Italian East Africa.

The construction boom, which has left its mark on Asmara evident even today, did not begin immediately upon Mussolini’s rise to power in 1922. Asmara was initially little more than a some military installations and a colonial settlement. It grew into a small town by the 1920s. Despite the strategic location of Asmara, at first, the capital was really only used as an administrative headquarters for the Italian colonial regime. Contrary to the statement that Asmara had already grown to the size of a small city 30 years before, Bodenschatz claims that Asmara was only the size of a small village until the Ethiopian invasion in 1935. According to Bodenschatz, in the 1920s Asmara had around 18,000 inhabitants, of whom 3,000 were Italians. The focus on city development centered on the erection of administrative buildings and accommodations for the political and military elite.

Modern architecture, equally lauded by architecture aficionados and city historians, slowly found its way to Eritrea in the 1920s. At this time in Italy, architecture was relatively uninfluenced by European avant-garde modernism which had taken hold in Holland, Germany, and France in particular. It wasn’t until 1927 that Gruppo 7, a coalition of young architects from Milan, developed the Italian variation of avant-garde modern architecture called Rationalismo. Until 1935, the majority of structures in Asmara were built in traditional styles from established architecture schools like Novecento and the Scuola Romana, both of which appropriated certain formal elements from the Italian Gothic, the Renaissance, the Baroque, the Romantic, and the Classical time periods. The architecture remained largely traditional and was based mostly on the models of the Italian motherland. In Asmara’s most representative buildings built before 1935, this historicism is especially apparent. Asmara’s theater exhibits stylistic elements from both the Romantic and Renaissance periods. The Bank of Eritrea’s building is built in the Neo-Gothic style, and the Governor’s Palace as well as the post office building were constructed using neoclassical elements. To position Mussolini’s rule within the time period when modern architecture found its way to the Eritrean highland is, at least for the first half of his reign, incorrect. In reality, nearly all of the avant-garde structures were conscripted and built between 1935 and 1941. Even if the building boom did not begin until the middle of the 1930s, Mussolini’s fascist ideology would have already influenced the urban development plans during the 1920s and the beginning of the 1930s. 

At the beginning of the 20th century, the citizens of Asmara were separated by race. Neighborhoods for indigenous and European populations were clearly delineated from each other, however a public area could be used by everyone. The preliminary plans to separate people from different ethnic groups were expanded and concretized under Mussolini. The improved plans from 1930 laid out how the city would be divided into four clear and separate zones: the residential quarter for the indigenous population in the north, where there were already high population densities and erratic structures; the diagonal blocks of the industry zone; the “Villa quarter” for the Europeans, located south of present-day Harnet Avenue; and, the mixed zone around the market. Located in the market zone were administrative and commercial entities that were of equal significance for both groups of people (i.e. the indigenous and the European). Additionally, this zone housed centralized cultural institutions, and was interspersed with additional housing. The categorization of inhabitants by race reached its peak under fascism, with the strict separation of indigenous folks and occupiers.

With the plan to raid Abyssinia and to incorporate it into the African colony, Asmara’s cityscape began to change. More and more soldiers were being stationed in Eritrea, and the city had only a short time to manage a rapid increase in its population. Between 1932 and 1936 alone the number of inhabitants rose from 18,000 to 98,000 people. Until 1935, administrative and commercial structures were expanded in order to build up Asmara as a military base for the upcoming war. The village became a bustling administrative city and a trading hub, that required its new inhabitants address its rising sanitation, building, transportation, and safety needs. And so, housing, businesses, recreation services, as well as churches appeared out of thin air in a matter of a few short years. 

Before delving deeper into the details of Asmara’s development, a brief summary of the origin and evolution story of this Italian colony on the Horn of Africa should be given in order to better contextualize, and hopefully make clearer, Asmara’s extreme growth at the end of the 1930s. 

In 1935, Italy, a significant colonial power, had already occupied Eritrea for nearly 40 years. The fascist regime nevertheless aspired to expand its colony located on the Horn of Africa. Already in 1896, Italy had unsuccessfully battled Adwa with the aim of taking Ethiopia. Driven by this disgrace, in 1935 Italian troops invaded the Abyssinian empire without any declaration of war. Asmara, because of its location in the highlands, was an ideal military location, which offered Europeans a relatively comfortable climate and was a good site strategically. And so, Asmara became the main supply base for the Italian offensive during the Abyssinian War. With a vengeance, the regime under Mussolini set their sights on taking the kingdom. With nearly 400,000 men and 450 aircraft – these comprised half of the Regia Aeronautica – the Italian forces were far superior to the Abyssinians. The Italians also had the support of the Askari, who followed the Italian troops during the war. During the war against Ethiopia, 60,000 Askari fought on the Italian side. In the almost one-year-long fight, during which even poison gas was used, around 150,000 Abyssinians were killed. Historian Hans Woller marks the war in Abyssinia as the bloodiest military conflict to take place after WWI.

After claiming victory following the bloody siege of Addis Abeba in May 1936 that ended the war, the African colony – called Orientale Italiana by Italians – now included Eritrea, Ethiopia, and extensive stretches of land along the coast of Somalia; it was, as a result, the third largest colonial empire in the world. On May 9, 1936 from the Palazzo Venezia, Benito Mussolini announced to a jubilant crowd: “L’Italia ha finalmente il suo Impero“ [Italy finally has its empire]. “Im Konzert der Mächte trat das faschistische Kriegsregime fortan als zweites Imperium Romanum auf, das in Ostafrika – wie einst die Römer rund um das Mittelmeer – eine, wie sie es nannten ‚Zivilisierungsmission’ zu erfüllen hätte.” Only ten days after the proclamation of the Empire, Mussolini ordered the construction of a new transportation infrastructure in East Africa. A road network 2850 kilometers long would expand across the entire colony. At last, the hour of city planning and designing had arrived. 

Mattioli labels Italian East Africa as “a playground for architects and city planners.” After the war, Italian East Africa was to become a settlement colony. With its six administrative districts, anywhere between 1 and 6.5 million Italians from the motherland could reside in the city. The effort to colonize the Horn of Africa with Europeans required an enormous amount of infrastructure planning. Streets, administrative facilities, housing, and commercial buildings had to be planned out and built. Additionally, engineers, architects, and urban planners who had invested interests in living out their creative urges on the playground of East Africa were needed. Just a week after the proclamation of the empire, the most influential architect in the regime, Marcello Piacentini, turned to Mussolini and proposed that he be hired to coordinate the necessary construction projects under one general plan. Never before in history had such an  opportunity presented itself: to systematically control the architectonic aspects and urban development of a territory that had been, until this time, completely untouched by any previous “civilization initiatives”, Piacentini said.

An announcement made in 1936 stipulated that Italian architects and city planners would not just be those who were marginally aligned with fascist ideology, but those who wholeheartedly support it. In his speech, the architect Carlo Quadrelli outlined the basic principles that architects in the colonies should follow when handling the housing issue, which he formulated as follows:“[...] I consider the following basic principles to be essential:

But it was not only Italian architects who sought council with Mussolini in hopes of having their ideas realized under the Duce’s rule. Le Corbusier wanted his own plans implemented on the East African “playground”, and from 1932 onwards, he continuously pursued the opportunity to meet privately with Mussolini. His attempts were made in vain, however, and so he wrote to the Italian ambassador in Brazil and recommended that the social structure of the residential city of Addis Abeba be abolished. He made concrete plans for a dominant central axis that would strictly separate the Europeans’ residential neighborhoods from the indigenous population. This abrasive plan based on segregation did not take any of the local traditions nor the existing city structure into account.

Independent of Le Corbusier, the policies requiring the strict racial separation of the population were followed. Not only was the separation of races of the utmost importance when writing development policy for implementation in East Africa, the urge to perpetuate fascist ideology through urban planning without regard for existing structures and cultures was as well. Just as in Italy, architecture and urban construction policies were deployed in the colony in a dialectic of destruction and radical new design. The concepts of dominance, order, and racial segregation were to be inscribed in the cities of the Impero. “In the second half of the 1930s, Mussolini's urban planners designed projects that treated settlements in Italian East Africa not as historic cities but as tabula rasa.” They regarded the people of East Africa as “barbarians” who did not live in real cities, and who did not have the right kind of housing nor culture. In order to realize the plans of an imperial spatial order, planners under Mussolini did not hesitate to destroy entire historical districts.

From 1935 to 1940, one large project known as Africa Orientale Italiana consumed about twenty percent of the entire Italian national budget. These enormous financial expenses illustrate the importance of the little colony on the Horn of Africa. Mussolini was obsessed with the idea of the Impero, not least of all because it brought him great sympathy in his home country. When looking at the importance of the colony for the fascist regime as well as for the Italians, it quickly becomes clear that Asmara was not a meaningless colonial city “just somewhere” in Africa. Asmara stood for colonial triumph. The attention that the city attracted in the Eritrean highlands provided an ideal platform from which to demonstrate political, economic and social superiority.

After 1936, the cities in East Africa experienced a genuine building boom. As stormy as the city can be because of its location 2,350 meters above sea level, urban development did not take off anywhere else during the Africa Orientale Italiana project like it did in Asmara. The victory over Abyssinia represents a turning point in the history of Asmara, and the years up to 1941 were to become the Eritrean capital’s most formative.

In order to meet the demand for new buildings, many workers were brought to Asmara. There was plenty of work from 1935 to 1941 in the city whose ability to grow seemed limitless. Initially, locals were only hired to complete menial tasks. This changed, however at the end of the 1930s, as the high number of European workers was simply too expensive. Funds for construction work were mainly provided by the Italian state. Asmara soon became known as the most advanced center of the Impero and was celebrated as Piccola Roma. In a guidebook from 1938, Asmara was presented as the most graceful Italian city with an average population size; it was portrayed as being completely new and full of youthful energy directed towards achieving a truly imperial future. This youthful energy was especially reflected in the funicular railroad built in 1937. It was the longest cableway in the world at that time and connected Asmara with the harbor town of Massawa.

In Asmara, as in other Italian colonial cities, there was no consideration given to the existing settlement structures of the local population. Nearly all traditional huts - the Agdos and Hidmos - were demolished to clear the way for new construction projects. Only in the north, in the official indigenous settlement, did Agdos and Hidmos remain untouched and isolated from the development taking place elsewhere in the city. After the destruction of most of the huts in Asmara, 45,000 local residents were moved to the citta indigene. 

In the 1930s, the city map became the most important topic of discussion in the field of modern architecture. From the point of view of the international representatives of the New Building (Neues Bauen) Movement, cities had not yet adapted to the demands of the machine age. The architects and planners of this movement criticized the orderlessness of the cities and warned that the basic biological, psychological, and hygienic needs of the inhabitants were at risk. To them, this lack of order in Asmara always meant that the plans for complete racial segregation had not yet been fully implemented. With the resettlement of the locals and the demolition of the Agdos and Hidmos, developers had gained additional room for growth in the middle of the 1930s. Italian architects were faced with a wealth of new, extensive building tasks. The designers of the New Building Movement deliberately broke away from the tradition of classical Italian architecture, but also from local African architecture. In Asmara’s historic city center, there is only one building that is recognizable as Eritrean architecture and as stemming from Eritrean culture. It cannot be said that Asmara is a fusion of modern architecture and African highland culture, as Edward Denison writes. Aram Mattioli finds much more critical words: "In Asmara, it was not a question of fusing together different cultures, and certainly not an aesthetic, creative reception of European-influenced modernism by African architects, but an imperial and racist societal experiment that used a modern design vocabulary."

From the end of the thirties, when the war against Abyssinia had ended, the Italians concentrated on the expansion of civilian institutions. Soldiers and craftsmen had families that had followed them, thus the city had to move forward with the expansion of educational institutions and housing. At that time, ideas about a policy of racial segregation were being articulated more and more clearly. This was, however, not only the tenor of Italian politicians; racial separation was the standard for all European colonial powers in Africa. In 1937, a law came into effect that criminalized cohabitation of Italian citizens and black women. One year later, the regime’s implementation of radical racial laws followed. The laws took into account all areas of life, and were not only meant to pursue social and political objectives, but also biological and economic goals. Above all else, though, they served to emphasize the superiority and the racialized hierarchical position of the occupiers, as well as to secure a place among the privileged for the future. And so, it was hardly surprising that the architect Cafiero was commissioned in 1938 to create a new development plan for Asmara. He was to express the new racial laws in a plan and to rearrange the city with a vision of racial segregation in mind. Therefore, in Asmara, racial segregation was pursued from the legal level as well as from the different but intersecting level of urban planning.

However, the restructuring of Asmara was difficult. Asmara had grown rapidly in recent years, and urban life had already taken hold in most parts of the city. In order to carry out the separation of races, Cafiero used the pre-existing zones. The mixed neighborhood around the market square now served as a buffer zone to clearly distinguish the settlements of the natives in the north from the neighborhoods of the occupiers in the south and west. This buffer zone, consisting of the industrial and commercial sectors, was to be the only area in the city where white and black people could meet. However, access was permitted only to educated Eritreans. In addition, a green stripe was painted along what is now known as Segeneyti Street, which served as an additional barrier that locals were not allowed to cross. Cafiero anticipated that one hectare of land would be allocated for every 380 locals, while only 140 Italians would inhabit the same amount of space in their zone.

Not just the residential areas would be separated from each other according to racial doctrine. Planners ensured that there were racially segregated restaurants, theaters, infirmaries, churches, brothels, and sometimes even separate access routes to the different functional zones of their cities.“Mussolini’s racial laws called for a complete separation of the population along racial lines. This new strategy was far more brutal and saw the persecution of Eritreans taken to a new level.”Fearing that political antagonists might begin to pop up among the educational elite, Eritreans’ access to schools was reduced to a minimum. There could be no doubt as to who was the master and who was the slave, and to ensure this remained the case, an educational system was set up which limited the entire school career of black children to three years. There were absolutely no other schools for these children to attend.

The living conditions for the Eritrean population in Asmara were bad. The locals’ quarters were completely neglected in the plans of the architects and city planners. There are no avant-garde buildings erected by the young aspirants of the 1930s; the roads remained unpaved and the connections to the electricity and water supply did not find their way into the neighborhoods in the north; there was no foundation for establishing an intact infrastructure for medical, educational, or sanitation facilities. The Italian population lived only a few meters away. By 1941 water connections and a sewer system were installed in the Europeans’ zones. Medical infrastructure was secured, and people enjoyed strolling up and down the streets, which were lined with trees and plants, and always guarded by patrolling police.

The Italian occupiers designed Asmara according to their ideas. So that they did not miss anything while in the East African colony, bars, restaurants, brothels and cinemas sprouted up all over the city. The high density of cinemas in Asmara is still as impressive today as it was back then. At the end of the 1930s, numerous cinemas had been built, all within the shortest possible timeframe, including Cinema Impero (1937), Cinema Capitol (1938), Cinema Roma (1937) and Cinema Odeon (1937). All of these cinemas were accessible exclusively to Europeans. However, locals were also to have their own cinema. In 1936, the architect Inginio Marabelli built Cinema Hamasien in the city’s northern zone. Over time, the number of leisure activities meant to entertain the colonial masters increased exponentially. There were golf courses, tennis clubs, soccer tournaments, bicycle racing clubs, and motorsport events.

Motorsport events were held in Asmara for a reason. At the end of the 1930s, the number of Italian-made automobiles exported to Eritrea had exploded. Within a few years, the traffic volume in Asmara was higher than it was in Rome. With approximately 50,000 cars in Asmara in 1938, there were officially enough for each Italian in the city to have one. In the years 1936-1938, 15,158 cars and 13,719 motorcycles from different manufacturers were delivered to East Africa. This constituted about a fifth of the auto industry’s total exports in 1938, and in 1937, about half. The automotive industry flourished in Asmara. Not only did the number of domestic car imports grow steadily in Italy, automobile companies themselves built prestigious branches in the Eritrean highlands to improve their image and sales as well. In the 1930s, for example, these automobile companies commissioned architects to design their representative offices in Asmara. Oftentimes, unknown architects and engineers were planning and constructing state-of-the-art buildings for the most important automobile companies in the world. Among the clients were Agip, Pirelli, Alfa Romeo, Lancia, and, of course, Fiat. Even the leading automobile club in Italy, RACI, opened a branch in the Eritrean capital to attract new members from the growing group of citizens with cars. The use of avant-garde architecture was a convenient means through which to attract the attention of potential car buyers, and the colony in East Africa offered the most ideal conditions for creating such striking structures. In Italy, architects had to tolerate the limitations of old building structures. In Asmara, on the other hand, the designers had no specific restrictions. Far from the what the motherland was able to provide, Eritrea offered the best conditions for innovative and experimental architecture.

It is not the case that Asmara consists only of buildings commissioned by Mussolini or his governors. It is true, however, that all town measures, including development plans, were always managed by the occupying regime, which included what was possible and permissible in terms of building development. In reality, private individuals and, above all, industry, owned a large share of the city, including many individual buildings that still characterize the cityscape of Asmara to this day. Built in 1938, the Fiat Tagliero petrol station is a particularly impressive building in the capital city, and is perhaps the most famous. Designed by Italian architect Giuseppe Pettazzi, the gas station takes the form of an airplane with its 30-meter-long, cantilevered, tapered concrete wings, which extend from a tower-like central structure. The aircraft was an important symbol for the futurists at this time because it symbolized progress and speed. Frequently, architects used the symbolism of the airplane to demonstrate how superiority was achieved through progress.

Buildings that were meant to symbolize the progress of the time were the Shell Service Station (1937), which is reminiscent of a passing vessel with its porthole-shaped windows and its curves, or the Bar Zilli building, which is shaped like a radio from the thirties. From the main entrance, the same number of windows and doors appear at regular intervals, distributed in perfect symmetry. The round windows are meant to remind onlookers of radio knobs. Another one of Asmara’s symbolic buildings is the former headquarters of the Fascist Party; today it houses the Ministry of Education. The house looks like an “F” lying on its back. This was no coincidence, as the “F” in this case stood for Fascismo.

Time and again, the Italian architects designed buildings that stood for progress. Unlike in Italy or in Nazi Germany, there were no over-sized monumental buildings constructed in Asmara. Nevertheless, the architectural styles and currents of the time are very present, and by taking a look at the context in which these buildings were constructed, the fascist ideology is also reflected in the present-day city landscape. In this demonstrative way, the modern was vividly juxtaposed with the unmodern, symbolizing the so-called superiority of the Italians over the Eritreans. Unlike Hitler, Mussolini was open to modern architecture. He had a passion for speed, automobiles, and airplanes, which is reflected in many of Asmara’s buildings. Mussolini stood for classical tradition and, simultaneously, for modernism as it was conceived of by futurists and rationalists.

In the years between 1935 and 1941, numerous buildings in the styles of Rationalismo and Novecento were erected within a area, and although many others were planned, they would never be realized. The buildings ranged from small, one-story apartments to giant high-rise office buildings. About 10 years after Gruppo 7 had outlined its basic principles for reforming Italian architecture in La Rassegna Italiana magazine, numerous buildings were built in Asmara according to these specifications. The group’s principles were based on the idea that architecture had to be rational and reduced to simple forms, and that all architectural ornamentation had to be renounced.

In addition to numerous buildings, the colonial administration also built roads and squares, which were also supposed to represent the fascist regime and its power over the Eritrean population. The Viale Mussolini (today Harnet Avenue), the Piazza Roma (today Post Square) and the Viale de Bono (today Semeatat Boulevard) were built to unite the masses and to display the power of the occupiers in the form of festivals, parades and marches. With its administrative buildings, the Cinema Impero, the theater, St. Joseph's Cathedral, high-rise apartment buildings, and numerous bars and shops, Viale Mussolini was the center of the city. As the day faded into the night, Viale Mussolini was used as a promenade – the so-called Passeggiata. Italians walked up and down the street without any particular aim. The indigenous population, however, had no access to the palm-lined boulevard during the day or at night.

The idea that Asmara would continue to grow steadily was not in question; more and more Italians were moving to the Eritrean highlands. In 1939, 98,000 people were already living in Asmara. Of these, 53,000 were Italians and 45,000 were Eritrean. At the end of the 1930s, the population of Italians living in Asmara peaked around 70,000. However, growth would soon come to a screeching halt. During the Second World War, the Great Powers, Italy and the UK, fought battles all across Eritrea. After a nearly one-year-long military operation, Mussolini's troops were finally defeated by the British in 1941. Until the fall of his Empire, Mussolini had never before set foot in Italy’s small colony on the Horn of Africa.

The British must have been very impressed by this European city in Africa. After the capture of Asmara, the British Information Ministry reported that they had conquered a European city with extensive boulevards, fantastic cinemas, outstanding fascist buildings, cafes, shops, two-laned streets, and a first-class hotel. With the British takeover, the compulsive need to build in the Eritrean capital came to an abrupt end.

To this day an outstanding and perhaps unique collection of modern buildings gives impressive insight into the world of  Italian modernist architecture of the 1920s and 1930s. In particular, the avant-garde buildings, which were created between 1935 and 1941 and were designed and realized according to rationalist principles, are looking for their equals across the world. But equally important for the time of the Duce was the policy of racial segregation, especially in Asmara, which was refined in the urban development plans and which led to a complete separation of natives and occupiers by the end of the 1930s.

On April 1, 1941, after the Allies’ victory in Eritrea, Italy was forced to awaken from the dream of having the ideal colony in East Africa, which was already taking shape in Asmara. A transitional administration under British control was established immediately to organize the former colony. As the Second World War came to a close, this administration was renamed the British Military Administration (BMA). The task of officials in the BMA was to oversee the transition of the Italian colony to an undefined political entity. The British occupation did not bring about any significant changes in the construction of cities nor in the architecture of Asmara, but it did mean the beginning of the struggle for independence for the Eritrean people, which was to last 50 years. It would have a decisive impact on the identity of the Asmarinos, especially regarding their relationship to the Italian colonial period, and to a lesser degree on the structural form of Asmara.

The British Empire had no interest in managing the country as a separate colony, and instead spoke of the "Eritrean eradication". The land of the Eritreans was to be divided and distributed to other countries. The highlands, whose population was only referred to as the “Abyssinians” – with the same term they used to refer to the Ethiopians – was to be incorporated into British Sudan, while the lowlands, which the British simply said to be “inhabited by Muslims”, would be given to Ethiopia. It was as a result of these designations that the chief commander of the East Africa Command wanted only to provide 19 military officers to the administration of Eritrea, including Brigadier General Kennedy-Cooke, eight British officers, and nine Sudanese police officers. Since the British were still in the middle of the war and resources were needed in other places, the Italian administrators, who had for the most part not left Asmara, decided to continue to manage the country – now under the supervision of the British. All departments except the police were left to the Italians. Through the use of anti-fascist propaganda, an agreement was made with the Italian population, which was comprised of approximately 60,000 people suffering from food and water shortages.

The 100,000 Eritreans in Asmara suffered from the same shortages, lived under poorer sanitary and building conditions, and had to accept displaced people from all over Eritrea, but were hardly supported by the BMA. At the beginning of the 1940s the Askari, who had supported the Italian war in Somalia and Libya, returned. The disrespect for the Eritrean population was demonstrated in part by the deployment of the divide et impera [divide and conquer] strategy. The traditional system rooted in an understanding of the importance of citizen participation, which to a certain extent had survived the Italian colonial era, was undermined by the BMA. Political divisions were encouraged in order to prevent the population from engaging in decision-making processes. In Tessenei, for example, while political groups were allowed to meet separately, a conference aimed at discussing the future of the region open to all groups was strictly prohibited.

The British administration implemented a policy aimed at rigorously dismantling the country, before handing Eritrea over to Ethiopia, during which it systematically sold manufacturers and their equipment off, sending them abroad or to other British colonies. Among other things, the dry docks in Massawa, a cement factory in Massawa, the Gura airport, the Massawa-Asmara funicular railroad, and parts of the railroad line from Massawa to Asmara were dismantled and shipped to Sudan, Egypt and India. A statement from the terrified council of elders of the former industrial city of Decamhare reads: “the English did not leave behind even pins, not even needles from our factories, they took away everything.” Despite this loss of valuable industrial faculties, Asmara retained its importance as a commercial center.

Among the British, racial separation was at least partially abolished. A training program for civilian administration was offered to Eritreans for the first time, and the first Eritrean teachers' schools were set up. As a result, from 1943 onwards, schools were open and institutionalized education made accessible for local children at last. Under Italian rule, Eritrean children had been prohibited from attending elementary schools. For this reason, a secret school, which was hidden in the bell tower of the Protestant church, had been established. By 1954, the British Administration had built over 100 schools for locals. In addition, an active public press that published in Tigrinya, Arabic, and English was launched.

An outstanding example of how Italian racial segregation was overcome began on an early morning in 1943. Eritrean workers found no seats on a bus that was to take them from Asmara’s outskirts to work in the city center, and decided to take the places normally reserved for Italians. As those Italians who were running late began to arrive and find that their seats had been taken, they became outraged, and a fight broke out. The problem was ultimately brought to the British administrator. He stated that he could not initiate any changes as long as the Italian laws were still on the books, but suggested that the Eritreans organize their transport independently. Within a few days, the locals had collected enough money, bought a truck, and nailed benches to the truck bed. Due to a loss of business, the Italian owner of the bus had no choice but to sell his bus to the Eritreans. The Eritrean workers were now the proud owners of a bus and had free seats.

With the end of the Second World War, Eritrea was experiencing a severe crisis that had led to rising unemployment. While the Italian population was supported by subsidies, the local population had to fight for survival. This meant that they were stuck trying to reclaim their lands which were formerly expropriated by the Italians and now managed by the British. A deep resentment began to develop from the resulting anger within the indigenous community. In the eyes of the Eritreans, the British Military Administration was worse than the Italians, which they perceived as less duplicitous and sneaky. The BMA seldom kept their promises and used the Eritreans, for example, by imposing high fines if Eritreans peacefully sought to use their right to expression; this led more and more frequently to the development of protests into riots.

In 1946, Sudanese soldiers serving under British mandate massacred 70 Christian residents in the poor Abbu Shaul district. Rapid military intervention on the part of the British would have been the only way this tragedy could have been prevented. At the funeral of the victims, Eritrean Bishop Markos laid an accusation against the British:“We Abyssinians had longed for the British to come and liberate us from [our] Fascist yoke. We had trusted you to lead us to freedom. But what have you done? You have given us the freedom to die, the only freedom you have given us so far. Neither the Italians nor the British, in whom we had believed, will help us. So we must return to our Mother Ethiopia who will receive her Abyssinian children with open arms.”Bishop Markos put voice to the way people felt across the country. The hope of liberation, which the Eritreans had once associated with the takeover of the British, gave way to disappointment. At the same time, Markos’ words were those of a well-meaning Ethiopian. A long-standing desire for political change was palpable: "[...] most Eritreans (Christians and Moslems) were united in their goal of freedom and independence." The voices of the local population were, however, split between separatists and supporters of reunification with Ethiopia. Separatist movements gained ground, however, according to a classified estimate by the US Department of State in August 1949, and about 75% of Eritreans supported independence.

An Eritrean delegation to the UN General Assembly in 1952 noted that the ideas presented there regarding Eritrea’s future strongly suggested that Eritrea be given to the US-backed Ethiopia. Haile Selassie had already spent a lot of energy on building a positive relationship with the United States. In 1950, he had sent members of his own security service to support U.S. troops in the Korean War, and in the same year had signed a contract that would allow the U.S. to build a strategically-located military command center in Eritrea. The United States itself did not recognize Eritrea as a sovereign nation, instead saw it for its strategic value: “From the point of view of justice, the opinions of the Eritrean people must receive consideration. Nevertheless, the strategic interests of the United States in the Red Sea basin and considerations of security and world peace make it necessary that the country has to be linked with our ally, Ethiopia.” Finally Ethiopia’s wish was fulfilled and Eritrea became an Ethiopian province.

Two assumptions have dominated the discussion about the relationship between Ethiopia and Eritrea over the past 70 years. First, during Haile Selassie’s regime, the Ethiopian people and the international community first became aware of the idea that Eritrea is a natural and logical part of Ethiopia. A common history, religion and culture, as well as the belief that Eritrea is Ethiopia's access to the sea, were ideas used to serve as support for this viewpoint. The second premise, supplemented by the first, is that Eritrea was economically weak and its survival was dependent on Ethiopian resources. The country had been cut off ethnically and linguistically, and was merely an "Italian creation without the conditions for a state." Leading world powers like the United States and Great Britain supported this viewpoint in order to meet their own strategic needs, thereby leading the still relatively young UN in initiating a process of federation through which Eritrea would be subsumed under the Ethiopian crown.

Thus, according to a federal act passed in 1952 in the United Nations, Eritrea was to be bound to Ethiopia as an autonomous region, despite the fact that this did not at all reflect the will of the people. Asmara had to give up its title as a capital, and subsequently sank slowly into provinciality in the years that followed. Though Asmara did not undergo any noteworthy changes to its appearance during Ethiopian occupation, the shift in how Eritreans understood their identity was dramatic as they struggled under new foreign rule. The almost 40 years from 1952 to independence in 1991 were decisive in terms of understanding what is happening with policy in Eritrea today, and thus also for dealing with the country’s legacy.

Ethiopia has consistently interpreted Federalism in ways that have benefited itself. The Ethiopian state did not leave Eritrea any room for development, although according to the Eritrean Pact, the responsibilities of each of the three branches of government in the separation of powers model were to be carried out independently. The President of the Eritrean National Council, Ali Redai, filed a complaint against the British Consul: "Ethiopians never answered letters, never gave answers to specific queries, and in fact ignored the Eritrean government." Almost all of the administrative bodies and offices were occupied by Ethiopian officials and the Ethiopian military was reportedly in Eritrea for security reasons. Customs, postal, telecommunications, railway, defense, justice, and transportation operations were all run by Ethiopians - most of them inefficiently.

Government officials in Addis Abeba constantly introduced ever-stricter laws. Eritrean political parties were banned, the Eritreans were prohibited from collecting their portion of customs and tax earnings, and Eritrean newspapers were censored. While the early years of the Selassie regime saw some significant developments in Asmara, such as the construction of churches, mosques, schools, and hospitals, later a lack of investment and a decline in industrial activities led to the stagnation of urban development in the city. The few industries remaining after the retreat of the British were dismantled and relocated to Addis Abeba to ensure that every economic capacity of the country was undermined. In 1959, native languages Tigrinya and Arabic were forbidden as instructional languages in schools and universities, and instead Amharic was introduced; for many Eritreans, this meant that making any progress in their education had become impossible. Student protests and boycotts called by civil officials and factory workers were answered by the Ethiopian police with increasing violence. A general strike in 1958 left many dead and injured, and in other protests Eritreans were arrested or forced into exile. Appeals to the United Nations were ignored.

The change of the road names in Asmara also had a significant impact on the city’s identity. In honor of a visit by the British Queen in 1965, Haile Selassie renamed Viale de Bono, now Nakfa Avenue, Queen Elizabeth II Avenue. The central Corso Italia, today Harnet Avenue, which was still called Viale Benito Mussolini before the British occupation, was renamed Haile Selassie I Boulevard. Almost all of Asmara’s street names were changed during Ethiopian occupation in order to repress the memory of the Italian past and to establish Ethiopian authority in regard to the city’s identity itself.

Haile Selassie, in an open policy, did not hide the fact that Ethiopia "is interested in the country and not in the people of Eritrea", especially with regard to Muslim communities, whose villages were systematically burnt down by Ethiopian troops, and whose inhabitants were massacred throughout the 1950s, 1960s, and during the first half of the 1970s. Haile Selassie dissolved Eritrea’s autonomy bit by bit, for example, through the demotion of the autonomous government to a mere administrative body in 1960. Two years later, with the United Nations' approval, Ethiopia decided to ultimately end the federation. Through the besiegement of Asmara, Ethiopia forced the Eritrean parliament to dissolve itself completely, and subsequently annexed the whole of Eritrea. Soon after, Eritrean opposition leaders came to the conclusion that the only hope of resisting Ethiopian rule lay in armed struggle.

In 1974, when Haile Selassie was overthrown, the resistance movement was fighting not only a daring guerrilla war against the Ethiopian army, but also a bitter internal struggle. The movement had long since broken apart into hostile factions, mostly because of differing political views. The Eritrean People's Liberation Front (EPLF), which was founded around 1970, finally succeeded in recruiting many former enemy fighters. Fighters’ passion was for the war of independence and less so for handling the movement’s internal conflicts, so the fragmentation had no lasting effect on the unity of the Eritreans. The central strength of the resistance’s struggle for independence was the active encouragement of members from all religious communities associated with the movement to participate.

In the meantime, the Derg, the Ethiopian military junta from 1974 to 1987, had a similar Eritrean policy as its predecessor, Haile Selassie, though with much more brutal means and the support of the Soviets, Cubans and East Germans. Asmara was transformed from an attractive city into a largely isolated military camp through what was known as the "Red Terror". Ethiopian army divisions occupied entire districts, displacing inhabitants. Italian buildings were turned into prisons. Yet, Asmara's importance as a trade center grew and, contrary to all expectations, was left almost unscathed. Building activity in Asmara even recovered slowly, with the construction of individual high-rise buildings, as well as an expansion of the city administration building and the Ambassador Hotel located on present-day Harnet Avenue. Perhaps it was the limitations on urban development during this period that ultimately saved Asmara’s character. Particularly noteworthy is the monumental stadium on Bahti-Meskerem Square at the eastern end of Harnet Avenue. The monstrous concrete terraces were originally supposed to stand on both sides of the square to allow room for the communist rulers’ political and military to take place. However, only one part has been completed, so that today there is mostly just a barren landscape - an appropriate memorial to the iron-fisted Derg regime.

Towards the end of the 1980s, the Soviet Union decided against extending the cooperation agreement with Ethiopia. Without the resources of the Soviet Union, and due to a drought and simultaneous economic crisis in both countries, the morale of Ethiopian soldiers fell. Many of them served as mercenaries without real conviction for the fight against the Eritrean resistance, which allowed members of the sworn EPLF to edge their way into Ethiopian positions. In May 1991, under the pressure of the Ethiopian opposition the Derg regime finally fell. EPLF talks with the Ethiopian Transitional Government were successful, thereby allowing Eritrea to hold a referendum on its independence. The overwhelming majority, 99.83% of Eritrean people, voted for their country’s independence, and on May 24, 1993 Eritrea was declared an independent country. A new parliament was established and then elected Isayas Afewerki, prominent head of the EPLF, as president.

The armed conflict between Ethiopia and Eritrea lasted for almost 30 years. It is a remarkable history of human resistance on the one hand, and of terrible loss suffered by two of the world's poorest countries, on the other. The Eritrean people, themselves the product of a compulsory unification carried out by an outside power, fought, for the most part, alone against the statistically superior and better equipped Ethiopian army, while both Ethiopian regimes were ready to do their own as well as the Americans’ and the Soviets’ bidding in the most brutal ways, at least until the fall of Haile Selassie.

What made this conflict even more destructive was that the Eritreans fought each other during the war for a long time. Unified in the desire to gain control over their own country, however, they were able to create a coherent ideological framework within a short period of time. Considering the fact that Eritrea is a land of unequal tribes, three religions, and both urban and nomadic cultures, the successful outcome of the war, the unification of its many diverse people, and the declaration of the country’s independence, were all tremendous achievements. And with the coronation, regarded by some as the ultimate paradox of war and by others simply as a miracle, Eritreans’ capital, Asmara, survived the many years of warfare almost entirely unscathed. Its inhabitants today still reaffirm the unifying aim of the struggle: “Asmara is what we fought for.”

More than 50 years had passed since the last time Italian planners were given free reign to explore their creative drives. Almost 10 years under British occupation and another 10 years in a repressive federation with Ethiopia followed 30 years of bloody fighting, at the end of which the country would be freed of foreign domination. Asmara remained standing, having seen little fighting within its city limits. Afterwards, however, the city was in a state of deterioration and chaos:“Scenes of boys and girls carrying barrels of water on improvised carts, military ramparts on top of apartment buildings, and bricked-up windows with broken glass and barbed wire fortifying residences facing the street are still vivid in the minds of residents of Asmara. The euphoria of victory did not remove the foul smell from clogged sewage, and the many beggar women and children indicated widespread poverty. Many colourless, decaying buildings attested to the neglect and destruction of the years of Ethiopian occupation. The basic infrastructure of Asmara was largely in need of a complete overhaul.”It was a picture with scenes of joy about the country’s newly-acquired sovereignty and, at the same time, scenes of a city’s degeneration. The fact that no intensive fighting had taken place in Asmara, could not prevent the city’s utter breakdown. Moreover, the already ruinous state of the city could not be overcome during the course of the war after the dismantling policy of British and Ethiopian occupation.
The population increased due to a flood of refugees arriving from rural regions, as well as the return of many from the diaspora. Asmara, newly renamed the capital, became the political heart of Eritrea once again. During the years that followed, Eritrea experienced an economic upturn and countless investments that, together with its renewed function as the capital, led to the city’s dramatic growth. While the pattern in many other African cities saw a rise in poor and illegal settlements, Asmara’s development could be relatively controlled. Enormous challenges lay ahead; perhaps above all else, inhabitants of Asmara faced the problem of accessing clean drinking water, and of the lack of an adequate sewage system, as the infrastructure was still mostly in ruins following the long war.

A strong desire for self-driven development after decades of foreign rule gave rise to a debate about the colonial legacy of Asmara’s architecture. It is important to note here that for the first time – and this was 50 years after the end of Italian occupation – that Asmara’s city center was recognized as having a legacy at all. Although it could have been expected that the modern architecture left behind by the Italians would be perceived as a symbol of an imposing foreign culture, it was more often the case that people saw it as a reminder of the turbulent history of the country. Asmara’s citizens fought for the preservation of the old buildings they saw as monuments, instead of trying to erase the history of foreign rule through demolition and reconstruction. Subsequently, the construction of a four-floor office building at the site of Caserma Mussolini across from the catholic cathedral was thwarted through a citizen initiative, and the barracks were spared.

There was awareness that Asmara’s city center should be preserved, but at the same time, it needed to serve the community’s economic, social and political needs. In 1997, the Eritrean government established the Cultural Assets Rehabilitation Project (CARP) under the strategic direction of the World Bank, to coordinate the preservation and rehabilitation of cultural goods, especially Asmara’s architecture. Through CARP, a survey was conducted, and in 2001 an historic city center, an area of ​​approximately 4km2, was identified and placed under protection. The hope was that this unique urban area could be protected by persuasively articulating its legacy, by proposing ways to ensure its preservation, and by paying close attention to maintaining the buildings’ integrity, and thus the area would be added to the list of protected UNESCO World Heritage Sites. In the hope of making the city more accessible to tourists in the future, a map of Asmara was developed, today still the only official map of the city. In addition, the authors of the CARP independently published a book called "Asmara: Africa’s Secret Modernist City", which provides an overview of the development of the modern city and its buildings. The book was published in 2003, and has inspired increased worldwide interest in Asmara and its architecture, which has been satisfied quite often through exhibits meant to expand upon the book’s contents. The project failed, however, and was abandoned by the government early on.

Due to an unexpected political shift, Eritrea is now slipping deeper and deeper into a dictatorial and patriarchal state system, in the center of which is President Isayas Afewerki, surrounded by other autocratic leaders with vested interests in holding onto power. Although the first steps towards democratization were made in the mid-1990s, including economic reforms and the formulation of a constitution, today it seems as if all of this had been done in order to legitimize the actions of the political elite. Bereket Habte Selassie, a former member of the Eritrean Constitutional Committee, has also criticized this abrupt change in the trajectory of Eritrea’s politics: “It seems to me [that] the rule of law has gone to the dogs in Eritrea. There was a very good beginning, a very promising beginning. We all hailed Isaias [sic] Afewerki and his colleagues in creating an enabling environment to lead to democracy and we were waiting for that when he and his group – in my view – hijacked the constitution.” Eritrea still does not have a viable constitution. 

Since the border war of 1998-2000 with Ethiopia, Eritrea has been in an official state of emergency, its citizens effectively living under Martial Law. The political elites’ control of the nation has far-reaching consequences, such as the repudiation of freedom of speech, of assembly, and of the press. The Washington Post published an article saying, “While striving to be an egalitarian, self-reliant utopia, Eritrea has become the most unapologetically repressive country on earth.” The population is faced with a steady political and cultural indoctrination, relying upon the rhetoric of nationalism and devoted to the idea of its superiority over Ethiopia. A state-controlled economy that encompasses agriculture, industry and construction, is heavily dependent on the recruitment of conscripts who serve in national service, often for life.
In addition, Afewerki has introduced a strict policy of self-sufficiency. Under the pretext that Eritrea has historically refused to fall victim to the strategic desires of other nations, which is an idea that is not totally unfounded, Afewerki rejects any kind of help from abroad. In 2007 alone, the President turned down 200 million dollars meant to aide in the country’s relief efforts. This has fatal consequences when it comes to caring for Asmara’s heritage, something one of the poorest nations in the world cannot do alone. As mentioned above, the CARP, with its $5 million budget allotted by the World Bank, was abandoned. Another project with the purpose of renewing Asmara’s city center, this time sponsored by the European Union, together with other EU aid programs were accounted for in 2011. Other programs aimed at preserving Asmara’s city center are not on the table at the moment, and it is difficult to imagine that Afewerki will bypass his tendency toward isolationism and accept the necessary financial and technical assistance from abroad that could revive such programs. It is difficult to say whether Asmara had more hope for improvement under the rule of its oppressive archenemy, Ethiopia, or if anything can be said of hope at all under its current dictator.

In 2017, the historical core of Asmara was finally included on the UNESCO World Heritage List.




</doc>
<doc id="6639133" url="https://en.wikipedia.org/wiki?curid=6639133" title="Economy">
Economy

An economy (from Greek οίκος – "household" and νέμoμαι – "manage") is an area of the production, distribution and trade, as well as consumption of goods and services by different agents. Understood in its broadest sense, 'The economy is defined as a social domain that emphasize the practices, discourses, and material expressions associated with the production, use, and management of resources'. Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two groups or parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. However, monetary transactions only account for a small part of the economic domain. Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology (automation, accelerator of process, reduction of cost functions), innovation (new products, services, processes, expanding markets, diversification of markets, niche markets, increases revenue functions) such as, that which produces intellectual property and changes in industrial relations (most notably child labor being replaced in some parts of the world with universal access to education). A given economy is the result of a set of processes that involves its culture, values, education, technological evolution, history, social organization, political structure and legal systems, as well as its geography, natural resource endowment, and ecology, as main factors. These factors give context, content, and set the conditions and parameters in which an economy functions. In other words, the economic domain is a social domain of human practices and transactions. It does not stand alone.

A market-based economy is one where goods and services are produced and exchanged according to demand and supply between participants (economic agents) by barter or a medium of exchange with a credit or debit value accepted within the network, such as a unit of currency. A command-based economy is one where political agents directly control what is produced and how it is sold and distributed. A green economy is low-carbon, resource efficient and socially inclusive. In a green economy, growth in income and employment is driven by public and private investments that reduce carbon emissions and pollution, enhance energy and resource efficiency, and prevent the loss of biodiversity and ecosystem services. A gig economy is one in which short-term jobs are assigned or chosen via online platforms. New economy is a term referred to the whole emerging ecosystem where new standards and practices were introduced, usually as a result of technological innovations.

Today the range of fields of study examining the economy revolves around the social science of economics, but may include sociology (economic sociology), history (economic history), anthropology (economic anthropology), and geography (economic geography). Practical fields directly related to the human activities involving production, distribution, exchange, and consumption of goods and services as a whole are engineering, management, business administration, applied science, and finance.

All professions, occupations, economic agents or economic activities, contribute to the economy. Consumption, saving, and investment are variable components in the economy that determine macroeconomic equilibrium. There are three main sectors of economic activity: primary, secondary, and tertiary.

Due to the growing importance of the economical sector in modern times, the term "real economy" is used by analysts as well as politicians to denote the part of the economy that is concerned with the actual production of goods and services, as ostensibly contrasted with the "paper economy", or the financial side of the economy, which is concerned with buying and selling on the financial markets. Alternate and long-standing terminology distinguishes measures of an economy expressed in real values (adjusted for inflation), such as real GDP, or in nominal values (unadjusted for inflation).

The English words "economy" and "economics" can be traced back to the Greek word (i.e. "household management"), a composite word derived from ("house;household;home") and νέμω ("manage; distribute;to deal out;dispense") by way of ("household management").

The first recorded sense of the word "economy" is in the phrase "the management of œconomic affairs", found in a work possibly composed in a monastery in 1440. "Economy" is later recorded in more general senses, including "thrift" and "administration".

The most frequently used current sense, denoting "the economic system of a country or an area", seems not to have developed until the 1650s.

As long as someone has been making, supplying and distributing goods or services, there has been some sort of economy; economies grew larger as societies grew and became more complex. Sumer developed a large-scale economy based on commodity money, while the Babylonians and their neighboring city states later developed the earliest system of economics as we think of, in terms of rules/laws on debt, legal contracts and law codes relating to business practices, and private property.

The Babylonians and their city state neighbors developed forms of economics comparable to currently used civil society (law) concepts. They developed the first known codified legal and administrative systems, complete with courts, jails, and government records.

The ancient economy was mainly based on subsistence farming. The Shekel referred to an ancient unit of weight and currency. The first usage of the term came from Mesopotamia circa 3000 BC. and referred to a specific mass of barley which related other values in a metric such as silver, bronze, copper etc. A barley/shekel was originally both a unit of currency and a unit of weight, just as the British Pound was originally a unit denominating a one-pound mass of silver.

For most people, the exchange of goods occurred through social relationships. There were also traders who bartered in the marketplaces. In Ancient Greece, where the present English word 'economy' originated, many people were bond slaves of the freeholders. The economic discussion was driven by scarcity.

In Medieval times, what we now call economy was not far from the subsistence level. Most exchange occurred within social groups. On top of this, the great conquerors raised what we now call venture capital (from "ventura", ital.; "risk") to finance their captures. The capital should be refunded by the goods they would bring up in the New World. The discoveries of Marco Polo (1254–1324), Christopher Columbus (1451–1506) and Vasco da Gama (1469–1524) led to a first global economy. The first enterprises were trading establishments. In 1513, the first stock exchange was founded in Antwerpen. Economy at the time meant primarily trade.

The European captures became branches of the European states, the so-called colonies. The rising nation-states Spain, Portugal, France, Great Britain and the Netherlands tried to control the trade through custom duties and (from "mercator", lat.: merchant) was a first approach to intermediate between private wealth and public interest.
The secularization in Europe allowed states to use the immense property of the church for the development of towns. The influence of the nobles decreased. The first Secretaries of State for economy started their work. Bankers like Amschel Mayer Rothschild (1773–1855) started to finance national projects such as wars and infrastructure. Economy from then on meant national economy as a topic for the economic activities of the citizens of a state.

The first economist in the true modern meaning of the word was the Scotsman Adam Smith (1723–1790) who was inspired partly by the ideas of physiocracy, a reaction to mercantilism and also later Economics student, Adam Mari. He defined the elements of a national economy: products are offered at a natural price generated by the use of competition - supply and demand - and the division of labor. He maintained that the basic motive for free trade is human self-interest. The so-called self-interest hypothesis became the anthropological basis for economics. Thomas Malthus (1766–1834) transferred the idea of supply and demand to the problem of overpopulation.

The Industrial Revolution was a period from the 18th to the 19th century where major changes in agriculture, manufacturing, mining, and transport had a profound effect on the socioeconomic and cultural conditions starting in the United Kingdom, then subsequently spreading throughout Europe, North America, and eventually the world. The onset of the Industrial Revolution marked a major turning point in human history; almost every aspect of daily life was eventually influenced in some way.
In Europe wild capitalism started to replace the system of mercantilism (today: protectionism) and led to economic growth. The period today is called industrial revolution because the system of Production, production and division of labor enabled the mass production of goods.

The contemporary concept of "the economy" wasn't popularly known until the American Great Depression in the 1930s.

After the chaos of two World Wars and the devastating Great Depression, policymakers searched for new ways of controlling the course of the economy. This was explored and discussed by Friedrich August von Hayek (1899–1992) and Milton Friedman (1912–2006) who pleaded for a global free trade and are supposed to be the fathers of the so-called neoliberalism. However, the prevailing view was that held by John Maynard Keynes (1883–1946), who argued for a stronger control of the markets by the state. The theory that the state can alleviate economic problems and instigate economic growth through state manipulation of aggregate demand is called Keynesianism in his honor. In the late 1950s, the economic growth in America and Europe—often called Wirtschaftswunder (ger: "economic miracle") —brought up a new form of economy: mass consumption economy. In 1958, John Kenneth Galbraith (1908–2006) was the first to speak of an affluent society. In most of the countries the economic system is called a social market economy.

With the fall of the Iron Curtain and the transition of the countries of the Eastern Bloc towards democratic government and market economies, the idea of the post-industrial society is brought into importance as its role is to mark together the significance that the service sector receives instead of industrialization. Some attribute the first use of this term to Daniel Bell's 1973 book, "The Coming of Post-Industrial Society", while others attribute it to social philosopher Ivan Illich's book, "Tools for Conviviality". The term is also applied in philosophy to designate the fading of postmodernism in the late 90s and especially in the beginning of the 21st century.

With the spread of Internet as a mass media and communication medium especially after 2000-2001, the idea for the Internet and information economy is given place because of the growing importance of e-commerce and electronic businesses, also the term for a global information society as understanding of a new type of "all-connected" society is created. In the late 2000s, the new type of economies and economic expansions of countries like China, Brazil, and India bring attention and interest to different from the usually dominating Western type economies and economic models.

The economy may be considered as having developed through the following phases or degrees of precedence.

In modern economies, these phase precedences are somewhat differently expressed by the three-sector theory. 

Other sectors of the developed community include :


There are a number of concepts associated with the economy, such as these:

The GDP (gross domestic product) of a country is a measure of the size of its economy. The most conventional economic analysis of a country relies heavily on economic indicators like the GDP and GDP per capita. While often useful, GDP only includes economic activity for which money is exchanged.

An informal economy is economic activity that is neither taxed nor monitored by a government, contrasted with a formal economy. The informal economy is thus not included in that government's gross national product (GNP). Although the informal economy is often associated with developing countries, all economic systems contain an informal economy in some proportion.

Informal economic activity is a dynamic process that includes many aspects of economic and social theory including exchange, regulation, and enforcement. By its nature, it is necessarily difficult to observe, study, define, and measure. No single source readily or authoritatively defines informal economy as a unit of study.

The terms "underground", "under the table" and "off the books" typically refer to this type of economy. The term black market refers to a specific subset of the informal economy. The term "informal sector" was used in many earlier studies, and has been mostly replaced in more recent studies which use the newer term.

The informal sector makes up a significant portion of the economies in developing countries but it is often stigmatized as troublesome and unmanageable. However, the informal sector provides critical economic opportunities for the poor and has been expanding rapidly since the 1960s. As such, integrating the informal economy into the formal sector is an important policy challenge.

Economic research is conducted in fields as different as economics, economic sociology, economic anthropology, and economic history.




</doc>
<doc id="9057549" url="https://en.wikipedia.org/wiki?curid=9057549" title="Cultural mediation">
Cultural mediation

Cultural mediation describes a profession that studies the cultural differences between people, using the data in problem solving.
It is one of the fundamental mechanisms of distinctly human development according to cultural–historical psychological theory introduced by Lev Vygotsky and developed in the work of his numerous followers worldwide.

Vygotsky investigated child development and how this was guided by the role of culture and interpersonal communication. Vygotsky observed how higher mental functions developed through social interactions with significant people in a child's life, particularly parents, but also other adults. Through these interactions, a child came to learn the habits of mind of her/his culture, including speech patterns, written language, and other symbolic knowledge through which the child derives meaning and affects a child's construction of his or her knowledge. This key premise of Vygotskian psychology is often referred to as "cultural mediation". The specific knowledge gained by a child through these interactions also represented the shared knowledge of a culture. This process is known as internalization.

The easiest way to understand mediation is to start with an example and follow with the Vygotskian principles behind it.

At a North American girl's fourth birthday, she sits at the table with friends and family. As the candles on her birthday cake are lit and it is placed on the table, the child gains a feeling of deeply felt joy. This is not only because she knows the cake is sweet and she likes sweet food, nor that the candles' sparkling is pleasing to her eyes. While these would be sufficient reason to arouse an emotional response in an ape, there are mental processes in a four-year-old that extend well beyond this. She patiently waits as her family and friends sing "Happy Birthday to You". The joy is not in the cake itself but in the cake's specific meaning to her. It is a sign that today is a special day for her in which she is the center of attention and that her friends and family are praising her. It's also a sign that she is bigger and as such has higher status among her peers. It's not just a cake, it is a birthday cake and, more specifically, it is her own. The true significance of the birthday cake then, is not in its physical properties at all, but rather in the significance bestowed upon it by the culture the daughter is growing into. This is not restricted to such artifacts as a birthday cake. A classroom, a game of soccer, a fire engine are all first and foremost cultural artifacts from which children derive meaning.

This example can help us understand Vygotsky's approach to human development. Like animals, we have lower mental functions tied closely to biological processes. In our birthday cake example, a toddler may well have reached out to take a handful of cream from the cake as soon as she saw it and the four-year-old may have been tempted to do the same. In humans, however, lower mental functions facilitate a new line of development qualitatively unique to humans. Vygotsky referred to this as the higher mental functions. The lower mental functions cannot be equated to those of an ape as they are interwoven with the line of higher mental functions and are essential to them.
However, it is this higher line of development that explains the birthday cake example with profound insight.

From the perspective of an individual child's development, the higher psychological line of development is one guided by the development of "tools" and "signs" within the culture. In our example above, the birthday cake is much more than a source of nourishment, it is a sign with much deeper and broader meaning. The sign "mediates" between the immediate sensory input and the child's response, and in so doing allows for a moment of reflection and self-regulation that would not otherwise be possible. To the extent that these signs can be used to influence or change our physical or social environment they are tools. Even the birthday cake can be considered as a tool in that the parents use it to establish that their daughter is now older and has a new status in society.

The cake is a sophisticated example. Tools and signs can be much simpler, such as an infant pointing to an object she desires. At first she may simply be trying to reach the object, but the mother's response of passing the object helps the infant realize that the action of pointing is a tool to change the environment according to her needs. It is from these simple inter-subjective beginnings that the world of meaning in the child mediated by tools and signs, including language, develops.

A fundamental premise of Vygotsky's therefore, is that tools and signs are first and foremost shared between individuals in society and only then can they be internalized by individuals developing in the society as is reflected in this famous quote:



</doc>
<doc id="9216811" url="https://en.wikipedia.org/wiki?curid=9216811" title="Cultural retention">
Cultural retention

Cultural retention is the act of retaining the culture of a specific ethnic group of people, especially when there is reason to believe that the culture, through inaction, may be lost. Many African-American, European and Asian organizations have cultural retention programs in place.



</doc>
<doc id="12593785" url="https://en.wikipedia.org/wiki?curid=12593785" title="Culture change">
Culture change

Culture change is a term used in public policy making that emphasizes the influence of cultural capital on individual and community behavior. It has been sometimes called repositioning of culture, which means the reconstruction of the cultural concept of a society. It places stress on the social and cultural capital determinants of decision making and the manner in which these interact with other factors like the availability of information or the financial incentives facing individuals to drive behavior.

These cultural capital influences include the role of parenting, families and close associates; organizations such as schools and workplaces; communities and neighborhoods; and wider social influences such as the media. It is argued that this cultural capital manifests into specific values, attitudes or social norms which in turn guide the behavioral "intentions" that individuals adopt in regard to particular decisions or courses of action. These behavioral intentions interact with other factors driving behavior such as financial incentives, regulation and legislation, or levels of information, to drive actual behavior and ultimately feed back into underlying cultural capital.

In general, cultural stereotypes present great resistance to change and to their own redefinition. Culture, often appears fixed to the observer at any one point in time because cultural mutations occur incrementally. Cultural change is a long-term process. Policymakers need to make a great effort to improve some basics aspects of a society’s cultural traits.

The term is used by Knott et al. of the Prime Minister's Strategy Unit in the publication: "Achieving Culture Change: A Policy Framework" (Knott et al., 2008). The paper sets out how public policy can achieve social and cultural change through 'downstream' interventions including fiscal incentives, legislation, regulation and information provision and also 'upstream' interventions such as parenting, peer and mentoring programs, or development of social and community networks.

The key concepts the paper is based on include:


Knott et al. use examples from a range of policy areas to demonstrate how the culture change framework can be applied to policymaking. for example:





</doc>
<doc id="1525262" url="https://en.wikipedia.org/wiki?curid=1525262" title="Middlebrow">
Middlebrow

The term middlebrow describes easily accessible art, usually literature, and the people who use the arts to acquire culture and "class" (social prestige). First used in the British satire magazine "Punch" in 1925, the term "middlebrow" is the intermediary "brow" descriptor between "highbrow" and "lowbrow", which are terms derived from the pseudo-science of phrenology.

The term middlebrow became a pejorative usage in the modernist cultural criticism, by Dwight Macdonald, Virginia Woolf, and Russell Lynes, which served the cause of the marginalization of the popular culture in favor of high culture. Culturally, the middlebrow is classed as a forced and ineffective attempt at cultural and intellectual achievement, and as characterizing literature that emphasizes emotional and sentimental connections, rather than intellectual quality and literary innovation; although postmodernism more readily perceives the advantages of the middlebrow cultural-position that is aware of high culture, but is able to balance aesthetic claims with the claims of the everyday world.

Virginia Woolf derided the middlebrow in an un-posted letter to the editor of the "New Statesman & Nation", concerning a radio broadcast that attacked the Highbrows. That letter was posthumously published in the essay collection "The Death of the Moth" (1942).

Woolf criticizes middlebrows as petty purveyors of highbrow cultures for their own shallow benefit. Rather than selecting books for their intrinsic cultural value, middlebrow people select and read what they are told is best. Middlebrows are concerned with "how" what they do makes them appear, unlike highbrows, the avant-garde men and women who act according to their indelible commitment to beauty, value, art, form, and integrity. Woolf said that, "We highbrows read what we like and do what we like and praise what we like". Likewise, a lowbrow is devoted to a singular interest, a person "of thoroughbred vitality who rides his body in pursuit of a living at a gallop across life"; and, therefore, the lowbrow are equally worthy of reverence, as they, too, are living for what they intrinsically know as valuable.

Instead of such freedom, the middlebrows are "betwixt and between", which Woolf classifies as "in pursuit of no single object, neither Art itself nor life itself, but both mixed indistinguishably, and rather nastily, with money, fame, power, or prestige." Their value system rewards quick gains through literature already designated as 'Classic' and 'Great', never of their own choosing, because "to buy living art requires living taste." The middlebrow are meretricious—which is much less demanding than authenticity.

"Harper's Magazine" editor Russell Lynes satirized Virginia Woolf's highbrow scorn in the article "Highbrow, Lowbrow, Middlebrow". Quoting her and other highbrow proponents, such as art critic Clement Greenberg, Lynes parodied the highbrow's pompous superiority by noting how the subtle distinctions Woolf found significant among the "brows" were just means of upholding cultural superiority. Specifically, he parodies the highbrow claim that the products a person uses distinguishes his or her level of cultural worth, by satirically identifying the products that would identify a middlebrow person.

Lynes continued distinguishing among "brows", dividing middlebrow into upper-middlebrow and lower-middlebrow. The upper-middlebrow's arts patronage makes highbrow activity possible. Museums, orchestras, operas, and publishing houses are run by upper-middlebrows. The lower middlebrows attempt using the arts for self-enhancement: "hell-bent on improving their minds as well as their fortunes". They also intend to live the simple, easy life outlined in advertisements; "lower middlebrow-ism" was "a world that smells of soap". Caricaturing Woolf, Lynes outlined the perfect world without middlebrows; lowbrows work and highbrows create pure art.

Months later, "Life" magazine asked Lynes to specifically distinguish among the right foods, furniture, clothes, and arts for each of the four 'brows'. That began a national preoccupation, as people tried to identify their proper social class, based upon their favorite things. Although "middlebrow" often has connoted contempt, Lynes lauded the zeal and aspirations of the middlebrows.

J. B. Priestley sought to create a positive cultural space around the concept of middlebrow – one characterised by earnestness, friendliness and ethical concerns. He couched his defense of the middlebrow in terms of radio stations, praising the BBC Home Service for its cosiness and plainness, midway between the Light Programme and the Third Programme: "Between the raucous lowbrows and the lisping highbrows is a fine gap, meant for the middle or broadbrows...our homely fashion".

In a struggle that involved competition for readers as well as for cultural capital, Virginia Woolf responded by renaming the BBC the "Betwixt and Between Company".

Dwight Macdonald's critique of middlebrow culture, "Masscult and Midcult" (1960), associated the modern industrial drive, away from specialization and the folk, with creating a mass-market arts, and, therefore, anonymous consumers of the arts. In the U.S., highbrow culture is associated with specialization for the connoisseurs, while lowbrow culture entails authentic folk products made for specific communities. Mass culture (masscult) copies and manipulates both traditions, with factory-created products, made without innovation or care, expressly for the market, "to please the crowd by any means", thereby creating an American society in which "a pluralistic culture cannot exist", wherein the rule is cultural homogeneity.

In contrast Midcult (middle culture), came about with middlebrow culture, and dangerously copies and adulterates high culture, by way of "a tepid ooze of Midcult", which threatens high culture, with dramaturgy, literature, and architecture, such as "Our Town" (1938), "The Old Man and the Sea" (1952), and American collegiate gothic architecture.

The Middlebrow "pretends to respect the standards of High Culture, while, in fact, it waters them down and vulgarizes them." Macdonald recommended a separation of the brows, so that "the few who care about good writing, painting, music, architecture, philosophy, etc. have their High Culture, and don't fuzz up the distinction with the Midcult."

The Book-of-the-Month Club and Oprah Winfrey's Book Club have been widely characterized as middlebrow, marketed to bring classics and 'highbrow' literature to the middle class. This was particularly highlighted when author Jonathan Franzen, after his book "The Corrections" was selected, remarked in several publications that some of Oprah's book club picks were middlebrow In her seminal account of the Book-of-the-Month Club (as it was from its inception in 1926 to the 1980s before it transformed to a purely commercial operation), "A Feeling for Books", Janice Radway argues that middlebrow culture is not simply a diluted impersonation of highbrow, but instead distinctly defined itself in defiance of avant-garde high culture. The club provided subscribers with literature selected by expert and 'generalist' judges, but held the personal, emotional experience of reading a good book as paramount, while simultaneously maintaining 'high standards' for literary quality. In this way, the club was in opposition to the general criticism of middlebrow culture in that it is forced high culture. Instead, Radway demonstrates that the middlebrow culture allows readers to simultaneously access the emotional and intellectual challenges that good reading provides. Radway also identifies the conflicting gender messages sent by the selections. While the club was marketed extensively to the female reader, including its emphasis on the emotional pleasure of books, the focus on intellectual, academic literature of the middlebrow trapped the reader into the constrictive masculine standards of value, classifying 'great books' as those that fell in line with male, technical classifications of excellence.

"Slate Magazine" suggests that the late 2000s and early 2010s could potentially be considered the "golden age of middlebrow art"—pointing to television shows "Breaking Bad", "Mad Men", "The Sopranos" and "The Wire" and novels "Freedom", "The Marriage Plot" and "A Visit from the Goon Squad". "Slate" also defines the films of Aaron Sorkin as middlebrow. Some argue that "Slate" itself is middlebrow journalism.

In a March 2012 article for "Jewish Ideas Daily", Peodair Leihy described the work of poet and songwriter Leonard Cohen as "a kind of pop—upper-middle-brow to lower-high-brow, to be sure, but pop nonetheless." This aesthetic was further theorized in an essay from November that year for "The American Scholar" that saw William Deresiewicz propose the addition of "upper middle brow," a culture falling between masscult and midcult. He defined it as, "infinitely subtler than Midcult. It is post- rather than pre-ironic, its sentimentality hidden by a veil of cool. It is edgy, clever, knowing, stylish, and formally inventive."

In "The New Yorker", Macy Halford characterizes "Harper's Magazine" and "The New Yorker" itself as "often [being] viewed as prime examples of the middlebrow: both magazines are devoted to the high but also to making it accessible to many; to bringing ideas that might remain trapped in ivory towers and academic books, or in high-art (or film or theatre) scenes, into the pages of a relatively inexpensive periodical that can be bought at bookstores and newsstands across the country (and now on the Internet)." She also notes the internet's effect on the middlebrow debate: "Internet is forcing us to rethink (again) what "middlebrow" means: in an era when the highest is as accessible as the lowest—accessible in the sense that both are only a click away ... —we actually have to think anew about how to walk that middle line." Halford describes Wikipedia: "...Wiki is itself a kind of middlebrow product" and links to this middlebrow entry "because it actually provides a smart summary."



</doc>
<doc id="1396834" url="https://en.wikipedia.org/wiki?curid=1396834" title="High culture">
High culture

High culture encompasses the cultural objects of aesthetic value, which a society collectively esteem as exemplary art. It may also include intellectual works considered to be of supreme philosophical, historical, or literary value, as well as the education which cultivates such aesthetic and intellectual pursuits. In popular usage, the term "high culture" identifies the culture of an upper class (an aristocracy) or of a status class (the intelligentsia); and also identifies a society’s common repository of broad-range knowledge and tradition (e.g. folk culture) that transcends the social-class system of the society. Sociologically, the term "high culture" is contrasted with the term "low culture", the forms of popular culture characteristic of the less-educated social classes, such as the barbarians, the Philistines, and "hoi polloi" (the masses).

In European history, high culture was understood as a cultural concept common to the humanities, until the mid-19th century, when Matthew Arnold introduced the term "high culture" in the book "Culture and Anarchy" (1869). The Preface defines culture as "the disinterested endeavour after man’s perfection" pursued, obtained, and achieved by effort to "know the best that has been said and thought in the world". Such a literary definition of high culture also includes philosophy. Moreover, the philosophy of aesthetics proposed in high culture is a force for moral and political good. Critically, the term "high culture" is contrasted with the terms "popular culture" and "mass culture".

In "Notes Towards the Definition of Culture" (1948), T. S. Eliot said that high culture and popular culture are necessary and complementary parts of the culture of a society. In "The Uses of Literacy" (1957), Richard Hoggart presents the sociologic experience of the working-class man and woman in acquiring the cultural literacy, at university, which facilitates social upward mobility. In the U.S., Harold Bloom and F. R. Leavis pursued the definition of high culture, by way of the Western canon of literature.
Media theorist Steven Johnson writes that, unlike popular culture, "the classics—and soon to be classics—are" in their own right descriptions and explanations of the cultural systems that produced them." He says that "a crucial way in which mass culture differs from high art" is that individual works of mass culture are less interesting than the broader cultural trends which produced them.

The high culture of the West originated in the classical-world traditions of intellectual and aesthetic life in Ancient Greece (from c. 8th century BC – AD 147) and Ancient Rome (753 BC – AD 476). In the classical Greco-Roman tradition, the ideal mode of language was published and preserved in works of elevated style (correct grammar, syntax, and diction). Certain forms of language used by authors in valorized epochs were held up in antiquity and the Renaissance as eternal valid models and normative standards of excellence; e.g. the Attic dialect of ancient Greek spoken and written by the playwrights and philosophers of Periclean Athens (fifth century BC); and the form of classical Latin used in the "Golden Age" of Roman culture (c. 70 B.C. – AD 18) represented by such figures as Cicero and Virgil. This form of education was known to the Greeks as παιδεία, which was translated by the Romans into Latin as "humanitas" since it reflected a form of education aiming at the refinement of human nature, rather than the acquisition of technical or vocational skills. Indeed, the Greco-Roman world tended to see such manual, commercial, and technical labor as subordinate to purely intellectual activities. 

From the idea of the "free" man with sufficient leisure to pursue such intellectual and aesthetic refinement, arose the classical distinction between the "liberal" arts which are intellectual and done for their own sake, as against the "servile" or "mechanical" arts which were associated with manual labor and done to earn a living. This implied an association between high culture and the upper classes whose inherited wealth provided such time for intellectual cultivation. The leisured gentleman not weighed down by the necessity of earning a living, was free to devote himself to activities proper to such a "free man" – those deemed to involve true excellence and nobility as opposed to mere utility.
During the Renaissance, the classical intellectual values of the fully rediscovered Græco–Roman culture were the cultural capital of the upper classes (and the aspiring), and aimed at the complete development of human intellectual, aesthetic, and moral faculties. This ideal associated with humanism (a later term derived from the humanities or "studia humanitatis"), was communicated in Renaissance Italy through institutions such as the Renaissance court schools. Renaissance humanism soon spread through Europe becoming much of the basis of upper class education for centuries. For the socially ambitious man and woman who means to rise in society, "The Book of the Courtier" (1528), by Baldasare Castiglione, instructs the reader to acquire and possess knowledge of the Græco–Roman Classics, being education integral to the social-persona of the aristocrat. A key contribution of the Renaissance was the elevation of painting and sculpture to a status equal to the liberal arts (hence the visual arts lost for elites any lingering negative association with manual artisanship.) The early Renaissance treatises of Leon Battista Alberti were instrumental in this regard.

The evolution of the concept of high culture initially was defined in educational terms largely as critical study and knowledge of the Græco–Roman arts and humanities which furnished much of the foundation for European cultures and societies. However, aristocratic patronage through most of the modern era was also pivotal to the support and creation of new works of high culture across the range of arts, music, and literature. The subsequent prodigious development of the modern European languages and cultures meant that the modern definition of the term "high culture" embraces not only Greek and Latin texts, but a much broader canon of select literary, philosophical, historical, and scientific books in both ancient and modern languages. Of comparable importance are those works of art and music considered to be of the highest excellence and broadest influence (e.g. the Parthenon, the painting and sculpture of Michelangelo, the music of J. S. Bach, etc). Together these texts and art works constitute the exemplary artifacts representing the high culture of the Western world. 
In the Western and some East Asian traditions, art that demonstrates the imagination of the artist is accorded the status of high art. In the West this tradition began in Ancient Greece, was reinforced in the Renaissance, and by Romanticism, which eliminated the hierarchy of genres within the fine arts, which was established in the Renaissance. In China there was a distinction between the literati painting by the scholar-officials and the work produced by common artists, working in largely different styles, or the decorative arts such as Chinese porcelain which were produced by unknown craftsmen working in large factories. In both China and the West the distinction was especially clear in landscape painting, where for centuries imaginary views, produced from the imagination of the artist, were considered superior works.

In socially-stratified Europe and the Americas, a first-hand immersion to the high culture of the West, the Grand Tour of Europe, was a rite of passage that complemented and completed the book education of a gentleman, from the nobility, the aristocracy, and the bourgeoisie, with a worldly perspective of society and civilisation. The post-university tour of the cultural centres of Europe was a social-class benefit of the cultural capital transmitted through the high-status institutions (schools, academies, universities) meant to produce the ideal gentleman of that society. 

The European concept of high culture included cultivation of refined etiquette and manners; the education of taste in the fine arts such as sculpture and painting; an appreciation of classical music and opera in its diverse history and myriad forms; knowledge of the humane letters ("literae humaniores") represented by the best Greek and Latin authors, and more broadly of the liberal arts traditions (e.g. philosophy, history, drama, rhetoric, and poetry) of Western civilisation, as well as a general acquaintance with important concepts in theology, science, and political thought.

Much of high culture consists of the appreciation of what is sometimes called "high art". This term is rather broader than Arnold's definition and besides literature includes music, visual arts (especially painting), and traditional forms of the performing arts (including some cinema). The decorative arts would not generally be considered high art.

The cultural products most often regarded as forming part of high culture are most likely to have been produced during periods of high civilization, for which a large, sophisticated, and wealthy urban-based society provides a coherent and conscious aesthetic framework, and a large-scale milieu of training, and, for the visual arts, sourcing materials and financing work. Such an environment enables artists, as near as possible, to realize their creative potential with as few as possible practical and technical constraints. Although the Western concept of high culture naturally concentrates on the Greco-Roman tradition, and its resumption from the Renaissance onwards, such conditions existed in other places at other times.

Art music (or serious music or erudite music) is an umbrella term used to refer to musical traditions implying advanced structural and theoretical considerations and a written musical tradition. The notion of art music is a frequent and well-defined musicological distinction – musicologist Philip Tagg, for example, refers to art music as one of an "axiomatic triangle consisting of 'folk', 'art' and 'popular' musics". He explains that each of these three is distinguishable from the others according to certain criteria, with high cultural music often performed to an audience whilst folk music would traditionally be more participatory. In this regard, "art music" frequently occurs as a contrasting term to "popular music" and to "traditional" or "folk music".

Art film is the result of filmmaking which is typically a serious, independent film aimed at a niche market rather than a mass market audience. Film critics and film studies scholars typically define an "art film" using a "...canon of films and those formal qualities that mark them as different from mainstream Hollywood films", which includes, among other elements: a social realism style; an emphasis on the authorial expressivity of the director or writer; and a focus on the thoughts and dreams of characters, rather than presenting a clear, goal-driven story. According to the film scholar David Bordwell, "art cinema itself is a film genre, with its own distinct conventions."

The term has always been susceptible to attack for elitism, and, in response, many proponents of the concept devoted great efforts to promoting high culture among a wider public than the highly educated bourgeoisie whose natural territory it was supposed to be. There was a drive, beginning in the 19th century, to open museums and concert halls to give the general public access to high culture. Figures such as John Ruskin and Lord Reith of the BBC in Britain, Leon Trotsky and others in Communist Russia, and many others in America and throughout the western world have worked to widen the appeal of elements of high culture such as classical music, art by old masters and the literary classics.

With the widening of access to university education, the effort spread there, and all aspects of high culture became the objects of academic study, which with the exception of the classics had not often been the case until the late 19th century. University liberal arts courses still play an important role in the promotion of the concept of high culture, though often now avoiding the term itself.

Especially in Europe, governments have been prepared to subsidize high culture through the funding of museums, opera and ballet companies, orchestras, cinema, public broadcasting stations such as BBC Radio 3, ARTE, and in other ways. Organizations such as the Arts Council of Great Britain, and in most European countries, whole ministries administer these programs. This includes the subsidy of new works by composers, writers and artists. There are also many private philanthropic sources of funding, which are especially important in the US, where the federally funded Corporation for Public Broadcasting also funds broadcasting. These may be seen as part of the broader concept of official culture, although often a mass audience is not the intended market.

The relations between high culture and mass culture are concerns of cultural studies, media studies, critical theory, sociology, Postmodernism and Marxist philosophy. In the essay "The Work of Art in the Age of Mechanical Reproduction" (1936), Walter Benjamin explored the relations of value of the arts (high and mass) when subjected to industrial reproduction. The critical theoreticians Theodor W. Adorno and Antonio Gramsci interpreted the high-art and mass-art cultural relations as an instrument of social control, with which the ruling class maintain their cultural hegemony upon society.

For the Orientalist Ernest Renan and for the rationalist philosopher Ernest Gellner, high culture was conceptually integral to the politics and ideology of nationalism, as a requisite part of a healthy national identity. Gellner expanded the conceptual scope of the phrase in "Nations and Nationalism" (1983) stating that high art is "a literate, codified culture, which permits context-free communication" among cultures.

In "Distinction: A Social Critique of the Judgement of Taste" (1979), the sociologist Pierre Bourdieu proposed that æsthetic taste (cultural judgement) is in large part derived from social class. Social class establishes the definitions of high art, e.g. in social etiquette, gastronomy, oenology, military service. In such activities of aesthetic judgement, the ruling-class person uses social codes unknown to middle-class and lower-class persons in the pursuit and practice of activities of taste.



</doc>
<doc id="8950930" url="https://en.wikipedia.org/wiki?curid=8950930" title="Cultural memory">
Cultural memory

Because memory is not just an individual, private experience but is also part of the collective domain, cultural memory has become a topic in both historiography (Pierre Nora, Richard Terdiman) and cultural studies (e.g., Susan Stewart). These emphasize cultural memory’s process (historiography) and its implications and objects (cultural studies), respectively. Two schools of thought have emerged, one articulates that the present shapes our understanding of the past. The other assumes that the past has an influence on our present behavior. It has, however, been pointed out (most notably by Guy Beiner) that these two approaches are not necessarily mutually exclusive.

Crucial in understanding cultural memory as a phenomenon is the distinction between memory and history. Pierre Nora (1931 - ) put forward this distinction, pinpointing a niche in-between history and memory.

Scholars disagree as to when to locate the moment representation "took over". Nora points to the formation of European nation states. For Richard Terdiman, the French revolution is the breaking point: the change of a political system, together with the emergence of industrialization and urbanization, made life more complex than ever before. This not only resulted in an increasing difficulty for people to understand the new society in which they were living, but also, as this break was so radical, people had trouble relating to the past "before" the revolution. In this situation, people no longer had an implicit understanding of their past. In order to understand the past, it had to be represented through history. As people realized that history was only one version of the past, they became more and more concerned with their own cultural heritage (in French called "patrimoine") which helped them shape a collective and national identity. In search for an identity to bind a country or people together, governments have constructed collective memories in the form of commemorations which should bring and keep together minority groups and individuals with conflicting agendas. What becomes clear is that the obsession with memory coincides with the fear of forgetting and the aim for authenticity.

However, more recently questions have arisen whether there ever was a time in which "pure", non-representational memory existed – as Nora in particular put forward. Scholars like Tony Bennett rightly point out that representation is a crucial precondition for human perception in general: pure, organic and objective memories can never be witnessed as such.

It is because of a sometimes too contracted conception of memory as just a temporal phenomenon, that the concept of cultural memory has often been exposed to misunderstanding. Nora pioneered connecting memory to physical, tangible locations, nowadays globally known and incorporated as "lieux de mémoire". He certifies these in his work as "mises en abîme"; entities that symbolize a more complex piece of our history. Although he concentrates on a spatial approach to remembrance, Nora already points out in his early historiographical theories that memory goes beyond just tangible and visual aspects, thereby making it flexible and in flux. This rather problematic notion, also characterized by Terdiman as the "omnipresence" of memory, implies that for instance on a sensory level, a smell or a sound can become of cultural value, due to its commemorative effect.

Either in visualized or abstracted form, one of the largest complications of memorializing our past is the inevitable fact that it is absent. Every memory we try to reproduce becomes – as Terdiman states – a "present past". This impractical desire for recalling what is gone forever brings to surface a feeling of nostalgia, noticeable in many aspects of daily life but most specifically in cultural products.

Recently, interest has developed in the area of 'embodied memory'. According to Paul Connerton the body can also be seen as a container, or carrier of memory, of two different types of social practice; inscribing and incorporating. The former includes all activities which are helpful for storing and retrieving information: photographing, writing, taping, etc. The latter implies skilled performances which are sent by means of physical activity, like a spoken word or a handshake. These performances are accomplished by the individual in an unconscious manner, and one might suggest that this memory carried in gestures and habits, is more authentic than 'indirect' memory via inscribing.

The first conceptions of embodied memory, in which the past is 'situated' in the body of the individual, derive from late nineteenth century thoughts of evolutionists like Jean Baptiste Lamarck and Ernst Haeckel. Lamarck’s law of inheritance of acquired characteristics and Haeckel's theory of ontogeny recapitulating phylogeny, suggested that the individual is a summation of the whole history that had preceded him or her. (However, neither of these concepts is accepted by current science.)

Memory can, for instance, be contained in objects. Souvenirs and photographs inhabit an important place in the cultural memory discourse. Several authors stress the fact that the relationship between memory and objects has changed since the nineteenth century. Stewart, for example, claims that our culture has changed from a culture of production to a culture of consumption. Products, according to Terdiman, have lost 'the memory of their own process' now, in times of mass-production and commodification. At the same time, he claims, the connection between memories and objects has been institutionalized and exploited in the form of trade in souvenirs. These specific objects can refer to either a distant time (an antique) or a distant (exotic) place. Stewart explains how our souvenirs authenticate our experiences and how they are a survival sign of events that exist only through the invention of narrative.

This notion can easily be applied to another practice that has a specific relationship with memory: photography. Catherine Keenan explains how the act of taking a picture can underline the importance of remembering, both individually and collectively. Also she states that pictures cannot only stimulate or help memory, but can rather eclipse the actual memory – when we remember in terms of the photograph – or they can serve as a reminder of our propensity to forget. Others have argued that photographs can be incorporated in memory and therefore supplement it.

Edward Chaney has coined the term 'Cultural Memorials' to describe both generic types, such as obelisks or sphinxes, and specific objects, such as the Obelisk of Domitian, Abu Simbel or 'The Young Memnon', which have meanings attributed to them that evolve over time. Readings of ancient Egyptian artefacts by Herodotus, Pliny, the Collector Earl of Arundel, 18th-century travellers, Napoleon, Shelley, William Bankes, Harriet Martineau, Florence Nightingale or Sigmund and Lucian Freud, reveal a range of interpretations variously concerned with reconstructing the intentions of their makers.

Historian Guy Beiner argued that "studies of cultural memory tend to privilege literary and artistic representations of the past. As such, they often fail to engage with the social dynamics of memory. Monuments, artworks, novels, poems, plays and countless other productions of cultural memory do not in themselves remember. Their function as "aides-mémoire" is subject to popular reception. We need to be reminded that remembrance, like trauma, is formulated in human consciousness and that this is shared through social interaction".

As a contrast to the sometimes generative nature of previously mentioned studies on cultural memory, an alternative 'school' with its origins in gender and postcolonial studies underscored the importance of the individual and particular memories of those unheard in most collective accounts: women, minorities, homosexuals, etc.

Experience, whether it be lived or imagined, relates mutually to culture and memory. It is influenced by both factors, but determines these at the same time. Culture influences experience by offering mediated perceptions that affect it, as Frigga Haug states by opposing conventional theory on femininity to lived memory. In turn, as historians such as Neil Gregor have argued, experience affects culture, since individual experience becomes communicable and therefore collective. A memorial, for example, can represent a shared sense of loss.

The influence of memory is made obvious in the way the past is experienced in present conditions, for – according to Paul Connerton, for instance – it can never be eliminated from human practice. On the other hand, it is perception driven by a longing for authenticity that colors memory, which is made clear by a desire to experience the real (Susan Stewart). Experience, therefore, is substantial to the interpretation of culture as well as memory, and vice versa.

Traumatic transmissions are articulated over time not only through social sites or institutions but also through cultural, political, and familial generations, a key social mechanism of continuity and renewal across human groups, cohorts, and communities. The intergenerational transmission of collective trauma is a well-established phenomenon in the scholarly literature on psychological, familial, sociocultural, and biological modes of transmission. Ordinary processes of remembering and transmission can be understood as cultural practices by which people recognize a lineage, a debt to their past, and through which "they express moral continuity with that past." The intergenerational preservation, transformation, and transmutation of traumatic memory such as of genocide tragic historical legacy can be assimilated, redeemed, and transformed.

Recent research and theorizing in cultural memory has emphasized the importance of considering the content of cultural identities in understanding the study of social relations and predicting cultural attitudes.

The Institute of Germanic & Romance Studies, School of Advanced Study, University of London, has developed its MA degree around the above-mentioned topics.

The MA in Cultural Memory has now been running for 10 years. This unique degree explores the many different ways in which culture is based on the construction, manipulation and transmission of memories, and the role played by memory in collective and individual identity formation.

The degree programme is supplemented by a Cultural Memory Seminar and by the new Centre for the Study of Cultural Memory.

In 2008, the first issue of quarterly journal "Memory Studies" concerning subjects of and relating to cultural memory was published by SAGE.

Jan Assmann in his book "Das kulturelle Gedächtnis", drew further upon Maurice Halbwachs's theory on collective memory. Other scholars like Andreas Huyssen have identified a general interest in memory and mnemonics since the early 1980s, illustrated by phenomena as diverse as memorials and retro-culture. Some might see cultural memory as becoming more democratic, due to liberalization and the rise of new media. Others see cultural memory as remaining concentrated in the hands of corporations and states.

Cultural Memory has long been disregarded in terms of a native people's right to their culture and the memories contained therein. 

Lowry Burgess has also been spearheading the creation of “The 31 Article, Declaration and Resolution for The United Nations Universal Declaration on Human Rights, The Right to Historic Memory”. These revolutionary documents create an ecosystem where The UN and the World Bank can provide protection for cultural historical memory and artifacts from destructive groups by using their existing credit system to provide humanitarian benefit to countries that protect and maintain historic sites and related cultural artifacts.

‘The Right to historic Cultural Memory’ derives from 50 years of the author’s public political experience, institutional administration, study and research, arts practice, world travel, teaching, seminars, public lectures, international conferences, books, publications, and bibliographies.




</doc>
<doc id="26642577" url="https://en.wikipedia.org/wiki?curid=26642577" title="Sophistication">
Sophistication

Sophistication has come to mean a few things, but its original definition was "to denature, or simplify". Today it is common as a measure of refinement—displaying good taste, wisdom and subtlety rather than crudeness, stupidity and vulgarity.
In the perception of social class, sophistication can be linked with concepts such as status, privilege and superiority.

In social terms, the connotations of sophistication depends on whether one is an insider or an outsider of the sophisticated class. Sophistication can be seen as "a form of snobbery," or as "among the most desirable of human qualities."

A study of style conveys an idea of the range of possible elements through which one can demonstrate sophistication in elegance and fashion, covering the art of "[...] the shoemaker, the hairdresser, the cosmetologist, the cookbook writers, the chef, the diamond merchant, the couturieres, and the fashion queens, the inventors of the folding umbrella ... and of champagne."

In Ancient Greece, "sophia" was the special insight of poets and prophets. This then became the wisdom of philosophers such as sophists. But their use of rhetoric to win arguments gave sophistication a derogatory quality. Sophistry was then the art of misleading.

The system of modern Western sophistication has its roots in France, arguably helped along its way by the policies of King Louis XIV (reigned 1643–1715).

The English regarded sophistication as decadent and deceptive until the aristocratic sensibilities and refined elegance of Regency dandies such as Beau Brummell (1778–1840) became fashionable and admired.

Recognised varieties of sophistication include:


In the analysis of humor, Victor Raskin distinguishes "two types of sophistication: limited access, or allusive knowledge, and complex processing".

Methods of acquiring the appearance of personal sophistication include:



On a societal level commentators can associate various forms of sophistication with civilization.

Alecia Watterson



</doc>
<doc id="25907070" url="https://en.wikipedia.org/wiki?curid=25907070" title="Ethnoscience">
Ethnoscience

Ethnoscience has been defined as an attempt "to reconstitute what serves as science for others, their practices of looking after themselves and their bodies, their botanical knowledge, but also their forms of classification, of making connections, etc." (Augé, 1999: 118).

Ethnoscience has not always focused on ideas distinct from those of "cognitive anthropology", "component analysis", or "the New Ethnography"; it is a specialization of indigenous knowledge-systems, such as ethno-botany, ethno-zoology, ethno-medicine, etc. (Atran, 1991: 595). According to Scott Atran, ethnoscience looks at culture with a scientific perspective (1991: 650), although most anthropologists abhor this definition. Ethnoscience helps to understand how people develop with different forms of knowledge and beliefs, and focuses on the ecological and historical contributions people have been given (Atran, 1991: 650). Tim Ingold describes ethnoscience as a cross-discipline (2000: 160). He writes that ethnoscience is based on increased collaboration between social sciences and the humanities (e.g., anthropology, sociology, psychology, and philosophy) with natural sciences such as biology, ecology, or medicine (Ingold, 2000: 406-7). At the same time, ethnoscience is increasingly transdisciplinary in its nature (Ingold, 2000: 407).

Of course, naturally over time, the ways in which data has been collected and studied has changed and the field has evolved, becoming more detailed and specific (Urry, 1972: 45). The ideas, mechanics, and methods of ethnoscience evolved from something else - a combination of several things. This pretext amalgamation of theories, processes, and –isms led to the evolution of today's ethnoscience.

Early on, Franz Boas established cultural relativism as an approach to understanding indigenous scientific practices (Uddin, 2005: 980). Cultural relativism identifies people's differences and shows how they are a result of the social, historical, and geographical conditions (Uddin, 2005: 980). Boas is known for his work in Northern Vancouver, British Columbia, Canada, working with the Kwakwaka'wakw Indians, which is where he established the importance of culture (Uddin, 2005: 980). Lévi-Strauss' structuralism was a strong contributor to the ideas of ethnoscience (Uddin, 2005: 980). It, itself, was the leading idea of providing structure to the research and a guide to organizing and relating different cultures. "Ethnoscience refers to a 'reduction of chaos' achieved by a particular culture, rather than to the 'highest possible and conscious degree' to which such chaos may be reduced;" basically, the ethnoscience of a society creates its culture (Sturtevant, 1964: 100). Much of the influence of anthropology, e.g., geographical determinism, was through the contributions of Jean Bodin (Harris, 1968: 42). In his text, he tried to explain why "northern people were faithful, loyal to the government, cruel, and sexually uninterested, compared to why southern people were malicious, craft, wise, expert in science but ill-adapted to political activity (Harris, 1968: 52)." The Greek historian, Polybius, asserted "we mortals have an irresistible tendency to yield to climatic influences; and to this cause, and no other, may be traced the great distinctions that prevail among us in character, physical formation, complexion, as well as in most of our habits…" (quoted in Harris, 1968: 41).

Another aspect of anthropology prior to ethnoscience is enculturation. Newton and Newton described enculturation as a process whereby the novice, or "outsider", learns what is important to the "insider" (1998). Marvin Harris writes, "One of [enculturation's] most important technical expressions is the doctrine of 'psychic unity,' the belief that in the study of sociocultural differences, hereditary (genetic) differences cancel each other out, leaving 'experience' as the most significant variable" (Harris, 1968: 15). This is one of the many starts of people opening up to the idea that just because people are different, doesn't mean they are wrong in their thinking. Harris describes how religious beliefs hinder and affect the progress of anthropology and ethnography. The moral beliefs and restrictions of religion fought against anthropological ideas, possibly due to (especially at the time) to the newly hyped idea of evolutionism and Darwinism (Harris, 1968).

Bronislaw Malinowski was one of many who contributed heavily to the precursor of ethnoscience. His earlier work brought attention to sociological studies; his earliest publication focused on a family in Australia, using a sociological study perspective (Harris, 1968: 547). After the First World War, anthropological work was at a stand still; nothing had evolved, if not regressed (Urry, 1972: 54). This allowed him to start from scratch, and rebuild his ideas and methods (Harris, 1968: 547).

Later, however, Malinowski branched out to political evolution during World War II. The period after World War II is what led to ethnoscience; anthropologists learned their skills could be applied to problems that were affecting modern societies (Mead, 1973: 1). Malinowski said "… with his tables of kinship terms, genealogies, maps, plans and diagrams, proves an extensive and big organization, shows the contribution of the tribe, of the clan, of the family, and he gives a picture of the natives subjected to a strict code of behavior and good manners, to which in comparison the life at the Court of Versailles or Escurial was free and easy" (1922: 10). After World War II, there was an extreme amount of growth in the anthropological field, not only with research opportunities but academically, as well (Mead, 1973: 2).

The anthropologist Robin Horton, who taught at several Nigerian universities, considered the traditional knowledge of indigenous peoples as incorporated within conceptual world views that bear certain similarities to, and differences from, the modern scientific worldview. Like modern science, traditional thought provides a theoretical structure that "places things in a causal order wider than that provided by common sense" (Horton, 1967, p. 53). In contrast to modern science, he saw traditional thought as having a limited awareness of theoretical alternatives and, consequently, displaying "an absolute acceptance of the established theoretical tenets" (Horton, 1967, pp. 155–6).

There are dozens, if not hundreds, of related methods and processes that preceded ethnoscience. Ethnoscience is just another way to study the human culture and the way people interact in society. Taking a look at the ideas and analyses prior to ethnoscience can help understand why it was developed in the first place. Although, it is not widely used and there is criticism on both ends, ethnoscience allows for a more comprehensive way to collect data and patterns of a people. This is not to say the process is its best or that there will be nothing better. That is the best part: everything evolves, even thought. Just as the ideas did in the past, they can improve over time and regress over time but change is inevitable.

Ethnoscience is a new term and study that came into anthropological theory in the 1960s. Often referred to as "indigenous knowledge", ethnoscience introduces a perspective based on native perceptions. It is based on a complete emic perspective, which excludes all observations, interpretations and or any personal notions belonging to the ethnographer. The taxonomy and classification of indigenous systems, to name a few, used to categorize plants, animals, religion and life is adapted from a linguistic analysis. The concept of "Native Science" is also related to the understanding the role of the environment intertwined with the meaning humans place upon their lives. Understanding the language and the native people's linguistic system is one method to understand a native people's system of knowledge of organization. Not only is there categorization for things pertaining to nature and culture thought language, but more importantly and complex is the relationship between environment and culture. Ethnoscience looks at the intricacies of the connection between culture and its surrounding environment. There are also potential limitations and shortcomings in interpreting these systems of knowledge as a dictation of culture and behavior.

Since an ethnographer is not able to physically enter inside an indigenous person's mind, it is essential to not only create a setting or question-answer format to understand perspective but to analyze semantics and word order of given answer to derive an emic understanding. The main focus on a particular component of the languages is placed on its lexicon. The terms "etic" and "emic" are derived from the linguistic terms of "phonetic" and "phonemic".

As introduced by Gregory Cajete, some limitations the concept of indigenous knowledge, is the potential to bypass non-indigenous knowledge as pertinent and valuable. The labels of "indigenous" are overly accepted by those who seek more support by outsiders to further their cause. There might also be an unequal distribution of knowledge amongst a tribe or peoples. There is also the idea that culture is bound by environment. Some theorists conclude that indigenous people's culture is not operated by mental concentrations but solely by the earth that surrounds them. Some theorists go the extent to state that biological processes are based upon the availability, of lack thereof, environmental resources. The methods for sustainability are founded through the workings of the land. These techniques are exercised from the basis of tradition. The importance of the combination of ecological process, social structures, environmental ethics and spiritual ecology are crucial to the expression of the true connection between the natural world and "ecological consciousness".

The origin of Ethnoscience began between the years 1960 to 1965; deriving from the concept of "ethno- + science". Ethno- a combining form meaning "race", "culture", "people", used in the formation of compound words: ethnography. The two concepts later emerged into "ethno-science". The origin of the word 'science' involves the empiric observation of measurable quantities and the testing of hypotheses to falsify or support them. 
"Ethnoscience refers to the system of knowledge and cognition typical of a given culture...to put it another way a culture itself amounts to the sum of a given society's folk classifications, all of that society's ethnoscience, its particular ways of classifying its material and social universe" (Sturtevant 1964: 99–100). The aim of ethnoscience is to gain a more complete description of cultural knowledge. Ethnoscience has been successfully used on several studies of given cultures relating to their linguistics, folk taxonomy, and how they classify their foods, animals and plants.

Ethnoscience is the examination of the perceptions, knowledge, and classifications of the world as reflected in their use of language, which can help anthropologists understand a given culture. By using an ethnographic approach to studying a culture and learning their lexicon and syntax they are able to gain more knowledge in understanding how a particular culture classifies its material and social universe. In addition, this approach "adopted provides simultaneously a point at which the discipline of linguistics, or at least some of its general attitudes, may sensibly be used in anthropology and as a means of gaining insight not only into the nature of man but also into the nature of culture" (Videbeck and Pia, 1966).

Researchers can use linguistics to study what a given culture considers important in a given situation or unforeseen event, and can rank those potential situations in terms of their likelihood to recur. In addition, "understanding the contingencies is helpful in the task of comprehending folk taxonomies on the one hand, and, on the other, an understanding of the taxonomy is required for a full scale appreciation of criteria considered relevant in a given culture (Videbeck and Pia, 1966).

Ethnoscience can be used to analyze the kinship terminology of a given culture, using their language and according to how they view members of their society. Taxonomies "are models of analysis whose purpose is the description of particular types of hierarchical relationships between members of a given set of elements" (Perchonock and Werner, 1969). For example, in our society we classify family groups by giving members the title of father, mother, sister, daughter, brother, son, grandfather, grandmother, etc.

Ethnoscience deals with how a given culture classifies certain principles in addition to how it is express through their language. By understanding a given culture through how they view the world, anthropologists attempt to eliminate any bias through translation as well as categorized their principles in their own ways. "The new methods, which focus on the discovery and description of folk systems, have come to be known as Ethnoscience. Ethnoscience analysis has thus far concentrated on systems of classification within such cultural and linguistic domains as colors, plants, and medicines" (Perchonock and Werner, 1969). An ethnoscientific approach can be used to better understand a given culture and their knowledge of their culture. Using an ethnographic approach can help anthropologists understand how that given culture views and categorizes their own foods, animal kingdom, medicines, as well as plants.

Ethnoscience can be effectively summed up as a classification system for a particular culture in the same way that a botanist would use a taxonomic system for the classification of plant species. Everything from class levels, food consumption, clothing, and material culture objects would be subjected to a taxonomic classification system. In essence, ethnoscience is a way of classifying cultural systems in a structured order to better understand the culture.
The roots of ethnoscience can be traced back to influential anthropologists such as Franz Boas, Bronislaw Malinowski, and Benjamin Whorf who attempted to understand other cultures from an insider's perspective. Ward Goodenough is accredited for bringing ethnoscience to the stage when he define cultural systems of knowledge by stating:

""A societies culture consists of whatever it is one has to know or believe in order to operate in a manner acceptable to its members. Culture is not a material phenomenon; it does not consist of things, behavior, or emotions. It is rather an organization of these things. It is the form of things that people have in mind, their models for perceiving, relating, and otherwise interpreting them."
" (Goodenough 1957:167)
In order to properly put ethnoscience in context we must first understand the definition of ethnoscience. it is defined as "an attempt at cultural description from a totally emic perspective (a perspective in ethnography that uses the concepts and categories that are relevant and meaningful to the culture that is insider analysis) standpoint, this eliminating all of the ethnographer's own categories" (Morey and Luthans 27). Ethnoscience is also a way of learning and understanding how an individual or group perceive their environment and how they fit in with their environment as reflected in their own words and actions.

Ethnoscience has many techniques when applied to an emic perspective. Ethnosemantics, ethnographic semantics, ethnographic ethnoscience, formal analysis, and componential analysis are the terms that apply to the practice of ethnoscience. Ethnosemantics looks at the meaning of words in order to place them in context of the culture being studied. It allows for taxonomy of a certain part of the culture being looked at so that there is a clear breakdown which in turn leads to a deeper understanding of the subject at hand. Ethnographic semantics are very similar to cognitive anthropology in that its primary focus is the intellectual and rational perspectives of the culture being studied. Ethnographic semantics specifically looks at how language is used throughout the culture. Lastly, ethnographic ethnoscience is related to ethnosemantics such that, it uses a taxonomic system to understand how cultural knowledge is accessible through language. Ethnographic ethnoscience uses similar classification systems for cultural domains like ethnobotany and ethnoanatomy. Again, ethnoscience is a way of understanding a how a culture sees itself through its own language. Understanding the cultural language allows the ethnographer to have a deeper and more intimate understanding of the culture.




</doc>
<doc id="2036118" url="https://en.wikipedia.org/wiki?curid=2036118" title="Cultural lag">
Cultural lag

The term cultural lag refers to the notion that culture takes time to catch up with technological innovations, and the resulting social problems that are caused by this lag. In other words, cultural lag occurs whenever there is an unequal rate of change between different parts of culture causing a gap between material and non-material culture. Subsequently, cultural lag does not only apply to this idea only, but also relates to theory and explanation. It helps by identifying and explaining social problems to predict future problems in society. The term was first coined in William F. Ogburn's 1922 work "Social Change with Respect to Culture and Original Nature".

As explained by James W. Woodward, when the material conditions change, changes are occasioned in the adaptive culture, but these changes in the adaptive culture do not synchronize exactly with the change in the material culture, this delay is the culture lag. If people fail to adjust to the rapid environmental and technological changes it will cause a lag or a gap between the cultures. This resonates with ideas of technological determinism, which means that technology determines the development of its cultural values and social structure. That is, it can presuppose that technology has independent effects on society at large. However it does not necessarily assign causality to technology. Rather cultural lag focuses examination on the period of adjustment to new technologies. According to William F. Ogburn, cultural lag is a common societal phenomenon due to the tendency of material culture to evolve and change rapidly and voluminously while non-material culture tends to resist change and remain fixed for a far longer period of time. This is due to the fact that ideals and values are much harder to change than physical things are. Due to the opposing nature of these two aspects of culture, adaptation of new technology becomes rather difficult. This can cause a disconnect between people and their society or culture.This distinction between material and non-material culture is also a contribution of Ogburn's 1922 work on social change. Ogburn's classic example of cultural lag was the period of adaptation when automobiles became faster and more efficient. It took some time for society to start building infrastructure that would tailor mainly to the new, more efficient, vehicles. This is because people are not comfortable with change and it takes them a little time to adapt. Hence, the term cultural lag.

"Social Change with Respect to Nature and Original Change" is a 1922 work by sociologist William F. Ogburn. This work was crucial in drawing attention to issues with social changes and responses. In this work he first coined the term 'cultural lag' to describe a lag between material and non-material cultures. Ogburn states that there is a gap between traditional cultural values and the technical realities in the world. This work was innovative at the time of its release and brought light to the issues of 'cultural lag' and the possible solutions that could fix these issues. This was not the first time these issues have been looked at, but this is the first time that real solutions were presented. Ogburn's theory was not widely accepted at first due to people having different interpretations of the work. In the book he also details the four factors of technical development, which are: invention, accumulation, diffusion, and adjustment. In the work he suggests that primary engine of change and progress is technology, but that it is tempered by social responses. The book had mixed a mixed response due to the fact that many interpreted his findings in many different ways.

In "Social Change with Respect to Nature and Original Change," renowned sociologist William F. Ogburn coins the term 'cultural lag'. Ogburn states his thesis of cultural lag in this work. He says that the source of most modern social change is material culture. His theory of cultural lag suggests that a period of maladjustment occurs when the non-material culture is struggling to adapt to new material conditions. The rapid changes material culture force other parts of culture to change, but the rate of change in these other parts of culture is much slower. He states that people live in a state of 'maladjustment' because of this. Ogburn makes claims that he played a considerable role in solving the issue of social evolution. He goes on to say that the fours solving factors of social evolution are: invention, exponential accumulation, diffusion, and adjustment. This work was unique and innovative at the time of its publication.

"On Culture and Change" is a work by William F. Ogburn which is a collection of 25 works from the years 1912–1961. It is an examination of social change and culture from the perspective of a sociologist. The 25 topics discussed in the work are separated into four topics: social evolution, social trends, short-run changes, and the subjective in the social sciences. This collection of works examines culture and social change in the world. The findings and information in "On Culture and Change" continues to be influential and useful to this day.

In "Future Shock," Alvin Toffler outlines the shattering stress and disorientation that rapid change people feel when they are subjected to too much change in too short of a time. Toffler says that society is undergoing a transformation from an industrial society to a "super-industrial" society. He states that this accelerating rate of change is causing people to feel disconnected from the culture. Toffler argues that balance is needed between the accelerated rates of change in society and the limited pace of human response. Toffler says that it is not impossible to attempt to slow or even control the rapid change and that it is possible for the future to arrive before society is ready for it. Toffler says that the only way to keep equilibrium would be to create social and new personal regulators. Strategies need to be put in place so that rapid culture change can be shaped and controlled. 

In "Cultural Lag: Conception & Theory," Richard & June Brinkman go into what the theory and concept of cultural lag actually is. They go into detail about the points supporting and the points disputing the concept of cultural lag. They evaluate Ogburn's claims about cultural lag and make them more understandable. The work evaluates the existence of cultural lag and its ability to possibly predict and describe cultural change in society. The work also goes into the relevance of the concept of cultural lag to socioeconomic policies in the world.

Material and non-material culture both are a big part of the theory of cultural lag. The theory states that material culture evolves and changes much quicker than non-material culture. Material culture being physical things, such as technology & infrastructure, and non-material culture being non-physical things, such as religion, ideals, and rules. Non-material culture lags behind material culture because the pace of human response is much slower than the pace of material change. New inventions and physical things that make people's lives easier are developed every single day, things such as religions and ideals are not. This is why there is cultural lag, if there is an invention created that goes against people's ideals it will take some time in order for them to accept the new invention and use it. 

Material culture is a term used by sociologists that refers to all physical objects that humans create that give meaning or define a culture. These are physical things that can be touched, feel, taste, or observe with a sense. The term can include things like houses, churches, machines, furniture, or anything else that a person may have some sentimental for. The term can also include somethings that cannot be seen but can be used. Things like the internet and television are also covered under the material culture definition. Material culture changes rapidly and changes depending where in the world somebody is. The environment may present different challenges in different parts of the world that is why material culture is so different everywhere. For example, houses in the heart of Tokyo are going to be smaller than the houses in Austin, Texas. 

Non-material culture is a term used by sociologists that refers to non-physical things such as ideas, values, beliefs, and rules that shape a culture. There are different belief systems everywhere in the world, different religions, myths, and legends that people may believe in. These non-physical things can be information passed down from past generations or new ideas thought up by somebody in today's world. Non-Material culture tends to lag behind material culture due to the fact that it is easier to create a physical object that people will use than it is to create a system of beliefs or ideals that people will use and follow. Non-material culture tends to be very different wherever in the world someone is. This is because people from different backgrounds and areas in the world were raised on different ideals and beliefs that help shape society and culture. 

Cultural lag creates problems for a society in a multitude of ways. The issue of cultural lag tends to permeate any discussion in which the implementation of some new technology is a topic. For example, the advent of stem cell research has given rise to many new, potentially beneficial medical technologies; however these new technologies have also raised serious ethical questions about the use of stem cells in medicine. In this example, the cultural lag is the fear of people to use a new possibly beneficial medical practices because of ethical issues. This shows that there really is a disconnect between material culture (Stem cell research) and non-material culture (Issues with ethics). Cultural lag is seen as an issue because failure to develop broad social consensus on appropriate applications of modern technology may lead to breakdowns in social solidarity and the rise of social conflict.

Another issue that cultural lag causes is the rise of social conflict. Sometimes, people realize that they are disconnected with what is going on in society and they try to do everything they can to get back into the loop. This may result in a race to eliminate the cultural lag. For example, in the 1980s the arms race was in full effect. This is partly because one country discovered how to efficiently and safely use the widely thought unsafe nuclear power/energy. Once the United States was able to successfully harvest nuclear energy into a weapon many other countries realized that maybe nuclear energy isn't that bad and started to build weapons of mass destruction of their own.

Issues can also arise when an aspect of culture changes so rapidly that society is unable to prepare or adjust to it. This is seen in the example of cars overtaking other modes of transportation in the past. Since the production and ownership of cars increased so rapidly society was unable to keep up with it. Broader roads, traffic rules, and separate lanes for horses did not come until some time after automobiles became a part of the mainstream culture. This caused dangerous situations for pedestrians and the people driving these new automobiles. Sometimes society is not ready for the future and this could cause dangerous situations for certain people or groups of people. 





</doc>
<doc id="12870646" url="https://en.wikipedia.org/wiki?curid=12870646" title="Legal culture">
Legal culture

Legal cultures are described as being temporary outcomes of interactions and occur pursuant to a challenge and response paradigm. Analyses of core legal paradigms shape the characteristics of individual and distinctive legal cultures.
"Comparative legal cultures are examined by a field of scholarship, which is situated at the line bordering comparative law and historical jurisprudence."

Legal cultures can be examined by reference to fundamentally different legal systems. However, such cultures can also be differentiated between systems with a shared history and basis which are now otherwise influenced by factors that encourage cultural change. Students learn about legal culture in order to better understand how the law works in society. This can be seen as the study of Law and Society. These studies are available at schools such as Drake University in Des Moines, Iowa.

Western legal culture is unified in the systematic reliance on legal constructs. Such constructs include corporations, contracts, estates, rights and powers. These concepts are not only nonexistent in primitive or traditional legal systems but they can also be predominately incapable of expression in those language systems which form the basis of such legal cultures.

As a general proposition, the concept of legal culture depends on language and symbols and any attempt to analyze non-western legal systems in terms of categories of modern western law can result in distortion attributable to differences in language. So while legal constructs are unique to classical Roman, modern civil and common law cultures, legal concepts or primitive and archaic law get their meaning from sensed experience based on facts as opposed to theory or abstract. Legal culture therefore in the former group is influenced by academics, learned members of the profession and historically, philosophers. The latter group's culture is harnessed by beliefs, values and religion at a foundational level.

Traditional law in Africa is based on natural justice and lacks abstract concepts. This is characteristic of cultures that have an absence of written language which is necessary to elaborate concepts into theory. The doctrines of traditional African law are based on social considerations whereby parties to disputes seek not declarations of right or wrong but rather they seek restitution of social relationships.

The trier of fact and law adjudicates between closely related people from communities as opposed to strangers in commerce. Judgments stress the importance of living together in generous, loving kindness, mutual helpfulness and reciprocity. Evidence suggests that 'African law demonstrates that all men, because they live in society, have some theory of rules of justice which they believe arise from reason itself; [and Gluckman's evidence] suggests that Africans may well have formulated, in embryonic form at least, a theory of natural justice coming from human kindness itself.'

The Islamic legal system exemplifies law as part of a larger culture where the concepts of knowledge, right and human nature play a central role. A case study by Lawrence Rosen explains the anthropological, procedural and judicial discretion aspects of bringing a case to court in Sefrou, Morocco. The case study makes explicit those fundamentals in Islamic society that shape Islamic legal culture and differentiate this from western legal cultures.

Rigid procedural rules and strict court room decorum or etiquette which is entrenched in western legal cultures clears the way for a more natural process of dispute resolution. In Morocco, close attention is paid to social origins, connections and identity where these concepts influence a qadi's (judge) judicial interrogation and discretion.

While the systems of law found in the western world consist of conceptualisation and implementation that mimic the extrajudicial world only slightly, in the Islamic courts of Morocco, the culture of law being propounded reflects the overall culture of its people. This is attributable to the goals of law in Islamic society, which is not to hold state or religious power as supreme or to develop an exacting body of legal doctrine, but to restore relationships and then facilitate the resolution of disputes independently of rigid precedent.

The traditional focus between common law culture and civil law culture has been highlighted by court room procedure, whereby the former nurtures an adversarial environment and the latter an inquisitorial one. Indeed no system of court procedure can ever be purely adversarial or purely inquisitorial.

In fact France, which subscribes to a civil legal system, historically gave the judge a passive role and left the parties to engage in an accusatorial manner. Nonetheless the common law culture predominately consists of oral arguments where legal representors steer the case in search of justice and reinforcement of rights.

The use of a Jury in the common law as a judge of fact is unique when compared to civil law systems. The Jury are triers of fact in both civil and criminal cases and this reflects a particular culture of law; namely the direct involvement of society in the legal framework. In France a judge's role as trier of law and fact is merely as an administrator without creating binding legal principle. Hence the civil law culture is more rational, orderly, authoritative and paternalistic.

Common law has a culture of judicial inventiveness and even flexibility. Enunciation of principle is not forever paramount but indeed a continuing flow of cases and statutes add to the ebb and flow of the law, whereby 'case law represented the modern man's realisation of his own limitations.' Further differences include where a civilian lawyer speaks in terms of the law of nature while the common lawyer speaks to reason. It follows that the culture of these legal systems has been moulded by perceptions of justice and the means available to attain it.

Legal culture can differ between countries despite their conformity to a similar if not identical legal system. Both the United States and England possess common law systems of law and yet each country embodies a distinctive legal culture. This has been attributable by contrasting both the institutions within the legal system and characteristics of the profession (judges, barristers and solicitors).

According to Posner during 1996 there was about 15 times more American judges than English judges but only about 10 times more American lawyers than English lawyers. Posner suggests that English judges have more prestige than American judges and a related point is that the ratio of judges to lawyers is lower in England than the United States. The consequence of this is that the English common law system, as opposed to the American legal system, displays a legal culture of greater prestige and elitism not only in the judiciary but also those who are candidates for the judiciary.

In England, and other Commonwealth jurisdictions, barristers are apt candidates for judicial nomination. The reasons for this stem from the common law systems which have a culture to encourage, harness and capture high quality intellect and experience within a concentrated portion of non-judicial officers of the legal profession known as barristers (which includes and accounts for their subsequent appointments to higher ranking queens counsel and senior counsel).

Barristers are engaged upon a solicitor's brief instead of direct engagement with the client. This insulation avoids lay persons being taken advantage of by unscrupulous lawyers which is evidently "a big problem in the United States, where incompetent lawyers, and known to be such both by judges and by other lawyers, often wow naïve clients."

The cost of pursuing litigation influences the culture of each legal system in terms of what society perceives as the net benefit gained from the court and the profession. To litigate similar cases in England and the United States would cost approximately the same; however English courts are not as generous as their American counterparts in awarding damages, especially punitive damages. Therefore the net expected benefit of litigation being greater in the United States encourages a legal culture that is more litigious in nature than England.

National character is inherent in the legal institutions of the courts and parliament, their formation and their output in terms of legislation or judgments. For example it has been said that many factors have contributed to the litigiousness of the United States, including: the rights afforded to the people, a written constitution, immigrant origins of its population, racial and ethnic heterogeneity and the wealth and spoils of its population. To this end national character and history influence current legal culture.

The legal culture of People's Republic of China, as well as its social and economic culture, continues to undergo dramatic change since the People's Republic of China's reforms of 1978. Transformation has occurred by legal modernisation whereby a rule of law has been suggested to replace the rule of man. The latter is a characteristic of the traditional rural Chinese society where unwritten rules, personal relationships and trust govern citizens' legal relationships; analogous to gemeinschaft. In the modern society of China, institutional, customary and legal reform (a rule of law that embodies universal rules uniformly enforced by a centralised and bureaucratic state) is necessary to govern legal relations; analogous to gesellschaft.

Direct transplants of western legal systems or culture may not provide an adequate rule of law where the life of ordinary Chinese may be marginalised in favour of legal elite who use legal instruments for self-promotion. Furthermore, implanting western legal norms disregards the local culture and relations; thus potentially destroying significant cultural bonds and relationships in the rural community. The traditional rural Chinese legal culture which is premised on personal and informal relations faces erosion unless legal pluralism is promoted.

A top down approach in analysing the legal culture of China suggests that both under Deng Xiaoping and Jiang Zemin, China is "a country under rule by law, not rule of law." Evidence comes from post Mao-China, where law is seen as necessary for institutionalising and generalising ad hoc policies for economic reform and as maintaining party leadership.

Further problems with the Chinese legal culture include a piecemeal approach to law making with an imbalance between law and policy; denials of private law; neglect towards human rights and individual liberties; and poor enforcement of laws. According to Chen, the consensus in China among scholars is that the lack of democracy and rule of law are interdependent concepts whereby "the rule of law is legitimate only if it is the product of democratic government." This is where one could look at Taiwan (known officially as the Republic of China) which is a unitary semi-presidential constitutional republic. Taiwan is characterized as a representative democracy. Despite its democratic values underpinned by a constitution based on the German civil law it does not receive wide recognition as a state separate from the People's Republic of China.

What is evident with the People's Republic of China experience is that legal culture is susceptible to change in pursuance to socio-economic and political forces. While such a change could be beneficial for portions of the society and international relations, traditional and established cultural methods face extinction.




</doc>
<doc id="32839108" url="https://en.wikipedia.org/wiki?curid=32839108" title="Superficiality">
Superficiality

The discourses in philosophy regarding social relation. What social psychologists call "the principle of superficiality versus depth" has pervaded Western culture since at least the time of Plato.

Socrates sought to convince his debaters to turn from the superficiality of a worldview based on the acceptance of convention to the examined life of philosophy, founded (as Plato at least considered) upon the underlying Ideas. For more than two millennia, there was in the Platonic wake a general valorisation of critical thought over the superficial subjectivity that refused deep analysis. The salon style of the Précieuses might for a time affect superficiality, and play with the possibility of treating serious topics in a light-hearted fashion; but the prevailing western consensus firmly rejected elements such as everyday chatter or the changing vagaries of fashion as superficial distractions from a deeper reality.

By contrast, Nietzsche opened the modernist era with a self-conscious praise of superficiality: "What is required is to stop courageously at the surface, the fold, the skin, to adore appearance, to believe in forms, tones, words, in the whole Olympus of appearance! Those Greeks were superficial – "out of profundity"!".

His (still) preference for superficiality was however over-shadowed for most of the 20th century by modernism's full subscription to the depth/surface model, and to the privileging of the former over the latter. Frederic Jameson has highlighted four main modernist versions of the belief in a "deeper" reality - Marxist, psychoanalytic, existential, and semiotic - in each of which reality is understood to be concealed behind an inauthentic surface or façade. Jameson contrast these models sharply with the lack of depth, the ahistoricity, the surface-focus and flatness of the postmodern consciousness, with its new cult of the image and the simulacrum.

In the last third of the 20th century, Lyotard began challenging the Platonic view of a true meaning hidden behind surface as a "theatrical" world-view, insisting instead that sense manifestations had their own reality which necessarily impacted upon the purely verbal order of intelligibility. Similarly, deconstruction has increasingly sought to undo the depth/surface hierarchy, proposing in ironic style that superficiality is as deep as depth. The result has been the call to abandon the idea that behind appearances there is any ultimate truth to be found; and in consequence the growing postmodern replacement of depth by surface, or by multiple surfaces. 

That process of substitution was well under way by the 1990s, when notoriously "surface was depth", and in the new millennium has led to a state of what has been called hypervisibility: everything is on view. In this new era of exposure we are all submerged in what the psychoanalyst Michael Parsons has called "the totalist world where there is a horror of inwardness; everything must be revealed".

If postmodernism's proponents welcomed the way a new transcendence of the surface /depth dichotomy allowed a fuller appreciation of the possibilities of the superficial - the surface consciousness of the now, as opposed to the depths of historical time - critics like J. G. Ballard object that the end-product is a world of "laws without penalties, events without significance, a sun without shadows": of surface without depth. They see postmodern superficiality as a by-product of the false consciousness of global capitalism, where surface distractions, news, and entertainment supersaturate the zapping mind in such a way as to foreclose the possibility of envisioning any critical alternative.

Almost all depth psychologies defy the postmodern to value depth over surface - to aim, in David Cooper's words, for "change from the depths of oneself upwards into the "superficies" of one's social appearance". Debates may rage over whether to "begin" analysis at the surface or by way of deep interpretations, but this is essentially a question of timing. Thus for example Jungians would highlight at the start of therapy what they call the "persona-restoring" phase as an effort to preserve superficiality, but would later optimally see the client moving from the surface to deeper emotion and creativity.

Fritz Perls by contrast maintained that "the simplicity of the Gestalt approach is that we pay attention to the obvious, to the utmost surface. We don't delve into a region which we don't know anything about, into the so-called 'unconscious. A similar focus on the superficial has fuelled much of the Freud Wars of late modernity, in which, according to Jonathan Lear, "the real object of attack - for which Freud is only a stalking-horse - is the very idea that humans have unconscious motivation". Given a choice of surface or depth—"are we to see humans as having depth, layers of meaning which lie beneath the surface of their own understanding?"—he asks: "Or are we to take ourselves as transparent to ourselves...to ignore the complexity, depth and darkness of human life"; the postmodern bias remains towards superficiality.

Social psychology considers that in everyday life social processing veers between superficiality, where we rely on first impressions and immediate judgements, and a deeper form of processing in which we seek to understand the other person more fully. In the ordinary course of life, we necessarily take others at face-value, and use ideal types/stereotypes to guide our daily activities; while institutions too can rely on the superficial consensus of groupthink to preclude deeper investigation.

Some circumstances however necessitate a shift from superficial to extensive processing. When things become serious, we must put more and deeper thought into understanding, leaving superficial judgements to cases where the stakes are low, not high.




</doc>
<doc id="33301100" url="https://en.wikipedia.org/wiki?curid=33301100" title="Cultural radicalism">
Cultural radicalism

Cultural radicalism (Danish: "Kulturradikalisme") was a movement in first Danish, but later also Norwegian culture. It was particular strong in the Interwar Period, but its philosophy has its origin in the 1870s and a great deal of modern social commentary still refer to it.

At the time of the height of the cultural radical movement it was referred to as modern. The words cultural radical and cultural radicalism was first used in an essay by Elias Bredsdorff in the broadsheet newspaper, "Politiken", in 1956. Bredsdorff described cultural radicals as people who are socially responsible with an international outlook.

Cultural radicalism has usually been described as the heritage of Georg Brandes's Modern Breakthrough, the foundation and early editorials of the newspaper "Politiken", the foundation of the political party "Radikale Venstre", to the magazine "Kritisk Revy" by Poul Henningsen (PH). By opponents of cultural radicalism though, it often simply refers to the liberal intellectual elite.

The values most commonly associated with cultural radicalism are among others: criticism of religion, opposition to social norms, criticism of Victorian sexual morality, anti-militarism and an openness to new cultural input other than the classic western (e.g. jazz, modern architecture, art, literature and theater).

"Cultural radicalism" is also used outside of Denmark. In Scandinavia, it often refers to the Danish movement, but elsewhere, the concept may just share the etymology. In Sweden, cultural radicalism has been seen as opposition to the Swedish church and to the Neo-Victorian sexual moral. In Norway the movement has been associated with the magazine Mot Dag in 1930s and its authors such as Sigurd Hoel and Arnulf Øverland. In the US, "cultural radicalism" is sometimes used as the opposite of cultural conservatism, especially in the context of culture wars.





</doc>
<doc id="22810417" url="https://en.wikipedia.org/wiki?curid=22810417" title="Official culture">
Official culture

Official culture is the culture that receives social legitimation or institutional support in a given society. Official culture is usually identified with bourgeoisie culture. For revolutionary Guy Debord, official culture is a "rigged game", where conservative powers forbid subversive ideas to have direct access to the public discourse, and where such ideas are integrated only after being trivialized and sterilized.

A widespread observation is that a great talent has a free spirit. For instance Pushkin, which some scholar regard as Russia's first great writer, attracted the mad irritation of the Russian officialdom and particularly of the Tsar, since he 




</doc>
<doc id="34454406" url="https://en.wikipedia.org/wiki?curid=34454406" title="Philosophy of culture">
Philosophy of culture

Philosophy of culture is a branch of philosophy that examines the essence and meaning of culture.

The German philosopher Immanuel Kant (1724–1804) has formulated an individualist definition of "enlightenment" similar to the concept of "bildung": "Enlightenment is man's emergence from his self-incurred immaturity." He argued that this immaturity comes not from a lack of understanding, but from a lack of courage to think independently. Against this intellectual cowardice, Kant urged: "Sapere aude", "Dare to be wise!" In reaction to Kant, German scholars such as Johann Gottfried Herder (1744–1803) argued that human creativity, which necessarily takes unpredictable and highly diverse forms, is as important as human rationality. Moreover, Herder proposed a collective form of "bildung": "For Herder, Bildung was the totality of experiences that provide a coherent identity, and sense of common destiny, to a people."

In 1795, the great linguist and philosopher Wilhelm von Humboldt (1767–1835) called for an anthropology that would synthesize Kant's and Herder's interests. During the Romantic era, scholars in Germany, especially those concerned with nationalist movements—such as the nationalist struggle to create a "Germany" out of diverse principalities, and the nationalist struggles by ethnic minorities against the Austro-Hungarian Empire—developed a more inclusive notion of culture as "worldview"("Weltanschauung"). According to this school of thought, each ethnic group has a distinct worldview that is incommensurable with the worldviews of other groups. Although more inclusive than earlier views, this approach to culture still allowed for distinctions between "civilized" and "primitive" or "tribal" cultures.

In 1860, Adolf Bastian (1826–1905) argued for "the psychic unity of mankind". He proposed that a scientific comparison of all human societies would reveal that distinct worldviews consisted of the same basic elements. According to Bastian, all human societies share a set of "elementary ideas" ("Elementargedanken"); different cultures, or different "folk ideas" ("Völkergedanken"), are local modifications of the elementary ideas. This view paved the way for the modern understanding of culture. Franz Boas (1858–1942) was trained in this tradition, and he brought it with him when he left Germany for the United States.
In the 19th century, humanists such as English poet and essayist Matthew Arnold (1822–1888) used the word "culture" to refer to an ideal of individual human refinement, of "the best that has been thought and said in the world." This concept of culture is comparable to the German concept of "bildung": "...culture being a pursuit of our total perfection by means of getting to know, on all the matters which most concern us, the best which has been thought and said in the world."

In practice, "culture" referred to an élite ideal and was associated with such activities as art, classical music, and haute cuisine. As these forms were associated with urban life, "culture" was identified with "civilization" (from lat. "civitas", city). Another facet of the Romantic movement was an interest in folklore, which led to identifying a "culture" among non-elites. This distinction is often characterized as that between high culture, namely that of the ruling social group, and low culture. In other words, the idea of "culture" that developed in Europe during the 18th and early 19th centuries reflected inequalities within European societies.

Matthew Arnold contrasted "culture" with anarchy; other Europeans, following philosophers Thomas Hobbes and Jean-Jacques Rousseau, contrasted "culture" with "the state of nature". According to Hobbes and Rousseau, the Native Americans who were being conquered by Europeans from the 16th centuries on were living in a state of nature; this opposition was expressed through the contrast between "civilized" and "uncivilized." According to this way of thinking, one could classify some countries and nations as more civilized than others and some people as more cultured than others. This contrast led to Herbert Spencer's theory of Social Darwinism and Lewis Henry Morgan's theory of cultural evolution. Just as some critics have argued that the distinction between high and low cultures is really an expression of the conflict between European elites and non-elites, some critics have argued that the distinction between civilized and uncivilized people is really an expression of the conflict between European colonial powers and their colonial subjects.

Other 19th-century critics, following Rousseau have accepted this differentiation between higher and lower culture, but have seen the refinement and sophistication of high culture as corrupting and unnatural developments that obscure and distort people's essential nature. These critics considered folk music (as produced by "the folk", i.e., rural, illiterate, peasants) to honestly express a natural way of life, while classical music seemed superficial and decadent. Equally, this view often portrayed indigenous peoples as "noble savages" living authentic and unblemished lives, uncomplicated and uncorrupted by the highly stratified capitalist systems of the West.

In 1870 the anthropologist Edward Tylor (1832–1917) applied these ideas of higher versus lower culture to propose a theory of the evolution of religion. According to this theory, religion evolves from more polytheistic to more monotheistic forms. In the process, he redefined culture as a diverse set of activities characteristic of all human societies. This view paved the way for the modern understanding of culture.




</doc>
<doc id="13831" url="https://en.wikipedia.org/wiki?curid=13831" title="Human rights">
Human rights

Human rights are moral principles or norms that describe certain standards of human behaviour and are regularly protected as natural and legal rights in municipal and international law. They are commonly understood as inalienable, fundamental rights "to which a person is inherently entitled simply because she or he is a human being" and which are "inherent in all human beings", regardless of their age, ethnic origin, location, language, religion, ethnicity, or any other status. They are applicable everywhere and at every time in the sense of being universal, and they are egalitarian in the sense of being the same for everyone. They are regarded as requiring empathy and the rule of law and imposing an obligation on persons to respect the human rights of others, and it is generally considered that they should not be taken away except as a result of due process based on specific circumstances; for example, human rights may include freedom from unlawful imprisonment, torture, and execution.

The doctrine of human rights has been highly influential within international law and global and regional institutions. Actions by states and non-governmental organisations form a basis of public policy worldwide. The idea of human rights suggests that "if the public discourse of peacetime global society can be said to have a common moral language, it is that of human rights". The strong claims made by the doctrine of human rights continue to provoke considerable scepticism and debates about the content, nature and justifications of human rights to this day. The precise meaning of the term "right" is controversial and is the subject of continued philosophical debate; while there is consensus that human rights encompasses a wide variety of rights such as the right to a fair trial, protection against enslavement, prohibition of genocide, free speech or a right to education, there is disagreement about which of these particular rights should be included within the general framework of human rights; some thinkers suggest that human rights should be a minimum requirement to avoid the worst-case abuses, while others see it as a higher standard.

Many of the basic ideas that animated the human rights movement developed in the aftermath of the Second World War and the events of the Holocaust, culminating in the adoption of the Universal Declaration of Human Rights in Paris by the United Nations General Assembly in 1948. Ancient peoples did not have the same modern-day conception of universal human rights. The true forerunner of human rights discourse was the concept of natural rights which appeared as part of the medieval natural law tradition that became prominent during the European Enlightenment with such philosophers as John Locke, Francis Hutcheson and Jean-Jacques Burlamaqui and which featured prominently in the political discourse of the American Revolution and the French Revolution. From this foundation, the modern human rights arguments emerged over the latter half of the 20th century, possibly as a reaction to slavery, torture, genocide and war crimes, as a realisation of inherent human vulnerability and as being a precondition for the possibility of a just society.

Ancient peoples did not have the same modern-day conception of universal human rights. The true forerunner of human-rights discourse was the concept of natural rights which appeared as part of the medieval natural law tradition that became prominent during the European Enlightenment. From this foundation, the modern human rights arguments emerged over the latter half of the 20th century.
17th-century English philosopher John Locke discussed natural rights in his work, identifying them as being "life, liberty, and estate (property)", and argued that such fundamental rights could not be surrendered in the social contract. In Britain in 1689, the English Bill of Rights and the Scottish Claim of Right each made illegal a range of oppressive governmental actions. Two major revolutions occurred during the 18th century, in the United States (1776) and in France (1789), leading to the United States Declaration of Independence and the French Declaration of the Rights of Man and of the Citizen respectively, both of which articulated certain human rights. Additionally, the Virginia Declaration of Rights of 1776 encoded into law a number of fundamental civil rights and civil freedoms.

Philosophers such as Thomas Paine, John Stuart Mill and Hegel expanded on the theme of universality during the 18th and 19th centuries. In 1831 William Lloyd Garrison wrote in a newspaper called "The Liberator" that he was trying to enlist his readers in "the great cause of human rights" so the term "human rights" probably came into use sometime between Paine's "The Rights of Man" and Garrison's publication. In 1849 a contemporary, Henry David Thoreau, wrote about human rights in his treatise "On the Duty of Civil Disobedience" which was later influential on human rights and civil rights thinkers. United States Supreme Court Justice David Davis, in his 1867 opinion for Ex Parte Milligan, wrote "By the protection of the law, human rights are secured; withdraw that protection and they are at the mercy of wicked rulers or the clamor of an excited people."

Many groups and movements have managed to achieve profound social changes over the course of the 20th century in the name of human rights. In Western Europe and North America, labour unions brought about laws granting workers the right to strike, establishing minimum work conditions and forbidding or regulating child labour. The women's rights movement succeeded in gaining for many women the right to vote. National liberation movements in many countries succeeded in driving out colonial powers. One of the most influential was Mahatma Gandhi's movement to free his native India from British rule. Movements by long-oppressed racial and religious minorities succeeded in many parts of the world, among them the civil rights movement, and more recent diverse identity politics movements, on behalf of women and minorities in the United States.

The foundation of the International Committee of the Red Cross, the 1864 Lieber Code and the first of the Geneva Conventions in 1864 laid the foundations of International humanitarian law, to be further developed following the two World Wars.

The League of Nations was established in 1919 at the negotiations over the Treaty of Versailles following the end of World War I. The League's goals included disarmament, preventing war through collective security, settling disputes between countries through negotiation, diplomacy and improving global welfare. Enshrined in its Charter was a mandate to promote many of the rights which were later included in the Universal Declaration of Human Rights.

The League of Nations had mandates to support many of the former colonies of the Western European colonial powers during their transition from colony to independent state.

Established as an agency of the League of Nations, and now part of United Nations, the International Labour Organization also had a mandate to promote and safeguard certain of the rights later included in the Universal Declaration of Human Rights (UDHR):

On the issue of "universal", the declarations did not apply to domestic discrimination or racism. Henry J. Richardson III has argued:

The Universal Declaration of Human Rights (UDHR) is a non-binding declaration adopted by the United Nations General Assembly in 1948, partly in response to the barbarism of World War II. The UDHR urges member states to promote a number of human, civil, economic and social rights, asserting these rights are part of the "foundation of freedom, justice and peace in the world". The declaration was the first international legal effort to limit the behavior of states and press upon them duties to their citizens following the model of the rights-duty duality.

The UDHR was framed by members of the Human Rights Commission, with Eleanor Roosevelt as Chair, who began to discuss an "International Bill of Rights" in 1947. The members of the Commission did not immediately agree on the form of such a bill of rights, and whether, or how, it should be enforced. The Commission proceeded to frame the UDHR and accompanying treaties, but the UDHR quickly became the priority. Canadian law professor John Humprey and French lawyer Rene Cassin were responsible for much of the cross-national research and the structure of the document respectively, where the articles of the declaration were interpretative of the general principle of the preamble. The document was structured by Cassin to include the basic principles of dignity, liberty, equality and brotherhood in the first two articles, followed successively by rights pertaining to individuals; rights of individuals in relation to each other and to groups; spiritual, public and political rights; and economic, social and cultural rights. The final three articles place, according to Cassin, rights in the context of limits, duties and the social and political order in which they are to be realized. Humphrey and Cassin intended the rights in the UDHR to be legally enforceable through some means, as is reflected in the third clause of the preamble:

Some of the UDHR was researched and written by a committee of international experts on human rights, including representatives from all continents and all major religions, and drawing on consultation with leaders such as Mahatma Gandhi. The inclusion of both civil and political rights and economic, social and cultural rights was predicated on the assumption that basic human rights are indivisible and that the different types of rights listed are inextricably linked. Though this principle was not opposed by any member states at the time of adoption (the declaration was adopted unanimously, with the abstention of the Soviet bloc, Apartheid South Africa and Saudi Arabia), this principle was later subject to significant challenges.

The onset of the Cold War soon after the UDHR was conceived brought to the fore divisions over the inclusion of both economic and social rights and civil and political rights in the declaration. Capitalist states tended to place strong emphasis on civil and political rights (such as freedom of association and expression), and were reluctant to include economic and social rights (such as the right to work and the right to join a union). Socialist states placed much greater importance on economic and social rights and argued strongly for their inclusion.

Because of the divisions over which rights to include, and because some states declined to ratify any treaties including certain specific interpretations of human rights, and despite the Soviet bloc and a number of developing countries arguing strongly for the inclusion of all rights in a so-called "Unity Resolution", the rights enshrined in the UDHR were split into two separate covenants, allowing states to adopt some rights and derogate others. Though this allowed the covenants to be created, it denied the proposed principle that all rights are linked which was central to some interpretations of the UDHR.

Although the UDHR is a non-binding resolution, it is now considered to be a central component of international customary law which may be invoked under appropriate circumstances by state judiciaries and other judiciaries.

In 1966, the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR) were adopted by the United Nations, between them making the rights contained in the UDHR binding on all states. However, they came into force only in 1976, when they were ratified by a sufficient number of countries (despite achieving the ICCPR, a covenant including no economic or social rights, the US only ratified the ICCPR in 1992). The ICESCR commits 155 state parties to work toward the granting of economic, social, and cultural rights (ESCR) to individuals. 
Since then numerous other treaties (pieces of legislation) have been offered at the international level. They are generally known as "human rights instruments". Some of the most significant are:


The United Nations (UN) is the only multilateral governmental agency with universally accepted international jurisdiction for universal human rights legislation. All UN organs have advisory roles to the United Nations Security Council and the United Nations Human Rights Council, and there are numerous committees within the UN with responsibilities for safeguarding different human rights treaties. The most senior body of the UN with regard to human rights is the Office of the High Commissioner for Human Rights. The United Nations has an international mandate to:

The UN Human Rights Council, created in 2005, has a mandate to investigate alleged human rights violations. 47 of the 193 UN member states sit on the Council, elected by simple majority in a secret ballot of the United Nations General Assembly. Members serve a maximum of six years and may have their membership suspended for gross human rights abuses. The Council is based in Geneva, and meets three times a year; with additional meetings to respond to urgent situations.

Independent experts ("rapporteurs") are retained by the Council to investigate alleged human rights abuses and to report to the Council.

The Human Rights Council may request that the Security Council refer cases to the International Criminal Court (ICC) even if the issue being referred is outside the normal jurisdiction of the ICC.

In addition to the political bodies whose mandate flows from the UN charter, the UN has set up a number of "treaty-based" bodies, comprising committees of independent experts who monitor compliance with human rights standards and norms flowing from the core international human rights treaties. They are supported by and are created by the treaty that they monitor, With the exception of the CESCR, which was established under a resolution of the Economic and Social Council to carry out the monitoring functions originally assigned to that body under the Covenant, they are technically autonomous bodies, established by the treaties that they monitor and accountable to the state parties of those treaties – rather than subsidiary to the United Nations, though in practice they are closely intertwined with the United Nations system and are supported by the UN High Commissioner for Human Rights (UNHCHR) and the UN Centre for Human Rights.

Each treaty body receives secretariat support from the Human Rights Council and Treaties Division of Office of the High Commissioner on Human Rights (OHCHR) in Geneva except CEDAW, which is supported by the Division for the Advancement of Women (DAW). CEDAW formerly held all its sessions at United Nations headquarters in New York but now frequently meets at the United Nations Office in Geneva; the other treaty bodies meet in Geneva. The Human Rights Committee usually holds its March session in New York City.

There are many regional agreements and organizations promoting and governing human rights.

The African Union (AU) is a supranational union consisting of fifty-five African states. Established in 2001, the AU's purpose is to help secure Africa's democracy, human rights, and a sustainable economy, especially by bringing an end to intra-African conflict and creating an effective common market.

The African Commission on Human and Peoples' Rights (ACHPR) is a quasi-judicial organ of the African Union tasked with promoting and protecting human rights and collective (peoples') rights throughout the African continent as well as interpreting the African Charter on Human and Peoples' Rights and considering individual complaints of violations of the Charter. The Commission has three broad areas of responsibility:


In pursuit of these goals, the Commission is mandated to "collect documents, undertake studies and researches on African problems in the field of human and peoples, rights, organise seminars, symposia and conferences, disseminate information, encourage national and local institutions concerned with human and peoples' rights and, should the case arise, give its views or make recommendations to governments" (Charter, Art. 45).

With the creation of the African Court on Human and Peoples' Rights (under a protocol to the Charter which was adopted in 1998 and entered into force in January 2004), the Commission will have the additional task of preparing cases for submission to the Court's jurisdiction. In a July 2004 decision, the AU Assembly resolved that the future Court on Human and Peoples' Rights would be integrated with the African Court of Justice.

The Court of Justice of the African Union is intended to be the "principal judicial organ of the Union" (Protocol of the Court of Justice of the African Union, Article 2.2). Although it has not yet been established, it is intended to take over the duties of the African Commission on Human and Peoples' Rights, as well as act as the supreme court of the African Union, interpreting all necessary laws and treaties. The Protocol establishing the African Court on Human and Peoples' Rights entered into force in January 2004 but its merging with the Court of Justice has delayed its establishment. The Protocol establishing the Court of Justice will come into force when ratified by 15 countries.

There are many countries in Africa accused of human rights violations by the international community and NGOs.

The Organization of American States (OAS) is an international organization, headquartered in Washington, D.C., United States. Its members are the thirty-five independent states of the Americas. Over the course of the 1990s, with the end of the Cold War, the return to democracy in Latin America, and the thrust toward globalization, the OAS made major efforts to reinvent itself to fit the new context. Its stated priorities now include the following:


The Inter-American Commission on Human Rights (the IACHR) is an autonomous organ of the Organization of American States, also based in Washington, D.C. Along with the Inter-American Court of Human Rights, based in San José, Costa Rica, it is one of the bodies that comprise the inter-American system for the promotion and protection of human rights. The IACHR is a permanent body which meets in regular and special sessions several times a year to examine allegations of human rights violations in the hemisphere. Its human rights duties stem from three documents:


The Inter-Americal Court of Human Rights was established in 1979 with the purpose of enforcing and interpreting the provisions of the American Convention on Human Rights. Its two main functions are thus adjudicatory and advisory. Under the former, it hears and rules on the specific cases of human rights violations referred to it. Under the latter, it issues opinions on matters of legal interpretation brought to its attention by other OAS bodies or member states.

There are no Asia-wide organisations or conventions to promote or protect human rights. Countries vary widely in their approach to human rights and their record of human rights protection.

The Association of Southeast Asian Nations (ASEAN) is a geo-political and economic organization of 10 countries located in Southeast Asia, which was formed in 1967 by Indonesia, Malaysia, the Philippines, Singapore and Thailand. The organisation now also includes Brunei Darussalam, Vietnam, Laos, Myanmar and Cambodia. In October 2009, the ASEAN Intergovernmental Commission on Human Rights was inaugurated, and subsequently, the ASEAN Human Rights Declaration was adopted unanimously by ASEAN members on 18 November 2012.

The Arab Charter on Human Rights (ACHR) was adopted by the Council of the League of Arab States on 22 May 2004.

The Council of Europe, founded in 1949, is the oldest organisation working for European integration. It is an international organisation with legal personality recognised under public international law and has observer status with the United Nations. The seat of the Council of Europe is in Strasbourg in France. The Council of Europe is responsible for both the European Convention on Human Rights and the European Court of Human Rights. These institutions bind the Council's members to a code of human rights which, though strict, are more lenient than those of the United Nations charter on human rights. The Council also promotes the European Charter for Regional or Minority Languages and the European Social Charter. Membership is open to all European states which seek European integration, accept the principle of the rule of law and are able and willing to guarantee democracy, fundamental human rights and freedoms.

The Council of Europe is an organisation that is not part of the European Union, but the latter is expected to accede to the European Convention and potentially the Council itself. The EU has its own human rights document; the Charter of Fundamental Rights of the European Union.

The European Convention on Human Rights defines and guarantees since 1950 human rights and fundamental freedoms in Europe. All 47 member states of the Council of Europe have signed this Convention and are therefore under the jurisdiction of the European Court of Human Rights in Strasbourg. In order to prevent torture and inhuman or degrading treatment (Article 3 of the Convention), the European Committee for the Prevention of Torture was established.

Several theoretical approaches have been advanced to explain how and why human rights become part of social expectations.

One of the oldest Western philosophies on human rights is that they are a product of a natural law, stemming from different philosophical or religious grounds.

Other theories hold that human rights codify moral behavior which is a human social product developed by a process of biological and social evolution (associated with Hume). Human rights are also described as a sociological pattern of rule setting (as in the sociological theory of law and the work of Weber). These approaches include the notion that individuals in a society accept rules from legitimate authority in exchange for security and economic advantage (as in Rawls) – a social contract.

Natural law theories base human rights on a "natural" moral, religious or even biological order which is independent of transitory human laws or traditions.

Socrates and his philosophic heirs, Plato and Aristotle, posited the existence of natural justice or natural right ("dikaion physikon", "δικαιον φυσικον", Latin "ius naturale"). Of these, Aristotle is often said to be the father of natural law, although evidence for this is due largely to the interpretations of his work of Thomas Aquinas.

The development of this tradition of natural justice into one of natural law is usually attributed to the Stoics.

Some of the early Church fathers sought to incorporate the until then pagan concept of natural law into Christianity. Natural law theories have featured greatly in the philosophies of Thomas Aquinas, Francisco Suárez, Richard Hooker, Thomas Hobbes, Hugo Grotius, Samuel von Pufendorf, and John Locke.

In the Seventeenth Century Thomas Hobbes founded a contractualist theory of legal positivism on what all men could agree upon: what they sought (happiness) was subject to contention, but a broad consensus could form around what they feared (violent death at the hands of another). The natural law was how a rational human being, seeking to survive and prosper, would act. It was discovered by considering humankind's natural rights, whereas previously it could be said that natural rights were discovered by considering the natural law. In Hobbes' opinion, the only way natural law could prevail was for men to submit to the commands of the sovereign. In this lay the foundations of the theory of a social contract between the governed and the governor.

Hugo Grotius based his philosophy of international law on natural law. He wrote that "even the will of an omnipotent being cannot change or abrogate" natural law, which "would maintain its objective validity even if we should assume the impossible, that there is no God or that he does not care for human affairs." ("De iure belli ac pacis", Prolegomeni XI). This is the famous argument "etiamsi daremus" ("non-esse Deum"), that made natural law no longer dependent on theology.

John Locke incorporated natural law into many of his theories and philosophy, especially in "Two Treatises of Government". Locke turned Hobbes' prescription around, saying that if the ruler went against natural law and failed to protect "life, liberty, and property," people could justifiably overthrow the existing state and create a new one.

The Belgian philosopher of law Frank van Dun is one among those who are elaborating a secular conception of natural law in the liberal tradition. There are also emerging and secular forms of natural law theory that define human rights as derivative of the notion of universal human dignity.

The term "human rights" has replaced the term "natural rights" in popularity, because the rights are less and less frequently seen as requiring natural law for their existence.

The philosopher John Finnis argues that human rights are justifiable on the grounds of their instrumental value in creating the necessary conditions for human well-being. Interest theories highlight the duty to respect the rights of other individuals on grounds of self-interest:

The biological theory considers the comparative reproductive advantage of human social behavior based on empathy and altruism in the context of natural selection.

The most common categorization of human rights is to split them into civil and political rights, and economic, social and cultural rights.

Civil and political rights are enshrined in articles 3 to 21 of the Universal Declaration of Human Rights and in the ICCPR. Economic, social and cultural rights are enshrined in articles 22 to 28 of the Universal Declaration of Human Rights and in the ICESCR. The UDHR included both economic, social and cultural rights and civil and political rights because it was based on the principle that the different rights could only successfully exist in combination:

This is held to be true because without civil and political rights the public cannot assert their economic, social and cultural rights. Similarly, without livelihoods and a working society, the public cannot assert or make use of civil or political rights (known as the "full belly thesis")

Although accepted by the signaturies to the UDHR, most of them do not in practice give equal weight to the different types of rights. Western cultures have often given priority to civil and political rights, sometimes at the expense of economic and social rights such as the right to work, to education, health and housing. For example, in the United States there is no universal access to healthcare free at the point of use. That is not to say that Western cultures have overlooked these rights entirely (the welfare states that exist in Western Europe are evidence of this). Similarly the ex Soviet bloc countries and Asian countries have tended to give priority to economic, social and cultural rights, but have often failed to provide civil and political rights.

Another categorization, offered by Karel Vasak, is that there are "three generations of human rights": first-generation civil and political rights (right to life and political participation), second-generation economic, social and cultural rights (right to subsistence) and third-generation solidarity rights (right to peace, right to clean environment). Out of these generations, the third generation is the most debated and lacks both legal and political recognition. This categorisation is at odds with the indivisibility of rights, as it implicitly states that some rights can exist without others. Prioritisation of rights for pragmatic reasons is however a widely accepted necessity. Human rights expert Philip Alston argues:

He, and others, urge caution with prioritisation of rights:

Some human rights are said to be "inalienable rights." The term inalienable rights (or unalienable rights) refers to "a set of human rights that are fundamental, are not awarded by human power, and cannot be surrendered."

The adherence to the principle of indivisibility by the international community was reaffirmed in 1995:

This statement was again endorsed at the 2005 World Summit in New York (paragraph 121).

The UDHR enshrines, by definition, rights that apply to all humans equally, whichever geographical location, state, race or culture they belong to.

Proponents of cultural relativism suggest that human rights are not all universal, and indeed conflict with some cultures and threaten their survival.

Rights which are most often contested with relativistic arguments are the rights of women. For example, Female genital mutilation occurs in different cultures in Africa, Asia and South America. It is not mandated by any religion, but has become a tradition in many cultures. It is considered a violation of women's and girl's rights by much of the international community, and is outlawed in some countries.

Universalism has been described by some as cultural, economic or political imperialism. In particular, the concept of human rights is often claimed to be fundamentally rooted in a politically liberal outlook which, although generally accepted in Europe, Japan or North America, is not necessarily taken as standard elsewhere.

For example, in 1981, the Iranian representative to the United Nations, Said Rajaie-Khorassani, articulated the position of his country regarding the Universal Declaration of Human Rights by saying that the UDHR was "a secular understanding of the Judeo-Christian tradition", which could not be implemented by Muslims without trespassing the Islamic law. The former Prime Ministers of Singapore, Lee Kuan Yew, and of Malaysia, Mahathir bin Mohamad both claimed in the 1990s that "Asian values" were significantly different from western values and included a sense of loyalty and foregoing personal freedoms for the sake of social stability and prosperity, and therefore authoritarian government is more appropriate in Asia than democracy. This view is countered by Mahathir's former deputy:

and also by Singapore's opposition leader Chee Soon Juan who states that it is racist to assert that Asians do not want human rights.

An appeal is often made to the fact that influential human rights thinkers, such as John Locke and John Stuart Mill, have all been Western and indeed that some were involved in the running of Empires themselves.

Relativistic arguments tend to neglect the fact that modern human rights are new to all cultures, dating back no further than the UDHR in 1948. They also don't account for the fact that the UDHR was drafted by people from many different cultures and traditions, including a US Roman Catholic, a Chinese Confucian philosopher, a French Zionist and a representative from the Arab League, amongst others, and drew upon advice from thinkers such as Mahatma Gandhi.

Michael Ignatieff has argued that cultural relativism is almost exclusively an argument used by those who wield power in cultures which commit human rights abuses, and that those whose human rights are compromised are the powerless. This reflects the fact that the difficulty in judging universalism versus relativism lies in who is claiming to represent a particular culture.

Although the argument between universalism and relativism is far from complete, it is an academic discussion in that all international human rights instruments adhere to the principle that human rights are universally applicable. The 2005 World Summit reaffirmed the international community's adherence to this principle:

Companies, NGOs, political parties, informal groups, and individuals are known as "non-State actors". Non-State actors can also commit human rights abuses, but are not subject to human rights law other than International Humanitarian Law, which applies to individuals.

Multi-national companies play an increasingly large role in the world, and are responsible for a large number of human rights abuses. Although the legal and moral environment surrounding the actions of governments is reasonably well developed, that surrounding multi-national companies is both controversial and ill-defined. Multi-national companies' primary responsibility is to their shareholders, not to those affected by their actions. Such companies are often larger than the economies of the states in which they operate, and can wield significant economic and political power. No international treaties exist to specifically cover the behavior of companies with regard to human rights, and national legislation is very variable. Jean Ziegler, Special Rapporteur of the UN Commission on Human Rights on the right to food stated in a report in 2003:

In August 2003 the Human Rights Commission's Sub-Commission on the Promotion and Protection of Human Rights produced draft "Norms on the responsibilities of transnational corporations and other business enterprises with regard to human rights". These were considered by the Human Rights Commission in 2004, but have no binding status on corporations and are not monitored.

Realism and national loyalties have been described as a destructive influence on the human rights movement because they deny people's innately similar human qualities.

With the exception of non-derogable human rights (international conventions class the right to life, the right to be free from slavery, the right to be free from torture and the right to be free from retroactive application of penal laws as non-derogable), the UN recognises that human rights can be limited or even pushed aside during times of national emergency – although

Rights that cannot be derogated for reasons of national security in any circumstances are known as peremptory norms or "jus cogens". Such International law obligations are binding on all states and cannot be modified by treaty.

The human rights enshrined in the UDHR, the Geneva Conventions and the various enforced treaties of the United Nations are enforceable in law. In practice, many rights are very difficult to legally enforce due to the absence of consensus on the application of certain rights, the lack of relevant national legislation or of bodies empowered to take legal action to enforce them.

There exist a number of internationally recognized organisations with worldwide mandate or jurisdiction over certain aspects of human rights:


The ICC and other international courts (see Regional human rights above exist to take action where the national legal system of a state is unable to try the case itself. If national law is able to safeguard human rights and punish those who breach human rights legislation, it has primary jurisdiction by complementarity. Only when all "local remedies" have been exhausted does international law take effect.

In over 110 countries National human rights institutions (NHRIs) have been set up to protect, promote or monitor human rights with jurisdiction in a given country. Although not all NHRIs are compliant with the Paris Principles, the number and effect of these institutions is increasing. The Paris Principles were defined at the first International Workshop on National Institutions for the Promotion and Protection of Human Rights in Paris on 7–9 October 1991, and adopted by United Nations Human Rights Commission Resolution 1992/54 of 1992 and the General Assembly Resolution 48/134 of 1993. The Paris Principles list a number of responsibilities for national institutions.

Universal jurisdiction is a controversial principle in international law whereby states claim criminal jurisdiction over persons whose alleged crimes were committed outside the boundaries of the prosecuting state, regardless of nationality, country of residence, or any other relation with the prosecuting country. The state backs its claim on the grounds that the crime committed is considered a crime against all, which any state is authorized to punish. The concept of universal jurisdiction is therefore closely linked to the idea that certain international norms are erga omnes, or owed to the entire world community, as well as the concept of jus cogens. In 1993 Belgium passed a "law of universal jurisdiction" to give its courts jurisdiction over crimes against humanity in other countries, and in 1998 Augusto Pinochet was arrested in London following an indictment by Spanish judge Baltasar Garzon under the universal jurisdiction principle. The principle is supported by Amnesty International and other human rights organisations as they believe certain crimes pose a threat to the international community as a whole and the community has a moral duty to act, but others, including Henry Kissinger (who has himself been accused of war crimes by several commentators), argue that state sovereignty is paramount, because breaches of rights committed in other countries are outside states' sovereign interest and because states could use the principle for political reasons.

Human rights violations occur when any state or non-state actor breaches any of the terms of the UDHR or other international human rights or humanitarian law. In regard to human rights violations of United Nations laws. Article 39 of the United Nations Charter designates the UN Security Council (or an appointed authority) as the only tribunal that may determine UN human rights violations.

Human rights abuses are monitored by United Nations committees, national institutions and governments and by many independent non-governmental organizations, such as Amnesty International, Human Rights Watch, World Organisation Against Torture, Freedom House, International Freedom of Expression Exchange and Anti-Slavery International. These organisations collect evidence and documentation of human rights abuses and apply pressure to promote human rights

Wars of aggression, war crimes and crimes against humanity, including genocide, are breaches of International humanitarian law.




</doc>
<doc id="1852708" url="https://en.wikipedia.org/wiki?curid=1852708" title="Occidentalism">
Occidentalism

Occidentalism refers to and identifies representations of the Western world (the Occident) in two ways: (i) as dehumanizing stereotypes of the Western world, Europe, the Americas, Australia, New Zealand, South Africa, and Israel; and (ii) as ideological representations of the West, as applied in "Occidentalism: A Theory of Counter-Discourse in Post-Mao China" (1995), by Chen Xiaomei; "Occidentalism: Images of the West" (1995), by James G. Carrier; and "Occidentalism: The West in the Eyes of its Enemies" (2004), Ian Buruma and Avishai Margalit. Occidentalism is often counterpart to the term orientalism as used by Edward Said in his book of that title, which refers to and identifies Western stereotypes of the Eastern world, the Orient.

In China "Traditions Regarding Western Countries" became a regular part of the "Twenty-Four Histories" from the 5th century CE, when commentary about The West concentrated upon on an area that did not extend farther than Syria. The extension of European imperialism in the 18th and 19th centuries established, represented, and defined the existence of an "Eastern world" and of a "Western world". Western stereotypes appear in works of Indian, Chinese and Japanese art of those times. At the same time, Western influence in politics, culture, economics and science came to be constructed through an imaginative geography of West and East.

In "Occidentalism: The West in the Eyes of its Enemies" (2004), Buruma and Margalit said that nationalist and nativist resistance to the West replicates Eastern-world responses against the socio-economic forces of modernization, which originated in Western culture, among utopian radicals and conservative nationalists who viewed capitalism, liberalism, and secularism as forces destructive of their societies and cultures. That the early responses to the West were a genuine encounter between alien cultures, many of the later manifestations of "Occidentalism" betray the influence of Western ideas upon Eastern intellectuals, such as the supremacy of the nation-state, the Romantic rejection of rationality, and the spiritual impoverishment of the citizenry of liberal democracies.

Buruma and Margalit trace that resistance to German Romanticism and to the debates, between the Westernisers and the Slavophiles in 19th-century Russia, and that like arguments appear in the ideologies of Zionism, Maoism, Islamism, and Imperial Japanese nationalism. Nonetheless, Alastair Bonnett rejects the analyses of Buruma and Margalit as Eurocentric, and said that the field of Occidentalism emerged from the interconnection of Eastern and Western intellectual traditions.




</doc>
<doc id="35291011" url="https://en.wikipedia.org/wiki?curid=35291011" title="Semiotics of culture">
Semiotics of culture

Semiotics of culture is a research field within semiotics that attempts to define culture from semiotic perspective and as a type of human symbolic activity, creation of signs and a way of giving meaning to everything around. Therefore, here culture is understood as a system of symbols or meaningful signs. Because the main sign system is the linguistic system, the field is usually referred to as semiotics of culture and language. Under this field of study symbols are analyzed and categorized in certain class within the hierarchal system. With postmodernity, metanarratives are no longer as pervasive and thus categorizing these symbols in this postmodern age is more difficult and rather critical.

The research field was of particular interest for the Tartu–Moscow Semiotic School (USSR). Linguists and semioticians by the Tartu School viewed culture as a hierarchical semiotic system consisting of a set of functions correlated to it, and linguistic codes that are used by social groups to maintain coherence. These codes are viewed as superstructures based on natural language, and here the ability of humans to symbolize is central.

The study received a research ground also in Japan where the idea that culture and nature should not be contrasted and contradicted but rather harmonized was developed.



</doc>
<doc id="36791036" url="https://en.wikipedia.org/wiki?curid=36791036" title="Genre-busting">
Genre-busting

"Genre-busting" is a term used occasionally in reviews of written work, music and visual art and refers to the author or artist's ability to cross over two or more established styles. For instance, in writing, to combine the horror genre with a western or hard-boiled detective story with science fiction. In music the term may refer to a song combining styles or defying classification.

The sound of the term calls to mind other uses of "buster" such as "crime buster", "Gangbusters", "Ghostbusters", "Dambusters", etc.

Creative people don't always feel comfortable working within an established category. So genre-busting within the publishing world has become a type of literary fiction. The publisher Atticus Books has gone so far as to declare, on their website: "We specialize in genre-busting literary fiction—i.e., titles that fall between the cracks of genre fiction and compelling narratives that feature memorable main characters."

The "Video Movie Guide 1998" stated in its foreword, "In past years, reviews in VMG have been broken down into genre categories. Now, by popular demand, we are listing all movies together in alphabetical order... So many movies today mix genres... and there are no clear-cut categories anymore."

Interviewed in "Mustard" comedy magazine in 2005, writer Alan Moore said: "I mean, this is probably a bad thing to say to someone from a comedy magazine, but I don't like genre. I think that genre was made up by some spotty clerk in WH Smiths in the 1920s to make his worthless fucking job a little easier for him: "it'd be easier if these books said what they were about on the spine."" going on to say: "In the novel I'm writing, Jerusalem, there's an awful lot of funny stuff, and there's supernatural stuff; there's stuff in the prologue that's as good as Stephen King and it's just a description of my brother walking through a block of flats. It's horror. And there's social history, there's political stuff. Why not mix it all together? Because that's what life is actually like. We laugh, we cry, you know, we buy the t-shirt." 



</doc>
<doc id="33596709" url="https://en.wikipedia.org/wiki?curid=33596709" title="Individualistic culture">
Individualistic culture

Individualistic culture is a society which is characterized by individualism, which is the prioritization or emphasis of the individual over the entire group. Individualistic cultures are oriented around the self, being independent instead of identifying with a group mentality. They see each other as only loosely linked, and value personal goals over group interests. Individualistic cultures tend to have a more diverse population and are characterized with emphasis on personal achievements, and a rational assessment of both the beneficial and detrimental aspects of relationships with others. Individualistic cultures have such unique aspects of communication as being a low power-distance culture and having a low-context communication style. The United States, Australia, United Kingdom, Canada, the Netherlands, New Zealand, Ireland, Germany and South Africa have been identified as highly individualistic cultures.

Power distance is defined to be the degree to which unequal distribution of power is accepted in a culture. Low power distance cultures challenge authority, encourage a reduction of power differences between management and employees, and encourage the use of power legitimately. Low power distance is more likely to occur in an individualistic culture, because in a collectivist culture, people protect the well being of the group and established order so they would be less likely to challenge authority or people in power. Even though individualistic cultures are more likely to be low power distance, these cultures don't expect to completely eliminate power difference. People within this low power distance culture, however, are more likely to respond to such imbalances in power with more negative emotional responses than in the alternative, high power distance cultures. Low power distance cultures include Austria, Israel, Denmark, New Zealand, the Republic of Ireland, and Sweden. The U.S. ranks 38th on the scale.

Individualistic cultures are also more likely to have a low-context communication style. This means that communication is precise, direct, and specific. Unlike in high-context communication, reading between the lines is not necessary in low-context communication. This explicit communication is used in order to prevent any form of misunderstanding between cultures. The ability to articulate the thoughts and opinions one holds as well as to express them eloquently are encouraged, as is persuasive speaking. Low-context communication is all content and no relationship dimension.

Individualistic cultures tend to prioritize the individual person over the group, and this can be seen in how the display rules vary from a collectivist culture compared to an individualistic culture. Display rules are the rules that exist in different cultures that determine how emotion should be displayed publicly. In an individualistic culture, self-expression is highly valued, making the display rules less strict and allowing people to display intense emotion such as: happiness, anger, love, etc. While in a collectivist culture, moderation and self-control is highly valued for the well being of the group, and collectivist cultures therefore tend to restrain from showing emotion in public.

Conflict strategies are methods used to resolve different problems. There are different approaches to resolving conflict, and depending the culture a person is brought up in, the more likely it is for them to use a certain approach. Since individualistic culture sets greater value to personal achievement, contrary to collectivist cultures who value harmony, it is more likely for a person from an individualistic culture to use competition as their method of resolving conflict. When using competition as an approach to resolving conflict, a person is more confrontational and seeks to achieve his or her own goals with no regard of the goals of others. Using this approach a person seeks domination, which means to get others to do what the person wants instead of what they initially wanted. On the contrary, a collectivist culture would more likely use a less confrontational approach such as accommodation to end the conflict with compromise so that each party is benefited.



</doc>
<doc id="30049818" url="https://en.wikipedia.org/wiki?curid=30049818" title="Welfare culture">
Welfare culture

Welfare culture refers to the behavioral consequences of providing poverty relief (i.e., welfare) to low-income individuals. Welfare is considered a type of social protection, which may come in the form of remittances, such as 'welfare checks', or subsidized services, such as free/reduced healthcare, affordable housing, and more. Pierson (2006) has acknowledged that, like poverty, welfare creates behavioral ramifications, and that studies differ regarding whether welfare empowers individuals or breeds dependence on government aid. Pierson also acknowledges that the evidence of the behavioral effects of welfare varies across countries (such as Norway, France, Denmark, and Germany), because different countries implement different systems of welfare.

In the United States, the debate over the impact of welfare traces back as far as the New Deal, but it later became a more mainstream political controversy with the birth of modern welfare under President Lyndon B. Johnson's Great Society. The term "welfare culture," however, was not coined until 1986, by Lawrence Mead.

Welfare may be used to refer to any government-based aid used to promote the well-being of its citizens. In recent decades, however, welfare has been restricted to refer to the Temporary Assistance to Needy Families program (TANF), which provides monthly stipends for indigent families that meet a specific array of criteria.

The term "welfare culture" uses the more broad interpretation of welfare, all government social programs. However, scholars like David Ellwood and Lawrence Summers (1985) believe that the debate over welfare culture could be more accurate if each specific welfare program were examined individually. Specific programs include Medicare, Medicaid, unemployment benefits, and disability benefits.

Kent R. Weaver argues that most scholars cite the Social Security Act of 1935 as the origin of the American welfare state. That reform enacted a wide expanse of services for the poor and financially stressed, including unemployment benefits, Aid to Families with Dependent Children (later replaced in by the Temporary Assistance to Needy Families program under the Clinton administration), retirement income stipends, subsidized housing, and many others.

Scholars such as June Axinn and Mark J. Stern (2007) estimate that the Social Security Act of 1935 and the newly institutionalized programs accompanying the New Deal increased the capacity to find employment, avoid starvation, and secure some form of affordable housing. Furthermore, economist Robert Cohen (1973) estimated that the New Deal sparked a reduction in unemployment from 20% to 15% by the end of the 1940s.
Stanley Feldman and John Zaller (1992) cite a number of economists and political historians who opposed government-based aid, because such critics credit the economic stimulus during World War II as the true solution to the unemployment and poverty of the Great Depression. During the war, American industries began to produce military weapons, food, and other material needs for the troops. The new economic incentive, in addition to a net export and an influx in gold, reduced interest rates, increased investments, and sparked job growth. Christine Romber (1992) and various other economic historians began to criticize the New Deal as the cause for unnecessary and unjustified reliance on government programs.

However, Jerold Rusk (2008), a political scientist, recognizes a consensus among economic, history, and political scholars, which acknowledges that the effects of the New Deal are difficult to separate from the effects of World War II, which prevents any legitimate conclusion from being drawn on the debate.

In the early 1960s, President Johnson began his War on Poverty by introducing many new elements to welfare, including Medicare, Medicaid, increases in subsidized public housing, and more. David Frum (2002) believed such increases in government programs were counterproductive and found positive correlations between government aid and those who could not stay above the poverty line without such aid. Frum concluded that welfare bred dependence on the government.
During the Johnson administration, a sociologist, Senator Daniel Patrick Moynihan, published a study on the impacts of welfare on behavior during the 1960s. His report, "" (1965), is commonly referred to as the "Moynihan Report."

The Moynihan Report advocates for increased welfare for poor black families but that welfare does not empower the destitute to find solutions to their financial troubles. Moynihan stated, "The breakdown of the negro family has led to a startling increase in welfare dependency." Welfare, although helpful, was a reactive measure failing to address the true roots of poverty. Moynihan concluded that more proactive means to empower black families include the promotion of vocational training and a value in education.

Johnson's precedent for increasing welfare benefits hit its pinnacle in the late 1970s under President Jimmy Carter when Temporary Assistance to Needy Family (TANF) recipients were receiving $238 a month, adjusted for inflation. According to the Census Bureau, a strong correlation with poverty reduction is noted, suggesting a link between welfare and empowerment. Poverty dropped from 23% of the population to 12% during the Johnson years. Poverty did not see an increase again until 1982 with 15% of Americans facing poverty, two years after welfare programs experienced serious cuts under President Ronald Reagan.

However, the findings are not without their criticisms. According to the US Census Bureau, poverty had already begun to decrease before Johnson passed the Equal Opportunity Act. Additionally, unemployment reached some of its lowest rates in history under President Dwight Eisenhower near the end of the 1950s. Before Eisenhower left office, unemployment was estimated to be less than 5%.

In 1986, Lawrence Mead introduced a series of studies on welfare culture. Mead compared changes in income levels and welfare benefits across urban dwellers from the 1960s through the 1980s. Mead's studies suggest that over half of all welfare recipients will not need to stay on welfare for more than 10 years, but only 12% will be off welfare in less than 3 years. Mead concludes that welfare has demonstrated some proven effects for helping impoverished families meet their basic needs and find employment, thus acting as a tool for empowerment. However, Mead acknowledges that the welfare system can do better. Mead believes welfare culture could breed empowerment more effectively if mandatory participation in education/job training programs were required for welfare recipients.

Anthropologist Oscar Lewis studied the behavioral effects of poverty on indigent Mexicans. He introduced the concept of the "culture of poverty" and 70 personality traits that he saw in the mentality of the impoverished, including helplessness, disdain for the government, lack of confidence, hopelessness, and a sense of futility that accompanies the search for employment.




</doc>
<doc id="38566488" url="https://en.wikipedia.org/wiki?curid=38566488" title="Trademark look">
Trademark look

Trademark look or signature look is the characteristic clothes or other distinguishing signs used by a certain character or performer, making the person more recognizable by the audience. Politicians may also have trademark signs, such as the suit of American President Barack Obama or the Merkel-Raute hand gesture of German Chancellor Angela Merkel. It can also refer to the clothes of a certain subculture.

Some trademark signatures may have started as in-jokes, but have then come to have been recognised by a wider audience.

Sometimes, when a celebrity stops using a trademark look, people might even find it hard to recognise them.

The term trademark look (or anything similar) is not used in trademark law and a trademark look is not necessarily trademark protected in itself.



</doc>
<doc id="2490371" url="https://en.wikipedia.org/wiki?curid=2490371" title="Low culture">
Low culture

"Low culture" is a derogatory term for forms of popular culture that have mass appeal. Its contrast is "high culture", which can also be derogatory. It has been said by culture theorists that both high culture and low culture are subcultures.

In his book "Popular Culture and High Culture", Herbert J. Gans gives a definition of how to identify and create low culture:
Herbert Gans states in his book "Popular Culture and High Culture" that the different classes of culture are linked correspondingly to socio-economic and educational classes. For any given socio-economic class, there is a culture for that class. Hence the terms high and low culture and the manifestation of those terms as they appeal to their respective constituents.

All cultural products (especially high culture) have a certain demographic to which they appeal most. Low culture appeals to very simple and basic human needs plus offers a perceived return to innocence, the escape from real world problems, or the experience of living vicariously through viewing someone else’s life on television.

Low culture can be formulaic, employing trope conventions, stock characters and character archetypes in a manner that can be perceived as more simplistic, crude, emotive, unbalanced, or blunt compared to high culture's implementations—which may be perceived as more subtle, balanced, or refined and open for interpretations.


</doc>
<doc id="143364" url="https://en.wikipedia.org/wiki?curid=143364" title="Culture hero">
Culture hero

A culture hero is a mythological hero specific to some group (cultural, ethnic, religious, etc.) who changes the world through invention or discovery. Although many culture heroes help with the creation of the world, most culture heroes are important because of their effect on the world after creation. A typical culture hero might be credited as the discoverer of fire, agriculture, songs, tradition, law or religion, and is usually the most important legendary figure of a people, sometimes as the founder of its ruling dynasty.

In many Native American mythologies and beliefs, the coyote spirit stole fire from the gods (or stars or sun) and is more of a trickster than a culture hero. Natives from the Southeastern United States typically saw a rabbit trickster/culture hero, and Pacific Northwest native stories often feature a raven in this role: in some stories, Raven steals fire from his uncle Beaver and eventually gives it to humans. The Western African trickster spider Ananse is also common. In Norse mythology, Odin steals the mead of poetry from Jotunheim and is credited as the discoverer of the runes.

The term "culture hero" was first brought about by historian Kurt Breysig; however, he used the German word "heilbringer," which translates to 'savior'. Over the years, "culture hero" has been interpreted in many ways. Older interpretations by Breysig, Paul Ehrenreich, and Wilhelm Schmidt thought that the journeys of culture heroes were ways in which humans could attempt to understand things in nature, such as the rising and setting of the sun, or the movement of the stars and constellations. Their interpretations eventually got rejected and replaced with newer interpretations by scholars such as Hermann Baumann, Adolf E. Jensen, Mircea Eliade, Otto Zerries, Raffaele Pettazzoni, and Harry Tegnaeus which evolved as a result of having more access to ethnological data, creating the present day and famously known version of the culture hero.

A culture hero is able to perform unbelievable tasks in life because he is different from the normal human. It is often believed that the culture hero is not from this world. All of a culture hero's power originates from their birth. Culture heroes are rarely born regularly. When their mothers get pregnant, it is not because of a man but instead is the result of the wind, or a drop of water. Once a culture hero is born, they are either very powerful babies or even come out of the womb as already full grown. The main point this makes is that the culture hero is not from this world.

A culture hero generally goes on an adventure (often called the hero's journey) that in turn does one of the following:


Culture heroes often time have more than one form, such as having the ability to transform from human to some form of animal, such as a fish or bird. A culture hero has many good qualities about him/her but also has bad ones as well which is why they must go on their journeys. In some journeys the hero is known as a trickster. They act in their own selfish way and the benefits from what they have done ends up being shared with the humans as a side effect.

Once the Culture hero has finished their task, they usually end up disappearing. In many stories, the hero is transformed back to their origin. Other times the place they die will be marked with a stone, tree, or body of water. The end of a Culture Hero's life will lead to the creation of something else, such as a river, constellations, food, animals, and the moon and sun. Culture heroes are the etiological explanation for many humans about the things occurring in their daily lives.



</doc>
<doc id="3039067" url="https://en.wikipedia.org/wiki?curid=3039067" title="High-context and low-context cultures">
High-context and low-context cultures

In anthropology, high-context culture and low-context culture is a measure of how explicit the messages exchanged in a culture are, and how important the context is in communication. High and low context cultures fall on a continuum that describes how a person communicates with others through their range of communication abilities: utilizing gestures, relations, body language, verbal messages, or non-verbal messages. These concepts were first introduced by the anthropologist Edward T. Hall in his 1976 book "Beyond Culture". Cultures and communication in which the context of the message is of great importance to structuring actions are referred to as high context. High context defines cultures that are usually relational and collectivist, and which most highlight interpersonal relationships. Hall identifies high-context cultures as those in which harmony and the well-being of the group is preferred over individual achievement. In low context, communication members' communication must be more explicit, direct, and elaborate because individuals are not expected to have knowledge of each other's histories or background, and communication is not necessarily shaped by long-standing relationships between speakers. Because low-context communication concerns more direct messages, the meaning of these messages is more dependent on the words being spoken rather than on the interpretation of more subtle or unspoken cues.

High-context cultures often stem from less direct verbal and nonverbal communication, utilizing small communication gestures and reading into these less direct messages with more meaning. Low-context cultures are the opposite; direct verbal communication is needed to properly understand a message being said and doing so relies heavily on explicit verbal skills.

"High" and "low" context cultures typically refer to language groups, nationalities, or regional communities. However, they have also been applied to corporations, professions and other cultural groups, as well as settings such as online and offline communication. 

The model of high-context and low-context cultures is a popular framework in intercultural communication studies, but has been criticized as lacking empirical validation. A 2008 meta-analysis concluded that the model was "unsubstantiated and underdeveloped".

Cultural contexts are not absolutely "high" or "low". Instead, a comparison between cultures may find communication differences to a greater or lesser degree. Typically a high-context culture will be relational, collectivist, intuitive, and contemplative. They place a high value on interpersonal relationships and group members are a very close-knit community. Typically a low-context culture will be less close-knit, and so individuals communicating will have fewer relational cues when interpreting messages. Therefore, it is necessary for more explicit information to be included in the message so it is not misinterpreted. Not all individuals in a culture can be defined by cultural stereotypes, and there will be variations within a national culture in different settings. For example, Hall describes how Japanese culture has both low- and high-context situations. However, understanding the broad tendencies of predominant cultures can help inform and educate individuals on how to better facilitate communication between individuals of differing cultural backgrounds. 

Although the concept of high- and low-context cultures is usually applied in the field of analyzing national cultures, it can also be used to describe scientific or corporate cultures, or specific settings such as airports or law courts. A simplified example mentioned by Hall is that scientists working in "hard science" fields (like chemistry and physics) tend to have lower-context cultures: because their knowledge and models have fewer variables, they will typically include less context for each event they describe. In contrast, scientists working with living systems need to include more context because there can be significant variables which impact the research outcomes. 

Croucher’s study examines the assertion that culture influences communication style (high/low context) preference. Data was gathered in India, Ireland, Thailand, and the United States where the results confirm that "high-context nations (India and Thailand) prefer the avoiding and obliging conflict styles more than low-context nations (Ireland and the United States), whereas low-context nations prefer the uncompromising and dominating communication style more than high-context nations."

In addition, Hall identified countries such as Japan, Arabic countries and some Latin American Countries to practice high-context culture; “High context communication carries most of its information within physical acts and features such as avoiding eye contact or even the shrug of a shoulder.” On the other hand, he identified countries such as Germany, the United States and Scandinavia as low context cultures. These countries are quite explicit and elaborate without having prior knowledge to each member’s history or background. 

Cultures and languages are defined as higher or lower context on a spectrum. For example, it could be argued that the Canadian French language is higher context than Canadian English, but lower context than Spanish or French French. An individual from Texas (a higher-context culture) may communicate with a few words or use of a prolonged silence characteristic of Texan English, where a New Yorker would be very explicit (as typical of New York City English), although both speak the same language (American English) and are part of a nation (the United States of America) which is lower-context relative to other nations. Hall notes a similar difference between Navajo-speakers and English-speakers in a United States school. 

Hall and Hall proposed a "spectrum" of national cultures from "High-Context cultures" to "Low-Context Cultures. This has been expanded to further countries by Sheposh & Shaista.

Cultural context can also shift and evolve. For instance, a study has argued that both Japan and Finland (high-context cultures) are becoming lower-context with the increased influence of Western European and United States culture.

This study, done by Kim Dunghoon, was to test the major aspects the high versus low context culture concepts. Three samples were gathered from the U.S, China, and Korea, three different cultures. From each culture, Ninety-six business managers were surveyed for the American and Chinese sample and 50 managers were surveyed from Korea. According to Hall's theory, Chinese and Korean samples represented higher context cultures while the American sample represents lower context. 16 items were tested in this study. Each of them covers different aspects of the high-versus low-context concept including: “¬¬¬social orientation, responsibility, confrontation, communication, commitment, and dealing with new situations". "The results show that American, Chinese, and Korean samples were significantly different on 15 of the sixteen items. Out of the 15 items, 11 are significant at the .01 level, 1 at the .05 level, and 3 at the .10 level. The composite score also shows a significant difference among the three samples at the .01 level". The American sample scored the lowest compared to the two “Oriental samples” which is consistent with Hall's concept. Overall, this study offers more evidence supporting the high versus low context culture concepts with Chinese, Korean, and American test participants. The results show that in high context cultures, such as China and Korea, people appear to be “more socially oriented, less confrontational, and more complacent with existing ways of living” compared to people from low context cultures, like America.

This case study was done on 30 Romanian and 30 Russian employees, to show a comparison of western cultural patterns based on high and low context cultures. Russia is a country that is a high context culture, while the people in Romania like to believe they are of western culture, which is a low context culture. Research done proves that Romanian culture is one of high context, even though its employees do not think so. "The total points from the table below show without any reason of a doubt the match between Romanian and Russian culture as patterns of long run, therefore of high context culture". This table shows the major differences and similarities on individual queries. The total numbers found at the end of the table prove how similar Romanian and Russians are, showing very few differences between the people in each culture.

The categories of context cultures are not totally separate. Both often take many aspects of the other's cultural communication abilities and strengths into account. The terms high- and low-context cultures are not classified with strict individual characteristics or boundaries. Instead, many cultures tend to have a mixture or at least some concepts that are shared between them, overlapping the two context cultures.

Ramos suggests that "in low context culture, communication members’ communication must be more explicit. As such, what is said is what is meant, and further analysis of the message is usually unnecessary."This implies that communication is quite direct and detailed because members of the culture are not expected to have knowledge of each other's histories, past experience or background. Because low-context communication concerns more direct messages, the meaning of these messages is more dependent on the words being spoken rather than on the interpretation of more subtle or unspoken cues.

The Encyclopedia of Diversity and Social Justice states that, "high context defines cultures that are relational and collectivist, and which most highlight interpersonal relationships. Cultures and communication in which context is of great importance to structuring actions is referred to as high context."In such cultures, people are highly perceptive of actions. Furthermore, cultural aspects such as tradition, ceremony, and history are also highly valued. Because of this, many features of cultural behavior in high-context cultures, such as individual roles and expectations, do not need much detailed or thought-out explanation.

According to Watson, "the influence of cultural variables interplays with other key factors – for example, social identities, those of age, gender, social class and ethnicity; this may include a stronger or weaker influence." A similarity that the two communication styles share is its influence on social characteristics such as age, gender, social class and ethnicity. For example, for someone who is older and more experienced within a society, the need for social cues may be higher or lower depending on the communication style. The same applies for the other characteristics in varied countries.

On the other hand, certain intercultural communication skills are unique for each culture and it is significant to note that these overlaps in communication techniques are represented subgroups within social interactions or family settings. Many singular cultures that are large have subcultures inside of them, making communication and defining them more complicated than the low context and high context culture scale. The diversity within a main culture shows how the high and low scale differs depending on social settings such as school, work, home, and in other countries; variation is what allows the scale to fluctuate even if a large culture is categorized as primarily one or the other.

Between each type of culture context, there will be forms of miscommunication because of the difference in gestures, social cues, and intercultural adjustments; however, it is important to recognize these differences and learn how to avoid miscommunication to benefit certain situations. Since all sets of cultures differ, especially from a global standpoint where language also creates a barrier for communication, social interactions specific to a culture normally require a range of appropriate communication abilities that an opposing culture may not understand or know about. This significance follows into many situations such as the workplace, which can be prone to diversified cultures and opportunities for collaboration and working together. Awareness of miscommunication between high and low context cultures within the workplace or intercultural communication settings advocates for collected unification within a group through the flexibility and ability to understand one another. 

Families, subcultures and in-groups typically favour higher-context communication. Groups that are able to rely on a common background may not need to use words as explicitly to understand each other. Settings and cultures where people come together from a wider diversity of backgrounds such as international airports, large cities, or multi-national firms, tend to use lower-context communication forms.

Hall links language to culture through the work of Sapir-Whorf on linguistic relativity. A trade language will typically need to explicitly explain more of the context than a dialect which can assume a high level of shared context. Because a low-context setting cannot rely on shared understanding of potentially ambiguous messages, low-context cultures tend to give more information, or to be precise in their language. In contrast, a high-context language like Japanese or Chinese can use a high number of homophones but still be understood by a listener who knows the context. 

The concept of elaborated and restricted codes is introduced by sociologist Basil Bernstein in his book "Class, Codes and Control". An elaborated code indicates that the speaker is expressing his/her idea by phrasing from an abundant selection of alternatives without assuming the listener shares significant amounts of common knowledge, which allows the speaker to explain their idea explicitly. In contrast, restricted codes are phrased from more limited alternatives, usually with collapsed and shortened sentences. Therefore, restricted codes require listeners to share a great deal of common perspective to understand the implicit meanings and nuances of a conversation.

Restricted codes are commonly used in high-context culture groups, where group members share the same cultural background and can easily understand the implicit meanings "between the lines" without further elaboration. Conversely, in cultural groups with low context, where people share less common knowledge or ‘value individuality above group identification’, detailed elaboration becomes more essential to avoid misunderstanding.

The concepts of collectivism and individualism have been applied to high- and low-context cultures by Dutch psychologist Geert Hofstede in his Cultural Dimensions Theory. Collectivist societies prioritize the group over the individual, and vice versa for individualist ones. In high-context cultures, language may be used to assist and maintain relationship-building and to focus on process. India and Japan are typically high-context, highly collectivistic cultures, where business is done by building relationships and maintaining respectful communication.

Individualistic cultures promote the development of individual values and independent social groups. Individualism may lead to communicating to all people in a group in the same way, rather than offering hierarchical respect to certain members. Because individualistic cultures may value cultural diversity, a more explicit way of communicating is often required to avoid misunderstanding. Language may be used to achieve goals or exchange information. The USA and Australia are typically low-context, highly individualistic cultures, where transparency and competition in business are prized.

High-context cultures tend to be more stable, as their communication is more economical, fast, efficient and satisfying; but these are gained at a price of devoting time into preprogramming cultural background, and their high stability might come with a price of a high barrier for development. By contrast, low-context cultures tend to change more rapidly and drastically, allowing extension to happen at faster rates. This also means that low-context communication may fail due to the overload of information, which makes culture lose its screening function.

Therefore, higher-context cultures tend to correlate with cultures that also have a strong sense of tradition and history, and change little over time. For example, Native Americans in the United States have higher-context cultures with a strong sense of tradition and history, compared to general American culture. Focusing on tradition creates opportunities for higher context messages between individuals of each new generation, and the high-context culture feeds back to the stability hence allows the tradition to be maintained. This is in contrast to lower-context cultures in which the shared experiences upon which communication is built can change drastically from one generation to the next, creating communication gaps between parents and children, as in the United States.

Culture also affects how individuals interpret other people's facial expressions. An experiment performed by the University of Glasgow shows that different cultures have different understanding of the facial expression signals of the six basic emotions, which are the so-called "universal language of emotion"—happiness, surprise, fear, disgust, anger and sadness. In high-context cultures, facial expressions and gestures take on greater importance in conveying and understanding a message, and the receiver may require more cultural context to understand "basic" displays of emotions.

Cultural differences in advertising and marketing may also be explained through high- and low-context cultures. One study on McDonald's online advertising compared Japan, China, Korea, Hong Kong, Pakistan, Germany, Denmark, Sweden, Norway, Finland, and the United States, and found that in high-context countries, the advertising used more colors, movements, and sounds to give context, while in low-context cultures the advertising focused more on verbal information and linear processes.

In a 2008 meta-analysis of 224 articles published between 1990 and 2006, Peter W. Cardon wrote:[T]he theory was never described by Hall with any empirical rigor, and no known research involving any instrument or measure of contexting validates it ... Ironically, contexting is most frequently discussed in terms of directness, yet empirical studies nearly all fail to support this relationship. In other words, the relationship between directness and contexting based on traditional classifications of [high-context] and [low-context] cultures is particularly tenuous. Most of the contexting categories simply have not been researched enough to make firm conclusions. But the fact that contexting has not been empirically validated should not necessarily be construed as a failure of the theory ... Nonetheless, the contexting model simply cannot be described as an empirically validated model.





</doc>
<doc id="41748961" url="https://en.wikipedia.org/wiki?curid=41748961" title="Resistance through culture">
Resistance through culture

Resistance through culture (also called cultural resistance, resistance through the aesthetic, or intellectual resistance) is a form of nonconformism. It is not open dissent, but a discreet stance.

A revolt "so well hidden that it seem[s] inexistent", it is a quest "to extend the boundaries of official tolerance, either by adopting a line considered by authorities to be ideologically suspect, or by highlighting certain contemporary social problems, or both." Criticized for being "utopian, and thus inadequate to the realities of that age", during the time of the Communist regimes in Europe, it was also a surviving formula, a modality for writers and artists to cheat Communist censorship without going the whole way into open political opposition.

One of the most sharply criticized phrases in post-revolutionary Romania, considered to be not much more than "blowing in the wind" by Romanian-born German Nobel literature prize winner Herta Müller, and "not only resignation [...] but complicity with the terorist communism" by Romanian exiled writer Paul Goma, so-called "resistance through culture" has often been linked to Constantin Noica's so-called "Păltiniș School".

In the fine arts, Corneliu Baba, among others, is sometimes considered to be an example of a painter who was nonconformist in this way.


</doc>
<doc id="590768" url="https://en.wikipedia.org/wiki?curid=590768" title="Classic">
Classic

A classic is an outstanding example of a particular style; something of lasting worth or with a timeless quality; of the first or highest quality, class, or rank – something that exemplifies its class. The word can be an adjective (a "classic" car) or a noun (a "classic" of English literature). It denotes a particular quality in art, architecture, literature, design, technology, or other cultural artifacts. In commerce, products are named 'classic' to denote a long-standing popular version or model, to distinguish it from a newer variety. "Classic" is used to describe many major, long-standing sporting events. Colloquially, an everyday occurrence (e.g. a joke or mishap) may be described in some dialects of English as 'an absolute classic'.

"Classic" should not be confused with "classical", which refers specifically to certain cultural styles, especially in music and architecture: styles generally taking inspiration from the Classical tradition, hence classicism.

The classics are the literature of ancient Greece and Rome, known as classical antiquity, and once the principal subject studied in the humanities. Classics (without the definite article) can refer to the study of philosophy, literature, history and the arts of the ancient world, as in "reading classics at Cambridge". From that usage came the more general concept of 'classic'.

The Chinese classics occupy a similar position in Chinese culture, and various other cultures have their own classics.

Books, films and music particularly may become "a classic" but a painting would more likely be called a masterpiece. A classic is often something old that is still popular.

The first known use of "classic" in this sense — a work so excellent that it is on the level of the "classics" (Greek and Latin authors) — is by the 18th-century scholar Rev. John Bowle. He applied the term to "Don Quixote", of which Bowle prepared an innovative edition, such as he judged that a classic work needed.

Some other examples would be the book "The Adventures of Tom Sawyer" by Mark Twain, the 1941 film "Citizen Kane", and the song Blue Suede Shoes by Elvis Presley. Lists of classics are long and wide-ranging, and would vary depending on personal opinion. Classic rock is a popular radio format, playing a repertoire of old but familiar recordings.

A contemporary work may be hailed as an "instant classic", but the criteria for classic status tends to include the test of time. The term "classic" is in fact often generalized to refer to any work of a certain age, regardless of whether it is any good. A cult classic may be well known but is only favored by a minority.

A well known and reliable procedure, such as a demonstration of well-established scientific principle, may be described as classic: e.g. the cartesian diver experiment.

Manufacturers frequently describe their products as classic, to distinguish the original from a new variety, or to imply qualities in the product – although the Ford Consul Classic, a car manufactured 1961–1963, has the "classic" tag for no apparent reason. The iPod classic was simply called the iPod until the sixth generation, when "classic" was added to the name because other designs were also available – an example of a retronym. "Coca-Cola Classic" is the name used for the relaunch of Coca-Cola after the failure of the New Coke recipe change. Similarly, the Classic (transit bus), a transit bus manufactured from 1982–97, succeeded an unpopular futuristic design.

A classic can be something old that remains prized or valuable (but not an antique). Classic cars, for example, are recognised by various collectors' organisations such as the Classic Car Club of America, who regulate the qualifying attributes that constitute classic status.

Many sporting events take the name "classic":


In Spanish-speaking countries, the term "Clásico" refers to a match between two football teams known as traditional rivals, e.g. El Clásico in Spain.



</doc>
<doc id="42534554" url="https://en.wikipedia.org/wiki?curid=42534554" title="Theology of culture">
Theology of culture

Theology of culture is a branch of theology that studies culture and cultural phenomenas. It lies close to philosophy of culture, but has focus more on existentialism and spiritualism.

Paul Tillich was the first theologian who wrote about the theology of culture. He discussed about making difference between the sacred and the secular. Nowadays, the theology of culture also deals with cultural differences between religions and thus shares many features with the theology of religions.


</doc>
<doc id="37886950" url="https://en.wikipedia.org/wiki?curid=37886950" title="Culture and social cognition">
Culture and social cognition

Culture and social cognition is the relationship between human culture and human cognitive capabilities. Cultural cognitive evolution proposes that humans’ unique cognitive capacities are not solely due to biological inheritance, but are in fact due in large part to cultural transmission and evolution (Tomasello, 1999). Modern humans and great apes are separated evolutionarily by about six million years. Proponents of cultural evolution argue that this would not have been enough time for humans to develop the advanced cognitive capabilities required to create tools, language, and build societies through biological evolution. Biological evolution could not have individually produced each of these cognitive capabilities within that period of time. Instead, humans must have evolved the capacity to learn through cultural transmission (Tomasello, 1999). This provides a more plausible explanation that would fit within the given time frame. Instead of having to biologically account for each cognitive mechanism that distinguishes modern humans from previous relatives, one would only have to account for one significant biological adaptation for cultural learning. According to this view, the ability to learn through cultural transmission is what distinguishes humans from other primates (Tomasello, 1999). Cultural learning allows humans to build on existing knowledge and make collective advancements, also known as the “ratchet effect”. The ratchet effect simply refers to the way in which humans continuously add on to existing knowledge through modifications and improvements. This unique ability distinguishes humans from related primates, who do not seem to build collaborative knowledge over time. Instead, primates seem to build individual knowledge, in which the expertise of one animal is not built on by others, and does not progress across time.

Human cultural learning involves:
Cultural learning is made possible by a deep understanding of social cognition. Humans have the unique capacity to identify and relate to others and view them as intentional beings. Humans are able to understand that others have intentions, goals, desires, and beliefs. It is this deep understanding, this cognitive adaptation, that allows humans to learn from and with others through cultural transmission (Tomasello, 1999).

Primates show distinct characteristics of social cognition in comparison to mammals. Mammals are able to identify members of their species, understand basic kinships and basic social hierarchies, make predictions about others’ behavior based on emotion and movement, and engage in social learning (Tomasello, 1999). Primates, however, show a more extensive understanding of these concepts. Primates not only understand kinship and social hierarchies, but they also have an understanding of relational categories. That is, primates are able to understand social relations that extend beyond their individual interaction with others. Mammals are able to form direct relationships based on social hierarchies, but primates have an understanding of social hierarchies and relationships that extend beyond them personally. Researchers believe that this understanding of relational categories might have been the evolutionary precursor to humans’ deeper understanding of desires, beliefs, and goals underlying causal relationships, and thereby allowing humans to relate to and understand other individuals, making way for cultural evolution (Tomasello, 1999).

Although it is now believed that non-human primates such as chimpanzees have some limited understanding of others as intentional beings, it is clear that these understanding are not as deep as human understanding of others as intentional agents. Chimpanzees, for example, demonstrated an ability to think about what others see, and predict behavior based on these beliefs in several studies conducted by Tomasello and Hare (2003). For example, subordinate chimpanzees in one experiment avoided food that they knew the dominant chimpanzee could see, but sought food that the dominant chimpanzee could not see due to a physical barrier. In another experiment, subordinate chimpanzees made decisions about approaching food based on whether or not the dominant chimpanzee had seen the human researcher place the food behind the barrier. Chimpanzees were also found to react differently to humans who were unwilling versus unable to provide food (teasing the chimpanzee with food, or pretending to have an accident with it), thereby showing some ability to discriminate intentionality.

Dogs have also shown some interesting but limited abilities at social cognition in a series of studies by Hare and Tomasello (2005). Dogs have the ability to read human social cues, even to a greater extent than chimpanzees. Dogs are able to respond to human pointing, the human gaze, and subtle human nods without training. Researchers now believe that these abilities are the result of convergent evolution between humans and dogs through domestication. Research with domesticated foxes has shown that the likely mechanism for this convergent evolution was the selection of tame behavior in dogs. This finding suggests that perhaps humans had to evolve a propensity to cooperate before cultural evolution was able to take place (Hare & Tomasello, 2005).

Sociogenesis refers to collaborative inventiveness. It is the process by which two or more humans collectively interact and invent something new which could not have been developed by one individual alone, such as language and mathematics (Tomasello, 1999). Sociogenesis can occur across time, or simultaneously (Tomasello, 1999). Socigenesis across times occurs through the ratchet effect, when one individual modifies something they had previously learned through others. Over time, ideas, tools, and language advance. Simultaneous sociogenesis occurs when two or more individuals work together at the same time and develop something new.

In response to the nature versus nurture and learned versus innate debate, proponents of cultural evolution argue that cognitive psychology must take into account historical processes when studying and discussion cognition (Tomasello, 1999). For example, the similarities between languages have led many researchers to decry that language or aspects of language must be innate. The extreme variability in math and counting systems across cultures has prevented similar conclusions for math. However, Tomasello argues that if you look at these concepts with historical processes in mind, another plausible explanation could be that language, but not math, developed before people split into different populations. Math developed only after such split, and because the cultural needs of these people differed, differential counting and mathematical systems resulted. The critique is that categorizing concepts as innate or learned does not tell us anything about the process by which they originally developed.

 Hare, B., & Tomasello, M. (2005). Human-like social skills in dogs? Trends in Cognitive Sciences, Vol. 9 (9), 439-444.


</doc>
<doc id="12401182" url="https://en.wikipedia.org/wiki?curid=12401182" title="Design theory">
Design theory

Design theory is a subfield of design research concerned with various theoretical approaches towards understanding and delineating design principles, design knowledge, and design practice.

Design theory has been approached and interpreted in many ways, from personal statements of design principles, through constructs of the philosophy of design to a search for a design science.

The essay "Ornament and Crime" by Adolf Loos from 1908 is one of the early 'principles' design-theoretical texts. Others include Le Corbusier's "Vers une architecture", and Victor Papanek's "Design for the real world" (1972).

In a 'principles' approach to design theory, the De Stijl movement promoted a geometrical abstract, "ascetic" form of purism that was limited to functionality. This modernist attitude underpinned the Bauhaus movement. Principles were drawn up for design that were applicable to all areas of modern aesthetics.

For an introduction to the philosophy of design see the article by Per Galle at the Royal Danish Academy.

An example of early design science was Altshuller's "Theory of inventive problem solving", known as TRIZ, from Russia in the 1940s. Herbert Simon's 1969 "The sciences of the artificial" began a more scientific basis to the study of design. Since then the further development of fields such as design methods, design research, design science and design thinking has promoted a wider understanding of design theory.





</doc>
<doc id="32017750" url="https://en.wikipedia.org/wiki?curid=32017750" title="Languaculture">
Languaculture

Languaculture is a term meaning that a language includes not only elements such as grammar and vocabulary, but also past knowledge, local and cultural information, habits and behaviours. The term was created by the American anthropologist Michael Agar.

Agar used the term "languaculture" for the first time in his book "Language Shock: Understanding the culture of conversation". Languaculture is a supposed improvement on the term "linguaculture" coined by the American linguistic anthropologist Paul Friedrich. Agar explains the change stating that "language" is a more commonly used word in English. It seems that "linguaculture" is getting more common again (cf. Risager 2012).

When Agar talks about languaculture, he defines it as the necessary tie between language and culture. He underlines that languages and cultures are always closely related and it is not possible to distinguish languages from cultures. Therefore, you cannot really know a language if you do not know also the culture expressed by that language.

The notion of culture and its understanding involve the link between two different languacultures that Agar define LC1 (source languaculture) and LC2 (target languaculture).

The learning of target languaculture is driven by "rich points". We realize that a culture is different from ours when we face some behaviours which we do not understand. Rich points are those surprises, those departures from an outsider's expectations that signal a difference between source languaculture and target languaculture. They are the moments of incomprehension, when you suddenly do not know what is happening. In this situation different reactions are possible. You can ignore the rich point and hope that the next part makes sense. You can perceive it as evidence that the person who produced it has some lacks. Or you can wonder why you do not understand and if maybe some other languaculture comes into play. Therefore, rich points belong to daily life and not only to language. Agar highlights that the term "rich" has the positive connotations of thickness, wealth and abundance. The largest rich point is the total incomprehension due to huge differences between source languaculture and target languaculture. In this case we are facing a ‘culture shock’ that causes a deep bewilderment. The smallest rich point can occur among different groups of the same community.
The existence of rich points comes from the fact that every statement implicitly refers to various elements that are taken for granted in a certain culture and do not match the elements of another culture (cultural implicitness).

According to Agar, culture is a construction, a translation between source languaculture and target languaculture. Like a translation, it makes no sense to talk about the culture of X without saying the culture of X for Y, taking into account the standpoint from which it is observed. For this reason culture is relational.
Moreover, culture is always plural. No person or group can be described, explained or generalized completely with a single cultural label.



</doc>
<doc id="1525258" url="https://en.wikipedia.org/wiki?curid=1525258" title="Highbrow">
Highbrow

Used colloquially as a noun or adjective, "highbrow" is synonymous with intellectual; as an adjective, it also means elite, and generally carries a connotation of high culture. The word draws its metonymy from the pseudoscience of phrenology, and was originally simply a physical descriptor.

"Highbrow" can be applied to music, implying most of the classical music tradition; to literature—i.e., literary fiction and poetry; to films in the arthouse line; and to comedy that requires significant understanding of analogies or references to appreciate. The term "highbrow" is considered by some (with corresponding labels as 'middlebrow' 'lowbrow') as discriminatory or overly selective; and "highbrow" is currently distanced from the writer by quotation marks: "We thus focus on the consumption of two generally recognised 'highbrow' genres—opera and classical". The first usage in print of "highbrow" was recorded in 1884. The term was popularized in 1902 by Will Irvin, a reporter for "The Sun", who adhered to the phrenological notion of more intelligent people having high foreheads.

The opposite of "highbrow" is "lowbrow", and between them is "middlebrow", describing culture that is neither high nor low; as a usage, "middlebrow" is derogatory, as in Virginia Woolf's unsent letter to the "New Statesman", written in the 1930s and published in "The Death of the Moth and Other Essays" (1942). According to the "Oxford English Dictionary", the word "middlebrow" first appeared in print in 1925, in "Punch": "The BBC claims to have discovered a new type—'the middlebrow'. It consists of people who are hoping that some day they will get used to the stuff that they ought to like". The term had previously appeared in hyphenated form in "The Nation", on 25 January 1912:
It was popularized by the American writer and poet Margaret Widdemer, whose essay "Message and Middlebrow" appeared in the "Review of Literature" in 1933. The three genres of fiction, as American readers approached them in the 1950s and as obscenity law differentially judged them, are the subject of Ruth Pirsig Wood, "Lolita in Peyton Place: Highbrow, Middlebrow, and Lowbrow Novels", 1995.

Prince Hamlet was considered by Virginia Woolf as a highbrow lacking orientation in the world once he had lost the lowbrow Ophelia with her grip on earthly realities: this, she thought, explained why in general highbrows "honour so wholeheartedly and depend so completely upon those who are called lowbrows".




</doc>
<doc id="47917953" url="https://en.wikipedia.org/wiki?curid=47917953" title="Ceremonial pole">
Ceremonial pole

A ceremonial pole symbolizes a variety of concepts in several different cultures. For example, in the Miao culture in Yunnan China. In "The Evolution of the Idea of God", Grant Allen notes that Samoyeds of Siberia, and Damara of South Africa plant stakes at the graves of ancestors. According to Zelia Nuttall in "The Fundamental Principles Of Old and New World Civilizations", tree and pole reverence to Anu in ancient Babylonia-Assyria may have evolved from the fire-drill and beam of the oil press, stating that it was extremely probable that the primitive employment of a fire-stick by the priesthood, for the production of "celestial fire," may have played an important role in causing the stick, and thence the pole and tree, to become the symbol of Anu.

"Kay Htoe Boe" is a Karenni ancient dance and prayer festival, held by the men in the Kayan community in Myanmar (Burma). In the Kayan creation story, the Eugenia tree is the first tree in the world. Kay Htoe Boe poles are usually made from the Eugenia tree.

"Kay Htoe Boe" poles have four levels, named for the stars, sun and moon, and the fourth level is a ladder made with a long white cotton cloth.

A "jangseung" or "village guardian" is a Korean ceremonial pole, usually made of wood. Jangseungs were traditionally placed at the edges of villages to mark for village boundaries and frighten away demons. They were also worshipped as village tutelary deities.

An Asherah pole is a sacred tree or pole that stood near Canaanite religious locations to honor the Ugaritic mother-goddess Asherah, consort of El. The relation of the literary references to an "asherah" and archaeological finds of Judaean pillar-figurines has engendered a literature of debate.

The "asherim" were also cult objects related to the worship of the fertility goddess Asherah, the consort of either Ba'al or, as inscriptions from Kuntillet ‘Ajrud and Khirbet el-Qom attest, Yahweh, and thus objects of contention among competing cults. The insertion of "pole" begs the question by setting up unwarranted expectations for such a wooden object: "we are never told exactly what it was", observes John Day. Though there was certainly a movement against goddess-worship at the Jerusalem Temple in the time of King Josiah, it did not long survive his reign, as the following four kings "did what was evil in the eyes of Yahweh" (2 Kings 23:32, 37; 24:9, 19). Further exhortations came from Jeremiah. The traditional interpretation of the Biblical text is that the Israelites imported pagan elements such as the Asherah poles from the surrounding Canaanites. In light of archeological finds, however, modern scholars now theorize that the Israelite folk religion was Canaanite in its inception and always polytheistic, and it was the prophets and priests who denounced the Asherah poles who were the innovators; such theories inspire ongoing debate.

In present times in Indian subcontinent several festivals and celebrations, as in Hinglajmata Sindh, Gudi Padwa, KathiKawadi, Jatarakathi, Nandidhwaja, Khambadev (Maharashtra), Nimad (Madhya Pradesh), Gogaji temple (Rajasthan) and Khambeshvari (Odisha) then in Tripura and in Manipur, central poles are features in temple and festival settings.

According to Adi Parva (critical edition) of Indian epic Mahabharata a Bamboo festival named "Shakrotsava" was Celebrated in Chedi Kingdom. Uparichara Vasu was a king of Chedi belonging to the Puru dynasty. He was known as the friend of Indra. During his reign, Chedi kingdom introduced "Shakrotsava" festival in his kingdom in the honor of Indra. The festival involved planting of a bamboo pole every year, in honor of Indra. The king will then pray for the expansion of his cities and kingdom. After erecting the pole, people decked it with golden cloth and scents and garlands and various ornaments. (1,63).

A maypole is a tall wooden pole erected as a part of various European folk festivals, around which a maypole dance often takes place.

The festivals may occur on May Day or Pentecost (Whitsun), although in some countries it is instead erected at Midsummer. In some cases the maypole is a permanent feature that is only utilised during the festival, although in other cases it is erected specifically for the purpose before being taken down again.

Primarily found within the nations of Germanic Europe and the neighbouring areas which they have influenced, its origins remain unknown, although it has been speculated that it originally had some importance in the Germanic paganism of Iron Age and early Medieval cultures, and that the tradition survived Christianisation, albeit losing any original meaning that it had. It has been a recorded practice in many parts of Europe throughout the Medieval and Early Modern periods, although became less popular in the 18th and 19th centuries. Today, the tradition is still observed in some parts of Europe and among European communities in North America.

The fact that they were found primarily in areas of Germanic Europe, where, prior to Christianisation, Germanic paganism was followed in various forms, has led to speculation that the maypoles were in some way a continuation of a Germanic pagan tradition. One theory holds that they were a remnant of the Germanic reverence for sacred trees, as there is evidence for various sacred trees and wooden pillars that were venerated by the pagans across much of Germanic Europe, including Thor's Oak and the Irminsul. It is also known that, in Norse paganism, cosmological views held that the universe was a world tree, known as Yggdrasil.

The floor of the Mære Church, Norway, was excavated in 1969 and found to contain the remains of a pagan cult structure. The nature of that structure was not clear. Lidén felt this represented the remains of a building, but a critique by Olsen (1969:26) in the same work suggested this may have been a site for pole-related rituals. A recent review of the evidence by Walaker (Norddide 2011: 107-113) concluded that this site was similar to the site in Hove (Åsen, also in Nord-Trøndelag) and was therefore likely the site of a ceremonial pole.

In Māori mythology, Rongo – the god of cultivated food, especially the kūmara, a vital food crop – is represented by a god stick called "whakapakoko atua".

In the Cook Islands Cult figures called staff-gods or "atua rakau" from Rarotonga, apparently combine images of gods with their human descendants. They range in length between 28 inches (71 cm) and 18 feet (5.5 m) and were carried and displayed horizontally.



</doc>
<doc id="25414" url="https://en.wikipedia.org/wiki?curid=25414" title="Religion">
Religion

Religion is a social-cultural system of designated behaviors and practices, morals, worldviews, texts, sanctified places, prophecies, ethics, or organizations, that relates humanity to supernatural, transcendental, or spiritual elements. However, there is no scholarly consensus over what precisely constitutes a religion.

Different religions may or may not contain various elements ranging from the divine, sacred things, faith, a supernatural being or supernatural beings or "some sort of ultimacy and transcendence that will provide norms and power for the rest of life". Religious practices may include rituals, sermons, commemoration or veneration (of deities and/or saints), sacrifices, festivals, feasts, trances, initiations, funerary services, matrimonial services, meditation, prayer, music, art, dance, public service, or other aspects of human culture. Religions have sacred histories and narratives, which may be preserved in sacred scriptures, and symbols and holy places, that aim mostly to give a meaning to life. Religions may contain symbolic stories, which are sometimes said by followers to be true, that have the side purpose of explaining the origin of life, the universe, and other things. Traditionally, faith, in addition to reason, has been considered a source of religious beliefs.

There are an estimated 10,000 distinct religions worldwide. About 84% of the world's population is affiliated with either Christianity, Islam, Hinduism, Buddhism, or some form of folk religion. The religiously unaffiliated demographic includes those who do not identify with any particular religion, atheists, and agnostics. While the religiously unaffiliated have grown globally, many of the religiously unaffiliated still have various religious beliefs.<ref name="Pew Global Unaffiliated 12/2012"></ref>

The study of religion encompasses a wide variety of academic disciplines, including theology, comparative religion and social scientific studies. Theories of religion offer various explanations for the origins and workings of religion, including the ontological foundations of religious being and belief.

"Religion" (from O.Fr. "religion" religious community, from L. "religionem" (nom. "religio") "respect for what is sacred, reverence for the gods, sense of right, moral obligation, sanctity", "obligation, the bond between man and the gods") is derived from the Latin "religiō", the ultimate origins of which are obscure. One possible interpretation traced to Cicero, connects ' read, i.e. "re" (again) with "lego" in the sense of choose, go over again or consider carefully. The definition of "religio" by Cicero is "cultum deorum", "the proper performance of rites in veneration of the gods." Julius Caesar used "religio" to mean "obligation of an oath" when discussing captured soldiers making an oath to their captors. The Roman naturalist Pliny the Elder used the term "religio" on elephants in that they venerate the sun and the moon. Modern scholars such as Tom Harpur and Joseph Campbell favor the derivation from ' bind, connect, probably from a prefixed "", i.e. "re" (again) + "ligare" or to reconnect, which was made prominent by St. Augustine, following the interpretation given by Lactantius in "Divinae institutiones", IV, 28. The medieval usage alternates with "order" in designating bonded communities like those of monastic orders: "we hear of the 'religion' of the Golden Fleece, of a knight 'of the religion of Avys'".

In classic antiquity, 'religio' broadly meant conscientiousness, sense of right, moral obligation, or duty to anything. In the ancient and medieval world, the etymological Latin root "religio" was understood as an individual virtue of worship in mundane contexts; never as doctrine, practice, or actual source of knowledge. In general, "religio" referred to broad social obligations towards anything including family, neighbors, rulers, and even towards God. "Religio" was most often used by the ancient Romans not in the context of a relation towards gods, but as a range of general emotions such as hesitation, caution, anxiety, fear; feelings of being bound, restricted, inhibited; which arose from heightened attention in any mundane context. The term was also closely related to other terms like "scrupulus" which meant "very precisely" and some Roman authors related the term "superstitio", which meant too much fear or anxiety or shame, to "religio" at times. When "religio" came into English around the 1200s as religion, it took the meaning of "life bound by monastic vows" or monastic orders. The compartmentalized concept of religion, where religious things were separated from worldly things, was not used before the 1500s. The concept of religion was first used in the 1500s to distinguish the domain of the church and the domain of civil authorities.

In the ancient Greece, the Greek term "threskeia" was loosely translated into Latin as "religio" in late antiquity. The term was sparsely used in classical Greece but became more frequently used in the writings of Josephus in the first century CE. It was used in mundane contexts and could mean multiple things from respectful fear to excessive or harmfully distracting practices of others; to cultic practices. It was often contrasted with the Greek word "deisidaimonia" which meant too much fear.

The modern concept of religion, as an abstraction that entails distinct sets of beliefs or doctrines, is a recent invention in the English language. Such usage began with texts from the 17th century due to events such the splitting of Christendom during the Protestant Reformation and globalization in the age of exploration, which involved contact with numerous foreign cultures with non-European languages.
Some argue that regardless of its definition, it is not appropriate to apply the term religion to non-Western cultures. Others argue that using religion on non-Western cultures distorts what people do and believe.

The concept of religion was formed in the 16th and 17th centuries, despite the fact that ancient sacred texts like the Bible, the Quran, and others did not have a word or even a concept of religion in the original languages and neither did the people or the cultures in which these sacred texts were written. For example, there is no precise equivalent of religion in Hebrew, and Judaism does not distinguish clearly between religious, national, racial, or ethnic identities. One of its central concepts is "halakha", meaning the walk or path sometimes translated as law, which guides religious practice and belief and many aspects of daily life. Even though the beliefs and traditions of Judaism are found in the ancient world, ancient Jews saw Jewish identity as being about an ethnic or national identity and did not entail a compulsory belief system or regulated rituals. Even in the 1st century CE, Josephus had used the Greek term "ioudaismos", which some translate as Judaism today, even though he used it as an ethnic term, not one linked to modern abstract concepts of religion as a set of beliefs. It was in the 19th century that Jews began to see their ancestral culture as a religion analogous to Christianity. The Greek word "threskeia", which was used by Greek writers such as Herodotus and Josephus, is found in the New Testament. "Threskeia" is sometimes translated as religion in today's translations, however, the term was understood as worship well into the medieval period. In the Quran, the Arabic word "din" is often translated as religion in modern translations, but up to the mid-1600s translators expressed "din" as law.

The Sanskrit word dharma, sometimes translated as religion, also means law. Throughout classical South Asia, the study of law consisted of concepts such as penance through piety and ceremonial as well as practical traditions. Medieval Japan at first had a similar union between imperial law and universal or Buddha law, but these later became independent sources of power.

Throughout the Americas, Native Americans never had a concept of "religion" and any suggestion otherwise is a colonial imposition by Christians.

Though traditions, sacred texts, and practices have existed throughout time, most cultures did not align with Western conceptions of religion since they did not separate everyday life from the sacred. In the 18th and 19th centuries, the terms Buddhism, Hinduism, Taoism, Confucianism, and world religions first entered the English language. No one self-identified as a Hindu or Buddhist or other similar terms before the 1800s. "Hindu" has historically been used as a geographical, cultural, and later religious identifier for people indigenous to the Indian subcontinent. Throughout its long history, Japan had no concept of religion since there was no corresponding Japanese word, nor anything close to its meaning, but when American warships appeared off the coast of Japan in 1853 and forced the Japanese government to sign treaties demanding, among other things, freedom of religion, the country had to contend with this Western idea.

According to the philologist Max Müller in the 19th century, the root of the English word religion, the Latin "religio", was originally used to mean only reverence for God or the gods, careful pondering of divine things, piety (which Cicero further derived to mean diligence). Max Müller characterized many other cultures around the world, including Egypt, Persia, and India, as having a similar power structure at this point in history. What is called ancient religion today, they would have only called law.

Scholars have failed to agree on a definition of religion. There are, however, two general definition systems: the sociological/functional and the phenomenological/philosophical.

Religion is a modern Western concept. Parallel concepts are not found in many current and past cultures; there is no equivalent term for religion in many languages. Scholars have found it difficult to develop a consistent definition, with some giving up on the possibility of a definition. Others argue that regardless of its definition, it is not appropriate to apply it to non-Western cultures.

An increasing number of scholars have expressed reservations about ever defining the essence of religion. They observe that the way we use the concept today is a particularly modern construct that would not have been understood through much of history and in many cultures outside the West (or even in the West until after the Peace of Westphalia). The MacMillan Encyclopedia of Religions states:
The anthropologist Clifford Geertz defined religion as a
Alluding perhaps to Tylor's "deeper motive", Geertz remarked that
The theologian Antoine Vergote took the term supernatural simply to mean whatever transcends the powers of nature or human agency. He also emphasized the cultural reality of religion, which he defined as
Peter Mandaville and Paul James intended to get away from the modernist dualisms or dichotomous understandings of immanence/transcendence, spirituality/materialism, and sacredness/secularity. They define religion as
According to the MacMillan Encyclopedia of Religions, there is an experiential aspect to religion which can be found in almost every culture:

Friedrich Schleiermacher in the late 18th century defined religion as "das schlechthinnige Abhängigkeitsgefühl", commonly translated as "the feeling of absolute dependence".

His contemporary Georg Wilhelm Friedrich Hegel disagreed thoroughly, defining religion as "the Divine Spirit becoming conscious of Himself through the finite spirit."

Edward Burnett Tylor defined religion in 1871 as "the belief in spiritual beings". He argued that narrowing the definition to mean the belief in a supreme deity or judgment after death or idolatry and so on, would exclude many peoples from the category of religious, and thus "has the fault of identifying religion rather with particular developments than with the deeper motive which underlies them". He also argued that the belief in spiritual beings exists in all known societies.

In his book "The Varieties of Religious Experience", the psychologist William James defined religion as "the feelings, acts, and experiences of individual men in their solitude, so far as they apprehend themselves to stand in relation to whatever they may consider the divine". By the term divine James meant "any object that is god"like", whether it be a concrete deity or not" to which the individual feels impelled to respond with solemnity and gravity.

The sociologist Émile Durkheim, in his seminal book "The Elementary Forms of the Religious Life", defined religion as a "unified system of beliefs and practices relative to sacred things". By sacred things he meant things "set apart and forbidden—beliefs and practices which unite into one single moral community called a Church, all those who adhere to them". Sacred things are not, however, limited to gods or spirits. On the contrary, a sacred thing can be "a rock, a tree, a spring, a pebble, a piece of wood, a house, in a word, anything can be sacred". Religious beliefs, myths, dogmas and legends are the representations that express the nature of these sacred things, and the virtues and powers which are attributed to them.

Echoes of James' and Durkheim's definitions are to be found in the writings of, for example, Frederick Ferré who defined religion as "one's way of valuing most comprehensively and intensively". Similarly, for the theologian Paul Tillich, faith is "the state of being ultimately concerned", which "is itself religion. Religion is the substance, the ground, and the depth of man's spiritual life."

When religion is seen in terms of sacred, divine, intensive valuing, or ultimate concern, then it is possible to understand why scientific findings and philosophical criticisms (e.g., those made by Richard Dawkins) do not necessarily disturb its adherents.

Traditionally, faith, in addition to reason, has been considered a source of religious beliefs. The interplay between faith and reason, and their use as perceived support for religious beliefs, have been a subject of interest to philosophers and theologians.

The word "myth" has several meanings.

Ancient polytheistic religions, such as those of Greece, Rome, and Scandinavia, are usually categorized under the heading of mythology. Religions of pre-industrial peoples, or cultures in development, are similarly called myths in the anthropology of religion. The term myth can be used pejoratively by both religious and non-religious people. By defining another person's religious stories and beliefs as mythology, one implies that they are less real or true than one's own religious stories and beliefs. Joseph Campbell remarked, "Mythology is often thought of as "other people's" religions, and religion can be defined as mis-interpreted mythology."

In sociology, however, the term myth has a non-pejorative meaning. There, myth is defined as a story that is important for the group whether or not it is objectively or provably true. Examples include the resurrection of their real-life founder Jesus, which, to Christians, explains the means by which they are freed from sin, is symbolic of the power of life over death, and is also said to be a historical event. But from a mythological outlook, whether or not the event actually occurred is unimportant. Instead, the symbolism of the death of an old life and the start of a new life is what is most significant. Religious believers may or may not accept such symbolic interpretations.

The practices of a religion may include rituals, sermons, commemoration or veneration (of a deity, gods, or goddesses), sacrifices, festivals, feasts, trances, initiations, funerary services, matrimonial services, meditation, prayer, religious music, religious art, sacred dance, public service, or other aspects of human culture.

Religions have a societal basis, either as a living tradition which is carried by lay participants, or with an organized clergy, and a definition of what constitutes adherence or membership.

A number of disciplines study the phenomenon of religion: theology, comparative religion, history of religion, evolutionary origin of religions, anthropology of religion, psychology of religion (including neuroscience of religion and evolutionary psychology of religion), law and religion, and sociology of religion.

Daniel L. Pals mentions eight classical theories of religion, focusing on various aspects of religion: animism and magic, by E.B. Tylor and J.G. Frazer; the psycho-analytic approach of Sigmund Freud; and further Émile Durkheim, Karl Marx, Max Weber, Mircea Eliade, E.E. Evans-Pritchard, and Clifford Geertz.

Michael Stausberg gives an overview of contemporary theories of religion, including cognitive and biological approaches.

Sociological and anthropological theories of religion generally attempt to explain the origin and function of religion. These theories define what they present as universal characteristics of religious belief and practice.

The origin of religion is uncertain. There are a number of theories regarding the subsequent origins of religious practices.

According to anthropologists John Monaghan and Peter Just, "Many of the great world religions appear to have begun as revitalization movements of some sort, as the vision of a charismatic prophet fires the imaginations of people seeking a more comprehensive answer to their problems than they feel is provided by everyday beliefs. Charismatic individuals have emerged at many times and places in the world. It seems that the key to long-term success—and many movements come and go with little long-term effect—has relatively little to do with the prophets, who appear with surprising regularity, but more to do with the development of a group of supporters who are able to institutionalize the movement."

The development of religion has taken different forms in different cultures. Some religions place an emphasis on belief, while others emphasize practice. Some religions focus on the subjective experience of the religious individual, while others consider the activities of the religious community to be most important. Some religions claim to be universal, believing their laws and cosmology to be binding for everyone, while others are intended to be practiced only by a closely defined or localized group. In many places, religion has been associated with public institutions such as education, hospitals, the family, government, and political hierarchies.

Anthropologists John Monoghan and Peter Just state that, "it seems apparent that one thing religion or belief helps us do is deal with problems of human life that are significant, persistent, and intolerable. One important way in which religious beliefs accomplish this is by providing a set of ideas about how and why the world is put together that allows people to accommodate anxieties and deal with misfortune."

While religion is difficult to define, one standard model of religion, used in religious studies courses, was proposed by Clifford Geertz, who simply called it a "cultural system". A critique of Geertz's model by Talal Asad categorized religion as "an anthropological category". Richard Niebuhr's (1894–1962) five-fold classification of the relationship between Christ and culture, however, indicates that religion and culture can be seen as two separate systems, though not without some interplay.

One modern academic theory of religion, social constructionism, says that religion is a modern concept that suggests all spiritual practice and worship follows a model similar to the Abrahamic religions as an orientation system that helps to interpret reality and define human beings. Among the main proponents of this theory of religion are Daniel Dubuisson, Timothy Fitzgerald, Talal Asad, and Jason Ānanda Josephson. The social constructionists argue that religion is a modern concept that developed from Christianity and was then applied inappropriately to non-Western cultures.

Cognitive science of religion is the study of religious thought and behavior from the perspective of the cognitive and evolutionary sciences. The field employs methods and theories from a very broad range of disciplines, including: cognitive psychology, evolutionary psychology, cognitive anthropology, artificial intelligence, cognitive neuroscience, neurobiology, zoology, and ethology. Scholars in this field seek to explain how human minds acquire, generate, and transmit religious thoughts, practices, and schemas by means of ordinary cognitive capacities.

Hallucinations and delusions related to religious content occurs in about 60% of people with schizophrenia. While this number varies across cultures, this had led to theories about a number of influential religious phenomenon and possible relation to psychotic disorders. A number of prophetic experiences are consistent with psychotic symptoms, although retrospective diagnoses are practically impossible. Schizophrenic episodes are also experienced by people who do not have belief in gods.

Religious content is also common in temporal lobe epilepsy, and obsessive-compulsive disorder. Atheistic content is also found to be common with temporal lobe epilepsy.

Comparative religion is the branch of the study of religions concerned with the systematic comparison of the doctrines and practices of the world's religions. In general, the comparative study of religion yields a deeper understanding of the fundamental philosophical concerns of religion such as ethics, metaphysics, and the nature and form of salvation. Studying such material is meant to give one a richer and more sophisticated understanding of human beliefs and practices regarding the sacred, numinous, spiritual and divine.

In the field of comparative religion, a common geographical classification of the main world religions includes Middle Eastern religions (including Zoroastrianism and Iranian religions), Indian religions, East Asian religions, African religions, American religions, Oceanic religions, and classical Hellenistic religions.

In the 19th and 20th centuries, the academic practice of comparative religion divided religious belief into philosophically defined categories called world religions. Some academics studying the subject have divided religions into three broad categories:

Some recent scholarship has argued that not all types of religion are necessarily separated by mutually exclusive philosophies, and furthermore that the utility of ascribing a practice to a certain philosophy, or even calling a given practice religious, rather than cultural, political, or social in nature, is limited. The current state of psychological study about the nature of religiousness suggests that it is better to refer to religion as a largely invariant phenomenon that should be distinguished from cultural norms (i.e. religions).

Some scholars classify religions as either "universal religions" that seek worldwide acceptance and actively look for new converts, or "ethnic religions" that are identified with a particular ethnic group and do not seek converts. Others reject the distinction, pointing out that all religious practices, whatever their philosophical origin, are ethnic because they come from a particular culture. Christianity, Islam, Buddhism and Jainism are universal religions while Hinduism and Judaism are ethnic religions.

The five largest religious groups by world population, estimated to account for 5.8 billion people and 84% of the population, are Christianity, Islam, Buddhism, Hinduism (with the relative numbers for Buddhism and Hinduism dependent on the extent of syncretism) and traditional folk religion.

A global poll in 2012 surveyed 57 countries and reported that 59% of the world's population identified as religious, 23% as not religious, 13% as convinced atheists, and also a 9% decrease in identification as religious when compared to the 2005 average from 39 countries. A follow-up poll in 2015 found that 63% of the globe identified as religious, 22% as not religious, and 11% as convinced atheists. On average, women are more religious than men. Some people follow multiple religions or multiple religious principles at the same time, regardless of whether or not the religious principles they follow traditionally allow for syncretism.

Abrahamic religions are monotheistic religions which believe they descend from Abraham.

Judaism is the oldest Abrahamic religion, originating in the people of ancient Israel and Judea. The Torah is its foundational text, and is part of the larger text known as the Tanakh or Hebrew Bible. It is supplemented by oral tradition, set down in written form in later texts such as the Midrash and the Talmud. Judaism includes a wide corpus of texts, practices, theological positions, and forms of organization. Within Judaism there are a variety of movements, most of which emerged from Rabbinic Judaism, which holds that God revealed his laws and commandments to Moses on Mount Sinai in the form of both the Written and Oral Torah; historically, this assertion was challenged by various groups. The Jewish people were scattered after the destruction of the Temple in Jerusalem in 70 CE. Today there are about 13 million Jews, about 40 per cent living in Israel and 40 per cent in the United States. The largest Jewish religious movements are Orthodox Judaism (Haredi Judaism and Modern Orthodox Judaism), Conservative Judaism and Reform Judaism.

Christianity is based on the life and teachings of Jesus of Nazareth (1st century) as presented in the New Testament. The Christian faith is essentially faith in Jesus as the Christ, the Son of God, and as Savior and Lord. Almost all Christians believe in the Trinity, which teaches the unity of Father, Son (Jesus Christ), and Holy Spirit as three persons in one Godhead. Most Christians can describe their faith with the Nicene Creed. As the religion of Byzantine Empire in the first millennium and of Western Europe during the time of colonization, Christianity has been propagated throughout the world. The main divisions of Christianity are, according to the number of adherents:

There are also smaller groups, including:

Islam is based on the Qur'an, one of the holy books considered by Muslims to be revealed by God, and on the teachings (hadith) of the Islamic prophet Muhammad, a major political and religious figure of the 7th century CE. Islam is based on the unity of all religious philosophies and accepts all of the Abrahamic prophets of Judaism, Christianity and other Abrahamic religions before Muhammad. It is the most widely practiced religion of Southeast Asia, North Africa, Western Asia, and Central Asia, while Muslim-majority countries also exist in parts of South Asia, Sub-Saharan Africa, and Southeast Europe. There are also several Islamic republics, including Iran, Pakistan, Mauritania, and Afghanistan.

Other denominations of Islam include Nation of Islam, Ibadi, Sufism, Quranism, Mahdavia, and non-denominational Muslims. Wahhabism is the dominant Muslim schools of thought in the Kingdom of Saudi Arabia.

Whilst Judaism, Christianity and Islam are commonly seen as the three Abrahamic faiths, there are smaller and newer traditions which lay claim to the designation as well.

For example, the Bahá'í Faith is a new religious movement that has links to the major Abrahamic religions as well as other religions (e.g. of Eastern philosophy). Founded in 19th-century Iran, it teaches the unity of all religious philosophies and accepts all of the prophets of Judaism, Christianity, and Islam as well as additional prophets (Buddha, Mahavira), including its founder Bahá'u'lláh. It is an offshoot of Bábism. One of its divisions is the Orthodox Bahá'í Faith.

Even smaller regional Abrahamic groups also exist, including Samaritanism (primarily in Israel and the West Bank), the Rastafari movement (primarily in Jamaica), and Druze (primarily in Syria and Lebanon).

East Asian religions (also known as Far Eastern religions or Taoic religions) consist of several religions of East Asia which make use of the concept of Tao (in Chinese) or Dō (in Japanese or Korean). They include:



Indian religions are practiced or were founded in the Indian subcontinent. They are sometimes classified as the "dharmic religions", as they all feature dharma, the specific law of reality and duties expected according to the religion.




Indigenous religions or folk religions refers to a broad category of traditional religions that can be characterised by shamanism, animism and ancestor worship, where traditional means "indigenous, that which is aboriginal or foundational, handed down from generation to generation…". These are religions that are closely associated with a particular group of people, ethnicity or tribe; they often have no formal creeds or sacred texts. Some faiths are syncretic, fusing diverse religious beliefs and practices.

Folk religions are often omitted as a category in surveys even in countries where they are widely practiced, e.g. in China.

African traditional religion encompasses the traditional religious beliefs of people in Africa. In West Africa, these religions include the Akan religion, Dahomey (Fon) mythology, Efik mythology, Odinani, Serer religion (A ƭat Roog), and Yoruba religion, while Bushongo mythology, Mbuti (Pygmy) mythology, Lugbara mythology, Dinka religion, and Lotuko mythology come from central Africa. Southern African traditions include Akamba mythology, Masai mythology, Malagasy mythology, San religion, Lozi mythology, Tumbuka mythology, and Zulu mythology. Bantu mythology is found throughout central, southeast, and southern Africa. In north Africa, these traditions include Berber and ancient Egyptian.

There are also notable African diasporic religions practiced in the Americas, such as Santeria, Candomble, Vodun, Lucumi, Umbanda, and Macumba.
Iranian religions are ancient religions whose roots predate the Islamization of Greater Iran. Nowadays these religions are practiced only by minorities.

Zoroastrianism is based on the teachings of prophet Zoroaster in the 6th century BCE. Zoroastrians worship the creator Ahura Mazda. In Zoroastrianism, good and evil have distinct sources, with evil trying to destroy the creation of Mazda, and good trying to sustain it.

Mandaeism is a monotheistic religion with a strongly dualistic worldview. Mandaeans are sometime labeled as the Last Gnostics.

Kurdish religions include the traditional beliefs of the Yazidi, Alevi, and Ahl-e Haqq. Sometimes these are labeled Yazdânism.


Sociological classifications of religious movements suggest that within any given religious group, a community can resemble various types of structures, including churches, denominations, sects, cults, and institutions.

The study of law and religion is a relatively new field, with several thousand scholars involved in law schools, and academic departments including political science, religion, and history since 1980. Scholars in the field are not only focused on strictly legal issues about religious freedom or non-establishment, but also study religions as they are qualified through judicial discourses or legal understanding of religious phenomena. Exponents look at canon law, natural law, and state law, often in a comparative perspective. Specialists have explored themes in Western history regarding Christianity and justice and mercy, rule and equity, and discipline and love. Common topics of interest include marriage and the family and human rights. Outside of Christianity, scholars have looked at law and religion links in the Muslim Middle East and pagan Rome.

Studies have focused on secularization. In particular, the issue of wearing religious symbols in public, such as headscarves that are banned in French schools, have received scholarly attention in the context of human rights and feminism.

Science acknowledges reason, empiricism, and evidence; and religions include revelation, faith and sacredness whilst also acknowledging philosophical and metaphysical explanations with regard to the study of the universe. Both science and religion are not monolithic, timeless, or static because both are complex social and cultural endeavors that have changed through time across languages and cultures.

The concepts of science and religion are a recent invention: the term religion emerged in the 17th century in the midst of colonization and globalization and the Protestant Reformation. The term science emerged in the 19th century out of natural philosophy in the midst of attempts to narrowly define those who studied nature (natural science), and the phrase religion and science emerged in the 19th century due to the reification of both concepts. It was in the 19th century that the terms Buddhism, Hinduism, Taoism, and Confucianism first emerged. In the ancient and medieval world, the etymological Latin roots of both science ("scientia") and religion ("religio") were understood as inner qualities of the individual or virtues, never as doctrines, practices, or actual sources of knowledge.

In general the scientific method gains knowledge by testing hypotheses to develop theories through elucidation of facts or evaluation by experiments and thus only answers cosmological questions about the universe that can be observed and measured. It develops theories of the world which best fit physically observed evidence. All scientific knowledge is subject to later refinement, or even rejection, in the face of additional evidence. Scientific theories that have an overwhelming preponderance of favorable evidence are often treated as "de facto" verities in general parlance, such as the theories of general relativity and natural selection to explain respectively the mechanisms of gravity and evolution.

Religion does not have a method per se partly because religions emerge through time from diverse cultures and it is an attempt to find meaning in the world, and to explain humanity's place in it and relationship to it and to any posited entities. In terms of Christian theology and ultimate truths, people rely on reason, experience, scripture, and tradition to test and gauge what they experience and what they should believe. Furthermore, religious models, understanding, and metaphors are also revisable, as are scientific models.

Regarding religion and science, Albert Einstein states (1940): "For science can only ascertain what is, but not what should be, and outside of its domain value judgments of all kinds remain necessary. Religion, on the other hand, deals only with evaluations of human thought and action; it cannot justifiably speak of facts and relationships between facts…Now, even though the realms of religion and science in themselves are clearly marked off from each other, nevertheless there exist between the two strong reciprocal relationships and dependencies. Though religion may be that which determine the goals, it has, nevertheless, learned from science, in the broadest sense, what means will contribute to the attainment of the goals it has set up." 

Many religions have value frameworks regarding personal behavior meant to guide adherents in determining between right and wrong. These include the Triple Jems of Jainism, Judaism's Halacha, Islam's Sharia, Catholicism's Canon Law, Buddhism's Eightfold Path, and Zoroastrianism's good thoughts, good words, and good deeds concept, among others.

Religion and morality are not synonymous. While it is "an almost automatic assumption." in Christianity, morality can have a secular basis.

The study of religion and morality can be contentious due to ethnocentric views on morality, failure to distinguish between in group and out group altruism, and inconsistent definitions of religiosity.

Religion has had a significant impact on the political system in many countries. Notably, most Muslim-majority countries adopt various aspects of sharia, the Islamic law. Some countries even define themselves in religious terms, such as The Islamic Republic of Iran. The sharia thus affects up to 23% of the global population, or 1.57 billion people who are Muslims. However, religion also affects political decisions in many western countries. For instance, in the United States, 51% of voters would be less likely to vote for a presidential candidate who did not believe in God, and only 6% more likely. Christians make up 92% of members of the US Congress, compared with 71% of the general public (as of 2014). At the same time, while 23% of U.S. adults are religiously unaffiliated, only one member of Congress (Kyrsten Sinema, D-Arizona), or 0.2% of that body, claims no religious affiliation. In most European countries, however, religion has a much smaller influence on politics although it used to be much more important. For instance, same-sex marriage and abortion were illegal in many European countries until recently, following Christian (usually Catholic) doctrine. Several European leaders are atheists (e.g. France’s former president Francois Hollande or Greece's prime minister Alexis Tsipras). In Asia, the role of religion differs widely between countries. For instance, India is still one of the most religious countries and religion still has a strong impact on politics, given that Hindu nationalists have been targeting minorities like the Muslims and the Christians, who historically belonged to the lower castes. By contrast, countries such as China or Japan are largely secular and thus religion has a much smaller impact on politics.

Secularization is the transformation of the politics of a society from close identification with a particular religion's values and institutions toward nonreligious values and secular institutions. The purpose of this is frequently modernization or protection of the populations religious diversity.

One study has found there is a negative correlation between self-defined religiosity and the wealth of nations. In other words, the richer a nation is, the less likely its inhabitants to call themselves religious, whatever this word means to them (Many people identify themselves as part of a religion (not irreligion) but do not self-identify as religious).

Sociologist and political economist Max Weber has argued that Protestant Christian countries are wealthier because of their Protestant work ethic.

According to a study from 2015, Christians hold the largest amount of wealth (55% of the total world wealth), followed by Muslims (5.8%), Hindus (3.3%) and Jews (1.1%). According to the same study it was found that adherents under the classification Irreligion or other religions hold about 34.8% of the total global wealth.

Mayo Clinic researchers examined the association between religious involvement and spirituality, and physical health, mental health, health-related quality of life, and other health outcomes. The authors reported that: "Most studies have shown that religious involvement and spirituality are associated with better health outcomes, including greater longevity, coping skills, and health-related quality of life (even during terminal illness) and less anxiety, depression, and suicide."

The authors of a subsequent study concluded that the influence of religion on health is largely beneficial, based on a review of related literature. According to academic James W. Jones, several studies have discovered "positive correlations between religious belief and practice and mental and physical health and longevity." 

An analysis of data from the 1998 US General Social Survey, whilst broadly confirming that religious activity was associated with better health and well-being, also suggested that the role of different dimensions of spirituality/religiosity in health is rather more complicated. The results suggested "that it may not be appropriate to generalize findings about the relationship between spirituality/religiosity and health from one form of spirituality/religiosity to another, across denominations, or to assume effects are uniform for men and women.

Critics like Hector Avalos Regina Schwartz, Christopher Hitchens and Richard Dawkins have argued that religions are inherently violent and harmful to society by using violence to promote their goals, in ways that are endorsed and exploited by their leaders.

Anthropologist Jack David Eller asserts that religion is not inherently violent, arguing "religion and violence are clearly compatible, but they are not identical." He asserts that "violence is neither essential to nor exclusive to religion" and that "virtually every form of religious violence has its nonreligious corollary."

Done by some (but not all) religions, animal sacrifice is the ritual killing and offering of an animal to appease or maintain favour with a deity. It has been banned in India.

Greek and Roman pagans, who saw their relations with the gods in political and social terms, scorned the man who constantly trembled with fear at the thought of the gods ("deisidaimonia"), as a slave might fear a cruel and capricious master. The Romans called such fear of the gods "superstitio". Ancient Greek historian Polybius described superstition in Ancient Rome as an "instrumentum regni", an instrument of maintaining the cohesion of the Empire.

Superstition has been described as the non rational establishment of cause and effect. Religion is more complex and is often composed of social institutions and has a moral aspect. Some religions may include superstitions or make use of magical thinking. Adherents of one religion sometimes think of other religions as superstition. Some atheists, deists, and skeptics regard religious belief as superstition.

The Roman Catholic Church considers superstition to be sinful in the sense that it denotes a lack of trust in the divine providence of God and, as such, is a violation of the first of the Ten Commandments. The Catechism of the Catholic Church states that superstition "in some sense represents a perverse excess of religion" (para. #2110). "Superstition," it says, "is a deviation of religious feeling and of the practices this feeling imposes. It can even affect the worship we offer the true God, e.g., when one attributes an importance in some way magical to certain practices otherwise lawful or necessary. To attribute the efficacy of prayers or of sacramental signs to their mere external performance, apart from the interior dispositions that they demand is to fall into superstition. Cf. Matthew 23:16–22" (para. #2111)

The terms atheist (lack of belief in any gods) and agnostic (belief in the unknowability of the existence of gods), though specifically contrary to theistic (e.g. Christian, Jewish, and Muslim) religious teachings, do not by definition mean the opposite of religious. There are religions (including Buddhism, Taoism, and Hinduism), in fact, that classify some of their followers as agnostic, atheistic, or nontheistic. The true opposite of religious is the word irreligious. Irreligion describes an absence of any religion; antireligion describes an active opposition or aversion toward religions in general.

Because religion continues to be recognized in Western thought as a universal impulse, many religious practitioners have aimed to band together in interfaith dialogue, cooperation, and religious peacebuilding. The first major dialogue was the Parliament of the World's Religions at the 1893 Chicago World's Fair, which affirmed universal values and recognition of the diversity of practices among different cultures. The 20th century has been especially fruitful in use of interfaith dialogue as a means of solving ethnic, political, or even religious conflict, with Christian–Jewish reconciliation representing a complete reverse in the attitudes of many Christian communities towards Jews.

Recent interfaith initiatives include A Common Word, launched in 2007 and focused on bringing Muslim and Christian leaders together, the "C1 World Dialogue", the Common Ground initiative between Islam and Buddhism, and a United Nations sponsored "World Interfaith Harmony Week".

Culture and religion have usually been seen as closely related. Paul Tillich looked at religion as the soul of culture and culture as the form or framework of religion. In his own words:
Religion as ultimate concern is the meaning-giving substance of culture, and culture is the totality of forms in which the basic concern of religion expresses itself. In abbreviation: religion is the substance of culture, culture is the form of religion. Such a consideration definitely prevents the establishment of a dualism of religion and culture. Every religious act, not only in organized religion, but also in the most intimate movement of the soul, is culturally formed.
Ernst Troeltsch, similarly, looked at culture as the soil of religion and thought that, therefore, transplanting a religion from its original culture to a foreign culture would actually kill it in the same manner that transplanting a plant from its natural soil to an alien soil would kill it. However, there have been many attempts in the modern pluralistic situation to distinguish culture from religion. Domenic Marbaniang has argued that elements grounded on beliefs of a metaphysical nature (religious) are distinct from elements grounded on nature and the natural (cultural). For instance, language (with its grammar) is a cultural element while sacralization of language in which a particular religious scripture is written is more often a religious practice. The same applies to music and the arts.

Criticism of religion is criticism of the ideas, the truth, or the practice of religion, including its political and social implications.







</doc>
<doc id="167703" url="https://en.wikipedia.org/wiki?curid=167703" title="Empowerment">
Empowerment

Empowerment is a set of measures designed to increase the degree of autonomy and self-determination in people and in communities in order to enable them to represent their interests in a responsible and self-determined way, acting on their own authority. It is the process of becoming stronger and more confident, especially in controlling one's life and claiming one's rights. Empowerment as action refers both to the process of self-empowerment and to professional support of people, which enables them to overcome their sense of powerlessness and lack of influence, and to recognize and use their resources. To do work with power.

As a term, empowerment originates from American community psychology and is associated with the social scientist Julian Rappaport (1981). However, the roots of empowerment theory extend further into history and are linked to Marxist sociological theory. These sociological ideas have continued to be developed and refined through Neo-Marxist Theory (also known as Critical Theory).

In social work, empowerment forms a practical approach of resource-oriented intervention. In the field of citizenship education and democratic education, empowerment is seen as a tool to increase the responsibility of the citizen. Empowerment is a key concept in the discourse on promoting civic engagement. Empowerment as a concept, which is characterized by a move away from a deficit-oriented towards a more strength-oriented perception, can increasingly be found in management concepts, as well as in the areas of continuing education and self-help.

Robert Adams points to the limitations of any single definition of 'empowerment', and the danger that academic or specialist definitions might take away the word and the connected practices from the very people they are supposed to belong to. Still, he offers a minimal definition of the term: 
'Empowerment: the capacity of individuals, groups and/or communities to take control of their circumstances, exercise power and achieve their own goals, and the process by which, individually and collectively, they are able to help themselves and others to maximize the quality of their lives.'

One definition for the term is "an intentional, ongoing process centered in the local community, involving mutual respect, critical reflection, caring, and group participation, through which people lacking an equal share of resources gain greater access to and control over those resources".

Rappaport's (1984) definition includes: "Empowerment is viewed as a process: the mechanism by which people, organizations, and communities gain mastery over their lives."

Sociological empowerment often addresses members of groups that social discrimination processes have excluded from decision-making processes through – for example – discrimination based on disability, race, ethnicity, religion, or gender. Empowerment as a methodology is also associated with feminism.

Empowerment is the process of obtaining basic opportunities for marginalized people, either directly by those people, or through the help of non-marginalized others who share their own access to these opportunities. It also includes actively thwarting attempts to deny those opportunities. Empowerment also includes encouraging, and developing the skills for, self-sufficiency, with a focus on eliminating the future need for charity or welfare in the individuals of the group. This process can be difficult to start and to implement effectively.

One empowerment strategy is to assist marginalized people to create their own nonprofit organization, using the rationale that only the marginalized people, themselves, can know what their own people need most, and that control of the organization by outsiders can actually help to further entrench marginalization. Charitable organizations lead from outside of the community, for example, can disempower the community by entrenching a dependence charity or welfare. A nonprofit organization can target strategies that cause structural changes, reducing the need for ongoing dependence. Red Cross, for example, can focus on improving the health of indigenous people, but does not have authority in its charter to install water-delivery and purification systems, even though the lack of such a system profoundly, directly and negatively impacts health. A nonprofit composed of the indigenous people, however, could ensure their own organization does have such authority and could set their own agendas, make their own plans, seek the needed resources, do as much of the work as they can, and take responsibility – and credit – for the success of their projects (or the consequences, should they fail).

The process of which enables individuals/groups to fully access personal or collective power, authority and influence, and to employ that strength when engaging with other people, institutions or society. In other words, "Empowerment is not giving people power, people already have plenty of power, in the wealth of their knowledge and motivation, to do their jobs magnificently. We define empowerment as letting this power out." It encourages people to gain the skills and knowledge that will allow them to overcome obstacles in life or work environment and ultimately, help them develop within themselves or in the society.

To empower a female "...sounds as though we are dismissing or ignoring males, but the truth is, both genders desperately need to be equally empowered." Empowerment occurs through improvement of conditions, standards, events, and a global perspective of life.

Before there can be the finding that a particular group requires empowerment and that therefore their self-esteem needs to be consolidated on the basis of awareness of their strengths, there needs to be a deficit diagnosis usually carried out by experts assessing the problems of this group. The fundamental asymmetry of the relationship between experts and clients is usually not questioned by empowerment processes. It also needs to be regarded critically, in how far the empowerment approach is really applicable to all patients/clients. It is particularly questionable whether [mentally ill] people in acute crisis situations are in a position to make their own decisions. According to Albert Lenz, people behave primarily regressive in acute crisis situations and tend to leave the responsibility to professionals. It must be assumed, therefore, that the implementation of the empowerment concept requires a minimum level of communication and reflectivity of the persons involved.

In social work, empowerment offers an approach that allows social workers to increase the capacity for self-help of their clients. For example, this allows clients not to be seen as passive, helpless 'victims' to be rescued but instead as a self-empowered person fighting abuse/ oppression; a fight, in which the social worker takes the position of a facilitator, instead of the position of a 'rescuer'.

Marginalized people who lack self-sufficiency become, at a minimum, dependent on charity, or welfare. They lose their self-confidence because they cannot be fully self-supporting. The opportunities denied them also deprive them of the pride of accomplishment which others, who have those opportunities, can develop for themselves. This in turn can lead to psychological, social and even mental health problems. "Marginalized" here refers to the overt or covert trends within societies whereby those perceived as lacking desirable traits or deviating from the group norms tend to be excluded by wider society and ostracized as undesirables.

As a concept, and model of practice, empowerment is also used in health promotion research and practice.
The key principle is for individuals to gain increased control over factors that influence their health status . 

To empower individuals and to obtain more equity in health, it is also important to address health-related behaviors . 

Studies suggest that health promotion interventions aiming at empowering adolescents should enable active learning activities, use visualizing tools to facilitate self-reflection, and allow the adolescents to influence intervention activities .

According to Robert Adams, there is a long tradition in the UK and the USA respectively to advance forms of self-help that have developed and contributed to more recent concepts of empowerment. For example, the free enterprise economic theories of Milton Friedman embraced self-help as a respectable contributor to the economy. Both the Republicans in the US and the Conservative government of Margaret Thatcher built on these theories. 'At the same time, the mutual aid aspects of the concept of self-help retained some currency with socialists and democrats.'

In economic development, the empowerment approach focuses on mobilizing the self-help efforts of the poor, rather than providing them with social welfare. Economic empowerment is also the empowering of previously disadvantaged sections of the population, for example, in many previously colonized African countries.

The World Pensions Council (WPC) has argued that large institutional investors such as pension funds and endowments are exercising a greater influence on the process of adding and replacing corporate directors – as they are themselves steered to do so by their own board members (pension trustees).

This could eventually put more pressure on the CEOs of publicly listed companies, as “more than ever before, many [North American], UK and European Union pension trustees speak enthusiastically about flexing their fiduciary muscles for the UN’s Sustainable Development Goals”, and other ESG-centric investment practices 

Legal empowerment happens when marginalised people or groups use the legal mobilisation i.e., law, legal systems and justice mechanisms to improve or transform their social, political or economic situations. Legal empowerment approaches are interested in understanding how they can use the law to advance interests and priorities of the marginalised.

According to 'Open society foundations' (an NGO) "Legal empowerment is about strengthening the capacity of all people to exercise their rights, either as individuals or as members of a community. Legal empowerment is about grass root justice, about ensuring that law is not confined to books or courtrooms, but rather is available and meaningful to ordinary people.

Lorenzo Cotula in his book ' "Legal Empowerment for Local Resource Control" ' outlines the fact that legal tools for securing local resource rights are enshrined in legal system, does not necessarily mean that local resource users are in position to use them and benefit from them. The state legal system is constrained by a range of different factors – from lack of resources to cultural issues. Among these factors economic, geographic, linguistic and other constraints on access to courts, "lack of legal awareness as well as legal assistance " tend to be recurrent problems.

In many context, marginalised groups do not trust the legal system owing to the widespread manipulation that it has historically been subjected to by the more powerful. 'To what extent one knows the law, and make it work for themselves with 'para legal tools', is legal empowerment; assisted utilizing innovative approaches like legal literacy and awareness training, broadcasting legal information, conducting participatory legal discourses, supporting local resource user in negotiating with other agencies and stake holders and to strategies combining use of legal processes with advocacy along with media engagement, and socio legal mobilisation.

Sometimes groups are marginalized by society at large, with governments participating in the process of marginalization. Equal opportunity laws which actively oppose such marginalization, are supposed to allow empowerment to occur. These laws made it illegal to restrict access to schools and public places based on race. They can also be seen as a symptom of minorities' and women's empowerment through lobbying.

Gender empowerment conventionally refers to the empowerment of women, which is a significant topic of discussion in regards to development and economics nowadays. It also points to approaches regarding other marginalized genders in a particular political or social context. This approach to empowerment is partly informed by feminism and employed legal empowerment by building on international human rights. Empowerment is one of the main procedural concerns when addressing human rights and development. The Human Development and Capabilities Approach, The Millennium Development Goals, and other credible approaches/goals point to empowerment and participation as a necessary step if a country is to overcome the obstacles associated with poverty and development. The UN Sustainable Development Goals targets gender equality and women's empowerment for the global development agenda.

According to Thomas A. Potterfield, many organizational theorists and practitioners regard employee empowerment as one of the most important and popular management concepts of our time.

Ciulla discusses an inverse case: that of bogus empowerment.

In the sphere of management and organizational theory, "empowerment" often refers loosely to processes for giving subordinates (or workers generally) greater discretion and resources: distributing control in order to better serve both customers and the interests of employing organizations.

One account of the history of workplace empowerment in the United States recalls the clash of management styles in railroad construction in the American West in the mid-19th century, where "traditional" hierarchical East-Coast models of control encountered individualistic pioneer workers, strongly supplemented by methods of efficiency-oriented "worker responsibility" brought to the scene by Chinese laborers. In this case, empowerment at the level of work teams or brigades achieved a notable (but short-lived) demonstrated superiority. See the views of Robert L. Webb.

Since the 1980s and 1990s, empowerment has become a point of interest in management concepts and business administration. In this context, empowerment involves approaches that promise greater participation and integration to the employee in order to cope with their tasks as independently as possible and responsibly can. A strength-based approach known as "empowerment circle" has become an instrument of organizational development. Multidisciplinary empowerment teams aim for the development of quality circles to improve the organizational culture, strengthening the motivation and the skills of employees. The target of subjective job satisfaction of employees is pursued through flat hierarchies, participation in decisions, opening of creative effort, a positive, appreciative team culture, self-evaluation, taking responsibility (for results), more self-determination and constant further learning. The optimal use of existing potential and abilities can supposedly be better reached by satisfied and active workers. Here, knowledge management contributes significantly to implement employee participation as a guiding principle, for example through the creation of communities of practice.

However, it is important to ensure that the individual employee has the skills to meet their allocated responsibilities and that the company's structure sets up the right incentives for employees to reward their taking responsibilities. Otherwise there is a danger of being overwhelmed or even becoming lethargic.

Empowerment of employees requires a culture of trust in the organization and an appropriate information and communication system. The aim of these activities is to save control costs, that become redundant when employees act independently and in a self-motivated fashion. 
In the book "Empowerment Takes More Than a Minute", the authors illustrate three keys that organizations can use to open the knowledge, experience, and motivation power that people already have. The three keys that managers must use to empower their employees are:

According to Stewart, in order to guarantee a successful work environment, managers need to exercise the "right kind of authority" (p. 6). To summarize, "empowerment is simply the effective use of a manager’s authority", and subsequently, it is a productive way to maximize all-around work efficiency.

These keys are hard to put into place and it is a journey to achieve empowerment in the workplace. It is important to train employees and makes sure they have trust in what empowerment will bring to a company.

The implementation of the concept of empowerment in management has also been criticised for failing to live up to its claims.

Marshall McLuhan insisted that the development of electronic media would eventually weaken the hierarchical structures that underpin central governments, large corporation, academia and, more generally, rigid, “linear-Cartesian” forms of social organization. 
From that perspective, new, “electronic forms of awareness” driven by information technology would empower citizen, employees and students by disseminating in near-real-time vast amounts of information once reserved to a small number of experts and specialists. Citizens would be bound to ask for substantially more say in the management of government affairs, production, consumption, and education 

World Pensions Council (WPC) economist Nicolas Firzli has argued that rapidly rising cultural tides, notably new forms of online engagement and increased demands for ESG-driven public policies and managerial decisions are transforming the way governments and corporation interact with citizen-consumers in the “Age of Empowerment” 



</doc>
<doc id="50693529" url="https://en.wikipedia.org/wiki?curid=50693529" title="Critical consumerism">
Critical consumerism

Critical consumption is the conscious choice of buying or not buying a specific product according to ethical and political beliefs. The critical consumer recognizes the importance of considering some characteristics of the product and its realization, such as environmental sustainability and respect of workers’ rights. Indeed, critical consumers take full responsibility for the environmental, social and political effects of their choices. The critical consumer can sympathize with certain social movement goals and contributes towards them through modifying their consumption behavior.

Work on critical consumption has differed in the terms used to refer to boycotting and buycotting actions. The more prominent include ethical consumption and political consumerism, while sustainable consumption, more linked with policy, has also increased in usage.

Often consumer and citizen are considered as different because consumers only show self-interest, whereas citizens denote expanded self-interest. The general idea is that, consumers ‘buy what they want—or what they have been persuaded to want—within the limits of what they can get. Citizenship, on the other hand, carries duties or responsibilities along with various rights. Since consumers are seen also as citizens they have to behave in a community-oriented, moral and political way, rather than as a self-interested one.

A specificity of critical consumption is the political use of consumption, which is the consumers’ choice of “"producers and products with the aim of changing ethically or politically objectionable institutional or market practices"”. Their choices depend on different factors as non economic issues that concern personal and family well-being, issue of fairness, justice, ethical or political assessment. Main forms and tools of political use of consumption are boycotting, "buycotting" (anti-boycotting) and also culture jamming or adbusting.

Political consumerism can be considered as an alternative form of political engagement, especially for young generations. In addition, market-based political strategies of young citizens go beyond boycotting and “buycotting”; they also started to participate in internet campaigns becoming active consumers. Their individual choices become political movements able to challenge political and economic powers. Therefore, as a political actor, the consumer “is seen as directly responsible not only for him or herself but also for the world”. The phenomenon of political consumerism takes into account social transformations like globalization, the ever-increasing role of the market and individualization.

Studies from the UK (Harrison et al. 2005, Varul and Wilson-Kovacs 2008, Zaccai 2007), Germany (Baringhorst et al. 2007; Lamla and Neckel 2006), Italy (Forno 2006, Tosi 2006, Sassatelli 2010), France (Chessel and Cochoy 2004, Dubuisson-Queller 2009), North America (Johnston et al. 2011, Johnston and Bauman 2009, Johnston 2008) and Scandinavia (Micheletti et al. 2004) argued that consumes are becoming increasingly politicized according to the boycott and buycott principles. In particular, Scandinavian people seems to be more committed in political consumerism, for example Sweden increased his average of boycotting episodes from 15 percent in 1987 to 29 percent in 1997.

Nevertheless, it is important to consider that even if a growing number of citizens are turning to the market to express their political and moral concerns, it is difficult to assess whether political consumerism can also be considered as a meaningful or effective form of political participation.

The chase for a fair consumption has deep roots in consumption history, starting for example with the American Revolution. Sympathizers of the American cause, in those years, refused to buy English goods, to support colons rebellion. This act of conscious choice can be seen as the beginning of both critical and political consumption. Traces of these two concepts can be found at the turn of the nineteenth century, in the United States, where the National Consumer League promoted the so-called “Whitelists”, in which all the companies that treated fairly their employees were listed.

At the end of the century, also the first forms of political activism in consumption took place in the United States and Europe, like the “Dont Buy Jewish” boycotts. Several organizations were born in those times and in the following centuries, asking the consumers to join the mobilizations as active subjects.

A variety of discourses about the “duty” and the “responsibilities” of social actors arose after the 1999 World Trade Organization protests in Seattle. People were explicitly asked to think that to shop is to vote.

Boycotting and "buycotting" (Anti-boycott), as a particularly self-conscious form of consumption, are expressions of an individual’s political, ethical or environmental stance. Both boycotting and "buycotting" are discrete acts of critical consumption and they are mutually contingent. In fact, if the use-value or utility of a product is important, then it is difficult to view them as separate actions.

Boycotting refers to abstaining from buying, avoiding specific products or brands to punish companies for undesirable policies or business practices. "Buycotting" is a term coined by Friedman (1996); it refers to “positive buying” that aims to foster corporations that represent values – fair trade, environmentalism, sustainable development – that consumers choose to support.

When one boycotts a product or service, it does not mean that he abstains from consuming at all, but that he may select an alternative product or service. Equally, a choice to "buycott" could be understood as including a rejection or boycott of the non-ethical alternative. This interdependence is useful to explain the traditional pairing of boycotting and "buycotting" in much analysis of consumer politics.

One of the rising types of boycotting is the ad hoc one, which underlines the importance of consumers as political subjects. These initiatives show that critical consumption is really impacting in special occasions, gaining much more visibility than everyday boycotts. An example of this type of events is the Buy Nothing Day (BND).

The notion of sustainability has both a temporal dimension demonstrated by the trade-off between present and future generations, and a justice dimension which considers the different distribution of harm and benefit. Under the term sustainability, notions of sustainable resource consumption by recycling, environmental protection, animal welfare, social justice, and climate responsibilities are gathered.

Although the "good" purposes of critical consumerism there are some critics and pitfalls connected to this practice of consumption: 


There are many examples of critical consumerism:





</doc>
<doc id="50951733" url="https://en.wikipedia.org/wiki?curid=50951733" title="Human interactions with microbes">
Human interactions with microbes

Human interactions with microbes include both practical and symbolic uses of microbes, and negative interactions in the form of human, domestic animal, and crop diseases.

Practical use of microbes began in ancient times with fermentation in food processing; bread, beer and wine have been produced by yeasts from the dawn of civilisation, such as in ancient Egypt. More recently, microbes have been used in activities from biological warfare to the production of chemicals by fermentation, as industrial chemists discover how to manufacture a widening variety of organic chemicals including enzymes and bioactive molecules such as hormones and competitive inhibitors for use as medicines. Fermentation is used, too, to produce substitutes for fossil fuels in forms such as ethanol and methane; fuels may also be produced by algae. Anaerobic microorganisms are important in sewage treatment. In scientific research, yeasts and the bacterium "Escherichia coli" serve as model organisms especially in genetics and related fields.

On the symbolic side, an early poem about brewing is the Sumerian "Hymn to Ninkasi", from 1800 BC. In the Middle Ages, Giovanni Boccaccio's "The Decameron" and Geoffrey Chaucer's "The Canterbury Tales": addressed people's fear of deadly contagion and the moral decline that could result. Novelists have exploited the apocalyptic possibilities of pandemics from Mary Shelley's 1826 "The Last Man" and Jack London's 1912 "The Scarlet Plague" onwards. Hilaire Belloc wrote a humorous poem to "The Microbe" in 1912. Dramatic plagues and mass infection have formed the story lines of many Hollywood films, starting with "Nosferatu" in 1922. In 1971, "The Andromeda Strain" told the tale of an extraterrestrial microbe threatening life on Earth. Microbiologists since Alexander Fleming have used coloured or fluorescing colonies of bacteria to create miniature artworks.

Microorganisms such as bacteria and viruses are important as pathogens, causing disease to humans, crop plants, and domestic animals. 

Culture consists of the social behaviour and norms found in human societies and transmitted through social learning. Cultural universals in all human societies include expressive forms like art, music, dance, ritual, religion, and technologies like tool usage, cooking, shelter, and clothing. The concept of material culture covers physical expressions such as technology, architecture and art, whereas immaterial culture includes principles of social organization, mythology, philosophy, literature, and science. This article describes the roles played by microorganisms in human culture.

Since microbes were not known until the Early Modern period, they appear in earlier literature indirectly, through descriptions of baking and brewing. Only with the invention of the microscope, as used by Robert Hooke in his 1665 book "Micrographia", and by Antonie Van Leeuwenhoek in the 1670s, the germ theory of disease, and progress in microbiology in the 19th century were microbes observed directly, identified as living organisms, and put to use on a scientific basis. The same knowledge also allowed microbes to appear explicitly in literature and the arts.

Controlled fermentation with microbes in brewing, wine making, baking, pickling and cultured dairy products such as yogurt and cheese, is used to modify ingredients to make foods with desirable properties. The principal microbes involved are yeasts, in the case of beer, wine, and ordinary bread; and bacteria, in the case of anaerobically fermented vegetables, dairy products, and sourdough bread. The cultures variously provide flavour and aroma, inhibit pathogens, increase digestibility and palatability, make bread rise, reduce cooking time, and create useful products including alcohol, organic acids, vitamins, amino acids, and carbon dioxide. Safety is maintained with the help of food microbiology.

Oxidative sewage treatment processes rely on microorganisms to oxidise organic constituents. Anaerobic microorganisms reduce sludge solids producing methane gas and a sterile mineralised residue. In potable water treatment, one method, the slow sand filter, employs a complex gelatinous layer composed of a wide range of microorganisms to remove both dissolved and particulate material from raw water.

Microorganisms are used in fermentation to produce ethanol, and in biogas reactors to produce methane. Scientists are researching the use of algae to produce liquid fuels, and bacteria to convert various forms of agricultural and urban waste into usable fuels.

Microorganisms are used for many commercial and industrial purposes, including the production of chemicals, enzymes and other bioactive molecules, often through protein engineering. For example, acetic acid is produced by the bacterium "Acetobacter aceti", while citric acid is produced by the fungus "Aspergillus niger". Microorganisms are used to prepare a widening range of bioactive molecules and enzymes. For example, Streptokinase produced by the bacterium "Streptococcus" and modified by genetic engineering is used to remove clots from the blood vessels of patients who have suffered a heart attack. Cyclosporin A is an immunosuppressive agent in organ transplantation, while statins produced by the yeast "Monascus purpureus" serve as blood cholesterol lowering agents, competitively inhibiting the enzyme that synthesizes cholesterol.

Microorganisms are essential tools in biotechnology, biochemistry, genetics, and molecular biology. The yeasts brewer's yeast ("Saccharomyces cerevisiae") and fission yeast ("Schizosaccharomyces pombe") are important model organisms in science, since they are simple eukaryotes that can be grown rapidly in large numbers and are easily manipulated. They are particularly valuable in genetics, genomics and proteomics, for example in protein production. The easily cultured gut bacterium "Escherichia coli", a prokaryote, is similarly widely used as a model organism.

Microbes can form an endosymbiotic relationship with larger organisms. For example, the bacteria that live within the human digestive system contribute to human health through gut immunity, the synthesis of vitamins such as folic acid and biotin, and the fermentation of complex indigestible carbohydrates. Future drugs and food chemicals may need to be tested on the gut microbiota; it is already clear that probiotic supplements can promote health, and that gut microbes are affected by both diet and medicines.

Pathogenic microbes, and toxins that they produce, have been developed as possible agents of warfare. Crude forms of biological warfare have been practiced since antiquity. In the 6th century BC, the Assyrians poisoned enemy wells with a fungus said to render the enemy delirious. In 1346, the bodies of Mongol warriors of the Golden Horde who had died of plague were thrown over the walls of the besieged Crimean city of Kaffa, possibly assisting the spread of the Black Death into Europe.
Advances in bacteriology in the 20th century increased the sophistication of possible bio-agents in war. Biological sabotage—in the form of anthrax and glanders—was undertaken on behalf of the Imperial German government during World War I, with indifferent results. In World War II, Britain weaponised tularemia, anthrax, brucellosis, and botulism toxins, but never used them.
The USA similarly explored biological warfare agents, developing anthrax spores, brucellosis, and botulism toxins for possible military use. Japan developed biological warfare agents, with the use of experiments on human prisoners, and was about to use them when the war ended.

Being very small, and unknown until the invention of the microscope, microbes do not feature directly in art or literature before Early Modern times (though they appear indirectly in works about brewing and baking), when Antonie van Leeuwenhoek observed microbes in water in 1676; his results were soon confirmed by Robert Hooke. A few major diseases such as tuberculosis appear in literature, art, film, opera and music.

The literary possibilities of post-apocalyptic stories about pandemics (worldwide outbreaks of disease) have been explored in novels and films from Mary Shelley's 1826 "The Last Man" and Jack London's 1912 "The Scarlet Plague" onwards. Medieval writings that deal with plague include Giovanni Boccaccio's "The Decameron" and Geoffrey Chaucer's "The Canterbury Tales": both treat the people's fear of contagion and the resulting moral decline, as well as bodily death.

The making of beer has been celebrated in verse since the time of Ancient Sumeria, c. 1800 BC, when the "Hymn to Ninkasi" was inscribed on a clay tablet. Ninkasi, tutelary goddess of beer, and daughter of the creator Enki and the "queen of the sacred lake" Ninki, "handles the dough and with a big shovel, mixing in a pit, the bappir with [date] honey, ... waters the malt set on the ground, ... soaks the malt in a jar, ... spreads the cooked mash on large reed mats, coolness overcomes, ... holds with both hands the great sweet wort, brewing it with honey".

Wine is a frequent topic in English literature, from the spiced French and Italian "ypocras", "claree", and "vernage" in Chaucer's "The Merchant's Tale" onwards. William Shakespeare's Falstaff drank Spanish "sherris sack", in contrast to Sir Toby Belch's preference for "canary". Wine references in later centuries branch out to more winegrowing regions.

"The Microbe" is a humorous 1912 poem by Hilaire Belloc, starting with the lines "The microbe is so very small / You cannot make him out at all,/ But many sanguine people hope / To see him through a microscope. "Microbes and Man" is an admired "classic" book, first published in 1969, by the "father figure of British microbiology" John Postgate on the whole subject of microorganisms and their relationships with humans.

Microbes feature in many highly dramatized films. Hollywood was quick to exploit the possibilities of deadly disease, mass infection and drastic government reaction, starting as early as 1922 with "Nosferatu", in which a Dracula-like figure, Count Orlok, sleeps in unhallowed ground contaminated with the Black Death, which he brings with him wherever he goes. Another classic film, Ingmar Bergman's 1957 "The Seventh Seal", deals with the plague theme very differently, with the grim reaper directly represented by an actor in a hood. More recently, the 1971 "The Andromeda Strain", based on a novel by Michael Crichton, portrayed an extraterrestrial microbe contaminating the Earth.

"A Very Cellular Song," a song from the British psychedelic folk band The Incredible String Band's 1968 album "The Hangman's Beautiful Daughter," is told partially from the point of view of an amoeba, a protistan.

Microbial art is the creation of artworks by culturing bacteria, typically on agar plates, to form desired patterns. These may be chosen to fluoresce under ultraviolet light in different colours. Alexander Fleming, the discoverer of penicillin, created "germ paintings" using different species of bacteria that were naturally pigmented in different colours.

An instance of a protist in an artwork is the artist Louise Bourgeois's bronze sculpture "Amoeba". It has a white patina resembling plaster, and was designed in 1963–5, based on drawings of a pregnant woman's belly that she made as early as the 1940s. According to the Tate Gallery, the work "is a roughly modelled organic form, its bulges and single opening suggesting a moving, living creature in the stages of evolution."

Microorganisms are the causative agents (pathogens) in many infectious diseases of humans and domestic animals. Pathogenic bacteria cause diseases such as plague, tuberculosis and anthrax. Protozoa cause diseases including malaria, sleeping sickness, dysentery and toxoplasmosis. Microscopic fungi cause diseases such as ringworm, candidiasis and histoplasmosis. Pathogenic viruses cause diseases such as influenza, yellow fever and AIDS.

The practice of hygiene was created to prevent infection or food spoiling by eliminating microbes, especially bacteria, from the surroundings.

Microorganisms including bacteria, fungi, and viruses are important as plant pathogens, causing disease to crop plants. Fungi cause serious crop diseases such as maize leaf rust, wheat stem rust, and powdery mildew. Bacteria cause plant diseases including leaf spot and crown galls. Viruses cause plant diseases such as leaf mosaic. The oomycete "Phytophthora infestans" causes potato blight, contributing to the Great Irish Famine of the 1840s.

The tulip breaking virus played a role in the tulip mania of the Dutch Golden Age. The famous Semper Augustus tulip, in particular, owed its striking pattern to infection with the plant disease, a kind of mosaic virus, making it the most expensive of all the tulip bulbs sold.


</doc>
<doc id="40142750" url="https://en.wikipedia.org/wiki?curid=40142750" title="Peng's Coefficient">
Peng's Coefficient

Peng’s Coefficient is an economic term which refers to the proportion of an individual’s spending on culture- and spirit-related products or services, such as books, movie, opera, concert, travelling, training and so forth, to her/his total expenditure. Peng’s Coefficient is inversely proportional to Engel’s Coefficient, because the more proportion people spend on food, the less proportion on culture and spirit.
The concept was named after its creator Peng Bing from China.
Equation: P=S/T


</doc>
<doc id="5903" url="https://en.wikipedia.org/wiki?curid=5903" title="Cultural movement">
Cultural movement

A cultural movement is a change in the way a number of different disciplines approach their work. This embodies all art forms, the sciences, and philosophies. Historically, different nations or regions of the world have gone through their own independent sequence of movements in culture, but as world communications have accelerated this geographical distinction has become less distinct. When cultural movements go through revolutions from one to the next, genres tend to get attacked and mixed up, and often new genres are generated and old ones fade. These changes are often reactions against the prior cultural form, which typically has grown stale and repetitive. An obsession emerges among the mainstream with the new movement, and the old one falls into neglect – sometimes it dies out entirely, but often it chugs along favored in a few disciplines and occasionally making reappearances (sometimes prefixed with "neo-").

There is continual argument over the precise definition of each of these periods, and one historian might group them differently, or choose different names or descriptions. As well, even though in many cases the popular change from one to the next can be swift and sudden, the beginning and end of movements are somewhat subjective, as the movements did not spring fresh into existence out of the blue and did not come to an abrupt end and lose total support, as would be suggested by a date range. Thus use of the term "period" is somewhat deceptive. "Period" also suggests a linearity of development, whereas it has not been uncommon for two or more distinctive cultural approaches to be active at the same time. Historians will be able to find distinctive traces of a cultural movement before its accepted beginning, and there will always be new creations in old forms. So it can be more useful to think in terms of broad "movements" that have rough beginnings and endings. Yet for historical perspective, some rough date ranges will be provided for each to indicate the "height" or accepted time span of the movement.

This current article covers western, notably European and American cultural movements. They have, however, been paralleled by cultural movements in the Orient and elsewhere. In the late 20th and early 21st century in Thailand, for example, there has been a cultural shift away from western social and political values more toward Japanese and Chinese. As well, That culture has reinvigorated monarchical concepts to accommodate state shifts away from western ideology regarding democracy and monarchies. 






</doc>
<doc id="43569192" url="https://en.wikipedia.org/wiki?curid=43569192" title="Aversion to happiness">
Aversion to happiness

Aversion to happiness, also called cherophobia or fear of happiness, is an attitude towards happiness in which individuals may deliberately avoid experiences that invoke positive emotions or happiness.

One of several reasons why cherophobia may develop is the belief that when one becomes happy, a negative event will soon occur that will taint that happiness, as if punishing that individual for satisfaction. This belief is thought to be more prevalent in Eastern cultures. In Western cultures, such as American culture, "it is almost taken for granted that happiness is one of the most important values guiding people's lives". Western cultures are more driven by an urge to maximize happiness and to minimize sadness. Failing to appear happy often gives cause for concern. The value placed on happiness echoes through Western positive psychology and through research on subjective well-being. Fear of happiness is associated with fragility of happiness beliefs, suggesting that one of the causes of aversion to happiness may be the belief that happiness is unstable and fragile. Research shows that fear of happiness is associated with avoidant and anxious attachment styles.

Joshanloo and Weijers identify four reasons for an aversion to happiness: (1)a belief that happiness will cause bad things to happen; (2) that happiness will cause you to become a bad person; (3) that expressing happiness is somehow bad for you and others; and (4) that pursuing happiness is bad for you and others. For example, "some people—in Western and Eastern cultures—are wary of happiness because they believe that bad things, such as unhappiness, suffering, and death, tend to happen to happy people."

These findings "call into question the notion that happiness is the ultimate goal, a belief echoed in any number of articles and self-help publications about whether certain choices are likely to make you happy". Also, "in cultures that believe worldly happiness to be associated with sin, shallowness, and moral decline will actually feel less satisfied when their lives are (by other standards) going well", so measures of personal happiness cannot simply be considered a yardstick for satisfaction with one's life, and attitudes such as aversion to happiness have important implications for measuring happiness across cultures and ranking nations on happiness scores.


</doc>
<doc id="53628878" url="https://en.wikipedia.org/wiki?curid=53628878" title="Cognitive adequacy">
Cognitive adequacy

Cognitive adequacy is a term proposed by Rein Raud as a standard of judging cultural phenomena. According to this method, a cultural phenomenon is cognitively adequate if it provides the means of solving certain problems in a certain socio-cultural context. This is true even when that solution is, according to other criteria, wrong. For example, before the Great Depression in the US many people thought that it is cognitively adequate to think of getting rich quickly through land speculation. All cultural phenomena are replaced by others when they are no longer cognitively adequate. For example, when a community has embraced a new religion, or when science has displaced religion as the primary explanatory discourse for their world.


</doc>
<doc id="54004404" url="https://en.wikipedia.org/wiki?curid=54004404" title="Criminal tradition">
Criminal tradition

Criminal tradition - of the cultural transmission of criminal values. Criminal traditions are transmitted from the older generation to the younger generation, such as social customs are in other forms of society.

Studies of the criminal tradition involved Clifford R. Shaw and Henry D. McKay. They put forward a theory of “cultural transmission”, focuses on the development in some urban neighborhoods of a criminal tradition that persists from one generation to another despite constant changes in population. This theory stresses the value systems of different areas

Also worth noting theory of “differential association,” in which Edwin H. Sutherland described the processes by which criminal values are taken over by the individual. Edwin H. Sutherland asserted that criminal behavior is learned and that it is learned in interaction with others who have already incorporated criminal values.

Research by Shaw and McKay on the concept of cultural transmission indicates that a criminal tradition or subculture does exist in areas of larger cities. According to their studies Criminal tradition arises and is maintained in areas of instability, and the values, norms, and behaviors of the participants in the criminal tradition are viewed as normal by these people.

Traditions are not personified, so adolescents and boys are easier to obey than direct instructions from specific individuals. This is the power of tradition. The norms of the life of groups in which the will of their members manifest themselves take the form of tradition most often. In the tradition that there is no personification, adolescents find it easier to obey them than to a particular person. Against this background, the criminal traditions that are prevalent in youth criminal groups, especially in closed educational and correctional institutions are especially dangerous. In a criminal environment, there are two kinds of traditions:


At the same time, there is a transformation of existing criminal traditions and the emergence of new ones. The reason for this is the changes in the social, economic, legal and other spheres.

On the criminal tradition in different countries of the world, there is a huge amount of work. There is an impressive number of works on the Russian criminal tradition, written in different languages. 
We can also highlight works by Jonny Steinberg on the numbers gangs of South Africa

There is also the view that it is impossible to consider all the traditions of the criminal environment as antisocial and harmful, including, for the reason that some of the traditions in the cells of the remand center, contribute to hygiene and the maintenance of sanitary norms.

In Russian criminal tradition adherents of the criminal (criminal) tradition are characterized by active participation in the life of the "thieves' community"; Living on tangible assets obtained by criminal means; Propaganda of "thieves' customs and traditions, as well as criminal way of life; Compulsion to keep a word not only before the "brother", but also the criminal-criminal world; Organization of collection of "obschekovyh" funds and control over their use; Guardianship and assistance to detainees and convicts, the so-called "vagabonds" and "honest prisoners"; Compliance with the decisions of "gatherings"; Demanding of "brotherhood" and control over their compliance; Organization of counteraction to state bodies.



</doc>
<doc id="9020225" url="https://en.wikipedia.org/wiki?curid=9020225" title="Cultural policy">
Cultural policy

Cultural policy is the government actions, laws and programs that regulate, protect, encourage and financially (or otherwise) support activities related to the arts and creative sectors, such as painting, sculpture, music, dance, literature, and filmmaking, among others and culture, which may involve activities related to language, heritage and diversity. The idea of cultural policy was developed at UNESCO in the 1960s. Generally, this involves governments setting in place processes, legal classifications, regulations, legislation and institutions (e.g., galleries, museums, libraries, opera houses, etc.) which promote and facilitate cultural diversity and creative expressions in a range of art forms and creative activities. Cultural policies vary from one country to another, but generally they aim to improve the accessibility of arts and creative activities to citizens and promote the artistic, musical, ethnic, sociolinguistic, literary and other expressions of all people in a country. In some countries, especially since the 1970s, there is an emphasis on supporting the culture of Indigenous peoples and marginalized communities and ensuring that cultural industries (e.g., filmmaking or TV production) are representative of a country's diverse cultural heritage and ethnic and linguistic demographics. 

Cultural policy can be done at a nation-state level, at a sub-national level (e.g., U.S. states or Canadian provinces), at a regional level or at a municipal level (e.g., a city government creating a museum or arts centre). Examples of cultural policy-making at the nation-state level could include anything from funding music education or theatre programs at little to no cost, to hosting corporate-sponsored art exhibitions in a government museum, to establishing legal codes (such as the U.S. Internal Revenue Service’s 501(c)(3) tax designation for not-for-profit enterprises) and creating political institutions (such as the various ministries of culture and departments of culture and the National Endowment for the Humanities and the National Endowment for the Arts in the United States), arts granting councils, and cultural institutions such as galleries and museums. Similar significant organisations in the United Kingdom include the Department for Culture, Media and Sport (DCMS), and Arts Council England.

Throughout much of the twentieth century, many of the activities that compose cultural policy in the 2010s were governed under the title of "arts policy". Arts policy includes direct funding to artists, creators and art institutions and indirect funding to artists and arts institutions through the tax system (e.g., by making donations to arts charities tax-deductible). However, as Kevin Mulcahy has observed, "cultural policy encompasses a much broader array of activities than were addressed under arts policy. Whereas arts policy was effectively limited to addressing aesthetic concerns (e.g., funding art galleries and opera houses), the significance of the transformation to cultural policy can be observed in its demonstrable emphases on cultural identity, valorization of indigineity [Indigenous people's culture] and analyses of historical dynamics (such as hegemony and colonialism)." A general trend in Western industrialized nations is a shift, since the 1970s and 1980s, away from solely supporting a small number of relatively elite, professionalized art forms and institutions (e.g., Classical music, painting, sculpture, art galleries) to also supporting amateur and community cultural and creative activities (e.g., community theatre) and cultural forms which were not considered part of the Western canon by previous generations (e.g., traditional music such as blues, World music, and so on).

Prior to the twentieth century, the arts were typically supported by the patronage of the church, aristocrats such as kings and queens, and wealthy merchants. During the nineteenth century, artists increased their use of the private marketplace to earn revenue. For example, the composer Beethoven put on public concerts in the 19th century for which admission was charged. During the twentieth century, governments began to take over some of the arts patronage roles. Governments' first efforts to support culture were typically the establishment of archives, museums and libraries. Over the twentieth century, governments established a range of other institutions, such as arts councils and departments of culture. The first departments of culture typically supported the major arts that are part of the Western canon, such as painting and sculpture, and the major performing arts (Classical music and theatre).

In the twentieth century, Western governments in the U.K., Canada, Australia, New Zealand and many European nations developed arts policy measures to promote, support and protect the arts, artists and arts institutions. These governments' arts policy initiatives generally had two aims: supporting excellence in the arts and broadening access to the arts by citizens. An example of an arts policy initiative that supports excellence would be a government grant program which provides funding to the highest-achieving artists in the country. A concrete example would be a literary prize of $100,000 for the best fiction authors from the country, as selected by a panel of top experts. An example of an arts policy initiative that aims at increasing access to the arts would be a music in the schools program funded by the government. A concrete example would be a program which funded an orchestra or jazz quartet and paid them to play free concerts in elementary schools. This would enable children from lower- and middle-income families to hear live music. 

The two goals, supporting excellence and broadening access, are often trade-offs, as any increase in emphasis on one policy objective typically has an adverse effect on the other goal. To give an example, if a hypothetical country has a $12 million per year grant program for orchestras in the country, if the government focuses on the goal of supporting musical excellence, it may decide to provide $4 million per year to the three top orchestras in the country, as determined by a panel of independent professional music critics, conductors and music professors. This decision would strongly support the goal of enhancing excellence, as funding would only go to the top musical groups. However, this approach would only enable citizens in three cities to have access to professional orchestras.

On the other hand, if the government was focusing on broadening access to symphony concerts, it might direct the independent panel to pick 12 orchestras in the country, with the stipulation that only one orchestra per city be selected. By proving $1 million per year to 12 orchestras in 12 cities, this would enable citizens from 12 cities in the country to see live orchestra shows. However, by funding 12 orchestras, this would mean that funding would go to ensembles that do not meet the highest standards of excellence. Thus, excellence and broadening access are often trade-offs.

Cultural policy, while a small part of the budgets of even the most generous of governments, governs a sector of immense complexity. It entails “a large, heterogeneous set of individuals and organizations engaged in the creation, production, presentation, distribution, and preservation of and education about aesthetic heritage, and entertainment activities, products and artifacts”. A cultural policy necessarily encompasses a broad array of activities and typically involves public support for:

Some governments may place policy areas from this list in other ministries or departments. For example, national parks may be assigned to an environment department, or public humanities may be delegated to an education department. 

Since culture is a public good (i.e., contributes a public value to society for which it is hard to exclude non-payers, as all of society benefits from arts and culture) and something that is generally viewed as a merit good, governments have pursued programs to promote greater accessibility. In this way of thinking, significant aesthetic works such as paintings and sculptures should be made broadly available to the public. In other words, “high culture” should not be the exclusive preserve of a particular social class or of a metropolitan location. Rather, the benefits of the highest reaches of cultural excellence should be made in an egalitarian manner; national cultural treasures should be accessible without regard to the impediments of class circumstances, educational attainment or place of habitation. A democratic state cannot be seen as simply indulging the aesthetic preferences of a few, however enlightened, or of overtly infusing art with political values. Consequently, a democratic cultural policy must articulate its purposes in ways that demonstrate how the public interest is being served. These purposes have often been expressed as involving either the creation of cultural democracy or the democratization of culture.

The objective of cultural democratization is the aesthetic enlightenment, enhanced dignity, and educational development of the general citizenry. “Dissemination was the key concept with the aim of establishing equal opportunity for all citizens to participate in publicly organized and financed cultural activities”. To further this goal, performances and exhibitions are low cost; public art education promotes equality of aesthetic opportunity; national institutions tour and perform in work places, retirement homes and housing complexes.

As indicated earlier, the “democratization of culture” is a top-down approach that promulgates certain forms of cultural programming that are deemed to be a public good. Clearly, such an objective is open to criticism for what is termed cultural elitism; that is, the assumption that some aesthetic expressions are inherently superior - at least as determined by a cognoscenti concerned with the acquisition of cultural capital. “The problem with this policy [is] that, fundamentally, it intend[s] to create larger audiences for performances whose content [is] based on the experience of society’s privileged groups. In sum, it has… taken for granted that the cultural needs of all society’s members [are] alike”. The objective of cultural democracy, on the other hand, is to provide for a more participatory (or populist) approach in the definition and provision of cultural opportunities.

The coupling of the concept of democratization of culture to cultural democracy has a pragmatic as well as a philosophical component. Cultural patronage in democratic governments is markedly different from patronage by wealthy individuals or corporations. Private or politically paramount patrons are responsible only to themselves and are free to indulge in their tastes and preferences. Democratic governments, on the other hand, are responsible to the electorate and are held accountable for their policy decisions.

The two objectives just discussed - dissemination of high culture and participation in a broader range of cultural activities - evoke a related debate about the content of public culture: “elitist” or “populist.” 
Proponents of the elitist position argue that cultural policy should emphasize aesthetic quality as the determining criterion for public subvention. This view is typically supported by the major cultural organizations, creative artists in the traditionally defined field of the fine arts, cultural critics, and the well-educated, well-to-do audiences for these art forms. Ronald Dworkin terms this the “lofty approach,” which “insists that art and culture must reach a certain degree of sophistication, richness, and excellence in order for human nature to flourish, and that the state must provide this excellence if the people will not or cannot provide it for themselves”. Advocates of the elitist position generally focus on supporting the creation, preservation and performance of works of the Western canon, a group of artworks that are viewed as the best artistic and cultural products of Western society.

By contrast, the populist position advocates defining culture broadly and inclusively and making this culture broadly available. The populist approach emphasizes a less traditional and more pluralist notion of artistic merit and consciously seeks to create a policy of cultural diversity. With a focus on personal enhancement, the populist’s position posits very limited boundaries between amateur and professional arts activities. Indeed, the goal is to provide opportunities for those outside the professional mainstream. To give an example, whereas an elite approach advocates support for professional musicians, particularly those from Classical music, a populist approach would advocate support for amateur, community singers and musicians.

“Proponents of populism are frequently advocates of minority arts, folk arts, ethnic arts, or counter-cultural activities” as Kevin V. Mulcahy said. Cultural “elitists,” on the other hand, argue in support of excellence over amateurism and favor an emphasis on aesthetic discipline over “culture as everything.” There are “two key tensions for national cultural policy between the goals of excellence versus access, and between government roles as facilitator versus architect”.
Kevin V. Mulcahy argued that in effect, elitism is cultural democracy as populism is to the democratization of culture. Unfortunately, there has been a tendency to see these positions as mutually exclusive, rather than complementary. “Elitists” are denounced as “high brow snobs” advocating an esoteric culture which focuses on art music and the types of art seen in museums and galleries; populists are dismissed as “pandering philistines” promoting a trivialized and commercialized culture, as they endorse the value of popular music and folk art. However, these mutual stereotypes belie complementariness between two bookends of an artistically autonomous and politically accountable cultural policy. There is a synthesis that can be termed a “latitudinarian approach” to public culture; that is, one that is aesthetically inclusive and broadly accessible.

Musicologists David Hebert and Mikolaj Rykowski write that when “music is recognized as invaluable cultural heritage, entailing unique artefacts of intellectual property, new developments in this field then become acknowledged as important forms of social "innovation";” However, they caution policy-makers that with glocalization, the rise of “‘big data’ offers unprecedentedly powerful tools but also inevitably entails many risks for all kinds of artists (both musicians and their collaborators in other arts) as well as the sustainability of traditional cultural practices.”

Such a public-cultural policy would remain faithful to the highest standards of excellence from a broad range of aesthetic expressions while providing the widest possible access to people from different geographic locales, socio-economic strata, and educational background, as Dr. Mulcahy said. In conceiving of public policy as an opportunity to provide alternatives not readily available in the marketplace, public cultural agencies would be better positioned to complement the efforts of the private sector rather than duplicate their activities. Similarly, cultural agencies can promote community development by supporting artistic heritages that are at a competitive disadvantage in a cultural world that is increasingly profit-driven. In sum, excellence should be viewed as the achievements of greatness from a horizontal, rather than a vertical, perspective and a cultural policy as supporting the totality of these varieties of excellence.

These attitudes about a public cultural responsibility stand in marked contrast to much of the rest of the world, where culture is a question of historic patrimony, or the national identities of peoples, whether in independent states or regions within more powerful states. Inevitably, sensitive issues are involved in any discussion of culture as a public policy. However, given the demands in a democratic system that public policies show a return to the taxpayer, cultural policy has frequently argued for support on the basis of utility. It can be argued that there is a parity between the state’s responsibility for its citi’ social-economic-physical needs and their access to culture and opportunities for artistic self-expression. However, the aesthetic dimension of public policy has never been widely perceived as intuitively obvious or politically imperative. Accordingly, the cultural sector has often argued its case from the secondary, ancillary benefits that result from public support for programs that are seemingly only aesthetic in nature. Cultural policy is not typically justified solely on the grounds that it is a good-in-itself, but rather that it yields other good results.

The future of cultural policy would seem to predict an increasingly inexorable demand that the arts “carry their own weight” rather than rely on a public subsidy to pursue “art for art’s sake”. Kevin V. Mulcahy dubbed this “cultural Darwinism” is most pronounced in the United States where public subsidy is limited and publicly supported aesthetic activities are expected to demonstrate a direct public benefit. Non-American cultural institutions are less constrained by the need to maintain diversified revenue streams that demand high levels of earned income and individual and corporate donations to compensate for limited government appropriations.

On the other hand, cultural institutions everywhere are increasingly market-driven in their need for supplementary funds and as a justification for continued public support. The American model of an essentially privatized culture is increasingly attractive to governments seeking to curtail their cultural subsidies. In a system of mixed funding, public culture can nurture the arts groups and cultural activities that contribute to individual self-worth and community definition even if counting for less in the economic bottom-line. At root, a cultural policy is about creating public spheres that are not dependent upon profit motives nor validated by commercial values. As political democracy is dependent upon the existence of civil society and socio-economic pluralism, cultural policy stands as an essential public commitment in realizing these fundamental preconditions.

One of the available and yet underappreciated tools in cultural policy at the national level is the reduction of VAT rates for cultural goods and services. Economic theory can be used to explain how reduced fiscal rates are expected to decrease prices and increase quantities of consumed cultural goods and services. Fiscal policy can be an important part of cultural policy, in particular the VAT rate discounts on cultural consumption, yet it receives less attention than deserved.

At the international level UNESCO is in charge of cultural policy. Contact information for ministries of culture and national arts councils in 160 countries is available from the website of the International Federation of Arts Councils and Culture Agencies (IFACCA). On a local scale, subnational (e.g., state or provincial governments), city and local governments offer citizens and local authorities the opportunity to develop arts and culture with the Agenda 21 for Culture.

Cultural policy research is a field of academic inquiry that grew out of cultural studies in the 1990s. It grew out of the idea that cultural studies should not only be critical, but also try to be useful. Since the 2010s, there are many departments of cultural policy studies around the world. A document that gives an overview over cultural policies worldwide is the Global Report (2018) of the 2005 Convention on the Protection and Promotion of the Diversity of Cultural Expressions. 

Read more about the Global Report: Re|Shaping Cultural Policies





</doc>
<doc id="861492" url="https://en.wikipedia.org/wiki?curid=861492" title="Intercultural communication">
Intercultural communication

Intercultural communication is a discipline that studies communication across different cultures and social groups, or how culture affects communication. It describes the wide range of communication processes and problems that naturally appear within an organization or social context made up of individuals from different religious, social, ethnic, and educational backgrounds. In this sense it seeks to understand how people from different countries and cultures act, communicate and perceive the world around them. 

Many people in intercultural business communication argue that culture determines how individuals encode messages, what medium they choose for transmitting them, and the way messages are interpreted. With regard to intercultural communication proper, it studies situations where people from different cultural backgrounds "interact". Aside from language, intercultural communication focuses on social attributes, thought patterns, and the cultures of different groups of people. It also involves understanding the different cultures, languages and customs of people from other countries. 

Intercultural communication plays a role in social sciences such as anthropology, cultural studies, linguistics, psychology and communication studies. Intercultural communication is also referred to as the base for international businesses. Several cross-cultural service providers assist with the development of intercultural communication skills. Research is a major part of the development of intercultural communication skills. "Intercultural communication" is in a way the 'interaction with speakers of other languages on equal terms and respecting their identities'.

Identity and culture are also studied within the discipline of communication to analyze how globalization influences ways of thinking, beliefs, values, and identity, within and between cultural environments. Intercultural communication scholars approach theory with a dynamic outlook and do not believe culture can be measured nor that cultures share universal attributes. Scholars acknowledge that culture and communication shift along with societal changes and theories should consider the constant shifting and nuances of society.

The study of intercultural communication requires intercultural understanding, which is an ability to understand and value cultural differences. Language is an example of an important cultural component that is linked to intercultural understanding. 

The following types of theories can be distinguished in different strands: focus on effective outcomes, on accommodation or adaption, on identity negotiation and management, on communication networks,on acculturation and adjustment.





A study on cultural and intercultural communication came up with three perspectives, which are the indigenous approach, cultural approach, and cross-cultural approach.




While indigenous and cultural approaches focal point is emics, cross-cultural approaches are etics.


Authentic intercultural communication is possible. A theory that was found in 1984 and revisited on 1987 explains the importance of truth and intention of getting an understanding. Furthermore, if strategic intent is hidden, there can't be any authentic intercultural communication.

In intercultural communication, there could be miscommunication, and the term is called "misfire." Later on, a theory was founded that has three layers of intercultural communication. The first level is effective communication, second-level miscommunication, and third-level systemically distorted communication. It is difficult to go to the first level due to the speaker's position and the structure.

Forced assimilation was very common in the European colonial empires the 18th, 19th, and 20th centuries. Colonial policies regarding religion conversion, the removal of children, the division of community property, and the shifting of gender roles primarily impacted North and South America, Australia, Africa, and Asia. 
Voluntary assimilation has also been a part of history dating back to the Spanish Inquisition of the late 14th and 15th centuries, when many Muslims and Jews voluntarily converted to Roman Catholicism as a response to religious prosecution while secretly continuing their original practices. Another example is when the Europeans moved to the United States.

Intercultural communication is competent when it accomplishes the objectives in a manner that is appropriate to the context and relationship. Intercultural communication thus needs to bridge the dichotomy between appropriateness and effectiveness: Proper means of intercultural communication leads to a 15% decrease in miscommunication.


Competent communication is an interaction that is seen as effective in achieving certain rewarding objectives in a way that is also related to the context in which the situation occurs. In other words, it is a conversation with an achievable goal that is used at an appropriate time/location.

Intercultural communication can be linked with identity, which means the competent communicator is the person who can affirm others' avowed identities. As well as goal attainment is also a focus within intercultural competence and it involves the communicator to convey a sense of communication appropriateness and effectiveness in diverse cultural contexts.

Ethnocentrism plays a role in intercultural communication. The capacity to avoid ethnocentrism is the foundation of intercultural communication competence. Ethnocentrism is the inclination to view one's own group as natural and correct, and all others as aberrant.

People must be aware that to engage and fix intercultural communication there is no easy solution and there is not only one way to do so. Listed below are some of the components of intercultural competence.


The following are ways to improve communication competence:



Effective communication depends on the informal understandings among the parties involved that are based on the trust developed between them. When trust exists, there is implicit understanding within communication, cultural differences may be overlooked, and problems can be dealt with more easily. The meaning of trust and how it is developed and communicated vary across societies. Similarly, some cultures have a greater propensity to be trusting than others.

The problems in intercultural communication usually come from problems in message transmission and in reception. In communication between people of the same culture, the person who receives the message interprets it based on values, beliefs, and expectations for behavior similar to those of the person who sent the message. When this happens, the way the message is interpreted by the receiver is likely to be fairly similar to what the speaker intended. However, when the receiver of the message is a person from a different culture, the receiver uses information from his or her culture to interpret the message. The message that the receiver interprets may be very different from what the speaker intended.

Cross-cultural business communication is very helpful in building cultural intelligence through coaching and training in cross-cultural communication management and facilitation, cross-cultural negotiation, multicultural conflict resolution, customer service, business and organizational communication. Cross-cultural understanding is not just for incoming expats. Cross-cultural understanding begins with those responsible for the project and reaches those delivering the service or content. The ability to communicate, negotiate and effectively work with people from other cultures is vital to international business.

Important points to consider:


There is a connection between a person's personality traits and the ability to adapt to the host-country's environment—including the ability to communicate within that environment.

Two key personality traits are openness and resilience. Openness includes traits such as tolerance for ambiguity, extroversion and introversion, and open-mindedness. Resilience, on the other hand, includes having an internal locus of control, persistence, tolerance for ambiguity, and resourcefulness.

These factors, combined with the person's cultural and racial identity and level of liberalism, comprise that person's potential for adaptation.

In a business environment, communication is vital, and there could be many instances where there could be miscommunication. Globalization is a significant factor in intercultural communication and affects business environments. In a business setting, it could be more difficult to communicate due to different ways of thinking, feeling, and behaving. Due to globalization, more employees have negative emotions in a business environment. The reason why one gets negative feelings is because of miscommunication.

One study done entails the communication between non-native English speaking and native English speaking people in the United States. The study showed that, in a business environment, non-native English speakers and native English speakers had similar experiences in the workplace. Although native English speakers tried to breakdown the miscommunication, non-native English speakers were offended by the terms they used.

There are common conceptualizations of attributes that define collectivistic and individualistic cultures. Operationalizing the perceptions of cultural identities works under the guise that cultures are static and homogeneous, when in fact cultures within nations are multi-ethnic and individuals show high variation in how cultural differences are internalized and expressed.

Globalization plays a central role in theorizing for mass communication, media, and cultural communication studies. Intercultural communication scholars emphasize that globalization emerged from the increasing diversity of cultures throughout the world and thrives with the removal of cultural barriers. The notion of nationality, or the construction of national space, is understood to emerge dialectically through communication and globalization.

The Intercultural Praxis Model by Kathryn Sorrells, PH.D shows us how to navigate through the complexities of cultural differences along with power differences. This model will help you understand who you are as an individual, and how you can better communicate with others that may be different from you. In order to continue living in a globalized society one can use this Praxis model to understand cultural differences (based on race, ethnicity, gender, class, sexual orientation, religion, nationality, etc.) within the institutional and historical systems of power. Intercultural Communication Praxis Model requires us to respond to someone who comes from a different culture than us, in the most open way we can. The media are influential in what we think of other cultures and what we think about our own selves. However it is important, we educate ourselves, and learn how to communicate with others through Sorrells Praxis Model.

Sorrells’ process is made up of six points of entry in navigating intercultural spaces, including inquiry, framing, positioning, dialogue, reflection, and action. Inquiry, as the first step of the Intercultural Praxis Model, is an overall interest in learning about and understanding individuals with different cultural backgrounds and world-views, while challenging one's own perceptions. Framing, then, is the awareness of “local and global contexts that shape intercultural interactions;” thus, the ability to shift between the micro, meso, and macro frames. Positioning is the consideration of one's place in the world compared to others, and how this position might influence both world-views and certain privileges. Dialogue is the turning point of the process during which further understanding of differences and possible tensions develops through experience and engagement with cultures outside of one's own. Next, reflection allows for one to learn through introspection the values of those differences, as well as enables action within the world “in meaningful, effective, and responsible ways." This finally leads to action, which aims to create a more conscious world by working toward social justice and peace among different cultures. As Sorrells argues, “In the context of globalization, [intercultural praxis] … offers us a process of critical, reflective thinking and acting that enables us to navigate … intercultural spaces we inhabit interpersonally, communally, and globally."

Cross-cultural communication endeavours to bring together such relatively unrelated areas as cultural anthropology and established areas of communication. Its core is to establish and understand how people from different cultures communicate with each other. Its charge is to also produce some guidelines with which people from different cultures can better communicate with each other.

Cross-cultural communication, as with many scholarly fields, is a combination of many other fields. These fields include anthropology, cultural studies, psychology and communication. The field has also moved both toward the treatment of interethnic relations, and toward the study of communication strategies used by co-cultural populations, i.e., communication strategies used to deal with majority or mainstream populations.

The study of languages other than one's own can serve not only to help one understand what we as humans have in common, but also to assist in the understanding of the diversity which underlines our languages' methods of constructing and organizing knowledge. Such understanding has profound implications with respect to developing a critical awareness of social relationships. Understanding social relationships and the way other cultures work is the groundwork of successful globalization business affairs.

Language socialization can be broadly defined as “an investigation of how language both presupposes and creates anew, social relations in cultural context”. It is imperative that the speaker understands the grammar of a language, as well as how elements of language are socially situated in order to reach communicative competence. Human experience is culturally relevant, so elements of language are also culturally relevant. One must carefully consider semiotics and the evaluation of sign systems to compare cross-cultural norms of communication. There are several potential problems that come with language socialization, however. Sometimes people can over-generalize or label cultures with stereotypical and subjective characterizations. Another primary concern with documenting alternative cultural norms revolves around the fact that no social actor uses language in ways that perfectly match normative characterizations. A methodology for investigating how an individual uses language and other semiotic activity to create and use new models of conduct and how this varies from the cultural norm should be incorporated into the study of language socialization.

Verbal communication consist of messages being sent and received continuously with the speaker and the listener, it is focused on the way messages are portrayed. Verbal communication is based on language and use of expression, the tone in which the sender of the message relays the communication can determine how the message is received and in what context.

Factors that affect verbal communication:


The way a message is received is dependent on these factors as they give a greater interpretation for the receiver as to what is meant by the message. By emphasizing a certain phrase with the tone of voice, this indicates that it is important and should be focused more on.

Along with these attributes, verbal communication is also accompanied with non-verbal cues. These cues make the message clearer and give the listener an indication of what way the information should be received.

Example of non-verbal cues


In terms of intercultural communication there are language barriers which are effected by verbal forms of communication. In this instance there is opportunity for miscommunication between two or more parties. Other barriers that contribute to miscommunication would be the type of words chosen in conversation. Due to different cultures there are different meaning in vocabulary chosen, this allows for a message between the sender and receiver to be misconstrued.

Nonverbal communication is behavior that communicates without words—though it often may be accompanied by words. Minor variations in body language, speech rhythms, and punctuality often cause differing interpretations of the situation among cross-cultural parties. Kinesic behavior is communication through body movement—e.g., posture, gestures, facial expressions and eye contact. The meaning of such behavior varies across countries. Clothing and the way people dress is used as a form of nonverbal communication.

Object language or material culture refers to how people communicate through material artifacts—e.g., architecture, office design and furniture, clothing, cars, cosmetics, and time. In monochronic cultures, time is experienced linearly and as something to be spent, saved, made up, or wasted. Time orders life, and people tend to concentrate on one thing at a time. In polychronic cultures, people tolerate many things happening simultaneously and emphasize involvement with people. In these cultures, people may be highly distractible, focus on several things at once, and change plans often.

Occulesics are a form of kinesics that includes eye contact and the use of the eyes to convey messages. Proxemics concern the influence of proximity and space on communication (e.g., in terms of personal space and in terms of office layout). For example, space communicates power in the US and Germany.

Paralanguage refers to how something is said, rather than the content of what is said—e.g., rate of speech, tone and inflection of voice, other noises, laughing, yawning, and silence.

Nonverbal communication has been shown to account for between 65% and 93% of interpreted communication. Minor variations in body language, speech rhythms, and punctuality often cause mistrust and misperception of the situation among cross-cultural parties. This is where nonverbal communication can cause problems with intercultural communication. Misunderstandings with nonverbal communication can lead to miscommunication and insults with cultural differences. For example, a handshake in one culture may be recognized as appropriate, whereas another culture may recognize it as rude or inappropriate.




</doc>
<doc id="4543340" url="https://en.wikipedia.org/wiki?curid=4543340" title="Cultural intelligence">
Cultural intelligence

Cultural intelligence or cultural quotient (CQ) is a term used in business, education, government and academic research. Cultural intelligence can be understood as the capability to relate and work effectively across cultures. Originally, the term cultural intelligence and the abbreviation "CQ" was developed by the research done by Christopher Earley (2002) and Earley and Soon Ang (2003). During the same period, researchers David Thomas and Kerr Inkson worked on a complementary framework of CQ as well. A few years later, Ang Soon and Linn Van Dyne Worked on a scale development of the CQ construct as a researched-based way of measuring and predicting intercultural performance.

The term is relatively recent: early definitions and studies of the concepts were given by P. Christopher Earley and Soon Ang in the book "Cultural Intelligence: Individual Interactions Across Cultures" (2003) and more fully developed later by David Livermore in the book, "Leading with Cultural Intelligence". The concept is related to that of cross-cultural competence. but goes beyond that to actually look at intercultural capabilities as a form of intelligence that can be measured and developed. According to Earley, Ang, and Van Dyne, cultural intelligence can be defined as "a person's capability to adapt as s/he interacts with others from different cultural
regions", and has behavioral, motivational, and metacognitive aspects. Without cultural intelligence, both business and military actors seeking to engage foreigners are susceptible to mirror imaging.

Cultural intelligence or CQ is measured on a scale, similar to that used to measure an individual's intelligence quotient. People with higher CQs are regarded as better able to successfully blend into any environment, using more effective business practices, than those with a lower CQ. CQ is assessed using the academically validated assessment created by Linn Van Dyne and Soon Ang. Both self-assessments and multi-rater assessments are available through the Cultural Intelligence Center in East Lansing, Michigan and the Center makes the CQ Scale available to other academic researchers at no charge. Research demonstrates that CQ is a consistent predictor of performance in multicultural settings. Cultural intelligence research has been cited and peer-reviewed in more than seventy academic journals. The research and application of cultural intelligence is being driven by the Cultural Intelligence Center in the U.S. and Nanyang Business School in Singapore. Additional research and application of cultural intelligence has been conducted by Liliana Gil Valletta, who holds the trademark for the term since 2013. Defined as the ability to be aware of, understand and apply cultural competence into everyday business decisions, Gil Valletta has expanded the definition of cultural intelligence into a capability that yields a commercial advantage by turning cultural trends into profits and P&L impact. Since 2010, the firm CIEN+ and data science platform Culturintel is the first using artificial intelligence and big data tools to report measures of cultural intelligence and enable corporations to embed inclusion for business growth.

Ang, Van Dyne, & Livermore describe four CQ capabilities: motivation (CQ Drive), cognition (CQ Knowledge), meta-cognition (CQ Strategy) and behavior (CQ Action). CQ Assessments report scores on all four capabilities as well as several sub-dimensions for each capability. The four capabilities stem from the intelligence-based approach to intercultural adjustment and performance.

CQ-Drive is a person's interest and confidence in functioning effectively in culturally diverse settings. It includes:

CQ-Knowledge is a person's knowledge about how cultures are similar and how cultures are different. It includes:

CQ-Strategy is how a person makes sense of culturally diverse experiences. It occurs when people make judgments about their own thought processes and those of others. It includes:

CQ-Action is a person's capability to adapt verbal and nonverbal behavior to make it appropriate to diverse cultures. It involves having a flexible repertoire of behavioral responses that suit a variety of situations. It includes:

Additional research on cultural intelligence is being conducted by academics around the globe, including research on culturally intelligent organizations, the correlation between neuroscience and the development of cultural intelligence, and situational judgment making and CQ Assessment.

Cultural intelligence, also known within business as "cultural quotient" or "CQ", is a theory within management and organisational psychology, positing that understanding the impact of an individual's cultural background on their behaviour is essential for effective business, and measuring an individual's ability to engage successfully in any environment or social setting.

Elaine Mosakowski and her husband Christopher Earley in the October 2004 issue of "Harvard Business Review" described cultural intelligence. CQ has been gaining acceptance throughout the business community. CQ teaches strategies to improve cultural perception in order to distinguish behaviours driven by culture from those specific to an individual, suggesting that allowing knowledge and appreciation of the difference to guide responses results in better business practice.

Since 2010 and as presented in academia, national television and other industry forums, Liliana Gil Valletta and the firm CIEN+ have expanded the definition and application of cultural intelligence from the individual to the organizational construct and architecture. Their model allows corporations and business teams to assess their level of cultural intelligence excellence index (Cix) based on how well they integrate cross-cultural analytics, insights, metrics, rewards, senior support, R&D and profit plans to make inclusion the default. As defined by Gil Valletta, traditional CQ focuses on achieving individual competence while Cix focuses on achieving commercial growth.

CQ is developed through:

Ilan Alon, Michele Boulange, Judith Meyer, and Vasyl Taras have developed a new survey they call the BCIQ (Business Cultural Intelligence Quotient). While not rooted in the academic literature of multiple loci of intelligence, the survey provides practitioners with a tool to reflect on their understanding for use in an international management context

The only peer reviewed measurement of CQ is the multi-rater assessment developed by Soon Ang and Linn Van Dyne.

Cultural intelligence refers to the cognitive, motivational, and behavioral capacities to understand and effectively respond to the beliefs, values, attitudes, and behaviors of individuals and groups under complex and changing circumstances in order to effect a desired change. The application and integration of cultural intelligence into the workings and practices of local government is advanced by community planner, Anindita Mitra in 2016 as a way to improve the effectiveness of local governments to respond to and serve a growing and diverse population.

Cultural knowledge and warfare are bound together as cultural intelligence is central to ensuring successful military operations. Culture is composed of factors including language, society, economy, customs, history, and religion. For military operations, cultural intelligence concerns the ability to make decisions based an understanding of these factors.

In the military sense, cultural intelligence is a complicated pursuit of anthropology, psychology, communications, sociology, history, and above all, military doctrine.

Diplomacy is the conduct by governmental officials of negotiations and other relations between nations. The use of cultural intelligence and other methods of soft power have been endorsed and encouraged as a primary tool of statecraft as opposed to more coercive forms of national power; its further development is being stressed as a primary exercise of power as opposed to the expensive (politically and financial) coercive options such as military action or economic sanctions. For example, in 2007, US Secretary of Defense Robert Gates called for "strengthening our capacity to use 'soft' power and for better integrating it with 'hard' power," stating that using these other instruments "could make it less likely that military force will have to be used in the first place, as local problems might be dealt with before they become crises." In a speech in 2006, Secretary of State Condoleezza Rice urged similar actions in support of her doctrine of "transformational diplomacy;" she made a similar speech, again, in 2008.

Governmental negotiation and other diplomatic efforts can be made much more effective if knowledge of a peoples is understood and practiced with skill. Joseph Nye, a leading political scientist, asserts in his book "Soft Power" that "a country may obtain the outcomes it wants in world politics because other countries – admiring its values, emulating its example, aspiring to its level of prosperity and openness – want to follow it. In this sense, it is also important to set the agenda and attract others in world politics, and not only to force them to change by threatening military force or economic sanctions. This soft power – getting others to want the outcomes that you want – co-opts people rather than coerces them."

The sorts of effects Nye describes are much more effective if there is a willingness on the part of the influencing agent to respect and understand the other agent's cultural background. An example of diplomacy was a provision within the USA PATRIOT Act "condemning discrimination against Arab and Muslim Americans" response to the events of 9/11. This provision ensures the protection of U.S. Muslims and Arabs, ensures a distinction between them and those that committed those terrorist acts, and lives up to the ideals of the U.S. constitution of non-discrimination. This precedent sets up an attitude of an awareness of and respect for peaceful, law-abiding Muslims.

However, cultural intelligence can be used to the opposite effect. In 2006 and 2007, Russian president Vladimir Putin used his knowledge of German chancellor Angela Merkel and her fear of dogs to intimidate her during negotiations by bringing his Labrador Retriever, Koni.

Cultural Intelligence as a U.S. military term did not gain prominence until the late 20th century with the rise of low-intensity and counterinsurgency warfare. However, the importance of cultural intelligence has only recently become commonly accepted with the counterinsurgency campaigns the U.S. has conducted in Afghanistan and Iraq.

Since the Iraq War and the War in Afghanistan, cultural intelligence is being seen as playing a more important role in the success of military operations in counterinsurgency. The U.S. Army and Marine Corps Counterinsurgency Field manual is explicit on this point:

"Cultural knowledge is essential to waging a successful counterinsurgency," and goes further, urging "counterinsurgents… should strive to avoid imposing their ideals of normalcy on a foreign cultural problem."

The manual's logic is that the "primary goal of any COIN operation is to foster development of effective governance by legitimate government." And the manual points out that different cultures have different ideas of what legitimacy entails, and that operations at building legitimacy need to meet the host nation's peoples' criteria. Failure to recognize and respect a host nation's culture has resulted in the deaths of some NATO troops, and attempts have been made to make Afghans aware of Western culture and vice versa to mitigate some of these unintentional effects. But the cultural attitudes of the host nation's peoples aren't the only consideration. The culture of the insurgents is crucial as well – for that information helps to develop "effective programs that attack the insurgency's root causes." In this way, this information helps to shape counterinsurgent military operations.

To this effect, the U.S. Army developed the Human Terrain System in February 2007 to provide cultural information of host nations. The HTS program was the primary unified effort to provide this information to supplement military operations in areas where armed services were deployed. The program was also controversial, with the American Anthropological Association arguing that such efforts represented a conflict of interest and a possible violation of the ethical standards of anthropologists; but it was defended by others as ethical. The U.S. Army Human Terrain System ended operations in September 2014.





</doc>
<doc id="53169305" url="https://en.wikipedia.org/wiki?curid=53169305" title="Assessment culture">
Assessment culture

Assessment culture is a subset of organizational culture defined by the values, beliefs, and assumptions held by its members. In higher education, a positive assessment culture is characterized by trusting relationships, data-informed decision-making, a respect for the profession of teaching, and an internally-driven thirst for discovery about student learning. Positive assessment culture generally connotes the existence of conditions for collaboration among practitioners, reward structures, professional development opportunities for faculty and staff, student involvement, and a shared commitment among leaders to making institutional improvements that are sustainable.

Assessment culture may be revealed behaviorally through factors such as: celebration of successes, comprehensive program review, shared use of common terminology and language, provision of technical support, and use of affirmative messaging to effectively convey meaning. The culture of assessment has been measured by scholars of perceptions among faculty to determine motivations, sense of support, and levels of fear related to assessment.


</doc>
<doc id="7745490" url="https://en.wikipedia.org/wiki?curid=7745490" title="Cultural universal">
Cultural universal

A cultural universal (also called an anthropological universal or human universal), as discussed by Emile Durkheim, George Murdock, Claude Lévi-Strauss, Donald Brown and others, is an element, pattern, trait, or institution that is common to all human cultures worldwide. Taken together, the whole body of cultural universals is known as the human condition. Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations. Some anthropological and sociological theorists that take a cultural relativist perspective may deny the existence of cultural universals: the extent to which these universals are "cultural" in the narrow sense, or in fact biologically inherited behavior is an issue of "nature versus nurture".

In his book "Human Universals" (1991), Donald Brown defines human universals as comprising "those features of culture, society, language, behavior, and psyche for which there are no known exception", providing a list of hundreds of items he suggests as universal. Among the cultural universals listed by Donald Brown are:

The observation of the same or similar behavior in different cultures does not prove that they are the results of a common underlying psychological mechanism. One possibility is that they may have been invented independently due to a common practical problem.

Since any cultures that have been studied by anthropologists have had contact with at least the anthropologists that studied it, and anthropological research ethics slows the studies down so that other groups unbound by such ethics, often at least locally represented by people of the same skin color as the supposedly isolated tribe but significantly culturally globalized, reach the tribe before the anthropologists do, no truly uncontacted culture has ever been scientifically studied. This allows outside influence to be an explanation for cultural universals as well. This does not preclude multiple independent inventions of civilization and is therefore not the same thing as hyperdiffusionism, it merely means that cultural universals are not proof of innateness.




</doc>
<doc id="13144407" url="https://en.wikipedia.org/wiki?curid=13144407" title="Cultural practice">
Cultural practice

Cultural practice is the manifestation of a culture or sub-culture, especially in regard to the traditional and customary practices of a particular ethnic or other cultural group. 

The term is gaining in importance due to the increased controversy over "rights of cultural practice", which are protected in many jurisdictions for indigenous peoples and sometimes ethnic minorities. It is also a major component of the field of cultural studies, and is a primary focus of international works such as the United Nations declaration of the rights of indigenous Peoples.

Cultural practice is also a subject of discussion in questions of cultural survival. If an ethnic group retains its formal ethnic identity but loses its core cultural practices or the knowledge, resources, or ability to continue them, questions arise as to whether the culture is able to actually survive at all. International bodies such as the United Nations Permanent Forum on Indigenous Issues continually work on these issues, which are increasingly at the forefront of globalization questions.


The real question of what qualifies as a legitimate cultural practice is the subject of much legal and ethnic community debate. The question arises in controversial subject areas such as genital mutilation, indigenous hunting and gathering practices, and the question of licensing of traditional medical practitioners.

Many traditional cultures acknowledge members outside of their ethnicity as cultural practitioners, but only under special circumstances. Generally, the knowledge or title must be passed in a traditional way, such as family knowledge shared through adoption, or through a master of that practice choosing a particular student who shows qualities desired for that practice, and teaching that student in a hands-on manner, in which they are able to absorb the core values and belief systems of the culture. The degree to which these non-ethnic practitioners are able to exercise "customary and traditional" rights, and the degree to which their practice is acknowledged as valid, is often a subject of considerable debate among indigenous and other ethnic communities, and sometimes with the legal systems under which these communities function. The difference between bona fide non-native cultural practitioners and cultural piracy, or cultural appropriation, is a major issue within the study of globalization and modernization.

The evolution of traditional cultures is a subject of much discussion in legal, scholarly, and community forums. It is generally accepted that all cultures are to some degree in a continual state of sociocultural evolution. However, major questions surround the legitimacy of newly evolved cultural expressions, especially when these are influenced by modernization or by the influence of other cultures. Also, there is significant debate surrounding the source of evolution: for example, an indigenous community may accept the use of store-bought materials in the creation of traditional arts, but may reject requirements to apply for a permit for certain gathering purposes; the central difference being that one is an "internal" cultural evolution, while the other is "externally" driven by the society or legal body that surrounds the culture.


</doc>
<doc id="19159508" url="https://en.wikipedia.org/wiki?curid=19159508" title="Culture">
Culture

Culture () is an umbrella term which encompasses the social behavior and norms found in human societies, as well as the knowledge, beliefs, arts, laws, customs, capabilities and habits of the individuals in these groups.

Humans acquire culture through the learning processes of enculturation and socialization, which is shown by the diversity of cultures across societies.

A cultural norm codifies acceptable conduct in society; it serves as a guideline for behavior, dress, language, and demeanor in a situation, which serves as a template for expectations in a social group.
Accepting only a monoculture in a social group can bear risks, just as a single species can wither in the face of environmental change, for lack of functional responses to the change. 
Thus in military culture, valor is counted a typical behavior for an individual, as are duty, honor, and loyalty to the social group are counted as virtues or functional responses in the continuum of conflict. In the practice of religion, analogous attributes can be identified in a social group.

Culture is considered a central concept in anthropology, encompassing the range of phenomena that are transmitted through social learning in human societies. Cultural universals are found in all human societies; these include expressive forms like art, music, dance, ritual, religion, and technologies like tool usage, cooking, shelter, and clothing. The concept of material culture covers the physical expressions of culture, such as technology, architecture and art, whereas the immaterial aspects of culture such as principles of social organization (including practices of political organization and social institutions), mythology, philosophy, literature (both written and oral), and science comprise the intangible cultural heritage of a society.

In the humanities, one sense of culture as an attribute of the individual has been the degree to which they have cultivated a particular level of sophistication in the arts, sciences, education, or manners. The level of cultural sophistication has also sometimes been used to distinguish civilizations from less complex societies. Such hierarchical perspectives on culture are also found in class-based distinctions between a high culture of the social elite and a low culture, popular culture, or folk culture of the lower classes, distinguished by the stratified access to cultural capital. In common parlance, culture is often used to refer specifically to the symbolic markers used by ethnic groups to distinguish themselves visibly from each other such as body modification, clothing or jewelry. Mass culture refers to the mass-produced and mass mediated forms of consumer culture that emerged in the 20th century. Some schools of philosophy, such as Marxism and critical theory, have argued that culture is often used politically as a tool of the elites to manipulate the lower classes and create a false consciousness. Such perspectives are common in the discipline of cultural studies. In the wider social sciences, the theoretical perspective of cultural materialism holds that human symbolic culture arises from the material conditions of human life, as humans create the conditions for physical survival, and that the basis of culture is found in evolved biological dispositions.

When used as a count noun, a "culture" is the set of customs, traditions, and values of a society or community, such as an ethnic group or nation. Culture is the set of knowledge acquired over time. In this sense, multiculturalism values the peaceful coexistence and mutual respect between different cultures inhabiting the same planet. Sometimes "culture" is also used to describe specific practices within a subgroup of a society, a subculture (e.g. "bro culture"), or a counterculture. Within cultural anthropology, the ideology and analytical stance of cultural relativism hold that cultures cannot easily be objectively ranked or evaluated because any evaluation is necessarily situated within the value system of a given culture.

The modern term "culture" is based on a term used by the Ancient Roman orator Cicero in his "Tusculanae Disputationes", where he wrote of a cultivation of the soul or "cultura animi," using an agricultural metaphor for the development of a philosophical soul, understood teleologically as the highest possible ideal for human development. Samuel Pufendorf took over this metaphor in a modern context, meaning something similar, but no longer assuming that philosophy was man's natural perfection. His use, and that of many writers after him, "refers to all the ways in which human beings overcome their original barbarism, and through artifice, become fully human."

In 1986, philosopher Edward S. Casey wrote, "The very word "culture" meant 'place tilled' in Middle English, and the same word goes back to Latin "colere", 'to inhabit, care for, till, worship' and "cultus", 'A cult, especially a religious one.' To be cultural, to have a culture, is to inhabit a place sufficiently intensive to cultivate it—to be responsible for it, to respond to it, to attend to it caringly."

Culture described by Richard Velkley: ... originally meant the cultivation of the soul or mind, acquires most of its later modern meaning in the writings of the 18th-century German thinkers, who were on various levels developing Rousseau's criticism of "modern liberalism and Enlightenment." Thus a contrast between "culture" and "civilization" is usually implied in these authors, even when not expressed as such.

In the words of anthropologist E.B. Tylor, it is "that complex whole which includes knowledge, belief, art, morals, law, custom and any other capabilities and habits acquired by man as a member of society." Alternatively, in a contemporary variant, "Culture is defined as a social domain that emphasizes the practices, discourses and material expressions, which, over time, express the continuities and discontinuities of social meaning of a life held in common.

The "Cambridge English Dictionary" states that culture is "the way of life, especially the general customs and beliefs, of a particular group of people at a particular time." Terror management theory posits that culture is a series of activities and worldviews that provide humans with the basis for perceiving themselves as "person[s] of worth within the world of meaning"—raising themselves above the merely physical aspects of existence, in order to deny the animal insignificance and death that "Homo sapiens" became aware of when they acquired a larger brain.

The word is used in a general sense as the evolved ability to categorize and represent experiences with symbols and to act imaginatively and creatively. This ability arose with the evolution of behavioral modernity in humans around 50,000 years ago and is often thought to be unique to humans. However, some other species have demonstrated similar, though much less complicated, abilities for social learning. It is also used to denote the complex networks of practices and accumulated knowledge and ideas that are transmitted through social interaction and exist in specific human groups, or cultures, using the plural form.

It has been estimated from archaeological data that the human capacity for cumulative culture emerged somewhere between 500,000–170,000 years ago.

Raimon Panikkar identified 29 ways in which cultural change can be brought about, including growth, development, evolution, involution, renovation, reconception, reform, innovation, revivalism, revolution, mutation, progress, diffusion, osmosis, borrowing, eclecticism, syncretism, modernization, indigenization, and transformation. In this context, modernization could be viewed as adoption of Enlightenment era beliefs and practices, such as science, rationalism, industry, commerce, democracy, and the notion of progress. Rein Raud, building on the work of Umberto Eco, Pierre Bourdieu and Jeffrey C. Alexander, has proposed a model of cultural change based on claims and bids, which are judged by their cognitive adequacy and endorsed or not endorsed by the symbolic authority of the cultural community in question.

Cultural invention has come to mean any innovation that is new and found to be useful to a group of people and expressed in their behavior but which does not exist as a physical object. Humanity is in a global "accelerating culture change period," driven by the expansion of international commerce, the mass media, and above all, the human population explosion, among other factors. Culture repositioning means the reconstruction of the cultural concept of a society.
Cultures are internally affected by both forces encouraging change and forces resisting change. These forces are related to both social structures and natural events, and are involved in the perpetuation of cultural ideas and practices within current structures, which themselves are subject to change. (See structuration.)

Social conflict and the development of technologies can produce changes within a society by altering social dynamics and promoting new cultural models, and spurring or enabling generative action. These social shifts may accompany ideological shifts and other types of cultural change. For example, the U.S. feminist movement involved new practices that produced a shift in gender relations, altering both gender and economic structures. Environmental conditions may also enter as factors. For example, after tropical forests returned at the end of the last ice age, plants suitable for domestication were available, leading to the invention of agriculture, which in turn brought about many cultural innovations and shifts in social dynamics.

Cultures are externally affected via contact between societies, which may also produce—or inhibit—social shifts and changes in cultural practices. War or competition over resources may impact technological development or social dynamics. Additionally, cultural ideas may transfer from one society to another, through diffusion or acculturation. In diffusion, the form of something (though not necessarily it's meaning) moves from one culture to another. For example, Western restaurant chains and culinary brands sparked curiosity and fascination to the Chinese as China opened its economy to international trade in the late 20th-century. "Stimulus diffusion" (the sharing of ideas) refers to an element of one culture leading to an invention or propagation in another. "Direct borrowing," on the other hand, tends to refer to technological or tangible diffusion from one culture to another. Diffusion of innovations theory presents a research-based model of why and when individuals and cultures adopt new ideas, practices, and products.

acculturation has different meanings. Still, in this context, it refers to the replacement of traits of one culture with another, such as what happened to certain Native American tribes and many indigenous peoples across the globe during the process of colonization. Related processes on an individual level include assimilation (adoption of a different culture by an individual) and transculturation. The transnational flow of culture has played a major role in merging different cultures and sharing thoughts, ideas, and beliefs.

Immanuel Kant (1724–1804) formulated an individualist definition of "enlightenment" similar to the concept of "bildung": "Enlightenment is man's emergence from his self-incurred immaturity." He argued that this immaturity comes not from a lack of understanding, but from a lack of courage to think independently. Against this intellectual cowardice, Kant urged: "Sapere Aude", "Dare to be wise!" In reaction to Kant, German scholars such as Johann Gottfried Herder (1744–1803) argued that human creativity, which necessarily takes unpredictable and highly diverse forms, is as important as human rationality. Moreover, Herder proposed a collective form of "Bildung": "For Herder, Bildung was the totality of experiences that provide a coherent identity, and sense of common destiny, to a people."
In 1795, the Prussian linguist and philosopher Wilhelm von Humboldt (1767–1835) called for an anthropology that would synthesize Kant's and Herder's interests. During the Romantic era, scholars in Germany, especially those concerned with nationalist movements—such as the nationalist struggle to create a "Germany" out of diverse principalities, and the nationalist struggles by ethnic minorities against the Austro-Hungarian Empire—developed a more inclusive notion of culture as "worldview" ("Weltanschauung"). According to this school of thought, each ethnic group has a distinct worldview that is incommensurable with the worldviews of other groups. Although more inclusive than earlier views, this approach to culture still allowed for distinctions between "civilized" and "primitive" or "tribal" cultures.

In 1860, Adolf Bastian (1826–1905) argued for "the psychic unity of mankind." He proposed that a scientific comparison of all human societies would reveal that distinct worldviews consisted of the same basic elements. According to Bastian, all human societies share a set of "elementary ideas" ("Elementargedanken"); different cultures, or different "folk ideas" ("Völkergedanken"), are local modifications of the elementary ideas. This view paved the way for the modern understanding of culture. Franz Boas (1858–1942) was trained in this tradition, and he brought it with him when he left Germany for the United States.

In the 19th century, humanists such as English poet and essayist Matthew Arnold (1822–1888) used the word "culture" to refer to an ideal of individual human refinement, of "the best that has been thought and said in the world." This concept of culture is also comparable to the German concept of "bildung": "...culture being a pursuit of our total perfection by means of getting to know, on all the matters which most concern us, the best which has been thought and said in the world."

In practice, "culture" referred to an elite ideal and was associated with such activities as art, classical music, and haute cuisine. As these forms were associated with urban life, "culture" was identified with "civilization" (from lat. "civitas", city). Another facet of the Romantic movement was an interest in folklore, which led to identifying a "culture" among non-elites. This distinction is often characterized as that between high culture, namely that of the ruling social group, and low culture. In other words, the idea of "culture" that developed in Europe during the 18th and early 19th centuries reflected inequalities within European societies.

Matthew Arnold contrasted "culture" with anarchy; other Europeans, following philosophers Thomas Hobbes and Jean-Jacques Rousseau, contrasted "culture" with "the state of nature." According to Hobbes and Rousseau, the Native Americans who were being conquered by Europeans from the 16th centuries on were living in a state of nature; this opposition was expressed through the contrast between "civilized" and "uncivilized." According to this way of thinking, one could classify some countries and nations as more civilized than others and some people as more cultured than others. This contrast led to Herbert Spencer's theory of Social Darwinism and Lewis Henry Morgan's theory of cultural evolution. Just as some critics have argued that the distinction between high and low cultures is an expression of the conflict between European elites and non-elites, other critics have argued that the distinction between civilized and uncivilized people is an expression of the conflict between European colonial powers and their colonial subjects.

Other 19th-century critics, following Rousseau, have accepted this differentiation between higher and lower culture, but have seen the refinement and sophistication of high culture as corrupting and unnatural developments that obscure and distort people's essential nature. These critics considered folk music (as produced by "the folk," i.e., rural, illiterate, peasants) to honestly express a natural way of life, while classical music seemed superficial and decadent. Equally, this view often portrayed indigenous peoples as "noble savages" living authentic and unblemished lives, uncomplicated and uncorrupted by the highly stratified capitalist systems of the West.

In 1870 the anthropologist Edward Tylor (1832–1917) applied these ideas of higher versus lower culture to propose a theory of the evolution of religion. According to this theory, religion evolves from more polytheistic to more monotheistic forms. In the process, he redefined culture as a diverse set of activities characteristic of all human societies. This view paved the way for the modern understanding of religion.

Although anthropologists worldwide refer to Tylor's definition of culture, in the 20th century "culture" emerged as the central and unifying concept of American anthropology, where it most commonly refers to the universal human capacity to classify and encode human experiences symbolically, and to communicate symbolically encoded experiences socially. American anthropology is organized into four fields, each of which plays an important role in research on culture: biological anthropology, linguistic anthropology, cultural anthropology, and in the United States, archaeology. The term "Kulturbrille", or "culture glasses," coined by German American anthropologist Franz Boas, refers to the "lenses" through which we see our own countries. Martin Lindstrom asserts that "Kulturbrille", which allow us to make sense of the culture we inhabit, also "can blind us to things outsiders pick up immediately."

The sociology of culture concerns culture as manifested in society. For sociologist Georg Simmel (1858–1918), culture referred to "the cultivation of individuals through the agency of external forms which have been objectified in the course of history." As such, culture in the sociological field can be defined as the ways of thinking, the ways of acting, and the material objects that together shape a people's way of life. Culture can be any of two types, non-material culture or material culture. Non-material culture refers to the non-physical ideas that individuals have about their culture, including values, belief systems, rules, norms, morals, language, organizations, and institutions, while material culture is the physical evidence of a culture in the objects and architecture they make or have made. The term tends to be relevant only in archeological and anthropological studies, but it specifically means all material evidence which can be attributed to culture, past or present.

Cultural sociology first emerged in Weimar Germany (1918–1933), where sociologists such as Alfred Weber used the term "Kultursoziologie" (cultural sociology). Cultural sociology was then "reinvented" in the English-speaking world as a product of the "cultural turn" of the 1960s, which ushered in structuralist and postmodern approaches to social science. This type of cultural sociology may be loosely regarded as an approach incorporating cultural analysis and critical theory. Cultural sociologists tend to reject scientific methods, instead hermeneutically focusing on words, artifacts and symbols. "Culture" has since become an important concept across many branches of sociology, including resolutely scientific fields like social stratification and social network analysis. As a result, there has been a recent influx of quantitative sociologists to the field. Thus, there is now a growing group of sociologists of culture who are, confusingly, not cultural sociologists. These scholars reject the abstracted postmodern aspects of cultural sociology, and instead, look for a theoretical backing in the more scientific vein of social psychology and cognitive science. 

The sociology of culture grew from the intersection between sociology (as shaped by early theorists like Marx, Durkheim, and Weber) with the growing discipline of anthropology, wherein researchers pioneered ethnographic strategies for describing and analyzing a variety of cultures around the world. Part of the legacy of the early development of the field lingers in the methods (much of cultural, sociological research is qualitative), in the theories (a variety of critical approaches to sociology are central to current research communities), and in the substantive focus of the field. For instance, relationships between popular culture, political control, and social class were early and lasting concerns in the field.

In the United Kingdom, sociologists and other scholars influenced by Marxism such as Stuart Hall (1932–2014) and Raymond Williams (1921–1988) developed cultural studies. Following nineteenth-century Romantics, they identified "culture" with consumption goods and leisure activities (such as art, music, film, food, sports, and clothing). They saw patterns of consumption and leisure as determined by relations of production, which led them to focus on class relations and the organization of production.

In the United States, cultural studies focuses largely on the study of popular culture; that is, on the social meanings of mass-produced consumer and leisure goods. Richard Hoggart coined the term in 1964 when he founded the Birmingham Centre for Contemporary Cultural Studies or CCCS. It has since become strongly associated with Stuart Hall, who succeeded Hoggart as Director. Cultural studies in this sense, then, can be viewed as a limited concentration scoped on the intricacies of consumerism, which belongs to a wider culture sometimes referred to as "Western civilization" or "globalism."

From the 1970s onward, Stuart Hall's pioneering work, along with that of his colleagues Paul Willis, Dick Hebdige, Tony Jefferson, and Angela McRobbie, created an international intellectual movement. As the field developed, it began to combine political economy, communication, sociology, social theory, literary theory, media theory, film/video studies, cultural anthropology, philosophy, museum studies, and art history to study cultural phenomena or cultural texts. In this field researchers often concentrate on how particular phenomena relate to matters of ideology, nationality, ethnicity, social class, and/or gender. Cultural studies is concerned with the meaning and practices of everyday life. These practices comprise the ways people do particular things (such as watching television or eating out) in a given culture. It also studies the meanings and uses people attribute to various objects and practices. Specifically, culture involves those meanings and practices held independently of reason. Watching television to view a public perspective on a historical event should not be thought of as culture unless referring to the medium of television itself, which may have been selected culturally; however, schoolchildren watching television after school with their friends to "fit in" certainly qualifies since there is no grounded reason for one's participation in this practice.

In the context of cultural studies, the idea of a "text" includes not only written language, but also films, photographs, fashion or hairstyles: the texts of cultural studies comprise all the meaningful artifacts of culture. Similarly, the discipline widens the concept of "culture." "Culture" for a cultural-studies researcher not only includes traditional high culture (the culture of ruling social groups) and popular culture, but also everyday meanings and practices. The last two, in fact, have become the main focus of cultural studies. A further and recent approach is comparative cultural studies, based on the disciplines of comparative literature and cultural studies.

Scholars in the United Kingdom and the United States developed somewhat different versions of cultural studies after the late 1970s. The British version of cultural studies had originated in the 1950s and 1960s, mainly under the influence of Richard Hoggart, E.P. Thompson, and Raymond Williams, and later that of Stuart Hall and others at the Centre for Contemporary Cultural Studies at the University of Birmingham. This included overtly political, left-wing views, and criticisms of popular culture as "capitalist" mass culture; it absorbed some of the ideas of the Frankfurt School critique of the "culture industry" (i.e. mass culture). This emerges in the writings of early British cultural-studies scholars and their influences: see the work of (for example) Raymond Williams, Stuart Hall, Paul Willis, and Paul Gilroy.

In the United States, Lindlof and Taylor write, "Cultural studies [were] grounded in a pragmatic, liberal-pluralist tradition." The American version of cultural studies initially concerned itself more with understanding the subjective and appropriative side of audience reactions to, and uses of, mass culture; for example, American cultural-studies advocates wrote about the liberatory aspects of fandom. The distinction between American and British strands, however, has faded. Some researchers, especially in early British cultural studies, apply a Marxist model to the field. This strain of thinking has some influence from the Frankfurt School, but especially from the structuralist Marxism of Louis Althusser and others. The main focus of an orthodox Marxist approach concentrates on the "production" of meaning. This model assumes a mass production of culture and identifies power as residing with those producing cultural artifacts. In a Marxist view, the mode and relations of production form the economic base of society, which constantly interacts and influences superstructures, such as culture. Other approaches to cultural studies, such as feminist cultural studies and later American developments of the field, distance themselves from this view. They criticize the Marxist assumption of a single, dominant meaning, shared by all, for any cultural product. The non-Marxist approaches suggest that different ways of consuming cultural artifacts affect the meaning of the product. This view comes through in the book "Doing Cultural Studies: The Story of the Sony Walkman" (by Paul du Gay "et al."), which seeks to challenge the notion that those who produce commodities control the meanings that people attribute to them. Feminist cultural analyst, theorist, and art historian Griselda Pollock contributed to cultural studies from viewpoints of art history and psychoanalysis. The writer Julia Kristeva is among influential voices at the turn of the century, contributing to cultural studies from the field of art and psychoanalytical French feminism.

Petrakis and Kostis (2013) divide cultural background variables into two main groups:

In 2016, a new approach to culture was suggested by Rein Raud, who defines culture as the sum of resources available to human beings for making sense of their world and proposes a two-tiered approach, combining the study of texts (all reified meanings in circulation) and cultural practices (all repeatable actions that involve the production, dissemination or transmission of purposes), thus making it possible to re-link anthropological and sociological study of culture with the tradition of textual theory.

Starting in the 1990s, psychological research on culture influence began to grow and challenge the universality assumed in general psychology. Culture psychologists began to try to explore the relationship between emotions and culture, and answer whether the human mind is independent from culture. For example, people from collectivistic cultures, such as the Japanese, suppress their positive emotions more than their American counterparts. Culture may affect the way that people experience and express emotions. On the other hand, some researchers try to look for differences between people's personalities across cultures. As different cultures dictate distinctive norms, culture shock is also studied to understand how people react when they are confronted with other cultures. Cognitive tools may not be accessible or they may function differently cross culture. For example, people that are raised in a culture with an abacus are trained with distinctive reasoning style. Cultural lenses may also make people view the same outcome of events differently. Westerners are more motivated by their successes than their failures, while East Asians are better motivated by the avoidance of failure. Culture is important for psychologists to consider when understanding the human mental operation.






</doc>
<doc id="505730" url="https://en.wikipedia.org/wiki?curid=505730" title="Cultural assimilation">
Cultural assimilation

Cultural assimilation is the process in which a minority group or culture comes to resemble a dominant group or assume the values, behaviors, and beliefs of another group. A conceptualization describes cultural assimilation as similar to acculturation while another merely considers the former as one of the latter's phases. Assimilation could also involve the so-called additive acculturation wherein, instead of replacing the ancestral culture, an individual expands their existing cultural repertoire.

Cultural assimilation may involve either a quick or a gradual change depending on circumstances of the group. Full assimilation occurs when members of a society become indistinguishable from those of the dominant group.

Whether it is desirable for a given group to assimilate is often disputed by both members of the group and those of the dominant society. Cultural assimilation does not guarantee social alikeness. Geographical and other natural barriers between cultures, even if created by the dominant culture, may be culturally different.Cultural assimilation can happen either spontaneously or forcibly ("see forced assimilation"). A culture can spontaneously adopt a different culture. Also, older, richer, or otherwise more dominant cultures can forcibly absorb subordinate cultures.

The term "assimilation" is often used with regard to not only indigenous groups but also immigrants settled in a new land. A new culture and new attitudes toward the origin culture are obtained through contact and communication. Assimilation assumes that a relatively-tenuous culture gets to be united to one unified culture. That process happens by contact and accommodation between each culture. The current definition of assimilation is usually used to refer to immigrants, but in multiculturalism, cultural assimilation can happen all over the world and within varying social contexts and is not limited to specific areas. For example, a shared language gives people the chance to study and work internationally, without being limited to the same cultural group. People from different countries contribute to diversity and form the "global culture" which means the culture combined by the elements from different countries. That "global culture" can be seen as a part of assimilation, which causes cultures from different areas to affect one another.

During the 19th and 20th centuries, and continuing until 1996, when the last residential school was closed, the Canadian government, aided by Christian Churches began a campaign to forcibly assimilate Aboriginals. The government consolidated power over Aboriginal land through treaties and the use of force, eventually isolating most indigenous people to reserves. Marriage practices and spiritual ceremonies were banned, and spiritual leaders were imprisoned. Additionally, the Canadian government instituted an extensive residential school system to assimilate children. Indigenous children were separated from their families and no longer permitted to express their culture at these new schools. They were not allowed to speak their language or practice their own traditions without receiving punishment. There were many cases in which violent or sexual abuse by the Christian church was committed. The Truth and Reconciliation Commission of Canada concluded that this effort amounted to cultural genocide. The schools actively worked to alienate children from their cultural roots. Students were prohibited from speaking their native languages, were regularly abused, and were arranged marriages by the government after their graduation. The explicit goal of the Canadian government, through the Catholic and Anglican churches, was to completely assimilate the Aboriginals into European culture and destroy all traces of their native history.

In January 2019, newly elected Brazil's president Jair Bolsonaro has stripped the indigenous affairs agency FUNAI of the responsibility to identify and demarcate indigenous lands. He argued that those territories have very tiny isolated populations and proposed to integrate them into the larger Brazilian society. According to the Survival International, "Taking responsibility for indigenous land demarcation away from FUNAI, the Indian affairs department, and giving it to the Agriculture Ministry is virtually a declaration of open warfare against Brazil’s tribal peoples."

Social scientists rely on four primary benchmarks to assess immigrant assimilation: socioeconomic status, geographic distribution, second language attainment, and intermarriage. William A.V. Clark defines immigrant assimilation in the United States as "a way of understanding the social dynamics of American society and that it is the process that occurs spontaneously and often unintended in the course of interaction between majority and minority groups."

There has been little to no existing research or evidence that demonstrates whether and how immigrant’s mobility gains—assimilating to a dominant country such as language ability, socioeconomic status etc.— causes changes in the perception of those who were born in the dominant country. This essential type of research provides information on how immigrants are accepted into dominant countries. In an article by Ariela Schachter, titled "From "different" to "similar": an experimental approach to understanding assimilation", a survey was taken of white American citizens to view their perception of immigrants who now resided in the United States. The survey indicated the whites tolerated immigrants in their home country. White natives are open to having "structural" relation with the immigrants-origin individuals, for instance, friends and neighbors; however, this was with the exception of black immigrants and natives and undocumented immigrants. However, at the same time, white Americans viewed all non-white Americans, regardless of legal status, as dissimilar.

A similar journal by Jens Hainmueller and Daniel J. Hopkins titled "The Hidden American Immigration Consensus: A Conjoint Analysis of Attitudes toward Immigrants" confirmed similar attitudes towards immigrants. The researchers used an experiment to reach their goal which was to test nine theoretical relevant attributes of hypothetical immigrants. Asking a population-based sample of U.S citizens to decide between pairs of immigrants applying for admission to the United States, the U.S citizen would see an application with information for two immigrants including notes about their education status, country, origin, and other attributes. The results showed Americans viewed educated immigrants in high-status jobs favorably, whereas they view the following groups unfavorably: those who lack plans to work, those who entered without authorization, those who do not speak fluent English and those of Iraqi descent.

As the number of international students entering the US has increased, so has the number of international students in US colleges and universities. The adaption of these newcomers is important in cross-cultural research. In the study "Cross-Cultural Adaptation of International College Student in the United States" by Yikang Wang, the goal was to examine how the psychological and socio-cultural adaption of international college students varied over time. The survey contained a sample of 169 international students attending a coeducational public university. The two subtypes of adaption: psychological and socio-cultural were examined. Psychological adaption refers to "feelings of well-being or satisfaction during cross-cultural transitions;" while socio-cultural refers to the ability to fit into the new culture. The results show both graduate and undergraduate students showed both the satisfactory and socio-cultural skilled changed over time. Psychological adaption had the most significant change for a student who has resided in the US for at least 24 months while socio-cultural adaption steadily increased over time. It can be concluded that eventually over time, the minority group will shed some of their culture's characteristic when in a new country and incorporate new culture qualities. Also, it was confirmed that the more time spent in a new country would result in becoming more accustomed to the dominate countries aspects of characteristics.

Figure 2 demonstrates as the length of time resided in the United States increase—the dominant country, the life satisfaction and socio-cultural skill increase as well—positive correlation.

In a study by Viola Angelini, "Life Satisfaction of Immigrant: Does cultural assimilation matter?", the theory of assimilation as being beneficial is confirmed. The goal of this study was to assess the difference between cultural assimilation and the subjective well-being of immigrants. The journal included a study that examined a "direct measure of assimilation with a host culture and immigrants' subjective well-being." Using data from the German Socio-Economic Panel, it was concluded that there was a positive correlation between cultural assimilation and an immigrant's life's satisfaction/wellbeing even after discarding factors such as employment status, wages, etc. "Life Satisfaction of Immigrant: Does cultural assimilation matter?" also confirms "association with life satisfaction is stronger for established immigrants than for recent ones." It was found that the more immigrants that identified with the German culture and who spoke the fluent national language—dominant country language, the more they reported to be satisfied with their lives. Life satisfaction rates were higher for those who had assimilated to the dominant country than those who had not assimilated since those who did incorporate the dominant language, religion, psychological aspects, etc.

One's willingness to assimilate is, surprisingly, not only based solely on their decision to adapt but other factors as well, such as how they’re introduced to the dominant country. In the study "Examination of cultural shock, inter-cultural sensitivity and willingness to adopt" by Clare D’Souza, the study uses a diary method to analyze the data collected. The study involved students undergoing a study abroad tour. The results show negative intercultural sensitivity is much greater in participants who experience "culture shock". Those who experience culture shock have emotional expression and responses of hostility, anger, negativity, anxiety frustration, isolation, and regression. Also, for one who has traveled to the country before permanently moving, they would have predetermined beliefs about the culture and their status within the country. The emotional expression for this individual includes excitement, happiness, eagerness, and euphoria. This article addresses each theme, pre-travel, culture shock, negative cultural sensitivity and positive cultural sensitivity, their perception, emotional expression and responses, their gender and the interpretation for the responses.

Similar to Clare D’Souza’s journal "Examination of cultural shock, inter-cultural sensitivity and willingness to adapt," another journal titled "International Students from Melbourne Describing Their Cross-Cultural Transitions Experiences: Culture Shock, Social Interaction, and Friendship Development" by Nish Belford focuses on cultural shock. Belford interviewed international students to explore their experience after living and studying in Melbourne, Australia. The data collected were narratives from the students that focused on variables such as "cultural similarity, intercultural communication competence, intercultural friendship, and relational identity to influence their experiences." The names of the students have been changed for privacy purposes. Jules, one of the students, stated "It's just the small things that bother me a lot. For example, if people are just walking on the floor with their shoes and then just lying on the bed with their shoes. It bothers me a lot because that's not part of my culture." Man and Jeremy commented "Like yeah ... I found few things as a culture shock. Like one of my housemates, once like she said I have a step-mother, so in India I was like in India we don’t have step-mothers - yes she was Aussie. And I mean this was one of those things. The way people speak was different." Last, Jeremy described his experience as "Yeah, like in Chinese background we normally do not stare at people - when talking to people - so eye contact is quite different and when I walk down the street - like random people say hi, how are you? To me - so which I found it was quite interesting because we Chinese we don't do that, like when you stop someone and if you talk to strangers to China it can be considered that you want something from me - yeah. Yes, it is a completely different experience." It is common that international students who come into a new country to study abroad are confronted with "strangeness." This exert focuses only on culture shock and does not include the responses from the students about social interaction and friendship development.

Between 1880 and 1920, the United States took in roughly 24 million immigrants. This increase in immigration can be attributed to many historical changes. The beginning of the 21st century has also marked a massive era of immigration, and sociologists are once again trying to make sense of the impacts that immigration has on society and on the immigrants themselves.

Assimilation had various meanings in American sociology. Henry Pratt Fairchild associates American assimilation with Americanization or the "melting pot" theory. Some scholars also believed that assimilation and acculturation were synonymous. According to a common point of view, assimilation is a "process of interpretation and fusion" from another group or person. That may include memories, behaviors and sentiments. By sharing their experiences and histories, they blend into the common cultural life. A related theory is structural pluralism proposed by American sociologist Milton Gordon. It describes the American situation wherein despite the cultural assimilation of ethnic groups to mainstream American society, they maintained structural separation. Gordon maintained that there is limited integration of the immigrants into American social institutions such as educational, occupational, political, and social cliques.

The long history of immigration in the established gateways means that the place of immigrants in terms of class, racial, and ethnic hierarchies in the traditional gateways is more structured or established, but on the other hand, the new gateways do not have much immigration history and so the place of immigrants in terms of class, racial, and ethnic hierarchies is less defined, and immigrants may have more influence to define their position. Secondly, the size of new gateways may influence immigrant assimilation. Having a smaller gateway may influence the level of racial segregation among immigrants and native-born people. Thirdly, the difference in institutional arrangements may influence immigrant assimilation. Traditional gateways, unlike new gateways, have many institutions set up to help immigrants such as legal aid, bureaus, social organizations. Finally, Waters and Jimenez have only speculated that those differences may influence immigrant assimilation and the way researchers that should assess immigrant assimilation.

Canada's multicultural history dates back to its European colonization in the 16th century, when French settlers, British settlers, and indigenous peoples vied for control of the region.

Canada retains one of the largest immigrant populations in the world. The 2016 census recorded 7.5 million documented immigrants, representing a fifth of the country's total population. Focus has shifted from a rhetoric of cultural assimilation to cultural integration. In contrast to assimilation, integration aims to preserve the roots of a minority society while still allowing for smooth coexistence with the dominant

Culture-specific:



</doc>
<doc id="18964621" url="https://en.wikipedia.org/wiki?curid=18964621" title="Cultural competency training">
Cultural competency training

Cultural Competency Training is an instruction to achieve cultural competence and the ability to appreciate and interpret accurately other cultures.

Cultural competence refers to an ability to interact effectively with people of different cultures. Cultural competence comprises four components: (a) awareness of one's own cultural worldview, (b) attitude towards cultural differences, (c) knowledge of different cultural practices and worldviews, and (d) cross-cultural skills. Developing cultural competence results in an ability to understand, communicate with, and effectively interact with people across cultures and leads to a 15% decrease in miscommunication. Cultural Competency has a fundamental importance in every aspect of a work field and that includes school and government setting. With the amalgamation of different cultures in American society, it has become imperative for teachers and government employees to have some form of cultural competency training.

To cater to an increasingly globalized society, many hospitals, organizations, and employers may choose to implement forms of cultural competency training methods to enhance transparency between language, values, beliefs, and cultural differences. Training in cultural competence often includes careful consideration of how best to approach people's various forms of diversity. This new found awareness oftentimes allows military members, educators, medical practitioners, workers, and common citizens to establish equity in their environments and enhances interrelationships between one another for increased productivity levels. There have been numerous developed theories as to how best to conduct cultural competency training, which oftentimes is dependent on the specific environment and type of work.

When defining the ideas that surround cultural competence training, defining what culture is can help to understand the ideas that shape the concept. Culture is defined as the set of shared attitudes, values, goals, and practices that characterizes an institution or organization. When looking at culture in terms of cultural competence training, certain groups of individuals should be focused on because of their relevance to society. There are many groups that are marginalized and underrepresented; however, four specific areas to look at are:

LGBTQIA community, race, and religion. These areas, along with others, represent concepts that make up one's identity. The approach to identity helps to shape the ideas and themes that go into cultural competence training.
The acronym LGBTQIAP stands for Lesbian, Gay, Bisexual, Transsexual, Transgender, Queer, Questioning, Intersex, and Asexual. This particular group of individuals has faced numerous obstacles and has historical events to highlight the inequalities they face such as the Stonewall riots. The Stonewall riots became a symbol for the gay liberation movement when police attempted a raid at the Stonewall Inn bar to arrest the gay and lesbian patrons and the gay community fought back. Numerous systemic oppressions historically and currently target LGBT individuals. Cultural competence training helps professionals develop knowledge and skills on how to address issues and be more aware on the type of language that is politically correct.

Race is a sensitive aspect of cultural competency training that requires professionals to be able to identify, acknowledge and value cultural differences. Training on this aspect of cultural competence teaches professionals that to ignore racial differences, is a form of microaggression that can help exacerbate racial inequalities. In order to begin to understand intercultural communications one must understand the historical and social context under which different cultural groups operate. For example, the history related to the cultural genocide of indigenous peoples in North America, understanding the said group's value system, their ways of learning, and logic is essential in being able to understand how certain aspects of their culture may be similar or different from our own. Such distinction must be approached with respect and without ascribing superiority or inferiority to the difference.

Religious differences can play a role in how professionals interact and communicate with others. Religiosity refers to the nature and extent of public and private religious activity, including belief in God, prayer, and place of worship attendance. Religiosity is usually linked to formal religious traditions (such as Christianity), institutions (such as mosques), sacred texts (such as The Book of Mormon), and a definitive moral code (such as the Decalogue). Spirituality can be an important part of religion but can also exist independent of extant faith traditions, involving a variety of more individual subjective beliefs and activities related to the sacred. In this aspect of cultural competence training professionals should learn how to have religious competence. Religious competence refers to skills, practices, and orientations that recognize, explore, and harness patient religiosity to facilitate diagnosis, recovery, and healing. Religious competence involves the learning and deployment of generic competencies, including active listening and a nonjudgmental stance. It is also an overarching orientation, providing a safe place for discussion of religious issues and identities received in a humble, respectful, and empathetic manner.

In terms of nationality, particularly for people who are immigrants, the recent increase in global migration make them an increasingly common demographic everywhere. Though they will have varying cultures as well. In this aspect, it is important for those who are trained to understand both similarities and differences between them, and the individual they are helping. With this knowledge, it makes the process of aiding the individual more efficient, and successful. Both the past Nation the individual has come from, and their journey of immigration as an experience, can shape their mentality. To have specialists with specific nationalities help explain some differences is a helpful strategy.

School is considered to be the second learning home for kids. Every year a large number of people come to the United States. These groups of people are often families, including small children. In today's world, cultural competency plays a very vital role in shaping the kids future. In the United States, there is an underlying difference among parents as to how a kid should be raised, but it is clear that cultural competency should be taught at a young age. The United States is not the front runner in cultural competency training amongst children, as Canada and Australia are seemingly far more progressive in this sector. Cultural competency training can be a huge help for the families who are thinking of adopting a foster child, specifically, if that child was born outside of United States. A school is a mixture of different races and cultures and as an educator, one must be sensitive to everyone's needs. Different cultures act uniquely to the different situations, and as an educator, one has to not only value diversity, but also have a strategy for everyone to feel welcomed.

Over the years, there have been new developed ways of practicing cultural competency in the workforce. There are many different methods that would allow assistance in cultural competency such as: Global leadership programs, international team building exercises and specific cross-cultural skills training for special executive positions. Having a good grasp on the many different cultures that exist is increasingly becoming a major principle in the workforce. The techniques for cultural competency training must be practiced more than just in class room lecture. Trainers must be extremely educated in this matter to be able to sufficiently train people. They must take notice of their own biases perspective and about the different types cultures that receive discrimination.

In the medical setting, effective communication between clinicians, patients, families and other health care providers is fundamental.

Health disparities refer to gaps in the quality of health and health care across racial, ethnic, and socioeconomic groups. Studies have demonstrated the multiple factors that contribute to health disparities.

Cultural Competence Online for Medical Practice (CCOMP) is an attempt in the United States to address one of the factors - the patient-doctor interaction. The CCOMP project is funded by a grant from the National Institutes of Health (NIH) through the National Heart Lung and Blood Institute (NHLBI). CCOMP offers a clinician's guide to reduce cardiovascular disparities, intended to create effective cross-cultural approaches to care for African-American patients with cardiovascular disease, especially hypertension. Videos with real patient scenarios and case-based modules are aimed at developing this increased awareness.




</doc>
<doc id="30963584" url="https://en.wikipedia.org/wiki?curid=30963584" title="Cultural consensus theory">
Cultural consensus theory

Cultural consensus theory is an approach to information pooling (aggregation, data fusion) which supports a framework for the measurement and evaluation of beliefs as cultural; shared to some extent by a group of individuals. Cultural consensus models guide the aggregation of responses from individuals to estimate (1) the culturally appropriate answers to a series of related questions (when the answers are unknown) and (2) individual competence (cultural competence) in answering those questions. The theory is applicable when there is sufficient agreement across people to assume that a single set of answers exists. The agreement between pairs of individuals is used to estimate individual cultural competence. Answers are estimated by weighting responses of individuals by their competence and then combining responses.

Cultural consensus theory assumes that cultural beliefs are learned and shared across people and that there is a common understanding of what the world and society are all about. Since the amount of information in a culture is too large for any one individual to master, individuals know different subsets of the cultural knowledge and vary in their cultural competence. Cultural beliefs are beliefs held by a majority of culture members. Given a set of questions, on the same topic, shared cultural beliefs or norms regarding the answers can be estimated by aggregating the responses across a sample of culture members. When an agreement is close to absolute, estimating answers is straightforward. The problem addressed by cultural consensus theory is how to estimate beliefs when there is some degree of heterogeneity present in responses. In general, cultural consensus theory provides a framework for determining whether responses are sufficiently homogeneous to estimate a single set of shared answers and then estimating the answers and individual cultural competence in answering the questions.

Cultural consensus models do not create consensus or explain why consensus exists; they simply facilitate the discovery and description of possible consensus. A high degree of agreement among raters must be present in responses to use consensus theory – only with high agreement does it make sense to aggregate responses to estimate beliefs of the group. Although there are statistical methods to evaluate whether agreement among raters is greater than chance (Binomial test, Friedman test, or Kendall’s coefficient of concordance), these methods do not provide a best estimate of the “true” answers nor do they estimate competence of the raters. Cultural consensus theory can estimate competence from the agreement between subjects and then, answers are estimated by “weighting” individual responses by competence prior to aggregation.

A very important feature in the aggregation of responses is that the combined responses of individuals will be more accurate than the responses of each individual included in the aggregation. Reliability theory in psychology (specifically, the reliability coefficient and the Spearman–Brown prediction formula) provides a mathematical estimate of the accuracy or validity of aggregated responses from the number of units being combined and the level of agreement among the units. In this case, the accuracy of aggregated responses can be calculated from the number of subjects and the average Pearson correlation coefficient between all pairs of subjects (across questions).

To use cultural consensus theory, at least three assumptions must be met:

Cultural consensus theory encompasses formal and informal models. Practically speaking, these models are often used to estimate cultural beliefs, including the degree to which individuals report such beliefs. The formal cultural consensus model models the decision-making process for answering questions. This version is limited to categorical-type responses: multiple-choice type questions (including those with dichotomous true/false or yes/no responses) and responses to open-ended questions (with a single word or short phrase response for each question). This version of the model has a series of additional assumptions that must be met, i.e., no response bias. The formal model has direct parallels in signal detection theory and latent class analysis. An informal version of the model is available as a set of analytic procedures and obtains similar information with fewer assumptions. The informal model parallels a factor analysis on people (without rotation) and thus has similarities to Q factor analysis (as in Q Methodology). The informal version of the model can accommodate interval estimates and ranked response data. Both approaches provide estimates of the culturally correct answers and estimates of individual differences in the accuracy of reported information. 

One specific method of the formal version used in the analysis of data is the mathematical model, which is a set of logical axioms as well as derived propositions and assumptions that explain how empirical variables fit in the model's parameters. The informal model, on the other hand, uses reliability analysis.

Cultural competence is estimated from the similarity in responses between pairs of subjects since the agreement between a pair of respondents is a function of their individual competencies. In the formal model, the similarity is the probability that matched responses occur (match method. or the probability of particular response combinations occur (covariance method). Simple match or covariance measures are then corrected for guessing and the proportion of positive responses, respectively. In the informal model, similarity is calculated with a Pearson correlation coefficient.

A matrix of agreement coefficients between all pairs of subjects is then factored with a minimum residual factoring method (principal axis factoring without rotation) to solve for the unknown competence values on the main diagonal. (For the informal model, the maximum likelihood factor analysis algorithm is preferred, but principal axis factoring can be used as well.) To determine whether the solution meets cultural consensus criteria, that only a single factor is present, a goodness of fit rule is used. If the ratio of the first to second eigenvalues is large with subsequently small values and all first factor loadings are positive, then it is assumed that the data contain only a single factor or a single response pattern.

Individual competence values are used to weight the responses and estimate the culturally correct answers. In the formal model, a confidence level (Bayesian adjusted probabilities) is obtained for each answer from the pattern of responses and the individual competence scores. In the informal model, responses are also weighted, using a linear model. When factoring a correlation matrix, the estimated answers appear as the first set of factor scores. Also, note that factor scores are usually provided as standardized variables (mean of zero), but may be transformed back to your original data collection units.

When used as a method for analysis, the cultural consensus theory allows the following: the determination whether the observed variability in knowledge is cultural; the measurement of cultural competence that each individual possesses; and, the determination of culturally correct knowledge.

Cultural Consensus analyses may be performed with software applications. The formal consensus model is currently only available in the software packages ANTHROPAC or UCINET. Analysis procedures for the informal model are available in most statistical packages. The informal model can be run within a factor analysis procedure, requesting the minimum-residual (principal axis factoring) algorithm method that solves for the missing diagonal without rotation. However, when factor analysis is used for consensus applications, ~~the data must be transposed, so that questions are the unit of analysis (the rows in a data matrix) and people are the variables~~ (the columns in the data matrix).

An advantage of cultural consensus is the availability of necessary sample size information and that necessary sample sizes do not need to be very large. Sample size determination in a consensus analysis is similar to other types of analyses; namely, that when variability is low, power is high and small samples will suffice. Here, variability is the agreement (competence) among subjects. For the formal model, sample size can be estimated from the level of agreement (e.g., assuming a low average competence level of .50), the proportion of items to be correctly classified (assuming a high level, .95), and high confidence (.999) a minimum sample size of 29 (per subgroup) is necessary.[1,5] For higher levels of competence and lower levels of accuracy and confidence, smaller samples sizes are necessary. Similarly, sample size can be estimated with reliability theory and the Spearman–Brown prophecy formula (applied to people instead of items). For a relatively low level of agreement (an average correlation of .25 between people, comparable to an average competence of .50) and a high degree of desired validity (.95 correlation between the estimated answers and the true answers), a study would require a minimum sample size of 30 subjects.

In summary, cultural consensus theory offers a framework for estimating cultural beliefs. A formal model is based on the decision-making process model of how questions are answered (with parameters for competence, response bias, and guessing). The model proceeds from axioms and uses mathematical proofs to arrive at estimates of competence and answers to a series of questions. The informal model is a set of statistical procedures that provides similar information. Given a series of related questions, the agreement between people’s reported answers is used to estimate their cultural competence. Cultural competence is how much an individual knows or shares group beliefs. Since the extraction of individual competencies depends upon having a single factor solution, the ratio of the first and second eigenvalues (> 3:1) serves as a goodness-of-fit indicator that a single factor is present in the pattern of responses. Culturally correct answers are estimated by weighting and combining individuals’ responses.



</doc>
<doc id="32962014" url="https://en.wikipedia.org/wiki?curid=32962014" title="Cultural deprivation">
Cultural deprivation

Cultural deprivation is a theory in sociology where a person has inferior norms, values, skills and knowledge. The theory states that people of the working class experience cultural deprivation and this disadvantages them, as a result of which the gap between classes increases. 

For example, in education, lower-class students suffer from cultural deprivation as their parents do not know the best school for their child but middle-class parents know the system and so send their children to the best school for them. This puts the lower-class students at a disadvantage, thus increasing inequality and the gap between middle-class and lower-class students. 

Proponents of this theory argue that working class culture (regardless of race, gender, ethnicity or other factors) inherently differs from that of people in the middle class. This difference in culture means that while middle-class children can easily acquire cultural capital by observing their parents, working-class children cannot, and this deprivation is self-perpetuating.

The theory claims that the middle class gains cultural capital as the result of primary socialization, while the working class does not. Cultural capital helps the middle class succeed in society because their norms and values facilitate educational achievement and subsequent employability. Working class members of society that lack cultural capital do not pass it on to their children, perpetuating the class system. Middle-class children's cultural capital allows them to communicate with their middle-class teachers more effectively than working-class children and this contributes to social inequality.

Bourdieu claimed that state schools are set up to make everybody middle class, although only the middle class and some high achieving working class have the cultural capital to achieve this. From a Marxist perspective cultural deprivation observes that the resources available to the working class are limited and that working-class children enter school less-well prepared than others.




</doc>
<doc id="164660" url="https://en.wikipedia.org/wiki?curid=164660" title="Cultural diversity">
Cultural diversity

Cultural diversity is the quality of diverse or different cultures, as opposed to monoculture, the global monoculture, or a homogenization of cultures, akin to cultural decay. The phrase cultural diversity can also refer to having different cultures respect each other's differences. The phrase "cultural diversity" is also sometimes used to mean the variety of human societies or cultures in a specific region, or in the world as a whole. Globalization is often said to have a negative effect on the world's cultural diversity.

Diversity refers to the attributes that people use to confirm themselves with respect to others, “that person is different from me.” These attributes include demographic factors (such as race, gender, and age) as well as values and cultural norms. The many separate societies that emerged around the globe differ markedly from each other, and many of these differences persist to this day. The more obvious cultural differences that exist between people are language, dress, and traditions, there are also significant variations in the way societies organize themselves, such as in their shared conception of morality, religious belief, and in the ways, they interact with their environment. Cultural diversity can be seen as analogous to biodiversity.
By analogy with biodiversity, which is thought to be essential to the long-term survival of life on earth, it can be argued that cultural diversity may be vital for the long-term survival of humanity; and that the conservation of indigenous cultures may be as important to humankind as the conservation of species and ecosystems is to life in general. The General Conference of UNESCO took this position in 2001, asserting in Article 1 of the Universal Declaration on Cultural Diversity that "...cultural diversity is as necessary for humankind as biodiversity is for nature."

This position is rejected by some people, on several grounds. Firstly, like most evolutionary accounts of human nature, the importance of cultural diversity for survival may be an un-testable hypothesis, which can neither be proved nor disproved. Secondly, it can be argued that it is unethical deliberately to conserve "less developed" societies because this will deny people within those societies the benefits of technological and medical advances enjoyed by those in the "developed" world.

In the same manner that the promotion of poverty in underdeveloped nations as "cultural diversity" is unethical. It is unethical to promote all religious practices simply because they are seen to contribute to cultural diversity. Particular religious practices are recognized by the WHO and UN as unethical, including female genital mutilation, polygamy, child brides, and human sacrifice.

With the onset of globalization, traditional nation-states have been placed under enormous pressures. Today, with the development of technology, information and capital are transcending geographical boundaries and reshaping the relationships between the marketplace, states, and citizens. In particular, the growth of the mass media industry has largely impacted on individuals and societies across the globe. Although beneficial in some ways, this increased accessibility has the capacity to negatively affect a society's individuality. With information being so easily distributed throughout the world, cultural meanings, values, and tastes run the risk of becoming homogenized. As a result, the strength of identity of individuals and societies may begin to weaken.

Some individuals maintain that it is in the best interests of individuals and of humanity as a whole that all people adhere to a specific model for society or specific aspects of such a model.
Nowadays, communication between different countries has become much more frequent. And many more students are choosing to study overseas to broaden their understanding of cultural diversity. For example, according to Fengling, Chen, Du Yanjun, and Yu Ma's paper "Academic Freedom in the People's Republic of China and the United States Of America", in regards to Chinese education, they pointed out that "traditionally, teaching has consisted of spoon-feeding, and learning has been largely by rote. China's traditional system of education has sought to make students accept fixed and ossified content." And "In the classroom, Chinese professors are the laws and authorities; Students in China show great respect to their teachers in general." On another hand, in the United States of America, the paper states "American students treat college professors as equals." Also "American students are encouraged to debate topics. The free open discussion on various topics is due to the academic freedom which most American colleges and universities enjoy." The discussion above gives us an overall idea of the differences between China and the United States on education. But we cannot simply judge which one is better, because each culture has its own advantages and features. Thanks to those differences form the culture diversity and those make our world more colorful. For students who go abroad for education, if they can combine positive culture elements from two different cultures to their self-development, it would be a competitive advantage in their whole career. Especially, with the current process of global economics, people who owned different perspectives on cultures stand at a more competitive position in the current world.

Cultural diversity is difficult to quantify, but a good indication is thought to be a count of the number of languages spoken in a region or in the world as a whole. By this measure, we may be going through a period of the precipitous decline in the world's cultural diversity. Research carried out in the 1990s by David Crystal (Honorary Professor of Linguistics at the University of Wales, Bangor) suggested that at that time, on average, one language was falling into disuse every two weeks. He calculated that if that rate of the language death were to continue, then by the year 2100 more than 90% of the languages currently spoken in the world will have gone extinct.

Overpopulation, immigration and imperialism (of both the militaristic and cultural kind) are reasons that have been suggested to explain any such decline. However, it could also be argued that with the advent of globalism, a decline in cultural diversity is inevitable because information sharing often promotes homogeneity.

The Universal Declaration on Cultural Diversity adopted by UNESCO in 2001 is a legal instrument that recognizes cultural diversity as the "common heritage of humanity" and considers its safeguarding to be a concrete and ethical imperative inseparable from respect for human dignity.

Beyond the Declaration of Principles adopted in 2003 at the Geneva Phase of the World Summit on the Information Society (WSIS), the UNESCO Convention on the Protection and Promotion of the Diversity of Cultural Expressions, adopted in October 2005, is a legally binding instrument to all States Parties to the Convention that recognizes

It was adopted in response to "growing pressure exerted on countries to waive their right to enforce cultural policies and to put all aspects of the cultural sector on the table when negotiating international trade agreements". To date, 116 member states, as well as the European Union, have ratified the Convention, except the US, Australia and Israel. States Parties recognize the specificity of cultural goods and services, as well as state sovereignty and public services in this area. Thought for world trade, this soft law instrument (meaning non-binding) clearly became a crucial reference to the definition of the European policy choice. In 2009, the European Court of Justice favored a broad view of culture—beyond cultural values through the protection of film or the objective of promoting linguistic diversity yet previously recognized. On top of it, under this Convention, the EU and China have committed to fostering more balanced cultural exchanges, strengthening international cooperation and solidarity with business and trade opportunities in cultural and creative industries. The most motivating factor behind Beijing's willingness to work in partnership at the business level might certainly be the access to creative talents and skills from foreign markets.

There is also the Convention for the Safeguarding of the Intangible Cultural Heritage ratified on June 20, 2007, by 78 states which said: 

Cultural diversity was also promoted by the Montreal Declaration of 2007, and by the European Union. The idea of a global multicultural heritage covers several ideas, which are not exclusive (see multiculturalism). In addition to language, diversity can also include religious or traditional practice.

On a local scale, Agenda 21 for culture, the first document of world scope that establishes the foundations for a commitment by cities and local governments to cultural development, supports local authorities committed to cultural diversity.

The defense of cultural diversity can take several meanings:
In a specific occasion of social life, or custom, cultural uniformity can be observed and displayed in behaviors of a community.

Cultural diversity is presented as the antithesis of cultural uniformity.

Some (including UNESCO) fear this hypothesis of a trend towards cultural uniformity. To support this argument they emphasize different aspects:

There are several international organizations that work towards protecting threatened societies and cultures, including Survival International and UNESCO. The UNESCO Universal Declaration on Cultural Diversity, adopted by 185 Member States in 2001, represents the first international standard-setting instrument aimed at preserving and promoting cultural diversity and intercultural dialogue.

Indeed, the notion of "cultural diversity" has been echoed by more neutral organizations, particularly within the UNESCO. Beyond the Declaration of Principles adopted in 2003 at the Geneva Phase of the World Summit on the information Society (WSIS), the UNESCO Convention on the Protection and Promotion of the Diversity of Cultural Expressions was adopted on 20 October 2005, but neither ratified by the US, Australia nor by Israel. It is instead a clear recognition of the specificity of cultural goods and services, as well as state sovereignty and public services in this area. Thought for world trade, this soft law instrument (strength in not binding) clearly became a crucial reference to the definition of the European policy choice. In 2009, the European Court of Justice favored a broad view of culture—beyond cultural values—through the protection of film or the objective of promoting linguistic diversity yet previously recognized. On top of it, under this Convention, the EU and China have committed to fostering more balanced cultural exchanges, strengthening international cooperation and solidarity with business and trade opportunities in cultural and creative industries.

The European Commission-funded "Network of Excellence on "Sustainable Development in a Diverse World"" (known as "SUS.DIV") builds upon the UNESCO Declaration to investigate the relationship between cultural diversity and sustainable development.





</doc>
<doc id="323912" url="https://en.wikipedia.org/wiki?curid=323912" title="Cultural ecology">
Cultural ecology

Cultural ecology is the study of human adaptations to social and physical environments. Human adaptation refers to both biological and cultural processes that enable a population to survive and reproduce within a given or changing environment. This may be carried out diachronically (examining entities that existed in different epochs), or synchronically (examining a present system and its components). The central argument is that the natural environment, in small scale or subsistence societies dependent in part upon it, is a major contributor to social organization and other human institutions. In the academic realm, when combined with study of political economy, the study of economies as polities, it becomes political ecology, another academic subfield. It also helps interrogate historical events like the Easter Island Syndrome.

Anthropologist Julian Steward (1902-1972) coined the term, envisioning cultural ecology as a methodology for understanding how humans adapt to such a wide variety of environments. In his "Theory of Culture Change: The Methodology of Multilinear Evolution" (1955), cultural ecology represents the "ways in which culture change is induced by adaptation to the environment." A key point is that any particular human adaptation is in part historically inherited and involves the technologies, practices, and knowledge that allow people to live in an environment. This means that while the environment influences the character of human adaptation, it does not determine it. In this way, Steward wisely separated the vagaries of the environment from the inner workings of a culture that occupied a given environment. Viewed over the long term, this means that environment and culture are on more or less separate evolutionary tracks and that the ability of one to influence the other is dependent on how each is structured. It is this assertion - that the physical and biological environment affects culture - that has proved controversial, because it implies an element of environmental determinism over human actions, which some social scientists find problematic, particularly those writing from a Marxist perspective. Cultural ecology recognizes that ecological locale plays a significant role in shaping the cultures of a region.

Steward's method was to:


Steward's concept of cultural ecology became widespread among anthropologists and archaeologists of the mid-20th century, though they would later be critiqued for their environmental determinism. Cultural ecology was one of the central tenets and driving factors in the development of processual archaeology in the 1960s, as archaeologists understood cultural change through the framework of technology and its effects on environmental adaptation.

Cultural ecology as developed by Steward is a major subdiscipline of anthropology. It derives from the work of Franz Boas and has branched out to cover a number of aspects of human society, in particular the distribution of wealth and power in a society, and how that affects such behaviour as hoarding or gifting (e.g. the tradition of the potlatch on the Northwest North American coast).

One 2000s-era conception of cultural ecology is as a general theory that regards ecology as a paradigm not only for the natural and human sciences, but for cultural studies as well. In his "Die Ökologie des Wissens" (The Ecology of Knowledge), Peter Finke explains that this theory brings together the various cultures of knowledge that have evolved in history, and that have been separated into more and more specialized disciplines and subdisciplines in the evolution of modern science (Finke 2005). In this view, cultural ecology considers the sphere of human culture not as separate from but as interdependent with and transfused by ecological processes and natural energy cycles. At the same time, it recognizes the relative independence and self-reflexive dynamics of cultural processes. As the dependency of culture on nature, and the ineradicable presence of nature in culture, are gaining interdisciplinary attention, the difference between cultural evolution and natural evolution is increasingly acknowledged by cultural ecologists. Rather than genetic laws, information and communication have become major driving forces of cultural evolution (see Finke 2006, 2007). Thus, causal deterministic laws do not apply to culture in a strict sense, but there are nevertheless productive analogies that can be drawn between ecological and cultural processes.

Gregory Bateson was the first to draw such analogies in his project of an Ecology of Mind (Bateson 1973), which was based on general principles of complex dynamic life processes, e.g. the concept of feedback loops, which he saw as operating both between the mind and the world and within the mind itself. Bateson thinks of the mind neither as an autonomous metaphysical force nor as a mere neurological function of the brain, but as a "dehierarchized concept of a mutual dependency between the (human) organism and its (natural) environment, subject and object, culture and nature", and thus as "a synonym for a cybernetic system of information circuits that are relevant for the survival of the species." (Gersdorf/ Mayer 2005: 9).

Finke fuses these ideas with concepts from systems theory. He describes the various sections and subsystems of society as 'cultural ecosystems' with their own processes of production, consumption, and reduction of energy (physical as well as psychic energy). This also applies to the cultural ecosystems of art and of literature, which follow their own internal forces of selection and self-renewal, but also have an important function within the cultural system as a whole (see next section).

The interrelatedness between culture and nature has been a special focus of literary culture from its archaic beginnings in myth, ritual, and oral story-telling, in legends and fairy tales, in the genres of pastoral literature, nature poetry. Important texts in this tradition include the stories of mutual transformations between human and nonhuman life, most famously collected in Ovid’s Metamorphoses, which became a highly influential text throughout literary history and across different cultures. This attention to culture-nature interaction became especially prominent in the era of romanticism, but continues to be characteristic of literary stagings of human experience up to the present. 

The mutual opening and symbolic reconnection of culture and nature, mind and body, human and nonhuman life in a holistic and yet radically pluralistic way seems to be one significant mode in which literature functions and in which literary knowledge is produced. From this perspective, literature can itself be described as the symbolic medium of a particularly powerful form of "cultural ecology" (Zapf 2002). Literary texts have staged and explored, in ever new scenarios, the complex feedback relationship of prevailing cultural systems with the needs and manifestations of human and nonhuman "nature." From this paradoxical act of creative regression they have derived their specific power of innovation and cultural self-renewal.

German ecocritic Hubert Zapf argues that literature draws its cognitive and creative potential from a threefold dynamics in its relationship to the larger cultural system: as a "cultural-critical metadiscourse," an "imaginative counterdiscourse," and a "reintegrative interdiscourse" (Zapf 2001, 2002). It is a textual form which breaks up ossified social structures and ideologies, symbolically empowers the marginalized, and reconnects what is culturally separated. In that way, literature counteracts economic, political or pragmatic forms of interpreting and instrumentalizing human life, and breaks up one-dimensional views of the world and the self, opening them up towards their repressed or excluded other. Literature is thus, on the one hand, a sensorium for what goes wrong in a society, for the biophobic, life-paralyzing implications of one-sided forms of consciousness and civilizational uniformity, and it is, on the other hand, a medium of constant cultural self-renewal, in which the neglected biophilic energies can find a symbolic space of expression and of (re-)integration into the larger ecology of cultural discourses. This approach has been applied and widened in volumes of essays by scholars from over the world (ed. Zapf 2008, 2016), as well as in a recent monograph (Zapf 2016).

In geography, cultural ecology developed in response to the "landscape morphology" approach of Carl O. Sauer. Sauer's school was criticized for being unscientific and later for holding a "reified" or "superorganic" conception of culture. Cultural ecology applied ideas from ecology and systems theory to understand the adaptation of humans to their environment. These cultural ecologists focused on flows of energy and materials, examining how beliefs and institutions in a culture regulated its interchanges with the natural ecology that surrounded it. In this perspective humans were as much a part of the ecology as any other organism. Important practitioners of this form of cultural ecology include Karl Butzer and David Stoddart.

The second form of cultural ecology introduced decision theory from agricultural economics, particularly inspired by the works of Alexander Chayanov and Ester Boserup. These cultural ecologists were concerned with how human groups made decisions about how they use their natural environment. They were particularly concerned with the question of agricultural intensification, refining the competing models of Thomas Malthus and Boserup. Notable cultural ecologists in this second tradition include Harold Brookfield and Billie Lee Turner II. Starting in the 1980s, cultural ecology came under criticism from political ecology. Political ecologists charged that cultural ecology ignored the connections between the local-scale systems they studied and the global political economy. Today few geographers self-identify as cultural ecologists, but ideas from cultural ecology have been adopted and built on by political ecology, land change science, and sustainability science.

Books about culture and ecology began to emerge in the 1950s and 1960s. One of the first to be published in the United Kingdom was "The Human Species" by a zoologist, Anthony Barnett. It came out in 1950-subtitled "The biology of man" but was about a much narrower subset of topics. It dealt with the cultural bearing of some outstanding areas of environmental knowledge about health and disease, food, the sizes and quality of human populations, and the diversity of human types and their abilities. Barnett's view was that his selected areas of information "...are all topics on which knowledge is not only desirable, but for a twentieth-century adult, necessary". He went on to point out some of the concepts underpinning human ecology towards the social problems facing his readers in the 1950s as well as the assertion that human nature cannot change, what this statement could mean, and whether it is true. The third chapter deals in more detail with some aspects of human genetics.

Then come five chapters on the evolution of man, and the differences between groups of men (or races) and between individual men and women today in relation to population growth (the topic of 'human diversity'). Finally, there is a series of chapters on various aspects of human populations (the topic of "life and death"). Like other animals man must, in order to survive, overcome the dangers of starvation and infection; at the same time he must be fertile. Four chapters therefore deal with food, disease and the growth and decline of human populations.

Barnett anticipated that his personal scheme might be criticised on the grounds that it omits an account of those human characteristics, which distinguish humankind most clearly, and sharply from other animals. That is to say, the point might be expressed by saying that human behaviour is ignored; or some might say that human psychology is left out, or that no account is taken of the human mind. He justified his limited view, not because little importance was attached to what was left out, but because the omitted topics were so important that each needed a book of similar size even for a summary account. In other words, the author was embedded in a world of academic specialists and therefore somewhat worried about taking a partial conceptual, and idiosyncratic view of the zoology of "Homo sapiens".

Moves to produce prescriptions for adjusting human culture to ecological realities were also afoot in North America. Paul Sears, in his 1957 Condon Lecture at the University of Oregon, titled "The Ecology of Man," he mandated "serious attention to the ecology of man" and demanded "its skillful application to human affairs." Sears was one of the few prominent ecologists to successfully write for popular audiences. Sears documents the mistakes American farmers made in creating conditions that led to the disastrous Dust Bowl. This book gave momentum to the soil conservation movement in the United States.

During this same time was J.A. Lauwery's "Man's Impact on Nature", which was part of a series on 'Interdependence in Nature' published in 1969. Both Russel's and Lauwerys' books were about cultural ecology, although not titled as such. People still had difficulty in escaping from their labels. Even "Beginnings and Blunders", produced in 1970 by the polymath zoologist Lancelot Hogben, with the subtitle "Before Science Began", clung to anthropology as a traditional reference point. However, its slant makes it clear that 'cultural ecology' would be a more apt title to cover his wide-ranging description of how early societies adapted to environment with tools, technologies and social groupings. In 1973 the physicist Jacob Bronowski produced "The Ascent of Man", which summarised a magnificent thirteen part BBC television series about all the ways in which humans have moulded the Earth and its future.

By the 1980s the human ecological-functional view had prevailed. It had become a conventional way to present scientific concepts in the ecological perspective of human animals dominating an overpopulated world, with the practical aim of producing a greener culture. This is exemplified by I. G. Simmons' book "Changing the Face of the Earth", with its telling subtitle "Culture, Environment History" which was published in 1989. Simmons was a geographer, and his book was a tribute to the influence of W.L Thomas' edited collection, "Man's role in 'Changing the Face of the Earth" that came out in 1956.

Simmons' book was one of many interdisciplinary culture/environment publications of the 1970s and 1980s, which triggered a crisis in geography with regards its subject matter, academic sub-divisions, and boundaries. This was resolved by officially adopting conceptual frameworks as an approach to facilitate the organisation of research and teaching that cuts cross old subject divisions. Cultural ecology is in fact a conceptual arena that has, over the past six decades allowed sociologists, physicists, zoologists and geographers to enter common intellectual ground from the sidelines of their specialist subjects.

In the first decade of the 21st century, there are publications dealing with the ways in which humans can develop a more acceptable cultural relationship with the environment. An example is sacred ecology, a sub-topic of cultural ecology, produced by Fikret Berkes in 1999. It seeks lessons from traditional ways of life in Northern Canada to shape a new environmental perception for urban dwellers. This particular conceptualisation of people and environment comes from various cultural levels of local knowledge about species and place, resource management systems using local experience, social institutions with their rules and codes of behaviour, and a world view through religion, ethics and broadly defined belief systems.

Despite the differences in information concepts, all of the publications carry the message that culture is a balancing act between the mindset devoted to the exploitation of natural resources and that, which conserves them. Perhaps the best model of cultural ecology in this context is, paradoxically, the mismatch of culture and ecology that have occurred when Europeans suppressed the age-old native methods of land use and have tried to settle European farming cultures on soils manifestly incapable of supporting them. There is a sacred ecology associated with environmental awareness, and the task of cultural ecology is to inspire urban dwellers to develop a more acceptable sustainable cultural relationship with the environment that supports them.




</doc>
<doc id="13775689" url="https://en.wikipedia.org/wiki?curid=13775689" title="Cultural emphasis">
Cultural emphasis

Cultural emphasis is an important aspect of a culture which is often reflected though language and, more specifically, vocabulary (Ottenheimer, 2006, p. 266). This means that the vocabulary people use in a culture indicates what is important to that group of people. If there are a lot of words to describe a certain topic in a specific culture, then there is a good chance that that topic is considered important to that culture.

The idea of cultural emphasis is rooted form the work of Franz Boas, who is considered to be one of the founders of American Anthropology (Ottenheimer, 2006, p. 15). Franz Boas developed and taught concepts such as cultural relativism and the "cultural unconscious", which allowed anthropologists who studied under him, like Edward Sapir and Ruth Benedict, to further study and develop ideas on language and culture (Hart, 2005, p. 179).

One way in which cultural emphasis is exemplified is a populace talks about the weather. For example, in a place where it is cold and it snows a lot, a large collection of words to describe the snow would be expected.

In a place where it is hot, a cornucopia of associated terms would be expected.

A concentration of related terms for similar phenomena suggests the importance in distinguishing between them. Furthermore, if you are not from the area, or that culture, you might not have experienced or know the difference between, for example, a dry heat or a humid heat, when the difference may have huge implications for the outcome of a particular action.




</doc>
<doc id="8135793" url="https://en.wikipedia.org/wiki?curid=8135793" title="Cultural framework">
Cultural framework

Cultural framework is a term used in social science to explain traditions, value systems, myths and symbols that are common in a given society. A given society may have multiple cultural frameworks (for example, United States society has different cultural frameworks for its white and African American populations). Usually cultural frameworks are mixed; as certain individuals or entire groups can be familiar with many cultural frameworks.

There is an important relation between cultural frameworks and ideologies, as most successful ideologies are closely connected to cultural frameworks of societies they spread in. Cultural framework should not, however, be confused with ideology, as those concepts are separate. For example, in Nazi Germany, Nazism was an ideology, while religious beliefs, patriotism and traditions dating back to Germanic and Frankish tribes were part of the German cultural framework.



</doc>
<doc id="477975" url="https://en.wikipedia.org/wiki?curid=477975" title="Cultural identity">
Cultural identity

Cultural identity is the identity or feeling of belonging to a group. It is part of a person's self-conception and self-perception and is related to nationality, ethnicity, religion, social class, generation, locality or any kind of social group that has its own distinct culture. In this way, cultural identity is both characteristic of the individual but also of the culturally identical group of members sharing the same cultural identity or upbringing.

Cultural (and Ethnic) Identity is a subset of the communication theory of identity that establishes four "frames of identity" that allow us to view how we build identity. These frames include the personal frame, enactment of communication frame, relationship frame, and communal frame. The communal frame refers to the cultural constraints or the sense of "right" that people live by (which varies by cultural group). Therefore, Cultural (and Ethnic) Identity become central to a persons identity, how they see themselves and how they relate to the world. 

Various modern cultural studies and social theories have investigated cultural identity and understanding . In recent decades, a new form of identification has emerged which breaks down the understanding of the individual as a coherent whole subject into a collection of various cultural identifiers. These cultural identifiers may be the result of various conditions including: location, gender, race, history, nationality, language, sexuality, religious beliefs, ethnicity, aesthetics, and even food. As one author writes, recognizing both coherence and fragmentation:

The divisions between cultures can be very fine in some parts of the world, especially in rapidly changing cities where the population is ethnically diverse and social unity is based primarily on locational contiguity.

As a "historical reservoir," culture is an important factor in shaping identity. Since one of the main characteristics of a culture is its "historical reservoir," many if not all groups entertain revisions, either consciously or unconsciously, in their historical record in order to either bolster the strength of their cultural identity or to forge one which gives them precedent for actual reform or change. 
Some critics of cultural identity argue that the preservation of cultural identity, being based upon difference, is a divisive force in society, and that cosmopolitanism gives individuals a greater sense of shared citizenship. When considering practical association in international society, states may share an inherent part of their 'make up' that gives common ground and an alternative means of identifying with each other. Nations provide the framework for culture identities called external cultural reality, which influences the unique internal cultural realities of the individuals within the nation.

Also of interest is the interplay between cultural identity and new media.

Rather than necessarily representing an individual's interaction within a certain group, cultural identity may be defined by the social network of people imitating and following the social norms as presented by the media. Accordingly, instead of learning behaviour and knowledge from cultural/religious groups, individuals may be learning these social norms from the media to build on their cultural identity.

A range of cultural complexities structure the way individuals operate with the cultural realities in their lives. Nation is a large factor of the cultural complexity, as it constructs the foundation for individual's identity but it may contrast with ones cultural reality. Cultural identities are influenced by several different factors such as ones religion, ancestry, skin colour, language, class, education, profession, skill, family and political attitudes. These factors contribute to the development of one's identity.

Components of cultural complexity also interact with the emotional experience and behavior of individuals. Trnka et al. (2018) distinguish five main components of cultural complexity relating to emotions: "1) emotion language, 2) conceptual knowledge about emotions, 3) emotion-related values, 4) feelings rules, i.e. norms for subjective experience, and 5) display rules, i.e. norms for emotional expression.” 

Cultural identity is essentially how we as individuals cater to all positions of our lives. We may be teachers, students, friends, bosses, employees, etc. How we act and how our schemas contribute to our positions are the building blocks of your overall cultural identity.

It is also noted that an individual's "cultural arena", or place where one lives, impacts the culture that person chooses to abide by. The surroundings, the environment, the people in these places play a factor in how one feels about the culture they wish to adopt. Many immigrants find the need to change their culture in order to fit into the culture of most citizens in the country. This can conflict with an immigrant's current belief in their culture and might pose a problem, as the immigrant feels compelled to choose between the two presenting cultures.

Some might be able to adjust to the various cultures in the world by committing to two or more cultures. It is not required to stick to one culture. Many people socialize and interact with people in one culture in addition to another group of people in another culture. Thus cultural identity is able to take many forms and can change depending on the cultural area. The nature of the impact of cultural arena has changed with the advent of the Internet, bringing together groups of people with shared cultural interests who before would have been more likely to integrate into their real world cultural arena. This plasticity is what allows people to feel like part of society wherever they go.

Language develops from the wants of the people who tend to disperse themselves in a common given location over a particular period of time. This tends to allow people to share a way of life that generally links individuals in a certain culture that is identified by the people of that group. The affluence of communication that comes along with sharing a language promotes connections and roots to ancestors and cultural histories. Language can function as a fluid and ever changing identifier, and can be developed in response or rebellion of another cultural code, such as creole languages in the US . 

Language also includes the way people speak with peers, family members, authority figures, and strangers, including the tone and familiarity that is included in the language.

Language learning process can also be affected by cultural identity via the understanding of specific words, and the preference for specific words when learning and using a second language.

Since many aspects of a person's cultural identity can be changed, such as citizenship or influence from outside cultures can change cultural traditions, language is a main component of cultural identity.

Kevin McDonough pointed out, in his article, several factors concerning support or rejection of the government for different cultural identity education systems. Other authors have also shown concern for the state support regarding equity for children, school transitions and multicultural education. During March 1998, the two authors, Linda D. Labbo and Sherry L. Field collected several useful books and resources to promote multicultural education in South Africa.

Identity development among immigrant groups has been studied across a multi-dimensional view of acculturation. Dina Birman and Edison Trickett (2001) conducted a qualitative study through informal interviews with first-generation Soviet Jewish Refugee adolescents looking at the process of acculturation through three different dimensions: language competence, behavioral acculturation, and cultural identity. The results indicated that, “…acculturation appears to occur in a linear pattern over time for most dimensions of acculturation, with acculturation to the American culture increasing and acculturation to the Russian culture decreasing. However, Russian language competence for the parents did not diminish with length of residence in the country” (Birman & Trickett, 2001).

In a similar study, Phinney, Horencyzk, Liebkind, and Vedder (2001) focused on a model, which concentrates on the interaction between immigrant characteristics and the responses of the majority society in order to understand the psychological effects of immigration. The researchers concluded that most studies find that being bicultural, having a combination of having a strong ethnic and national identity, yields the best adaptation in the new country of residence. An article by LaFromboise, L. K. Colemna, and Gerton, reviews the literature on the impact of being bicultural. It is shown that it is possible to have the ability to obtain competence within two cultures without losing one’s sense of identity or having to identity with one culture over the other. (LaFromboise Et Al. 1993) The importance of ethnic and national identity in the educational adaptation of immigrants indicates that a bicultural orientation is advantageous for school performance (Portes & Rumbaut, 1990). Educators can assume their positions of power in beneficially impactful ways for immigrant students, by providing them with access to their native cultural support groups, classes, after–school activities, and clubs in order to help them feel more connected to both native and national cultures. It is clear that the new country of residence can impact immigrants’ identity development across multiple dimensions. Biculturalism can allow for a healthy adaptation to life and school. With many new immigrant youth, a school district in Alberta, Canada has gone as far as to partner with various agencies and professionals in an effort to aid the cultural adjustment of new Filipino immigrant youths . In the study cited, a combination of family workshops and teacher professional development aimed to improve the language learning and emotional development of these youths and families .

How great is "Achievement Loss Associated with the Transition to Middle School and High School"? John W. Alspaugh's research is in the September/October 1998 "Journal of Educational Research" (vol. 92, no. 1), 2026. Comparing three groups of 16 school districts, the loss was greater where the transition was from sixth grade than from a K-8 system. It was also greater when students from multiple elementary schools merged into a single middle school. Students from both K-8 and middle schools lost achievement in transition to high school, though this was greater for middle school students, and high school dropout rates were higher for districts with grades 6-8 middle schools than for those with K-8 elementary schools.

The Jean S. Phinney Three-Stage Model of Ethnic Identity Development is a widely accepted view of the formation of cultural identity. In this model cultural Identity is often developed through a three-stage process: unexamined cultural identity, cultural identity search, and cultural identity achievement.

Unexamined cultural identity: "a stage where one's cultural characteristics are taken for granted, and consequently there is little interest in exploring cultural issues." This for example is the stage one is in throughout their childhood when one doesn't distinguish between cultural characteristics of their household and others. Usually a person in this stage accepts the ideas they find on culture from their parents, the media, community, and others.

An example of thought in this stage: "I don't have a culture I'm just an American." "My parents tell me about where they lived, but what do I care? I've never lived there."

Cultural identity search: "is the process of exploration and questioning about one's culture in order to learn more about it and to understand the implications of membership in that culture." During this stage a person will begin to question why they hold their beliefs and compare it to the beliefs of other cultures. For some this stage may arise from a turning point in their life or from a growing awareness of other cultures. This stage is characterized by growing awareness in social and political forums and a desire to learn more about culture. This can be expressed by asking family members questions about heritage, visiting museums, reading of relevant cultural sources, enrolling in school courses, or attendance at cultural events. This stage might have an emotional component as well.

An example of thought in this stage: "I want to know what we do and how our culture is different from others." "There are a lot of non-Japanese people around me, and it gets pretty confusing to try and decide who I am."

Cultural identity achievement: "is characterized by a clear, confident acceptance of oneself and an internalization of one's cultural identity." In this stage people often allow the acceptance of their cultural identity play a role in their future choices such as how to raise children, how to deal with stereotypes and any discrimination, and approach negative perceptions. This usually leads to an increase in self-confidence and positive psychological adjustment

There is a set of phenomena that occur in conjunction between virtual culture – understood as the modes and norms of behavior associated with the internet and the online world – and youth culture. While we can speak of a duality between the virtual (online) and real sphere (face-to-face relations), for youth, this frontier is implicit and permeable. On occasions – to the annoyance of parents and teachers – these spheres are even superposed, meaning that young people may be in the real world without ceasing to be connected.

In the present techno-cultural context, the relationship between the real world and the virtual world cannot be understood as a link between two independent and separate worlds, possibly coinciding at a point, but as a Moebius strip where there exists no inside and outside and where it is impossible to identify limits between both. For new generations, to an ever-greater extent, digital life merges with their home life as yet another element of nature. In this naturalizing of digital life, the learning processes from that environment are frequently mentioned not just since they are explicitly asked but because the subject of the internet comes up spontaneously among those polled. The ideas of active learning, of googling 'when you don’t know', of recourse to tutorials for 'learning' a program or a game, or the expression 'I learnt English better and in a more entertaining way by playing' are examples often cited as to why the internet is the place most frequented by the young people polled.

The internet is becoming an extension of the expressive dimension of the youth condition. There, youth talk about their lives and concerns, design the content that they make available to others and assess others reactions to it in the form of optimized and electronically mediated social approval. Many of today's youth go through processes of affirmation procedures and is often the case for how youth today grow dependency for peer approval. When connected, youth speak of their daily routines and lives. With each post, image or video they upload, they have the possibility of asking themselves who they are and to try out profiles differing from those they assume in the ‘real’ world. The connections they feel in more recent times have become much less interactive through personal means compared to past generations. The influx of new technology and access has created new fields of research on effects on teens and young adults. They thus negotiate their identity and create senses of belonging, putting the acceptance and censure of others to the test, an essential mark of the process of identity construction.

Youth ask themselves about what they think of themselves, how they see themselves personally and, especially, how others see them. On the basis of these questions, youth make decisions which, through a long process of trial and error, shape their identity. This experimentation is also a form through which they can think about their insertion, membership and sociability in the ‘real’ world.

From other perspectives, the question arises on what impact the internet has had on youth through accessing this sort of ‘identity laboratory’ and what role it plays in the shaping of youth identity. On the one hand, the internet enables young people to explore and perform various roles and personifications while on the other, the virtual forums – some of them highly attractive, vivid and absorbing (e.g. video games or virtual games of personification) – could present a risk to the construction of a stable and viable personal identity.



</doc>
<doc id="4442271" url="https://en.wikipedia.org/wiki?curid=4442271" title="Cultural institution">
Cultural institution

A cultural institution or cultural organization is an organization within a culture/subculture that works for the preservation or promotion of culture. The term is especially used of public and charitable organizations, but its range of meaning can be very broad. Examples of cultural institutions in modern society are museums, libraries and archives, churches, art galleries.



</doc>
<doc id="43732489" url="https://en.wikipedia.org/wiki?curid=43732489" title="Purple economy">
Purple economy

The purple economy is that part of the economy which contributes to sustainable development by promoting the cultural potential of goods and services.

“The purple economy refers to taking account of cultural aspects in economics. It designates an economy that adapts to the human diversity in globalization and that relies on the cultural dimension to give value to goods and services.” These two trends, one vertical and one horizontal, feed one another. In fact the growth in the cultural component attached to products is linked to each territory’s cultural vitality.

The context of the purple economy is that of the growing importance of culture in contemporary society. The factors involved in this include in particular: a global economic and political readjustment in favour of emerging countries, a return to local environments (once again perceived as centres for stability), new forms of claims (following on from the collapse of the great ideologies), growing social demand for quality based on cultural consumption patterns (which go hand in hand with the logic of popularization, individualization and longer life expectancies), innovative approaches (that presuppose a cultural state of mind and interdisciplinarity conducive to serendipity), and so on.

The purple economy is multidisciplinary, in that it enriches all goods and services by capitalizing on the cultural dimension inherent to every sector. The sensory, experiential economy is one application of this.

It differs from the cultural economy, which is sector-based.

In June 2013, the conclusions of a first inter-institutional working group on the purple economy, formed of experts from UNESCO, the OECD, the International Organisation of the Francophonie, French ministries, various companies and civil society. That document underscored the impact of the phenomenon of culturalization, which now affects the entire economy, with follow-on effects on employment and training. The report differentiates between "purple jobs" and "purplifying professions": the former are directly linked to the cultural environment by their very purpose (like town planners and developers), while the latter are merely caused to transform under the effect of culturalization (such as positions in human resources or in marketing and communications).

Another reference document published in June 2017 mentioned various aspects of the human environment in which economics are likely to produce cultural benefits: architecture, art, colours, enjoyment, ethics, heritage, imagination, learning, social skills, singularity, etc.

The term first appeared in 2011, in France, in a manifest published on Le Monde.fr. The signatories included the board members of the association Diversum, which organized the first International Purple Economy Forum under the patronage of UNESCO, the European Parliament and the European Commission.

The purple economy emphasizes the presence of externalities: the cultural environment from which agents draw and on which, in return, they leave their own footprints is a common good. As a result, the purple economy sees culture as an axis for sustainable development.

In fact, culture has been a whole sub-section of sustainability since the beginning. Corporate social responsibility can even be said to have originated in the International Covenant on Economic, Social and Cultural Rights adopted by the United Nations in 1966.

This issue is just one of the different components of sustainable development, alongside concerns relating to the natural environment (green economy) and to the social environment (social economy). The complementary nature of these aspects of the sustainable economy was reaffirmed in a call published by "Le Monde Économie" in 2015, leading up to the 21st United Nations Conference on Climate Change.


</doc>
<doc id="1713306" url="https://en.wikipedia.org/wiki?curid=1713306" title="Primitive culture">
Primitive culture

Primitive Culture is an 1871 book by Edward Burnett Tylor. According to Tylor a defining characteristic of primitive cultures is a greater amount of leisure time than in more complex societies. 

In 1953, John Carothers, a colonial psychiatrist who had previously worked at Mathari Mental Hospital in Nairobi, Kenya, published a report for the World Health Organization claiming and quoting several authors that compared African psychology to that of children, to immaturity compared the African mind to a European brain that had undergone a lobotomy. They were caricatures of primitive people at peace with nature, dwelling in a fascinating world of hallucinations and witch doctors.

However, African researchers have dismissed this concept, Thomas Adeoye Lambo, a leading psychiatrist and member of the Yoruba people wrote about the subject that they were "glorified pseudo-scientific novels or anecdotes with a subtle racial bias, having so many gaps and inconsistencies, that they can no longer be seriously presented as valuable observations of scientific merit". Even so, views like Carothers's had been echoed over decades of colonialism, becoming so commonplace that they were considered to be somewhat of a truism.

Further research published in JAMA has found very high rates of clinical depression in impoverished nations, such as Zimbabwe, and that depression wasn't a Western disease but a human one, and in fact glossing over primitive culture as being leisure filled and stress-free was entirely opposite of scientific findings.

Cultural primitivism has also been applied to interpretations of unfamiliar cuisines. The eating practices of Native American cultures have been likened to the ways of the noble savage, whose eating practices are characterized as equitable and inclusive. These qualifications are made from an etic perspective. Barbecue in particular has been studied by the scholar Andrew Warnes.




</doc>
<doc id="24723521" url="https://en.wikipedia.org/wiki?curid=24723521" title="Outline of culture">
Outline of culture

The following outline is provided as an overview of and topical guide to culture:

Culture – set of patterns of human activity within a community or social group and the symbolic structures that give significance to such activity. Customs, laws, dress, architectural style, social standards, religious beliefs, and traditions are all examples of cultural elements. Since 2010, Culture is considered the Fourth Pillar of Sustainable Development by UNESCO. More: Agenda 21 for Culture or in short Culture 21.






Subculture




Area studies



Culture of Africa


Culture of Asia


</doc>
<doc id="18290472" url="https://en.wikipedia.org/wiki?curid=18290472" title="Artificiality">
Artificiality

Artificiality (also called factitiousness, or the state of being artificial or man-made) is the state of being the product of intentional human manufacture, rather than occurring naturally through processes not involving or requiring human activity.

Artificiality often carries with it the implication of being false, counterfeit, or deceptive. The philosopher Aristotle wrote in his "Rhetoric":

However, artificiality does not necessarily have a negative connotation, as it may also reflect the ability of humans to replicate forms or functions arising in nature, as with an artificial heart or artificial intelligence. Political scientist and artificial intelligence expert Herbert A. Simon observes that "some artificial things are imitations of things in nature, and the imitation may use either the same basic materials as those in the natural object or quite different materials. Simon distinguishes between the artificial and the synthetic, the former being an imitation of something found in nature (for example, an artificial sweetener which generates sweetness using a formula not found in nature), and the latter being a replication of something found in nature (for example, a sugar created in a laboratory that is chemically indistinguishable from a naturally occurring sugar). Some philosophers have gone further and asserted that, in a deterministic world, "everything is natural and nothing is artificial", because everything in the world (including everything made by humans) is a product of the physical laws of the world.

It is generally possible for humans, and in some instances, for computers, to distinguish natural from artificial environments. The artificial environment tends to have more physical regularity both spatially and over time, with natural environments tending to have both irregular structures and structures that change over time. However, on close observation it is possible to discern some mathematical structures and patterns in natural environments, which can then be replicated to create an artificial environment with a more natural appearance.

For example, by identifying and imitating natural means of pattern formation, some types of automata have been used to generate organic-looking textures for more realistic shading of 3D objects.



</doc>
<doc id="677443" url="https://en.wikipedia.org/wiki?curid=677443" title="Homo faber">
Homo faber

Homo faber (Latin for "Man the Maker") is the concept that human beings are able to control their fate and their environment as a result of the use of tools.

In Latin literature, Appius Claudius Caecus uses this term in his "Sententiæ", referring to the ability of man to control his destiny and what surrounds him: "Homo faber suae quisque fortunae" ("Every man is the artifex of his destiny").

In older anthropological discussions, "Homo faber", as the "working man", is confronted with "Homo ludens", the "playing man", who is concerned with amusements, humor, and leisure. It is also used in George Kubler's book, The Shape of Time as a reference to individuals who create works of art.

The classic "homo faber suae quisque fortunae" was "rediscovered" by humanists in 14th century and was central in the Italian Renaissance.

In the 20th century, Max Scheler and Hannah Arendt made the philosophical concept central again.

Henri Bergson also referred to the concept in "Creative Evolution" (1907), defining intelligence, in its original sense, as the "faculty to create artificial objects, in particular tools to make tools, and to indefinitely variate its makings."

"Homo Faber" is the title of an influential novel by the Swiss author Max Frisch, published in 1957.

"Homo faber" can be also used in opposition or juxtaposition to "deus faber" ("God the Creator"), an archetype of which are the various gods of the forge.

"Homo faber" is used by Pierre Schaeffer in the Traité des objects Musicaux as the man creator of music, which uses its brute experience, an instinctive practice in music creation; Concluding that the "homo faber" aways precedes the Homo sapiens in the process of creation.

Frisch' book was made into the film "Voyager", starring Sam Shepard and Julie Delpy.

"Homo Faber" was one of the five International Baccalaureate Middle Years Programme areas of interaction, before it was replaced with "Human Ingenuity".

The concept of "homo faber" is referenced in Umberto Eco's "Open Work". Eco refutes its negative connotation and instead argues that "homo faber" is a manifestation of man's innate being in nature. Use of "homo faber" in this negative light is argued by Eco to represent the alienation from and objectification of nature.

"Homo Faber" is also the title of a short poem by Frank Bidart that is included in his collection "Desire" (1997).

"Homo faber" is often placed in juxtaposition to "homo adorans", the worshiping man. In other words, under traditional Judeo-Christian philosophy, the ultimate purpose of humankind is to worship God, whereas, under (for example) Marxist or Capitalist ideology, the purpose of humankind was embedded in what he or she can make or produce.





</doc>
<doc id="57165694" url="https://en.wikipedia.org/wiki?curid=57165694" title="Cultural manager">
Cultural manager

A cultural manager () is a person who is motivated by the improvement of art, works independently and professionally with knowledge of the subject, and develops work as a mediator between governmental and/or private cultural institutions with artists from different areas to articulate their work in the market with promotion and national and international dissemination.

The cultural manager works by looking for specific measures of success, always intending to improve the level of culture, seeking active cohesion between society, the governmental sector, the private sector, and the artists. The work of culture management poses learning challenges in diverse areas such as the administration of economic resources, training, and artistic communication.

Cultural management is a new profession. A person who is dedicated to this work is characterized by having abilities to visualize and interpret talent, knowing how to establish a dialogue with artists to link them to cultural projects to develop.

Universities that offer degree programs in cultural management include the University of Antioquia, the Latin American Social Sciences Institute, the University of Chile, the University of Córdoba (Spain), the National University of Colombia, the University of Barcelona, and the University of Piura.


</doc>
<doc id="6258" url="https://en.wikipedia.org/wiki?curid=6258" title="Civilization">
Civilization

A civilization or civilisation is any complex society characterized by urban development, social stratification imposed by a cultural elite, symbolic systems of communication (for example, writing systems), and a perceived separation from and domination over the natural environment.

Civilizations are intimately associated with and often further defined by other socio-politico-economic characteristics, including centralization, the domestication of both humans and other organisms, specialization of labour, culturally ingrained ideologies of progress and supremacism, monumental architecture, taxation, societal dependence upon farming and expansionism. Historically, civilization has often been understood as a larger and "more advanced" culture, in contrast to smaller, supposedly primitive cultures. Similarly, some scholars have described civilization as being necessarily multicultural. In this broad sense, a civilization contrasts with non-centralized tribal societies, including the cultures of nomadic pastoralists, Neolithic societies or hunter-gatherers, but sometimes it also contrasts with the cultures found within civilizations themselves. As an uncountable noun, "civilization" also refers to the process of a society developing into a centralized, urbanized, stratified structure. Civilizations are organized in densely populated settlements divided into hierarchical social classes with a ruling elite and subordinate urban and rural populations, which engage in intensive agriculture, mining, small-scale manufacture and trade. Civilization concentrates power, extending human control over the rest of nature, including over other human beings.

Civilization, as its etymology (below) suggests, is a concept originally linked to towns and cities. The earliest emergence of civilizations is generally associated with the final stages of the Neolithic Revolution, culminating in the relatively rapid process of urban revolution and state formation, a political development associated with the appearance of a governing elite.

The English word "civilization" comes from the 16th-century French "civilisé" ("civilized"), from Latin "civilis" ("civil"), related to "civis" ("citizen") and "civitas" ("city"). The fundamental treatise is Norbert Elias's "The Civilizing Process" (1939), which traces social mores from medieval courtly society to the Early Modern period. In "The Philosophy of Civilization" (1923), Albert Schweitzer outlines two opinions: one purely material and the other material and ethical. He said that the world crisis was from humanity losing the ethical idea of civilization, "the sum total of all progress made by man in every sphere of action and from every point of view in so far as the progress helps towards the spiritual perfecting of individuals as the progress of all progress".

Adjectives like "civility" developed in the mid-16th century. The abstract noun "civilization", meaning "civilized condition", came in the 1760s, again from French. The first known use in French is in 1757, by Victor de Riqueti, marquis de Mirabeau, and the first use in English is attributed to Adam Ferguson, who in his 1767 "Essay on the History of Civil Society" wrote, "Not only the individual advances from infancy to manhood, but the species itself from rudeness to civilisation". The word was therefore opposed to barbarism or rudeness, in the active pursuit of progress characteristic of the Age of Enlightenment.

In the late 1700s and early 1800s, during the French Revolution, "civilization" was used in the singular, never in the plural, and meant the progress of humanity as a whole. This is still the case in French. The use of "civilizations" as a countable noun was in occasional use in the 19th century, but has become much more common in the later 20th century, sometimes just meaning culture (itself in origin an uncountable noun, made countable in the context of ethnography). Only in this generalized sense does it become possible to speak of a "medieval civilization", which in Elias's sense would have been an oxymoron.

Already in the 18th century, civilization was not always seen as an improvement. One historically important distinction between culture and civilization is from the writings of Rousseau, particularly his work about education, "". Here, civilization, being more rational and socially driven, is not fully in accord with human nature, and "human wholeness is achievable only through the recovery of or approximation to an original prediscursive or prerational natural unity" (see noble savage). From this, a new approach was developed, especially in Germany, first by Johann Gottfried Herder, and later by philosophers such as Kierkegaard and Nietzsche. This sees cultures as natural organisms, not defined by "conscious, rational, deliberative acts", but a kind of pre-rational "folk spirit". Civilization, in contrast, though more rational and more successful in material progress, is unnatural and leads to "vices of social life" such as guile, hypocrisy, envy and avarice. In World War II, Leo Strauss, having fled Germany, argued in New York that this opinion of civilization was behind Nazism and German militarism and nihilism.

Social scientists such as V. Gordon Childe have named a number of traits that distinguish a civilization from other kinds of society. Civilizations have been distinguished by their means of subsistence, types of livelihood, settlement patterns, forms of government, social stratification, economic systems, literacy and other cultural traits. Andrew Nikiforuk argues that "civilizations relied on shackled human muscle. It took the energy of slaves to plant crops, clothe emperors, and build cities" and considers slavery to be a common feature of pre-modern civilizations.

All civilizations have depended on agriculture for subsistence, with the possible exception of some early civilizations in Peru which may have depended upon maritime resources. Grain farms can result in accumulated storage and a surplus of food, particularly when people use intensive agricultural techniques such as artificial fertilization, irrigation and crop rotation. It is possible but more difficult to accumulate horticultural production, and so civilizations based on horticultural gardening have been very rare. Grain surpluses have been especially important because grain can be stored for a long time. A surplus of food permits some people to do things besides produce food for a living: early civilizations included soldiers, artisans, priests and priestesses, and other people with specialized careers. A surplus of food results in a division of labour and a more diverse range of human activity, a defining trait of civilizations. However, in some places hunter-gatherers have had access to food surpluses, such as among some of the indigenous peoples of the Pacific Northwest and perhaps during the Mesolithic Natufian culture. It is possible that food surpluses and relatively large scale social organization and division of labour predates plant and animal domestication.

Civilizations have distinctly different settlement patterns from other societies. The word "civilization" is sometimes simply defined as ""'living in cities"'". Non-farmers tend to gather in cities to work and to trade.

Compared with other societies, civilizations have a more complex political structure, namely the state. State societies are more stratified than other societies; there is a greater difference among the social classes. The ruling class, normally concentrated in the cities, has control over much of the surplus and exercises its will through the actions of a government or bureaucracy. Morton Fried, a conflict theorist and Elman Service, an integration theorist, have classified human cultures based on political systems and social inequality. This system of classification contains four categories

Economically, civilizations display more complex patterns of ownership and exchange than less organized societies. Living in one place allows people to accumulate more personal possessions than nomadic people. Some people also acquire landed property, or private ownership of the land. Because a percentage of people in civilizations do not grow their own food, they must trade their goods and services for food in a market system, or receive food through the levy of tribute, redistributive taxation, tariffs or tithes from the food producing segment of the population. Early human cultures functioned through a gift economy supplemented by limited barter systems. By the early Iron Age, contemporary civilizations developed money as a medium of exchange for increasingly complex transactions. In a village, the potter makes a pot for the brewer and the brewer compensates the potter by giving him a certain amount of beer. In a city, the potter may need a new roof, the roofer may need new shoes, the cobbler may need new horseshoes, the blacksmith may need a new coat and the tanner may need a new pot. These people may not be personally acquainted with one another and their needs may not occur all at the same time. A monetary system is a way of organizing these obligations to ensure that they are fulfilled. From the days of the earliest monetarized civilizations, monopolistic controls of monetary systems have benefited the social and political elites.

Writing, developed first by people in Sumer, is considered a hallmark of civilization and "appears to accompany the rise of complex administrative bureaucracies or the conquest state". Traders and bureaucrats relied on writing to keep accurate records. Like money, writing was necessitated by the size of the population of a city and the complexity of its commerce among people who are not all personally acquainted with each other. However, writing is not always necessary for civilization, as shown by the Inca civilization of the Andes, which did not use writing at all except from a complex recording system consisting of cords and nodes instead: the "Quipus", and still functioned as a civilized society.
Aided by their division of labour and central government planning, civilizations have developed many other diverse cultural traits. These include organized religion, development in the arts, and countless new advances in science and technology.

Through history, successful civilizations have spread, taking over more and more territory, and assimilating more and more previously-uncivilized people. Nevertheless, some tribes or people remain uncivilized even to this day. These cultures are called by some "primitive", a term that is regarded by others as pejorative. "Primitive" implies in some way that a culture is "first" (Latin = "primus"), that it has not changed since the dawn of humanity, though this has been demonstrated not to be true. Specifically, as all of today's cultures are contemporaries, today's so-called primitive cultures are in no way antecedent to those we consider civilized. Anthropologists today use the term "non-literate" to describe these peoples.

Civilization has been spread by colonization, invasion, religious conversion, the extension of bureaucratic control and trade, and by introducing agriculture and writing to non-literate peoples. Some non-civilized people may willingly adapt to civilized behaviour. But civilization is also spread by the technical, material and social dominance that civilization engenders.

Assessments of what level of civilization a polity has reached are based on comparisons of the relative importance of agricultural as opposed to trade or manufacturing capacities, the territorial extensions of its power, the complexity of its division of labour, and the carrying capacity of its urban centres. Secondary elements include a developed transportation system, writing, standardized measurement, currency, contractual and tort-based legal systems, art, architecture, mathematics, scientific understanding, metallurgy, political structures and organized religion.

Traditionally, polities that managed to achieve notable military, ideological and economic power defined themselves as "civilized" as opposed to other societies or human groupings outside their sphere of influence – calling the latter barbarians, savages, and primitives.

"Civilization" can also refer to the culture of a complex society, not just the society itself. Every society, civilization or not, has a specific set of ideas and customs, and a certain set of manufactures and arts that make it unique. Civilizations tend to develop intricate cultures, including a state-based decision making apparatus, a literature, professional art, architecture, organized religion and complex customs of education, coercion and control associated with maintaining the elite.

The intricate culture associated with civilization has a tendency to spread to and influence other cultures, sometimes assimilating them into the civilization (a classic example being Chinese civilization and its influence on nearby civilizations such as Korea, Japan and Vietnam). Many civilizations are actually large cultural spheres containing many nations and regions. The civilization in which someone lives is that person's broadest cultural identity.

Many historians have focused on these broad cultural spheres and have treated civilizations as discrete units. Early twentieth-century philosopher Oswald Spengler, uses the German word "Kultur", "culture", for what many call a "civilization". Spengler believed a civilization's coherence is based on a single primary cultural symbol. Cultures experience cycles of birth, life, decline and death, often supplanted by a potent new culture, formed around a compelling new cultural symbol. Spengler states civilization is the beginning of the decline of a culture as "the most external and artificial states of which a species of developed humanity is capable".

This "unified culture" concept of civilization also influenced the theories of historian Arnold J. Toynbee in the mid-twentieth century. Toynbee explored civilization processes in his multi-volume "A Study of History," which traced the rise and, in most cases, the decline of 21 civilizations and five "arrested civilizations". Civilizations generally declined and fell, according to Toynbee, because of the failure of a "creative minority", through moral or religious decline, to meet some important challenge, rather than mere economic or environmental causes.

Samuel P. Huntington defines civilization as "the highest cultural grouping of people and the broadest level of cultural identity people have short of that which distinguishes humans from other species". Huntington's theories about civilizations are discussed below.

Another group of theorists, making use of systems theory, looks at a civilization as a complex system, i.e., a framework by which a group of objects can be analysed that work in concert to produce some result. Civilizations can be seen as networks of cities that emerge from pre-urban cultures and are defined by the economic, political, military, diplomatic, social and cultural interactions among them. Any organization is a complex social system and a civilization is a large organization. Systems theory helps guard against superficial and misleading analogies in the study and description of civilizations.

Systems theorists look at many types of relations between cities, including economic relations, cultural exchanges and political/diplomatic/military relations. These spheres often occur on different scales. For example, trade networks were, until the nineteenth century, much larger than either cultural spheres or political spheres. Extensive trade routes, including the Silk Road through Central Asia and Indian Ocean sea routes linking the Roman Empire, Persian Empire, India and China, were well established 2000 years ago, when these civilizations scarcely shared any political, diplomatic, military, or cultural relations. The first evidence of such long distance trade is in the ancient world. During the Uruk period, Guillermo Algaze has argued that trade relations connected Egypt, Mesopotamia, Iran and Afghanistan. Resin found later in the Royal Cemetery at Ur is suggested was traded northwards from Mozambique.

Many theorists argue that the entire world has already become integrated into a single "world system", a process known as globalization. Different civilizations and societies all over the globe are economically, politically, and even culturally interdependent in many ways. There is debate over when this integration began, and what sort of integration – cultural, technological, economic, political, or military-diplomatic – is the key indicator in determining the extent of a civilization. David Wilkinson has proposed that economic and military-diplomatic integration of the Mesopotamian and Egyptian civilizations resulted in the creation of what he calls the "Central Civilization" around 1500 BCE. Central Civilization later expanded to include the entire Middle East and Europe, and then expanded to a global scale with European colonization, integrating the Americas, Australia, China and Japan by the nineteenth century. According to Wilkinson, civilizations can be culturally heterogeneous, like the Central Civilization, or homogeneous, like the Japanese civilization. What Huntington calls the "clash of civilizations" might be characterized by Wilkinson as a clash of cultural spheres within a single global civilization. Others point to the Crusades as the first step in globalization. The more conventional viewpoint is that networks of societies have expanded and shrunk since ancient times, and that the current globalized economy and culture is a product of recent European colonialism. 

The notion of world history as a succession of "civilizations" is an entirely modern one.
In the European Age of Discovery, emerging Modernity was put into stark contrast with the
Neolithic and Mesolithic stage of the cultures of the New World, suggesting
that the complex states had emerged at some time in prehistory.
The term "civilization" as it is now most commonly understood, a complex state with centralization, social stratification and specialization of labour, corresponds to early empires that arise in the Fertile Crescent in the Early Bronze Age, around roughly 3000 BC.
Gordon Childe defined the emergence of civilization as the result of two successive revolutions: the Neolithic Revolution, triggering the development of settled communities, and the Urban Revolution.

At first, the Neolithic was associated with shifting subsistence cultivation, where continuous farming led to the depletion of soil fertility resulting in the requirement to cultivate fields further and further removed from the settlement, eventually compelling the settlement itself to move. In major semi-arid river valleys, annual flooding renewed soil fertility every year, with the result that population densities could rise significantly.
This encouraged a secondary products revolution in which people used domesticated animals not just for meat, but also for milk, wool, manure and pulling ploughs and carts – a development that spread through the Eurasian Oecumene.

The earlier neolithic technology and lifestyle was established first in Western Asia (for example at Göbekli Tepe, from about 9,130 BCE), and later in the Yellow River and Yangtze basins in China (for example the Pengtoushan culture from 7,500 BCE), and later spread.
Mesopotamia is the site of the earliest developments of the Neolithic Revolution from around 10,000 BCE, with civilizations developing from 6,500 years ago. This area has been identified as having "inspired some of the most important developments in human history including the invention of the wheel, the planting of the first cereal crops and the development of cursive script."
Similar pre-civilized "neolithic revolutions" also began independently from 7,000 BCE in northwestern South America (the Norte Chico civilization) and Mesoamerica.

The 8.2 Kiloyear Arid Event and the 5.9 Kiloyear Interpluvial saw the drying out of semiarid regions and a major spread of deserts. This climate change shifted the cost-benefit ratio of endemic violence between communities, which saw the abandonment of unwalled village communities and the appearance of walled cities, associated with the first civilizations.
This "urban revolution" marked the beginning of the accumulation of transferable surpluses, which helped economies and cities develop. It was associated with the state monopoly of violence, the appearance of a soldier class and endemic warfare, the rapid development of hierarchies, and the appearance of human sacrifice.

The civilized urban revolution in turn was dependent upon the development of sedentism, the domestication of grains and animals and development of lifestyles that facilitated economies of scale and accumulation of surplus production by certain social sectors. The transition from "complex cultures" to "civilizations", while still disputed, seems to be associated with the development of state structures, in which power was further monopolized by an elite ruling class who practiced human sacrifice.

Towards the end of the Neolithic period, various elitist Chalcolithic civilizations began to rise in various "cradles" from around 3300 BCE, expanding into large-scale empires in the course of the Bronze Age (Old Kingdom of Egypt, Akkadian Empire, Assyrian Empire, Old Assyrian Empire, Hittite Empire).

A parallel development took place independently in the Pre-Columbian Americas, where the Mayans began to be urbanized around 500 BCE, and the fully fledged Aztec and Inca emerged by the 15th century, briefly before European contact.

The Bronze Age collapse was followed by the Iron Age around 1200 BCE, during which a number of new civilizations emerged, culminating in a period from the 8th to the 3rd century BCE which Karl Jaspers termed the Axial Age, presented as a critical transitional phase leading to classical civilization.
William Hardy McNeill proposed that this period of history was one in which culture contact between previously separate civilizations saw the "closure of the oecumene" and led to accelerated social change from China to the Mediterranean, associated with the spread of coinage, larger empires and new religions. This view has recently been championed by Christopher Chase-Dunn and other world systems theorists.

A major technological and cultural transition to modernity began approximately 1500 CE in Western Europe, and from this beginning new approaches to science and law spread rapidly around the world, incorporating earlier cultures into the industrial and technological civilization of the present.

Civilizations have generally ended in one of two ways; either through being incorporated into another expanding civilization (e.g. As Ancient Egypt was incorporated into Hellenistic Greek, and subsequently Roman civilizations), or by collapse and reversion to a simpler form, as happens in what are called Dark Ages.

There have been many explanations put forward for the collapse of civilization. Some focus on historical examples, and others on general theory.

Political scientist Samuel Huntington has argued that the defining characteristic of the 21st century will be a clash of civilizations. According to Huntington, conflicts between civilizations will supplant the conflicts between nation-states and ideologies that characterized the 19th and 20th centuries. These views have been strongly challenged by others like Edward Said, Muhammed Asadi and Amartya Sen. Ronald Inglehart and Pippa Norris have argued that the "true clash of civilizations" between the Muslim world and the West is caused by the Muslim rejection of the West's more liberal sexual values, rather than a difference in political ideology, although they note that this lack of tolerance is likely to lead to an eventual rejection of (true) democracy. In "Identity and Violence" Sen questions if people should be divided along the lines of a supposed "civilization", defined by religion and culture only. He argues that this ignores the many others identities that make up people and leads to a focus on differences.

Cultural Historian Morris Berman suggests in "Dark Ages America: the End of Empire" that in the corporate consumerist United States, the very factors that once propelled it to greatness―extreme individualism, territorial and economic expansion, and the pursuit of material wealth―have pushed the United States across a critical threshold where collapse is inevitable. Politically associated with over-reach, and as a result of the environmental exhaustion and polarization of wealth between rich and poor, he concludes the current system is fast arriving at a situation where continuation of the existing system saddled with huge deficits and a hollowed-out economy is physically, socially, economically and politically impossible. Although developed in much more depth, Berman's thesis is similar in some ways to that of Urban Planner, Jane Jacobs who argues that the five pillars of United States culture are in serious decay: community and family; higher education; the effective practice of science; taxation and government; and the self-regulation of the learned professions. The corrosion of these pillars, Jacobs argues, is linked to societal ills such as environmental crisis, racism and the growing gulf between rich and poor.

Cultural critic and author Derrick Jensen argues that modern civilization is directed towards the domination of the environment and humanity itself in an intrinsically harmful, unsustainable, and self-destructive fashion. Defending his definition both linguistically and historically, he defines civilization as "a culture... that both leads to and emerges from the growth of cities", with "cities" defined as "people living more or less permanently in one place in densities high enough to require the routine importation of food and other necessities of life". This need for civilizations to import ever more resources, he argues, stems from their over-exploitation and diminution of their own local resources. Therefore, civilizations inherently adopt imperialist and expansionist policies and, to maintain these, highly militarized, hierarchically structured, and coercion-based cultures and lifestyles.

The Kardashev scale classifies civilizations based on their level of technological advancement, specifically measured by the amount of energy a civilization is able to harness. The scale is only hypothetical, but it puts energy consumption in a cosmic perspective. The Kardashev scale makes provisions for civilizations far more technologically advanced than any currently known to exist.

The current scientific consensus is that human beings are the only animal species with the cognitive ability to create civilizations. A recent thought experiment, however, has considered whether it would "be possible to detect an industrial civilization in the geological record" given the paucity of geological information about eras before the quaternary.




</doc>
<doc id="782895" url="https://en.wikipedia.org/wiki?curid=782895" title="Interculturalism">
Interculturalism

Interculturalism refers to support for cross-cultural dialogue and challenging self-segregation tendencies within cultures. Interculturalism involves moving beyond mere passive acceptance of a multicultural fact of multiple cultures effectively existing in a society and instead promotes dialogue and interaction between cultures.

Interculturalism has arisen in response to criticisms of existing policies of multiculturalism, such as criticisms that such policies had failed to create inclusion of different cultures within society, but instead have divided society by legitimizing segregated separate communities that have isolated themselves and accentuated their specificity. It is based on the recognition of both differences and similarities between cultures. It has addressed the risk of the creation of absolute relativism within postmodernity and in multiculturalism.

Philosopher Martha Nussbaum in her work "Cultivating Humanity", describes interculturalism as involving "the recognition of common human needs across cultures and of dissonance and critical dialogue within cultures" and that interculturalists "reject the claim of identity politics that only members of a particular group have the ability to understand the perspective of that group". Ali Rattansi, in his book "Multiculturalism: A Very Short Introduction" (2011) argues that Interculturalism offers a more fruitful way than conventional multiculturalism for different ethnic groups to co-exist in an atmosphere that encourages both better inter-ethnic understanding and civility; he provides useful examples of how interculturalist projects in the UK have shown in practice a constructive way forward for promoting multi-ethnic civility. Based on a considerable body of research, he also sets out the outlines of a new interpretation of global history which shows that concepts of tolerance are not restricted to the West, and that what is usually regarded as a unique Western cultural achievement should more appropriately be regarded as a Eurasian achievement. He thus offers a more interculturalist view of global history which undermines notions of 'a clash of civilisations'.

Interculturalism has both supporters and opponents amongst people who endorse multiculturalism. Gerald Delanty views interculturalism as capable of incorporating multiculturalism within it. In contrast, Nussbaum views interculturalism as distinct from multiculturalism and notes that several humanities professors have preferred interculturalism over multiculturalism because they view multiculturalism as being "associated with relativism and identity politics".

The United Nations' agency UNESCO adopted the Convention on the Protection and Promotion of the Diversity of Cultural Expressions in 2005 that declares support for interculturality. In Germany, all universities are required to have a section on intercultural competence in their social work programs, that involves students being able to be open to listen and communicate with people of different cultural backgrounds, have knowledge of the backgrounds of cultural groups, knowledge of existing stereotypes and prejudices involving cultural groups, and other criteria. Salman Cheema, the Head of Marketing and Communications of the British Council, in an article titled "From Multiculturalism to Interculturalism – A British perspective", spoke of an event co-hosted by the British Council and Canada's Institute for Research on Public Policy (IRPP) in Montreal, Quebec, Canada on April 11, 2013, interculturalist advocate Phil Wood declared that multiculturalism has faced serious problems that need to be resolved through interculturalism, and rejected those opponents of multiculturalism who seek to restore a pre-multiculturalist monoculturalist society. Several days later in Montreal, the New Democratic Party of Canada (NDP) declared support for interculturalism in the preamble of its constitution adopted its federal convention held in Montreal on April 14, 2013.




</doc>
<doc id="1982394" url="https://en.wikipedia.org/wiki?curid=1982394" title="Cultural appropriation">
Cultural appropriation

Cultural appropriation, at times also phrased cultural misappropriation, is the adoption of elements of one culture by members of another culture. This can be controversial when members of a dominant culture appropriate from disadvantaged minority cultures.

According to critics of the practice, cultural appropriation differs from acculturation, assimilation, or equal cultural exchange in that this appropriation is a form of colonialism: cultural elements are copied from a minority culture by members of a dominant culture, and these elements are used outside of their original cultural context—sometimes even against the expressly stated wishes of members of the originating culture.

Cultural appropriation is considered harmful by various groups and individuals, including Indigenous people working for cultural preservation, those who advocate for collective intellectual property rights of the originating, minority cultures, and those who have lived or are living under colonial rule. Often unavoidable when multiple cultures come together, cultural appropriation can include exploitation of another culture's religious and cultural traditions, fashion, symbols, language, and music.

Those who see this appropriation as exploitative state that the original meaning of these cultural elements is lost or distorted when they are removed from their originating cultural contexts, and that such displays are disrespectful or even a form of desecration. Cultural elements that may have deep meaning to the original culture may be reduced to "exotic" fashion or toys by those from the dominant culture. Kjerstin Johnson has written that, when this is done, the imitator, "who does not experience that oppression is able to 'play', temporarily, an 'exotic' other, without experiencing any of the daily discriminations faced by other cultures." The African-American academic, musician and journalist Greg Tate argues that appropriation and the "fetishising" of cultures, in fact, alienates those whose culture is being appropriated.

The concept of cultural appropriation has been heavily criticized. Critics note that the concept is often misunderstood or misapplied by the general public, and that charges of "cultural appropriation" are at times misapplied to situations such as eating food from a variety of cultures or simply learning about different cultures. Others state that the act of cultural appropriation as it is usually defined does not meaningfully constitute social harm, or the term lacks conceptual coherence. Still others argue that the term sets arbitrary limits on intellectual freedom, artists' self-expression, reinforces group divisions, or itself promotes a feeling of enmity or grievance rather than liberation.

Cultural appropriation can involve the use of ideas, symbols, artifacts, or other aspects of human-made visual or non-visual culture. As a concept that is controversial in its applications, the propriety of cultural appropriation has been the subject of much debate. Opponents of cultural appropriation view many instances as wrongful appropriation when the subject culture is a minority culture or is subordinated in social, political, economic, or military status to the dominant culture or when there are other issues involved, such as a history of ethnic or racial conflict. Linda Martín Alcoff writes that this is often seen in cultural outsiders' use of an oppressed culture's symbols or other cultural elements, such as music, dance, spiritual ceremonies, modes of dress, speech, and social behaviour when these elements are trivialized and used for fashion, rather than respected within their original cultural context. Opponents view the issues of colonialism, context, and the difference between appropriation and mutual exchange as central to analyzing cultural appropriation. They argue that mutual exchange happens on an "even playing field", whereas appropriation involves pieces of an oppressed culture being taken out of context by a people who have historically oppressed those they are taking from, and who lack the cultural context to properly understand, respect, or utilize these elements.

A different view of cultural appropriation states the criticism of cultural appropriation is "a deeply conservative project", despite progressive roots, that “first seeks to preserve in formaldehyde the content of an established culture and second tries [to] prevent others from interacting with that culture." Proponents view it as often benign or mutually beneficial, citing mutation, product diversity, technological diffusion, and cultural empathy among its benefits. For example, the film "Star Wars" used elements from Akira Kurosawa's "The Hidden Fortress", which itself used elements from Shakespeare; culture in the aggregate is arguably better off for each instance of appropriation. Fusion between cultures has produced such foods as American Chinese cuisine, modern Japanese sushi, and bánh mì, each of which is sometimes argued to reflect part of its respective culture's identity.

Cultural appropriation is a relatively recent subject of academic study.
The term emerged in the 1980s, in discussions of post-colonial critiques of Western expansionism, though the concept had been explored earlier, such as in "Some General Observations on the Problems of Cultural Colonialism" by Kenneth Coutts‐Smith in 1976.

Cultural and racial theorist George Lipsitz has used the term "strategic anti-essentialism" to refer to the calculated use of a cultural form, outside of one's own, to define oneself or one's group. Strategic anti-essentialism can be seen in both minority cultures and majority cultures, and is not confined only to the use of the other. However, Lipsitz argues, when the majority culture attempts to strategically anti-essentialize itself by appropriating a minority culture, it must take great care to recognize the specific socio-historical circumstances and significance of these cultural forms so as not to perpetuate the already existing majority vs. minority unequal power relations.

A common example of cultural appropriation is the adoption of the iconography of another culture, and using it for purposes that are unintended by the original culture or even offensive to that culture's mores. Examples include sports teams using Native American tribal names or images as mascots; wearing jewelry or fashion with religious symbols such as the war bonnet, medicine wheel, or cross without any belief in those religions; and copying iconography from another culture's history such as Polynesian tribal tattoos, Chinese characters, or Celtic art worn without regard to their original cultural significance. Critics of the practice of cultural appropriation contend that divorcing this iconography from its cultural context or treating it as kitsch risks offending people who venerate and wish to preserve their cultural traditions.

In Australia, Aboriginal artists have discussed an "authenticity brand" to ensure consumers are aware of artworks claiming false Aboriginal significance. The movement for such a measure gained momentum after the 1999 conviction of John O'Loughlin for selling paintings that he falsely described as the work of renowned Aboriginal artist, Clifford Possum Tjapaltjarri. In Canada, visual artist Sue Coleman has garnered negative attention for appropriating and amalgamating styles of Indigenous art into her work. Coleman, who has been accused of "copying and selling Indigenous-style artwork" has described herself as a "translator" of Indigenous art forms, which drew further criticism. In his open letter to Coleman, Kwakwak'awakw/Salish Artist Carey Newman stressed the importance of artists being accountable within the Indigenous communities as the antidote to appropriation.

Historically, some of the most hotly debated cases of cultural appropriation have occurred in places where cultural exchange is the highest, such as along the trade routes in southwestern Asia and southeastern Europe. Some scholars of the Ottoman Empire and ancient Egypt argue that Ottoman and Egyptian architectural traditions have long been falsely claimed and praised as Persian or Arab.

Many Native Americans have criticized what they deem to be cultural appropriation of their sweat lodge and vision quest ceremonies by non-Natives, and even by tribes who have not traditionally had these ceremonies. They contend that there are serious safety risks whenever these events are conducted by those who lack the many years of training and cultural immersion required to lead them safely, pointing to the deaths or injuries in 1996, 2002, 2004, and several high-profile deaths in 2009.

Cultural appropriation is controversial in the fashion industry due to the belief that some trends commercialise and cheapen the ancient heritage of indigenous cultures. There is debate about whether designers and fashion houses understand the history behind the clothing they are taking from different cultures, besides the ethical issues of using these cultures' shared intellectual property without consent, acknowledgement, or compensation. In response to this criticism, many fashion experts claim that this occurrence is in fact "culture appreciation", rather than cultural appropriation. Companies and designers claim the use of unique cultural symbols is an effort to recognize and pay homage to that specific culture.

During the 17th century, the forerunner to the three piece suit was adapted by the English and French aristocracy from the traditional dress of diverse Eastern European and Islamic countries. The Justacorps frock coat was copied from the long zupans worn in Poland and the Ukraine, the necktie or cravat was derived from a scarf worn by Croatian mercenaries fighting for Louis XIII, and the brightly colored silk waistcoats popularised by Charles II of England were inspired by exotic Turkish, Indian and Persian attire acquired by wealthy English travellers.

During the Highland Clearances, the British aristocracy appropriated traditional Scottish clothing. Tartan was given spurious association with specific Highland clans after publications such as James Logan's romanticised work "The Scottish Gael" (1831) led the Scottish tartan industry to invent clan tartans and tartan became a desirable material for dresses, waistcoats and cravats. In America, plaid flannel had become workwear by the time of Westward expansion, and was widely worn by Old West pioneers and cowboys who were not of Scottish descent. In the 21st century, tartan remains ubiquitous in mainstream fashion.

By the 19th century the fascination had shifted to Asian culture. English Regency era dandies adapted the Indian churidars into slim fitting pantaloons, and frequently wore turbans within their own houses. Later, Victorian gentlemen wore smoking caps based on the Islamic fez, and fashionable turn of the century ladies wore Orientalist Japanese inspired kimono dresses. During the tiki culture fad of the 1950s, white women frequently donned the qipao to give the impression that they had visited Hong Kong, although the dresses were frequently made by seamstresses in America using rayon rather than genuine silk. At the same time, teenage British Teddy Girls wore Chinese coolie hats due to their exotic connotations.

In Mexico, the sombrero associated with the mestizo peasant class was adapted from an earlier hat introduced by the Spanish colonials during the 18th century. This, in turn, was adapted into the cowboy hat worn by American cowboys after the US Civil War. In 2016, the University of East Anglia prohibited the wearing of sombreros to parties on campus, in the belief that these could offend Mexican students, a move that was widely criticized.

American Western wear was copied from the work attire of 19th century Mexican Vaqueros, especially the pointed cowboy boots and the guayabera which was adapted into the embroidered Western shirt. The China poblana dress associated with Mexican women was appropriated from the choli and lehenga worn by Indian maidservants like Catarina de San Juan who arrived from Asia from the 17th century onwards.

In Britain, the rough tweed cloth clothing of the Irish, English and Scottish peasantry, including the flat cap and Irish hat were appropriated by the upper classes as the British country clothing worn for sports such as hunting or fishing, in imitation of the then Prince of Wales. The country clothing, in turn, was appropriated by the wealthy American Ivy League and later preppy subcultures during the 1950s and 1980s due to both its practicality and its association with the English elite. During the same period the British comedian Tommy Cooper was known for wearing a Fez throughout his performances.

When keffiyehs became popular in the late 2000s, experts made a clear distinction between the wearing of a genuine scarf, and a fake made in China. Palestinian independence activists and socialists denounced the wearing of scarves not made in Palestine as a form of cultural appropriation, but encouraged young white people and fellow Muslims to buy shemaghs made in the Herbawi factory to demonstrate solidarity with the Palestinian people and improve the economy of the West Bank. In 2017, Topshop caused controversy by selling Chinese-made playsuits that imitated the pattern of the keffiyeh.

Several fashion designers and models have featured imitations of Native American warbonnets in their fashion shows, such as Victoria's Secret in 2012, when model Karlie Kloss wore one during her walk on the runway; a Navajo Nation spokesman called it a "mockery". Cherokee academic Adrienne Keene wrote in "The New York Times":

For the [Native American] communities that wear these headdresses, they represent respect, power and responsibility. The headdress has to be earned, gifted to a leader in whom the community has placed their trust. When it becomes a cheap commodity anyone can buy and wear to a party, that meaning is erased and disrespected, and Native peoples are reminded that our cultures are still seen as something of the past, as unimportant in contemporary society, and unworthy of respect.

Both Victoria's Secret and Kloss issued apologies stating that they had no intentions of offending anyone.

Archbishop Justin Welby of the Anglican Church has claimed that the crucifix is "now just a fashion statement and has lost its religious meaning.". Crucifixes have been incorporated into Japanese lolita fashion by non-Christians in a cultural context that is distinct from its original meaning as a Christian religious symbol.


While the history of colonization and marginalization is not unique to the Americas, the practice of non-Native sports teams deriving team names, imagery, and mascots from indigenous peoples is still common in the United States and Canada, and has persisted to some extent despite protests from Indigenous groups. Cornel Pewewardy, Professor and Director of Indigenous Nations Studies at Portland State University, cites indigenous mascots as an example of dysconscious racism which, by placing images of Native American or First Nations people into an invented media context, continues to maintain the superiority of the dominant culture. It is argued that such practices maintain the power relationship between the dominant culture and the indigenous culture, and can be seen as a form of cultural imperialism.

Such practices may be seen as particularly harmful in schools and universities that have a stated purpose of promoting ethnic diversity and inclusion. In recognition of the responsibility of higher education to eliminate behaviors that create a hostile environment for education, in 2005 the NCAA initiated a policy against "hostile and abusive" names and mascots that led to the change of many derived from Native American culture, with the exception of those that established an agreement with particular tribes for the use of their specific names. Other schools retain their names because they were founded for the education of Native Americans, and continue to have a significant number of indigenous students. The trend towards the elimination of indigenous names and mascots in local schools has been steady, with two thirds having been eliminated over the past 50 years according to the National Congress of American Indians (NCAI).

In contrast, the Seminole Tribe of Florida, in what the "Washington Post" calls an unusual move, has approved of the Florida State Seminoles use of their historical leader, Osceola, and his Appaloosa horse as the mascots Osceola and Renegade. After the NCAA attempted to ban the use of Native American names and iconography in college sports in 2005, the Seminole Tribe of Florida passed a resolution offering explicit support for FSU's depiction of aspects of Florida Seminole culture and Osceola as a mascot. The university was granted a waiver, citing the close relationship with, and ongoing consultation between, the team and the Florida tribe. In 2013, the tribe's chairman objected to outsiders meddling in tribal approval, stating that the FSU mascot and use of Florida State Seminole iconography "represents the courage of the people who were here and are still here, known as the Unconquered Seminoles." Conversely, in 2013, the Seminole Nation of Oklahoma expressed disapproval of "the use of all American Indian sports-team mascots in the public school system, by college and university level and by professional sports teams." Additionally, not all members of the Florida State Seminoles are supportive of the stance taken by their leadership on this issue.

In other former colonies in Asia, Africa, and South America, the adoption of indigenous names for majority indigenous teams is also found. There are also ethnically-related team names derived from prominent immigrant populations in the area, such as the Boston Celtics, the Notre Dame Fighting Irish, and the Minnesota Vikings.

The 2018 Commonwealth Games to be held on the Gold Coast in Australia from 4 April 2018 has named its mascot Borobi, the local Yugambeh word for "koala," and has sought to trademark the word through IP Australia. The application is being opposed by a Yugambeh cultural heritage organisation, which argues that the Games organising committee used the word without proper consultation with the Yugambeh people.

The term wigger (common spelling "wigga") is a slang term for a white person who adopts the mannerisms, language, and fashions associated with African-American culture, particularly hip hop, and, in Britain, the grime scene, often implying the imitation is being done badly, although usually with sincerity rather than mocking intent. Wigger is a portmanteau of "white" and "nigger" or "nigga," and the related term wangsta is a mashup of "wannabe" or "white", and "gangsta". Among black hip-hop fans, the word "nigga" can sometimes be considered a friendly greeting, but when used by white people, it is usually viewed as offensive. "Wigger" may be derogatory, reflecting stereotypes of African-American, black British, and white culture (when used as synonym of white trash). The term is sometimes used in a racist manner, by other white people to belittle the person perceived as "acting black", but it is widely used by African Americans like 50 Cent offended by the wigga's perceived demeaning of black people and culture.

The phenomenon of white people adopting elements of black culture has been prevalent at least since slavery was abolished in the Western world. The concept has been documented in the United States, Canada, the United Kingdom, Australia, and other white-majority countries. An early form of this was the "white negro" in the jazz and swing music scenes of the 1920s and 1930s, as examined in the 1957 Norman Mailer essay "The White Negro". It was later seen in the zoot suiter of the 1930s and 1940s, the hipster of the 1940s, the beatnik of the 1950s–1960s, the blue-eyed soul of the 1970s, and the hip hop of the 1980s and 1990s. In 1993, an article in the UK newspaper "The Independent" described the phenomenon of white, middle-class kids who were "wannabe Blacks". 2005 saw the publication of "Why White Kids Love Hip Hop: Wangstas, Wiggers, Wannabes, and the New Reality of Race in America" by Bakari Kitwana, "a culture critic who's been tracking American hip hop for years""."

Robert A. Clift's documentary "Blacking Up: Hip-Hop's Remix of Race and Identity" questions white enthusiasts of black hip-hop culture. Clift's documentary examines "racial and cultural ownership and authenticity -- a path that begins with the stolen blackness seen in the success of Stephen Foster, Al Jolson, Benny Goodman, Elvis Presley, the Rolling Stones -- all the way up to Vanilla Ice (popular music's ur-wigger...) and Eminem." A review of the documentary refers to the wiggers as "white poseurs", and states that the term "wigger" "is used both proudly and derisively to describe white enthusiasts of black hip-hop culture".
The term "blackfishing" was popularised in 2018 by writer Wanna Thompson, describing female white social media influencers who adopt a look perceived to be African including braided hair, dark skin from tanning or make-up, full lips, and large thighs. Critics argue they take attention and opportunities from black influencers by appropriating their aesthetic and have likened the trend to blackface.

Among critics, the misuse and misrepresentation of indigenous culture is seen as an exploitative form of colonialism, and one step in the destruction of indigenous cultures.

The results of this use of indigenous knowledge have led some tribes, and the United Nations General Assembly, to issue several declarations on the subject. The "Declaration of War Against Exploiters of Lakota Spirituality" includes the passage:
Article 31 1 of the United Nations "Declaration on the Rights of Indigenous Peoples" states:

In 2015, a group of Native American academics and writers issued a statement against the Rainbow Family members whose acts of "cultural exploitation... dehumanize us as an indigenous Nation because they imply our culture and humanity, like our land, is anyone's for the taking."

In writing about Indigenous intellectual property for the Native American Rights Fund (NARF), board member Professor Rebecca Tsosie stresses the importance of these property rights being held collectively, not by individuals:

The long-term goal is to actually have a legal system, and certainly a treaty could do that, that acknowledges two things. Number one, it acknowledges that indigenous peoples are peoples with a right to self-determination that includes governance rights over all property belonging to the indigenous people. And, number two, it acknowledges that indigenous cultural expressions are a form of intellectual property and that traditional knowledge is a form of intellectual property, but they are collective resources – so not any one individual can give away the rights to those resources. The tribal nations actually own them collectively.

Use of minority languages is also cited as cultural appropriation when non-speakers of Scottish Gaelic or Irish get tattoos in those languages. Likewise, the use of incorrect Scottish Gaelic in a tokenistic fashion aimed at non-Gaelic speakers on signage and announcements has been criticized as disrespectful to fluent speakers of the language.

Since the early 2000s, it has become increasingly popular for people not of Asian descent, to get tattoos of Indian devanagari, Korean letters or Han characters (traditional, simplified or Japanese), often without knowing the actual meaning of the symbols being used.

As of the 2010 census, Asian-Americans made up 4.8 percent of the U.S. population. According to a study by the University of Southern California Annenberg School for Communication and Journalism in 2016, one out of 20 (which corresponds to 5 percent) speaking roles go to Asian-Americans. However, they are given only one percent of lead roles in film. White actors account for 76.2 percent of lead roles, while representing 72.4 percent of the population according to the last US census.

In 2017, "Ghost in the Shell", which is based on the seinen manga "Ghost in the Shell" by Masamune Shirow, provoked disputes over whitewashing. Scarlett Johansson, a white actress, took the role of Motoko Kusanagi, a Japanese character. This was seen as cultural appropriation by some fans of the original manga who expected the role to be taken by an Asian or Asian-American actor.

During Halloween, some people buy, wear, and sell Halloween costumes based on cultural or racial stereotypes. Costumes that depict cultural stereotypes, like "Indian Warrior" or "Pocahottie" are sometimes worn by people who do not belong to the cultural group being stereotyped. These costumes have been criticized as being in poor taste at best and, at worst, blatantly racist and dehumanizing. There have been public protests calling for the end to the manufacture and sales of these costumes and connecting their "degrading" portrayals of Indigenous women to the Missing and Murdered Indigenous Women (MMIW) crisis. In some cases, theme parties have been held where attendees are encouraged to dress up as stereotypes of a certain racial group. A number of these parties have been held at colleges, and at times other than Halloween, including Martin Luther King Jr. Day and Black History Month.

In chapter four of his book "Playing Indian", Native American historian Philip J. Deloria refers to the Koshare Indian Museum and Dancers as an example of "object hobbyists" who adopt the material culture of indigenous peoples of the past ("the vanishing Indian") while failing to engage with contemporary native peoples or acknowledge the history of conquest and dispossession. In the 1950s, the head councilman of the Zuni Pueblo saw a performance and said: "We know your hearts are good, but even with good hearts you have done a bad thing." In Zuni culture, religious object and practices are only for those that have earned the right to participate, following techniques and prayers that have been handed down for generations. In 2015, the Koshare's Winter Night dances were canceled after a late request was received from Cultural Preservation Office (CPO) of the Hopi Nation asking that the troop discontinue their interpretation of the dances of the Hopi and Pueblo Native Americans. Director of the CPO Leigh Kuwanwisiwma saw video of the performances online, and said the performers were "mimicking our dances, but they were insensitive, as far as I'm concerned." In both instances, unable to satisfy the concerns of the tribes and out of respect for the Native Americans, the Koshare Dance Team complied with the requests, removed dances found to be objectionable, and even went so far as to give items deemed culturally significant to the tribes.

The objections from some Native Americans towards such dance teams center on the idea that the dance performances are a form of cultural appropriation which place dance and costumes in inappropriate contexts devoid of their true meaning, sometimes mixing elements from different tribes. In contrast, the dance teams state that "[their] goal is to preserve Native American dance and heritage through the creation of dance regalia, dancing, and teaching others about the Native American culture."

Some people in the transgender community have protested against the casting of straight, cis-gender actors in trans acting roles, such as when Eddie Redmayne played the role of artist Lili Elbe in the film "The Danish Girl" and when Jared Leto played the role of a trans woman named Rayon in "Dallas Buyers Club". Some in the gay community have expressed concerns about the use of straight actors to play gay characters; this occurs in films such as "Call Me by Your Name" (straight actors Armie Hammer and Timothée Chalamet), "Brokeback Mountain" (Heath Ledger and Jake Gyllenhaal), "Philadelphia" (Tom Hanks), "Capote" (Philip Seymour Hoffman) and "Milk" (with Sean Penn playing the role of the real-life gay rights activist, Harvey Milk). Jay Caruso calls these controversies "wholly manufactured", on the grounds that the actors "are playing a role" using the "art of acting".

In some cases, a culture usually viewed as the target of cultural appropriation can be accused of appropriation, particularly after colonization and an extensive period re-organization of that culture under the nation-state system. For example, the government of Ghana has been accused of cultural appropriation in adopting the Caribbean Emancipation Day and marketing it to African American tourists as an "African festival".

For some members of the South-Asian community, the wearing of a bindi dot as a decorative item, by a non-Hindu, or by a woman who is not South Asian, is considered cultural appropriation.

A common term among Irish people for someone who imitates or misrepresents Irish culture is "Plastic Paddy".

In 2003, Prince Harry of the British royal family used Indigenous Australian art motifs in a painting for a school project. One Aboriginal group labelled it "misappropriation of our culture", saying that to Aboriginal people, the motifs have symbolic meanings "indicative of our spiritualism", whereas when non-Aborigines use the motifs they are simply "painting a pretty picture".

In the Victoria's Secret Fashion Show 2012, former Victoria's Secret model Karlie Kloss donned a Native American-style feathered headdress with leather bra and panties and high-heeled moccasins. This was said to be an example of cultural appropriation because the fashion show is showcasing the company's lingerie and image as a global fashion giant. The outfit was supposed to represent November, and thus "Thanksgiving", in the "Calendar Girls" segment. The outfit met with backlash and criticism as an appropriation of Native American culture and tradition. Victoria's Secret pulled it from the broadcast and apologized for its use. Kloss also commented on the decision by tweeting "I am deeply sorry if what I wore during the VS Show offended anyone. I support VS's decision to remove the outfit from the broadcast."

Avril Lavigne was cited by some as appropriating Japanese culture in her song "Hello Kitty". The song and music video depict Asian women dressed up in matching outfits and Lavigne eating Asian food while dressed in a pink tutu. Lavigne responded by stating "I love Japanese culture and I spend half of my time in Japan. I flew to Tokyo to shoot this video ... specifically for my Japanese fans, "with" my Japanese label, Japanese choreographers "and" a Japanese director "in" Japan." Feedback for Lavigne's song was favorable in Japan, but "[the] people who are blaming the artist for racism [were] non-Japanese."

When Selena Gomez wore the bindi during a performance, there was debate on her reasoning behind wearing the culture specific piece. Some viewed this as "casting her vote for Team India" but it was also viewed as misuse of the symbol as Gomez was seen as not supporting or relating the bindi to its origin of Hinduism, but furthering her own self-expression. In 2014, Pharrell Williams posed in a Native American war bonnet on the cover of "Elle" UK magazine, after much controversy and media surrounding the photo Williams apologized.

Actress Amandla Stenberg made a school-related video called "Don't Cash Crop on My Cornrows" about the use of black hairstyles and black culture by non-black people, accusing Katy Perry and Iggy Azalea of using "black culture as a way of being edgy and gaining attention". Stenberg later criticized Kylie Jenner for allegedly embracing African-American aesthetic values without addressing the issues that affect the community. The African-American hip hop artist Azealia Banks has also criticized Iggy Azalea "for failing to comment on 'black issues' despite capitalising on the appropriation of African American culture in her music." Banks has called Azalea a "wigger" and there have been "accusations of racism against Azalea" focused on her alleged "insensitivity to the complexities of race relations and cultural appropriation."
Activist Rachel Dolezal made headlines in 2015 when it was discovered that she was not African-American, as she had claimed. She is an American former civil rights activist known for being exposed as Caucasian while falsely claiming to be a black woman. Dolezal was president of the National Association for the Advancement of Colored People (NAACP) chapter in Spokane, Washington, from February 7, 2014 until June 15, 2015 when she resigned amid suspicion she had lied about nine alleged hate crimes against her. She received further public scrutiny when her white parents publicly stated that Dolezal was a white woman passing as black.

In 2017, in an interview with "Billboard" regarding her new image, Miley Cyrus criticized what she considered to be overly vulgar aspects of hip hop culture while expressing her admiration for the song "Humble" by Kendrick Lamar. This was met with backlash from people who felt Cyrus has a history of appropriating hip hop culture.

In June 2019, Kim Kardashian West was accused of cultural appropriation after announcing her shapewear line KIMONO, a pun on her first name. Critics argued that her goal to trademark the term was disrespectful to Japanese culture, as her filing for the international trademarking of the term "kimono" would prevent Japanese people from selling their traditional garb under the same name. The issue sparked the emergence of a Twitter hashtag, #KimOhNo, where Japanese people - both native and diaspora - tried to explain the cultural significance the kimono has in their culture. Additionally, a petition was started on Change.org, hoping to prevent West from gaining the trademark. The petitioner, Sono Fukunishi, argued that allowing West to obtain the trademark would allow foreigners to begin associating the original kimono with underwear. West later received a letter from Kyoto's mayor, Daisaku Kadokawa, asking her to reconsider trademarking the name. Kyoto is considered to be the kimono capital of the world. West has since announced to rebrand her shapewear under the name SKIMS.

In 2011, a group of students at Ohio University started a poster campaign denouncing the use of cultural stereotypes as costumes. The campaign features people of color alongside their respective stereotypes with slogans such as "This is not who I am and this is not okay." The goal of the movement was to raise awareness around racism during Halloween in the university and the surrounding community, but the images also circulated online.

"Reclaim the Bindi" has become a hashtag used by some people of South Asian descent who wear traditional garb, and object to its use by people not of their culture. At the 2014 Coachella festival one of the most noted fashion trends was the bindi, a traditional Hindu head mark. As pictures of the festival surfaced online there was public controversy over the casual wearing of the bindi by non-Indian individuals who did not understand the meaning behind it. #CoachellaShutdown has been used in conjunction with #ReclaimtheBindi in order to protest against the use of the bindi at music festivals, most notably the Coachella Valley Music and Arts Festival. Reclaim the Bindi Week is an event which seeks to promote the traditional cultural significance of the bindi and combat its use as a fashion statement.

John McWhorter, a professor at Columbia University, has criticized the concept, arguing that cultural borrowing and cross-fertilization is a generally positive thing, and is something which is usually done out of admiration, and with no intent to harm, the cultures being imitated; he also argued that the specific term "appropriation", which can mean theft, is misleading when applied to something like culture that is not seen by all as a limited resource: unlike appropriating a physical object, others imitating an idea taken from one group's culture don't inherently deprive that originating group of its use.

In 2016, author Lionel Shriver gave a speech at the Brisbane Writers Festival, asserting the right of authors to write from any point of view, including that of characters from cultural backgrounds other than their own – as writers "should be seeking to push beyond the constraining categories into which we have been arbitrarily dropped by birth. If we embrace narrow group-based identities too fiercely, we cling to the very cages in which others would seek to trap us." She also asserted the right of authors from a cultural majority to write in the voice of someone from a cultural minority, attacking the idea that this constitutes unethical "cultural appropriation". Referring to a case in which U.S. college students were facing disciplinary action for wearing sombreros to a 'tequila party', she said "The moral of the sombrero scandals is clear: "you're not supposed to try on other people's hats". Yet that's what we're paid to do, isn't it? Step into other people's shoes, and try on their hats."

In 2017, Canadian clinical psychologist, author, and professor of psychology at the University of Toronto Jordan Peterson stated in a Q&A session from a speech entitled "Strengthen the Individual", "The idea of cultural appropriation is nonsense, and that's that. There's no difference between cultural appropriation and learning from each other. They're the same thing. Now, that doesn't mean that there's no theft between people; there is. And it doesn't mean that once you encounter someone else's ideas, you have an absolute right to those ideas as if they're your own. But the idea that manifesting some element of another culture in your own behavior is immoral is insane. It's actually one of the bases of peace."

Upon winning the 2019 Booker Prize, Bernardine Evaristo dismissed the concept of cultural appropriation, stating that it is ridiculous to demand of writers that they not “write beyond your own culture.”

Ethics columnist for the New York Times, Kwame Anthony Appiah wrote that "Specific instances of what people criticize as cultural appropriation may well be wrong, but the term encourages us to call out a property crime when something else is going on. If I take a practice that is freighted with significance for some group and mock it or trivialize it, that’s contempt. (That’s why it’s usually not a good idea to wear, say, clerical garb because you like the look.) The key question in the use of symbols or regalia associated with another identity group is not: What are my rights of ownership? Rather it’s: Are my actions disrespectful?"

Others criticize it as a stumbling block for the adaptation of differing cultural elements or even discussion for fear of being labeled "racist" or "ignorant".


</doc>
<doc id="58929051" url="https://en.wikipedia.org/wiki?curid=58929051" title="Reshaping Cultural Policies">
Reshaping Cultural Policies

Reshaping Cultural Policies (styled as Re|Shaping Cultural Policies) is a report series published by UNESCO which monitors the implementation of the UNESCO Convention on the Protection and Promotion of the Diversity of Cultural Expressions (2005). The 2005 UNESCO Convention encourages its 146 parties to introduce policies for culture within a global context and commitment to protect and promote the diversity of cultural expressions. The second and most recent report (2018) subtitled “Advancing Creativity for Development” follows the first report (2015) with the subtitle “A Decade Promoting the Diversity of Cultural Expressions for Development”.

Primarily, the report series draws on reports of all parties to the Convention submitted every four years in which they present and describe the actions they have taken in order to implement the Convention. These reports are called quadrennial periodic reports (QPRs). In addition, the report series includes the analysis of other both governmental and non-governmental sources. In general, the report investigates how implementing the convention reshapes cultural policies. Additionally, it provides evidence of how the implementation process contributes to attaining the United Nations 2030 Sustainable Development Goals (SDGs) to end poverty, protect the planet, and ensure prosperity for every human being. The report series also analyses trends and issues concerning the creative economy, which currently is worth $2,250 billion and employs 30 million people worldwide.

The report puts forward a set of policy recommendations for the future, addressing the adaptation of cultural policies to rapid change in the digital environment, based on human rights and fundamental freedoms of expression. 

“Each Report is not an end-result, but a tool to be used in a long-term process that includes the forging of spaces for policy dialogue, reinforcing stakeholders’ capacities to work together to generate data and information, and advocate for policy innovation both nationally and globally.” 

The reports are published in English, French, Spanish, Russian, Portuguese, Arabic, Chinese, Indonesian, Vietnamese and German. UNESCO is the lead institutional author of the Global Report series and coordinates a broader network of independent experts who author chapters. 

In line with the Parties’ quadrennial periodic reporting, the series is produced every four years. The first cycle spanned the years 2012-2015 and the second runs from 2016 to 2019. Accordingly, the third publication will take place in December 2021.

The 2005 Convention is an international standard setting instrument providing a framework for the governance of culture. In this context, governance of culture refers to policies and measures governments establish to regulate, to promote and to protect all forms of creativity and artistic expressions. The most recent UNESCO Convention in the field of culture and ratified by 146 parties, it is the first international legal tool to encourage governments to invest in creativity. It frames the formulation and implementation of different types of legislative, regulatory, institutional and financial interventions to promote the emergence of dynamic cultural and creative industry sectors around the world.

Within the context of the 2005 Convention, the diversity of cultural expressions "″"refers to the manifold ways in which the cultures of groups and societies find expression. These expressions are passed on within and among groups and societies"″". Specifically, the Convention understands cultural expressions as all forms of creativity and artistic expressions, such as in cinema/audiovisual arts, design, digital arts, music, performing arts, publishing and the visual arts. The 2005 Convention was "since its beginnings, permeated by a material and economic perspective of cultural expressions, focused on the production and consumption of cultural goods and services, with a view to promote more balanced exchanges and sustainable development that takes into account cultural diversity concerns." 

The implementation of the 2005 Convention aims to contribute to achieving several Sustainable Development Goals (SDGs), precisely SDG 4 (Quality Education), SDG 5 (Gender Equality), SDG 8 (Decent Work and Economic Growth), SDG 10 (Reduced Inequalities), SDG 16 (Peace, Justice and Strong Institutions) and SDG 17 (Partnerships for the Goals). The implementation process identifies investing in creativity as a priority for sustainable development. At the global level, the convention calls for countries to provide financial assistance for creativity through their Official Development Assistance (ODA) by investing in the Convention’s International Fund for Cultural Diversity. Additionally, UNESCO, through the 2005 Convention, offers technical assistance to strengthen human and institutional capacities in developing countries.

The director general of UNESCO, Audrey Azoulay referring to the UNESCO General Conference's conviction that cultural activities, goods and services have both an economic and a cultural nature stated: ″[c]ulture is not a commodity: it carries values and identities, it gives markers to live together in a globalized world. Our role is to encourage, question, collect data, to understand and energize creative channels, to encourage the mobility of artists, to stimulate a rapidly changing sector in the new digital environment″. Annika Markovic, Ambassador and Permanent Delegate of Sweden to UNESCO, in 2018, claimed that the report is "“the only global document that presents an overview of cultural development world-wide and monitors state action to protect and promote the diversity of cultural expressions at all levels.”" 

The following aspects thematically summarize the core findings identified by the 2018 report with regard to the implementation of the 2005 UNESCO Convention.

For the first time, national development plans and strategies integrate culture, mainly of countries in the Global South. As a result, cities seem to invest more and more in cultural industries for development. The UN’s 2030 Agenda recognized the role of creativity in sustainable development in the implementation of the SDGs. However, the share of development aid spent on culture today is the lowest it has been in over 10 years. 

According to the report, digital revenues make up 50% of the recorded music market, growing almost 18% over the past year due to a sharp increase in the share of streaming revenues. 

The report states that the internet transforms the cultural value chain into a network platform. E-commerce challenges both culture and trade policies that intend to promote the diversity of cultural expressions. It articulates the urgency to improve data collection on revenues generated through digital channels in order to design better policies and negotiate fair trade agreements. The report claims that monitoring the relationship between large platforms, Big Data, artificial intelligence and the diversity of cultural expressions is crucial to ensure that a variety of distribution platforms and providers promote and protect future artistic creations. 

As informed by the report, attacks against artists have increased in the past years, including in the digital environment where surveillance and online trolling pose new threats to artistic freedom. In 2016, 430 cases were reported around the world (compared to 340 in 2015 and 90 in 2014). Musicians are the most threatened group, while authors also often become a target. In 2016, attacks against authors occurred most often in the Asia-Pacific Region (80 cases), the Middle East and North Africa (51 cases) and Europe (47 cases). The report reveals that meanwhile, there exists an increased awareness with regard to such threats leading to a larger number of initiatives to support the social and economic rights of artists, particularly in African countries. While there exists legal action to affirm the freedom of expression for artists, other laws addressing terrorism and state security repress artistic expressions. 

The report states that half of the persons working in the cultural and creative industries are female. However, a gender gap persists worldwide concerning equal pay, access to funding and prices charged for creative works. Consequently, women remain under-represented in key creative roles and are outnumbered in decision-making positions. Women make up only 34% of Ministers for Culture (compared to 24% in 2005) and only 31% of national arts program directors. Generally, women are represented in specific cultural fields such as arts education and training (60%), book publishing and press (54%), audiovisual and interactive media (26%), as well as design and creative services (33%).

The report demonstrates that predominantly restrictions in terms of mobility represent great challenges to persons pursuing careers in the cultural and creative industries, specifically to those from the Global South. It reveals that a holder of a German passport can travel to 176 countries without a visa while a holder of an Afghan passport can only travel to 24 countries without a visa. As a matter of fact, artists and cultural professionals need to travel to perform, to reach new audiences or to attend a residency or to engage in networking. The report exposes that travel restrictions, including difficulties in obtaining visas oftentimes impedes artists from the Global South to participate in art biennales or film festivals, even when invited to receive an award or to promote their works. 

As stated in the report, the 2005 Convention provides legitimacy for the formulation of cultural policies and their adaptation to changing circumstances and needs. The report underscores that collaborative governance and multi-stakeholder policy making have progressed, notably in some developing countries particularly in the creative economy and cultural education. As a result, parties to the Convention have made considerable progress in fostering digital arts creation, supporting creative entrepreneurship, accelerating the modernization of cultural sectors, promoting distribution and updating copyright legislation. However, the report also reveals a lack in civil society participation in policy making. It underlines the urgency for more effort to ensure the creation of open, transparent and participatory policy processes in order to involve civil society participation in policy making. 

In accordance with the report, the 2005 Convention formally recognizes that cultural goods and services not only have important economic value, but also convey identities, meanings and values. As a consequence, at least eight bilateral and regional free trade agreements concluded between 2015 and 2017 have introduced cultural clauses or list of commitments that promote the objectives and principles of the 2005 Convention. Despite the lack of the promotion of the objectives and principles of the 2005 Convention with regard to the negotiation of mega-regional partnership agreements, some Parties to the Trans Pacific Partnership (TTP) have succeeded in introducing important cultural reservations to protect and promote the diversity of cultural expressions. 

The report’s primary objective is “to provide key actors with better knowledge on how to support evidence-based policy, and to strengthen informed, transparent and participatory systems of governance for culture.” It aims to motivate governments and civil society actors to integrate findings and recommendations into their national cultural policy and development strategies and frameworks. 

Following the findings presented above, the implementation of the 2005 Convention "introduce[s] a range of different policy strategies for integrating culture into development processes" and culture is increasingly regarded as "an economic asset in pursuing sustainable development". Based on its analysis and findings, the Global Report of 2018 suggests the following road map for the parties to the 2005 Convention. Accordingly, parties could tackle major challenges in the implementation of the 2005 Convention by:


Speaking about the visibility of the progress in cultural policies shown by the report series, Bárbara Lovrinić stated that "“[u]nfortunately, where UNESCO is concerned, there is a lack of promotion in the media in general. In the long term, the report could have a positive impact on these issues, which would be enhanced if the public were made more aware of such work.”" She also points out that there is ""a risk that many people will not dwell on the 2005 Convention and the Sustainable Development Goals unless they are already somewhat familiar with the topic.”" With reference to the title of the report series, she concludes that ""cultural policy-making is still far from being reshaped, for it takes a serious amount of time to yield valuable results.”"




</doc>
<doc id="13598464" url="https://en.wikipedia.org/wiki?curid=13598464" title="Hypermobility (travel)">
Hypermobility (travel)

Hypermobile travelers are "highly mobile individuals" who take "frequent trips, often over great distances." They "account for a large share of the overall kilometres travelled, especially by air." These people contribute significantly to the overall amount of airmiles flown within a given society. Although concerns over hypermobility apply to several modes of transport, the environmental impact of aviation and especially its greenhouse gas emissions have brought particular focus on flying. Among the reasons for this focus is that these emissions, because they are made at high altitude, have a climate impact that is commonly estimated to be 2.7 higher than the same emissions if made at ground-level.

Although the amount of time people have spent in motion has remained constant since 1950, the shift from feet and bicycles to cars and planes has increased the speed of travel fivefold. This results in the twin effects of wider and shallower regions of social activity around each person (further exacerbated by electronic communication which can be seen as a form of virtual mobility), and a degradation of the social and physical environment brought about by the high speed traffic (as theorised by urban designer Donald Appleyard).

The changes are brought about locally due to the use of cars and motorways, and internationally by aeroplanes. Some of the social threats of hypermobility include:

The addictive properties of hypermobile travel have been noted by researchers.

Widespread Internet use is seen as a contributory factor towards hypermobility due to the increased ease which it enables travel to be desired and organized. To the extent that the Internet stimulates travel, it represents a lost opportunity to reduce overall emissions because online communication is a straightforward substitute for physical travel.

The term hypermobility arose around 1980 concerning the flow of capital, and since the early 1990s has also referred to excessive travel. ["See:" Hepworth and Ducatel (1992); Whitelegg (1993); Lowe (1994); van der Stoep (1995); Shields (1996); Cox (1997); Adams (1999); Khisty and Zeitler (2001); Gössling et al. (2009); Mander & Randles (2009); and (Higham 2014).] The term is widely credited as having been coined by Adams (1999), but apart from the title of the work it says nothing explicit about it except that "[t]he term hypermobility is used in this essay to suggest that it may be possible to have too much of a good thing."



</doc>
<doc id="6267" url="https://en.wikipedia.org/wiki?curid=6267" title="Cultural imperialism">
Cultural imperialism

Cultural imperialism, also called cultural colonialism, comprises the cultural aspects of imperialism. "Imperialism" here refers to the creation and maintenance of unequal relationships between civilizations, favoring a more powerful civilization. Thus, the cultural imperialism is the practice of promoting and imposing a culture, usually that of a politically powerful nation, over a less powerful society; in other words, the cultural hegemony of industrialized or politically and economically influential countries which determine general cultural values and standardize civilizations throughout the world. The term is employed especially in the fields of history, cultural studies, and postcolonial theory. It is usually used in a pejorative sense, often in conjunction with calls to reject such influence. Cultural imperialism may take various forms, such as an attitude, a formal policy, or military action, insofar as it reinforces cultural hegemony.

Although the "Oxford English Dictionary" has a 1921 reference to the "cultural imperialism of the Russians", John Tomlinson, in his book on the subject, writes that the term emerged in the 1960s and has been a focus of research since at least the 1970s. Terms such as "media imperialism", "structural imperialism", "cultural dependency and domination", "cultural synchronization", "electronic colonialism", "ideological imperialism", and "economic imperialism" have all been used to describe the same basic notion of cultural imperialism.

Various academics give various definitions of the term. American media critic Herbert Schiller wrote: "The concept of cultural imperialism today [1975] best describes the sum of the processes by which a society is brought into the modern world system and how its dominating stratum is attracted, pressured, forced, and sometimes bribed into shaping social institutions to correspond to, or even promote, the values and structures of the dominating centre of the system. The public media are the foremost example of operating enterprises that are used in the penetrative process. For penetration on a significant scale the media themselves must be captured by the dominating/penetrating power. This occurs largely through the commercialization of broadcasting."

Tom McPhail defined "Electronic colonialism as the dependency relationship established by the importation of communication hardware, foreign-produced software, along with engineers, technicians, and related information protocols, that vicariously establish a set of foreign norms, values, and expectations which, in varying degrees, may alter the domestic cultures and socialization processes." Paul Sui-Nam Lee observed that "communication imperialism can be defined as the process in which the ownership and control over the hardware and software of mass media as well as other major forms of communication in one country are singly or together subjugated to the domination of another country with deleterious effects on the indigenous values, norms and culture." Ogan saw "media imperialism often described as a process whereby the United States and Western Europe produce most of the media products, make the first profits from domestic sales, and then market the products in Third World countries at costs considerably lower than those the countries would have to bear to produce similar products at home."

Downing and Sreberny-Mohammadi state: "Imperialism is the conquest and control of one country by a more powerful one. Cultural imperialism signifies the dimensions of the process that go beyond economic exploitation or military force. In the history of colonialism, (i.e., the form of imperialism in which the government of the colony is run directly by foreigners), the educational and media systems of many Third World countries have been set up as replicas of those in Britain, France, or the United States and carry their values. Western advertising has made further inroads, as have architectural and fashion styles. Subtly but powerfully, the message has often been insinuated that Western cultures are superior to the cultures of the Third World."
Needless to say, all these authors agree that cultural imperialism promotes the interests of certain circles within the imperial powers, often to the detriment of the target societies.

The issue of cultural imperialism emerged largely from communication studies. However, cultural imperialism has been used as a framework by scholars to explain phenomena in the areas of international relations, anthropology, education, science, history, literature, and sports.

Many of today's academics that employ the term, "cultural imperialism," are heavily informed by the work of Foucault, Derrida, Said, and other poststructuralist and postcolonialist theorists. Within the realm of postcolonial discourse, "cultural imperialism" can be seen as the cultural legacy of colonialism, or forms of social action contributing to the continuation of Western hegemony. To some outside of the realm of this discourse, the term is critiqued as being unclear, unfocused, and/or contradictory in nature.

The work of French philosopher and social theorist Michel Foucault has heavily influenced use of the term "cultural imperialism," particularly his philosophical interpretation of power and his concept of governmentality.

Following an interpretation of power similar to that of Machiavelli, Foucault defines power as immaterial, as a "certain type of relation between individuals" that has to do with complex strategic social positions that relate to the subject's ability to control its environment and influence those around itself. According to Foucault, power is intimately tied with his conception of truth. "Truth", as he defines it, is a "system of ordered procedures for the production, regulation, distribution, circulation, and operation of statements" which has a "circular relation" with systems of power. Therefore, inherent in systems of power, is always "truth", which is culturally specific, inseparable from ideology which often coincides with various forms of hegemony. "Cultural imperialism" may be an example of this.

Foucault's interpretation of governance is also very important in constructing theories of transnational power structure. In his lectures at the Collège de France, Foucault often defines governmentality as the broad art of "governing", which goes beyond the traditional conception of governance in terms of state mandates, and into other realms such as governing "a household, souls, children, a province, a convent, a religious order, a family". This relates directly back to Machiavelli's treatise on how to retain political power at any cost, "The Prince", and Foucault's aforementioned conceptions of truth and power. (i.e. various subjectivities are created through power relations that are culturally specific, which lead to various forms of culturally specific governmentality such as neoliberal governmentality.)

Informed by the works of Noam Chomsky, Foucault, and Antonio Gramsci, Edward Saïd is a founding figure of postcolonialism, established with the book "Orientalism" (1978), a humanist critique of The Enlightenment, which criticizes Western knowledge of "The East"—specifically the English and the French constructions of what is and what is not "Oriental". Whereby said "knowledge" then led to cultural tendencies towards a binary opposition of the Orient vs. the Occident, wherein one concept is defined in opposition to the other concept, and from which they emerge as of unequal value. In "Culture and Imperialism" (1993), the sequel to "Orientalism", Saïd proposes that, despite the formal end of the “age of empire” after the Second World War (1939–45), colonial imperialism left a cultural legacy to the (previously) colonized peoples, which remains in their contemporary civilizations; and that said "cultural imperialism" is very influential in the international systems of power.

A self-described "practical Marxist-feminist-deconstructionist" Gayatri Chakravorty Spivak has published a number of works challenging the "legacy of colonialism" including "A Critique of Postcolonial Reason: Towards a History of the Vanishing Present" (1999), "Other Asias" (2005), and "Can the Subaltern Speak?" (1988).

In "Can the Subaltern Speak?" Spivak critiques common representations in the West of the Sati, as being controlled by authors other than the participants (specifically English colonizers and Hindu leaders). Because of this, Spivak argues that the subaltern, referring to the communities that participate in the Sati, are not able to represent themselves through their own voice. Spivak says that cultural imperialism has the power to disqualify or erase the knowledge and mode of education of certain populations that are low on the social hierarchy.

Throughout "Can the Subaltern Speak?", Spivak cites the works of Karl Marx, Michel Foucault, Walter Benjamin, Louis Althusser, Jacques Derrida, and Edward Said, among others.

In "A Critique of Postcolonial Reason", Spivak argues that Western philosophy has a history of not only exclusion of the subaltern from discourse, but also does not allow them to occupy the space of a fully human subject.

"Cultural imperialism" can refer to either the forced acculturation of a subject population, or to the voluntary embracing of a foreign culture by individuals who do so of their own free will. Since these are two very different referents, the validity of the term has been called into question.

Cultural influence can be seen by the "receiving" culture as either a threat to or an enrichment of its cultural identity. It seems therefore useful to distinguish between cultural imperialism as an (active or passive) attitude of superiority, and the position of a culture or group that seeks to complement its own cultural production, considered partly deficient, with imported products.

The imported products or services can themselves represent, or be associated with, certain values (such as consumerism). According to one argument, the "receiving" culture does not necessarily perceive this link, but instead absorbs the foreign culture passively through the use of the foreign goods and services. Due to its somewhat concealed, but very potent nature, this hypothetical idea is described by some experts as ""banal imperialism"." For example, it is argued that while "American companies are accused of wanting to control 95 percent of the world's consumers", "cultural imperialism involves much more than simple consumer goods; it involved the dissemination of American principles such as freedom and democracy", a process which "may sound appealing" but which "masks a frightening truth: many cultures around the world are disappearing due to the overwhelming influence of corporate and cultural America".

Some believe that the newly globalised economy of the late 20th and early 21st century has facilitated this process through the use of new information technology. This kind of cultural imperialism is derived from what is called "soft power". The theory of electronic colonialism extends the issue to global cultural issues and the impact of major multi-media conglomerates, ranging from Viacom, Time-Warner, Disney, News Corp, to Google and Microsoft with the focus on the hegemonic power of these mainly United States-based communication giants.

One of the reasons often given for opposing any form of cultural imperialism, voluntary or otherwise, is the preservation of cultural diversity, a goal seen by some as analogous to the preservation of ecological diversity. Proponents of this idea argue either that such diversity is valuable in itself, to preserve human historical heritage and knowledge, or instrumentally valuable because it makes available more ways of solving problems and responding to catastrophes, natural or otherwise.

Of all the areas of the world that scholars have claimed to be adversely affected by imperialism, Africa is probably the most notable. In the expansive "age of imperialism" of the nineteenth century, scholars have argued that European colonization in Africa has led to the elimination of many various cultures, worldviews, and epistemologies, particularly through neocolonization of public education. This, arguably has led to uneven development, and further informal forms of social control having to do with culture and imperialism. A variety of factors, scholars argue, lead to the elimination of cultures, worldviews, and epistemologies, such as "de-linguicization" (replacing native African languages with European ones), devaluing ontologies that are not explicitly individualistic, and at times going as far as to not only define Western culture itself as science, but that non-Western approaches to science, the Arts, indigenous culture, etc. are not even knowledge. One scholar, Ali A. Abdi, claims that imperialism inherently "involve[s] extensively interactive regimes and heavy contexts of identity deformation, misrecognition, loss of self-esteem, and individual and social doubt in self-efficacy." Therefore, all imperialism would always, already be cultural.

Neoliberalism is often critiqued by sociologists, anthropologists, and cultural studies scholars as being culturally imperialistic. Critics of neoliberalism, at times, claim that it is the newly predominant form of imperialism. Other Scholars, such as Elizabeth Dunn and Julia Elyachar have claimed that neoliberalism requires and creates its own form of governmentality.

In Dunn's work, "Privatizing Poland", she argues that the expansion of the multinational corporation, Gerber, into Poland in the 1990s imposed Western, neoliberal governmentality, ideologies, and epistemologies upon the post-soviet persons hired. Cultural conflicts occurred most notably the company's inherent individualistic policies, such as promoting competition among workers rather than cooperation, and in its strong opposition to what the company owners claimed was bribery.

In Elyachar's work, "Markets of Dispossession", she focuses on ways in which, in Cairo, NGOs along with INGOs and the state promoted neoliberal governmentality through schemas of economic development that relied upon "youth microentrepreneurs." Youth microentrepreneurs would receive small loans to build their own businesses, similar to the way that microfinance supposedly operates. Elyachar argues though, that these programs not only were a failure, but that they shifted cultural opinions of value (personal and cultural) in a way that favored Western ways of thinking and being.

Often, methods of promoting development and social justice to are critiqued as being imperialistic, in a cultural sense. For example, Chandra Mohanty has critiqued Western feminism, claiming that it has created a misrepresentation of the "third world woman" as being completely powerless, unable to resist male dominance. Thus, this leads to the often critiqued narrative of the "white man" saving the "brown woman" from the "brown man." Other, more radical critiques of development studies, have to do with the field of study itself. Some scholars even question the intentions of those developing the field of study, claiming that efforts to "develop" the Global South were never about the South itself. Instead, these efforts, it is argued, were made in order to advance Western development and reinforce Western hegemony.

The core of cultural imperialism thesis is integrated with the political-economy traditional approach in media effects research. Critics of cultural imperialism commonly claim that non-Western cultures, particularly from the Third World, will forsake their traditional values and lose their cultural identities when they are solely exposed to Western media. Nonetheless, Michael B. Salwen, in his book "Critical Studies in Mass Communication" (1991), claims that cross-consideration and integration of empirical findings on cultural imperialist influences is very critical in terms of understanding mass media in the international sphere. He recognizes both of contradictory contexts on cultural imperialist impacts. 
The first context is where cultural imperialism imposes socio-political disruptions on developing nations. Western media can distort images of foreign cultures and provoke personal and social conflicts to developing nations in some cases. 
Another context is that peoples in developing nations resist to foreign media and preserve their cultural attitudes. Although he admits that outward manifestations of Western culture may be adopted, but the fundamental values and behaviors remain still. Furthermore, positive effects might occur when male-dominated cultures adopt the “liberation” of women with exposure to Western media and it stimulates ample exchange of cultural exchange.

Critics of scholars who discuss cultural imperialism have a number of critiques. "Cultural imperialism" is a term that is only used in discussions where cultural relativism and constructivism are generally taken as true. (One cannot critique promoting Western values if one believes that said values are absolutely correct. Similarly, one cannot argue that Western epistemology is unjustly promoted in non-Western societies if one believes that those epistemologies are absolutely correct.) Therefore, those who disagree with cultural relativism and/or constructivism may critique the employment of the term, "cultural imperialism" on those terms.

John Tomlinson provides a critique of cultural imperialism theory and reveals major problems in the way in which the idea of cultural, as opposed to economic or political, imperialism is formulated. In his book "Cultural Imperialism: A Critical Introduction", he delves into the much debated “media imperialism” theory. Summarizing research on the Third World's reception of American television shows, he challenges the cultural imperialism argument, conveying his doubts about the degree to which US shows in developing nations actually carry US values and improve the profits of US companies. Tomlinson suggests that cultural imperialism is growing in some respects, but local transformation and interpretations of imported media products propose that cultural diversification is not at an end in global society. He explains that one of the fundamental conceptual mistakes of cultural imperialism is to take for granted that the distribution of cultural goods can be considered as cultural dominance. He thus supports his argument highly criticizing the concept that Americanization is occurring through global overflow of American television products. He points to a myriad of examples of television networks who have managed to dominate their domestic markets and that domestic programs generally top the ratings. He also doubts the concept that cultural agents are passive receivers of information. He states that movement between cultural/geographical areas always involves translation, mutation, adaptation, and the creation of hybridity.

Other key critiques are that the term is not defined well, and employs further terms that are not defined well, and therefore lacks explanatory power, that "cultural imperialism" is hard to measure, and that the theory of a legacy of colonialism is not always true.

David Rothkopf, managing director of Kissinger Associates and an adjunct professor of international affairs at Columbia University (who also served as a senior U.S. Commerce Department official in the Clinton Administration), wrote about cultural imperialism in his provocatively titled "In Praise of Cultural Imperialism?" in the summer 1997 issue of "Foreign Policy" magazine. Rothkopf says that the United States should embrace "cultural imperialism" as in its self-interest. But his definition of cultural imperialism stresses spreading the values of tolerance and openness to cultural change in order to avoid war and conflict between cultures as well as expanding accepted technological and legal standards to provide free traders with enough security to do business with more countries. Rothkopf's definition almost exclusively involves allowing individuals in other nations to accept or reject foreign cultural influences. He also mentions, but only in passing, the use of the English language and consumption of news and popular music and film as cultural dominance that he supports. Rothkopf additionally makes the point that globalization and the Internet are accelerating the process of cultural influence.

Culture is sometimes used by the organizers of society—politicians, theologians, academics, and families—to impose and ensure order, the rudiments of which change over time as need dictates. One need only look at the 20th century's genocides. In each one, leaders used culture as a political front to fuel the passions of their armies and other minions and to justify their actions among their people.

Rothkopf then cites genocide and s in Armenia, Russia, the Holocaust, Cambodia, Bosnia and Herzegovina, Rwanda and East Timor as examples of culture (in some cases expressed in the ideology of "political culture" or religion) being misused to justify violence. He also acknowledges that cultural imperialism in the past has been guilty of forcefully eliminating the cultures of natives in the Americas and in Africa, or through use of the Inquisition, "and during the expansion of virtually every empire.".The most important way to deal with cultural influence in any nation, according to Rothkopf, is to promote tolerance and allow, or even promote, cultural diversities that are compatible with tolerance and to eliminate those cultural differences that cause violent conflict:

Cultural dominance can also be seen in the 1930s in Australia where the Aboriginal Assimilation Policy acted as an attempt to wipe out the Native Australian people. The British settlers tried to biologically alter the skin colour of the Australian Aboriginal people through mixed breeding with white people. The policy also made attempts to forcefully conform the Aborigines to western ideas of dress and education.

Although the term was popularized in the 1960s, and was used by its original proponents to refer to cultural hegemonies in a post-colonial world, cultural imperialism has also been used to refer to times further in the past.

The Ancient Greeks are known for spreading their culture around the Mediterranean and Near East through trade and conquest. During the Archaic Period, the burgeoning Greek city-states established settlements and colonies across the Mediterranean Sea, especially in Sicily and southern Italy, influencing the Etruscan and Roman peoples of the region. In the late fourth century BC, Alexander the Great conquered Persian and Indian territories all the way to the Indus River Valley and Punjab, spreading Greek pagan religion, art, and science along the way. This resulted in the rise of Hellenistic kingdoms and cities across Egypt, the Near East, Central Asia, and Northwest India where Greek culture fused with the cultures of the indigenous peoples. The Greek influence prevailed even longer in science and literature, where medieval Muslim scholars in the Middle East studied the writings of Aristotle for scientific learning.

The Roman Empire was also an early example of cultural imperialism.

Early Rome, in its conquest of Italy, assimilated the people of Etruria by replacing the Etruscan language with Latin, which led to the demise of that language and many aspects of Etruscan civilization.

Cultural Romanization was imposed on many parts of Rome's empire by "many regions receiving Roman culture unwillingly, as a form of cultural imperialism." For example, when Greece was conquered by the Roman armies, Rome set about altering the culture of Greece to conform with Roman ideals. For instance, the Greek habit of stripping naked, in public, for exercise, was looked on askance by Roman writers, who considered the practice to be a cause of the Greeks' effeminacy and enslavement. The Roman example has been linked to modern instances of European imperialism in African countries, bridging the two instances with Slavoj Zizek's discussions of 'empty signifiers'.

The Pax Romana was secured in the empire, in part, by the "forced acculturation of the culturally diverse populations that Rome had conquered."

British worldwide expansion in the 18th and 19th centuries was an economic and political phenomenon. However, "there was also a strong social and cultural dimension to it, which Rudyard Kipling termed the 'white man's burden'." One of the ways this was carried out was by religious proselytising, by, amongst others, the London Missionary Society, which was "an agent of British cultural imperialism." Another way, was by the imposition of educational material on the colonies for an "imperial curriculum". Robin A. Butlin writes, "The promotion of empire through books, illustrative materials, and educational syllabuses was widespread, part of an education policy geared to cultural imperialism". This was also true of science and technology in the empire. Douglas M. Peers and Nandini Gooptu note that "Most scholars of colonial science in India now prefer to stress the ways in which science and technology worked in the service of colonialism, as both a 'tool of empire' in the practical sense and as a vehicle for cultural imperialism. In other words, science developed in India in ways that reflected colonial priorities, tending to benefit Europeans at the expense of Indians, while remaining dependent on and subservient to scientific authorities in the colonial metropolis."

The analysis of cultural imperialism carried out by Edward Said drew principally from a study of the British Empire. According to Danilo Raponi, the cultural imperialism of the British in the 19th century had a much wider effect than only in the British Empire. He writes, "To paraphrase Said, I see cultural imperialism as a complex cultural hegemony of a country, Great Britain, that in the 19th century had no rivals in terms of its ability to project its power across the world and to influence the cultural, political and commercial affairs of most countries. It is the 'cultural hegemony' of a country whose power to export the most fundamental ideas and concepts at the basis of its understanding of 'civilisation' knew practically no bounds." In this, for example, Raponi includes Italy.

The New Cambridge Modern History writes about the cultural imperialism of Napoleonic France. Napoleon used the Institut de France "as an instrument for transmuting French universalism into cultural imperialism." Members of the Institute (who included Napoleon), descended upon Egypt in 1798. "Upon arrival they organised themselves into an Institute of Cairo. The Rosetta Stone is their most famous find. The science of Egyptology is their legacy."

After the First World War, Germans were worried about the extent of French influence in the annexed Rhineland, with the French occupation of the Ruhr Valley in 1923. An early use of the term appeared in an essay by Paul Ruhlmann (as "Peter Hartmann") at that date, entitled "French Cultural Imperialism on the Rhine".

Keeping in line with the trends of international imperialistic endeavours, the expansion of Canadian and American territory in the 19 century saw cultural imperialism employed as a means of control over indigenous populations. This, when used in conjunction of more traditional forms of ethnic cleansing and genocide, saw devastating, lasting effects on indigenous communities.

In 2017 Canada celebrated its 150-year anniversary of the confederating of three British colonies. As Catherine Murton Stoehr points out in Origins, a publication organised by the history departments of Ohio State University and Miami University, the occasion came with remembrance of Canada's treatment of First Nations people.

“A mere 9 years after the 1867 signing of confederation Canada passed “The Indian Act,” a separate and not equal form of government especially for First Nations. The Indian Act remains in place today, confining and constraining Indigenous jurisdiction in every area of life, in direct contravention of the nation's founding treaties with indigenous nations.”

Numerous policies focused on indigenous persons came into effect shortly after. Most notable is the use of residential schools across Canada as a means to remove indigenous persons from their culture and instill in them the beliefs and values of the majorized colonial hegemony. The policies of these schools, as described by Ward Churchillin his book "Kill the Indian, Save the Man", were to forcefully assimilate students who were often removed with force from their families. These schools forbid students from using their native languages and participating in their own cultural practices.

Residential schools were largely run by Christian churches, operating in conjunction with Christian missions with minimal government oversight.

The book, "Stolen lives: The Indigenous peoples of Canada and the Indian Residentials Schools", describes this form of operation.

“The government provided little leadership, and the clergy in charge were left to decide what to teach and how to teach it. Their priority was to impart the teachings of their church or order—not to provide a good education that could help students in their post-graduation lives.”

In a New York Times op-ed, Gabrielle Scrimshaw describes her grandparents being forced to send her mother to one of these schools or risk imprisonment. After hiding her mother on “school pick up day” so as to avoid sending their daughter to institutions whose abuse was well known at the time (mid 20century). Scrimshaw's mother was left with limited options for further education she says and is today illiterate as a result.

Scrimshaw explains “Seven generations of my ancestors went through these schools. Each new family member enrolled meant a compounding of abuse and a steady loss of identity, culture and hope. My mother was the last generation. the experience left her broken, and like so many, she turned to substances to numb these pains.”

A report, republished by CBC news, estimates nearly 6,000 children died in the care of these schools.

The colonization of native peoples in North America remains active today despite the closing of the majority of residential schools. This form of cultural imperialism continues in the use of Native Americans as mascots for schools and athletic teams. Jason Edward Black,  Ph.D., is a Professor and Chair in the Department of Communication Studies at the University of North Carolina at Charlotte. He describes how the use of Native Americans as mascots furthers the colonial attitudes of the 18and 19centuries in his article “The ‘Mascotting’ of Native America: Construction, Commodity, and Assimilation.” 

“Indigenous groups, along with cultural studies scholars, view the Native mascots as hegemonic devices–commodification tools–that advance a contemporary manifest destiny by marketing Native culture as Euromerican identity.”

In "Deciphering Pocahontas", Kent Ono and Derek Buescher describe it as this. “Euro-American culture has made a habit of appropriating, and redefining what is ‘distinctive’ and constitutive of Native Americans.”

"Cultural imperialism" has also been used in connection with the expansion of German influence under the Nazis in the middle of the twentieth century. Alan Steinweis and Daniel Rogers note that even before the Nazis came to power, "Already in the Weimar Republic, German academic specialists on eastern Europe had contributed through their publications and teaching to the legitimization of German territorial revanchism and cultural imperialism. These scholars operated primarily in the disciplines Of history, economics, geography, and literature."

In the area of music, Michael Kater writes that during the WWII German occupation of France, Hans Rosbaud, a German conductor based by the Nazi regime in Strasbourg, became "at least nominally, a servant of Nazi cultural imperialism directed against the French."

In Italy during the war, Germany pursued "a European cultural front that gravitates around German culture". The Nazi propaganda minister Joseph Goebbels set up the European Union of Writers, "one of Goebbels's most ambitious projects for Nazi cultural hegemony. Presumably a means of gathering authors from Germany, Italy, and the occupied countries to plan the literary life of the new Europe, the union soon emerged as a vehicle of German cultural imperialism."

For other parts of Europe, Robert Gerwarth, writing about cultural imperialism and Reinhard Heydrich, states that the "Nazis' Germanization project was based on a historically unprecedented programme of racial stock-taking, theft, expulsion and murder." Also, "The full integration of the [Czech] Protectorate into this New Order required the complete Germanization of the Protectorate's cultural life and the eradication of indigenous Czech and Jewish culture."

The actions by Nazi Germany reflect on the notion of race and culture playing a significant role in imperialism. The idea that there is a distinction between the Germans and the Jews has created the illusion of Germans believing they were superior to the Jewish inferiors, the notion of us/them and self/others.

The terms "McDonaldization" and "Cocacolonization" have been coined to describe the spread of Western cultural influence.





</doc>
<doc id="30531158" url="https://en.wikipedia.org/wiki?curid=30531158" title="Authenticity in art">
Authenticity in art

Authenticity in art is the different ways in which a work of art or an artistic performance may be considered authentic.
Denis Dutton distinguishes between "nominal authenticity" and "expressive authenticity".
The first refers to the correct identification of the author of a work of art, to how closely a performance of a play or piece of music conforms to the author's intention, or to how closely a work of art conforms to an artistic tradition.
The second sense refers to how much the work possesses original or inherent authority, how much sincerity, genuineness of expression, and moral passion the artist or performer puts into the work.

A quite different concern is the authenticity of the experience, which may be impossible to achieve. A modern visitor to a museum may not only see an object in a very different context from that which the artist intended, but may be unable to understand important aspects of the work. The authentic experience may be impossible to recapture.

Authenticity is a requirement for inscription upon the UNESCO World Heritage List. According to the "Nara Document on Authenticity", it can be expressed through 'form and design; materials and substance; use and function; traditions and techniques; location and setting; spirit and feeling; and other internal and external factors.'

Authenticity of provenance means that the origin or authorship of a work of art has been correctly identified.
As Lionel Trilling points out in his 1972 book "Sincerity and Authenticity", the question of authenticity of provenance has acquired a profoundly moral dimension. Regardless of the appearance of the object or the quality of workmanship, there is great importance in knowing whether a vase is a genuine Ming vase or just a clever forgery.
This intense interest in authenticity is relatively recent and is largely confined to the western world. In the medieval period, and in countries such as modern Thailand, there was or is little interest in the identity of the artist.

The case of Han van Meegeren is well known. After failing to succeed as an artist in his own right, he turned to creating fake Vermeer paintings. These were accepted as genuine by experts and acclaimed as masterpieces. After being arrested for selling national treasures to the Germans, he caused a sensation when he publicly demonstrated that he was the artist.
To guard against forgeries like this, a certificate of authenticity may be used to prove that a work of art is authentic, but there is a sizable market in fake certificates.
Furthermore a combination of art historical, conservational and technical evidence can be used to authenticate a work of art. The financial importance of authenticity may bias collectors to acquiring recent works of art where provenance can more easily be proven, perhaps even by a statement from the artist.
For older works, an increasingly sophisticated array of forensic techniques may be deployed to establish authenticity of provenance.

The philosopher Nelson Goodman discusses at length the question raised by Aline B. Saarinen: "If a fake is so expert that even after the most thorough and trustworthy examination its authenticity is still open to doubt, is it or is it not as satisfactory a work of art as if it were unequivocally genuine?" Goodman concludes that the question is academic, since there must be some way to distinguish a forgery from the original, and once the forgery is known for what it is, that knowledge alters the perception of value.
However, Arthur Koestler in "The Act of Creation" answers that if a forgery fits into an artist's body of work and produces the same kind of aesthetic pleasure as other works by that artist, there is no reason to exclude it from a museum.

The question of the value of a forgery may be irrelevant to a curator, since they are concerned only with the provenance of the work and not with its artistic merit.
Even for the curator, in many cases provenance is a matter of probabilities rather than a certainty - absolute proof is not possible.
But once a forgery has been exposed, no matter how highly the work was praised when it was thought to be "authentic" there is rarely any interest in evaluating the work on its own merit.
Reproduction is inherent to some forms of art. In Medieval Europe, an artist might create a drawing which was used by another craftsman to create a woodcut block. The drawing was usually destroyed in the block-cutting process, and the block was thrown away when it became worn out. The copies printed from the block are all that remain of the work.
In a 1936 essay, Walter Benjamin discussed the new media of photography and film, in which the work of art can be reproduced many times with no one version being the authentic "original". He linked this shift from authentic objects to broadly accessible mass media with a transformation in the function of art from ritual to politics.
Modern art may raise new issues of authenticity of provenance. For example, the artist Duane Hanson instructed the conservators of his 1971 sculpture "Sunbather" to feel free to replace elements such as the bathing cap or swimsuit if they became faded.
As Julian H. Scaff points out, the computer and the internet further confuse the issue of authenticity of provenance, since a digital work of art may exist in thousands or millions of identical versions, and in variants where there is no way to determine the original version or even the author.

Authenticity of provenance is concerned with identifying the person who made the work, or at least pinning down the place and time in which the work was made as closely as possible. Cultural authenticity, or authenticity of style or tradition, is concerned with whether a work is a genuine expression of an artistic tradition, even when the author may be anonymous. Interest in this form of authenticity may be associated with a romantic sense of the value of the pure, unadulterated tradition, often linked to nationalistic and possibly racist beliefs.

A work of art may be considered an authentic example of a traditional culture or genre when it conforms to the style, materials, process of creation and other essential attributes of that genre.
Many traditions are thought to be "owned" by an ethnic group, and work in that genre is only considered authentic if it is created by a member of that group. Thus Inuit art can only be considered authentic if created by an Inuit. This may help to protect the originators of an art tradition from cultural appropriation, but there is a racist aspect to the view as described by Joel Rudinow in his essay "Race, Ethnicity, Expressive Authenticity: Can White People Sing the Blues?"

The market for "primitive art" developed in the western world towards the end of the 19th century as explorers or colonialists came into contact with formerly unknown cultural groups in Africa, Asia and the Pacific. These people quickly learned how to incorporate new materials supplied by traders into their art, such as cloth and glass beads, but found that they could not sell these "inauthentic" objects. However, they learned how to manufacture works from local materials that would be considered authentic for sale to the westerners.
This process of creating art that will be considered authentic by western buyers continues to this day. The objects may be designed or modified to give the impression of having popular attributes and provenance, including religious or ritual uses, antiquity and association with royalty.
Similarly, in the 1940s Haitian professional artists began to create imitations of images provided by foreign entrepreneurs. The images represented the foreigners' view of the essence of Haitian Vodou art. The Haitian works were later claimed to be authentic.
To distinguish from crude objects made for the tourist trade, many collectors consider that a work is only an authentic example of a traditional genre if it meets certain standards of quality and was made for the original purpose. Dutton gives the example of the Igorot of northern Luzon who have long created figurines ("bulul") for use in traditional ceremonies, but today produce them primarily for the tourist trade. An Igorot family may purchase a roughly carved "bulul" from a tourist booth and use it for traditional ceremonies, thus giving authenticity to the work that would not, perhaps, be present otherwise.
Although collectors place greater value on "tribal" masks or sculptures that have been used in an active ritual, it may be impossible to prove whether this is the case. Even if a video shows the mask being worn in a ritual dance, the dance may have been staged for tourists. Yet if the provenance of the mask is proven, if the mask was made by a member of the society using traditional designs and techniques, it is presumably an authentic example of the style or tradition.

It is not always clear what constitutes a style. For example, production of Zimbabwean stone sculptures is relatively recent, dating to the 1950s. It does not draw on any earlier tradition. However, the sculpture plays an important role in establishing the existence of a uniquely Zimbabwean culture, and the authenticity of this style is strongly emphasized by the government of Zimbabwe despite the difficulty of defining its characteristics.
Navajo sand paintings raise a different issue. The traditional paintings must be destroyed on completion of the ritual in which they are used. However, Navajo artists create sand paintings for sale with slightly modified designs. Can these paintings be considered authentic examples of Navajo art?

Traditions change. In an exploration of the evolution of the art of the Maroon people of French Guiana, Sally Price shows that contemporary styles have developed through a complex interaction between artists and buyers. The Maroons have a long tradition of artwork, primarily in the form of decoration of everyday objects such as paddles or shutters. This art was purely aesthetic in purpose, with no symbolic meaning. However, European collectors needed to assign symbolism to "native art". Over time, the Maroon artists have come to accept the European semiotic vocabulary and to assign symbolism to their work, which younger artists may believe to be based on ancestral traditions. The artists have also moved into new media and new designs. Their art may still be considered authentic examples of Maroon art, but the art form and the meaning associated with it is new.

With performance arts such as music and theater, both the composer or playwright and the performers are involved in creating an instance of the work. There are some who consider that a performance is only truly authentic if it approximates as closely as possible what the original author would have expected to see and hear. In a historically informed performance, the actors or musicians will make every effort to achieve this effect by using replicas of historical instruments, studying historical guides to acting and so on. They would consider, for example, that a performance of one of Mozart's piano concertos would be "inauthentic" if played on a modern concert grand piano, an instrument that would have been unknown to the composer.

Others would not take such a rigorous view. For example, they would accept a performance of a play by Shakespeare as authentic even if the female parts were played by women rather than boys, and if the words were spoken with modern pronunciation rather than with the pronunciation of the Elizabethan era, which would be difficult for a modern audience to understand.

Dutton's concept of expressive authenticity is based on the "Oxford English Dictionary" alternative definition of "possessing original or inherent authority". In this sense, authenticity is a measure of the degree to which the artist's work is a committed, personal expression rather than derived from other work. It includes concepts of originality, honesty and integrity.
In the case of a musical performance, authenticity of expression may conflict with authenticity of performance. The player is true to their personal musical sense and does not imitate someone else's method of playing. Their performance may thus differ significantly from that of a player attempting to follow the style common at the time the musical work was composed.
Expressive authenticity is related to the technical term "authenticity" as used in existential philosophy. It has always been thought right to know oneself and to act accordingly, and in existential psychology this form of authenticity is seen as central to mental health.
Prominent artists such as the Abstract Expressionists Jackson Pollock, Arshile Gorky, and Willem de Kooning have been understood in existentialist terms, as have filmmakers such as Jean-Luc Godard and Ingmar Bergman.
The greater popularity of performer-based music as opposed to composition-based music is relatively recent. It seems to reflect a growing interest in expressive authenticity, and thus in musicians who have a unique and charismatic style.

The question of whether an artistic work is an authentic expression depends on the artist's background, beliefs and ideals. Andrew Potter cites the example of Avril Lavigne, a teenage singer from Napanee, Ontario who released her debut album in 2002. She claimed to be a small-town skateboarder, with her background providing the subjects of her songs, and said these songs were her own compositions. These claims of authenticity of expression and of provenance were both challenged. However, her work could have been authentic in expression even if Lavigne had not written it, or authentic in provenance if she had written it but not authentic in expression if the carefully cultivated skater-girl image were false.
Authenticity of expression may thus be linked with authenticity of style or tradition. Many feel it is not permissible for someone to speak in the voice of another culture or racial background, and that such an expression cannot be authentic. For example, hip hop was originally an art form through which underprivileged minorities in the United States protested against their condition. As it has become less of an underground culture, there is debate over whether the spirit of hip hop can survive in a marketable integrated version.
In "Authenticity Within Hip Hop and Other Cultures Threatened with Assimilation," Kembrew McLeod argues that hip hop culture is threatened with assimilation by a larger, mainstream culture, and that authenticity of expression in this genre is being lost.

A quite different concern is the authenticity of the experience, which may be impossible to achieve. A modern visitor to a museum may not only see an object in a very different context from that which the artist intended, but also may be unable to understand important aspects of the work. The authentic experience may be impossible to recapture. A curator may accept this, perhaps attempting to present the works of art in their authentic condition, but accepting that the artificial setting and lighting are legitimate in providing a contemporary experience of the artwork, even if this experience cannot be "authentic".

Dutton discusses the importance of the audience, giving a hypothetical example based on "La Scala", the famous Milan opera house. He imagines that the natural audience, informed aficionados of the opera, lose interest and cease to attend, but the performances continue to be given to tourists who have no understanding of the work they are experiencing. In another example, he quotes a Pacific Island dancer saying "Culture? That's what we do for the tourists." In both cases, although the performances may be authentic in the sense of being true to the original, the authenticity of the experience is open to debate.



</doc>
<doc id="60852572" url="https://en.wikipedia.org/wiki?curid=60852572" title="Design culture">
Design culture

Design culture is an organizational culture focused on approaches that improve customer experiences through design. In every firm, the design is significant since it allows the company to understand users and their needs. Integration of design culture in any organisation aims at creating experiences that add value to their respective users. In general, design culture entails undertaking design as the forefront of every operation in the organisation, from strategy formulation to execution. Every organisation is responsible for ensuring a healthy design culture through the application of numerous strategies. For instance, an organisation should provide a platform that allows every stakeholder to engage in design recesses. Consequently, everyone needs to incorporate design thinking, which is associated with innovation and critical thinking.

Moreover, design culture has many characteristics that create a conducive integration within that work environment. It offers freedom to fail that presents an opportunity for design experimentation. Design process entails taking risks that are mistake bounded. Therefore, individuals involving in design processes learn from their mistakes and become more innovative. Hence design culture encourages risk taking in design processes that facilitate innovation and creativity in an organisation. Proactivity in design culture has an impact on the organisation, specifically on decision making and problem-solving. Design culture allows designers to engage in constructive tasks. In the process, designers can solve problems in an organisation and make crucial decisions towards innovations of the organisation. Design culture is concerned with the human side of the respective organisation. In the recent past, organisations have been running based on data-driven mentality with the success of the organisation measured through the level of efficiency in the operations. In contrast, design culture is interested in the participation of human in determining the success of the organisation through the level of innovation facilitated by their involvement. In return, design culture concerned with improving an organisation's culture into a pleasant and change have driven culture.

Creation of a design culture for an organisation leads to a better transformation of the organisation. According to a study conducted by Forrester Research Consulting in the year 2016, to investigate whether the design-led cultures gave companies a significant advantage in the business or not, it was evident that most of the enterprises that analysed during the research had digital experiences that outpaced competition. The study proved that focusing on design strengthens an organisation from the inside as well as from the outside.

In a design-led enterprise, the design permeates the organisation beyond the product teams that embedded in the culture and such organisations; there is always an ambition to do better.

These companies typically support a variety of skills from the more oriented designers to the junior designers or the more tactical designers. The teams use collaborative processes and tools in unifying the working groups of the organisation. An organisation driven by design is more proactive rather than reactive, and they tend to confirm the next challenge rather than waiting until the challenge presents itself. This is made possible by the values that built based on, which is done through collaboration, experimentation, empathy as well as user researches.

Furthermore, developing design culture requires a definition of the design and the necessary resources that will facilitate its integration into the organisation. This follows an evaluation of the organisation's stakeholders who will be involved in the design process. The evaluation depends on the organisation's culture, which is the defining aspect of an organisation's life. Consequently, identifying the designers to be involved in the designing process requires an in-depth understanding of the purpose of the design towards the organisation's culture and innovation as well.

Additionally, building a design culture entails creating an environment that presents a platform that ensures that every individual obtains solutions to some problems present in the organisation. There exist several factors necessary for developing a design culture in any organisation. Cultivating culture is the first approach towards developing design culture. This step entails identifying individuals, their characters, and including them into the design process. The management involved in the design process needs to set the tone for the organisation's culture. Besides, design culture needs to develop an organisation's value in line with the design and ensure that every member of the design team incorporates them in the field of interest.

Developing design culture require incorporation of skilled personnel, innovative and creative individuals as well. However, identifying such individuals, it takes a process that will present an effective design process. Therefore, the management needs to integrate an effective interview process that will help in the selection of the best skills. Also, it will require motivation for the personnel involved and be in line with the organisation's values. The design culture needs to foster social capital that is responsible for higher information flow, effective collaboration and collective action of the team. Therefore, building a design culture should facilitate the creation of employees values, recognition of their achievements, enhance communication in the organisation and establish a firm organisation.

Design culture plays a significant role in marketing systems and the surrounding society. It addresses market externalities and internalises associated with the overall performance of the organisations. In addition, design culture allows an organisation to understand users in the society and their needs hence playing a significant role in the business. Through design culture, the organisation supports more strategically oriented designers from the society that ensure effective operation in the business. A design-driven organisation tends to be more proactive in the market by defining challenges and strategically working to improve its overall performance. Design culture facilitates the growth of a firm from tiny startups to legacy enterprises. Therefore, in markets and societies, design culture aims at improving an organisation's output to the excellent quality of products, services and the overall societal relationships.

Additionally, design culture needs to consider the aspects of the surrounding society and ensure that the design process is incorporative of the values and culture that is in line with the societal culture defining the surrounding community. The society plays a significant role in the design culture by presenting skilled personnel who can be recruited into the design process. In relation to society, design culture aims at designing a brand for everyone. I. Moreover, the community presents a ready market for the brands designed by the organisation. Consequently, the branding process should consider all the necessary qualities that will maintain the brand in the market. This enhanced through consideration of the values defining the surrounding society. Moreover, the organisation's culture should be at per with the societal culture in order to promote collaboration.

Design culture aims at enhancing collaboration in the market with the respective stakeholders. Therefore, introducing design into the market requires intense research and planning that will facilitate the production of a brand that fits the requirements for all. The design process needs to be aware of the market trends and branded products with the aim of solving an existing problem in the market. In addition, the design process should involve designing a brand that provides a solution to various situations in the society. Addressing the market, design culture is concerned about developing a brand that meets the best competitive qualities. Through innovation, the organisation involved in the design process conducts research on different market trends and comes up with refined approaches to be integrated into the design process. Moreover, the organisation needs to maintain its culture that uniquely defines its operations and products in the market. Concerned about the future trend of the design, the management responsible for the design process need to ensure that necessary qualities are met in the design process 

As a guiding truth towards the successful firm organisation, design culture takes a psychological approach to different operations of the business. Positioning design professions entails defining numerous approaches necessary for building a healthy design culture. In addition, it focuses on professional strategies that get prospects and customers preferences that enable a business to stand firm in a competitive market. A design-centric organisation is usually biased against leaving anything to chance. A healthy design culture applies professional not only to the product but also to the organisation itself.  Products usually reflect the structure as well as the character of the organisation that is responsible for their production. A well-designed enterprise is capable of producing well-designed products and services. In a healthy design culture, everyone has a feeling of empowerment towards participation in the design process. Employees are usually encouraged to carry out experimentations with the understanding that they will often lead to mistakes, and this should not be a hindrance.

Design culture has innovation as one of the cultural traits. Therefore, the design profession is crucial in the design process as it incorporates necessary branding skills, design skills and knowledge of the design process. The process of cultivating culture requires skills necessary for analysing the surrounding society and determining the required skills for the design process. Setting the tone for an organisation is a professional approach that requires the development of an organisation's values. The design management needs to demonstrate knowledge and an understanding of the conduct of the design team and the level of innovation necessary for the design process.

Furthermore, positioning the design profession requires increased diversity that facilitates innovation. Gender diversity should be maintained in determining the team that will be involved in the design process. In addition, diversity brings individuals together who have varying skills, creativity and knowledge that help in branding different products. Branding a product for everyone in society requires extensive research. As a result, the research requires a professional approach that will help in identifying the cultural aspects defining the society. Moreover, identification of the market trends requires in-depth analysis approaches that are in line with design professions. Therefore, the design management team need to ensure an effective and strong position in the design culture that enhances innovation in the design process 

Effective design culture has to be made with intention and put into practice consistently. This requires the definition of approaches necessary for locating design culture. Discovering design culture is facilitated by the need to obtain a solution to a given challenge or the need to major on problem-solving approaches. Locating design culture is done through experimentation, collaboration, user research and empathy. It is a common characteristic for many companies to build a third design culture through trial and error. For example, a company such as Apple has been fine-tuning its design culture for about three decades, a corny though a relevant adage. Locating design culture require an effective definition of the characteristics of a robust design culture. It requires frequent experimentation that allow individuals to explore numerous solutions as possible that result in successful launches. In addition, locating design culture entails implementation of a system that provides answers for questions raised concerning the design culture. Moreover, it involves locating different tools that encourage collaboration allowing a given team to formulate plans, design presentations and work together for successful design culture. Concerning idea generation, it is a norm for every organisation to keep coming up with new ideas now and then, and this allows the organisation to iterate and even receive feedback more efficiently and in a short time 


</doc>
<doc id="62379378" url="https://en.wikipedia.org/wiki?curid=62379378" title="Duocentric social network">
Duocentric social network

A duocentric social network is a type of social network composed of the combined network members of a dyad. The network consists of mutual, overlapping ties between members of the dyad as well as non-mutual ties. While an explicit conceptualization of "duocentric social networks" appeared for the first time in an academic publication in 2008, the history of the analysis dates back to at least the 1950s and has spanned the fields of psychology, sociology, and health.

Coromina et al. coined the term "duocentered networks" to describe the analytical technique of combining two individuals’ (or "egos") social networks to examine both the shared network members (or "alters") between a dyad and those that are connected to only one individual. In this original conceptualization, Coromina et al. did not consider the relationships between the alters (i.e., the "ties" between alters) to be a necessary component of duocentric network analysis.

The impetus for this original conceptualization was a compromise between the two most commonly used social network analytical methods: "egocentric" and "sociocentric" network analyses. In an egocentric network analysis, a singular individual, his or her network members, and (occasionally), the ties between those alters are the focal point of the analysis. Egocentric analyses have been used in a wide range of fields, including physical health, psychopathology, family studies, and intimate relationships. On the other hand, the sociocentric network approach utilizes a bounded group as the unit of analyses, examining all ties between actors in the group. This has been utilized to study health in retirement communities and entire cities (e.g., the Framingham Heart Study), as well as in the workplace and classroom settings. Sociocentric networks could be used to answer research questions focused on dyads, but the time, cost, and difficulty of collecting network data from all members in a bounded group is often prohibitive. Coromina et al. also state that duocentered networks relieve issues of data collection in sociocentric networks. First, it reduces “respondent inaccuracy” in reporting network contacts, which will be more prevalent in less well socially connected individuals. Because the dyad is selected for a specific network research question, they are more likely to be central members of their networks and better positioned to accurately report on their network contacts. Second, the technique reduces “unit non-response," which is the failure of an eligible study participant to respond or provide enough information to deem the response statistically usable. Because the focus of a duocentered network is only two individuals rather than a larger group, it will ostensibly be easier to gather usable information.

Kennedy et al. maintained this basic framework, but redefined the concept as "duocentric networks", and suggested that information on the relatedness of ties in the network should be collected. Coromina et al. did not take this approach because of the respondent inaccuracy and unit non-response bias that similarly affect sociocentric analyses. Respondent inaccuracy in the context of duocentric networks means that people will inaccurately report the connections between their network members. Unit non-response follows from this difficulty; if people are unable to report connections, certain analyses that rely on these connections may not be possible.

There are several common structural metrics derived from duocentric social networks:


Compositional measures are the characteristics of the individuals who make up the network or other societal norms and structures that may influence the structure and function of a social network. Compositional measures include social support, intimate relationship approval from network members, proportion of family or friends in the network, demographic characteristics, and norms.

Although primarily supported in egocentric network analyses, evidence suggest that dyads can influence the composition and structure of the duocentric networks in which they are embedded. For example, Bryant, Conger & Meehan (2001) found that a wife's marital satisfaction predicted lower discord between husbands and the wife's parents at a later time.

One of the first studies examining duocentric social networks was Elizabeth Bott's 1957 finding that the density of spouse's separate networks is positively associated with marital role segregation, a finding now known as the Bott Hypothesis. While Bott did not examine the overlapping network of spouses, her work was among the first to collect network data separately from both members of a dyad, and use that data to predict a dyadic phenomenon.

Perhaps the most well-studied phenomenon utilizing a duocentric network approach in the context of intimate relationships is network overlap. Most of this research points to higher relationship satisfaction as the level of overlap increases. Network overlap increases as couples transition to cohabitation, and remarriages tend to have less overlap than first marriages. Additionally, one finding suggests that more equal numbers of each partner's family contained in the overlapping network is associated with higher marital satisfaction for heterosexual couples.

Other structural measures have received relatively less attention in the study of intimate relationships. Research shows a positive association between duocentric network size and marital satisfaction. Additionally, marriages in which both spouses are in their first marriage have larger networks than marriages in which both spouse is remarried. Other studies have highlighted non-results, including that social network density is not associated with relationship satisfaction and that density is not associated with marital role segregation (a refutation of the Bott Hypothesis).

The link between compositional duocentric network factors and intimate relationships is less well-studied. However, evidence from duocentric analyses suggest that discord with in-laws predicts lower satisfaction, commitment, and stability in marriages over time. Additionally, support and approval from the social network tends to be associated with higher commitment and marital satisfaction.

Duocentric social network analyses have been used less often outside the context of intimate relationships. One of the earliest examples examined the frequency with which two people mutually named one another in their respective network reports. The study recruited one person who listed their network members, then those network members were contacted and asked to list their own network members. About 86% of the time, people named by the original interviewee also named that interviewee on their own list. Another study of parents of children with brain tumors found that overlap of non-kin was near 50%, while overlap for kin was slightly higher. More peripheral overlapping ties (i.e., those to whom the couple is less close) was associated with lower rates of mental health disorders. Another study examined duocentric networks in sibling pairs aged 7–13. Monozygotic twins had the most overlap in their peer networks (82%), followed by same-sex dizygotic twins (67%), same-sex virtual twins (e.g., unrelated peers matched on certain characteristics; 62%), friend-friend pairs (48%), opposite-sex dizygotic twins (42%), same-sex full siblings (39%), opposite sex virtual twins (37%), and opposite-sex full siblings (27%). Genetics, sex (same- or opposite-sex), age, and relationship intimacy affected rates of peer overlap. Another example used pairs of corporations engaged in a business alliance as the focal unit, and found that the more common partners (i.e., overlap) between the two firms, the less likely their alliance would dissolve.

Kennedy et al. (2015) outline the most rigorous duocentric network study as one in which both members of the dyad report the specific individuals contained in their social networks. However, the time and cost of this form of data collection has led researchers to use less stringent techniques to gather information on a dyadic network.

Rather than asking respondents to list the specific people contained in their social network, researchers occasionally ask for global perceptions of network qualities from both members of a dyad. This methodology limits many structural analyses because the relationships between network members is unknown, unless the structural qualities are addressed at a global level (e.g., for global perceptions of overlap, see Kearns & Leonard (2004)). Therefore, these studies typically highlight how compositional network aspects affect the dyad. For example, in the study of intimate relationship research, this methodology has been used to show how global perceptions of approval from the network vary across relationship stage, closeness to family predicts changes in marital happiness, the degree to which liking one's partner's family predicts relationship dissolution, the effect of network support on relationship satisfaction, and the relationship between time spent with the network and relationship commitment.

Another variant of the duocentric network approach is to interview only one member of a focal dyad, but require the individual to report on both their own and the other person's social network, or the other individual's relationship to their own network. In many of these studies, respondents report global network perceptions. For example, one study asked respondents to report on the propensity for a respondent's relationship partner to receive support from their own network (a "global" measure of support). Support was positively associated with relationship satisfaction. Another study asked respondents to report their perception of approval from their relationship partner's family (another global measure), which was found to be negatively associated with relationship dissolution.

However, some research utilizes specific alter reporting in a single ego methodology. Milardo (1982) asked respondents to report their romantic partner's relationship to each of their own, specific, network members. This method allowed the researcher to understand how much of the ego's social network overlapped with his or her partner without collecting information from the partner. However, this approach risks the ego inaccurately reporting the relationship between their partner and the individual network members. Another study asked homeless youth to list recent sexual partners, other non-sexual partner network members, and the relationships between these alters. The risk of unprotected sex was higher to the degree that sexual partners knew other members of the youth's social network.



</doc>
<doc id="62758724" url="https://en.wikipedia.org/wiki?curid=62758724" title="Manipuri Guardians of the Directions">
Manipuri Guardians of the Directions

In Manipuri religion and Manipuri mythology, the Guardians of the Directions refers to the deities who rule the specific directions of space according to Sanamahism.

There are mainly ten Guardians of the Directions in Manipuri mythology. These are the following:

According to Manipuri mythology, before the creation of the universe, there were four deities who guard four realms. These are the following:
These four deities guardians guard the four main directions before the creation of the universe.

However, the following deities are also added to the guardianship of the directions after the creation of the universe:

The Manipuri directional deities have their Hindu equivalents. The following are as follows: 



</doc>
<doc id="62824987" url="https://en.wikipedia.org/wiki?curid=62824987" title="Enea Tree Museum">
Enea Tree Museum

The Enea Tree Museum is a 75,000 m² park near Lake Zurich in Rapperswil-Jona, Canton of St. Gallen, Switzerland. The tree museum shows over 50 trees from over 25 species as well as sculptures by international artists.

The tree museum, founded in 2010, was planned and built by landscape architect Enzo Enea. The property at Lake Zürich was leased from the Mariazell-Wurmsbach Cistercian Abbey. Before the construction, water had to be extracted from the former wetland. For this purpose, an avenue of swamp cypresses was set up. This tree species naturally extracts a lot of water from the soil (evapotranspiration). Today this avenue forms the entrance to the tree museum. The museum has been open to the public since it opened.

The trees growing in the tree museum should have been felled, but were rescued by Enea and replanted in the tree museum. The following specimens belong to the tree population:
Sculptures by international artists can be found in the tree museum. The following works of art are curated in the Tree Museum:



</doc>
<doc id="8206481" url="https://en.wikipedia.org/wiki?curid=8206481" title="The Beijing Center for Chinese Studies">
The Beijing Center for Chinese Studies

The Beijing Center for Chinese Studies (or TBC) was established by Fr. Ron Anton as a 501(c)(3) organization in 1998. TBC's mission is to promote mutual understanding between China and other cultures, while facilitating international academic opportunities for students, educators, and business professionals. The Beijing Center's school of record was first through Loyola Marymount University, followed byLoyola University-Chicago, and currently through the University of International Business and Economics (UIBE). 

The initial focus of TBC was cultural immersion through study abroad opportunities and short-term, faculty-led educational tours. As part of its programs, TBC introduced: 


When first entering the TBC Beijing location at the University of International Business and Economics, there is a replica of an old Beijing alleyway or “Hutong”. There is a water garden featuring 4,000 years of Chinese water jars, the conference room displays original copies of western maps of China from 1584 to 1745, and the director’s office replicates an old Ming scholar’s studio.

At TBC, students have the opportunity to study abroad for a semester or a full year, where they will have the chance to live on the campus of the University of International Business and Economics (UIBE). Students can choose from over 20 English-taught courses each semester and take a Chinese language course appropriate for their proficiency. In addition, at the beginning of each semester, students have a 2-week long academic excursion trip – the Silk Road during fall and Yunnan Province in spring. These excursions teach students about the diverse cultures of China and offer the opportunity for personal and cultural growth.  

Through the Intern Abroad programs, TBC offers semester, full-year, or summer internships for students looking to gain international working experience, learn Chinese, and experience another culture and its working environment. TBC arranges internships by working with a variety of organizations in different fields, to provide students with internships fitting of their professional goals.  

The ChinaContact division runs faculty-led short-term, non-credit programs all year long. Several times a year there are programs for faculty members and for university administrators including programs for university presidents. There are workshops and courses for undergraduate classes, MBA groups, and graduate humanities classes.

TBC Research Center specializes in supporting primary source research and helping young Chinese scholars in the doctoral or post-doc years. The center’s scholar-in-residence helps TBC students with academic projects during their stay in Beijing.

The Beijing Center Press specializes in publishing unique works on China-related topics, such as politics, philosophy, art, history, culture, religion, sciences, current affairs, etc. In partnership with Amazon’s KDP, all TBC works are published and available online at amazon.com. TBC Press publishes works in both Chinese and English.  

The Anton Library houses a collection of 27,000 volumes about Chinese life and society. Special collections include a small rare book collection of 17th century volumes that first revealed China to the West, a collection of approximately 3,000 older books from 1800 to 1949, and a multilingual collection on the history of the Catholic Church in China. The main collection of books about China written in English is the largest private such collection in Beijing and one of the largest in China.  

The TBC Library of Chinese Studies has obtained a few original copies of these works in its collection entitled: "How the West Learned of China". The collection dates from 1588 until 1840 (with a few later exceptions).
"How The West Learned of China" collection list

The collection has 1,200 books from the early 19th century to the founding of the People's Republic of China. Many books were published in the Qing Dynasty and some are considered rare or collectors items. 

This is a multilingual collection mainly of primary sources from around the world on early Christian history in China. Works here are in Chinese, Latin, Portuguese, Dutch, German, Spanish, French, etc. (not English). It contains the work of early Dominicans, Franciscans, Jesuits and others as well as works from the early local church in China.



</doc>
<doc id="509995" url="https://en.wikipedia.org/wiki?curid=509995" title="Abstract and concrete">
Abstract and concrete

Abstract and concrete are classifications that denote whether the object that a term describes has physical referents. Abstract objects have no physical referents, whereas concrete objects do. They are most commonly used in philosophy and semantics. Abstract objects are sometimes called abstracta (sing. abstractum) and concrete objects are sometimes called "concreta" (sing. "concretum"). An abstract object is an object that does not exist at any particular time or place, but rather exists as a type of thing—i.e., an idea, or abstraction. The term "abstract object" is said to have been coined by Willard Van Orman Quine. The study of abstract objects is called abstract object theory.

The type–token distinction identifies physical objects that are tokens of a particular type of thing. The "type" of which it is a part is in itself an abstract object. The abstract-concrete distinction is often introduced and initially understood in terms of paradigmatic examples of objects of each kind:

Abstract objects have often garnered the interest of philosophers because they raise problems for popular theories. In ontology, abstract objects are considered problematic for physicalism and some forms of naturalism. Historically, the most important ontological dispute about abstract objects has been the problem of universals. In epistemology, abstract objects are considered problematic for empiricism. If abstracta lack causal powers or spatial location, how do we know about them? It is hard to say how they can affect our sensory experiences, and yet we seem to agree on a wide range of claims about them. 

Some, such as Edward Zalta and arguably, Plato in his Theory of Forms, have held that abstract objects constitute the defining subject matter of metaphysics or philosophical inquiry more broadly. To the extent that philosophy is independent of empirical research, and to the extent that empirical questions do not inform questions about abstracta, philosophy would seem especially suited to answering these latter questions. 

In modern philosophy, the distinction between abstract and concrete was explored by Immanuel Kant and G. W. F. Hegel.

Gottlob Frege said that abstract objects, such as numbers, were members of a third realm, different from the external world or from internal consciousness. 

Another popular proposal for drawing the abstract-concrete distinction contends that an object is abstract if it lacks any causal powers. A causal power has the ability to affect something causally. Thus, the empty set is abstract because it cannot act on other objects. One problem for this view is that it is not clear exactly what it is to have a causal power. For a more detailed exploration of the abstract-concrete distinction, follow the link below to the "Stanford Encyclopedia" article.

Recently, there has been some philosophical interest in the development of a third category of objects known as the quasi-abstract. Quasi-abstract objects have drawn particular attention in the area of social ontology and documentality. Some argue that the over-adherence to the platonist duality of the concrete and the abstract has led to a large category of social objects having been overlooked or rejected as nonexisting because they exhibit characteristics that the traditional duality between concrete and abstract regards as incompatible. Specially, the ability to have temporal location, but not spatial location, and have causal agency (if only by acting through representatives). These characteristics are exhibited by a number of social objects, including states of the international legal system.

Jean Piaget uses the terms "concrete" and "formal" to describe two different types of learning. Concrete thinking involves facts and descriptions about everyday, tangible objects, while abstract (formal operational) thinking involves a mental process.


</doc>
<doc id="25200737" url="https://en.wikipedia.org/wiki?curid=25200737" title="Knowledge arena">
Knowledge arena

A knowledge arena is a virtual space where individuals can manipulate concepts and relationships to form a concept map. Individuals using a computer with appropriate software can represent concepts and the relationships between concepts in a node-relationship-node formalism. The process of thinking about the concepts and making associations between them has been called "off-loading" by McAleese.

The concept map is a form of a semantic network or semantic graph. It is formally based on graph theory. In the concept map, concepts are represented by nodes. The relationship between nodes are represented by "typed links" or "edges". (See Graph theory.) In creating a map or graphic representation of what is known an individual intentionally interacts with the graphical interface or map and through a reflective process adds nodes (concepts) and /or adds relationships (edges or typed links) or modifies existing node-relationship-node instances. It is likely that the process of engaging with concepts and relationships between concepts brings about the creation of understandings as well as making the understandings explicit.

Many different claims have been made for the utility of the concept map. The interactive and reflective nature of map creation is highlighted by the use of the description Knowledge Arena. Although maps may represent what an individual knows at a point in time; it is likely that by interacting with the concepts and relationships in the knowledge arena individual continues to create and modify what that individual "knows".

See also 


</doc>
<doc id="17306829" url="https://en.wikipedia.org/wiki?curid=17306829" title="Conceptual necessity">
Conceptual necessity

Conceptual necessity is a property of the certainty with which a state of affairs, as presented by a certain description, occurs: it occurs by conceptual necessity if and only if it occurs just by virtue of the meaning of the description. If someone is a bachelor, for instance, then he is bound to be unmarried by conceptual necessity, because the meaning of the word "bachelor" determines that he is.

Alternatively, there is metaphysical necessity, which is a certainty determined, not by the meaning of a description, but instead by facts in the world described.

Historically, Baruch Spinoza was a subscriber to this belief.



</doc>
<doc id="58267" url="https://en.wikipedia.org/wiki?curid=58267" title="Conceptual schema">
Conceptual schema

A 'conceptual schema' is a high-level description of a business's informational needs. It typically includes only the main concepts and the main relationships among them. Typically this is a first-cut model, with insufficient detail to build an actual database. This level describes the structure of the whole database for a group of users. The conceptual model is also known as the data model that can be used to describe the conceptual schema when a database system is implemented. It hides the internal details of physical storage and targets on describing entities, datatype, relationships and constraints.

A conceptual schema or conceptual data model is a map of concepts and their relationships used for databases. This describes the semantics of an organization and represents a series of assertions about its nature. Specifically, it describes the things of significance to an organization ("entity classes"), about which it is inclined to collect information, and characteristics of ("attributes") and associations between pairs of those things of significance ("relationships").

Because a conceptual schema represents the semantics of an organization, and not a database design, it may exist on various levels of abstraction. The original ANSI four-schema architecture began with the set of "external schema" that each represent one person's view of the world around him or her. These are consolidated into a single "conceptual schema" that is the superset of all of those external views. A data model can be as concrete as each person's perspective, but this tends to make it inflexible. If that person's world changes, the model must change. Conceptual data models take a more abstract perspective, identifying the fundamental things, of which the things an individual deals with are just examples.

The model does allow for what is called inheritance in object oriented terms. The set of instances of an entity class may be subdivided into entity classes in their own right. Thus, each instance of a "sub-type" entity class is also an instance of the entity class's "super-type". Each instance of the super-type entity class, then is also an instance of one of the sub-type entity classes.

Super-type/sub-type relationships may be "exclusive" or not. A methodology may require that each instance of a super-type may "only" be an instance of "one" sub-type. Similarly, a super-type/sub-type relationship may be "exhaustive" or not. It is exhaustive if the methodology requires that each instance of a super-type "must be" an instance of a sub-type. A sub-type named other is often necessary.


A data structure diagram (DSD) is a data model or diagram used to describe conceptual data models by providing graphical notations which document entities and their relationships, and the constraints that bind them.





</doc>
<doc id="4849201" url="https://en.wikipedia.org/wiki?curid=4849201" title="Object of the mind">
Object of the mind

An object of the mind is an object that exists in the imagination, but which, in the real world, can only be represented or modeled. Some such objects are abstractions, literary concepts, or fictional scenarios.

Closely related are intentional objects, which are what thoughts and feelings are about, even if they are not about anything real (such as thoughts 
about unicorns, or feelings of apprehension about a dental appointment which is subsequently cancelled). However, intentional objects may coincide with real objects (as in thoughts about horses, or a feeling of regret about a missed appointment).

Mathematics and geometry describe abstract objects that sometimes correspond to familiar shapes, and sometimes do not. Circles, triangles, rectangles, and so forth describe two-dimensional shapes that are often found in the real world. However, mathematical formulas do not describe individual physical circles, triangles, or rectangles. They describe ideal shapes that are objects of the mind. The incredible precision of mathematical expression permits a vast applicability of mental abstractions to real life situations.

Many more mathematical formulas describe shapes that are unfamiliar, or do not necessarily correspond to objects in the real world. For example, the Klein bottle is a one-sided, sealed surface with no inside or outside (in other words, it is the three-dimensional equivalent of the Möbius strip). Such objects can be represented by twisting and cutting or taping pieces of paper together, as well as by computer simulations. To hold them in the imagination, abstractions such as extra or fewer dimensions are necessary.

If-then arguments posit logical sequences that sometimes include objects of the mind. For example, a counterfactual argument proposes a hypothetical or subjunctive possibility which "could" or "would" be true, but "might not" be false. Conditional sequences involving subjunctives use intensional language, which is studied by modal logic, whereas classical logic studies the extensional language of necessary and sufficient conditions.

In general, a logical antecedent is a sufficient condition, and a logical consequent is a necessary condition (or the contingency) in a logical conditional. But logical conditionals accounting only for necessity and sufficiency do not always reflect every day if-then reasoning, and for this reason they are sometimes known as material conditionals. In contrast, indicative conditionals, sometimes known as non-material conditionals, attempt to describe if-then reasoning involving hypotheticals, fictions, or counterfactuals.

Truth tables for if-then statements identify four unique combinations of premises and conclusions: true premises and true conclusions; false premises and true conclusions; true premises and false conclusions; false premises and false conclusions. Strict conditionals assign a positive truth-value to every case except the case of a true premise and a false conclusion. This is sometimes regarded as counterintuitive, but makes more sense when false conditions are understood as objects of the mind.

A false antecedent is a premise known to be false, fictional, imaginary, or unnecessary. In a conditional sequence, a false antecedent may be the basis for any consequence, true or false.

The subjects of literature are sometimes false antecedents. For instance, the contents of false documents, the origins of stand-alone phenomena, or the implications of loaded words. Moreover, artificial sources, personalities, events, and histories. False antecedents are sometimes referred to as "nothing", or "nonexistent", whereas nonexistent referents are not referred to.

Art and acting often portray scenarios without any antecedent other than an artist's imagination. For example, mythical heroes, legendary creatures, gods and goddesses.

A false consequent, in contrast, is a conclusion known to be false, fictional, imaginary, or insufficient. In a conditional statement, a fictional conclusion is known as a non sequitur, which literally means "out of sequence". A conclusion that is out of sequence is not contingent on any premises that precede it, and it does not follow from them, so such a sequence is not conditional. A conditional sequence is a connected series of statements. A false consequent cannot follow from true premises in a connected sequence. But, on the other hand, a false consequent can follow from a false antecedent.

As an example, the name of a team, a genre, or a nation is a collective term applied ex post facto to a group of distinct individuals. None of the individuals on a sports team is the team itself, nor is any musical chord a genre, nor any person America. The name is an identity for a collection that is connected by consensus or reference, but not by sequence. A different name could equally follow, but it would have different social or political significance.

In philosophy, mind-body dualism is the doctrine that mental activities exist apart from the physical body, notably posited by René Descartes in "Meditations on First Philosophy".

Many objects in fiction follow the example of false antecedents or false consequents. For example, "The Lord of the Rings" by J.R.R. Tolkien is based on an imaginary book. In the "Appendices" to "The Lord of the Rings", Tolkien's characters name the "Red Book of Westmarch" as the source material for "The Lord of the Rings", which they describe as a translation. But the "Red Book of Westmarch" is a fictional document that chronicles events in an imaginary world. One might imagine a different translation, by another author.

Social reality is composed of many standards and inventions that facilitate communication, but which are ultimately objects of the mind. For example, money is an object of the mind which currency represents. Similarly, languages signify ideas and thoughts.

Objects of the mind are frequently involved in the roles that people play. For example, acting is a profession which predicates real jobs on fictional premises. Charades is a game people play by guessing imaginary objects from short play-acts.

Imaginary personalities and histories are sometimes invented to enhance the verisimilitude of fictional universes, and/or the immersion of role-playing games. In the sense that they exist independently of extant personalities and histories, they are believed to be fictional characters and fictional time frames.

Science fiction is abundant with future times, alternate times, and past times that are objects of the mind. For example, in the novel "Nineteen Eighty-Four" by George Orwell, the number 1984 represented a year that had not yet passed.

Calendar dates also represent objects of the mind, specifically, past and future times. In "", which was released in 1986, the narration opens with the statement, "It is the year 2005." In 1986, that statement was futuristic. During the year 2005, that reference to the year 2005 was factual. Now, "The Transformers: The Movie" is retro-futuristic. The number 2005 did not change, but the object of the mind that it represents did change.

Deliberate invention also may reference an object of the mind. The intentional invention of fiction for the purpose of deception is usually referred to as lying, in contrast to invention for entertainment or art. Invention is also often applied to problem solving. In this sense the physical invention of materials is associated with the mental invention of fictions.

Convenient fictions also occur in science.

The theoretical posits of one era's scientific theories may be demoted to mere objects of the mind by subsequent discoveries: some standard examples include phlogiston and ptolemaic epicycles.

This raises questions, in the debate between scientific realism and instrumentalism about the status of current posits, such as black holes and quarks. Are they still merely intentional, even if the theory is correct?

The situation is further complicated by the existence in scientific practice of entities which are explicitly held not to be real, but which nonetheless serve a purpose—convenient fictions. Examples include field lines, centers of gravity, and electron holes in semiconductor theory.

A reference that names an imaginary source is in some sense also a self-reference. A self-reference automatically makes a comment about itself. Premises that name themselves as premises are premises by self-reference; conclusions that name themselves as conclusions are conclusions by self-reference.

In their respective imaginary worlds the "Necronomicon", "The Hitchhiker's Guide to the Galaxy", and the "Red Book of Westmarch" are realities, but only because they are referred to as real. Authors use this technique to invite readers to pretend or to make-believe that their imaginary world is real. In the sense that the stories that quote these books are true, the quoted books exist; in the sense that the stories are fiction, the quoted books do not exist.

Austrian philosopher Alexius Meinong (1853–1920) advanced nonexistent objects in the 19th and 20th century within a “theory of objects”. He was interested in intentional states which are directed at nonexistent objects. Starting with the “principle of intentionality”, mental phenomena are intentionally directed towards an object. People may imagine, desire or fear something that does not exist. Other philosophers concluded that intentionality is not a real relation and therefore does not require the existence of an object, while Meinong concluded there is an object for every mental state whatsoever—if not an existent then at least a nonexistent one.




</doc>
<doc id="28547570" url="https://en.wikipedia.org/wiki?curid=28547570" title="Terminology model">
Terminology model

A terminology model is a refinement of a concept system. Within a terminology model the concepts (object types) of a specific problem or subject area are defined by subject matter experts in terms of concept (object type) definitions and definitions of subordinated concepts or characteristics (properties). Besides object types, the terminology model allows defining hierarchical classifications, definitions for object type and property behavior and definition of casual relations.

The terminology model is a means for subject matter experts to express their knowledge about the subject in subject specific terms. Since the terminology model is structured rather similar to an object-oriented database schema, is can be transformed without loss of information into an object-oriented database schema. Thus, the terminology model is a method for problem analysis on the one side and a mean of defining database schema on the other side.

Several terminology models have been developed and published in the field of statistics:




</doc>
<doc id="599917" url="https://en.wikipedia.org/wiki?curid=599917" title="Mental image">
Mental image

A mental image or mental picture is an experience that, on most occasions, significantly resembles the experience of perceiving some object, event, or scene, but occurs when the relevant object, event, or scene is not actually present to the senses. There are sometimes episodes, particularly on falling asleep (hypnagogic imagery) and waking up (hypnopompic), when the mental imagery, being of a rapid, phantasmagoric and involuntary character, defies perception, presenting a kaleidoscopic field, in which no distinct object can be discerned. Mental imagery can sometimes produce the same effects as would be produced by the behavior or experience imagined.

The nature of these experiences, what makes them possible, and their function (if any) have long been subjects of research and controversy in philosophy, psychology, cognitive science, and, more recently, neuroscience. As contemporary researchers use the expression, mental images or imagery can comprise information from any source of sensory input; one may experience auditory images, olfactory images, and so forth. However, the majority of philosophical and scientific investigations of the topic focus upon "visual" mental imagery. It has sometimes been assumed that, like humans, some types of animals are capable of experiencing mental images. Due to the fundamentally introspective nature of the phenomenon, there is little to no evidence either for or against this view.

Philosophers such as George Berkeley and David Hume, and early experimental psychologists such as Wilhelm Wundt and William James, understood ideas in general to be mental images. Today it is very widely believed that much imagery functions as mental representations (or mental models), playing an important role in memory and thinking. William Brant (2013, p. 12) traces the scientific use of the phrase "mental images" back to John Tyndall's 1870 speech called the "Scientific Use of the Imagination". Some have gone so far as to suggest that images are best understood to be, by definition, a form of inner, mental or neural representation; in the case of hypnagogic and hypnapompic imagery, it is not representational at all. Others reject the view that the image experience may be identical with (or directly caused by) any such representation in the mind or the brain, but do not take account of the non-representational forms of imagery.

In 2010, IBM applied for a patent on a method to extract mental images of human faces from the human brain. It uses a feedback loop based on brain measurements of the fusiform face area in the brain that activates proportionate with degree of facial recognition. It was issued in 2015.

The notion of a "mind's eye" goes back at least to Cicero's reference to mentis oculi during his discussion of the orator's appropriate use of simile.

In this discussion, Cicero observed that allusions to "the Syrtis of his patrimony" and "the Charybdis of his possessions" involved similes that were "too far-fetched"; and he advised the orator to, instead, just speak of "the rock" and "the gulf" (respectively)—on the grounds that "the eyes of the mind are more easily directed to those objects which we have seen, than to those which we have only heard".

The concept of "the mind's eye" first appeared in English in Chaucer's (c. 1387) Man of Law's Tale in his "Canterbury Tales", where he tells us that one of the three men dwelling in a castle was blind, and could only see with "the eyes of his mind"; namely, those eyes "with which all men see after they have become blind". 

The biological foundation of the mind's eye is not fully understood. Studies using fMRI have shown that the lateral geniculate nucleus and the V1 area of the visual cortex are activated during mental imagery tasks. Ratey writes:
The visual pathway is not a one-way street. Higher areas of the brain can also send visual input back to neurons in lower areas of the visual cortex. [...] As humans, we have the ability to see with the mind's eye—to have a perceptual experience in the absence of visual input. For example, PET scans have shown that when subjects, seated in a room, imagine they are at their front door starting to walk either to the left or right, activation begins in the visual association cortex, the parietal cortex, and the prefrontal cortex—all higher cognitive processing centers of the brain.

The rudiments of a biological basis for the mind's eye is found in the deeper portions of the brain below the neocortex, or where the center of perception exists. The thalamus has been found to be discrete to other components in that it processes all forms of perceptional data relayed from both lower and higher components of the brain. Damage to this component can produce permanent perceptual damage, however when damage is inflicted upon the cerebral cortex, the brain adapts to neuroplasticity to amend any occlusions for perception . It can be thought that the neocortex is a sophisticated memory storage warehouse in which data received as an input from sensory systems are compartmentalized via the cerebral cortex. This would essentially allow for shapes to be identified, although given the lack of filtering input produced internally, one may as a consequence, hallucinate—essentially seeing something that isn't received as an input externally but rather internal (i.e. an error in the filtering of segmented sensory data from the cerebral cortex may result in one seeing, feeling, hearing or experiencing something that is inconsistent with reality).

Not all people have the same internal perceptual ability. For many, when the eyes are closed, the perception of darkness prevails. However, some people are able to perceive colorful, dynamic imagery. The use of hallucinogenic drugs increases the subject's ability to consciously access visual (and auditory, and other sense) percepts.

Furthermore, the pineal gland is a hypothetical candidate for producing a mind's eye ; Rick Strassman and others have postulated that during near-death experiences (NDEs) and dreaming , the gland might secrete a hallucinogenic chemical "N","N"-Dimethyltryptamine (DMT) to produce internal visuals when external sensory data is occluded. However, this hypothesis has yet to be fully supported with neurochemical evidence and plausible mechanism for DMT production.

The condition where a person lacks mental imagery is called aphantasia. The term was first suggested in a 2015 study.

Common examples of mental images include daydreaming and the mental visualization that occurs while reading a book. Another is of the pictures summoned by athletes during training or before a competition, outlining each step they will take to accomplish their goal. When a musician hears a song, he or she can sometimes "see" the song notes in their head, as well as hear them with all their tonal qualities. This is considered different from an after-effect, such as an afterimage. Calling up an image in our minds can be a voluntary act, so it can be characterized as being under various degrees of conscious control.

According to psychologist and cognitive scientist Steven Pinker, our experiences of the world are represented in our minds as mental images. These mental images can then be associated and compared with others, and can be used to synthesize completely new images. In this view, mental images allow us to form useful theories of how the world works by formulating likely sequences of mental images in our heads without having to directly experience that outcome. Whether other creatures have this capability is debatable.

There are several theories as to how mental images are formed in the mind. These include the dual-code theory, the propositional theory, and the functional-equivalency hypothesis. The dual-code theory, created by Allan Paivio in 1971, is the theory that we use two separate codes to represent information in our brains: image codes and verbal codes. Image codes are things like thinking of a picture of a dog when you are thinking of a dog, whereas a verbal code would be to think of the word "dog". Another example is the difference between thinking of abstract words such as "justice" or "love" and thinking of concrete words like "elephant" or "chair." When abstract words are thought of, it is easier to think of them in terms of verbal codes—finding words that define them or describe them. With concrete words, it is often easier to use image codes and bring up a picture of a "human" or "chair" in your mind rather than words associated or descriptive of them.

The propositional theory involves storing images in the form of a generic propositional code that stores the meaning of the concept not the image itself. The propositional codes can either be descriptive of the image or symbolic. They are then transferred back into verbal and visual code to form the mental image.

The functional-equivalency hypothesis is that mental images are "internal representations" that work in the same way as the actual perception of physical objects. In other words, the picture of a dog brought to mind when the word "dog" is read is interpreted in the same way as if the person looking at an actual dog before them.

Research has occurred to designate a specific neural correlate of imagery; however, studies show a multitude of results. Most studies published before 2001 suggest neural correlates of visual imagery occur in Brodmann area 17. Auditory performance imagery have been observed in the premotor areas, precunes, and medial Brodmann area 40. Auditory imagery in general occurs across participants in the temporal voice area (TVA), which allows top-down imaging manipulations, processing, and storage of audition functions. Olfactory imagery research shows activation in the anterior piriform cortex and the posterior piriform cortex; experts in olfactory imagery have larger gray matter associated to olfactory areas. Tactile imagery is found to occur in the dorsolateral prefrontal area, inferior frontal gyrus, frontal gyrus, insula, precentral gyrus, and the medial frontal gyrus with basal ganglia activation in the ventral posteriomedial nucleus and putamen (hemisphere activation corresponds to the location of the imagined tactile stimulus). Research in gustatory imagery reveals activation in the anterior insular cortex, frontal operculum, and prefrontal cortex. Novices of a specific form of mental imagery show less gray matter than experts of mental imagery congruent to that form. A meta-analysis of neuroimagery studies revealed significant activation of the bilateral dorsal parietal, interior insula, and left inferior frontal regions of the brain.

Imagery has been thought to cooccur with perception; however, participants with damaged sense-modality receptors can sometimes perform imagery of said modality receptors. Neuroscience with imagery has been used to communicate with seemingly unconscious individuals through fMRI activation of different neural correlates of imagery, demanding further study into low quality consciousness. A study on one patient with one occipital lobe removed found the horizontal area of their visual mental image was reduced.

Visual imagery is the ability to create mental representations of things, people, and places that are absent from an individual’s visual field. This ability is crucial to problem-solving tasks, memory, and spatial reasoning. Neuroscientists have found that imagery and perception share many of the same neural substrates, or areas of the brain that function similarly during both imagery and perception, such as the visual cortex and higher visual areas. Kosslyn and colleagues (1999) showed that the early visual cortex, Area 17 and Area 18/19, is activated during visual imagery. They found that inhibition of these areas through repetitive transcranial magnetic stimulation (rTMS) resulted in impaired visual perception and imagery. Furthermore, research conducted with lesioned patients has revealed that visual imagery and visual perception have the same representational organization. This has been concluded from patients in which impaired perception also experience visual imagery deficits at the same level of the mental representation.

Behrmann and colleagues (1992) describe a patient C.K., who provided evidence challenging the view that visual imagery and visual perception rely on the same representational system. C.K. was a 33-year old man with visual object agnosia acquired after a vehicular accident. This deficit prevented him from being able to recognize objects and copy objects fluidly. Surprisingly, his ability to draw accurate objects from memory indicated his visual imagery was intact and normal. Furthermore, C.K. successfully performed other tasks requiring visual imagery for judgment of size, shape, color, and composition. These findings conflict with previous research as they suggest there is a partial dissociation between visual imagery and visual perception. C.K. exhibited a perceptual deficit that was not associated with a corresponding deficit in visual imagery, indicating that these two processes have systems for mental representations that may not be mediated entirely by the same neural substrates.

Schlegel and colleagues (2013) conducted a functional MRI analysis of regions activated during manipulation of visual imagery. They identified 11 bilateral cortical and subcortical regions that exhibited increased activation when manipulating a visual image compared to when the visual image was just maintained. These regions included the occipital lobe and ventral stream areas, two parietal lobe regions, the posterior parietal cortex and the precuneus lobule, and three frontal lobe regions, the frontal eye fields, dorsolateral prefrontal cortex, and the prefrontal cortex. Due to their suspected involvement in working memory and attention, the authors propose that these parietal and prefrontal regions, and occipital regions, are part of a network involved in mediating the manipulation of visual imagery. These results suggest a top-down activation of visual areas in visual imagery.

Using Dynamic Causal Modeling (DCM) to determine the connectivity of cortical networks, Ishai et al. (2010) demonstrated that activation of the network mediating visual imagery is initiated by prefrontal cortex and posterior parietal cortex activity. Generation of objects from memory resulted in initial activation of the prefrontal and the posterior parietal areas, which then activate earlier visual areas through backward connectivity. Activation of the prefrontal cortex and posterior parietal cortex has also been found to be involved in retrieval of object representations from long-term memory, their maintenance in working memory, and attention during visual imagery. Thus, Ishai et al. suggest that the network mediating visual imagery is composed of attentional mechanisms arising from the posterior parietal cortex and the prefrontal cortex.

Vividness of visual imagery is a crucial component of an individual’s ability to perform cognitive tasks requiring imagery. Vividness of visual imagery varies not only between individuals but also within individuals. Dijkstra and colleagues (2017) found that the variation in vividness of visual imagery is dependent on the degree to which the neural substrates of visual imagery overlap with those of visual perception. They found that overlap between imagery and perception in the entire visual cortex, the parietal precuneus lobule, the right parietal cortex, and the medial frontal cortex predicted the vividness of a mental representation. The activated regions beyond the visual areas are believed to drive the imagery-specific processes rather than the visual processes shared with perception. It has been suggested that the precuneus contributes to vividness by selecting important details for imagery. The medial frontal cortex is suspected to be involved in the retrieval and integration of information from the parietal and visual areas during working memory and visual imagery. The right parietal cortex appears to be important in attention, visual inspection, and stabilization of mental representations. Thus, the neural substrates of visual imagery and perception overlap in areas beyond the visual cortex and the degree of this overlap in these areas correlates with the vividness of mental representations during imagery.

Mental images are an important topic in classical and modern philosophy, as they are central to the study of knowledge. In the "Republic", Book VII, Plato has Socrates present the Allegory of the Cave: a prisoner, bound and unable to move, sits with his back to a fire watching the shadows cast on the cave wall in front of him by people carrying objects behind his back. These people and the objects they carry are representations of real things in the world. Unenlightened man is like the prisoner, explains Socrates, a human being making mental images from the sense data that he experiences.

The eighteenth-century philosopher Bishop George Berkeley proposed similar ideas in his theory of idealism. Berkeley stated that reality is equivalent to mental images—our mental images are not a copy of another material reality but that reality itself. Berkeley, however, sharply distinguished between the images that he considered to constitute the external world, and the images of individual imagination. According to Berkeley, only the latter are considered "mental imagery" in the contemporary sense of the term.

The eighteenth century British writer Dr. Samuel Johnson criticized idealism. When asked what he thought about idealism, he is alleged to have replied "I refute it thus!" as he kicked a large rock and his leg rebounded. His point was that the idea that the rock is just another mental image and has no material existence of its own is a poor explanation of the painful sense data he had just experienced.

David Deutsch addresses Johnson's objection to idealism in "The Fabric of Reality" when he states that, if we judge the value of our mental images of the world by the quality and quantity of the sense data that they can explain, then the most valuable mental image—or theory—that we currently have is that the world has a real independent existence and that humans have successfully evolved by building up and adapting patterns of mental images to explain it. This is an important idea in scientific thought.

Critics of scientific realism ask how the inner perception of mental images actually occurs. This is sometimes called the "homunculus problem" (see also the mind's eye). The problem is similar to asking how the images you see on a computer screen exist in the memory of the computer. To scientific materialism, mental images and the perception of them must be brain-states. According to critics, scientific realists cannot explain where the images and their perceiver exist in the brain. To use the analogy of the computer screen, these critics argue that cognitive science and psychology have been unsuccessful in identifying either the component in the brain (i.e., "hardware") or the mental processes that store these images (i.e. "software").

Cognitive psychologists and (later) cognitive neuroscientists have empirically tested some of the philosophical questions related to whether and how the human brain uses mental imagery in cognition.

One theory of the mind that was examined in these experiments was the "brain as serial computer" philosophical metaphor of the 1970s. Psychologist Zenon Pylyshyn theorized that the human mind processes mental images by decomposing them into an underlying mathematical proposition. Roger Shepard and Jacqueline Metzler challenged that view by presenting subjects with 2D line drawings of groups of 3D block "objects" and asking them to determine whether that "object" is the same as a second figure, some of which rotations of the first "object". Shepard and Metzler proposed that if we decomposed and then mentally re-imaged the objects into basic mathematical propositions, as the then-dominant view of cognition "as a serial digital computer" assumed, then it would be expected that the time it took to determine whether the object is the same or not would be independent of how much the object had been rotated. Shepard and Metzler found the opposite: a linear relationship between the degree of rotation in the mental imagery task and the time it took participants to reach their answer.

This mental rotation finding implied that the human mind—and the human brain—maintains and manipulates mental images as topographic and topological wholes, an implication that was quickly put to test by psychologists. Stephen Kosslyn and colleagues showed in a series of neuroimaging experiments that the mental image of objects like the letter "F" are mapped, maintained and rotated as an image-like whole in areas of the human visual cortex. Moreover, Kosslyn's work showed that there are considerable similarities between the neural mappings for imagined stimuli and perceived stimuli. The authors of these studies concluded that, while the neural processes they studied rely on mathematical and computational underpinnings, the brain also seems optimized to handle the sort of mathematics that constantly computes a series of topologically-based images rather than calculating a mathematical model of an object.

Recent studies in neurology and neuropsychology on mental imagery have further questioned the "mind as serial computer" theory, arguing instead that human mental imagery manifests both visually and kinesthetically. For example, several studies have provided evidence that people are slower at rotating line drawings of objects such as hands in directions incompatible with the joints of the human body, and that patients with painful, injured arms are slower at mentally rotating line drawings of the hand from the side of the injured arm.

Some psychologists, including Kosslyn, have argued that such results occur because of interference in the brain between distinct systems in the brain that process the visual and motor mental imagery. Subsequent neuroimaging studies showed that the interference between the motor and visual imagery system could be induced by having participants physically handle actual 3D blocks glued together to form objects similar to those depicted in the line-drawings. Amorim et al. have shown that, when a cylindrical "head" was added to Shepard and Metzler's line drawings of 3D block figures, participants were quicker and more accurate at solving mental rotation problems. They argue that motoric embodiment is not just "interference" that inhibits visual mental imagery but is capable of facilitating mental imagery.

As cognitive neuroscience approaches to mental imagery continued, research expanded beyond questions of serial versus parallel or topographic processing to questions of the relationship between mental images and perceptual representations. Both brain imaging (fMRI and ERP) and studies of neuropsychological patients have been used to test the hypothesis that a mental image is the reactivation, from memory, of brain representations normally activated during the perception of an external stimulus. In other words, if perceiving an apple activates contour and location and shape and color representations in the brain’s visual system, then imagining an apple activates some or all of these same representations using information stored in memory. Early evidence for this idea came from neuropsychology. Patients with brain damage that impairs perception in specific ways, for example by damaging shape or color representations, seem to generally to have impaired mental imagery in similar ways. Studies of brain function in normal human brains support this same conclusion, showing activity in the brain’s visual areas while subjects imagined visual objects and scenes.

The previously mentioned and numerous related studies have led to a relative consensus within cognitive science, psychology, neuroscience, and philosophy on the neural status of mental images. In general, researchers agree that, while there is no homunculus inside the head viewing these mental images, our brains do form and maintain mental images as image-like wholes. The problem of exactly how these images are stored and manipulated within the human brain, in particular within language and communication, remains a fertile area of study.

One of the longest-running research topics on the mental image has basis on the fact that people report large individual differences in the vividness of their images. Special questionnaires have been developed to assess such differences, including the Vividness of Visual Imagery Questionnaire (VVIQ) developed by David Marks. Laboratory studies have suggested that the subjectively reported variations in imagery vividness are associated with different neural states within the brain and also different cognitive competences such as the ability to accurately recall information presented in pictures Rodway, Gillies and Schepman used a novel long-term change detection task to determine whether participants with low and high vividness scores on the VVIQ2 showed any performance differences. Rodway et al. found that high vividness participants were significantly more accurate at detecting salient changes to pictures compared to low-vividness participants. This replicated an earlier study.

Recent studies have found that individual differences in VVIQ scores can be used to predict changes in a person's brain while visualizing different activities. Functional magnetic resonance imaging (fMRI) was used to study the association between early visual cortex activity relative to the whole brain while participants visualized themselves or another person bench pressing or stair climbing. Reported image vividness correlates significantly with the relative fMRI signal in the visual cortex. Thus, individual differences in the vividness of visual imagery can be measured objectively.

Logie, Pernet, Buonocore and Della Sala (2011) used behavioural and fMRI data for mental rotation from individuals reporting vivid and poor imagery on the VVIQ. Groups differed in brain activation patterns suggesting that the groups performed the same tasks in different ways. These findings help to explain the lack of association previously reported between VVIQ scores and mental rotation performance.

Some educational theorists have drawn from the idea of mental imagery in their studies of learning styles. Proponents of these theories state that people often have learning processes that emphasize visual, auditory, and kinesthetic systems of experience. According to these theorists, teaching in multiple overlapping sensory systems benefits learning, and they encourage teachers to use content and media that integrates well with the visual, auditory, and kinesthetic systems whenever possible.

Educational researchers have examined whether the experience of mental imagery affects the degree of learning. For example, imagining playing a five-finger piano exercise (mental practice) resulted in a significant improvement in performance over no mental practice—though not as significant as that produced by physical practice. The authors of the study stated that "mental practice alone seems to be sufficient to promote the modulation of neural circuits involved in the early stages of motor skill learning".

In general, Vajrayana Buddhism, Bön, and Tantra utilize sophisticated visualization or "imaginal" (in the language of Jean Houston of Transpersonal Psychology) processes in the thoughtform construction of the yidam sadhana, kye-rim, and dzog-rim modes of meditation and in the yantra, thangka, and mandala traditions, where holding the fully realized form in the mind is a prerequisite prior to creating an 'authentic' new art work that will provide a sacred support or foundation for deity.

Mental imagery can act as a substitute for the imagined experience: Imagining an experience can evoke similar cognitive, physiological, and/or behavioral consequences as having the corresponding experience in reality. At least four classes of such effects have been documented.



</doc>
<doc id="698226" url="https://en.wikipedia.org/wiki?curid=698226" title="Concept map">
Concept map

A concept map or conceptual diagram is a diagram that depicts suggested relationships between concepts. It is a graphical tool that instructional designers, engineers, technical writers, and others use to organize and structure knowledge.

A concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure. The relationship between concepts can be articulated in "linking phrases" such as "causes", "requires", or "contributes to".

The technique for visualizing these relationships among different concepts is called "concept mapping". Concept maps have been used to define the ontology of computer systems, for example with the object-role modeling or Unified Modeling Language formalism.

A concept map is a way of representing relationships between ideas, images, or words in the same way that a sentence diagram represents the grammar of a sentence, a road map represents the locations of highways and towns, and a circuit diagram represents the workings of an electrical appliance. In a concept map, each word or phrase connects to another, and links back to the original idea, word, or phrase. Concept maps are a way to develop logical thinking and study skills by revealing connections and helping students see how individual ideas form a larger whole. An example of the use of concept maps is provided in the context of learning about types of fuel.

Concept maps were developed to enhance meaningful learning in the sciences. A well-made concept map grows within a "context frame" defined by an explicit "focus question", while a mind map often has only branches radiating out from a central picture. Some research evidence suggests that the brain stores knowledge as productions (situation-response conditionals) that act on declarative memory content, which is also referred to as chunks or propositions. Because concept maps are constructed to reflect organization of the declarative memory system, they facilitate sense-making and meaningful learning on the part of individuals who make concept maps and those who use them.


Concept mapping was developed by Joseph D. Novak and his research team at Cornell University in the 1970s as a means of representing the emerging science knowledge of students. It has subsequently been used as a tool to increase meaningful learning in the sciences and other subjects as well as to represent the expert knowledge of individuals and teams in education, government and business. Concept maps have their origin in the learning movement called constructivism. In particular, constructivists hold that learners actively construct knowledge.

Novak's work is based on the cognitive theories of David Ausubel, who stressed the importance of prior knowledge in being able to learn (or "assimilate") new concepts: "The most important single factor influencing learning is what the learner already knows. Ascertain this and teach accordingly." Novak taught students as young as six years old to make concept maps to represent their response to focus questions such as "What is water?" "What causes the seasons?" In his book "Learning How to Learn", Novak states that a "meaningful learning involves the assimilation of new concepts and propositions into existing cognitive structures."

Various attempts have been made to conceptualize the process of creating concept maps. Ray McAleese, in a series of articles, has suggested that mapping is a process of "off-loading". In this 1998 paper, McAleese draws on the work of Sowa and a paper by Sweller & Chandler. In essence, McAleese suggests that the process of making knowledge explicit, using "nodes" and "relationships", allows the individual to become aware of what they know and as a result to be able to modify what they know. Maria Birbili applies that same idea to helping young children learn to think about what they know. The concept of the "knowledge arena" is suggestive of a virtual space where learners may explore what they know and what they do not know.

Concept maps are used to stimulate the generation of ideas, and are believed to aid creativity. Concept mapping is also sometimes used for brain-storming. Although they are often personalized and idiosyncratic, concept maps can be used to communicate complex ideas.

Formalized concept maps are used in software design, where a common usage is Unified Modeling Language diagramming amongst similar conventions and development methodologies.

Concept mapping can also be seen as a first step in ontology-building, and can also be used flexibly to represent formal argument — similar to argument maps.

Concept maps are widely used in education and business. Uses include:



</doc>
<doc id="30226192" url="https://en.wikipedia.org/wiki?curid=30226192" title="Polar concept argument">
Polar concept argument

A polar concept argument is a type of argument that posits the understanding of one concept, from the mere understanding of its polar opposite. A well-known instance of a polar concept argument is Gilbert Ryle's argument against scepticism (1960). According to Anthony Grayling's characterisation, Ryle's argument can be stated as follows:

According to Ryle's polar concept argument, counterfeit and genuine coins come in pairs, and one cannot conceive of counterfeit coins without also capturing the essence of the genuine coins at the same time. When one grasps the essence of one polar concept, one also grasps immediately the essence of its polar opposite. Ryle's original argument (1960) runs as follows:

A polar concept argument bears on some more or less strong version of dialectical monism, a philosophical doctrine that views reality as a unified whole, due to the complementarity of polar concepts.



</doc>
<doc id="34644725" url="https://en.wikipedia.org/wiki?curid=34644725" title="Jurisprudence of concepts">
Jurisprudence of concepts

The jurisprudence of concepts was the first "sub-school" of legal positivism, according to which, the written law must reflect concepts, when interpreted. Its main representatives were Ihering, Savigny and Puchta.

This school was, thus, the preceding trigger of the idea that law comes from a dogmatic source, imposition from man over man and not a "natural" consequence of other sciences or of metaphysical faith.

Among the main characters of the "jurisprudence of concepts" are:

So, according to this school, law should have prevailing sources based upon the legislative process, although needing to be proven by more inclusive ideas of a social sense.



</doc>
<doc id="26167139" url="https://en.wikipedia.org/wiki?curid=26167139" title="Definitionism">
Definitionism

Definitionism (also called the classical theory of concepts) is the school of thought in which it is believed that a proper explanation of a theory consists of all the concepts used by that theory being well-defined. This approach has been criticized for its dismissal of the importance of ostensive definitions.


</doc>
<doc id="10000937" url="https://en.wikipedia.org/wiki?curid=10000937" title="Category (Kant)">
Category (Kant)

In Kant's philosophy, a category ( in the original or "Kategorie" in modern German) is a pure concept of the understanding ("Verstand"). A Kantian category is a characteristic of the appearance of any object in general, before it has been experienced. Kant wrote that "They are concepts of an object in general…." Kant also wrote that, "…pure cоncepts [Categories] of the undеrstanding which apply to objects of intuition in general…." Such a category is not a classificatory division, as the word is commonly used. It is, instead, the condition of the possibility of objects in general, that is, objects as such, any and all objects, not specific objects in particular.

The word comes from the Greek κατηγορία, "katēgoria", meaning "that which can be said, predicated, or publicly declared and asserted, about something." A category is an attribute, property, quality, or characteristic that can be predicated of a thing. "…I remark concerning the categories…that their logical employment consists in their use as predicates of objects." Kant called them "ontological predicates."

A category is that which can be said of everything in general, that is, of anything that is an object. John Stuart Mill wrote: "The Categories, or Predicaments—the former a Greek word, the latter its literal translation in the Latin language—were believed to be an enumeration of all things capable of being named, an enumeration by the "summa genera" (highest kind), i.e., the most extensive classes into which things could be distributed, which, therefore, were so many highest Predicates, one or other of which was supposed capable of being affirmed with truth of every nameable thing whatsoever."

Aristotle had claimed that the following ten predicates or categories could be asserted of anything in general: substance, quantity, quality, relation, action, affection (passivity), place, time (date), position, and state. These are supposed to be the qualities or attributes that can be affirmed of each and every thing in experience. Any particular object that exists in thought must have been able to have the Categories attributed to it as possible predicates because the Categories are the properties, qualities, or characteristics of any possible object in general. The Categories of Aristotle and Kant are the general properties that belong to all things without expressing the peculiar nature of any particular thing. Kant appreciated Aristotle's effort, but said that his table was imperfect because " … as he had no guiding principle, he merely picked them up as they occurred to him..."

The Categories do not provide knowledge of individual, particular objects. Any object, however, must have Categories as its characteristics if it is to be an object of experience. It is presupposed or assumed that anything that is a specific object must possess Categories as its properties because Categories are predicates of an object in general. An object in general does not have all of the Categories as predicates at one time. For example, a general object cannot have the qualitative Categories of reality and negation at the same time. Similarly, an object in general cannot have both unity and plurality as quantitative predicates at once. The Categories of Modality exclude each other. Therefore, a general object cannot simultaneously have the Categories of possibility/impossibility and existence/non–existence as qualities.

Since the Categories are a list of that which can be said of every object, they are related only to human language. In making a verbal statement about an object, a speaker makes a judgment. A general object, that is, every object, has attributes that are contained in Kant's list of Categories. In a judgment, or verbal statement, the Categories are the predicates that can be asserted of every object and all objects.

Kant believed that the ability of the human understanding (German: "Verstand", Greek: "dianoia" "διάνοια", Latin: "ratio") to think about and know an object is the same as the making of a spoken or written judgment about an object. According to him, "Our ability to judge is equivalent to our ability to think."
A judgment is the thought that a thing is known to have a certain quality or attribute. For example, the sentence "The rose is red" is a judgment. Kant created a table of the forms of such judgments as they relate to all objects in general.

This table of judgments was used by Kant as a model for the table of categories. Taken together, these twelvefold tables constitute the formal structure for Kant's architectonic conception of his philosophical system.

Categories are entirely different from the appearances of objects. According to Kant, in order to relate to specific phenomena, categories must be "applied" through time. The way that this is done is called a schema.

Arthur Schopenhauer, in his criticism of the Kantian philosophy, found many errors in Kant's use of the Categories of Quality, Quantity, Relation, and Modality. Schopenhauer also noted that in accordance with Kant's claim, non-human animals would not be able to know objects. Animals would only know impressions on their sense organs, which Kant mistakenly calls perception.




</doc>
<doc id="2381958" url="https://en.wikipedia.org/wiki?curid=2381958" title="Conceptual model">
Conceptual model

A conceptual model is a representation of a system, made of the composition of concepts which are used to help people know, understand, or simulate a subject the model represents. It is also a set of concepts. Some models are physical objects; for example, a toy model which may be assembled, and may be made to work like the object it represents.

The term "conceptual model" may be used to refer to models which are formed after a conceptualization or generalization process. Conceptual models are often abstractions of things in the real world whether physical or social. Semantic studies are relevant to various stages of concept formation. Semantics is basically about concepts, the meaning that thinking beings give to various elements of their experience.

The term "conceptual model" is normal. It could mean "a model of concept" or it could mean "a model that is conceptual." A distinction can be made between "what models are" and "what models are made of". With the exception of iconic models, such as a scale model of Winchester Cathedral, most models are concepts. But they are, mostly, intended to be models of real world states of affairs. The value of a model is usually directly proportional to how well it corresponds to a past, present, future, actual or potential state of affairs. A model of a concept is quite different because in order to be a good model it need not have this real world correspondence. In artificial intelligence conceptual models and conceptual graphs are used for building expert systems and knowledge-based systems; here the analysts are concerned to represent expert opinion on what is true not their own ideas on what is true.

Conceptual models (models that are conceptual) range in type from the more concrete, such as the mental image of a familiar physical object, to the formal generality and abstractness of mathematical models which do not appear to the mind as an image. Conceptual models also range in terms of the scope of the subject matter that they are taken to represent. A model may, for instance, represent a single thing (e.g. the "Statue of Liberty"), whole classes of things (e.g. "the electron"), and even very vast domains of subject matter such as "the physical universe." The variety and scope of conceptual models is due to the variety of purposes had by the people using them.
Conceptual modeling is the activity of formally describing some aspects of the physical and social world around us for the purposes of understanding and communication."

A conceptual model's primary objective is to convey the fundamental principles and basic functionality of the system which it represents. Also, a conceptual model must be developed in such a way as to provide an easily understood system interpretation for the model's users. A conceptual model, when implemented properly, should satisfy four fundamental objectives.


The conceptual model plays an important role in the overall system development life cycle. Figure 1 below, depicts the role of the conceptual model in a typical system development scheme. It is clear that if the conceptual model is not fully developed, the execution of fundamental system properties may not be implemented properly, giving way to future problems or system shortfalls. These failures do occur in the industry and have been linked to; lack of user input, incomplete or unclear requirements, and changing requirements. Those weak links in the system design and development process can be traced to improper execution of the fundamental objectives of conceptual modeling. The importance of conceptual modeling is evident when such systemic failures are mitigated by thorough system development and adherence to proven development objectives/techniques.

As systems have become increasingly complex, the role of conceptual modelling has dramatically expanded. With that expanded presence, the effectiveness of conceptual modeling at capturing the fundamentals of a system is being realized. Building on that realization, numerous conceptual modeling techniques have been created. These techniques can be applied across multiple disciplines to increase the user's understanding of the system to be modeled. A few techniques are briefly described in the following text, however, many more exist or are being developed. Some commonly used conceptual modeling techniques and methods include: workflow modeling, workforce modeling, rapid application development, object-role modeling, and the Unified Modeling Language (UML).

Data flow modeling (DFM) is a basic conceptual modeling technique that graphically represents elements of a system. DFM is a fairly simple technique, however, like many conceptual modeling techniques, it is possible to construct higher and lower level representative diagrams. The data flow diagram usually does not convey complex system details such as parallel development considerations or timing information, but rather works to bring the major system functions into context. Data flow modeling is a central technique used in systems development that utilizes the structured systems analysis and design method (SSADM).

Entity-relationship modeling (ERM) is a conceptual modeling technique used primarily for software system representation. Entity-relationship diagrams, which are a product of executing the ERM technique, are normally used to represent database models and information systems. The main components of the diagram are the entities and relationships. The entities can represent independent functions, objects, or events. The relationships are responsible for relating the entities to one another. To form a system process, the relationships are combined with the entities and any attributes needed to further describe the process. Multiple diagramming conventions exist for this technique; IDEF1X, Bachman, and EXPRESS, to name a few. These conventions are just different ways of viewing and organizing the data to represent different system aspects.

The event-driven process chain (EPC) is a conceptual modeling technique which is mainly used to systematically improve business process flows. Like most conceptual modeling techniques, the event driven process chain consists of entities/elements and functions that allow relationships to be developed and processed. More specifically, the EPC is made up of events which define what state a process is in or the rules by which it operates. In order to progress through events, a function/ active event must be executed. Depending on the process flow, the function has the ability to transform event states or link to other event driven process chains. Other elements exist within an EPC, all of which work together to define how and by what rules the system operates. The EPC technique can be applied to business practices such as resource planning, process improvement, and logistics.

The dynamic systems development method uses a specific process called JEFFF to conceptually model a systems life cycle. JEFFF is intended to focus more on the higher level development planning that precedes a project's initialization. The JAD process calls for a series of workshops in which the participants work to identify, define, and generally map a successful project from conception to completion. This method has been found to not work well for large scale applications, however smaller applications usually report some net gain in efficiency.

Also known as Petri nets, this conceptual modeling technique allows a system to be constructed with elements that can be described by direct mathematical means. The petri net, because of its nondeterministic execution properties and well defined mathematical theory, is a useful technique for modeling concurrent system behavior, i.e. simultaneous process executions.

State transition modeling makes use of state transition diagrams to describe system behavior. These state transition diagrams use distinct states to define system behavior and changes. Most current modeling tools contain some kind of ability to represent state transition modeling. The use of state transition models can be most easily recognized as logic state diagrams and directed graphs for finite-state machines.

Because the conceptual modeling method can sometimes be purposefully vague to account for a broad area of use, the actual application of concept modeling can become difficult. To alleviate this issue, and shed some light on what to consider when selecting an appropriate conceptual modeling technique, the framework proposed by Gemino and Wand will be discussed in the following text. However, before evaluating the effectiveness of a conceptual modeling technique for a particular application, an important concept must be understood; Comparing conceptual models by way of specifically focusing on their graphical or top level representations is shortsighted. Gemino and Wand make a good point when arguing that the emphasis should be placed on a conceptual modeling language when choosing an appropriate technique. In general, a conceptual model is developed using some form of conceptual modeling technique. That technique will utilize a conceptual modeling language that determines the rules for how the model is arrived at. Understanding the capabilities of the specific language used is inherent to properly evaluating a conceptual modeling technique, as the language reflects the techniques descriptive ability. Also, the conceptual modeling language will directly influence the depth at which the system is capable of being represented, whether it be complex or simple.

Building on some of their earlier work, Gemino and Wand acknowledge some main points to consider when studying the affecting factors: the content that the conceptual model must represent, the method in which the model will be presented, the characteristics of the model's users, and the conceptual model languages specific task. The conceptual model's content should be considered in order to select a technique that would allow relevant information to be presented. The presentation method for selection purposes would focus on the technique's ability to represent the model at the intended level of depth and detail. The characteristics of the model's users or participants is an important aspect to consider. A participant's background and experience should coincide with the conceptual model's complexity, else misrepresentation of the system or misunderstanding of key system concepts could lead to problems in that system's realization. The conceptual model language task will further allow an appropriate technique to be chosen. The difference between creating a system conceptual model to convey system functionality and creating a system conceptual model to interpret that functionality could involve two completely different types of conceptual modeling languages.

Gemino and Wand go on to expand the affected variable content of their proposed framework by considering the focus of observation and the criterion for comparison. The focus of observation considers whether the conceptual modeling technique will create a "new product", or whether the technique will only bring about a more intimate understanding of the system being modeled. The criterion for comparison would weigh the ability of the conceptual modeling technique to be efficient or effective. A conceptual modeling technique that allows for development of a system model which takes all system variables into account at a high level may make the process of understanding the system functionality more efficient, but the technique lacks the necessary information to explain the internal processes, rendering the model less effective.

When deciding which conceptual technique to use, the recommendations of Gemino and Wand can be applied in order to properly evaluate the scope of the conceptual model in question. Understanding the conceptual models scope will lead to a more informed selection of a technique that properly addresses that particular model. In summary, when deciding between modeling techniques, answering the following questions would allow one to address some important conceptual modeling considerations.

Another function of the simulation conceptual model is to provide a rational and factual basis for assessment of simulation application appropriateness.

In cognitive psychology and philosophy of mind, a mental model is a representation of something in the mind, but a mental model may also refer to a nonphysical external model of the mind itself.

A metaphysical model is a type of conceptual model which is distinguished from other conceptual models by its proposed scope; a metaphysical model intends to represent reality in the broadest possible way. This is to say that it explains the answers to fundamental questions such as whether matter and mind are one or two substances; or whether or not humans have free will.

Conceptual Models and semantic models have many similarities, however the way they are presented, the level of flexibility and the use are different.
Conceptual models have a certain purpose in mind, hence the core semantic concepts are predefined in a so-called meta model. This enables a pragmatic modelling but reduces the flexibility, as only the predefined semantic concepts can be used. Samples are flow charts for process behaviour or organisational structure for tree behaviour.

Semantic models are more flexible and open, and therefore more difficult to model. Potentially any semantic concept can be defined, hence the modelling support is very generic. Samples are terminologies, taxonomies or ontologies.

In a concept model each concept has a unique and distinguishable graphical representation, whereas semantic concepts are by default the same. 
In a concept model each concept has predefined properties that can be populated, whereas semantic concepts are related to concepts that are interpreted as properties.
In a concept model operational semantic can be built-in, like the processing of a sequence, whereas a semantic model needs explicit semantic definition of the sequence.

The decision if a concept model or a semantic model is used, depends therefore on the "object under survey", the intended goal, the necessary flexibility as well as how the model is interpreted. In case of human-interpretation there may be a focus on graphical concept models, in case of machine interpretation there may be the focus on semantic models.

An epistemological model is a type of conceptual model whose proposed scope is the known and the knowable, and the believed and the believable.

In logic, a model is a type of interpretation under which a particular statement is true. Logical models can be broadly divided into ones which only attempt to represent concepts, such as mathematical models; and ones which attempt to represent physical objects, and factual relationships, among which are scientific models.

Model theory is the study of (classes of) mathematical structures such as groups, fields, graphs, or even universes of set theory, using tools from mathematical logic. A system that gives meaning to the sentences of a formal language is called a model for the language. If a model for a language moreover satisfies a particular sentence or theory (set of sentences), it is called a model of the sentence or theory. Model theory has close ties to algebra and universal algebra.

Mathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures.

A more comprehensive type of mathematical model uses a linguistic version of category theory to model a given situation. Akin to entity-relationship models, custom categories or sketches can be directly translated into database schemas. The difference is that logic is replaced by category theory, which brings powerful theorems to bear on the subject of modeling, especially useful for translating between disparate models (as functors between categories).

A scientific model is a simplified abstract view of a complex reality. A scientific model represents empirical objects, phenomena, and physical processes in a logical way. Attempts to formalize the principles of the empirical sciences use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system for which reality is the only interpretation. The world is an interpretation (or model) of these sciences, only insofar as these sciences are true.

A statistical model is a probability distribution function proposed as generating data. In a parametric model, the probability distribution function has variable parameters, such as the mean and variance in a normal distribution, or the coefficients for the various exponents of the independent variable in linear regression. A nonparametric model has a distribution function without parameters, such as in bootstrapping, and is only loosely confined by assumptions. Model selection is a statistical method for selecting a distribution function within a class of them; e.g., in linear regression where the dependent variable is a polynomial of the independent variable with parametric coefficients, model selection is selecting the highest exponent, and may be done with nonparametric means, such as with cross validation.

In statistics there can be models of mental events as well as models of physical events. For example, a statistical model of customer behavior is a model that is conceptual (because behavior is physical), but a statistical model of customer satisfaction is a model of a concept (because satisfaction is a mental not a physical event).

In economics, a model is a theoretical construct that represents economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified framework designed to illustrate complex processes, often but not always using mathematical techniques. Frequently, economic models use structural parameters. Structural parameters are underlying parameters in a model or class of models. A model may have various parameters and those parameters may change to create various properties.

A system model is the conceptual model that describes and represents the structure, behavior, and more views of a system. A system model can represent multiple views of a system by using two different approaches. The first one is the non-architectural approach and the second one is the architectural approach. The non-architectural approach respectively picks a model for each view. The architectural approach, also known as system architecture, instead of picking many heterogeneous and unrelated models, will use only one integrated architectural model.

In business process modelling the enterprise process model is often referred to as the "business process model". Process models are core concepts in the discipline of process engineering. Process models are:
The same process model is used repeatedly for the development of many applications and thus, has many instantiations.

One possible use of a process model is to prescribe how things must/should/could be done in contrast to the process itself which is really what happens. A process model is roughly an anticipation of what the process will look like. What the process shall be will be determined during actual system development.

Conceptual models of human activity systems are used in soft systems methodology (SSM), which is a method of systems analysis concerned with the structuring of problems in management. These models are models of concepts; the authors specifically state that they are not intended to represent a state of affairs in the physical world. They are also used in information requirements analysis (IRA) which is a variant of SSM developed for information system design and software engineering.

Logico-linguistic modeling is another variant of SSM that uses conceptual models. However, this method combines models of concepts with models of putative real world objects and events. It is a graphical representation of modal logic in which modal operators are used to distinguish statement about concepts from statements about real world objects and events.

In software engineering, an entity-relationship model (ERM) is an abstract and conceptual representation of data. Entity-relationship modeling is a database modeling method, used to produce a type of conceptual schema or semantic data model of a system, often a relational database, and its requirements in a top-down fashion. Diagrams created by this process are called entity-relationship diagrams, ER diagrams, or ERDs.

Entity-relationship models have had wide application in the building of information systems intended to support activities involving objects and events in the real world. In these cases they are models that are conceptual. However, this modeling method can be used to build computer games or a family tree of the Greek Gods, in these cases it would be used to model concepts.

A domain model is a type of conceptual model used to depict the structural elements and their conceptual constraints within a domain of interest (sometimes called the problem domain). A domain model includes the various entities, their attributes and relationships, plus the constraints governing the conceptual integrity of the structural model elements comprising that problem domain. A domain model may also include a number of conceptual views, where each view is pertinent to a particular subject area of the domain or to a particular subset of the domain model which is of interest to a stakeholder of the domain model.

Like entity-relationship models, domain models can be used to model concepts or to model real world objects and events.




</doc>
<doc id="38191512" url="https://en.wikipedia.org/wiki?curid=38191512" title="Concept-driven strategy">
Concept-driven strategy

A concept-driven strategy is a process for formulating strategy that draws on the explanation of how humans inquire provided by linguistic pragmatic philosophy. This argues that thinking starts by selecting (explicitly or implicitly) a set of concepts (frames, patterns, lens, principles, etc.) gained from our past experiences. These are used to reflect on whatever happens, or is done, in the future.

Concept-driven strategy therefore starts from agreeing and enacting a set of strategic concepts (organizing principles) that "works best" for an organisation. For example, a hospital might set its strategy as intending to be Caring, World Class, Local, Evidence Based, and Team Based. A University might set its strategy as intending to be Ranked, Problem Solving, Online, Equis, and Offering Pathways. A commercial corporation might set its strategy as intending to be Innovative, Global, Have Visible Supply Chains, Agile and Market Share Dominant. These strategic concepts make up its "Statement of Intent" (or Purpose).

Much of the strategic management literature mutates Peter Drucker's call for corporations to start the strategic management process by producing a statement of purpose, mission and objectives. This has been mutated into a call to start with a vision, mission and objectives statement. There is an alternative approach which focuses on the Statement of Purpose or Intent. Drucker's example for this statement for a commercial corporation was to state that the corporation's purpose was to create customers. That is, it was going to use the concept of 'customer creation' to coordinate and organise the cognition or mindset of those that worked for the organisation. This was why the corporation existed. Having one concept is now thought to be insufficient. George Armitage Miller's modified The Magical Number Seven, Plus or Minus Two and dialectic suggests a handful of concepts under tension would be preferable.

The Statement of Purpose, Statement of Intent or concept-driven approach to strategy formulation therefore focuses on setting and enacting a set strategic concepts. If a participatory approach is being used these concepts will be acquired through a process of collaboration with stakeholders. Once agreed the strategic concepts can be used to coordinate activities and act as a set of decision making criteria. The set of concepts that make up the Statement of Intent is then used to make sense of an unpredictable future across an organisation in a co-ordinated manner.

Linguistic pragmatism argues that our prior conceptions interpret our perception (sensory inputs). These conceptions are represented by concepts like running, smiling, justice, reasoning and agility. They are patterns of activity, experienced in our past and remembered. They can be named by those with language and so shared.

Bagginni explains pragmatic concepts using the classic example of whether the earth is flat or round.

Another example would be that we can think of the war in Iraqi differently by reflecting off the concepts of oil security, Imperialism, aggressive capitalism, liberation or democracy. 
The concept-driven approach to strategy formulation involves setting and using a set of linguistic pragmatic concepts.

The steps to formulating a participatory concept-driven strategy are:


Concept-driven strategy is the name given to a number of similar strategic thinking approaches.

Generally, the term 'concept-driven' is used to encourage a focus on the 'concepts' being used. See Concept learning or Management Concepts.

Some organisations produce a 'statement of intent' with little thought as to the concepts it contains. However, if it is a short list of concepts, high level objectives, principles, priorities or frames, then concept-driven strategy offers a philosophical basis for these statements.

Some organisations produce a 'strategic principles' statement which again is similar to a statement of intent and the same applies about the concepts approach offering a philosophical basis. The term 'strategic priorities' or 'strategic values' are often used in the same way as strategic principles.

The literature about 'corporate purpose' is also similar to that of strategic intent. Sometimes, purpose refers to present actions and intent to future ones. If purpose is expressed as a set of concepts, then the concepts approach again provides some philosophical basis.

There is a connection between 'systems thinking' and concept-driven strategy. The Churchman/Ackoff stream of systems thinking was interested in a developing generic system of concepts for thinking about problems. Rather than a generic set of concepts, the concept-driven approach uses whatever concepts stakeholders think work best for the future of their organisation.

There is a military planning approach called 'concept-led'. The military-like leadership seems to have moved the concepts from being drivers to be leaders. There seems to be very little difference otherwise.

In turbulent environments, concepts are thought 'more flexible than objectives' (goals, targets) as they provide why certain actions are preferable. The purpose and intent literature likes to distinguish itself from the objectives literature by saying purpose and intent provide the reasons for (why change), the driver for change. Objectives are where you end up. In complex dynamic situations, there may be many acceptable end points, many of which cannot be anticipated by planners. Arguably the only objective is to survive. How is explained in the statement of intent.

Perhaps strangely, there is a connection between 'metaphor', metaphoric criticism, or conceptual metaphor and concept-driven strategy. Pragmatic concepts are not images but most concepts relate to metaphors. For example, to say an organisation is like a machine, with cogs, or like an adaptive organism, is to use the concepts of machine and organism to reflect on organisations. Much of what has been written about the usefulness of metaphors in planning applies to concepts.

The term 'strategic frames' is not common given the extensive literature on frame analysis but frames and pragmatic concepts seem to be very similar. Amos Tversky defines a frame as a conception of outcomes.

The system of strategic concepts listed in a statement of intent, purpose, principles, frames or conceptual metaphor are organizing principle(s).

Also, as Karl Weick explains sensemaking as the process of conceptualising problems, concept-driven strategy might be thought of as a pragmatic means of sensemaking a strategy.




</doc>
<doc id="19777249" url="https://en.wikipedia.org/wiki?curid=19777249" title="Organizing principle">
Organizing principle

An organizing principle is a core assumption from which everything else by proximity can derive a classification or a value. It is like a central reference point that allows all other objects to be located, often used in a conceptual framework. Having an organizing principle might help one simplify and get a handle on a particularly complicated domain or phenomenon. On the other hand, it might create a deceptive prism that colors one's judgment.





</doc>
<doc id="1809875" url="https://en.wikipedia.org/wiki?curid=1809875" title="Notion (philosophy)">
Notion (philosophy)

A notion in logic and philosophy is a reflection in the mind of real objects and phenomena in their essential features and relations. Notions are usually described in terms of scope (sphere) and content. This is because notions are often created in response to empirical observations (or experiments) of covarying trends among variables.

"Notion" is the common translation for "Begriff" as used by Hegel in his Science of Logic (1816).

A primitive notion is used in logic or mathematics as an undefined term or concept at the foundation of an axiomatic system to be constructed. 

However, in philosophy the term "primitive notion" has historical content. For example, Gottfried Leibniz wrote "De Alphabeto Cogitationum Humanarum" (English: "An Alphabet for Human Thought"). Jaap Maat (2004) reviewed Leibniz for "Philosophical Languages of the 17th Century". According to Leibniz, "The alphabet of human thought is a catalogue of primitive notions, or those we cannot render clearer by any definitions." Maat explains, "a thing which is known without other intermediate notions can be considered to be primitive," and further, "a primitive notion is said to be conceived through itself".

Another example is in the "Meditations" of René Descartes:

"[Descartes'] claim that mind-body unity is a primitive notion on a par with the primitive notions of thinking and extended substance..."



</doc>
<doc id="1005874" url="https://en.wikipedia.org/wiki?curid=1005874" title="Principle">
Principle

A principle is a proposition or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 "Principles of Operation".

Examples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.

In common English, it is a substantive and collective term referring to rule governance, the absence of which, being "unprincipled", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only "in principle" but not in fact.

A principle represents values that orient and rule the conduct of persons in a particular society. To "act on principle" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.

It represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.

Archimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the "population principle", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.

The principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:

However, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that "the Earth is round", corresponds to a direct relation between the subject and the predicate.

According to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.

The principle of the excluding third or "principium tertium exclusum" is a principle of the traditional logic formulated canonically by Leibniz as: either "A" is "B" or "A" isn't "B". It is read the following way: either "P" is true, or its denial ¬"P" is.
It is also known as "tertium non datur" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).



</doc>
<doc id="42415226" url="https://en.wikipedia.org/wiki?curid=42415226" title="Conceptual combination">
Conceptual combination

Conceptual combination is a fundamental cognitive process by which two or more existing basic concepts are mentally synthesized to generate a composite, higher-order concept. The products of this process are sometimes referred to as "complex concepts." Combining concepts allows individuals to use a finite number of concepts which they already understand to construct a potentially limitless quantity of new, related concepts. It is an essential component of many abilities, such as perception, language, synthetic reasoning, creative thought and abstraction.

Conceptual combination is an important concept in the fields of cognitive psychology and cognitive science.

The mechanism by which conceptual combination occurs is debatable, both on cognitive and neurological levels. As such, multiple models have been developed or applied to better define how the process occurs.

Cognitive models attempt to functionally outline the mental computation involved in conceptual combination. 

Constraint theory stipulates that the concept that results from an attempt at conceptual combination is controlled by three constraints known as diagnosticity, plausibility and informativeness. "Diagnosticity" refers to the a complex concept's possession of the defining properties of its component simple concepts. Because such properties are diagnostic of the component concepts, at least some of them should be diagnostic of the higher-order representations constructed from those concepts. "Plausibility" refers to consistency with existing knowledge and prior experience. It is based on the assumption that a complex concept should be reasonably relevant to the context in which it is used. This assumption makes the most sense in a practical, linguistic context, particularly when a speaker is catering to the understanding of the listener. "Informativeness" is the property of having more meaning or properties than any individual component. If a complex concept were not distinguishable from any given component, it would be identical to that component. Because nothing can logically be both a component of something and the totality of something simultaneously, a complex concept must at least be the sum of its parts. Many argue that the interaction among component concepts should allow a complex concept to be greater than that sum. If multiple possible ways to structure or interpret a complex concept exist, the one which best satisfies or most satisfies these constraints is the one which will be used. The paradigm upon which constraint theory is based is computational, and therefore views the mind as a processor which operates on the basis of standard problem-solving protocols (i.e. algorithms and heuristics).

The spreading activation model is a model in connectionist theory sometimes designed to represent how concepts are activated in relation to one another. Though it is typically applied to information search processes like recognition, brainstorming, and recall, it can be used to explain how concepts are combined as well as connected.

Spreading activation models represent memory and knowledge as a network of interlinked concepts. Every concept manifests as a node within this network, with related nodes/concepts linked to one another with connections. Concepts that are more strongly associated with one another either in terms of content or an individual's past experience are correspondingly more strongly linked.

When one concept is employed in working memory, the corresponding node is also activated. This activation spreads through the node's links, making it easier to activate nodes to which the activated node is connected. This spreading activation stimulates the linked nodes, pressuring them to activate to an extent proportional to the strength of the connection between the stimulating node and the stimulated node. If sufficient net stimulation is accrued through a stimulated node's links, it will also activate. Thus, being connected to an activated node makes it easier for an inactive node to become active as well; concepts become more readily accessed when individuals are stimulated with related concepts first. This increase in ease of access is known as "priming."

Spreading activation models tend to imply that processing concepts occurs in series; that is, each concept is processed one-at-a-time, one after the other. As such, individuals tend to combine concepts more readily, easily, and quickly if they are more closely linked within the network of concepts. This implication, however, has caused spreading activation to come under a great deal of criticism, particularly with respect to how the concept is employed in feature theories.

The features and properties of complex concepts are generally assumed to be derived from the simple concepts that compose them. One popularly discussed model involves a two-stage serial process. In the initial stage, features from each of the component simple concepts are retrieved from memory through spreading activation. This allows a complex concept to accrue features with existing relationships with its component simple concepts. During this stage, the basic definition of what the complex concept is and/or means is generates. In the second stage, knowledge and reasoning are employed upon the features accrued in the previous stage to generate further features. For example, one might reason that the complex concept "white jacket," if worn in a blizzard, would make one difficult to see; it would follow that one should ascribe the property of "good for winter camouflage," despite the fact that this property is not closely attached to the component concepts "white" nor "jacket." This stage is especially useful when properties of complex concepts contradict those of their component concepts, such as the different colours of milk and chocolate milk.

This model, however, has come under criticism due to its inconsistency with empirical studies. If conceptual combination employed serial spreading activation, for instance, it should take longer to verify the properties of complex concepts, as they necessarily possess more concepts than their component simple concepts. Research has nonetheless shown that it takes less time to confirm complex concepts' properties than their components and about equal time to reject false properties for both. This occurred even when the properties of the complex concept contradicted those of the simple concepts. Likewise, when experiment participants are exposed to a set of features first, and then asked to verify whether or not they correspond to simple or complex concepts, the participants tend to provide correct verification answers for the complex concepts more quickly.

The neurological basis of conceptual combination has received considerably less attention than its cognitive basis. Nevertheless, research has revealed several specific brain regions that are intimately involved if not entirely responsible for neural processing involved in conceptual combination.

Of particular relevance is the left anterior temporal lobe. Studies have previously demonstrated an additive effect for stimulation in this subsection of neural cortex tissue. When experiment participants were verbally presented with certain simple concepts, the processing of the information causes electrical stimulation in the region. When the same participants were verbally presented with a single complex concept formed from the combination of the aforementioned simple concepts, the stimulation recorded was equivalent to the sum of the stimulation that resulted from each individual component simple concept. In other words, the stimulation caused by a complex concept is equivalent to the total stimulation caused by its component concepts. More recent data contradicts those results by indicating a multiplicative effect in which the activation caused by a complex concept is the product of the activation levels caused by its component concepts, rather than the sum.

Further support for the role of the left anterior temporal lobe has been previously established through neuropsychological studies. Semantic dementia is a disorder in which conceptual manipulation, including conceptual combination, is hindered. These indicate that the neural damage associated with semantic dementia occurs within this brain region. Unfortunately, neuropsychological studies that attempt to replicate this pattern have failed, leading uncertainty as to whether initial results were valid.

As language is the means by which concepts are communicated and expressed, the processed involved in linguistic expression and interpretation are heavily intertwined with combined concepts. Many theories of concept combination mechanisms, including constraint theory were developed within the context of language, and therefore make more sense when applied in a linguistic context. Study into the linguistic aspects of concept combination as has generally been focused on the interpretation mechanism.

A concept that can be expressed using a single word is called a "lexical concept." A lexical concept is usually treated as a basic concept, although it can just as easily be a complex concept.

Two lexical concepts are often used together as phrases to represent a combined concept of greater specificity. This is most readily seen in the use of adjectives to modify nouns and the use of adverbs to modify verbs and adjectives. Consider, for example, phrases such as "burnt toast," "eat roughly," and "readily loved." Multiple noun lexical concepts can also be used together in order to represent combined concepts. Through this process, a limited pool of nouns can be used to produce an exponentially larger pool of phrases such as "sound wave," "video game," and "sleeping pill."

In addition to constraint theory, there are two principal theories surrounding the mechanism by which noun-noun combinations are interpreted. The first of these is "dual-process theory." Dual-process theory proposed that there are two means by which people interpreted noun-noun phrases. "Relational interpretation" attempts to establish a relationship between the nouns and interprets the combined phrase in terms of that relationship. For example, one might relationally interpret the phrase "snake mouse" to refer to a mouse meant to be eaten by snakes, as the two concepts have a predatory relationship. "Property interpretation" identifies properties associated with the first noun and then applies them onto the second noun. In this case the phrase "snake mouse" might be interpreted as a mouse with poisonous fangs or an elongated body.

The second principal theory is known as the "Competition in Relations among Nominals" theory. It states that the assumed modification effect of a noun on its partner in a novel noun-noun combination is the one which it has been seen to employ most often in the past. For example, "chocolate cat" is usually interpreted as "a cat made of chocolate" rather than "a chocolate-eating cat" simply because the "made of" modifier is heavily conditioned to be associated with "chocolate."

Explanations of linguistic expression of complex concepts have been linked to spreading activation models. When an individual identifies a lexical concept through vision or hearing, the corresponding node in that individual's cognitive network is said to activate. This makes it easier for lexical concepts linked to the activated concept to be comprehended, as they are primed. This is consistent with current empirical data, which shows that when individuals are interpreting sentences, they process the linguistic content more quickly when several related words follow one another. In turn, it becomes easier for people to combine these related concepts together and understand them as a relationship, rather than two distinct entities. For example, consider the example, "John spread butter on a bagel." In this sentence, the lexical concepts "spread," "butter," and "bagel" are associated with one another and easy to combine into a mental representation of a breakfast scenario. Conversely, consider the example, "John baked a computer." Because "baked" and "computer" are not related lexical concepts, it takes more effort and time to build a mental representation of this unusual scenario.

However, spreading activation models of conceptual combination have been criticized in light of how humans have been observed to combined languages. Those who claim that the theory provides an insufficient account of linguistic conceptual combination refer to the ability of humans to readily understand lexical concept combinations with seemingly no apparent connection with one another. One example of this would be the sentence "John saw an elephant cloud." "Elephant" and "cloud" do not shared a close association, but it takes little effort to comprehend that the term "elephant cloud" refers to a cloud shaped like an elephant. This has led some to conclude that the combination of lexical concepts does not wholly rely on the simultaneous activation of linked lexical concepts alone. Rather, they claim that the process involves the use of existing nodes to generate entirely new concepts independent of their parent concepts.

Although many theories of novel noun-noun combination interpretation ignore the effect of social environment, some theorists have attempted to account for any contingencies social context may cause.

When lexical concept combinations are interpreted without the influence of social context, the interpretation carried out is termed "sense generation." This includes all processes that would normally occur excepting those dependent on a conversation partner. The "generation hypothesis" accordingly states that the interpretation mechanism of a noun-noun combination is essentially the same regardless of context. This does not rule out the possibility that social context can affect sense generation in some way, but it does assert that the basic structure of the process is unaffected. As seen above, debate as to what sense generation entails and how many sub-processes into which it should be divided is a contentious matter in cognitive science.

The "anaphor resolution hypothesis" instead asserts that before sense generation occurs, interpreters first search their memory of recent communication to see if the combination refers to something previously discussed. This process is termed "anaphor resolution'.' If a referent is identified, interpretation occurs without sense generation in light of that referent. Even if an explicit referent does not exist, anaphor resolution can help facilitate sense generation by providing more information that might hint at the combination's intended meaning.

The "dual-process hypothesis" not to be confused with dual-process theory, states that sense generation and anaphor resolution occur in parallel. Both processes begin to work once the noun-noun combination is presented. Proponents of this hypothesis disagree as to how the interpretation is eventually resolves. Some believe that whichever process reaches a conclusion first provides the answer. Others believe that both provide continuous input to a third, mediating process that eventually makes a decision based on input from both.

Creativity necessitates the employment of existing concepts in novel ways, and therefore requires conceptual combination. Surprisingly, this contribution seems to be limited. Conceptual combination is a significant contributor to convergent thinking, but not divergent thinking. For example, practice with generating new concepts through combination does not improve brainstorming. It does, however, assist in devising creative problem solving methods.

The psychological community's growing understanding of how concepts are manipulated has allowed educators to teach new concepts more effectively. Tools that are developed based on conceptual combination theory attempt to teach individual tasks, and then challenge students to exercise them together in order to promote both base subject skills and the critical thinking needed to apply them simultaneously to solve new problems. Máder & Vajda, for instance, developed a three-dimensional grid with cells of adjustable height which has been successfully used in numerous activities capable of improving the effectiveness of high school mathematics education.


</doc>
<doc id="5370" url="https://en.wikipedia.org/wiki?curid=5370" title="Category of being">
Category of being

In ontology, the different kinds or ways of being are called categories of being; or simply categories. To investigate the categories of being is to determine the most fundamental and the broadest classes of entities. A distinction between such categories, in making the categories or applying them, is called an ontological distinction.

The process of abstraction required to discover the number and names of the categories has been undertaken by many philosophers since Aristotle and involves the careful inspection of each concept to ensure that there is no higher category or categories under which that concept could be subsumed. The scholars of the twelfth and thirteenth centuries developed Aristotle's ideas, firstly, for example by Gilbert of Poitiers, dividing Aristotle's ten categories into two sets, primary and secondary, according to whether they inhere in the subject or not:
Secondly, following Porphyry’s likening of the classificatory hierarchy to a tree, they concluded that the major classes could be subdivided to form subclasses, for example, Substance could be divided into Genus and Species, and Quality could be subdivided into Property and Accident, depending on whether the property was necessary or contingent. An alternative line of development was taken by Plotinus in the second century who by a process of abstraction reduced Aristotle’s list of ten categories to five: Substance, Relation, Quantity, Motion and Quality. Plotinus further suggested that the latter three categories of his list, namely Quantity, Motion and Quality correspond to three different kinds of relation and that these three categories could therefore be subsumed under the category of Relation. This was to lead to the supposition that there were only two categories at the top of the hierarchical tree, namely Substance and Relation, and if relations only exist in the mind as many supposed, to the two highest categories, Mind and Matter, reflected most clearly in the dualism of René Descartes.

An alternative conclusion however began to be formulated in the eighteenth century by Immanuel Kant who realised that we can say nothing about Substance except through the relation of the subject to other things. In the sentence "This is a house" the substantive subject "house" only gains meaning in relation to human use patterns or to other similar houses. The category of Substance disappears from Kant's tables, and under the heading of Relation, Kant lists "inter alia" the three relationship types of Disjunction, Causality and Inherence. The three older concepts of Quantity, Motion and Quality, as Peirce discovered, could be subsumed under these three broader headings in that Quantity relates to the subject through the relation of Disjunction; Motion relates to the subject through the relation of Causality; and Quality relates to the subject through the relation of Inherence. Sets of three continued to play an important part in the nineteenth century development of the categories, most notably in G.W.F. Hegel's extensive tabulation of categories, and in C.S. Peirce's categories set out in his work on the logic of relations. One of Peirce's contributions was to call the three primary categories Firstness, Secondness and Thirdness which both emphasises their general nature, and avoids the confusion of having the same name for both the category itself and for a concept within that category.

In a separate development, and building on the notion of primary and secondary categories introduced by the Scholastics, Kant introduced the idea that secondary or "derivative" categories could be derived from the primary categories through the combination of one primary category with another. This would result in the formation of three secondary categories: the first, "Community" was an example that Kant gave of such a derivative category; the second, "Modality", introduced by Kant, was a term which Hegel, in developing Kant's dialectical method, showed could also be seen as a derivative category; and the third, "Spirit" or "Will" were terms that Hegel and Schopenhauer were developing separately for use in their own systems. Karl Jaspers in the twentieth century, in his development of existential categories, brought the three together, allowing for differences in terminology, as Substantiality, Communication and Will. This pattern of three primary and three secondary categories was used most notably in the nineteenth century by Peter Mark Roget to form the six headings of his Thesaurus of English Words and Phrases. The headings used were the three objective categories of Abstract Relation, Space (including Motion) and Matter and the three subjective categories of Intellect, Feeling and Volition, and he found that under these six headings all the words of the English language, and hence any possible predicate, could be assembled.

In the twentieth century the primacy of the division between the subjective and the objective, or between mind and matter, was disputed by, among others, Bertrand Russell and Gilbert Ryle. Philosophy began to move away from the metaphysics of categorisation towards the linguistic problem of trying to differentiate between, and define, the words being used. Ludwig Wittgenstein’s conclusion was that there were no clear definitions which we can give to words and categories but only a "halo" or "corona" of related meanings radiating around each term. Gilbert Ryle thought the problem could be seen in terms of dealing with "a galaxy of ideas" rather than a single idea, and suggested that category mistakes are made when a concept (e.g. "university"), understood as falling under one category (e.g. abstract idea), is used as though it falls under another (e.g. physical object). With regard to the visual analogies being used, Peirce and Lewis, just like Plotinus earlier, likened the terms of propositions to points, and the relations between the terms to lines. Peirce, taking this further, talked of univalent, bivalent and trivalent relations linking predicates to their subject and it is just the number and types of relation linking subject and predicate that determine the category into which a predicate might fall. Primary categories contain concepts where there is one dominant kind of relation to the subject. Secondary categories contain concepts where there are two dominant kinds of relation. Examples of the latter were given by Heidegger in his two propositions "the house is on the creek" where the two dominant relations are spatial location (Disjunction) and cultural association (Inherence), and "the house is eighteenth century" where the two relations are temporal location (Causality) and cultural quality (Inherence). A third example may be inferred from Kant in the proposition "the house is impressive or sublime" where the two relations are spatial or mathematical disposition (Disjunction) and dynamic or motive power (Causality). Both Peirce and Wittgenstein introduced the analogy of colour theory in order to illustrate the shades of meanings of words. Primary categories, like primary colours, are analytical representing the furthest we can go in terms of analysis and abstraction and include Quantity, Motion and Quality. Secondary categories, like secondary colours, are synthetic and include concepts such as Substance, Community and Spirit.

One of Aristotle’s early interests lay in the classification of the natural world, how for example the genus "animal" could be first divided into "two-footed animal" and then into "wingless, two-footed animal". He realised that the distinctions were being made according to the qualities the animal possesses, the quantity of its parts and the kind of motion that it exhibits. To fully complete the proposition "this animal is ..." Aristotle stated in his work on the Categories that there were ten kinds of predicate where ...

"... each signifies either substance or quantity or quality or relation or where or when or being-in-a-position or having or acting or being acted upon".

He realised that predicates could be simple or complex. The simple kinds consist of a subject and a predicate linked together by the "categorical" or inherent type of relation. For Aristotle the more complex kinds were limited to propositions where the predicate is compounded of two of the above categories for example "this is a horse running". More complex kinds of proposition were only discovered after Aristotle by the Stoic, Chrysippus, who developed the "hypothetical" and "disjunctive" types of syllogism and these were terms which were to be developed through the Middle Ages and were to reappear in Kant's system of categories.

"Category" came into use with Aristotle's essay "Categories", in which he discussed univocal and equivocal terms, predication, and ten categories:

Plotinus in writing his "Enneads" around AD 250 recorded that "philosophy at a very early age investigated the number and character of the existents ... some found ten, others less ... to some the genera were the first principles, to others only a generic classification of existents". He realised that some categories were reducible to others saying "why are not Beauty, Goodness and the virtues, Knowledge and Intelligence included among the primary genera?" He concluded that such transcendental categories and even the categories of Aristotle were in some way posterior to the three Eleatic categories first recorded in Plato's dialogue "Parmenides" and which comprised the following three coupled terms: 

Plotinus called these "the hearth of reality" deriving from them not only the three categories of Quantity, Motion and Quality but also what came to be known as "the three moments of the Neoplatonic world process":

Plotinus likened the three to the centre, the radii and the circumference of a circle, and clearly thought that the principles underlying the categories were the first principles of creation. "From a single root all being multiplies". Similar ideas were to be introduced into Early Christian thought by, for example, Gregory of Nazianzus who summed it up saying "Therefore Unity, having from all eternity arrived by motion at duality, came to rest in trinity".

In the "Critique of Pure Reason" (1781), Immanuel Kant argued that the categories are part of our own mental structure and consist of a set of "a priori" concepts through which we interpret the world around us. These concepts correspond to twelve logical functions of the understanding which we use to make judgements and there are therefore two tables given in the "Critique", one of the Judgements and a corresponding one for the Categories. To give an example, the logical function behind our reasoning from ground to consequence (based on the Hypothetical relation) underlies our understanding of the world in terms of cause and effect (the Causal relation). In each table the number twelve arises from, firstly, an initial division into two: the Mathematical and the Dynamical; a second division of each of these headings into a further two: Quantity and Quality, and Relation and Modality respectively; and, thirdly, each of these then divides into a further three subheadings as follows.
Table of Judgements

Mathematical
Dynamical

Table of Categories

Mathematical
Dynamical

Criticism of Kant's system followed, firstly, by Arthur Schopenhauer, who amongst other things was unhappy with the term "Community", and declared that the tables "do open violence to truth, treating it as nature was treated by old-fashioned gardeners", and secondly, by W.T.Stace who in his book "The Philosophy of Hegel" suggested that in order to make Kant's structure completely symmetrical a third category would need to be added to the Mathematical and the Dynamical. This, he said, Hegel was to do with his category of Notion.

G.W.F. Hegel in his "Science of Logic" (1812) attempted to provide a more comprehensive system of categories than Kant and developed a structure that was almost entirely triadic. So important were the categories to Hegel that he claimed "the first principle of the world, the Absolute, is a system of categories ... the categories must be the reason of which the world is a consequent".

Using his own logical method of combination, later to be called the Hegelian dialectic, of arguing from thesis through antithesis to synthesis, he arrived, as shown in W.T.Stace's work cited, at a hierarchy of some 270 categories. The three very highest categories were Logic, Nature and Spirit. The three highest categories of Logic, however, he called Being, Essence and Notion which he explained as follows:
Schopenhauer's category that corresponded with Notion was that of Idea, which in his "Four-Fold Root of Sufficient Reason" he complemented with the category of the Will. The title of his major work was "The World as Will and Idea". The two other complementary categories, reflecting one of Hegel's initial divisions, were those of Being and Becoming. At around the same time, Goethe was developing his colour theories in the "Farbenlehre" of 1810, and introduced similar principles of combination and complementation, symbolising, for Goethe, "the primordial relations which belong both to nature and vision". Hegel in his "Science of Logic" accordingly asks us to see his system not as a tree but as a circle.

Charles Sanders Peirce, who had read Kant and Hegel closely, and who also had some knowledge of Aristotle, proposed a system of merely three phenomenological categories: Firstness, Secondness, and Thirdness, which he repeatedly invoked in his subsequent writings. Like Hegel, C.S.Peirce attempted to develop a system of categories from a single indisputable principle, in Peirce's case the notion that in the first instance he could only be aware of his own ideas. 

Although Peirce's three categories correspond to the three concepts of relation given in Kant's tables, the sequence is now reversed and follows that given by Hegel, and indeed before Hegel of the three moments of the world-process given by Plotinus. Later, Peirce gave a mathematical reason for there being three categories in that although monadic, dyadic and triadic nodes are irreducible, every node of a higher valency is reducible to a "compound of triadic relations". Ferdinand de Saussure, who was developing "semiology" in France just as Peirce was developing "semiotics" in the US, likened each term of a proposition to "the centre of a constellation, the point where other coordinate terms, the sum of which is indefinite, converge".

Edmund Husserl (1962, 2000) wrote extensively about categorial systems as part of his phenomenology.

For Gilbert Ryle (1949), a category (in particular a "category mistake") is an important semantic concept, but one having only loose affinities to an ontological category.

Contemporary systems of categories have been proposed by John G. Bennett (The Dramatic Universe, 4 vols., 1956–65), Wilfrid Sellars (1974), Reinhardt Grossmann (1983, 1992), Johansson (1989), Hoffman and Rosenkrantz (1994), Roderick Chisholm (1996), Barry Smith (ontologist) (2003), and Jonathan Lowe (2006).





</doc>
<doc id="47792266" url="https://en.wikipedia.org/wiki?curid=47792266" title="Construction of Concept Map">
Construction of Concept Map

Concept is usually perceived as a regularity in events or objects, or in their records. While constructing a concept map, it is essential to keep in mind that the concept be built with reference to a focus question. Hence, initially, the focus question one seeks to answer is carefully chosen, as learners usually tend to deviate from this question relating only to domains, and thus fail to answer the question.

With the selected domain and the focus question, the next step is to identify the key concepts which apply to this domain. About 15 to 25 concepts are sufficient; they are usually ordered in a rank ordered list. Such a list should be established from the most general and inclusive concept for the particular chosen problem. That list will assist at least in the beginning of the construction of the concept map.

The list is referred to as a parking lot, since the list is constantly shuffled to build up the required network of the concept. Two or more concepts are connected to each other using linking words or phrases, which can only then complete a meaningful sentence. Another important characteristic of a concept map is its cross-links, as the relationship between two different domains used in the concept map. These help in a clear representation of the knowledge contained in the concept, and also give a clear background with specific examples. 


</doc>
<doc id="29358535" url="https://en.wikipedia.org/wiki?curid=29358535" title="Comply or explain">
Comply or explain

Comply or explain is a regulatory approach used in the United Kingdom, Germany, the Netherlands and other countries in the field of corporate governance and financial supervision. Rather than setting out binding laws, government regulators (in the UK, the Financial Reporting Council, in Germany, under the Aktiengesetz) set out a code, which listed companies may either comply with, or if they do not comply, explain publicly why they do not. The UK Corporate Governance Code, the German Corporate Governance Code (or Deutscher Corporate Governance Kodex) and the Dutch Corporate Governance Code 'Code Tabaksblat' () use this approach in setting minimum standards for companies in their audit committees, remuneration committees and recommendations for how good companies should divide authority on their boards.

The purpose of "comply or explain" is to "let the market decide" whether a set of standards is appropriate for individual companies. Since a company may deviate from the standard, this approach rejects the view that "one size fits all", but because of the requirement of disclosure of explanations to market investors, anticipates that if investors do not accept a company's explanations, then investors will sell their shares, hence creating a "market sanction", rather than a legal one. The concept was first introduced after the recommendations of the Cadbury Report of 1992.



</doc>
