<doc id="3383" url="https://en.wikipedia.org/wiki?curid=3383" title="Brazil">
Brazil

Brazil (; ), officially the Federative Republic of Brazil (, ), is the largest country in both South America and Latin America. At 8.5 million square kilometers (3.2 million square miles) and with over 211 million people, Brazil is the world's fifth-largest country by area and the sixth most populous. Its capital is Brasília, and its most populous city is São Paulo. The federation is composed of the union of the 26 states, the Federal District, and the 5,570 municipalities. It is the largest country to have Portuguese as an official language and the only one in the Americas; it is also one of the most multicultural and ethnically diverse nations, due to over a century of mass immigration from around the world.

Bounded by the Atlantic Ocean on the east, Brazil has a coastline of . It borders all other countries in South America except Ecuador and Chile and covers 47.3% of the continent's land area. Its Amazon River basin includes a vast tropical forest, home to diverse wildlife, a variety of ecological systems, and extensive natural resources spanning numerous protected habitats. This unique environmental heritage makes Brazil one of 17 megadiverse countries, and is the subject of significant global interest and debate regarding deforestation and environmental protection.

Brazil was inhabited by numerous tribal nations prior to the landing in 1500 of explorer Pedro Álvares Cabral, who claimed the area for the Portuguese Empire. Brazil remained a Portuguese colony until 1808, when the capital of the empire was transferred from Lisbon to Rio de Janeiro. In 1815, the colony was elevated to the rank of kingdom upon the formation of the United Kingdom of Portugal, Brazil and the Algarves. Independence was achieved in 1822 with the creation of the Empire of Brazil, a unitary state governed under a constitutional monarchy and a parliamentary system. The ratification of the first constitution in 1824 led to the formation of a bicameral legislature, now called the National Congress. The country became a presidential republic in 1889 following a military coup d'état. An authoritarian military junta came to power in 1964 and ruled until 1985, after which civilian governance resumed. Brazil's current constitution, formulated in 1988, defines it as a democratic federal republic. Due to its rich culture and history, the country ranks thirteenth in the world by number of UNESCO World Heritage Sites.

Brazil is considered an advanced emerging economy. It has the ninth largest GDP in the world by nominal, and eight by PPP measures. It is one of the world's major breadbaskets, being the largest producer of coffee for the last 150 years. It is classified as an upper-middle income economy by the World Bank and a newly industrialized country, with the largest share of global wealth in Latin America. Brazil is a regional power and sometimes considered a great or a middle power in international affairs. On account of its international recognition and influence, the country is subsequently classified as an emerging power and a potential superpower by several analysts. Brazil is a founding member of the United Nations, the G20, BRICS, Union of South American Nations, Mercosul, Organization of American States, Organization of Ibero-American States and the Community of Portuguese Language Countries.

It is likely that the word "Brazil" comes from the Portuguese word for brazilwood, a tree that once grew plentifully along the Brazilian coast. In Portuguese, brazilwood is called "pau-brasil", with the word "brasil" commonly given the etymology "red like an ember", formed from "brasa" ("ember") and the suffix "-il" (from "-iculum" or "-ilium"). As brazilwood produces a deep red dye, it was highly valued by the European textile industry and was the earliest commercially exploited product from Brazil. Throughout the 16th century, massive amounts of brazilwood were harvested by indigenous peoples (mostly Tupi) along the Brazilian coast, who sold the timber to European traders (mostly Portuguese, but also French) in return for assorted European consumer goods.

The official Portuguese name of the land, in original Portuguese records, was the "Land of the Holy Cross" ("Terra da Santa Cruz"), but European sailors and merchants commonly called it simply the "Land of Brazil" ("Terra do Brasil") because of the brazilwood trade. The popular appellation eclipsed and eventually supplanted the official Portuguese name. Some early sailors called it the "Land of Parrots".

In the Guarani language, an official language of Paraguay, Brazil is called "Pindorama". This was the name the indigenous population gave to the region, meaning "land of the palm trees".

Some of the earliest human remains found in the Americas, Luzia Woman, were found in the area of Pedro Leopoldo, Minas Gerais and provide evidence of human habitation going back at least 11,000 years.

The earliest pottery ever found in the Western Hemisphere was excavated in the Amazon basin of Brazil and radiocarbon dated to 8,000 years ago (6000 BC). The pottery was found near Santarém and provides evidence that the tropical forest region supported a complex prehistoric culture. The Marajoara culture flourished on Marajó in the Amazon delta from 800 CE to 1400 CE, developing sophisticated pottery, social stratification, large populations, mound building, and complex social formations such as chiefdoms.

Around the time of the Portuguese arrival, the territory of current day Brazil had an estimated indigenous population of 7 million people, mostly semi-nomadic, who subsisted on hunting, fishing, gathering, and migrant agriculture. The indigenous population of Brazil comprised several large indigenous ethnic groups (e.g. the Tupis, Guaranis, Gês and Arawaks). The Tupí people were subdivided into the Tupiniquins and Tupinambás, and there were also many subdivisions of the other groups.

Before the arrival of the Europeans, the boundaries between these groups and their subgroups were marked by wars that arose from differences in culture, language and moral beliefs. These wars also involved large-scale military actions on land and water, with cannibalistic rituals on prisoners of war. While heredity had some weight, leadership status was more subdued over time, than allocated in succession ceremonies and conventions. Slavery among the Indians had a different meaning than it had for Europeans, since it originated from a diverse socioeconomic organization, in which asymmetries were translated into kinship relations.

The land now called Brazil was claimed for the Portuguese Empire on 22 April 1500, with the arrival of the Portuguese fleet commanded by Pedro Álvares Cabral. The Portuguese encountered indigenous peoples divided into several tribes, most of whom spoke languages of the Tupi–Guarani family, and fought among themselves. Though the first settlement was founded in 1532, colonization effectively began in 1534, when King John III of Portugal divided the territory into the fifteen private and autonomous Captaincy Colonies of Brazil.

However, the decentralized and unorganized tendencies of the captaincy colonies proved problematic, and in 1549 the Portuguese king restructured them into the Governorate General of Brazil, a single and centralized Portuguese colony in South America. In the first two centuries of colonization, Indigenous and European groups lived in constant war, establishing opportunistic alliances in order to gain advantages against each other. By the mid-16th century, cane sugar had become Brazil's most important export, and slaves purchased in Sub-Saharan Africa, in the slave market of Western Africa (not only those from Portuguese allies of their colonies in Angola and Mozambique), had become its largest import, to cope with plantations of sugarcane, due to increasing international demand for Brazilian sugar.

By the end of the 17th century, sugarcane exports began to decline, and the discovery of gold by bandeirantes in the 1690s would become the new backbone of the colony's economy, fostering a Brazilian Gold Rush which attracted thousands of new settlers to Brazil from Portugal and all Portuguese colonies around the world. This increased level of immigration in turn caused some conflicts between newcomers and old settlers.

Portuguese expeditions known as Bandeiras gradually advanced the Portugal colonial original frontiers in South America to approximately the current Brazilian borders. In this era other European powers tried to colonize parts of Brazil, in incursions that the Portuguese had to fight, notably the French in Rio during the 1560s, in Maranhão during the 1610s, and the Dutch in Bahia and Pernambuco, during the Dutch–Portuguese War, after the end of Iberian Union.

The Portuguese colonial administration in Brazil had two objectives that would ensure colonial order and the monopoly of Portugal's wealthiest and largest colony: to keep under control and eradicate all forms of slave rebellion and resistance, such as the Quilombo of Palmares, and to repress all movements for autonomy or independence, such as the Minas Conspiracy.
In late 1807, Spanish and Napoleonic forces threatened the security of continental Portugal, causing Prince Regent João, in the name of Queen Maria I, to move the royal court from Lisbon to Rio de Janeiro. There they established some of Brazil's first financial institutions, such as its local stock exchanges, and its National Bank, additionally ending the Portuguese monopoly on Brazilian trade and opening Brazil to other nations. In 1809, in retaliation for being forced into exile, the Prince Regent ordered the Portuguese conquest of French Guiana.

With the end of the Peninsular War in 1814, the courts of Europe demanded that Queen Maria I and Prince Regent João return to Portugal, deeming it unfit for the head of an ancient European monarchy to reside in a colony. In 1815, to justify continuing to live in Brazil, where the royal court had thrived for six years, the Crown established the United Kingdom of Portugal, Brazil, and the Algarves, thus creating a pluricontinental transatlantic monarchic state. However, the leadership in Portugal, resentful of the new status of its larger colony, continued to demand the return of court to Lisbon ("v." Liberal Revolution of 1820). In 1821, acceding to the demands of revolutionaries who had taken the city of Porto, D. João VI departed for Lisbon. There he swore an oath to the new constitution, leaving his son, Prince Pedro de Alcântara, as Regent of the Kingdom of Brazil.

Tensions between Portuguese and Brazilians increased, and the Portuguese Cortes, guided by the new political regime imposed by the 1820 Liberal Revolution, tried to re-establish Brazil as a colony. The Brazilians refused to yield, and Prince Pedro decided to stand with them, declaring the country's independence from Portugal on 7 September 1822. A month later, Prince Pedro was declared the first Emperor of Brazil, with the royal title of Dom Pedro I, resulting in the foundation of the Empire of Brazil.

The Brazilian War of Independence, which had already begun along this process, spread through the northern, northeastern regions and in Cisplatina province. The last Portuguese soldiers surrendered on 8 March 1824; Portugal officially recognized Brazil on 29 August 1825.

On 7 April 1831, worn down by years of administrative turmoil and political dissent with both liberal and conservative sides of politics, including an attempt of republican secession, and unreconciled to the way that absolutists in Portugal had given in the succession of King John VI, Pedro I went to Portugal to reclaim his daughter's crown, abdicating the Brazilian throne in favor of his five-year-old son and heir (who thus became the Empire's second monarch, with the royal title of Dom Pedro II).
As the new Emperor could not exert his constitutional powers until he came of age, a regency was set up by the National Assembly. In the absence of a charismatic figure who could represent a moderate face of power, during this period a series of localized rebellions took place, such as the Cabanagem in Grão-Pará Province, the Malê Revolt in Salvador da Bahia, the Balaiada (Maranhão), the Sabinada (Bahia), and the Ragamuffin War, which began in Rio Grande do Sul and was supported by Giuseppe Garibaldi. These emerged from the dissatisfaction of the provinces with the central power, coupled with old and latent social tensions peculiar to a vast, slaveholding and newly independent nation state. This period of internal political and social upheaval, which included the Praieira revolt in Pernambuco, was overcome only at the end of the 1840s, years after the end of the regency, which occurred with the premature coronation of Pedro II in 1841.

During the last phase of the monarchy, internal political debate centered on the issue of slavery. The Atlantic slave trade was abandoned in 1850, as a result of the British Aberdeen Act, but only in May 1888 after a long process of internal mobilization and debate for an ethical and legal dismantling of slavery in the country, was the institution formally abolished.

The foreign-affairs policies of the monarchy dealt with issues with the countries of the Southern Cone with whom Brazil had borders. Long after the Cisplatine War that resulted in independence for Uruguay, Brazil won three international wars during the 58-year reign of Pedro II. These were the Platine War, the Uruguayan War and the devastating Paraguayan War, the largest war effort in Brazilian history.

Although there was no desire among the majority of Brazilians to change the country's form of government, on 15 November 1889, in attrition with the majority of Army officers, as well as with rural and financial elites (for different reasons), the monarchy was overthrown by a military coup. 15 November is now Republic Day, a national holiday.

The early republican government was nothing more than a military dictatorship, with army dominating affairs both in Rio de Janeiro and in the states. Freedom of the press disappeared and elections were controlled by those in power. Not until 1894, following an economic crisis and a military one, did civilians take power, remaining there until October 1930.

If in relation to its foreign policy, the country in this first republican period maintained a relative balance characterized by a success in resolving border disputes with neighboring countries, only broken by the Acre War (1899–1902) and its involvement in World War I (1914–1918), followed by a failed attempt to exert a prominent role in the League of Nations; Internally, from the "crisis of Encilhamento" and the Armada Revolts, a prolonged cycle of financial, political and social instability began until the 1920s, keeping the country besieged by various rebellions, both civilian and military.
Little by little, a cycle of general instability sparked by these crises undermined the regime to such an extent that in the wake of the murder of his running mate, the defeated opposition presidential candidate Getúlio Vargas, supported by most of the military, successfully led the October 1930 Coup. Vargas and the military were supposed to assume power temporarily, but instead closed the Congress, extinguished the Constitution, ruled with emergency powers and replaced the states' governors with their own supporters.

In the 1930s, three failed attempts to remove Vargas and his supporters from power occurred. The first was the Constitutionalist Revolution in 1932, led by the Paulista oligarchy. The second was a Communist uprising in November 1935, and the last one a "putsch" attempt by local fascists in May 1938. The 1935 uprising created a security crisis in which the Congress transferred more power to the executive. The 1937 "coup d'état" resulted in the cancellation of the 1938 election, formalized Vargas as dictator, beginning the Estado Novo era, which was noted for government brutality and censorship of the press.

Foreign policy during Vargas years was marked by the antecedents and World War II. Brazil remained neutral until August 1942, when the country entered on the allied side, after suffering retaliation by Nazi Germany and Fascist Italy, in a strategic dispute over the South Atlantic. In addition to its participation in the battle of the Atlantic, Brazil also sent an expeditionary force to fight in the Italian campaign.

With the Allied victory in 1945 and the end of the Nazi-fascist regimes in Europe, Vargas's position became unsustainable and he was swiftly overthrown in another military coup, with democracy "reinstated" by the same army that had ended it 15 years earlier. Vargas committed suicide in August 1954 amid a political crisis, after having returned to power by election in 1950.

Several brief interim governments followed Vargas's suicide. Juscelino Kubitschek became president in 1956 and assumed a conciliatory posture towards the political opposition that allowed him to govern without major crises. The economy and industrial sector grew remarkably, but his greatest achievement was the construction of the new capital city of Brasília, inaugurated in 1960.
Kubitschek's successor, Jânio Quadros, resigned in 1961 less than a year after taking office. His vice-president, João Goulart, assumed the presidency, but aroused strong political opposition and was deposed in April 1964 by a coup that resulted in a military regime.

The new regime was intended to be transitory but gradually closed in on itself and became a full dictatorship with the promulgation of the Fifth Institutional Act in 1968. Oppression was not limited to those who resorted to guerrilla tactics to fight the regime, but also reached institutional opponents, artists, journalists and other members of civil society, inside and outside the country through the infamous "Operation Condor". Despite its brutality, like other authoritarian regimes, due to an economic boom, known as an "economic miracle", the regime reached a peak in popularity in the early 1970s.

Slowly however, the wear and tear of years of dictatorial power that had not slowed the repression, even after the defeat of the leftist guerrillas, plus the inability to deal with the economic crises of the period and popular pressure, made an opening policy inevitable, which from the regime side was led by Generals Ernesto Geisel and Golbery do Couto e Silva. With the enactment of the Amnesty Law in 1979, Brazil began a slow return to democracy, which was completed during the 1980s.

Civilians returned to power in 1985 when José Sarney assumed the presidency. He became unpopular during his tenure through failure to control the economic crisis and hyperinflation he inherited from the military regime. Sarney's unsuccessful government led to the election in 1989 of the almost-unknown Fernando Collor, subsequently impeached by the National Congress in 1992.

Collor was succeeded by his vice-president, Itamar Franco, who appointed Fernando Henrique Cardoso Minister of Finance. In 1994, Cardoso produced a highly successful Plano Real, that, after decades of failed economic plans made by previous governments attempting to curb hyperinflation, finally stabilized the Brazilian economy. Cardoso won the 1994 election, and again in 1998.

The peaceful transition of power from Cardoso to his main opposition leader, Luiz Inácio Lula da Silva (elected in 2002 and re-elected in 2006), was seen as proof that Brazil had achieved a long-sought political stability. However, sparked by indignation and frustrations accumulated over decades from corruption, police brutality, inefficiencies of the political establishment and public service, numerous peaceful protests erupted in Brazil from the middle of first term of Dilma Rousseff, who had succeeded Lula after winning election in 2010.

Enhanced by political and economic crises with evidence of involvement by politicians from all the primary political parties in several bribery and tax evasion schemes, with large street protests for and against her, Rousseff was impeached by the Brazilian Congress in 2016. In 2017, the Supreme Court asked for the investigation of 71 Brazilian lawmakers and nine ministers in President Michel Temer's cabinet allegedly linked to the Petrobras corruption scandal. President Temer is himself accused of corruption. In 2018, 62% of the population on a poll claimed that corruption was Brazil's biggest problem.

Brazil occupies a large area along the eastern coast of South America and includes much of the continent's interior, sharing land borders with Uruguay to the south; Argentina and Paraguay to the southwest; Bolivia and Peru to the west; Colombia to the northwest; and Venezuela, Guyana, Suriname and France (French overseas region of French Guiana) to the north. It shares a border with every South American country except Ecuador and Chile.

It also encompasses a number of oceanic archipelagos, such as Fernando de Noronha, Rocas Atoll, Saint Peter and Paul Rocks, and Trindade and Martim Vaz. Its size, relief, climate, and natural resources make Brazil geographically diverse. Including its Atlantic islands, Brazil lies between latitudes 6°N and 34°S, and longitudes 28° and 74°W.

Brazil is the fifth largest country in the world, and third largest in the Americas, with a total area of , including of water. It spans four time zones; from UTC−5 comprising the state of Acre and the westernmost portion of Amazonas, to UTC−4 in the western states, to UTC−3 in the eastern states (the national time) and UTC−2 in the Atlantic islands.

Brazil is the only country in the world that has the equator and the Tropic of Capricorn running through it. Brazilian topography is also diverse and includes hills, mountains, plains, highlands, and scrublands. Much of the terrain lies between and in elevation. The main upland area occupies most of the southern half of the country. The northwestern parts of the plateau consist of broad, rolling terrain broken by low, rounded hills.
The southeastern section is more rugged, with a complex mass of ridges and mountain ranges reaching elevations of up to . These ranges include the Mantiqueira and Espinhaço mountains and the Serra do Mar.

In the north, the Guiana Highlands form a major drainage divide, separating rivers that flow south into the Amazon Basin from rivers that empty into the Orinoco River system, in Venezuela, to the north. The highest point in Brazil is the Pico da Neblina at , and the lowest is the Atlantic Ocean.

Brazil has a dense and complex system of rivers, one of the world's most extensive, with eight major drainage basins, all of which drain into the Atlantic. Major rivers include the Amazon (the world's second-longest river and the largest in terms of volume of water), the Paraná and its major tributary the Iguaçu (which includes the Iguazu Falls), the Negro, São Francisco, Xingu, Madeira and Tapajós rivers.

The climate of Brazil comprises a wide range of weather conditions across a large area and varied topography, but most of the country is tropical. According to the Köppen system, Brazil hosts six major climatic subtypes: desert, equatorial, tropical, semiarid, oceanic and subtropical. The different climatic conditions produce environments ranging from equatorial rainforests in the north and semiarid deserts in the northeast, to temperate coniferous forests in the south and tropical savannas in central Brazil. Many regions have starkly different microclimates.

An equatorial climate characterizes much of northern Brazil. There is no real dry season, but there are some variations in the period of the year when most rain falls. Temperatures average , with more significant temperature variation between night and day than between seasons.

Over central Brazil rainfall is more seasonal, characteristic of a savanna climate. This region is as extensive as the Amazon basin but has a very different climate as it lies farther south at a higher altitude. In the interior northeast, seasonal rainfall is even more extreme.

The semiarid climatic region generally receives less than of rain, most of which generally falls in a period of three to five months of the year and occasionally less than this, creating long periods of drought. Brazil's 1877–78 "Grande Seca" (Great Drought), the worst in Brazil's history, caused approximately half a million deaths. A similarly devastating drought occurred in 1915.

South of Bahia, near the coasts, and more southerly most of the state of São Paulo, the distribution of rainfall changes, with rain falling throughout the year. The south enjoys subtropical conditions, with cool winters and average annual temperatures not exceeding ; winter frosts and snowfall are not rare in the highest areas.

Brazil's large territory comprises different ecosystems, such as the Amazon rainforest, recognized as having the greatest biological diversity in the world, with the Atlantic Forest and the Cerrado, sustaining the greatest biodiversity. In the south, the Araucaria pine forest grows under temperate conditions. The rich wildlife of Brazil reflects the variety of natural habitats. Scientists estimate that the total number of plant and animal species in Brazil could approach four million, mostly invertebrates.

Larger mammals include carnivores pumas, jaguars, ocelots, rare bush dogs, and foxes, and herbivores peccaries, tapirs, anteaters, sloths, opossums, and armadillos. Deer are plentiful in the south, and many species of New World monkeys are found in the northern rain forests. Concern for the environment has grown in response to global interest in environmental issues. Brazil's Amazon Basin is home to an extremely diverse array of fish species, including the red-bellied piranha.

By 2013, Brazil's "dramatic policy-driven reduction in Amazon Basin deforestation" was a "global exception in terms of forest change", according to scientific journal "Science". From 2003 to 2011, compared to all other countries in the world, Brazil had the "largest decline in annual forest loss", as indicated in the study using high-resolution satellite maps showing global forest cover changes. The annual loss of forest cover decreased from a 2003/2004 record high of more than to a 2010/2011 low of under , reversing widespread deforestation from the 1970s to 2003.
However, in 2019, when the Bolsonaro government came to power, the rate of deforestation of the Amazon rainforest increased sharply threatening to reach a tipping point after it the forest will collapse, having severe consequences for the world. (see Tipping points in the climate system) This can also complicate the trade agreement with the European Union Speaking at the UN General Assembly, Bolsonaro criticised what he described as sensational reporting in the international media. "It is a fallacy to say that the Amazon is the heritage of humankind, and a misconception, as confirmed by scientists, to say that our Amazonian forests are the lungs of world. Using these fallacies, certain countries instead of helping, embarked on the media lies and behaved in a disrespectful manner and with a colonialist spirit." he said. President Jair Bolsonaro asserted Brazil's sovereignty over the Amazon.

According to a 2008 GreenPeace article, the natural heritage of Brazil is severely threatened by cattle ranching and agriculture, logging, mining, resettlement, oil and gas extraction, over-fishing, wildlife trade, dams and infrastructure, water pollution, climate change, fire, and invasive species. In many areas of the country, the natural environment is threatened by development. The construction of highways has opened up previously remote areas for agriculture and settlement; dams have flooded valleys and inundated wildlife habitats; and mines have scarred and polluted the landscape. At least 70 dams are said to be planned for the Amazon region, including the controversial Belo Monte hydroelectric dam. In summer 2019, 2 states in Brazil Paraná and Santa Catarina banned fracking, what will have positive effects on the climate and water quality, because the shale gas and shale oil reserves in the state of Parana are the larger in the southern hemisphere.

The form of government is a democratic federative republic, with a presidential system. The president is both head of state and head of government of the Union and is elected for a four-year term, with the possibility of re-election for a second successive term. The current president is Jair Bolsonaro. The previous president, Michel Temer, replaced Dilma Rousseff after her impeachment. The President appoints the Ministers of State, who assist in government. Legislative houses in each political entity are the main source of law in Brazil. The National Congress is the Federation's bicameral legislature, consisting of the Chamber of Deputies and the Federal Senate. Judiciary authorities exercise jurisdictional duties almost exclusively. Brazil is a democracy, according to the Democracy Index 2010.

The political-administrative organization of the Federative Republic of Brazil comprises the Union, the states, the Federal District, and the municipalities. The Union, the states, the Federal District, and the municipalities, are the "spheres of government". The federation is set on five fundamental principles: sovereignty, citizenship, dignity of human beings, the social values of labor and freedom of enterprise, and political pluralism. The classic tripartite branches of government (executive, legislative and judicial under a checks and balances system) are formally established by the Constitution. The executive and legislative are organized independently in all three spheres of government, while the judiciary is organized only at the federal and state and Federal District spheres.
All members of the executive and legislative branches are directly elected. Judges and other judicial officials are appointed after passing entry exams. For most of its democratic history, Brazil has had a multi-party system, proportional representation. Voting is compulsory for the literate between 18 and 70 years old and optional for illiterates and those between 16 and 18 or beyond 70.

Together with several smaller parties, four political parties stand out: Workers' Party (PT), Brazilian Social Democracy Party (PSDB), Brazilian Democratic Movement (MDB) and Democrats (DEM). Fifteen political parties are represented in Congress. It is common for politicians to switch parties, and thus the proportion of congressional seats held by particular parties changes regularly. Almost all governmental and administrative functions are exercised by authorities and agencies affiliated to the Executive.

Brazilian law is based on the civil law legal system and civil law concepts prevail over common law practice. Most of Brazilian law is codified, although non-codified statutes also represent a substantial part, playing a complementary role. Court decisions set out interpretive guidelines; however, they are seldom binding on other specific cases. Doctrinal works and the works of academic jurists have strong influence in law creation and in law cases.

The legal system is based on the Federal Constitution, promulgated on 5 October 1988, and the fundamental law of Brazil. All other legislation and court decisions must conform to its rules. , there have been 53 amendments. States have their own constitutions, which must not contradict the Federal Constitution. Municipalities and the Federal District have "organic laws" (), which act in a similar way to constitutions. Legislative entities are the main source of statutes, although in certain matters judiciary and executive bodies may enact legal norms. Jurisdiction is administered by the judiciary entities, although in rare situations the Federal Constitution allows the Federal Senate to pass on legal judgments. There are also specialized military, labor, and electoral courts. The highest court is the Supreme Federal Court.

This system has been criticized over the last few decades for the slow pace of decision-making. Lawsuits on appeal may take several years to resolve, and in some cases more than a decade elapses before definitive rulings. Nevertheless, the Supreme Federal Tribunal was the first court in the world to transmit its sessions on television, and also via YouTube. More recently, in December 2009, the Supreme Court adopted Twitter to display items on the day planner of the ministers, to inform the daily actions of the Court and the most important decisions made by them.

The armed forces of Brazil are the largest in Latin America by active personnel and the largest in terms of military equipment. It consists of the Brazilian Army (including the Army Aviation Command), the Brazilian Navy (including the Marine Corps and Naval Aviation), and the Brazilian Air Force. Brazil's conscription policy gives it one of the world's largest military forces, estimated at more than 1.6 million reservists annually.

Numbering close to 236,000 active personnel, the Brazilian Army has the largest number of armored vehicles in South America, including armored transports and tanks. It is also unique in Latin America for its large, elite forces specializing in unconventional missions, the Brazilian Special Operations Command, and the versatile Strategic Rapid Action Force, made up of highly mobilized and prepared Special Operations Brigade, Infantry Brigade Parachutist, 1st Jungle Infantry Battalion (Airmobile) and 12th Brigade Light Infantry (Airmobile) able to act anywhere in the country, on short notice, to counter external aggression. The states' Military Police and the Military Firefighters Corps are described as an ancillary forces of the Army by the constitution, but are under the control of each state's governor.

Brazil's navy, the second-largest in the Americas, once operated some of the most powerful warships in the world with the two dreadnoughts, which sparked a South American dreadnought race between Argentina, Brazil, and Chile. Today, it is a green water force and has a group of specialized elite in retaking ships and naval facilities, GRUMEC, unit specially trained to protect Brazilian oil platforms along its coast. It is the only navy in Latin America that operates an aircraft carrier, PHM Atlantico, and one of the ten navies of the world to operate one.

The Air Force is the largest in Latin America and has about 700 manned aircraft in service and effective about 67,000 personnel.

Brazil has not been invaded since 1865 during the Paraguayan War. Additionally, Brazil has no contested territorial disputes with any of its neighbors and neither does it have rivalries, like Chile and Bolivia have with each other. The Brazilian military has also three times intervened militarily to overthrow the Brazilian government. It has built a tradition of participating in UN peacekeeping missions such as in Haiti, East Timor and Central African Republic. Brazil signed the UN treaty on the Prohibition of Nuclear Weapons.

Brazil's international relations are based on Article 4 of the Federal Constitution, which establishes non-intervention, self-determination, international cooperation and the peaceful settlement of conflicts as the guiding principles of Brazil's relationship with other countries and multilateral organizations.

According to the Constitution, the President has ultimate authority over foreign policy, while the Congress is tasked with reviewing and considering all diplomatic nominations and international treaties, as well as legislation relating to Brazilian foreign policy.

Brazil's foreign policy is a by-product of the country's unique position as a regional power in Latin America, a leader among developing countries, and an emerging world power. Brazilian foreign policy has generally been based on the principles of multilateralism, peaceful dispute settlement, and non-intervention in the affairs of other countries.

Brazil is a founding member state of the Community of Portuguese Language Countries (CPLP), also known as the Lusophone Commonwealth, an international organization and political association of Lusophone nations across four continents, where Portuguese is an official language.

An increasingly well-developed tool of Brazil's foreign policy is providing aid as a donor to other developing countries. Brazil does not just use its growing economic strength to provide financial aid, but it also provides high levels of expertise and most importantly of all, a quiet non-confrontational diplomacy to improve governance levels. Total aid is estimated to be around $1 billion per year, which includes:


In addition, Brazil manages a peacekeeping mission in Haiti ($350 million) and makes in-kind contributions to the World Food Programme ($300 million). This is in addition to humanitarian assistance and contributions to multilateral development agencies. The scale of this aid places it on par with China and India. The Brazilian South-South aid has been described as a "global model in waiting."

In Brazil, the Constitution establishes five different police agencies for law enforcement: Federal Police Department, Federal Highway Police, Federal Railroad Police, Military Police and Civil Police. Of these, the first three are affiliated with federal authorities and the last two are subordinate to state governments. All police forces are the responsibility of the executive branch of any of the federal or state powers. The National Public Security Force also can act in public disorder situations arising anywhere in the country.

The country still has above-average levels of violent crime and particularly high levels of gun violence and homicide. In 2012, the World Health Organization (WHO) estimated the number of 32 deaths per 100,000 inhabitants, one of the highest rates of homicide of the world. The number considered tolerable by the WHO is about 10 homicides per 100,000 inhabitants. In 2018, Brazil had a record 63,880 murders. However, there are differences between the crime rates in the Brazilian states. While in São Paulo the homicide rate registered in 2013 was 10.8 deaths per 100,000 inhabitants, in Alagoas it was 64.7 homicides per 100,000 inhabitants.

Brazil also has high levels of incarceration and the third largest prison population in the world (behind only China and the United States), with an estimated total of approximately 700,000 prisoners around the country (June 2014), an increase of about 300% compared to the index registered in 1992. The high number of prisoners eventually overloaded the Brazilian prison system, leading to a shortfall of about 200,000 accommodations.

Brazil is a federation composed of 26 states, one federal district, and the 5570 municipalities. States have autonomous administrations, collect their own taxes and receive a share of taxes collected by the Federal government. They have a governor and a unicameral legislative body elected directly by their voters. They also have independent Courts of Law for common justice. Despite this, states have much less autonomy to create their own laws than in the United States. For example, criminal and civil laws can be voted by only the federal bicameral Congress and are uniform throughout the country.

The states and the federal district may be grouped into regions: Northern, Northeast, Central-West, Southeast and Southern. The Brazilian regions are merely geographical, not political or administrative divisions, and they do not have any specific form of government. Although defined by law, Brazilian regions are useful mainly for statistical purposes, and also to define the distribution of federal funds in development projects.

Municipalities, as the states, have autonomous administrations, collect their own taxes and receive a share of taxes collected by the Union and state government. Each has a mayor and an elected legislative body, but no separate Court of Law. Indeed, a Court of Law organized by the state can encompass many municipalities in a single justice administrative division called "comarca" (county).

Brazil is the largest national economy in Latin America, the world's ninth largest economy and the eighth largest in purchasing power parity (PPP) according to 2018 estimates. Brazil has a mixed economy with abundant natural resources. After rapid growth in preceding decades, the country entered an ongoing recession in 2014 amid a political corruption scandal and nationwide protests.

Its Gross domestic product (PPP) per capita was $15,919 in 2017 putting Brazil in the 77th position according to IMF data. Active in agricultural, mining, manufacturing and service sectors Brazil has a labor force of over 107 million (ranking 6th worldwide) and unemployment of 6.2% (ranking 64th worldwide).

The country has been expanding its presence in international financial and commodities markets, and is one of a group of four emerging economies called the BRIC countries. Brazil has been the world's largest producer of coffee for the last 150 years.
Brazil's diversified economy includes agriculture, industry, and a wide range of services. Agriculture and allied sectors like forestry, logging and fishing accounted for 5.1% of the GDP in 2007. Brazil is one of the largest producer of oranges, coffee, sugar cane, cassava and sisal, soybeans and papayas.

Industry in Brazil – from automobiles, steel and petrochemicals to computers, aircraft and consumer durables – accounted for 30.8% of the gross domestic product. Industry is highly concentrated in metropolitan São Paulo, Rio de Janeiro, Campinas, Porto Alegre, and Belo Horizonte. Brazil has become the fourth largest car market in the world. Major export products include aircraft, electrical equipment, automobiles, ethanol, textiles, footwear, iron ore, steel, coffee, orange juice, soybeans and corned beef. In total, Brazil ranks 23rd worldwide in value of exports.

Brazil pegged its currency, the real, to the U.S. dollar in 1994. However, after the East Asian financial crisis, the Russian default in 1998 and the series of adverse financial events that followed it, the Central Bank of Brazil temporarily changed its monetary policy to a managed float regime scheme while undergoing a currency crisis, until definitively changing the exchange regime to free-float in January 1999.

Brazil received an International Monetary Fund (IMF) rescue package in mid-2002 of $30.4 billion, a record sum at the time. Brazil's central bank repaid the IMF loan in 2005, although it was not due to be repaid until 2006. One of the issues the Central Bank of Brazil recently dealt with was an excess of speculative short-term capital inflows to the country, which may have contributed to a fall in the value of the U.S. dollar against the real during that period. Nonetheless, foreign direct investment (FDI), related to long-term, less speculative investment in production, is estimated to be $193.8 billion for 2007. Inflation monitoring and control currently plays a major part in the Central bank's role in setting short-term interest rates as a monetary policy measure.

Between 1993 and 2010, 7012 mergers and acquisitions with a total known value of $707 billion with the involvement of Brazilian firms were announced. The year 2010 was a new record in terms of value with US$115 billion in transactions. The largest transaction with involvement of Brazilian companies was the Cia. Vale do Rio Doce acquisition of Inco in a tender offer valued at US$18.9 billion.

Corruption costs Brazil almost $41 billion a year alone in 2010, with 69.9% of the country's firms identifying the issue as a major constraint in successfully penetrating the global market. Local government corruption is so prevalent that voters perceive it as a problem only if it surpasses certain levels, and only if a local media e.g. a radio station is present to divulge the findings of corruption charges. Initiatives, like this exposure, strengthen awareness which is indicated by the Transparency International's Corruption Perceptions Index; ranking Brazil 69th out of 178 countries in 2012.
The purchasing power in Brazil is eroded by the so-called Brazil cost.

Brazil also has a large cooperative sector that provides 50% of the food in the country. The world's largest healthcare cooperative Unimed is also located in Brazil, and accounts for 32% of the healthcare insurance market in the country.

Brazil is the world's tenth largest energy consumer with much of its energy coming from renewable sources, particularly hydroelectricity and ethanol; the Itaipu Dam is the world's largest hydroelectric plant by energy generation. The first car with an ethanol engine was produced in 1978 and the first airplane engine running on ethanol in 2005.

Recent oil discoveries in the Pre-salt layer have opened the door for a large increase in oil production. The governmental agencies responsible for the energy policy are the Ministry of Mines and Energy, the National Council for Energy Policy, the National Agency of Petroleum, Natural Gas and Biofuels, and the National Agency of Electricity.

Tourism in Brazil is a growing sector and key to the economy of several regions of the country. The country had 6.36 million visitors in 2015, ranking in terms of the international tourist arrivals as the main destination in South America and second in Latin America after Mexico. Revenues from international tourists reached billion in 2010, showing a recovery from the 2008–2009 economic crisis. Historical records of 5.4 million visitors and billion in receipts were reached in 2011.

Natural areas are its most popular tourism product, a combination of ecotourism with leisure and recreation, mainly sun and beach, and adventure travel, as well as cultural tourism. Among the most popular destinations are the Amazon Rainforest, beaches and dunes in the Northeast Region, the Pantanal in the Center-West Region, beaches at Rio de Janeiro and Santa Catarina, cultural tourism in Minas Gerais and business trips to São Paulo.

In terms of the 2015 Travel and Tourism Competitiveness Index (TTCI), which is a measurement of the factors that make it attractive to develop business in the travel and tourism industry of individual countries, Brazil ranked in the 28st place at the world's level, third in the Americas, after Canada and United States.

Brazil's main competitive advantages are its natural resources, which ranked 1st on this criteria out of all countries considered, and ranked 23rd for its cultural resources, due to its many World Heritage sites. The TTCI report notes Brazil's main weaknesses: its ground transport infrastructure remains underdeveloped (ranked 116th), with the quality of roads ranking in 105th place; and the country continues to suffer from a lack of price competitiveness (ranked 114th), due in part to high ticket taxes and airport charges, as well as high prices and high taxation. Safety and security have improved significantly: 75th in 2011, up from 128th in 2008.

According to the World Tourism Organization (WTO), international travel to Brazil accelerated in 2000, particularly during 2004 and 2005. However, in 2006 a slow-down took place, and international arrivals had almost no growth in 2007–08.

In spite of this trend, revenues from international tourism continued to rise, from USD 4 billion in 2005 to 5 billion in 2007, despite 330 000 fewer arrivals. This favorable trend is the result of the strong devaluation of the US dollar against the Brazilian Real, which began in 2004, but which makes Brazil a more expensive international destination.
This trend changed in 2009, when both visitors and revenues fell as a result of the Great Recession of 2008–09. By 2010, the industry had recovered, and arrivals grew above 2006 levels to 5.2 million international visitors, and receipts from these visitors reached US$6 billion. In 2011 the historical record was reached with 5.4 million visitors and billion in receipts.

Despite continuing record-breaking international tourism revenues, the number of Brazilian tourists travelling overseas has been growing steadily since 2003, resulting in a net negative foreign exchange balance, as more money is spent abroad by Brazilians than comes in as receipts from international tourists visiting Brazil.

Tourism expenditures abroad grew from US$5.8 billion in 2006, to US$8.2 billion in 2007, a 42% increase, representing a net deficit of US$3.3 billion in 2007, as compared to US$1.5 billion in 2006, a 125% increase from the previous year. This trend is caused by Brazilians taking advantage of the stronger Real to travel and making relatively cheaper expenditures abroad. Brazilians traveling overseas in 2006 represented 4% of the country's population.

In 2005, tourism contributed with 3.2% of the country's revenues from exports of goods and services, and represented 7% of direct and indirect employment in the Brazilian economy. In 2006 direct employment in the sector reached 1.9 million people.

Domestic tourism is a fundamental market segment for the industry, as 51 million people traveled throughout the country in 2005, and direct revenues from Brazilian tourists reached US$22 billion, 5.6 times more receipts than international tourists in 2005.

In 2005, Rio de Janeiro, Foz do Iguaçu, São Paulo, Florianópolis and Salvador were the most visited cities by international tourists for leisure trips. The most popular destinations for business trips were São Paulo, Rio de Janeiro and Porto Alegre. In 2006 Rio de Janeiro and Fortaleza were the most popular destinations for business trips.

Technological research in Brazil is largely carried out in public universities and research institutes, with the majority of funding for basic research coming from various government agencies. Brazil's most esteemed technological hubs are the Oswaldo Cruz Institute, the Butantan Institute, the Air Force's Aerospace Technical Center, the Brazilian Agricultural Research Corporation and the INPE.

The Brazilian Space Agency has the most advanced space program in Latin America, with significant resources to launch vehicles, and manufacture of satellites. Owner of relative technological sophistication, the country develops submarines, aircraft, as well as being involved in space research, having a Vehicle Launch Center Light and being the only country in the Southern Hemisphere the integrate team building International Space Station (ISS).

The country is also a pioneer in the search for oil in deep water, from where it extracts 73% of its reserves.
Uranium is enriched at the Resende Nuclear Fuel Factory, mostly for research purposes (as Brazil obtains 88% from its electricity from hydroelectricity) and the country's first nuclear submarine was delivered in 2015 (by France).

Brazil is one of the three countries in Latin America with an operational Synchrotron Laboratory, a research facility on physics, chemistry, material science and life sciences, and Brazil is the only Latin American country to have a semiconductor company with its own fabrication plant, the CEITEC. According to the Global Information Technology Report 2009-2010 of the World Economic Forum, Brazil is the world's 61st largest developer of information technology.

Brazil also has a large number of outstanding scientific personalities. Among the most renowned Brazilian inventors are priests Bartolomeu de Gusmão, Landell de Moura and Francisco João de Azevedo, besides Alberto Santos-Dumont, Evaristo Conrado Engelberg, Manuel Dias de Abreu, Andreas Pavel and Nélio José Nicolai.

Brazilian science is represented by the likes of César Lattes (Brazilian physicist Pathfinder of "Pi Meson"), Mário Schenberg (considered the greatest theoretical physicist of Brazil), José Leite Lopes (only Brazilian physicist holder of the "UNESCO Science Prize"), Artur Ávila (the first Latin American winner of the Fields Medal) and Fritz Müller (pioneer in factual support of the theory of evolution by Charles Darwin).

Brazilian roads are the primary carriers of freight and passenger traffic. The road system totaled 1.98 million km (1.23 million mi) in 2002. The total of paved roads increased from (22,056 mi) in 1967 to (114,425 mi) in 2002.

The first investments in road infrastructure have given up in the 1920s, the government of Washington Luis, being pursued in the governments of Getúlio Vargas and Eurico Gaspar Dutra. President Juscelino Kubitschek (1956–61), who designed and built the capital Brasília, was another supporter of highways. Kubitschek was responsible for the installation of major car manufacturers in the country (Volkswagen, Ford and General Motors arrived in Brazil during his rule) and one of the points used to attract them was support for the construction of highways. With the implementation of Fiat in 1976 ending an automobile market closed loop, from the end of the 1990s the country has received large foreign direct investments installing in its territory other major car manufacturers and utilities, such as Iveco, Renault, Peugeot, Citroen, Honda, Mitsubishi, Mercedes-Benz, BMW, Hyundai, Toyota among others. Brazil is the seventh most important country in the auto industry.

Brazil's railway system has been declining since 1945, when emphasis shifted to highway construction. The total length of railway track was in 2002, as compared with in 1970. Most of the railway system belonged to the Federal Railroad Corporation RFFSA, which was privatized in 2007. The São Paulo Metro was the first underground transit system in Brazil. The other metro systems are in Rio de Janeiro, Porto Alegre, Recife, Belo Horizonte, Brasília, Teresina and Fortaleza.

The country has an extensive rail network of in length, the tenth largest network in the world. Currently, the Brazilian government, unlike the past, seeks to encourage this mode of transport; an example of this incentive is the project of the Rio–São Paulo high-speed rail, that will connect the two main cities of the country to carry passengers.

There are about 2,500 airports in Brazil, including landing fields: the second largest number in the world, after the United States. São Paulo–Guarulhos International Airport, near São Paulo, is the largest and busiest airport with nearly 20 million passengers annually, while handling the vast majority of commercial traffic for the country.

For freight transport waterways are of importance, e.g. the industrial zones of Manaus can be reached only by means of the Solimões–Amazonas waterway ( with minimum depth). The country also has of waterways.

Coastal shipping links widely separated parts of the country. Bolivia and Paraguay have been given free ports at Santos. Of the 36 deep-water ports, Santos, Itajaí, Rio Grande, Paranaguá, Rio de Janeiro, Sepetiba, Vitória, Suape, Manaus and São Francisco do Sul are the most important. Bulk carriers have to wait up to 18 days before being serviced, container ships 36.3 hours on average.

The Brazilian public health system, the Unified Health System (SUS), is managed and provided by all levels of government, being the largest system of this type in the world. On the other hand, private healthcare systems play a complementary role.

Public health services are universal and offered to all citizens of the country for free. However, the construction and maintenance of health centers and hospitals are financed by taxes, and the country spends about 9% of its GDP on expenditures in the area. In 2012, Brazil had 1.85 doctors and 2.3 hospital beds for every 1,000 inhabitants.

Despite all the progress made since the creation of the universal health care system in 1988, there are still several public health problems in Brazil. In 2006, the main points to be solved were the high infant (2.51%) and maternal mortality rates (73.1 deaths per 1000 births).

The number of deaths from noncommunicable diseases, such as cardiovascular diseases (151.7 deaths per 100,000 inhabitants) and cancer (72.7 deaths per 100,000 inhabitants), also has a considerable impact on the health of the Brazilian population. Finally, external but preventable factors such as car accidents, violence and suicide caused 14.9% of all deaths in the country. The Brazilian health system was ranked 125th among the 191 countries evaluated by the World Health Organization (WHO) in 2000.

The Federal Constitution and the Law of Guidelines and Bases of National Education determine that the Union, the states, the Federal District, and the municipalities must manage and organize their respective education systems. Each of these public educational systems is responsible for its own maintenance, which manages funds as well as the mechanisms and funding sources. The constitution reserves 25% of the state budget and 18% of federal taxes and municipal taxes for education.

According to the IBGE, in 2011, the literacy rate of the population was 90.4%, meaning that 13 million (9.6% of population) people are still illiterate in the country; functional illiteracy has reached 21.6% of the population. Illiteracy is highest in the Northeast, where 19.9% of the population is illiterate.

Higher education starts with undergraduate or sequential courses, which may offer different options of specialization in academic or professional careers. Depending on the choice, students can improve their educational background with courses of post-graduate studies or broad sense.

Attending an institution of higher education is required by Law of Guidelines and Bases of Education. Kindergarten, elementary and medium education are required of all students, provided the student does not hold any disability, whether physical, mental, visual or hearing.

The University of São Paulo is the second best university in Latin America, according to recent 2019 QS World University Rankings. Of the top 20 Latin American universities, eight are Brazilian. Most of them are public.

Brazil's private institutions tend to be more exclusive and offer better quality education, so many high-income families send their children there. The result is a segregated educational system that reflects extreme income disparities and reinforces social inequality. However, efforts to change this are making impacts.

The Brazilian press was officially born in Rio de Janeiro on 13 May 1808 with the creation of the Royal Printing National Press by the Prince Regent Dom João.

The "Gazeta do Rio de Janeiro", the first newspaper published in the country, began to circulate on 10 September 1808. The largest newspapers are "Folha de S.Paulo" (from the state of São Paulo), "Super Notícia" (Minas Gerais 296.799), "O Globo" (RJ 277.876) and "O Estado de S. Paulo" (SP 235.217).

Radio broadcasting began on 7 September 1922, with a speech by then President Pessoa, and was formalized on 20 April 1923 with the creation of "Radio Society of Rio de Janeiro."

Television in Brazil began officially on 18 September 1950, with the founding of TV Tupi by Assis Chateaubriand. Since then television has grown in the country, creating large public networks such as Globo, SBT, Record and Bandeirantes. Today it is the most important factor in popular culture of Brazilian society, indicated by research showing that as much as 67% of the general population follow the same daily soap opera broadcast. Digital Television, using the SBTVD standard (based on the Japanese standard ISDB-T), was adopted 29 June 2006 and launched on 2 November 2007. In May 2010, Brazil launched TV Brasil Internacional, an international television station, initially broadcasting to 49 countries.

The population of Brazil, as recorded by the 2008 PNAD, was approximately 190 million (), with a ratio of men to women of 0.95:1 and 83.75% of the population defined as urban. The population is heavily concentrated in the Southeastern (79.8 million inhabitants) and Northeastern (53.5 million inhabitants) regions, while the two most extensive regions, the Center-West and the North, which together make up 64.12% of the Brazilian territory, have a total of only 29.1 million inhabitants.

The first census in Brazil was carried out in 1872 and recorded a population of 9,930,478. From 1880 to 1930, 4 million Europeans arrived. Brazil's population increased significantly between 1940 and 1970, because of a decline in the mortality rate, even though the birth rate underwent a slight decline. In the 1940s the annual population growth rate was 2.4%, rising to 3.0% in the 1950s and remaining at 2.9% in the 1960s, as life expectancy rose from 44 to 54 years and to 72.6 years in 2007.
It has been steadily falling since the 1960s, from 3.04% per year between 1950 and 1960 to 1.05% in 2008 and is expected to fall to a negative value of –0.29% by 2050 thus completing the demographic transition.

In 2008, the illiteracy rate was 11.48% and among the youth (ages 15–19) 1.74%. It was highest (20.30%) in the Northeast, which had a large proportion of rural poor. Illiteracy was high (24.18%) among the rural population and lower (9.05%) among the urban population.

According to the National Research by Household Sample (PNAD) of 2008, 48.43% of the population (about 92 million) described themselves as White; 43.80% (about 83 million) as Pardo (brown), 6.84% (about 13 million) as Black; 0.58% (about 1.1 million) as Asian; and 0.28% (about 536 thousand) as Amerindian (officially called "indígena", Indigenous), while 0.07% (about 130 thousand) did not declare their race.

In 2007, the National Indian Foundation estimated that Brazil has 67 different uncontacted tribes, up from their estimate of 40 in 2005. Brazil is believed to have the largest number of uncontacted peoples in the world.

Since the arrival of the Portuguese in 1500, considerable genetic mixing between Amerindians, Europeans, and Africans has taken place in all regions of the country (with European ancestry being dominant nationwide according to the vast majority of all autosomal studies undertaken covering the entire population, accounting for between 65% to 77%).
Brazilian society is more markedly divided by social class lines, although a high income disparity is found between race groups, so racism and classism can be conflated. Socially significant closeness to one racial group is taken in account more in the basis of appearance (phenotypes) rather than ancestry, to the extent that full siblings can pertain to different "racial" groups. Socioeconomic factors are also significant, because a minority of "pardos" are likely to start declaring themselves White or Black if socially upward. Skin color and facial features do not line quite well with ancestry (usually, Afro-Brazilians are evenly mixed and European ancestry is dominant in Whites and "pardos" with a significant non-European contribution, but the individual variation is great).

The brown population (officially called "pardo" in Portuguese, also colloquially "moreno") is a broad category that includes "caboclos" (assimilated Amerindians in general, and descendants of Whites and Natives), "mulatos" (descendants of primarily Whites and Afro-Brazilians) and "cafuzos" (descendants of Afro-Brazilians and Natives). People of considerable Amerindian ancestry form the majority of the population in the Northern, Northeastern and Center-Western regions.

Higher percents of Blacks, mulattoes and tri-racials can be found in the eastern coast of the Northeastern region from Bahia to Paraíba and also in northern Maranhão, southern Minas Gerais and in eastern Rio de Janeiro. From the 19th century, Brazil opened its borders to immigration. About five million people from over 60 countries migrated to Brazil between 1808 and 1972, most of them of Portuguese, Italian, Spanish, German, Ukrainian, Polish, Jewish, Russian, Chinese, Japanese, and Arab origin.. Brazil has the second largest Jewish community in Latin America making up 0.06% of its population .

Religion in Brazil was formed from the meeting of the Catholic Church with the religious traditions of enslaved African peoples and indigenous peoples. This confluence of faiths during the Portuguese colonization of Brazil led to the development of a diverse array of syncretistic practices within the overarching umbrella of Brazilian Catholic Church, characterized by traditional Portuguese festivities, and in some instances, Allan Kardec's Spiritism (a religion which incorporates elements of spiritualism and Christianity). Religious pluralism increased during the 20th century, and the Protestant community has grown to include over 22% of the population. The most common Protestant denominations are Pentecostal and Evangelical ones. Other Protestant branches with a notable presence in the country include the Baptists, Seventh-day Adventists, Lutherans and the Reformed tradition.

Roman Catholicism is the country's predominant faith. Brazil has the world's largest Catholic population. According to the 2000 Demographic Census (the PNAD survey does not inquire about religion), 73.57% of the population followed Roman Catholicism; 15.41% Protestantism; 1.33% Kardecist spiritism; 1.22% other Christian denominations; 0.31% Afro-Brazilian religions; 0.13% Buddhism; 0.05% Judaism; 0.06%; Islam; 0.01% Amerindian religions; 0.59% other religions, undeclared or undetermined; while 7.35% have no religion.

However, in the last ten years Protestantism, particularly in forms of Pentecostalism and Evangelicalism, has spread in Brazil, while the proportion of Catholics has dropped significantly. After Protestantism, individuals professing no religion are also a significant group, exceeding 7% of the population as of the 2000 census. The cities of Boa Vista, Salvador, and Porto Velho have the greatest proportion of Irreligious residents in Brazil. Teresina, Fortaleza, and Florianópolis were the most Roman Catholic in the country. Greater Rio de Janeiro, not including the city proper, is the most irreligious and least Roman Catholic Brazilian periphery, while Greater Porto Alegre and Greater Fortaleza are on the opposite sides of the lists, respectively.

According to IBGE (Brazilian Institute of Geography and Statistics) urban areas already concentrate 84.35% of the population, while the Southeast region remains the most populated one, with over 80 million inhabitants.
The largest urban agglomerations in Brazil are São Paulo, Rio de Janeiro, and Belo Horizonte – all in the Southeastern Region – with 21.1, 12.3, and 5.1 million inhabitants respectively. The majority of state capitals are the largest cities in their states, except for Vitória, the capital of Espírito Santo, and Florianópolis, the capital of Santa Catarina.

The official language of Brazil is Portuguese (Article 13 of the Constitution of the Federal Republic of Brazil), which almost all of the population speaks and is virtually the only language used in newspapers, radio, television, and for business and administrative purposes. Brazil is the only Portuguese-speaking nation in the Americas, making the language an important part of Brazilian national identity and giving it a national culture distinct from those of its Spanish-speaking neighbors.

Brazilian Portuguese has had its own development, mostly similar to 16th-century Central and Southern dialects of European Portuguese (despite a very substantial number of Portuguese colonial settlers, and more recent immigrants, coming from Northern regions, and in minor degree Portuguese Macaronesia), with a few influences from the Amerindian and African languages, especially West African and Bantu restricted to the vocabulary only. As a result, the language is somewhat different, mostly in phonology, from the language of Portugal and other Portuguese-speaking countries (the dialects of the other countries, partly because of the more recent end of Portuguese colonialism in these regions, have a closer connection to contemporary European Portuguese). These differences are comparable to those between American and British English.

In 1990, the Community of Portuguese Language Countries (CPLP), which included representatives from all countries with Portuguese as the official language, reached an agreement on the reform of the Portuguese orthography to unify the two standards then in use by Brazil on one side and the remaining lusophone countries on the other. This spelling reform went into effect in Brazil on 1 January 2009. In Portugal, the reform was signed into law by the President on 21 July 2008 allowing for a six-year adaptation period, during which both orthographies will co-exist. The remaining CPLP countries are free to establish their own transition timetables.

The sign language law legally recognized in 2002, (the law was regulated in 2005) the use of the Brazilian Sign Language, more commonly known by its Portuguese acronym LIBRAS, in education and government services. The language must be taught as a part of the education and speech and language pathology curricula. LIBRAS teachers, instructors and translators are recognized professionals. Schools and health services must provide access ("inclusion") to deaf people.

Minority languages are spoken throughout the nation. One hundred and eighty Amerindian languages are spoken in remote areas and a significant number of other languages are spoken by immigrants and their descendants. In the municipality of São Gabriel da Cachoeira, Nheengatu (a currently endangered South American creole language – or an 'anti-creole', according to some linguists – with mostly Indigenous Brazilian languages lexicon and Portuguese-based grammar that, together with its southern relative língua geral paulista, once was a major lingua franca in Brazil, being replaced by Portuguese only after governmental prohibition led by major political changes), Baniwa and Tucano languages had been granted co-official status with Portuguese.

There are significant communities of German (mostly the Brazilian Hunsrückisch, a High German language dialect) and Italian (mostly the Talian, a Venetian dialect) origins in the Southern and Southeastern regions, whose ancestors' native languages were carried along to Brazil, and which, still alive there, are influenced by the Portuguese language. Talian is officially a historic patrimony of Rio Grande do Sul, and two German dialects possess co-official status in a few municipalities.

Learning at least one second language (generally English or Spanish) is mandatory for all the 12 grades of the mandatory education system (primary and secondary education, there called "ensino fundamental" and "ensino médio" respectively). Brazil is the first country in South America to offer Esperanto to secondary students.

The core culture of Brazil is derived from Portuguese culture, because of its strong colonial ties with the Portuguese Empire. Among other influences, the Portuguese introduced the Portuguese language, Roman Catholicism and colonial architectural styles. The culture was, however, also strongly influenced by African, indigenous and non-Portuguese European cultures and traditions.
Some aspects of Brazilian culture were influenced by the contributions of Italian, German and other European as well as Japanese, Jewish and Arab immigrants who arrived in large numbers in the South and Southeast of Brazil during the 19th and 20th centuries. The indigenous Amerindians influenced Brazil's language and cuisine; and the Africans influenced language, cuisine, music, dance and religion.

Brazilian art has developed since the 16th century into different styles that range from Baroque (the dominant style in Brazil until the early 19th century) to Romanticism, Modernism, Expressionism, Cubism, Surrealism and Abstractionism. Brazilian cinema dates back to the birth of the medium in the late 19th century and has gained a new level of international acclaim since the 1960s.

The architecture of Brazil is influenced by Europe, especially Portugal. It has a history that goes back 500 years to the time when Pedro Cabral discovered Brazil in 1500. Portuguese colonial architecture was the first wave of architecture to go to Brazil. It is the basis for all Brazilian architecture of later centuries. In the 19th century during the time of the Empire of Brazil, Brazil followed European trends and adopted Neoclassical and Gothic Revival architecture. Then in the 20th century especially in Brasilia, Brazil experimented with Modernist architecture.

The colonial architecture of Brazil dates to the early 16th century when Brazil was first explored, conquered and settled by the Portuguese. The Portuguese built architecture familiar to them in Europe in their aim to colonize Brazil. They built Portuguese colonial architecture which included churches, civic architecture including houses and forts in Brazilian cities and the countryside. During 19th Century Brazilian architecture saw the introduction of more European styles to Brazil such as Neoclassical and Gothic Revival architecture. This was usually mixed with Brazilian influences from their own heritage which produced a unique form of Brazilian architecture. In the 1950s the modernist architecture was introduced when Brasilia was built as new federal capital in the interior of Brazil to help develop the interior. The architect Oscar Niemeyer idealized and built government buildings, churches and civic buildings in the modernist style.

The music of Brazil was formed mainly from the fusion of European and African elements. Until the nineteenth century, Portugal was the gateway to most of the influences that built Brazilian music, although many of these elements were not of Portuguese origin, but generally European. The first was José Maurício Nunes Garcia, author of sacred pieces with influence of Viennese classicism. The major contribution of the African element was the rhythmic diversity and some dances and instruments that had a bigger role in the development of popular music and folk, flourishing especially in the twentieth century.

Popular music since the late eighteenth century began to show signs of forming a characteristically Brazilian sound, with samba considered the most typical and on the UNESCO cultural heritage list. Maracatu and Afoxê are two Afro-Brazilian music traditions that have been popularized by their appearance in the annual Brazilian Carnivals. The sport of capoeira is usually played with its own music referred to as capoeira music, which is usually considered to be a call-and-response type of folk music. Forró is a type of folk music prominent during the Festa Junina in northeastern Brazil. Jack A. Draper III, a professor of Portuguese at the University of Missouri, argues that Forró was used as a way to subdue feelings of nostalgia for a rural lifestyle.

Choro is a very popular music instrumental style. Its origins are in 19th-century Rio de Janeiro. In spite of the name, the style often has a fast and happy rhythm, characterized by virtuosity, improvisation, subtle modulations and full of syncopation and counterpoint. Bossa nova is also a well-known style of Brazilian music developed and popularized in the 1950s and 1960s. The phrase "bossa nova" means literally "new trend". A lyrical fusion of samba and jazz, bossa nova acquired a large following starting in the 1960s.

Brazilian literature dates back to the 16th century, to the writings of the first Portuguese explorers in Brazil, such as Pêro Vaz de Caminha, filled with descriptions of fauna, flora and commentary about the indigenous population that fascinated European readers.

Brazil produced significant works in Romanticism – novelists like Joaquim Manuel de Macedo and José de Alencar wrote novels about love and pain. Alencar, in his long career, also treated indigenous people as heroes in the Indigenist novels "O Guarani", "Iracema" and "Ubirajara". Machado de Assis, one of his contemporaries, wrote in virtually all genres and continues to gain international prestige from critics worldwide.

Brazilian Modernism, evidenced by the Week of Modern Art in 1922, was concerned with a nationalist avant-garde literature, while Post-Modernism brought a generation of distinct poets like João Cabral de Melo Neto, Carlos Drummond de Andrade, Vinicius de Moraes, Cora Coralina, Graciliano Ramos, Cecília Meireles, and internationally known writers dealing with universal and regional subjects like Jorge Amado, João Guimarães Rosa, Clarice Lispector and Manuel Bandeira.

Brazilian cuisine varies greatly by region, reflecting the country's varying mix of indigenous and immigrant populations. This has created a national cuisine marked by the preservation of regional differences. Examples are Feijoada, considered the country's national dish; and regional foods such as beiju, feijão tropeiro, vatapá, moqueca, polenta (from Italian cuisine) and acarajé (from African cuisine).

The national beverage is coffee and cachaça is Brazil's native liquor. Cachaça is distilled from sugar cane and is the main ingredient in the national cocktail, Caipirinha.

A typical meal consists mostly of rice and beans with beef, salad, french fries and a fried egg. Often, it is mixed with cassava flour (farofa). Fried potatoes, fried cassava, fried banana, fried meat and fried cheese are very often eaten in lunch and served in most typical restaurants. Popular snacks are pastel (a fried pastry); coxinha (a variation of chicken croquete); pão de queijo (cheese bread and cassava flour / tapioca); pamonha (corn and milk paste); esfirra (a variation of Lebanese pastry); kibbeh (from Arabic cuisine); empanada (pastry) and empada, little salt pies filled with shrimps or heart of palm.

Brazil has a variety of desserts such as brigadeiros (chocolate fudge balls), bolo de rolo (roll cake with goiabada), cocada (a coconut sweet), beijinhos (coconut truffles and clove) and romeu e julieta (cheese with goiabada). Peanuts are used to make paçoca, rapadura and pé-de-moleque. Local common fruits like açaí, cupuaçu, mango, papaya, cocoa, cashew, guava, orange, lime, passionfruit, pineapple, and hog plum are turned in juices and used to make chocolates, ice pops and ice cream.

The Brazilian film industry began in the late 19th century, during the early days of the Belle Époque. While there were national film productions during the early 20th century, American films such as "Rio the Magnificent" were made in Rio de Janeiro to promote tourism in the city. The films "Limite" (1931) and "Ganga Bruta" (1933), the latter being produced by Adhemar Gonzaga through the prolific studio Cinédia, were poorly received at release and failed at the box office, but are acclaimed nowadays and placed among the finest Brazilian films of all time. The 1941 unfinished film "It's All True" was divided in four segments, two of which were filmed in Brazil and directed by Orson Welles; it was originally produced as part of the United States' Good Neighbor Policy during Getúlio Vargas' Estado Novo government.

During the 1960s, the Cinema Novo movement rose to prominence with directors such as Glauber Rocha, Nelson Pereira dos Santos, Paulo Cesar Saraceni and Arnaldo Jabor. Rocha's films "Deus e o Diabo na Terra do Sol" (1964) and "Terra em Transe" (1967) are considered to be some of the greatest and most influential in Brazilian film history.

During the 1990s, Brazil saw a surge of critical and commercial success with films such as "O Quatrilho" (Fábio Barreto, 1995), "O Que É Isso, Companheiro?" (Bruno Barreto, 1997) and "Central do Brasil" (Walter Salles, 1998), all of which were nominated for the Academy Award for Best Foreign Language Film, the latter receiving a Best Actress nomination for Fernanda Montenegro. The 2002 crime film "City of God", directed by Fernando Meirelles, was critically acclaimed, scoring 90% on Rotten Tomatoes, being placed in Roger Ebert's Best Films of the Decade list and receiving four Academy Award nominations in 2004, including Best Director. Notable film festivals in Brazil include the São Paulo and Rio de Janeiro International Film Festivals and the Gramado Festival.

The theatre in Brazil has its origins in the period of Jesuit expansion when theater was used for the dissemination of Catholic doctrine in the 16th century. in the 17th and 18th centuries the first dramatists who appeared on the scene of European derivation was for court or private performances. During the 19th century, dramatic theater gained importance and thickness, whose first representative was Luis Carlos Martins Pena (1813–1848), capable of describing contemporary reality. Always in this period the comedy of costume and comic production was imposed. Significant, also in the nineteenth century, was also the playwright Antônio Gonçalves Dias. There were also numerous operas and orchestras. The Brazilian conductor Antônio Carlos Gomes became internationally known with operas like "Il Guarany". At the end of the 19th century orchestrated dramaturgias became very popular and were accompanied with songs of famous artists like the conductress Chiquinha Gonzaga.

Already in the early 20th century there was the presence of theaters, entrepreneurs and actor companies, but paradoxically the quality of the products staggered, and only in 1940 the Brazilian theater received a boost of renewal thanks to the action of Paschoal Carlos Magno and his student's theater, the comedians group and the Italian actors Adolfo Celi, Ruggero Jacobbi and Aldo Calvo, founders of the "Teatro Brasileiro de Comedia". From the 1960s it was attended by a theater dedicated to social and religious issues and to the flourishing of schools of dramatic art. The most prominent authors at this stage were Jorge Andrade and Ariano Suassuna.

Brazilian painting emerged in the late 16th century, influenced by Baroque, Rococo, Neoclassicism, Romanticism, Realism, Modernism, Expressionism, Surrealism, Cubism and Abstracionism making it a major art style called Brazilian academic art. The Missão Artística Francesa (French Artistic Mission) arrived in Brazil in 1816 proposing the creation of an art academy modeled after the respected Académie des Beaux-Arts, with graduation courses both for artists and craftsmen for activities such as modeling, decorating, carpentry and others and bringing artists like Jean-Baptiste Debret.

Upon the creation of the Imperial Academy of Fine Arts, new artistic movements spread across the country during the 19th century and later the event called Week of Modern Art broke definitely with academic tradition in 1922 and started a nationalist trend which was influenced by modernist arts. Among the best-known Brazilian painters are Ricardo do Pilar and Manuel da Costa Ataíde (baroque and rococo), Victor Meirelles, Pedro Américo and Almeida Junior (romanticism and realism), Anita Malfatti, Ismael Nery, Lasar Segall, Emiliano di Cavalcanti, Vicente do Rego Monteiro, and Tarsila do Amaral (expressionism, surrealism and cubism), Aldo Bonadei, José Pancetti and Cândido Portinari (modernism).

The most popular sport in Brazil is football. The Brazilian men's national team is ranked among the best in the world according to the FIFA World Rankings, and has won the World Cup tournament a record five times.

Volleyball, basketball, auto racing, and martial arts also attract large audiences. The Brazil men's national volleyball team, for example, currently holds the titles of the World League, World Grand Champions Cup, World Championship and the World Cup. In auto racing, three Brazilian drivers have won the Formula One world championship eight times.

Some sport variations have their origins in Brazil: beach football, futsal (indoor football) and footvolley emerged in Brazil as variations of football. In martial arts, Brazilians developed Capoeira, Vale tudo, and Brazilian jiu-jitsu.
Brazil has hosted several high-profile international sporting events, like the 1950 FIFA World Cup and recently has hosted the 2014 FIFA World Cup and 2019 Copa América. The São Paulo circuit, Autódromo José Carlos Pace, hosts the annual Grand Prix of Brazil. São Paulo organized the IV Pan American Games in 1963, and Rio de Janeiro hosted the XV Pan American Games in 2007. On 2 October 2009, Rio de Janeiro was selected to host the 2016 Olympic Games and 2016 Paralympic Games, making it the first South American city to host the games and second in Latin America, after Mexico City. Furthermore, the country hosted the FIBA Basketball World Cups in 1954 and 1963. At the 1963 event, the Brazil national basketball team won one of its two world championship titles.

Government


</doc>
<doc id="16278495" url="https://en.wikipedia.org/wiki?curid=16278495" title="Outline of Brazil">
Outline of Brazil

The following outline is provided as an overview of and topical guide to Brazil:

Brazil – largest country in both South America and the Latin America region. It is the world's fifth largest country, both by geographical area, 8.5 million km², and by population, with over 206 million people. It is the largest lusophone country in the world, and the only one in the Americas.


Geography of Brazil

Environment of Brazil


Regions of Brazil



List of ecoregions in Brazil

Administrative divisions of Brazil

States of Brazil

Municipalities of Brazil

Demographics of Brazil
"For demographics data, see chart presented under "States of Brazil", above."

Politics of Brazil

Government of Brazil


Court system of Brazil

Federal courts of Brazil




Foreign relations of Brazil

The Federative Republic of Brazil is a member of:

Law of Brazil

Military of Brazil

Local government in Brazil


Culture of Brazil


Sports in Brazil

Economy of Brazil

Education in Brazil

Health in Brazil

Brazil







</doc>
<doc id="63383085" url="https://en.wikipedia.org/wiki?curid=63383085" title="Silvino Santos">
Silvino Santos

Silvino Simões Santos Silva (1886, Cernache do Bonjardim, Portugal – 14 May 1970, Manaus, Brazil) was a Portuguese-born cinematographer and photographer who emigrated and worked in Brazil. He is known for his role as director of the 1922 film "No País das Amazonas," which was one of the earliest documentary films that depicted the Amazon rainforest"." In addition to directing various other films about Brazil, Santos documented an expedition with the explorers Theodor Koch-Grunberg and Alexander H. Rice Jr. which was released as the 1924 film "No Rastro do Eldorado."

Born in Portugal, Santos left for Brazil early in his life. He practiced photography and was supported by the entrepreneur Julio César Arana, who was involved in the Amazon rubber industry. Arana sponsored Santos' trip to Paris in the early 1910s, where he experimented with cinematography through the Lumiere Brothers' inventions. Upon returning to Brazil with cinema film supplies, Santos created a film documenting Arana's rubber plantations along the Putomayo River. Santos settled in Manaus and around 1918 became involved in Amazônia Cine Film, a production company in the region.

His 1922 film "No País das Amazonas" was met with acclaim and shown in Rio de Janeiro at the Palais Cinema and in Paris. Santos continued to create documentary films about life and culture in Brazil. These included a film about the 1922–1923 Independence Centenary International Exposition and another about life in Rio de Janeiro, the capital of the country at the time. He worked for Joaquim Gonçalves de Araújo, a Brazilian film producer, for the remainder of his life. Santos married Anna Maria Schermuly and had two children. His last feature-length documentary was created in 1957, but Santos continued to make short films until the end of his life in 1970.





</doc>
<doc id="16278408" url="https://en.wikipedia.org/wiki?curid=16278408" title="Outline of Argentina">
Outline of Argentina

The following outline is provided as an overview of, and introduction to Argentina:

Argentina – country in South America, the continent's second largest by land area, after Brazil. It is constituted as a federation of 23 provinces and an autonomous city, Buenos Aires. It is the eighth-largest country in the world by land area and the largest among Spanish-speaking nations.


Geography of Argentina

Environment of Argentina


Regions of Argentina
The provinces of Argentina are often grouped into six geographical regions. From North to South and West to East, these are:

Provinces of Argentina
Municipalities of Argentina

Politics of Argentina

Government of Argentina



Court system of Argentina

Foreign relations of Argentina

The Argentine Republic is a member of:

Law of Argentina

Armed Forces of the Argentine Republic

History of Argentina




Culture of Argentina


Sports in Argentina

Economy of Argentina

Education in Argentina





</doc>
<doc id="18951905" url="https://en.wikipedia.org/wiki?curid=18951905" title="Argentina">
Argentina

Argentina (), officially the Argentine Republic (), is a country located mostly in the southern half of South America. Sharing the bulk of the Southern Cone with Chile to the west, the country is also bordered by Bolivia and Paraguay to the north, Brazil to the northeast, Uruguay and the South Atlantic Ocean to the east, and the Drake Passage to the south. With a mainland area of , Argentina is the eighth-largest country in the world, the fourth largest in the Americas, the second largest in South America after Brazil, and the largest Spanish-speaking nation. The sovereign state is subdivided into twenty-three provinces (, singular "provincia") and one autonomous city ("ciudad autónoma"), Buenos Aires, which is the federal capital of the nation () as decided by Congress. The provinces and the capital have their own constitutions, but exist under a federal system. Argentina claims sovereignty over part of Antarctica, the Falkland Islands (), and South Georgia and the South Sandwich Islands.

The earliest recorded human presence in modern-day Argentina dates back to the Paleolithic period. The Inca Empire expanded to the northwest of the country in Pre-Columbian times. The country has its roots in Spanish colonization of the region during the 16th century. Argentina rose as the successor state of the Viceroyalty of the Río de la Plata, a Spanish overseas viceroyalty founded in 1776. The declaration and fight for independence (1810–1818) was followed by an extended civil war that lasted until 1861, culminating in the country's reorganization as a federation of provinces with Buenos Aires as its capital city. The country thereafter enjoyed relative peace and stability, with several waves of European immigration, mainly Italians and Spaniards, radically reshaping its cultural and demographic outlook; 62.5% of the population has full or partial Italian ancestry, and the Argentine culture has significant connections to the Italian culture. The almost-unparalleled increase in prosperity led to Argentina becoming the seventh wealthiest nation in the world by the early 20th century.

Following the Great Depression in the 1930s, Argentina descended into political instability and economic decline that pushed it back into underdevelopment, though it remained among the fifteen richest countries for several decades. Following the death of President Juan Perón in 1974, his widow, Isabel Martínez de Perón, ascended to the presidency. She was overthrown in 1976 by a U.S.-backed coup which installed a right-wing military dictatorship. The military government persecuted and murdered numerous political critics, activists, and leftists in the Dirty War, a period of state terrorism that lasted until the election of Raúl Alfonsín as President in 1983. Several of the junta's leaders were later convicted of their crimes and sentenced to imprisonment.

Argentina is a prominent regional power in the Southern Cone and Latin America, and retains its historic status as a middle power in international affairs. Argentina has the second largest economy in South America, the third-largest in Latin America, and membership in the G-15 and G-20 major economies. It is also a founding member of the United Nations, World Bank, World Trade Organization, Mercosur, Union of South American Nations, Community of Latin American and Caribbean States and the Organization of Ibero-American States. Despite its history of economic instability, it ranks second highest in the Human Development Index in Latin America.

The description of the country by the word "Argentina" has been found on a Venetian map in 1536.

In English the name "Argentina" comes from the Spanish language, however the naming itself is not Spanish, but Italian. "Argentina" (masculine "argentino") means in Italian "(made) of silver, silver coloured", probably borrowed from the Old French adjective "argentine" "(made) of silver" > "silver coloured" already mentioned in the 12th century. The French word "argentine" is the feminine form of "argentin" and derives from "argent" "silver" with the suffix "-in" (same construction as Old French "acerin" "(made) of steel", from "acier" "steel" + "-in", or "sapin" "(made) of fir wood", from OF "sap" "fir" + "-in"). The Italian naming "Argentina" for the country implies "Terra Argentina" "land of silver" or "Costa Argentina" "coast of silver". In Italian, the adjective or the proper noun is often used in an autonomous way as a substantive and replaces it and it is said "l'Argentina".

The name "Argentina" was probably first given by the Venetian and Genoese navigators, such as Giovanni Caboto. In Spanish and Portuguese, the words for "silver" are respectively "plata" and "prata" and "(made) of silver" is said "plateado" and "prateado". "Argentina" was first associated with the silver mountains legend, widespread among the first European explorers of the La Plata Basin.

The first written use of the name in Spanish can be traced to "La Argentina", a 1602 poem by Martín del Barco Centenera describing the region.
Although "Argentina" was already in common usage by the 18th century, the country was formally named "Viceroyalty of the Río de la Plata" by the Spanish Empire, and "United Provinces of the Río de la Plata" after independence.

The 1826 constitution included the first use of the name "Argentine Republic" in legal documents.
The name "Argentine Confederation" was also commonly used and was formalized in the Argentine Constitution of 1853.
In 1860 a presidential decree settled the country's name as "Argentine Republic", and that year's constitutional amendment ruled all the names since 1810 as legally valid.

In English, the country was traditionally called "the Argentine", mimicking the typical Spanish usage "la Argentina" and perhaps resulting from a mistaken shortening of the fuller name 'Argentine Republic'. 'The Argentine' fell out of fashion during the mid-to-late 20th century, and now the country is simply referred to as "Argentina".

In Spanish, "Argentina" is feminine ("La [República] Argentina"), taking the feminine article "la", as the initial syllable of "Argentina" is unstressed.

The earliest traces of human life in the area now known as Argentina are dated from the Paleolithic period, with further traces in the Mesolithic and Neolithic.
Until the period of European colonization, Argentina was relatively sparsely populated by a wide number of diverse cultures with different social organizations, which can be divided into three main groups. The first group are basic hunters and food gatherers without development of pottery, such as the Selknam and Yaghan in the extreme south. The second group are advanced hunters and food gatherers which include the Puelche, Querandí and Serranos in the center-east; and the Tehuelche in the south—all of them conquered by the Mapuche spreading from Chile—and the Kom and Wichi in the north. The last group are farmers with pottery, like the Charrúa, Minuane and Guaraní in the northeast, with slash and burn semisedentary existence; the advanced Diaguita sedentary trading culture in the northwest, which was conquered by the Inca Empire around 1480; the Toconoté and Hênîa and Kâmîare in the country's center, and the Huarpe in the center-west, a culture that raised llama cattle and was strongly influenced by the Incas.

Europeans first arrived in the region with the 1502 voyage of Amerigo Vespucci. The Spanish navigators Juan Díaz de Solís and Sebastian Cabot visited the territory that is now Argentina in 1516 and 1526, respectively. In 1536 Pedro de Mendoza founded the small settlement of Buenos Aires, which was abandoned in 1541.

Further colonization efforts came from Paraguay—establishing the Governorate of the Río de la Plata—Peru and Chile. Francisco de Aguirre founded Santiago del Estero in 1553. Londres was founded in 1558; Mendoza, in 1561; San Juan, in 1562; San Miguel de Tucumán, in 1565. Juan de Garay founded Santa Fe in 1573 and the same year Jerónimo Luis de Cabrera set up Córdoba. Garay went further south to re-found Buenos Aires in 1580. San Luis was established in 1596.

The Spanish Empire subordinated the economic potential of the Argentine territory to the immediate wealth of the silver and gold mines in Bolivia and Peru, and as such it became part of the Viceroyalty of Peru until the creation of the Viceroyalty of the Río de la Plata in 1776 with Buenos Aires as its capital.

Buenos Aires repelled two ill-fated British invasions in 1806 and 1807. The ideas of the Age of Enlightenment and the example of the first Atlantic Revolutions generated criticism of the absolutist monarchy that ruled the country. As in the rest of Spanish America, the overthrow of Ferdinand VII during the Peninsular War created great concern.

Beginning a process from which Argentina was to emerge as successor state to the Viceroyalty, the 1810 May Revolution replaced the viceroy Baltasar Hidalgo de Cisneros with the First Junta, a new government in Buenos Aires composed by locals.
In the first clashes of the Independence War the Junta crushed a royalist counter-revolution in Córdoba, but failed to overcome those of the Banda Oriental, Upper Peru and Paraguay, which later became independent states.

Revolutionaries split into two antagonist groups: the Centralists and the Federalists—a move that would define Argentina's first decades of independence. The Assembly of the Year XIII appointed Gervasio Antonio de Posadas as Argentina's first Supreme Director.

On 9 July 1816, the Congress of Tucumán formalized the Declaration of Independence, which is now celebrated as Independence Day, a national holiday. One year later General Martín Miguel de Güemes stopped royalists on the north, and General José de San Martín took an army across the Andes and secured the independence of Chile; then he led the fight to the Spanish stronghold of Lima and proclaimed the independence of Peru. In 1819 Buenos Aires enacted a centralist constitution that was soon abrogated by federalists.

The 1820 Battle of Cepeda, fought between the Centralists and the Federalists, resulted in the "end of the Supreme Director rule". In 1826 Buenos Aires enacted another centralist constitution, with Bernardino Rivadavia being appointed as the first president of the country. However, the interior provinces soon rose against him, forced his resignation and discarded the constitution. Centralists and Federalists resumed the civil war; the latter prevailed and formed the Argentine Confederation in 1831, led by Juan Manuel de Rosas. During his regime he faced a French blockade (1838–1840), the War of the Confederation (1836–1839), and a combined Anglo-French blockade (1845–1850), but remained undefeated and prevented further loss of national territory. His trade restriction policies, however, angered the interior provinces and in 1852 Justo José de Urquiza, another powerful caudillo, beat him out of power. As new president of the Confederation, Urquiza enacted the liberal and federal 1853 Constitution. Buenos Aires seceded but was forced back into the Confederation after being defeated in the 1859 Battle of Cepeda.

Overpowering Urquiza in the 1861 Battle of Pavón, Bartolomé Mitre secured Buenos Aires predominance and was elected as the first president of the reunified country. He was followed by Domingo Faustino Sarmiento and Nicolás Avellaneda; these three presidencies set up the bases of the modern Argentine State.

Starting with Julio Argentino Roca in 1880, ten consecutive federal governments emphasized liberal economic policies. The massive wave of European immigration they promoted—second only to the United States'—led to a near-reinvention of Argentine society and economy that by 1908 had placed the country as the seventh wealthiest developed nation in the world.
Driven by this immigration wave and decreasing mortality, the Argentine population grew fivefold and the economy 15-fold: from 1870 to 1910 Argentina's wheat exports went from per year, while frozen beef exports increased from per year, placing Argentina as one of the world's top five exporters. Its railway mileage rose from . Fostered by a new public, compulsory, free and secular education system, literacy quickly increased from 22% to 65%, a level higher than most Latin American nations would reach even fifty years later. Furthermore, real GDP grew so fast that despite the huge immigration influx, per capita income between 1862 and 1920 went from 67% of developed country levels to 100%: In 1865, Argentina was already one of the top 25 nations by per capita income. By 1908, it had surpassed Denmark, Canada and The Netherlands to reach 7th place—behind Switzerland, New Zealand, Australia, the United States, the United Kingdom and Belgium. Argentina's per capita income was 70% higher than Italy's, 90% higher than Spain's, 180% higher than Japan's and 400% higher than Brazil's. Despite these unique achievements, the country was slow to meet its original goals of industrialization: after steep development of capital-intensive local industries in the 1920s, a significant part of the manufacture sector remained labor-intensive in the 1930s.

Between 1878 and 1884 the so-called Conquest of the Desert occurred, with the purpose of giving by means of the constant confrontations between natives and Criollos in the border, and the appropriation of the indigenous territories, tripling the Argentine territory. The first conquest, consisted of a series of military incursions into the Pampa and Patagonian territories dominated by the indigenous peoples, distributing them among the members of the "Sociedad Rural Argentina", financiers of the expeditions. The conquest of Chaco lasted up to fines of the century, since its full ownership of the national economic system only took place when the mere extraction of wood and tannin was replaced by the production of cotton. The Argentine government considered indigenous people as inferior beings, without the same rights as Criollos and Europeans.

In 1912, President Roque Sáenz Peña enacted universal and secret male suffrage, which allowed Hipólito Yrigoyen, leader of the Radical Civic Union (or UCR), to win the 1916 election. He enacted social and economic reforms and extended assistance to small farms and businesses. Argentina stayed neutral during World War I. The second administration of Yrigoyen faced an economic crisis, precipitated by the Great Depression.

In 1930, Yrigoyen was ousted from power by the military led by José Félix Uriburu. Although Argentina remained among the fifteen richest countries until mid-century, this coup d'état marks the start of the steady economic and social decline that pushed the country back into underdevelopment.

Uriburu ruled for two years; then Agustín Pedro Justo was elected in a fraudulent election, and signed a controversial treaty with the United Kingdom. Argentina stayed neutral during World War II, a decision that had full British support but was rejected by the United States after the attack on Pearl Harbor. A new military coup toppled the government, and Argentina declared war on the Axis Powers on March 27, 1945, a month before the end of World War II in Europe. The minister of welfare, Juan Domingo Perón, was fired and jailed because of his high popularity among workers. His liberation was forced by a massive popular demonstration, and he went on to win the 1946 election.

Perón created a political movement known as Peronism. He nationalized strategic industries and services, improved wages and working conditions, paid the full external debt and achieved nearly full employment. The economy, however, began to decline in 1950 because of over-expenditure. His highly popular wife, Eva Perón, played a central political role. She pushed Congress to enact women's suffrage in 1947, and developed an unprecedented social assistance to the most vulnerable sectors of society. However, her declining health did not allow her to run for the vice-presidency in 1951, and she died of cancer the following year. Perón was reelected in 1951, surpassing even his 1946 performance. In 1955 the Navy bombed the Plaza de Mayo in an ill-fated attempt to kill the President. A few months later, during the self-called Liberating Revolution coup, he resigned and went into exile in Spain.

The new head of State, Pedro Eugenio Aramburu, proscribed Peronism and banned all of its manifestations; nevertheless, Peronists kept an organized underground. Arturo Frondizi from the UCR won the following elections. He encouraged investment to achieve energetic and industrial self-sufficiency, reversed a chronic trade deficit and lifted Peronism proscription; yet his efforts to stay on good terms with Peronists and the military earned him the rejection of both and a new coup forced him out. But Senate Chief José María Guido reacted swiftly and applied the anti-power vacuum legislation, becoming president instead; elections were repealed and Peronism proscribed again. Arturo Illia was elected in 1963 and led to an overall increase in prosperity; however his attempts to legalize Peronism resulted in his overthrow in 1966 by the Juan Carlos Onganía-led coup d'état called the Argentine Revolution, creating a new military government that sought to rule indefinitely.

The "Dirty War" () was part of Operation Condor which included participation of the right-wing dictatorships of the Southern Cone. The Dirty War involved state terrorism in Argentina and elsewhere in the Southern Cone against political dissidents, with military and security forces employing urban and rural violence against left-wing guerrillas, political dissidents, and anyone believed to be associated with socialism or somehow contrary to the neoliberal economic policies of the regime. Victims of the violence in Argentina alone included an estimated 15,000 to 30,000 left-wing activists and militants, including trade unionists, students, journalists, Marxists, Peronist guerrillas and alleged sympathizers. Most were victims of state terrorism. The guerrillas' number of victims are nearly 500-540 between military and police officials and up to 230 civilians. Argentina received technical support and military aid from the United States government during the Johnson, Nixon, Ford, Carter, and Reagan administrations.

The exact chronology of the repression is still debated, however, as in some senses the long political war started in 1969. Trade unionists were targeted for assassination by the Peronist and Marxist paramilitaries as early as 1969, and individual cases of state-sponsored terrorism against Peronism and the left can be traced back to the Bombing of Plaza de Mayo in 1955. The Trelew massacre of 1972, the actions of the Argentine Anticommunist Alliance since 1973, and Isabel Martínez de Perón's "annihilation decrees" against left-wing guerrillas during "Operativo Independencia" (translates to Operation of Independence) in 1975, have also been suggested as dates for the beginning of the Dirty War.

Onganía shut down Congress, banned all political parties and dismantled student and worker unions. In 1969, popular discontent led to two massive protests: the "Cordobazo" and the "Rosariazo". The terrorist guerrilla organization Montoneros kidnapped and executed Aramburu. The newly chosen head of government, Alejandro Agustín Lanusse, seeking to ease the growing political pressure, let Héctor José Cámpora be the Peronist candidate instead of Perón. Cámpora won the March 1973 election, issued a pardon for condemned guerrilla members and then secured Perón's return from his exile in Spain.

On the day Perón returned to Argentina, the clash between Peronist internal factions—right-wing union leaders and left-wing youth from Montoneros—resulted in the Ezeiza Massacre. Cámpora resigned, overwhelmed by political violence, and Perón won the September 1973 election with his third wife Isabel as vice-president. He expelled Montoneros from the party and they became once again a clandestine organization. José López Rega organized the Argentine Anticommunist Alliance (AAA) to fight against them and the People's Revolutionary Army (ERP).
Perón died in July 1974 and was succeeded by his wife, who signed a secret decree empowering the military and the police to "annihilate" the left-wing subversion, stopping ERP's attempt to start a rural insurgence in Tucumán province. Isabel Perón was ousted one year later by a junta of the three armed forces, led by army general Jorge Rafael Videla. They initiated the National Reorganization Process, often shortened to "Proceso".

The "Proceso" shut down Congress, removed the judges of the Supreme Court, banned political parties and unions, and resorted to the forced disappearance of suspected guerrilla members and of anyone believed to be associated with the left-wing. By the end of 1976 Montoneros had lost near 2,000 members; by 1977, the ERP was completely defeated. A severely weakened Montoneros launched a counterattack in 1979, which was quickly annihilated, ending the guerrilla threat. Nevertheless, the junta stayed in power.

In 1982, the then head of state, General Leopoldo Galtieri, authorised the invasion of the British territories of South Georgia and, on 2 April, of the Falkland Islands. This led to the Falklands War with the United Kingdom and an Argentine surrender on 14 June. Rioting on the streets of Buenos Aires followed the defeat and the military leadership responsible for the humiliation stood down. Reynaldo Bignone replaced Galtieri and began to organize the transition to democratic rule.

Raúl Alfonsín won the 1983 elections campaigning for the prosecution of those responsible for human rights violations during the "Proceso": the Trial of the Juntas and other martial courts sentenced all the coup's leaders but, under military pressure, he also enacted the Full Stop and Due Obedience laws, which halted prosecutions further down the chain of command. The worsening economic crisis and hyperinflation reduced his popular support and the Peronist Carlos Menem won the 1989 election. Soon after, riots forced Alfonsín to an early resignation.

Menem embraced neo-liberal policies: a fixed exchange rate, business deregulation, privatizations and dismantling of protectionist barriers normalized the economy for a while. He pardoned the officers who had been sentenced during Alfonsín's government. The 1994 Constitutional Amendment allowed Menem to be elected for a second term. The economy began to decline in 1995, with increasing unemployment and recession; led by Fernando de la Rúa, the UCR returned to the presidency in the 1999 elections.
De la Rúa kept Menem's economic plan despite the worsening crisis, which led to growing social discontent. A massive capital flight was responded to with a freezing of bank accounts, generating further turmoil. The December 2001 riots forced him to resign. Congress appointed Eduardo Duhalde as acting president, who abrogated the fixed exchange rate established by Menem, causing many Argentines to lose a significant portion of their savings. By the late 2002 the economic crisis began to recede, but the assassination of two "piqueteros" by the police caused political commotion, prompting Duhalde to move elections forward. Néstor Kirchner was elected as the new president.

Boosting the neo-Keynesian economic policies laid by Duhalde, Kirchner ended the economic crisis attaining significant fiscal and trade surpluses, and steep GDP growth. Under his administration Argentina restructured its defaulted debt with an unprecedented discount of about 70% on most bonds, paid off debts with the International Monetary Fund, purged the military of officers with doubtful human rights records, nullified and voided the Full Stop and Due Obedience laws, ruled them as unconstitutional, and resumed legal prosecution of the Juntas' crimes. He did not run for reelection, promoting instead the candidacy of his wife, senator Cristina Fernández de Kirchner, who was elected in 2007 and reelected in 2011. Fernández de Kirchner's administration oversaw a positive foreign policy with good relations with other South American nations; however, relations with the United States and United Kingdom remained heavily strained. Jorge Rafael Videla, who had led the repression during the Dirty War, was sentenced to life in a civilian prison in 2010 under de Kirchner's administration; he later died in prison in 2013.

On 22 November 2015, after a tie in the first round of presidential elections on 25 October, Mauricio Macri won the first ballotage in Argentina's history, beating Front for Victory candidate Daniel Scioli and becoming president-elect. Macri is the first democratically elected non-radical or peronist president since 1916. He took office on 10 December 2015. In April 2016, the Macri Government introduced austerity measures intended to tackle inflation and public deficits.

With a mainland surface area of , Argentina is located in southern South America, sharing land borders with Chile across the Andes to the west; Bolivia and Paraguay to the north; Brazil to the northeast, Uruguay and the South Atlantic Ocean to the east; and the Drake Passage to the south; for an overall land border length of . Its coastal border over the Río de la Plata and South Atlantic Ocean is long.

Argentina's highest point is Aconcagua in the Mendoza province ( above sea level), also the highest point in the Southern and Western Hemispheres.
The lowest point is Laguna del Carbón in the "San Julián Great Depression" Santa Cruz province ( below sea level, also the lowest point in the Southern and Western Hemispheres, and the seventh lowest point on Earth)

The northernmost point is at the confluence of the Grande de San Juan and Río Mojinete rivers in Jujuy province; the southernmost is Cape San Pío in Tierra del Fuego province; the easternmost is northeast of Bernardo de Irigoyen, Misiones and the westernmost is within Los Glaciares National Park in Santa Cruz province.
The maximum north–south distance is , while the maximum east–west one is .

Some of the major rivers are the Paraná, Uruguay—which join to form the Río de la Plata, Paraguay, Salado, Negro, Santa Cruz, Pilcomayo, Bermejo and Colorado. These rivers are discharged into the Argentine Sea, the shallow area of the Atlantic Ocean over the Argentine Shelf, an unusually wide continental platform. Its waters are influenced by two major ocean currents: the warm Brazil Current and the cold Falklands Current.

Argentina is a one of the most biodiverse countries in the world hosting one of the greatest ecosystem varieties in the world: 15 continental zones, 2 marine zones, and the Antarctic region are all represented in its territory.
This huge ecosystem variety has led to a biological diversity that is among the world's largest:

The original pampa had virtually no trees; some imported species like the American sycamore or eucalyptus are present along roads or in towns and country estates ("estancias"). The only tree-like plant native to the pampa is the evergreen Ombú. The surface soils of the pampa are a deep black color, primarily mollisols, known commonly as "humus". This makes the region one of the most agriculturally productive on Earth; however, this is also responsible for decimating much of the original ecosystem, to make way for commercial agriculture. The western pampas receive less rainfall, this "dry pampa" is a plain of short grasses or steppe.

The National Parks of Argentina make up a network of 35 national parks in Argentina. The parks cover a very varied set of terrains and biotopes, from Baritú National Park on the northern border with Bolivia to Tierra del Fuego National Park in the far south of the continent. The Administración de Parques Nacionales (National Parks Administration) is the agency that preserves and manages these national parks along with Natural monuments and National Reserves within the country.

In general, Argentina has four main climate types: warm, moderate, arid, and cold, all determined by the expanse across latitude, range in altitude, and relief features. Although the most populated areas are generally temperate, Argentina has an exceptional amount of climate diversity, ranging from subtropical in the north to polar in the far south. Consequently, there is a wide variety of biomes in the country, including subtropical rain forests, semi-arid and arid regions, temperate plains in the Pampas, and cold subantarctic in the south. The average annual precipitation ranges from in the driest parts of Patagonia to over in the westernmost parts of Patagonia and the northeastern parts of the country. Mean annual temperatures range from in the far south to in the north.

Major wind currents include the cool Pampero Winds blowing on the flat plains of Patagonia and the Pampas; following the cold front, warm currents blow from the north in middle and late winter, creating mild conditions.
The Sudestada usually moderates cold temperatures but brings very heavy rains, rough seas and coastal flooding. It is most common in late autumn and winter along the central coast and in the Río de la Plata estuary.
The Zonda, a hot dry wind, affects Cuyo and the central Pampas. Squeezed of all moisture during the descent from the Andes, Zonda winds can blow for hours with gusts up to , fueling wildfires and causing damage; between June and November, when the Zonda blows, snowstorms and blizzard ("viento blanco") conditions usually affect higher elevations.

In the 20th century, Argentina experienced significant political turmoil and democratic reversals. Between 1930 and 1976, the armed forces overthrew six governments in Argentina; and the country alternated periods of democracy (1912–1930, 1946–1955, and 1973–1976) with periods of restricted democracy and military rule. Following a transition that began in 1983, full-scale democracy in Argentina was reestablished. Argentina's democracy endured through the 2001–02 crisis and to the present day; it is regarded as more robust than both its pre-1983 predecessors and other democracies in Latin America.

Argentina is a federal constitutional republic and representative democracy. The government is regulated by a system of checks and balances defined by the Constitution of Argentina, the country's supreme legal document. The seat of government is the city of Buenos Aires, as designated by Congress. Suffrage is universal, equal, secret and mandatory.

The federal government is composed of three branches:
The Legislative branch consists of the bicameral Congress, made up of the Senate and the Chamber of Deputies. The Congress makes federal law, declares war, approves treaties and has the power of the purse and of impeachment, by which it can remove sitting members of the government. The Chamber of Deputies represents the people and has 257 voting members elected to a four-year term. Seats are apportioned among the provinces by population every tenth year. ten provinces have just five deputies while the Buenos Aires Province, being the most populous one, has 70. The Chamber of Senators represents the provinces, has 72 members elected at-large to six-year terms, with each province having three seats; one third of Senate seats are up for election every other year. At least one-third of the candidates presented by the parties must be women.

In the Executive branch, the President is the commander-in-chief of the military, can veto legislative bills before they become law—subject to Congressional override—and appoints the members of the Cabinet and other officers, who administer and enforce federal laws and policies. The President is elected directly by the vote of the people, serves a four-year term and may be elected to office no more than twice in a row.

The Judicial branch includes the Supreme Court and lower federal courts interpret laws and overturn those they find unconstitutional. The Judicial is independent of the Executive and the Legislative. The Supreme Court has seven members appointed by the President—subject to Senate approval—who serve for life. The lower courts' judges are proposed by the Council of Magistracy (a secretariat composed of representatives of judges, lawyers, researchers, the Executive and the Legislative), and appointed by the President on Senate approval.

Argentina is a federation of twenty-three provinces and one autonomous city, Buenos Aires. Provinces are divided for administration purposes into departments and municipalities, except for Buenos Aires Province, which is divided into partidos. The City of Buenos Aires is divided into communes.

Provinces hold all the power that they chose not to delegate to the federal government; they must be representative republics and must not contradict the Constitution. Beyond this they are fully autonomous: they enact their own constitutions, freely organize their local governments, and own and manage their natural and financial resources. Some provinces have bicameral legislatures, while others have unicameral ones.

During the War of Independence the main cities and their surrounding countrysides became provinces though the intervention of their cabildos. The Anarchy of the Year XX completed this process, shaping the original thirteen provinces. Jujuy seceded from Salta in 1834, and the thirteen provinces became fourteen.
After seceding for a decade, Buenos Aires accepted the 1853 Constitution of Argentina in 1861, and was made a federal territory in 1880.

An 1862 law designated as national territories those under federal control but outside the frontiers of the provinces. In 1884 they served as bases for the establishment of the governorates of Misiones, Formosa, Chaco, La Pampa, Neuquén, Río Negro, Chubut, Santa Cruz and Tierra del Fuego.
The agreement about a frontier dispute with Chile in 1900 created the National Territory of Los Andes; its lands were incorporated into Jujuy, Salta and Catamarca in 1943. La Pampa and Chaco became provinces in 1951. Misiones did so in 1953, and Formosa, Neuquén, Río Negro, Chubut and Santa Cruz, in 1955. The last national territory, Tierra del Fuego, became the Tierra del Fuego, Antártida e Islas del Atlántico Sur Province in 1990. It has three components, although two are nominal because they are not under Argentine sovereignty. The first is the Argentine part of Tierra del Fuego; the second is an area of Antarctica claimed by Argentina that overlaps with similar areas claimed by the UK and Chile; the third comprises the two disputed British Overseas Territories of the Falkland Islands and South Georgia and the South Sandwich Islands.

Foreign policy is handled by the Ministry of Foreign Affairs, International Trade and Worship, which answers to the President.

A middle power, Argentina bases its foreign policies on the guiding principles of non-intervention, human rights, self-determination, international cooperation, disarmament and peaceful settlement of conflicts.

The country is one of the G-15 and G-20 major economies of the world, and a founding member of the UN, WBG, WTO and OAS.
In 2012 Argentina was elected again to a two-year non-permanent position on the United Nations Security Council and is participating in major peacekeeping operations in Haiti, Cyprus, Western Sahara and the Middle East.

A prominent Latin American and Southern Cone regional power, Argentina co-founded OEI, CELAC and UNASUR, of which the former President Néstor Kirchner was first Secretary General.
It is also a founding member of the Mercosur block, having Brazil, Paraguay, Uruguay and Venezuela as partners. Since 2002 the country has emphasized its key role in Latin American integration, and the block—which has some supranational legislative functions—is its first international priority.
Argentina claims in Antarctica, where it has the world's oldest continuous state presence, since 1904. This overlaps claims by Chile and the United Kingdom, though all such claims fall under the provisions of the 1961 Antarctic Treaty, of which Argentina is a founding signatory and permanent consulting member, with the Antarctic Treaty Secretariat being based in Buenos Aires.

Argentina disputes sovereignty over the Falkland Islands (), and South Georgia and the South Sandwich Islands, which are administered by the United Kingdom as Overseas Territories.

The President holds the title of commander-in-chief of the Argentine Armed Forces, as part of a legal framework that imposes a strict separation between national defense and internal security systems:

The National Defense System, an exclusive responsibility of the federal government, coordinated by the Ministry of Defense, and comprising the Army, the Navy and the Air Force. Ruled and monitored by Congress through the Houses' Defense Committees, it is organized on the essential principle of legitimate self-defense: the repelling of any external military aggression in order to guarantee freedom of the people, national sovereignty, and territorial integrity. Its secondary missions include committing to multinational operations within the framework of the United Nations, participating in internal support missions, assisting friendly countries, and establishing a sub-regional defense system.
Military service is voluntary, with enlistment age between 18 and 24 years old and no conscription. Argentina's defense has historically been one of the best equipped in the region, even managing its own weapon research facilities, shipyards, ordnance, tank and plane factories. However, real military expenditures declined steadily after 1981 and the defense budget in 2011 was about 0.74% of GDP, a historical minimum, below the Latin American average.

The Interior Security System, jointly administered by the federal and subscribing provincial governments. At the federal level it is coordinated by the Interior, Security and Justice ministries, and monitored by Congress. It is enforced by the Federal Police; the Prefecture, which fulfills coast guard duties; the Gendarmerie, which serves border guard tasks; and the Airport Security Police. At the provincial level it is coordinated by the respective internal security ministries and enforced by local police agencies.

Argentina was the only South American country to send warships and cargo planes in 1991 to the Gulf War under UN mandate and has remained involved in peacekeeping efforts in multiple locations like UNPROFOR in Croatia/Bosnia, Gulf of Fonseca, UNFICYP in Cyprus (where among Army and Marines troops the Air Force provided the UN Air contingent since 1994) and MINUSTAH in Haiti. Argentina is the only Latin American country to maintain troops in Kosovo during SFOR (and later EUFOR) operations where combat engineers of the Argentine Armed Forces are embedded in an Italian brigade.

In 2007, an Argentine contingent including helicopters, boats and water purification plants was sent to help Bolivia against their worst floods in decades. In 2010 the Armed Forces were also involved in Haiti and Chile humanitarian responses after their respective earthquakes.

Benefiting from rich natural resources, a highly literate population, a diversified industrial base, and an export-oriented agricultural sector, the economy of Argentina is Latin America's third-largest, and the second largest in South America. It has a "very high" rating on the Human Development Index and a relatively high GDP per capita, with a considerable internal market size and a growing share of the high-tech sector.
A middle emerging economy and one of the world's top developing nations, Argentina is a member of the G-20 major economies. Historically, however, its economic performance has been very uneven, with high economic growth alternating with severe recessions, income maldistribution and—in the recent decades—increasing poverty. Early in the 20th century Argentina achieved development, and became the world's seventh richest country. Although managing to keep a place among the top fifteen economies until mid-century, it suffered a long and steady decline, but it is still a high income country.

High inflation—a weakness of the Argentine economy for decades—has become a trouble once again, with an annual rate of 24.8% in 2017. To deter it and support the peso, the government imposed foreign currency control. Income distribution, having improved since 2002, is classified as "medium", although it is still considerably unequal.

Argentina ranks 85th out of 180 countries in the Transparency International's 2017 Corruption Perceptions Index, an improvement of 22 positions over its 2014 rankings. Argentina settled its long-standing debt default crisis in 2016 with the so-called vulture funds after the election of Mauricio Macri, allowing Argentina to enter capital markets for the first time in a decade.

 manufacturing accounted for 20.3% of GDP—the largest sector in the nation's economy. Well-integrated into Argentine agriculture, half of the industrial exports have rural origin.

With a 6.5% production growth rate , the diversified manufacturing sector rests on a steadily growing network of industrial parks (314 )

Córdoba is Argentina's major industrial center, hosting metalworking, motor vehicle and auto parts manufactures. Next in importance are the Greater Buenos Aires area (food processing, metallurgy, motor vehicles and auto parts, chemicals and petrochemicals, consumer durables, textiles and printing); Rosario (food processing, metallurgy, farm machinery, oil refining, chemicals, and tanning); San Miguel de Tucumán (sugar refining); San Lorenzo (chemicals and pharmaceuticals); San Nicolás de los Arroyos (steel milling and metallurgy); and Ushuaia and Bahía Blanca (oil refining).
Other manufacturing enterprises are located in the provinces of Santa Fe (zinc and copper smelting, and flour milling); Mendoza and Neuquén (wineries and fruit processing); Chaco (textiles and sawmills); and Santa Cruz, Salta and Chubut (oil refining).

The electric output of Argentina totaled over , of which about 37% was consumed by industrial activities.

Argentina has the largest railway system in Latin America, with of operating lines , out of a full network of almost . This system links all 23 provinces plus Buenos Aires City, and connects with all neighboring countries. There are four incompatible gauges in use; this forces virtually all interregional freight traffic to pass through Buenos Aires. The system has been in decline since the 1940s: regularly running up large budgetary deficits, by 1991 it was transporting 1,400 times less goods than it did in 1973. However, in recent years the system has experienced a greater degree of investment from the state, in both commuter rail lines and long distance lines, renewing rolling stock and infrastructure. In April 2015, by overwhelming majority the Argentine Senate passed a law which re-created Ferrocarriles Argentinos (2015), effectively re-nationalising the country's railways, a move which saw support from all major political parties on both sides of the political spectrum.
Nevertheless, this road infrastructure is still inadequate and cannot handle the sharply growing demand caused by deterioration of the railway system.

Some of the largest sea ports are La Plata–Ensenada, Bahía Blanca, Mar del Plata, Quequén–Necochea, Comodoro Rivadavia, Puerto Deseado, Puerto Madryn, Ushuaia and San Antonio Oeste.
Buenos Aires has historically been the most important port; however since the 1990s the Up-River port region has become dominant: stretching along of the Paraná river shore in Santa Fe province, it includes 17 ports and accounted for 50% of all exports.

Print media industry is highly developed in Argentina, with more than two hundred newspapers. The major national ones include "Clarín" (centrist, Latin America's best-seller and the second most widely circulated in the Spanish-speaking world), "La Nación" (center-right, published since 1870), "Página/12" (leftist, founded in 1987), the Buenos Aires Herald (Latin America's most prestigious English language daily, liberal, dating back to 1876), "La Voz del Interior" (center, founded in 1904), and the "Argentinisches Tageblatt" (German weekly, liberal, published since 1878)

Argentina began the world's first regular radio broadcasting on 27 August 1920, when Richard Wagner's "Parsifal" was aired by a team of medical students led by Enrique Telémaco Susini in Buenos Aires' Teatro Coliseo. there were 260 AM and 1150 FM registered radio stations in the country.

The Argentine television industry is large, diverse and popular across Latin America, with many productions and TV formats having been exported abroad. Since 1999 Argentines enjoy the highest availability of cable and satellite television in Latin America, totaling 87.4% of the country's households, a rate similar to those in the United States, Canada and Europe.

Argentines have received three Nobel Prizes in the Sciences. Bernardo Houssay, the first Latin American recipient, discovered the role of pituitary hormones in regulating glucose in animals, and shared the Nobel Prize in Physiology or Medicine in 1947. Luis Leloir discovered how organisms store energy converting glucose into glycogen and the compounds which are fundamental in metabolizing carbohydrates, receiving the Nobel Prize in Chemistry in 1970. César Milstein did extensive research in antibodies, sharing the Nobel Prize in Physiology or Medicine in 1984. Argentine research has led to treatments for heart diseases and several forms of cancer. Domingo Liotta designed and developed the first artificial heart that was successfully implanted in a human being in 1969. René Favaloro developed the techniques and performed the world's first coronary bypass surgery.

Argentina's nuclear programme has been highly successful. In 1957 Argentina was the first country in Latin America to design and build a research reactor with homegrown technology, the RA-1 Enrico Fermi. This reliance in the development of own nuclear related technologies, instead of simply buying them abroad, was a constant of Argentina's nuclear programme conducted by the civilian National Atomic Energy Commission (CNEA). Nuclear facilities with Argentine technology have been built in Peru, Algeria, Australia and Egypt. In 1983, the country admitted having the capability of producing weapon-grade uranium, a major step needed to assemble nuclear weapons; since then, however, Argentina has pledged to use nuclear power only for peaceful purposes. As a member of the Board of Governors of the International Atomic Energy Agency, Argentina has been a strong voice in support of nuclear non-proliferation efforts and is highly committed to global nuclear security. In 1974 it was the first country in Latin America to put in-line a commercial nuclear power plant, Atucha I. Although the Argentine built parts for that station amounted to 10% of the total, the nuclear fuel it uses are since entirely built in the country. Later nuclear power stations employed a higher percentage of Argentine built components; Embalse, finished in 1983, a 30% and the 2011 Atucha II reactor a 40%. 

Despite its modest budget and numerous setbacks, academics and the sciences in Argentina have enjoyed an international respect since the turn of the 1900s, when Luis Agote devised the first safe and effective means of blood transfusion as well as René Favaloro, who was a pioneer in the improvement of the coronary artery bypass surgery. Argentine scientists are still on the cutting edge in fields such as nanotechnology, physics, computer sciences, molecular biology, oncology, ecology and cardiology. Juan Maldacena, an Argentine-American scientist, is a leading figure in string theory.

Space research has also become increasingly active in Argentina. Argentine built satellites include LUSAT-1 (1990), Víctor-1 (1996), PEHUENSAT-1 (2007), and those developed by CONAE, the Argentine space agency, of the SAC series. Argentina has its own satellite programme, nuclear power station designs (4th generation) and public nuclear energy company INVAP, which provides several countries with nuclear reactors. Established in 1991, the CONAE has since launched two satellites successfully and, in June 2009, secured an agreement with the European Space Agency for the installation of a 35-m diameter antenna and other mission support facilities at the Pierre Auger Observatory, the world's foremost cosmic ray observatory. The facility will contribute to numerous ESA space probes, as well as CONAE's own, domestic research projects. Chosen from 20 potential sites and one of only three such ESA installations in the world, the new antenna will create a triangulation which will allow the ESA to ensure mission coverage around the clock

Tourism in Argentina is characterized by its cultural offerings and its ample and varied natural assets. The country had 5.57 million visitors in 2013, ranking in terms of the international tourist arrivals as the top destination in South America, and second in Latin America after Mexico. Revenues from international tourists reached billion in 2013, down from billion in 2012. The country's capital city, Buenos Aires, is the most visited city in South America. There are 30 National Parks of Argentina including many World Heritage Sites.

The 2010 census counted 40,117,096 inhabitants, up from 36,260,130 in 2001. Argentina ranks third in South America in total population, fourth in Latin America and 33rd globally. Its population density of 15 persons per square kilometer of land area is well below the world average of 50 persons. The population growth rate in 2010 was an estimated 1.03% annually, with a birth rate of 17.7 live births per 1,000 inhabitants and a mortality rate of 7.4 deaths per 1,000 inhabitants. Since 2010, the crude net migration rate has ranged from below zero to up to four immigrants per 1,000 inhabitants per year.

Argentina is in the midst of a demographic transition to an older and slower-growing population. The proportion of people under 15 is 25.6%, a little below the world average of 28%, and the proportion of people 65 and older is relatively high at 10.8%. In Latin America this is second only to Uruguay and well above the world average, which is currently 7%. Argentina has one of Latin America's lowest population growth rates as well as a comparatively low infant mortality rate. Its birth rate of 2.3 children per woman is considerably below the high of 7.0 children born per woman in 1895, though still nearly twice as high as in Spain or Italy, which are culturally and demographically similar. The median age is 31.9 years and life expectancy at birth is 77.14 years.

In 2010, Argentina became the first country in Latin America, the second in the Americas, and the tenth worldwide to legalize same-sex marriage.

As with other areas of new settlement, such as the United States, Canada, Australia, New Zealand, Brazil and Uruguay, Argentina is considered a country of immigrants. Argentines usually refer to the country as a "crisol de razas" (crucible of races, or melting pot).

Between 1857 and 1950 Argentina was the country with the second biggest immigration wave in the world, at 6.6 million, second only to the United States in the numbers of immigrants received (27 million) and ahead of other areas of new settlement like Canada, Brazil and Australia.

Strikingly, at those times, the national population doubled every two decades. This belief is endured in the popular saying "los argentinos descienden de los barcos" (Argentines descend from the ships). Therefore, most Argentines are descended from the 19th- and 20th-century immigrants of the great immigration wave to Argentina (1850–1955), with a great majority of these immigrants coming from diverse European countries, particularly Italy and Spain. The majority of Argentines descend from multiple European ethnic groups, primarily of Italian and Spanish descent, with over 25 million Argentines (almost 60% of the population) having some partial Italian origins.

Argentina is home to a significant Arab population; including those with partial descent, Arab Argentines number 1.3 to 3.5 million, mostly of Syrian and Lebanese origin. As in the United States, they are considered white . The majority of Arab Argentines are Christians belonging to the Maronite Church, Roman Catholic, Eastern Orthodox and Eastern Rite Catholic Churches. A minority are Muslims, albeit the largest Muslim community in the Americas. The Asian population in the country numbers around 180,000 individuals, most of whom are of Chinese and Korean descent, although an older Japanese community originating from the early 20th century still exists.

A 2010 study conducted on 218 individuals by the Argentine geneticist Daniel Corach established that the genetic map of Argentina is composed of 79% from different European ethnicities (mainly Spanish and Italian), 18% of different indigenous ethnicities, and 4.3% of African ethnic groups; 63.6% of the tested group had at least one ancestor who was Indigenous.

From the 1970s, immigration has mostly been coming from Bolivia, Paraguay and Peru, with smaller numbers from the Dominican Republic, Ecuador and Romania. The Argentine government estimates that 750,000 inhabitants lack official documents and has launched a program to encourage illegal immigrants to declare their status in return for two-year residence visas—so far over 670,000 applications have been processed under the program.


The "de facto" official language is Spanish, spoken by almost all Argentines.
The country is the largest Spanish-speaking society that universally employs "voseo", the use of the pronoun "vos" instead of "tú" ("you"), which imposes the use of alternative verb forms as well.
Due to the extensive Argentine geography, Spanish has a strong variation among regions, although the prevalent dialect is "Rioplatense", primarily spoken in the La Plata Basin and accented similarly to the Neapolitan language. Italian and other European immigrants influenced "Lunfardo"—the regional slang—permeating the vernacular vocabulary of other Latin American countries as well.

There are several second-languages in widespread use among the Argentine population:

The Constitution guarantees freedom of religion. Although it enforces neither an official nor a state faith, it gives Roman Catholicism a preferential status.

According to a 2008 CONICET poll, Argentines were 76.5% Catholic, 11.3% Agnostics and Atheists, 9% Evangelical Protestants, 1.2% Jehovah's Witnesses, and 0.9% Mormons, while 1.2% followed other religions, including Islam, Judaism and Buddhism. These figures appear to have changed quite significantly in recent years: data recorded in 2017 indicated that Catholics made up 66% of the population, indicating a drop of 10.5% in nine years, and the nonreligious in the country standing at 21% of the population, indicating an almost doubling over the same period.

The country is home to both the largest Muslim and largest Jewish communities in Latin America, the latter being the seventh most populous in the world. Argentina is a member of the International Holocaust Remembrance Alliance.

Argentines show high individualization and de-institutionalization of religious beliefs; 23.8% claim to always attend religious services; 49.1% seldom do and 26.8% never do.

On 13 March 2013, Argentine Jorge Mario Bergoglio, the Cardinal Archbishop of Buenos Aires, was elected Bishop of Rome and Supreme Pontiff of the Catholic Church. He took the name "Francis", and he became the first Pope from either the Americas or from the Southern Hemisphere; he is the first Pope born outside of Europe since the election of Pope Gregory III (who was Syrian) in 741.

Argentina is highly urbanized, with 92% of its population living in cities: the ten largest metropolitan areas account for half of the population.
About 3 million people live in the city of Buenos Aires, and including the Greater Buenos Aires metropolitan area it totals around 13 million, making it one of the largest urban areas in the world.

The metropolitan areas of Córdoba and Rosario have around 1.3 million inhabitants each. Mendoza, San Miguel de Tucumán, La Plata, Mar del Plata, Salta and Santa Fe have at least half a million people each.

The population is unequally distributed: about 60% live in the Pampas region (21% of the total area), including 15 million people in Buenos Aires province. The provinces of Córdoba and Santa Fe, and the city of Buenos Aires have 3 million each. Seven other provinces have over one million people each: Mendoza, Tucumán, Entre Ríos, Salta, Chaco, Corrientes and Misiones. With , Tucumán is the only Argentine province more densely populated than the world average; by contrast, the southern province of Santa Cruz has around .
The Argentine education system consists of four levels:

The Argentine state guarantees universal, secular and free-of-charge public education for all levels. Responsibility for educational supervision is organized at the federal and individual provincial states. In the last decades the role of the private sector has grown across all educational stages.

Health care is provided through a combination of employer and labor union-sponsored plans ("Obras Sociales"), government insurance plans, public hospitals and clinics and through private health insurance plans. Health care cooperatives number over 300 (of which 200 are related to labor unions) and provide health care for half the population; the national INSSJP (popularly known as PAMI) covers nearly all of the five million senior citizens.

There are more than 153,000 hospital beds, 121,000 physicians and 37,000 dentists (ratios comparable to developed nations). The relatively high access to medical care has historically resulted in mortality patterns and trends similar to developed nations': from 1953 to 2005, deaths from cardiovascular disease increased from 20% to 23% of the total, those from tumors from 14% to 20%, respiratory problems from 7% to 14%, digestive maladies (non-infectious) from 7% to 11%, strokes a steady 7%, injuries, 6%, and infectious diseases, 4%. Causes related to senility led to many of the rest. Infant deaths have fallen from 19% of all deaths in 1953 to 3% in 2005.

The availability of health care has also reduced infant mortality from 70 per 1000 live births in 1948 to 12.1 in 2009 and raised life expectancy at birth from 60 years to 76. Though these figures compare favorably with global averages, they fall short of levels in developed nations and in 2006, Argentina ranked fourth in Latin America.

Argentina is a multicultural country with significant European influences. Modern Argentine culture has been largely influenced by Italian, Spanish and other European immigration from France, United Kingdom, and Germany among others. Its cities are largely characterized by both the prevalence of people of European descent, and of conscious imitation of American and European styles in fashion, architecture and design. Museums, cinemas, and galleries are abundant in all the large urban centers, as well as traditional establishments such as literary bars, or bars offering live music of a variety of genres although there are lesser elements of Amerindian and African influences, particularly in the fields of music and art. The other big influence is the gauchos and their traditional country lifestyle of self-reliance. Finally, indigenous American traditions have been absorbed into the general cultural milieu.
Argentine writer Ernesto Sabato has reflected on the nature of the culture of Argentina as follows:
Although Argentina's rich literary history began around 1550, it reached full independence with Esteban Echeverría's "El Matadero", a romantic landmark that played a significant role in the development of 19th century's Argentine narrative, split by the ideological divide between the popular, federalist epic of José Hernández' "Martín Fierro" and the elitist and cultured discourse of Sarmiento's masterpiece, "Facundo".

The Modernist movement advanced into the 20th century including exponents such as Leopoldo Lugones and poet Alfonsina Storni; it was followed by Vanguardism, with Ricardo Güiraldes's "Don Segundo Sombra" as an important reference.

Jorge Luis Borges, Argentina's most acclaimed writer and one of the foremost figures in the history of literature, found new ways of looking at the modern world in metaphor and philosophical debate and his influence has extended to authors all over the globe. Short stories such as "Ficciones" and "The Aleph" are among his most famous works. He was a friend and collaborator of Adolfo Bioy Casares, who wrote one of the most praised science fiction novels, "The Invention of Morel".
Julio Cortázar, one of the leading members of the Latin American Boom and a major name in 20th century literature, influenced an entire generation of writers in the Americas and Europe.

A remarkable episode in the Argentine literature's history is the social and literarial dialectica between the so-called named this way because its members used to meet together at the at Florida street and published in the , like Jorge Luis Borges, , (artist), among others, versus the of Roberto Arlt,

Other highly regarded Argentine writers, poets and essayists include Estanislao del Campo, Eugenio Cambaceres, Pedro Bonifacio Palacios, Hugo Wast, Benito Lynch, Enrique Banchs, Oliverio Girondo, Ezequiel Martínez Estrada, Victoria Ocampo, Leopoldo Marechal, Silvina Ocampo, Roberto Arlt, Eduardo Mallea, Manuel Mujica Láinez, Ernesto Sábato, Silvina Bullrich, Rodolfo Walsh, María Elena Walsh, Tomás Eloy Martínez, Manuel Puig, Alejandra Pizarnik, and Osvaldo Soriano.

Tango, a "Rioplatense" musical genre with European and African influences, is one of Argentina's international cultural symbols.
The golden age of tango (1930 to mid-1950s) mirrored that of jazz and swing in the United States, featuring large orchestras like those of Osvaldo Pugliese, Aníbal Troilo, Francisco Canaro, Julio de Caro and Juan d'Arienzo.
After 1955, virtuoso Astor Piazzolla popularized "Nuevo tango", a subtler and more intellectual trend for the genre.
Tango enjoys worldwide popularity nowadays with groups like Gotan Project, Bajofondo and Tanghetto.

Argentina developed strong classical music and dance scenes that gave rise to renowned artists such as Alberto Ginastera, composer; Alberto Lysy, violinist; Martha Argerich and Eduardo Delgado, pianists; Daniel Barenboim, pianist and symphonic orchestra director; José Cura and Marcelo Álvarez, tenors; and to ballet dancers Jorge Donn, José Neglia, Norma Fontenla, "Maximiliano Guerra", Paloma Herrera, Marianela Núñez, Iñaki Urlezaga and Julio Bocca.
A national Argentine folk style emerged in the 1930s from dozens of regional musical genres and went to influence the entirety of Latin American music. Some of its interpreters, like Atahualpa Yupanqui and Mercedes Sosa, achieved worldwide acclaim.

The romantic ballad genre included singers of international fame such as Sandro de América.

Argentine rock developed as a distinct musical style in the mid-1960s, when Buenos Aires and Rosario became cradles of aspiring musicians.
Founding bands like Los Gatos, Sui Generis, Almendra and Manal were followed by Seru Giran, Los Abuelos de la Nada, Soda Stereo and Patricio Rey y sus Redonditos de Ricota, with prominent artists including Gustavo Cerati, Litto Nebbia, Andrés Calamaro, Luis Alberto Spinetta, Charly García, Fito Páez and León Gieco.

Tenor saxophonist Leandro "Gato" Barbieri and composer and big band conductor Lalo Schifrin are among the most internationally successful Argentine jazz musicians.

Another popular musical genre at present is Cumbia villera is a subgenre of cumbia music originated in the slums of Argentina and popularized all over Latin America and the Latin communities abroad.

Buenos Aires is one of the great theatre capitals of the world, with a scene of international caliber centered on Corrientes Avenue, "the street that never sleeps", sometimes referred to as an intellectual Broadway in Buenos Aires. Teatro Colón is a global landmark for opera and classical performances; its acoustics are considered among the world's top five. Other important theatrical venues include Teatro General San Martín, Cervantes, both in Buenos Aires City; Argentino in La Plata, El Círculo in Rosario, Independencia in Mendoza, and Libertador in Córdoba.
Griselda Gambaro, Copi, Roberto Cossa, Marco Denevi, Carlos Gorostiza, and Alberto Vaccarezza are a few of the most prominent Argentine playwrights.

Argentine theatre traces its origins to Viceroy Juan José de Vértiz y Salcedo's creation of the colony's first theatre, "La Ranchería", in 1783. In this stage, in 1786, a tragedy entitled "Siripo" had its premiere. "Siripo" is now a lost work (only the second act is conserved), and can be considered the first Argentine stage play, because it was written by Buenos Aires poet Manuel José de Lavardén, it was premiered in Buenos Aires, and its plot was inspired by an historical episode of the early colonization of the Río de la Plata Basin: the destruction of Sancti Spiritu colony by aboriginals in 1529. "La Ranchería" theatre operated until its destruction in a fire in 1792. The second theatre stage in Buenos Aires was Teatro Coliseo, opened in 1804 during the term of Viceroy Rafael de Sobremonte. It was the nation's longest-continuously operating stage. The musical creator of the Argentine National Anthem, Blas Parera, earned fame as a theatre score writer during the early 19th century. The genre suffered during the regime of Juan Manuel de Rosas, though it flourished alongside the economy later in the century. The national government gave Argentine theatre its initial impulse with the establishment of the Colón Theatre, in 1857, which hosted classical and operatic, as well as stage performances. Antonio Petalardo's successful 1871 gambit on the opening of the Teatro Opera, inspired others to fund the growing art in Argentina.

The Argentine film industry has historically been one of the three most developed in Latin American cinema, along with those produced in Mexico and Brazil. Started in 1896; by the early 1930s it had already become Latin America's leading film producer, a place it kept until the early 1950s. The world's first animated feature films were made and released in Argentina, by cartoonist Quirino Cristiani, in 1917 and 1918.

Argentine films have achieved worldwide recognition: the country has won two Academy Awards for Best Foreign Language Film, for "The Official Story" (1985) and "The Secret in Their Eyes" (2009), from seven nominations:

In addition, Argentine composers Luis Enrique Bacalov and Gustavo Santaolalla have been honored with Academy Awards for Best Original Score, and Armando Bó and Nicolás Giacobone shared in the Academy Award for Best Original Screenplay for 2014. Also, the Argentine French actress Bérénice Bejo received a nomination for the Academy Award for Best Supporting Actress in 2011 and won the César Award for Best Actress and won the Best Actress award in the Cannes Film Festival for her role in the film "The Past".

Argentina also has won seventeen Goya Awards for Best Spanish Language Foreign Film with "A King and His Movie" (1986), "A Place in the World" (1992), "Gatica, el mono" (1993), "Autumn Sun" (1996), "Ashes of Paradise" (1997), "The Lighthouse" (1998), "Burnt Money" (2000), "The Escape" (2001), "Intimate Stories" (2003), "Blessed by Fire" (2005), "The Hands" (2006), "XXY" (2007), "The Secret in Their Eyes" (2009), "Chinese Take-Away" (2011), "Wild Tales" (2014), "The Clan" (2015) and "The Distinguished Citizen" (2016), being by far the most awarded country in Latin America with twenty-four nominations.

Many other Argentine films have been acclaimed by the international critique: "Camila" (1984), "Man Facing Southeast" (1986), "A Place in the World" (1992), "Pizza, Beer, and Cigarettes" (1997), "Nine Queens" (2000), "A Red Bear" (2002), "The Motorcycle Diaries" (2004), "The Aura" (2005), "Chinese Take-Away" (2011) and "Wild Tales" (2014) being some of them.

Some of the best-known Argentine painters are Cándido López and Florencio Molina Campos (Naïve style); Ernesto de la Cárcova and Eduardo Sívori (Realism); Fernando Fader (Impressionism); Pío Collivadino, Atilio Malinverno and Cesáreo Bernaldo de Quirós (Postimpressionism); Emilio Pettoruti (Cubism); Julio Barragán (Concretism and Cubism) Antonio Berni (Neofigurativism); Roberto Aizenberg and Xul Solar (Surrealism); Gyula Košice (Constructivism); Eduardo Mac Entyre (Generative art); Luis Seoane, "Carlos Torrallardona", "Luis Aquino", and "Alfredo Gramajo Gutiérrez" (Modernism); Lucio Fontana (Spatialism); Tomás Maldonado and Guillermo Kuitca (Abstract art); León Ferrari and Marta Minujín (Conceptual art); and Gustavo Cabral (Fantasy art).

In 1946 Gyula Košice and others created The Madí Movement in Argentina, which then spread to Europe and United States, where it had a significant impact.
Tomás Maldonado was one of the main theorists of the Ulm Model of design education, still highly influential globally.

Other Argentine artists of worldwide fame include Adolfo Bellocq, whose lithographs have been influential since the 1920s, and Benito Quinquela Martín, the quintessential port painter, inspired by the immigrant-bound La Boca neighborhood.

Internationally laureate sculptors Erminio Blotta, Lola Mora and Rogelio Yrurtia authored many of the classical evocative monuments of the Argentine cityscape.

The colonization brought the Spanish Baroque architecture, which can still be appreciated in its simpler "Rioplatense" style in the reduction of San Ignacio Miní, the Cathedral of Córdoba, and the Cabildo of Luján. Italian and French influences increased at the beginning of the 19th century with strong eclectic overtones that gave the local architecture a unique feeling.

Numerous Argentine architects have enriched their own country's cityscape and those around the world: Juan Antonio Buschiazzo helped popularize Beaux-Arts architecture and Francisco Gianotti combined Art Nouveau with Italianate styles, each adding flair to Argentine cities during the early 20th century. Francisco Salamone and Viktor Sulčič left an Art Deco legacy, and Alejandro Bustillo created a prolific body of Neoclassical and Rationalist architecture. Alberto Prebisch and Amancio Williams were highly influenced by Le Corbusier, while Clorindo Testa introduced Brutalist architecture locally. César Pelli's and Patricio Pouchulu's Futurist creations have graced cities worldwide: Pelli's 1980s throwbacks to the Art Deco glory of the 1920s made him one of the world's most prestigious architects, with the Norwest Center and the Petronas Towers among his most celebrated creations.

"Pato" is the national sport, an ancient horseback game locally originated in the early 1600s and predecessor of horseball. The most popular sport is football. Along with Brazil and France, the men's national team is the only one to have won the most important international triplet: World Cup, Confederations Cup, and Olympic Gold Medal. It has also won 14 Copas América, 7 Pan American Gold Medals and many other trophies. Alfredo Di Stéfano, Diego Maradona and Lionel Messi are among the best players in the game's history.

The country's women's field hockey team "Las Leonas", is one of the world's most successful with four Olympic medals, two World Cups, a World League and seven Champions Trophy. Luciana Aymar is recognized as the best female player in the history of the sport, being the only player to have received the FIH Player of the Year Award eight times.

Basketball is a very popular sport. The men's national team is the only one in the FIBA Americas zone that has won the quintuplet crown: World Championship, Olympic Gold Medal, Diamond Ball, Americas Championship, and Pan American Gold Medal. It has also conquered 13 South American Championships, and many other tournaments. Emanuel Ginóbili, Luis Scola, Andrés Nocioni, Fabricio Oberto, Pablo Prigioni, Carlos Delfino and Juan Ignacio Sánchez are a few of the country's most acclaimed players, all of them part of the NBA. Argentina hosted the Basketball World Cup in 1950 and 1990. 

Rugby is another popular sport in Argentina. the men's national team, known as 'Los Pumas' has competed at the Rugby World Cup each time it has been held, achieving their highest ever result in 2007 when they came third. Since 2012 the Los Pumas have competed against Australia, New Zealand & South Africa in The Rugby Championship, the premier international Rugby competition in the Southern Hemisphere. Since 2009 the secondary men's national team known as the 'Jaguares' has competed against the US, Canada, and Uruguay first teams in the Americas Rugby Championship, which Los Jaguares have won six out of eight times it has taken place.

Argentina has produced some of the most formidable champions for Boxing, including Carlos Monzón, the best middleweight in history; Pascual Pérez, one of the most decorated flyweight boxers of all times; Horacio Accavallo, the former WBA and WBC world flyweight champion; Víctor Galíndez, record holder for consecutive world light heavyweight title defenses and Nicolino Locche, nicknamed "The Untouchable" for his masterful defense; they are all inductees into the International Boxing Hall of Fame.

Tennis has been quite popular among people of all ages. Guillermo Vilas is the greatest Latin American player of the Open Era, while Gabriela Sabatini is the most accomplished Argentine female player of all time—having reached #3 in the WTA Ranking, are both inductees into the International Tennis Hall of Fame.

Argentina reigns undisputed in Polo, having won more international championships than any other country and been seldom beaten since the 1930s. The Argentine Polo Championship is the sport's most important international team trophy. The country is home to most of the world's top players, among them Adolfo Cambiaso, the best in Polo history.

Historically, Argentina has had a strong showing within Auto racing. Juan Manuel Fangio was five times Formula One world champion under four different teams, winning 102 of his 184 international races, and is widely ranked as the greatest driver of all time. Other distinguished racers were Oscar Alfredo Gálvez, Juan Gálvez, José Froilán González and Carlos Reutemann.

Besides many of the pasta, sausage and dessert dishes common to continental Europe, Argentines enjoy a wide variety of Indigenous and Criollo creations, including "empanadas" (a small stuffed pastry), "locro" (a mixture of corn, beans, meat, bacon, onion, and gourd), "humita" and "mate".

The country has the highest consumption of red meat in the world, traditionally prepared as "asado", the Argentine barbecue. It is made with various types of meats, often including "chorizo", sweetbread, chitterlings, and blood sausage.

Common desserts include "facturas" (Viennese-style pastry), cakes and pancakes filled with "dulce de leche" (a sort of milk caramel jam), "alfajores" (shortbread cookies sandwiched together with chocolate, "dulce de leche" or a fruit paste), and "tortas fritas" (fried cakes)

Argentine wine, one of the world's finest, is an integral part of the local menu. Malbec, Torrontés, Cabernet Sauvignon, Syrah and Chardonnay are some of the most sought-after varieties.

Some of Argentina's national symbols are defined by law, while others are traditions lacking formal designation.
The Flag of Argentina consists of three horizontal stripes equal in width and colored light blue, white and light blue, with the Sun of May in the center of the middle white stripe. The flag was designed by Manuel Belgrano in 1812; it was adopted as a national symbol on 20 July 1816. The Coat of Arms, which represents the union of the provinces, came into use in 1813 as the seal for official documents.
The Argentine National Anthem was written by Vicente López y Planes with music by Blas Parera, and was adopted in 1813. The National Cockade was first used during the May Revolution of 1810 and was made official two years later. The Virgin of Luján is Argentina's patron saint.

The "hornero", living across most of the national territory, was chosen as the national bird in 1928 after a lower school survey.
The "ceibo" is the national floral emblem and national tree, while the "quebracho colorado" is the national forest tree.
Rhodochrosite is known as the national gemstone.
The national sport is "pato", an equestrian game that was popular among gauchos.

Argentine wine is the national liquor, and "mate", the national infusion.
"Asado" and "locro" are considered the national dishes.







</doc>
<doc id="27259450" url="https://en.wikipedia.org/wiki?curid=27259450" title="Laguna El Juncal">
Laguna El Juncal

Laguna El Juncal () was an important body of water located near the city of Viedma. It covered an approximate area of 60 km long (37 mi) and 4 km wide (2 mi), running parallel to Río Negro with a separation of between 6 and 7 km (3 and 4 mi). Being a supply of animal and vegetable food in the Patagonian plateau, it was highly populated by different communities. The vestiges of those communities are important archaeological sites that have been studied by many researchers since the 19th century.

After a severe flood in 1899 causing havoc in Viedma and surrounding areas, a series of evaluations were carried out to control the course of the river and the overflow of water from the lagoon.

During the 1930s, the lagoon was drained in order to prevent groundwater floods and river floods in the area, to improve the road links between neighboring towns, and to use the land for agricultural production.

For the last 3,000 years, people have settled in the shores of the lagoon. The vestiges of those settlements captured the first naturalists' attention who went on expeditions to the region during the 19th century. The abundant human remains (cemeteries) account for the exploitation of the lagoon by different communities.

After the lagoon was drained, while the land was being prepared for crop production, many human burials were discovered. In 1970, a team led by Rodolfo Casamiquela made plaster casts out of skeletons. A replica of those skeletons was made in order to be exhibited in different local museums of Río Negro Province, meeting the museological standards of that time. One of them was sent to in Cipolletti, and it was later lent to Museo Estación Cultural, in Fernández Oro.


</doc>
<doc id="61353558" url="https://en.wikipedia.org/wiki?curid=61353558" title="Carlos Ameghino Provincial Museum">
Carlos Ameghino Provincial Museum

The Carlos Ameghino Provincial Museum is a natural science museum founded by Professor Roberto Abel in 1971.. It is placed in Cipolletti city, Río Negro Province, Argentina. It is 

The museum hosts collections of recent and fossil species, minerals, as well as objects related to the early history of Cipolletti. Remains of recent species include birds, reptiles and mammals. Fossil remains include mesozoic tetrapods, such as the original skull of "Abelisaurus comahuensis", a meat-eating dinosaur that lived in Patagonia during the Cretaceous. Remains of other taxons of theropods are exhibited, including "Buitreraptor gonzalezorum, Alnashetri cerropoliciensis, Bonapartenykus ultimus" and "Quilmesaurus curriei," as well as ornithopods such as "Willinakaqe salitralensis". Also, there are fossil remains of snakes ("Najash" "rionegrina"), a sphenodontian ("Priosphenodon avelasi"), and a mammal ("Cronopio dentiacutus").

It is placed in Belgrano 2150, Cipolletti (8324), Río Negro Province, Argentine Republic.


</doc>
<doc id="26667" url="https://en.wikipedia.org/wiki?curid=26667" title="Spain">
Spain

Spain ( ), officially the Kingdom of Spain (), is a country in Southwestern Europe with some pockets of Spanish territory across the Strait of Gibraltar and the Atlantic Ocean. Its continental European territory is situated on the Iberian Peninsula. Its territory also includes two archipelagoes: the Canary Islands off the coast of Africa, and the Balearic Islands in the Mediterranean Sea. The African enclaves of Ceuta, Melilla, and Peñón de Vélez de la Gomera, make Spain the only European country to have a physical border with an African country (Morocco). Several small islands in the Alboran Sea are also part of Spanish territory. The country's mainland is bordered to the south and east by the Mediterranean Sea except for a small land boundary with Gibraltar; to the north and northeast by France, Andorra, and the Bay of Biscay; and to the west and northwest by Portugal and the Atlantic Ocean.

With an area of , Spain is the largest country in Southern Europe, the second-largest country in Western Europe, and the European Union, and is the fourth-largest country by area on the European continent. With a population exceeding 46 million, Spain is the sixth-most populous country in Europe, and the fifth-most populous country in the European Union. Spain's capital and largest city is Madrid; other major urban areas include Barcelona, Valencia, Seville, Zaragoza, Málaga, and Bilbao.

Modern humans first arrived in the Iberian Peninsula around 35,000 years ago. Iberian cultures along with ancient Phoenician, Greek, Celtic and Carthaginian settlements developed on the peninsula until it came under Roman rule around 200 BCE, after which the region was named "Hispania", based on the earlier Phoenician name "Sp(a)n" or "Spania". At the end of the Western Roman Empire the Germanic tribal confederations migrated from Central Europe, invaded the Iberian peninsula and established relatively independent realms in its western provinces, including the Suebi, Alans and Vandals. Eventually, the Visigoths would forcibly integrate all remaining independent territories in the peninsula, including the Byzantine province of Spania, into the Visigothic Kingdom, which more or less unified politically, ecclesiastically and legally all the former Roman provinces or successor kingdoms of what was then documented as Hispania.

In the early eighth century the Visigothic Kingdom was conquered by the Umayyad Islamic Caliphate, that arrived to the peninsula in the year 711. The Muslim rule in the Iberian Peninsula (al-Andalus) soon became autonomous from Baghdad. The handful of small Christian pockets in the north left out of Muslim rule, along the presence of the Carolingian Empire near the Pyreneean range, would eventually led to the emergence of the Christian kingdoms of León, Castile, Aragon, Portugal and Navarre. Along seven centuries, an intermittent southwards expansion of the latter kingdoms (metahistorically dubbed as a reconquest: the "Reconquista") took place, culminating with the Christian seizure of the last Muslim polity (the Nasrid Kingdom of Granada) in 1492, the same year Christopher Columbus arrived in the New World. A process of political conglomeration among the Christian kingdoms also ensued, and the late 15th-century saw the dynastic union of Castile and Aragon under the Catholic Monarchs, sometimes considered as the point of emergence of Spain as unified country. The Conquest of Navarre would take in 1512, while the Kingdom of Portugal was also ruled by the Hapsburg Dynasty between 1580 and 1640.

In the early modern period, Spain ruled one of the largest empires in history which was also one of the first global empires, leaving a large cultural and linguistic legacy that includes over 570 million Hispanophones, making Spanish the world's second-most spoken native language, after Mandarin Chinese. During the Golden Age there were also many advancements in the arts, with the rise of renowned painters such as Diego Velázquez. The most famous Spanish literary work, "Don Quixote", was also published during the Golden Age. Spain hosts the world's third-largest number of UNESCO World Heritage Sites.

Spain is a secular parliamentary democracy and a parliamentary monarchy, with King Felipe VI as head of state. It is a major developed country and a high income country, with the world's fourteenth-largest economy by nominal GDP and the sixteenth-largest by PPP. It is a member of the United Nations (UN), the European Union (EU), the Eurozone, the Council of Europe (CoE), the Organization of Ibero-American States (OEI), the Union for the Mediterranean, the North Atlantic Treaty Organization (NATO), the Organisation for Economic Co-operation and Development (OECD), Organization for Security and Co-operation in Europe (OSCE), the Schengen Area, the World Trade Organization (WTO) and many other international organisations. While not an official member, Spain has a "Permanent Invitation" to the G20 summits, participating in every summit, which makes Spain a "de facto" member of the group.

The origins of the Roman name "Hispania", from which the modern name "España" was derived, are uncertain due to inadequate evidence, although it is documented that the Phoenicians and Carthaginians referred to the region as "Spania", therefore the most widely accepted etymology is a Semitic-Phoenician one. Down the centuries there have been a number of accounts and hypotheses:
The Renaissance scholar Antonio de Nebrija proposed that the word "Hispania" evolved from the Iberian word "Hispalis", meaning "city of the western world".

Jesús Luis Cunchillos argues that the root of the term "span" is the Phoenician word "spy", meaning "to forge metals". Therefore, "i-spn-ya" would mean "the land where metals are forged". It may be a derivation of the Phoenician "I-Shpania", meaning "island of rabbits", "land of rabbits" or "edge", a reference to Spain's location at the end of the Mediterranean; Roman coins struck in the region from the reign of Hadrian show a female figure with a rabbit at her feet, and Strabo called it the "land of the rabbits". The word in question (compare modern Hebrew "Shafan") actually means "Hyrax", possibly due to Phoenicians confusing the two animals.

"Hispania" may derive from the poetic use of the term "Hesperia", reflecting the Greek perception of Italy as a "western land" or "land of the setting sun" ("Hesperia", "Ἑσπερία" in Greek) and Spain, being still further west, as "Hesperia ultima".

There is the claim that "Hispania" derives from the Basque word "Ezpanna" meaning "edge" or "border", another reference to the fact that the Iberian Peninsula constitutes the southwest corner of the European continent.

Two 15th-century Spanish Jewish scholars, Don Isaac Abravanel and Solomon ibn Verga, gave an explanation now considered folkloric. Both men wrote in two different published works that the first Jews to reach Spain were brought by ship by Phiros who was confederate with the king of Babylon when he laid siege to Jerusalem. Phiros was a Grecian by birth, but who had been given a kingdom in Spain. Phiros became related by marriage to Espan, the nephew of king Heracles, who also ruled over a kingdom in Spain. Heracles later renounced his throne in preference for his native Greece, leaving his kingdom to his nephew, Espan, from whom the country of "España" (Spain) took its name. Based upon their testimonies, this eponym would have already been in use in Spain by c. 350 BCE.

Iberia enters written records as a land populated largely by the Iberians, Basques and Celts. Early on its coastal areas were settled by Phoenicians who founded Western Europe's most ancient cities Cádiz and Málaga. Phoenician influence expanded as much of the Peninsula was eventually incorporated into the Carthaginian Empire, becoming a major theatre of the Punic Wars against the expanding Roman Empire. After an arduous conquest, the peninsula came fully under Roman rule. During the early Middle Ages it came under Visigothic rule, and then much of it was conquered by Muslim invaders from North Africa. In a process that took centuries, the small Christian kingdoms in the north gradually regained control of the peninsula. The last Muslim state fell in 1492, the same year Columbus reached the Americas. A global empire began which saw Spain become the strongest kingdom in Europe, the leading world power for a century and a half, and the largest overseas empire for three centuries.

Continued wars and other problems eventually led to a diminished status. The Napoleonic conflict in Spain led to chaos, triggering independence movements that tore apart most of the empire and left the country politically unstable. Spain suffered a devastating civil war in the 1930s and then came under the rule of an authoritarian government, which oversaw a period of stagnation that was followed by a surge in the growth of the economy. Eventually democracy was restored in the form of a parliamentary constitutional monarchy. Spain joined the European Union, experiencing a cultural renaissance and steady economic growth until the beginning of the 21st century, that started a new globalised world with economic and ecological challenges.

Archaeological research at Atapuerca indicates the Iberian Peninsula was populated by hominids 1.2 million years ago. In Atapuerca fossils have been found of the earliest known hominins in Europe, the Homo antecessor. Modern humans first arrived in Iberia, from the north on foot, about 35,000 years ago. The best known artefacts of these prehistoric human settlements are the famous paintings in the Altamira cave of Cantabria in northern Iberia, which were created from 35,600 to 13,500 BCE by Cro-Magnon. Archaeological and genetic evidence suggests that the Iberian Peninsula acted as one of several major refugia from which northern Europe was repopulated following the end of the last ice age.

The largest groups inhabiting the Iberian Peninsula before the Roman conquest were the Iberians and the Celts. The Iberians inhabited the Mediterranean side of the peninsula, from the northeast to the southeast. The Celts inhabited much of the inner and Atlantic sides of the peninsula, from the northwest to the southwest. Basques occupied the western area of the Pyrenees mountain range and adjacent areas, the Phoenician-influenced Tartessians culture flourished in the southwest and the Lusitanians and Vettones occupied areas in the central west. A number of cities were founded along the coast by Phoenicians, and trading outposts and colonies were established by Greeks in the East. Eventually, Phoenician-Carthaginians expanded inland towards the meseta; however, due to the bellicose inland tribes, the Carthaginians got settled in the coasts of the Iberian Peninsula.

During the Second Punic War, roughly between 210 and 205 BC the expanding Roman Republic captured Carthaginian trading colonies along the Mediterranean coast. Although it took the Romans nearly two centuries to complete the conquest of the Iberian Peninsula, they retained control of it for over six centuries. Roman rule was bound together by law, language, and the Roman road.

The cultures of the Celtic and Iberian populations were gradually Romanised (Latinised) at different rates depending on what part of Hispania they lived in, with local leaders being admitted into the Roman aristocratic class. Hispania served as a granary for the Roman market, and its harbours exported gold, wool, olive oil, and wine. Agricultural production increased with the introduction of irrigation projects, some of which remain in use. Emperors Hadrian, Trajan, Theodosius I, and the philosopher Seneca were born in Hispania. Christianity was introduced into Hispania in the 1st century AD and it became popular in the cities in the 2nd century AD. Most of Spain's present languages and religion, and the basis of its laws, originate from this period.
The weakening of the Western Roman Empire's jurisdiction in Hispania began in 409, when the Germanic Suebi and Vandals, together with the Sarmatian Alans entered the peninsula at the invitation of a Roman usurper. These tribes had crossed the Rhine in early 407 and ravaged Gaul. The Suebi established a kingdom in what is today modern Galicia and northern Portugal whereas the Vandals established themselves in southern Spain by 420 before crossing over to North Africa in 429 and taking Carthage in 439. As the western empire disintegrated, the social and economic base became greatly simplified: but even in modified form, the successor regimes maintained many of the institutions and laws of the late empire, including Christianity and assimilation to the evolving Roman culture.
The Byzantines established an occidental province, Spania, in the south, with the intention of reviving Roman rule throughout Iberia. Eventually, however, Hispania was reunited under Visigothic rule. These Visigoths, or Western Goths, after sacking Rome under the leadership of Alaric (410), turned towards the Iberian Peninsula, with Athaulf for their leader, and occupied the northeastern portion. Wallia extended his rule over most of the peninsula, keeping the Suebians shut up in Galicia. Theodoric I took part, with the Romans and Franks, in the Battle of the Catalaunian Plains, where Attila was routed. Euric (466), who put an end to the last remnants of Roman power in the peninsula, may be considered the first monarch of Spain, though the Suebians still maintained their independence in Galicia. Euric was also the first king to give written laws to the Visigoths. In the following reigns the Catholic kings of France assumed the role of protectors of the Hispano-Roman Catholics against the Arianism of the Visigoths, and in the wars which ensued Alaric II and Amalaric lost their lives.

Athanagild, having risen against King Agila, called in the Byzantines and, in payment for the succour they gave him, ceded to them the maritime places of the southeast (554). Liuvigild restored the political unity of the peninsula, subduing the Suebians, but the religious divisions of the country, reaching even the royal family, brought on a civil war. St. Hermengild, the king's son, putting himself at the head of the Catholics, was defeated and taken prisoner, and suffered martyrdom for rejecting communion with the Arians. Recared, son of Liuvigild and brother of St. Hermengild, added religious unity to the political unity achieved by his father, accepting the Catholic faith in the Third Council of Toledo (589). The religious unity established by this council was the basis of that fusion of Goths with Hispano-Romans which produced the Spanish nation. Sisebut and Suintila completed the expulsion of the Byzantines from Spain.

Intermarriage between Visigoths and Hispano-Romans was prohibited, though in practice it could not be entirely prevented and was eventually legalised by Liuvigild. The Spanish-Gothic scholars such as Braulio of Zaragoza and Isidore of Seville played an important role in keeping the classical Greek and Roman culture. Isidore was one of the most influential clerics and philosophers in the Middle Ages in Europe, and his theories were also vital to the conversion of the Visigothic Kingdom from an Arian domain to a Catholic one in the Councils of Toledo. Isidore created the first western encyclopedia which had a huge impact during the Middle Ages.

In the 8th century, nearly all of the Iberian Peninsula was conquered (711–718) by largely Moorish Muslim armies from North Africa. These conquests were part of the expansion of the Umayyad Caliphate. Only a small area in the mountainous north-west of the peninsula managed to resist the initial invasion. Legend has it that Count Julian, the governor of Ceuta, in revenge for the violation of his daughter, Florinda, by King Roderic, invited the Muslims and opened to them the gates of the peninsula.

Under Islamic law, Christians and Jews were given the subordinate status of dhimmi. This status permitted Christians and Jews to practice their religions as "People of the Book" but they were required to pay a special tax and had legal and social rights inferior to those of Muslims.

Conversion to Islam proceeded at an increasing pace. The "muladíes" (Muslims of ethnic Iberian origin) are believed to have formed the majority of the population of Al-Andalus by the end of the 10th century.

The Muslim community in the Iberian Peninsula was itself diverse and beset by social tensions. The Berber people of North Africa, who had provided the bulk of the invading armies, clashed with the Arab leadership from the Middle East. Over time, large Moorish populations became established, especially in the Guadalquivir River valley, the coastal plain of Valencia, the Ebro River valley and (towards the end of this period) in the mountainous region of Granada.

Córdoba, the capital of the caliphate since Abd-ar-Rahman III, was the largest, richest and most sophisticated city in western Europe. Mediterranean trade and cultural exchange flourished. Muslims imported a rich intellectual tradition from the Middle East and North Africa. Some important philosophers at the time were Averroes, Ibn Arabi and Maimonides. The Romanised cultures of the Iberian Peninsula interacted with Muslim and Jewish cultures in complex ways, giving the region a distinctive culture. Outside the cities, where the vast majority lived, the land ownership system from Roman times remained largely intact as Muslim leaders rarely dispossessed landowners and the introduction of new crops and techniques led to an expansion of agriculture introducing new produces which originally came from Asia or the former territories of the Roman Empire.

In the 11th century, the Muslim holdings fractured into rival Taifa states (Arab, Berber, and Slav), allowing the small Christian states the opportunity to greatly enlarge their territories. The arrival from North Africa of the Islamic ruling sects of the Almoravids and the Almohads restored unity upon the Muslim holdings, with a stricter, less tolerant application of Islam, and saw a revival in Muslim fortunes. This re-united Islamic state experienced more than a century of successes that partially reversed Christian gains.

The "Reconquista" (Reconquest) was the centuries-long period in which Christian rule was re-established over the Iberian Peninsula. The Reconquista is viewed as beginning with the Battle of Covadonga won by Don Pelayo in 722 and was concurrent with the period of Muslim rule on the Iberian Peninsula. The Christian army's victory over Muslim forces led to the creation of the Christian Kingdom of Asturias along the northwestern coastal mountains. Shortly after, in 739, Muslim forces were driven from Galicia, which was to eventually host one of medieval Europe's holiest sites, Santiago de Compostela and was incorporated into the new Christian kingdom.

The Vikings invaded Galicia in 844, but were heavily defeated by Ramiro I of Asturias at A Coruña. Many of the Vikings' casualties were caused by the Galicians' ballistas – powerful torsion-powered projectile weapons that looked rather like giant crossbows. 70 Viking ships were captured and burned. Vikings raided Galicia in 859, during the reign of Ordoño I of Asturias. Ordoño was at the moment engaged against his constant enemies the Moors; but a count of the province, Don Pedro, attacked the Vikings and defeated them.

The Kingdom of León was the strongest Christian kingdom for centuries. In 1188 the first modern parliamentary session in Europe was held in León (Cortes of León). The Kingdom of Castile, formed from Leonese territory, was its successor as strongest kingdom. The kings and the nobility fought for power and influence in this period. The example of the Roman emperors influenced the political objective of the Crown, while the nobles benefited from feudalism.

Muslim armies had also moved north of the Pyrenees but they were defeated by Frankish forces at the Battle of Poitiers, Frankia and pushed out of the very southernmost region of France along the seacoast by the 760s. Later, Frankish forces established Christian counties on the southern side of the Pyrenees. These areas were to grow into the kingdoms of Navarre and Aragon. For several centuries, the fluctuating frontier between the Muslim and Christian controlled areas of Iberia was along the Ebro and Douro valleys.
The County of Barcelona and the Kingdom of Aragon entered in a dynastic union and gained territory and power in the Mediterranean. In 1229 Majorca was conquered, so was Valencia in 1238. The break-up of Al-Andalus into the competing taifa kingdoms helped the long embattled Iberian Christian kingdoms gain the initiative. The capture of the strategically central city of Toledo in 1085 marked a significant shift in the balance of power in favour of the Christian kingdoms. Following a great Muslim resurgence in the 12th century, the great Moorish strongholds in the south fell to Christian Spain in the 13th century—Córdoba in 1236 and Seville in 1248. 
In the 13th and 14th centuries, the Marinid dynasty of Morocco invaded and established some enclaves on the southern coast but failed in their attempt to re-establish North African rule in Iberia and were soon driven out.
After 800 years of Muslim presence in Spain, the last Nasrid sultanate of Granada, a tributary state would finally surrender in 1492 to the Catholic monarchs Queen Isabella I of Castile and King Ferdinand II of Aragon.

From the mid 13th century, literature and philosophy started to flourish again in the Christian peninsular kingdoms, based on Roman and Gothic traditions. An important philosopher from this time is Ramon Llull. Abraham Cresques was a prominent Jewish cartographer. Roman law and its institutions were the model for the legislators. The king Alfonso X of Castile focused on strengthening this Roman and Gothic past, and also on linking the Iberian Christian kingdoms with the rest of medieval European Christendom. Alfonso worked for being elected emperor of the Holy Roman Empire and published the Siete Partidas code. The Toledo School of Translators is the name that commonly describes the group of scholars who worked together in the city of Toledo during the 12th and 13th centuries, to translate many of the philosophical and scientific works from Classical Arabic, Ancient Greek, and Ancient Hebrew.

The Islamic transmission of the classics is the main Islamic contributions to Medieval Europe. The Castilian language—more commonly known (especially later in history and at present) as "Spanish" after becoming the national language and "lingua franca" of Spain—evolved from Vulgar Latin, as did other Romance languages of Spain like the Catalan, Asturian and Galician languages, as well as other Romance languages in Latin Europe. Basque, the only non-Romance language in Spain, continued evolving from Early Basque to Medieval. The Glosas Emilianenses founded in the monasteries of San Millán de la Cogolla contain the first written words in both Basque and Spanish, having the first become an influence in the formation of the second as an evolution of Latin.
The 13th century also witnessed the Crown of Aragon, centred in Spain's north east, expand its reach across islands in the Mediterranean, to Sicily and Naples. Around this time the universities of Palencia (1212/1263) and Salamanca (1218/1254) were established. The Black Death of 1348 and 1349 devastated Spain.

The Catalans and Aragonese offered themselves to the Byzantine Emperor Andronicus II Palaeologus to fight the Turks. Having conquered these, they turned their arms against the Byzantines, who treacherously slew their leaders; but for this treachery the Spaniards, under Bernard of Rocafort and Berenguer of Entenca, exacted the terrible penalty celebrated in history as "The Catalan Vengeance" and moreover seized the Frankish Duchy of Athens (1311). The royal line of Aragon became extinct with Martin the Humane, and the Compromise of Caspe gave the Crown to the dynasty of Castile, thus preparing the final union.

Anti-Semitic animus accompanied the Reconquista. There were mass killings in Aragon in the mid-14th century, and 12,000 Jews were killed in Toledo. In 1391, Christian mobs went from town to town throughout Castile and Aragon, killing an estimated 50,000 Jews. Women and children were sold as slaves to Muslims, and many synagogues were converted into churches. According to Hasdai Crescas, about 70 Jewish communities were destroyed. St. Vincent Ferrer converted innumerable Jews, among them the Rabbi Josuah Halorqui, who took the name of Jerónimo de Santa Fe and in his town converted many of his former coreligionists in the famous Disputation of Tortosa (1413–14).

In 1469, the crowns of the Christian kingdoms of Castile and Aragon were united by the marriage of Isabella I of Castile and Ferdinand II of Aragon. 1478 commenced the completion of the conquest of the Canary Islands and in 1492, the combined forces of Castile and Aragon captured the Emirate of Granada from its last ruler Muhammad XII, ending the last remnant of a 781-year presence of Islamic rule in Iberia.
That same year, Spain's Jews were ordered to convert to Catholicism or face expulsion from Spanish territories during the Spanish Inquisition. As many as 200,000 Jews were expelled from Spain. This was followed by expulsions in 1493 in Aragonese Sicily and Portugal in 1497. The Treaty of Granada guaranteed religious tolerance towards Muslims, for a few years before Islam was outlawed in 1502 in the Kingdom of Castile and 1527 in the Kingdom of Aragon, leading to Spain's Muslim population becoming nominally Christian Moriscos. A few decades after the Morisco rebellion of Granada known as the War of the Alpujarras, a significant proportion of Spain's formerly-Muslim population was expelled, settling primarily in North Africa. From 1609–14, over 300,000 Moriscos were sent on ships to North Africa and other locations, and, of this figure, around 50,000 died resisting the expulsion, and 60,000 died on the journey.

The year 1492 also marked the arrival of Christopher Columbus in the New World, during a voyage funded by Isabella. Columbus's first voyage crossed the Atlantic and reached the Caribbean Islands, beginning the European exploration and conquest of the Americas, although Columbus remained convinced that he had reached the Orient. Large numbers of indigenous Americans died in battle against the Spaniards during the conquest, while others died from various other causes. Some scholars consider the initial period of the Spanish conquest— from Columbus's first landing in the Bahamas until the middle of the sixteenth century—as marking the most egregious case of genocide in the history of mankind. The death toll may have reached some 70 million indigenous people (out of 80 million) in this period.
The colonisation of the Americas started with "conquistadores" like Hernán Cortés and Francisco Pizarro. Miscegenation was the rule between the native and the Spanish cultures and people. Juan Sebastian Elcano completed the first voyage around the world in human history, the Magellan-Elcano circumnavigation. Florida was colonised by Pedro Menéndez de Avilés when he founded St. Augustine, Florida and then defeated an attempt led by the French Captain Jean Ribault to establish a French foothold in Spanish Florida territory. St. Augustine became a strategic defensive base for Spanish ships full of gold and silver sailing to Spain. Andrés de Urdaneta discovered the tornaviaje or return route from the Philippines to Mexico, making possible the Manila galleon trading route. The Spanish once again encountered Islam, but this time in Southeast Asia and in order to incorporate the Philippines, Spanish expeditions organised from newly Christianised Mexico had invaded the Philippine territories of the Sultanate of Brunei. The Spanish considered the war with the Muslims of Brunei and the Philippines, a repeat of the Reconquista. The Spanish explorer Blas Ruiz intervened in Cambodia's succession and installed Crown Prince Barom Reachea II as puppet.

As Renaissance New Monarchs, Isabella and Ferdinand centralised royal power at the expense of local nobility, and the word "España", whose root is the ancient name "Hispania", began to be commonly used to designate the whole of the two kingdoms.
With their wide-ranging political, legal, religious and military reforms, Spain emerged as the first world power. The death of their son Prince John caused the Crown to pass to Charles I (the Emperor Charles V), son of Juana la Loca.

The unification of the crowns of Aragon and Castile by the marriage of their sovereigns laid the basis for modern Spain and the Spanish Empire, although each kingdom of Spain remained a separate country socially, politically, legally, and in currency and language.
There were two big revolts against the new Habsburg monarch and the more authoritarian and imperial-style crown: Revolt of the Comuneros in Castile and Revolt of the Brotherhoods in Majorca and Valencia. After years of combat, Comuneros Juan López de Padilla, Juan Bravo and Francisco Maldonado were executed and María Pacheco went into exile. Germana de Foix also finished with the revolt in the Mediterranean.

Habsburg Spain was one of the leading world powers throughout the 16th century and most of the 17th century, a position reinforced by trade and wealth from colonial possessions and became the world's leading maritime power. It reached its apogee during the reigns of the first two Spanish Habsburgs—Charles I (1516–1556) and Philip II (1556–1598). This period saw the Italian Wars, the Schmalkaldic War, the Dutch Revolt, the War of the Portuguese Succession, clashes with the Ottomans, intervention in the French Wars of Religion and the Anglo-Spanish War.
Through exploration and conquest or royal marriage alliances and inheritance, the Spanish Empire expanded to include vast areas in the Americas, islands in the Asia-Pacific area, areas of Italy, cities in Northern Africa, as well as parts of what are now France, Germany, Belgium, Luxembourg, and the Netherlands. The first circumnavigation of the world was carried out in 1519–1521. It was the first empire on which it was said that the sun never set. This was an Age of Discovery, with daring explorations by sea and by land, the opening-up of new trade routes across oceans, conquests and the beginnings of European colonialism. Spanish explorers brought back precious metals, spices, luxuries, and previously unknown plants, and played a leading part in transforming the European understanding of the globe. The cultural efflorescence witnessed during this period is now referred to as the Spanish Golden Age. The expansion of the empire caused immense upheaval in the Americas as the collapse of societies and empires and new diseases from Europe devastated American indigenous populations. The rise of humanism, the Counter-Reformation and new geographical discoveries and conquests raised issues that were addressed by the intellectual movement now known as the School of Salamanca, which developed the first modern theories of what are now known as international law and human rights. Juan Luis Vives was another prominent humanist during this period.
Spain's 16th century maritime supremacy was demonstrated by the victory over the Ottomans at Lepanto in 1571, and then after the setback of the Spanish Armada in 1588, in a series of victories against England in the Anglo-Spanish War of 1585–1604. However, during the middle decades of the 17th century Spain's maritime power went into a long decline with mounting defeats against the United Provinces and then England; that by the 1660s it was struggling grimly to defend its overseas possessions from pirates and privateers.

The Protestant Reformation dragged the kingdom ever more deeply into the mire of religiously charged wars. The result was a country forced into ever expanding military efforts across Europe and in the Mediterranean. By the middle decades of a war- and plague-ridden 17th-century Europe, the Spanish Habsburgs had enmeshed the country in continent-wide religious-political conflicts. These conflicts drained it of resources and undermined the economy generally. Spain managed to hold on to most of the scattered Habsburg empire, and help the imperial forces of the Holy Roman Empire reverse a large part of the advances made by Protestant forces, but it was finally forced to recognise the separation of Portugal and the United Provinces, and eventually suffered some serious military reverses to France in the latter stages of the immensely destructive, Europe-wide Thirty Years' War. In the latter half of the 17th century, Spain went into a gradual decline, during which it surrendered several small territories to France and England; however, it maintained and enlarged its vast overseas empire, which remained intact until the beginning of the 19th century.
The decline culminated in a controversy over succession to the throne which consumed the first years of the 18th century. The War of the Spanish Succession was a wide-ranging international conflict combined with a civil war, and was to cost the kingdom its European possessions and its position as one of the leading powers on the Continent.
During this war, a new dynasty originating in France, the Bourbons, was installed. Long united only by the Crown, a true Spanish state was established when the first Bourbon king, Philip V, united the crowns of Castile and Aragon into a single state, abolishing many of the old regional privileges and laws.

The 18th century saw a gradual recovery and an increase in prosperity through much of the empire. The new Bourbon monarchy drew on the French system of modernising the administration and the economy. Enlightenment ideas began to gain ground among some of the kingdom's elite and monarchy. Bourbon reformers created formal disciplined militias across the Atlantic. Spain needed every hand it could take during the seemingly endless wars of the eighteenth century—the Spanish War of Succession or Queen Anne's War (1702–13), the War of Jenkins' Ear (1739–42) which became the War of the Austrian Succession (1740–48), the Seven Years' War (1756–63) and the Anglo-Spanish War (1779–83)—and its new disciplined militias served around the Atlantic as needed.

In 1793, Spain went to war against the revolutionary new French Republic as a member of the first Coalition. The subsequent War of the Pyrenees polarised the country in a reaction against the gallicised elites and following defeat in the field, peace was made with France in 1795 at the Peace of Basel in which Spain lost control over two-thirds of the island of Hispaniola. The Prime Minister, Manuel Godoy, then ensured that Spain allied herself with France in the brief War of the Third Coalition which ended with the British naval victory at the Battle of Trafalgar in 1805. In 1807, a secret treaty between Napoleon and the unpopular prime minister led to a new declaration of war against Britain and Portugal. Napoleon's troops entered the country to invade Portugal but instead occupied Spain's major fortresses. The Spanish king abdicated in favour of Napoleon's brother, Joseph Bonaparte.

Joseph Bonaparte was seen as a puppet monarch and was regarded with scorn by the Spanish. The 2 May 1808 revolt was one of many nationalist uprisings across the country against the Bonapartist regime. These revolts marked the beginning of a devastating war of independence against the Napoleonic regime. The most celebrated battles of this war were those of Bruch, in the highlands of Montserrat, in which the Catalan peasantry routed a French army; Bailén, where Castaños, at the head of the army of Andalusia, defeated Dupont; and the sieges of Zaragoza and Girona, which were worthy of the ancient Spaniards of Saguntum and Numantia.

Napoleon was forced to intervene personally, defeating several Spanish armies and forcing a British army to retreat. However, further military action by Spanish armies, guerrillas and Wellington's British-Portuguese forces, combined with Napoleon's disastrous invasion of Russia, led to the ousting of the French imperial armies from Spain in 1814, and the return of King Ferdinand VII.

During the war, in 1810, a revolutionary body, the Cortes of Cádiz, was assembled to co-ordinate the effort against the Bonapartist regime and to prepare a constitution. It met as one body, and its members represented the entire Spanish empire. In 1812, a constitution for universal representation under a constitutional monarchy was declared, but after the fall of the Bonapartist regime, Ferdinand VII dismissed the Cortes Generales and was determined to rule as an absolute monarch. These events foreshadowed the conflict between conservatives and liberals in the 19th and early 20th centuries.

Spain's conquest by France benefited Latin American anti-colonialists who resented the Imperial Spanish government's policies that favoured Spanish-born citizens (Peninsulars) over those born overseas (Criollos) and demanded retroversion of the sovereignty to the people. Starting in 1809 Spain's American colonies began a series of revolutions and declared independence, leading to the Spanish American wars of independence that ended Spanish control over its mainland colonies in the Americas. King Ferdinand VII's attempt to re-assert control proved futile as he faced opposition not only in the colonies but also in Spain and army revolts followed, led by liberal officers. By the end of 1826, the only American colonies Spain held were Cuba and Puerto Rico.

The Napoleonic War left Spain economically ruined, deeply divided and politically unstable. In the 1830s and 1840s Anti-liberal forces known as Carlists fought against liberals in the Carlist Wars. Liberal forces won, but the conflict between progressive and conservative liberals ended in a weak early constitutional period. After the Glorious Revolution of 1868 and the short-lived First Spanish Republic, a more stable monarchic period began characterised by the practice of "turnismo" (the rotation of government control between progressive and conservative liberals within the Spanish government).

In the late 19th century nationalist movements arose in the Philippines and Cuba. In 1895 and 1896 the Cuban War of Independence and the Philippine Revolution broke out and eventually the United States became involved. The Spanish–American War was fought in the spring of 1898 and resulted in Spain losing the last of its once vast colonial empire outside of North Africa. "El Desastre" (the Disaster), as the war became known in Spain, gave added impetus to the Generation of '98 who were conducting an analysis of the country.

Although the period around the turn of the century was one of increasing prosperity, the 20th century brought little peace; Spain played a minor part in the scramble for Africa, with the colonisation of Western Sahara, Spanish Morocco and Equatorial Guinea. It remained neutral during World War I (see Spain in World War I). The heavy losses suffered during the Rif War in Morocco brought discredit to the government and undermined the monarchy.

After a period of dictatorship during the governments of Generals Miguel Primo de Rivera and Dámaso Berenguer and Admiral Aznar-Cabañas (1923–1931), the first elections since 1923, largely understood as a plebiscite on Monarchy, took place: the 12 April 1931 municipal elections. These gave a resounding victory to the Republican-Socialist candidacies in large cities and provincial capitals, with a majority of monarchist councillors in rural areas. The king left the country and the proclamation of the Republic on 14 April ensued, with the formation of a provisional government.

A constitution for the country was passed in October 1931 following the June 1931 Constituent general election, and a series of cabinets presided by Manuel Azaña supported by republican parties and the PSOE followed. In the election held in 1933 the right triumphed and in 1936, the left. During the Second Republic there was a great political and social upheaval, marked by a sharp radicalisation of the left and the right. The violent acts during this period included the burning of churches, the 1932 failed coup d'état led by José Sanjurjo, the Revolution of 1934 and numerous attacks against rival political leaders. On the other hand, it is also during the Second Republic when important reforms in order to modernise the country were initiated: a democratic constitution, agrarian reform, restructuring of the army, political decentralisation or women's right to vote.

On 17 July and 18, 1936, part of the military carried out a coup d'état that triumphed in only part of the country. The situation led to a civil war, in which the territory was divided into two zones: one under the authority of the Republican government, that counted on outside support from the Soviet Union and Mexico, and the other controlled by the Nationalist rebels, most critically supported by Nazi Germany and Fascist Italy. General Francisco Franco was sworn in as the supreme leader of the rebels in the Autumn of 1936. An uneasy relation between the Republican government and the grassroots anarchists who maintained a partial Social revolution also ensued.

The Spanish Civil War broke out in 1936. For three years the Nationalist forces led by General Francisco Franco and supported by Nazi Germany and Fascist Italy fought the Republican side, which was supported by the Soviet Union, Mexico and International Brigades but it was not supported by the Western powers due to the British-led policy of non-intervention. The civil war was viciously fought and there were many atrocities committed by all sides. The war claimed the lives of over 500,000 people and caused the flight of up to a half-million citizens from the country. In 1939, General Franco emerged victorious and became a dictator.

The state as established under Franco was nominally neutral in the Second World War, although sympathetic to the Axis. The only legal party under Franco's post civil war regime was the Falange Española Tradicionalista y de las JONS (FET y de las JONS), formed in 1937 upon the merging of the Fascist Falange Española de las JONS and the Carlist traditionalists and to which the rest of right-wing groups supporting the rebels also added. The name of "Movimiento Nacional", sometimes understood as a wider structure than the FET y de las JONS proper, largely imposed over the later's name in official documents along the 1950s.

After World War II Spain was politically and economically isolated, and was kept out of the United Nations. This changed in 1955, during the Cold War period, when it became strategically important for the US to establish a military presence on the Iberian Peninsula as a counter to any possible move by the Soviet Union into the Mediterranean basin. In the 1960s, Spain registered an unprecedented rate of economic growth which was propelled by industrialisation, a mass internal migration from rural areas to Madrid, Barcelona and the Basque Country and the creation of a mass tourism industry. Franco's rule was also characterised by authoritarianism, promotion of a unitary national identity, the favouring of a very conservative form of Roman Catholicism known as National Catholicism, and discriminatory language policies.

On 17 January 1966, a fatal collision occurred between a B-52G and a KC-135 Stratotanker over Palomares. The conventional explosives in two of the Mk28-type hydrogen bombs detonated upon impact with the ground, dispersing plutonium over nearby farms.

In 1962, a group of politicians involved in the opposition to Franco's regime inside the country and in exile met in the congress of the European Movement in Munich, where they made a resolution in favour of democracy.

With Franco's death in November 1975, Juan Carlos succeeded to the position of King of Spain and head of state in accordance with the franquist law. With the approval of the new Spanish Constitution of 1978 and the restoration of democracy, the State devolved much authority to the regions and created an internal organisation based on autonomous communities. The Spanish 1977 Amnesty Law let people of Franco's regime continue inside institutions without consequences, even perpetrators of some crimes during transition to democracy like the Massacre of 3 March 1976 in Vitoria or 1977 Massacre of Atocha. The 'founding chairman' of the current leading political party in Spain, the People's Party, was Manuel Fraga who had been a minister in Franco's government and yet continued with his political career until shortly before his death in 2012.

In the Basque Country, moderate Basque nationalism coexisted with a radical nationalist movement led by the armed organisation ETA until the latter's dissolution in May 2018. The group was formed in 1959 during Franco's rule but has continued to wage its violent campaign even after the restoration of democracy and the return of a large measure of regional autonomy.

On 23 February 1981, rebel elements among the security forces seized the Cortes in an attempt to impose a military-backed government. King Juan Carlos took personal command of the military and successfully ordered the coup plotters, via national television, to surrender.

During the 1980s the democratic restoration made possible a growing open society. New cultural movements based on freedom appeared, like La Movida Madrileña and a culture of human rights arose with Gregorio Peces-Barba. On 30 May 1982 Spain joined NATO, followed by a referendum after a strong social opposition. That year the Spanish Socialist Workers Party (PSOE) came to power, the first left-wing government in 43 years. In 1986 Spain joined the European Economic Community, which later became the European Union. The PSOE was replaced in government by the Partido Popular (PP) in 1996 after scandals around participation of the government of Felipe González in the Dirty war against ETA; at that point the PSOE had served almost 14 consecutive years in office.

On 1 January 2002, Spain fully adopted the euro, and Spain experienced strong economic growth, well above the EU average during the early 2000s. However, well-publicised concerns issued by many economic commentators at the height of the boom warned that extraordinary property prices and a high foreign trade deficit were likely to lead to a painful economic collapse.
In 2002 the Prestige oil spill occurred with big ecological consequences along Spain's Atlantic coastline. In 2003 José María Aznar supported US president George W. Bush in the Iraq War, and a strong movement against war rose in Spanish society. On 11 March 2004 a local Islamist terrorist group inspired by Al-Qaeda carried out the largest terrorist attack in Spanish history when they killed 191 people and wounded more than 1,800 others by bombing commuter trains in Madrid. Though initial suspicions focused on the Basque terrorist group ETA, evidence soon emerged indicating Islamist involvement. Because of the proximity of the 2004 election, the issue of responsibility quickly became a political controversy, with the main competing parties PP and PSOE exchanging accusations over the handling of the incident. The elections on 14 March were won by the PSOE, led by José Luis Rodríguez Zapatero.
The proportion of Spain's foreign born population increased rapidly during its economic boom in the early 2000s, but then declined due to the financial crisis. In 2005 the Spanish government legalised same sex marriage. Decentralisation was supported with much resistance of Constitutional Court and conservative opposition, so did gender politics like quotas or the law against gender violence. Government talks with ETA happened, and the group announced its permanent cease of violence in 2010.

The bursting of the Spanish property bubble in 2008 led to the 2008–16 Spanish financial crisis. High levels of unemployment, cuts in government spending and corruption in Royal family and People's Party served as a backdrop to the 2011–12 Spanish protests. Catalan independentism also rose. In 2011, Mariano Rajoy's conservative People's Party won the election with 44.6% of votes. As prime minister, he continued to implement austerity measures required by the EU Stability and Growth Pact. On 19 June 2014, the monarch, Juan Carlos, abdicated in favour of his son, who became Felipe VI.

A Catalan independence referendum was held on 1 October 2017 and then, on 27 October, the Catalan parliament voted to unilaterally declare independence from Spain to form a Catalan Republic on the day the Spanish Senate was discussing approving direct rule over Catalonia as called for by the Spanish Prime Minister. Later that day the Senate granted the power to impose direct rule and Mr Rajoy dissolved the Catalan parliament and called a new election. No country recognised Catalonia as a separate state.

On 1 June 2018 the Congress of Deputies passed a motion of no-confidence against Rajoy and replaced him with the PSOE leader Pedro Sánchez.

At , Spain is the world's fifty-second largest country and Europe's fourth largest country. It is some smaller than France and larger than the US state of California. Mount Teide (Tenerife) is the highest mountain peak in Spain and is the third largest volcano in the world from its base. Spain is a transcontinental country, having territory in both Europe and Africa.

Spain lies between latitudes 27° and 44° N, and longitudes 19° W and 5° E.

On the west, Spain is bordered by Portugal; on the south, it is bordered by Gibraltar (a British overseas territory) and Morocco, through its exclaves in North Africa (Ceuta and Melilla, and the peninsula of Vélez de la Gomera). On the northeast, along the Pyrenees mountain range, it is bordered by France and the Principality of Andorra. Along the Pyrenees in Girona, a small exclave town called Llívia is surrounded by France.

Extending to , the Portugal–Spain border is the longest uninterrupted border within the European Union.

Spain also includes the Balearic Islands in the Mediterranean Sea, the Canary Islands in the Atlantic Ocean and a number of uninhabited islands on the Mediterranean side of the Strait of Gibraltar, known as ("places of sovereignty", or territories under Spanish sovereignty), such as the Chafarinas Islands and Alhucemas. The peninsula of Vélez de la Gomera is also regarded as a "plaza de soberanía". The isle of Alborán, located in the Mediterranean between Spain and North Africa, is also administered by Spain, specifically by the municipality of Almería, Andalusia. The little Pheasant Island in the River Bidasoa is a Spanish-French condominium.

Largest inhabited islands of Spain:

Mainland Spain is a mountainous country, dominated by high plateaus and mountain chains. After the Pyrenees, the main mountain ranges are the Cordillera Cantábrica (Cantabrian Range), Sistema Ibérico (Iberian System), Sistema Central (Central System), Montes de Toledo, Sierra Morena and the Sistema Bético (Baetic System) whose highest peak, the Mulhacén, located in Sierra Nevada, is the highest elevation in the Iberian Peninsula. The highest point in Spain is the Teide, a active volcano in the Canary Islands. The Meseta Central (often translated as "Inner Plateau") is a vast plateau in the heart of peninsular Spain.

There are several major rivers in Spain such as the Tagus ("Tajo"), Ebro, Guadiana, Douro ("Duero"), Guadalquivir, Júcar, Segura, Turia and Minho ("Miño"). Alluvial plains are found along the coast, the largest of which is that of the Guadalquivir in Andalusia.

Three main climatic zones can be separated, according to geographical situation and orographic conditions:

Apart from these main types, other sub-types can be found, like the alpine climate in areas with very high altitude, the humid subtropical climate in areas of northeastern Spain and the continental climates ("Dfc", "Dfb" / "Dsc", "Dsb") in the Pyrenees as well as parts of the Cantabrian Range, the Central System, Sierra Nevada and the Iberian System, and a typical desert climate ("BWk", "BWh") in the zone of Almería, Murcia and eastern Canary Islands. Low-lying areas of the Canary Islands average above during their coldest month, thus having a tropical climate.

The fauna presents a wide diversity that is due in large part to the geographical position of the Iberian peninsula between the Atlantic and the Mediterranean and between Africa and Eurasia, and the great diversity of habitats and biotopes, the result of a considerable variety of climates and well differentiated regions.

The vegetation of Spain is varied due to several factors including the diversity of the relief, the climate and latitude. Spain includes different phytogeographic regions, each with its own floristic characteristics resulting largely from the interaction of climate, topography, soil type and fire, biotic factors.

According to the Democracy Index of the EIU, Spain is one of the 19 full democracies in the world.

The Spanish Constitution of 1978 is the culmination of the Spanish transition to democracy.
The constitutional history of Spain dates back to the constitution of 1812. In June 1976, Spain's new King Juan Carlos dismissed Carlos Arias Navarro and appointed the reformer Adolfo Suárez as Prime Minister. The resulting general election in 1977 convened the "Constituent Cortes" (the Spanish Parliament, in its capacity as a constitutional assembly) for the purpose of drafting and approving the constitution of 1978. After a national referendum on 6 December 1978, 88% of voters approved of the new constitution.

As a result, Spain is now composed of 17 autonomous communities and two autonomous cities with varying degrees of autonomy thanks to its Constitution, which nevertheless explicitly states the indivisible unity of the Spanish nation. The constitution also specifies that Spain has no state religion and that all are free to practice and believe as they wish.

The Spanish administration approved the "Gender Equality Act" in 2007 aimed at furthering equality between genders in Spanish political and economic life. According to Inter-Parliamentary Union data as of Sept 1, 2018, 137 of the 350 members of the Congress were women (39.1%), while in the Senate, there were 101 women out of 266 (39.9%), placing Spain 16th on their list of countries ranked by proportion of women in the lower (or single) House. The Gender Empowerment Measure of Spain in the United Nations Human Development Report is 0.794, 12th in the world.

Spain is a constitutional monarchy, with a hereditary monarch and a bicameral parliament, the "Cortes Generales" (General Courts). The executive branch consists of a Council of Ministers of Spain presided over by the Prime Minister, nominated and appointed by the monarch and confirmed by the Congress of Deputies following legislative elections. By political custom established by King Juan Carlos since the ratification of the 1978 Constitution, the king's nominees have all been from parties who maintain a plurality of seats in the Congress.

The legislative branch is made up of the Congress of Deputies ("Congreso de los Diputados") with 350 members, elected by popular vote on block lists by proportional representation to serve four-year terms, and a Senate ("Senado") with 259 seats of which 208 are directly elected by popular vote, using a limited voting method, and the other 51 appointed by the regional legislatures to also serve four-year terms.

The Prime Minister, deputy prime ministers and the rest of ministers convene at the Council of Ministers.

Spain is organisationally structured as a so-called "Estado de las Autonomías" ("State of Autonomies"); it is one of the most decentralised countries in Europe, along with Switzerland, Germany and Belgium; for example, all autonomous communities have their own elected parliaments, governments, public administrations, budgets, and resources. Health and education systems among others are managed by the Spanish communities, and in addition, the Basque Country and Navarre also manage their own public finances based on foral provisions. In Catalonia, the Basque Country, Navarre and the Canary Islands, a full-fledged autonomous police corps replaces some of the State police functions (see "Mossos d'Esquadra", "Ertzaintza", "Policía Foral/Foruzaingoa" and "Policía Canaria").

The Spanish Constitution of 1978 "protect all Spaniards and all the peoples of Spain in the exercise of human rights, their cultures and traditions, languages and institutions".

According to Amnesty International (AI), government investigations of alleged police abuses are often lengthy and punishments were light. Violence against women was a problem, which the Government took steps to address.

Spain provides one of the highest degrees of liberty in the world for its LGBT community. Among the countries studied by Pew Research Center in 2013, Spain is rated first in acceptance of homosexuality, with 88% of those surveyed saying that homosexuality should be accepted.

The Spanish State is divided into 17 autonomous communities and 2 autonomous cities, both groups being the highest or first-order administrative division in the country. Autonomous communities are divided into provinces, of which there are 50 in total, and in turn, provinces are divided into municipalities. In Catalonia, two additional divisions exist, the "comarques" (sing. "comarca") and the "vegueries" (sing. "vegueria") both of which have administrative powers; "comarques" being aggregations of municipalities, and the "vegueries" being aggregations of "comarques". The concept of a "comarca" exists in all autonomous communities, however, unlike Catalonia, these are merely historical or geographical subdivisions.

Spain's autonomous communities are the first level administrative divisions of the country. They were created after the current constitution came into effect (in 1978) in recognition of the right to self-government of the ""nationalities" and regions of Spain". The autonomous communities were to comprise adjacent provinces with common historical, cultural, and economical traits. This territorial organisation, based on devolution, is literally known in Spain as the "State of Autonomies".

The basic institutional law of each autonomous community is the Statute of Autonomy. The Statutes of Autonomy establish the name of the community according to its historical and contemporary identity, the limits of its territories, the name and organisation of the institutions of government and the rights they enjoy according to the constitution.

The governments of all autonomous communities must be based on a division of powers and comprise

Catalonia, Galicia and the Basque Country, which identified themselves as "nationalities", were granted self-government through a rapid process. Andalusia also took that denomination in its first Statute of Autonomy, even though it followed the longer process stipulated in the constitution for the rest of the country. Progressively, other communities in revisions to their Statutes of Autonomy have also taken that denomination in accordance to their historical and modern identities, such as the Valencian Community, the Canary Islands, the Balearic Islands, and Aragon.

The autonomous communities have wide legislative and executive autonomy, with their own parliaments and regional governments. The distribution of powers may be different for every community, as laid out in their Statutes of Autonomy, since devolution was intended to be asymmetrical. Only two communities—the Basque Country and Navarre—have full fiscal autonomy. Beyond fiscal autonomy, the "nationalities"—Andalusia, the Basque Country, Catalonia, and Galicia—were devolved more powers than the rest of the communities, among them the ability of the regional president to dissolve the parliament and call for elections at any time. In addition, the Basque Country, Catalonia and Navarre have police corps of their own: Ertzaintza, Mossos d'Esquadra and the Policía Foral respectively. Other communities have more limited forces or none at all, like the "Policía Autónoma Andaluza" in Andalusia or the BESCAM in Madrid.

Nonetheless, recent amendments to existing Statutes of Autonomy or the promulgation of new Statutes altogether, have reduced the asymmetry between the powers originally granted to the "nationalities" and the rest of the regions.

Finally, along with the 17 autonomous communities, two autonomous cities are also part of the State of Autonomies and are first-order territorial divisions: Ceuta and Melilla. These are two exclaves located in the northern African coast.

Autonomous communities are divided into provinces, which served as their territorial building blocks. In turn, provinces are divided into municipalities. The existence of both the provinces and the municipalities is guaranteed and protected by the constitution, not necessarily by the Statutes of Autonomy themselves. Municipalities are granted autonomy to manage their internal affairs, and provinces are the territorial divisions designed to carry out the activities of the State.

The current provincial division structure is based—with minor changes—on the 1833 territorial division by Javier de Burgos, and in all, the Spanish territory is divided into 50 provinces. The communities of Asturias, Cantabria, La Rioja, the Balearic Islands, Madrid, Murcia and Navarre are the only communities that comprise a single province, which is coextensive with the community itself. In these cases, the administrative institutions of the province are replaced by the governmental institutions of the community.

After the return of democracy following the death of Franco in 1975, Spain's foreign policy priorities were to break out of the diplomatic isolation of the Franco years and expand diplomatic relations, enter the European Community, and define security relations with the West.

As a member of NATO since 1982, Spain has established itself as a participant in multilateral international security activities. Spain's EU membership represents an important part of its foreign policy. Even on many international issues beyond western Europe, Spain prefers to co-ordinate its efforts with its EU partners through the European political co-operation mechanisms.

Spain has maintained its special relations with Hispanic America and the Philippines. Its policy emphasises the concept of an Ibero-American community, essentially the renewal of the concept of "Hispanidad" or "Hispanismo", as it is often referred to in English, which has sought to link the Iberian Peninsula with Hispanic America through language, commerce, history and culture. It is fundamentally "based on shared values and the recovery of democracy."

Spain claims Gibraltar, a Overseas Territory of the United Kingdom in the southernmost part of the Iberian Peninsula. Then a Spanish town, it was conquered by an Anglo-Dutch force in 1704 during the War of the Spanish Succession on behalf of Archduke Charles, pretender to the Spanish throne.

The legal situation concerning Gibraltar was settled in 1713 by the Treaty of Utrecht, in which Spain ceded the territory in perpetuity to the British Crown stating that, should the British abandon this post, it would be offered to Spain first. Since the 1940s Spain has called for the return of Gibraltar. The overwhelming majority of Gibraltarians strongly oppose this, along with any proposal of shared sovereignty. UN resolutions call on the United Kingdom and Spain, both EU members, to reach an agreement over the status of Gibraltar.

The Spanish claim makes a distinction between the isthmus that connects the Rock to the Spanish mainland on the one hand, and the Rock and city of Gibraltar on the other. While the Rock and city were ceded by the Treaty of Utrecht, Spain asserts that the "occupation of the isthmus is illegal and against the principles of International Law". The United Kingdom relies on "de facto" arguments of possession by prescription in relation to the isthmus, as there has been "continuous possession [of the isthmus] over a long period".

Another claim by Spain is about the Savage Islands, part of Portugal. In clash with the Portuguese position, Spain claims that they are rocks rather than islands, and therefore Spain does not accept any extension of the Portuguese Exclusive Economic Zone (200 nautical miles) generated by the islands, while acknowledging the "Selvagens" having territorial waters (12 nautical miles). On 5 July 2013, Spain sent a letter to the UN expressing these views.

Spain claims the sovereignty over the Perejil Island, a small, uninhabited rocky islet located in the South shore of the Strait of Gibraltar. The island lies just off the coast of Morocco, from Ceuta and from mainland Spain. Its sovereignty is disputed between Spain and Morocco. It was the subject of an armed incident between the two countries in 2002. The incident ended when both countries agreed to return to the status quo ante which existed prior to the Moroccan occupation of the island. The islet is now deserted and without any sign of sovereignty.

Besides the Perejil Island, the Spanish-held territories claimed by other countries are two: Morocco claims the Spanish cities of Ceuta and Melilla and the "plazas de soberanía" islets off the northern coast of Africa. Portugal does not recognise Spain's sovereignty over the territory of Olivenza which was annexed by Spain in 1801 after the War of the Oranges. Portugal stance has been the territory being "de iure" Portuguese territory and "de facto" Spanish.

The armed forces of Spain are known as the Spanish Armed Forces ("Fuerzas Armadas Españolas"). Their Commander-in-chief is the King of Spain, Felipe VI. The next military authorities in line are the Prime Minister and the Minister of Defence. The fourth military authority of the State is the Chief of the Defence Staff (JEMAD). The Defence Staff ("Estado Mayor de la Defensa") assists the JEMAD as auxiliary body.

The Spanish Armed Forces are divided into three branches:

Military conscription was suppressed in 2001.

Since 1996, CO2 emissions have risen notably, not reaching the reduction emissions promised in the Kyoto Protocol for fighting climate change. In the period 1880–2000 more than half of the years have been qualified as dry or very dry. Spain is the country in Europe more exposed to climate change effects, according to Al Gore.

Electricity from renewable sources in Spain represented 42.8% of electricity demand coverage during 2014. The country has a very large wind power capability built up over many years and is one of the world leaders in wind power generation. Spain also positioned itself as a European leader in Solar power, by 2007–2010 the country was second only to Germany in installed capacity.

Vitoria-Gasteiz was awarded with the European Green Capital in 2012 after implementining good practices by the Agenda 21 and recovering Salburua wetland, protected by Ramsar Convention and Natura 2000 and a part of Green Belt of Vitoria-Gasteiz, funded partially with The LIFE Programme.

Spain's capitalist mixed economy is the 14th largest worldwide and the 5th largest in the European Union, as well as the Eurozone's 4th largest.

The centre-right government of former prime minister José María Aznar worked successfully to gain admission to the group of countries launching the euro in 1999. Unemployment stood at 17.1% in June 2017, below Spain's early 1990s unemployment rate of at over 20%. The youth unemployment rate (35% in March 2018) is extremely high compared to EU standards. Perennial weak points of Spain's economy include a large informal economy, and an education system which OECD reports place among the poorest for developed countries, together with the United States and UK.
By the mid-1990s the economy had commenced the growth that had been disrupted by the global recession of the early 1990s. The strong economic growth helped the government to reduce the government debt as a percentage of GDP and Spain's high unemployment rate began to steadily decline. With the government budget in balance and inflation under control Spain was admitted into the Eurozone in 1999.

Since the 1990s some Spanish companies have gained multinational status, often expanding their activities in culturally close Latin America. Spain is the second biggest foreign investor there, after the United States. Spanish companies have also expanded into Asia, especially China and India. This early global expansion is a competitive advantage over its competitors and European neighbours. The reason for this early expansion is the booming interest towards Spanish language and culture in Asia and Africa and a corporate culture that learned to take risks in unstable markets.

Spanish companies invested in fields like renewable energy commercialisation (Iberdrola was the world's largest renewable energy operator), technology companies like Telefónica, Abengoa, Mondragon Corporation (which is the world's largest worker-owned cooperative), Movistar, Hisdesat, Indra, train manufacturers like CAF, Talgo, global corporations such as the textile company Inditex, petroleum companies like Repsol or Cepsa and infrastructure, with six of the ten biggest international construction firms specialising in transport being Spanish, like Ferrovial, Acciona, ACS, OHL and FCC.

In 2005 the Economist Intelligence Unit's quality of life survey placed Spain among the top 10 in the world. In 2013 the same survey (now called the "Where-to-be-born index"), ranked Spain 28th in the world.

In 2010, the Basque city of Bilbao was awarded with the Lee Kuan Yew World City Prize, and its mayor at the time, Iñaki Azkuna, was awarded the World Mayor Prize in 2012. The Basque capital city of Vitoria-Gasteiz received the European Green Capital Award in 2012.

The automotive industry is one of the largest employers in the country. In 2015 Spain was the 8th largest automobile producer country in the world and the 2nd largest car manufacturer in Europe after Germany.

By 2016, the automotive industry was generating 8.7 percent of Spain's gross domestic product, employing about nine percent of the manufacturing industry. By 2008 the automobile industry was the 2nd most exported industry while in 2015 about 80% of the total production was for export.

German companies poured €4.8 billion into Spain in 2015, making the country the second-largest destination for German foreign direct investment behind only the U.S. The lion's share of that investment—€4 billion—went to the country's auto industry.

Crop areas were farmed in two highly diverse manners. Areas relying on non-irrigated cultivation ("secano"), which made up 85% of the entire crop area, depended solely on rainfall as a source of water. They included the humid regions of the north and the northwest, as well as vast arid zones that had not been irrigated. The much more productive regions devoted to irrigated cultivation ("regadío") accounted for 3 million hectares in 1986, and the government hoped that this area would eventually double, as it already had doubled since 1950. Particularly noteworthy was the development in Almería—one of the most arid and desolate provinces of Spain—of winter crops of various fruits and vegetables for export to Europe.

Though only about 17% of Spain's cultivated land was irrigated, it was estimated to be the source of between 40–45% of the gross value of crop production and of 50% of the value of agricultural exports. More than half of the irrigated area was planted in corn, fruit trees, and vegetables. Other agricultural products that benefited from irrigation included grapes, cotton, sugar beets, potatoes, legumes, olive trees, mangos, strawberries, tomatoes, and fodder grasses. Depending on the nature of the crop, it was possible to harvest two successive crops in the same year on about 10% of the country's irrigated land.

Citrus fruits, vegetables, cereal grains, olive oil, and wine—Spain's traditional agricultural products—continued to be important in the 1980s. In 1983 they represented 12%, 12%, 8%, 6%, and 4%, respectively, of the country's agricultural production. Because of the changed diet of an increasingly affluent population, there was a notable increase in the consumption of livestock, poultry, and dairy products. Meat production for domestic consumption became the single most important agricultural activity, accounting for 30% of all farm-related production in 1983. Increased attention to livestock was the reason that Spain became a net importer of grains. Ideal growing conditions, combined with proximity to important north European markets, made citrus fruits Spain's leading export. Fresh vegetables and fruits produced through intensive irrigation farming also became important export commodities, as did sunflower seed oil that was produced to compete with the more expensive olive oils in oversupply throughout the Mediterranean countries of the European Community.

In 2017, Spain was the second most visited country in the world, recording 82 million tourists which marked the fifth consecutive year of record-beating numbers. The headquarters of the World Tourism Organization are located in Madrid.

Spain's geographic location, popular coastlines, diverse landscapes, historical legacy, vibrant culture, and excellent infrastructure has made the country's international tourist industry among the largest in the world. In the last five decades, international tourism in Spain has grown to become the second largest in the world in terms of spending, worth approximately 40 billion Euros or about 5% of GDP in 2006.

Castile and Leon is the Spanish leader in rural tourism linked to its environmental and architectural heritage.

Spain is one of the world's leading countries in the development and production of renewable energy. In 2010 Spain became the solar power world leader when it overtook the United States with a massive power station plant called La Florida, near Alvarado, Badajoz. Spain is also Europe's main producer of wind energy. In 2010 its wind turbines generated 42,976 GWh, which accounted for 16.4% of all electrical energy produced in Spain. On 9 November 2010, wind energy reached an instantaneous historic peak covering 53% of mainland electricity demand and generating an amount of energy that is equivalent to that of 14 nuclear reactors. Other renewable energies used in Spain are hydroelectric, biomass and marine (2 power plants under construction).

Non-renewable energy sources used in Spain are nuclear (8 operative reactors), gas, coal, and oil. Fossil fuels together generated 58% of Spain's electricity in 2009, just below the OECD mean of 61%. Nuclear power generated another 19%, and wind and hydro about 12% each.

The Spanish road system is mainly centralised, with six highways connecting Madrid to the Basque Country, Catalonia, Valencia, West Andalusia, Extremadura and Galicia. Additionally, there are highways along the Atlantic (Ferrol to Vigo), Cantabrian (Oviedo to San Sebastián) and Mediterranean (Girona to Cádiz) coasts. Spain aims to put one million electric cars on the road by 2014 as part of the government's plan to save energy and boost energy efficiency. The former Minister of Industry Miguel Sebastián said that "the electric vehicle is the future and the engine of an industrial revolution."

Spain has the most extensive high-speed rail network in Europe, and the second-most extensive in the world after China. As of 2019, Spain has a total of over of high-speed tracks linking Málaga, Seville, Madrid, Barcelona, Valencia and Valladolid, with the trains operated at commercial speeds up to . On average, the Spanish high-speed train is the fastest one in the world, followed by the Japanese bullet train and the French TGV. Regarding punctuality, it is second in the world (98.5% on-time arrival) after the Japanese Shinkansen (99%). Should the aims of the ambitious AVE programme (Spanish high speed trains) be met, by 2020 Spain will have of high-speed trains linking almost all provincial cities to Madrid in less than three hours and Barcelona within four hours.

There are 47 public airports in Spain. The busiest one is the airport of Madrid (Barajas), with 50 million passengers in 2011, being the world's 15th busiest airport, as well as the European Union's fourth busiest. The airport of Barcelona (El Prat) is also important, with 35 million passengers in 2011, being the world's 31st-busiest airport. Other main airports are located in Majorca (23 million passengers), Málaga (13 million passengers), Las Palmas (Gran Canaria) (11 million passengers), Alicante (10 million passengers) and smaller, with the number of passengers between 4 and 10 million, for example Tenerife (two airports), Valencia, Seville, Bilbao, Ibiza, Lanzarote, Fuerteventura. Also, more than 30 airports with the number of passengers below 4 million.

In the 19th and 20th centuries science in Spain was held back by severe political instability and consequent economic underdevelopment. Despite the conditions, some important scientists and engineers emerged. The most notable were Miguel Servet, Santiago Ramón y Cajal, Narcís Monturiol, Celedonio Calatayud, Juan de la Cierva, Leonardo Torres y Quevedo, Margarita Salas and Severo Ochoa.

The Consejo Superior de Investigaciones Científicas (CSIC) is the leading public agency dedicated to scientific research in the country. It ranked as the 5th top governmental scientific institution worldwide (and 32nd overall) in the 2018 SCImago Institutions Rankings.

Since 2006 the Mobile World Congress has taken place in Barcelona.

In 2019 the population of Spain officially reached 47 million people, as recorded by the "Padrón municipal" (Spain's Municipal Register). Spain's population density, at 91/km² (235/sq mi), is lower than that of most Western European countries and its distribution across the country is very unequal. With the exception of the region surrounding the capital, Madrid, the most populated areas lie around the coast. The population of Spain has risen 2 1/2 times since 1900, when it stood at 18.6 million, principally due to the spectacular demographic boom in the 1960s and early 1970s.
In 2017 the average total fertility rate (TFR) across Spain was 1.33 children born per woman, one of the lowest in the world, below the replacement rate of 2.1, it remains considerably below the high of 5.11 children born per woman in 1865. Spain subsequently has one of the oldest populations in the world, with the average age of 43.1 years.

Native Spaniards make up 88% of the total population of Spain. After the birth rate plunged in the 1980s and Spain's population growth rate dropped, the population again trended upward initially upon the return of many Spaniards who had emigrated to other European countries during the 1970s, and more recently, fuelled by large numbers of immigrants who make up 12% of the population. The immigrants originate mainly in Latin America (39%), North Africa (16%), Eastern Europe (15%), and Sub-Saharan Africa (4%). In 2005, Spain instituted a three-month amnesty programme through which certain hitherto undocumented aliens were granted legal residency.

In 2008, Spain granted citizenship to 84,170 persons, mostly to people from Ecuador, Colombia and Morocco. A sizeable portion of foreign residents in Spain also comes from other Western and Central European countries. These are mostly British, French, German, Dutch, and Norwegian. They reside primarily on the Mediterranean coast and the Balearic islands, where many choose to live their retirement or telecommute.

Substantial populations descended from Spanish colonists and immigrants exist in other parts of the world, most notably in Latin America. Beginning in the late 15th century, large numbers of Iberian colonists settled in what became Latin America and at present most white Latin Americans (who make up about one-third of Latin America's population) are of Spanish or Portuguese origin. Around 240,000 Spaniards emigrated in the 16th century, mostly to Peru and Mexico. Another 450,000 left in the 17th century. The estimate between 1492–1832 is 1.86 million. Between 1846 and 1932 it is estimated that nearly 5 million Spaniards emigrated to the Americas, especially to Argentina and Brazil. Approximately two million Spaniards migrated to other Western European countries between 1960 and 1975. During the same period perhaps 300,000 went to Latin America.


Source: "Áreas urbanas +50", Ministry of Public Works and Transport (2013)

!rowspan="2"| Rank
!rowspan="2"| Metro area
!rowspan="2"| Autonomouscommunity
!colspan="2"| Population
!

The Spanish Constitution of 1978, in its second article, recognises several contemporary entities—"nationalities"— and regions, within the context of the Spanish nation.

Spain has been described as a "de facto" plurinational state. The identity of Spain rather accrues of an overlap of different territorial and ethnolinguistic identities than of a sole Spanish identity. In some cases some of the territorial identities may conflict with the dominant Spanish culture. Distinct traditional identities within Spain include the Basques, Catalans, Galicians, Andalusians and Valencians, although to some extent all of the 17 autonomous communities may claim a distinct local identity.

It is this last feature of "shared identity" between the more local level or autonomous community and the Spanish level which makes the identity question in Spain complex and far from univocal.

Spain has a number of descendants of populations from former colonies, especially Latin America and North Africa. Smaller numbers of immigrants from several Sub-Saharan countries have recently been settling in Spain. There are also sizeable numbers of Asian immigrants, most of whom are of Middle Eastern, South Asian and Chinese origin. The single largest group of immigrants are European; represented by large numbers of Romanians, Britons, Germans, French and others.

The arrival of the "gitanos", a Romani people, began in the 16th century; estimates of the Spanish Roma population range from 750,000 to over one million. There are also the "mercheros" (also "quinquis"), a formerly nomadic minority group. Their origin is unclear.

Historically, Sephardi Jews and Moriscos are the main minority groups originated in Spain and with a contribution to Spanish culture. The Spanish government is offering Spanish nationality to Sephardi Jews.

According to the Spanish government there were 5.7 million foreign residents in Spain in 2011, or 12% of the total population. According to residence permit data for 2011, more than 860,000 were Romanian, about 770,000 were Moroccan, approximately 390,000 were British, and 360,000 were Ecuadorian. Other sizeable foreign communities are Colombian, Bolivian, German, Italian, Bulgarian, and Chinese. There are more than 200,000 migrants from Sub-Saharan Africa living in Spain, principally Senegaleses and Nigerians. Since 2000, Spain has experienced high population growth as a result of immigration flows, despite a birth rate that is only half the replacement level. This sudden and ongoing inflow of immigrants, particularly those arriving illegally by sea, has caused noticeable social tension.

Within the EU, Spain had the 2nd highest immigration rate in percentage terms after Cyprus, but by a great margin, the highest in absolute numbers, up to 2008. The number of immigrants in Spain had grown up from 500,000 people in 1996 to 5.2 million in 2008 out of a total population of 46 million. In 2005 alone, a regularisation programme increased the legal immigrant population by 700,000 people. There are a number of reasons for the high level of immigration, including Spain's cultural ties with Latin America, its geographical position, the porosity of its borders, the large size of its underground economy and the strength of the agricultural and construction sectors, which demand more low cost labour than can be offered by the national workforce.

Another statistically significant factor is the large number of residents of EU origin typically retiring to Spain's Mediterranean coast. In fact, Spain was Europe's largest absorber of migrants from 2002 to 2007, with its immigrant population more than doubling as 2.5 million people arrived. In 2008, prior to the onset of the economic crisis, the "Financial Times" reported that Spain was the most favoured destination for Western Europeans considering a move from their own country and seeking jobs elsewhere in the EU.

In 2008, the government instituted a "Plan of Voluntary Return" which encouraged unemployed immigrants from outside the EU to return to their home countries and receive several incentives, including the right to keep their unemployment benefits and transfer whatever they contributed to the Spanish Social Security. The programme had little effect; during its first two months, just 1,400 immigrants took up the offer. What the programme failed to do, the sharp and prolonged economic crisis has done from 2010 to 2011 in that tens of thousands of immigrants have left the country due to lack of jobs. In 2011 alone, more than half a million people left Spain. For the first time in decades the net migration rate was expected to be negative, and nine out of 10 emigrants were foreigners.

Spain is legally multilingual, and the constitution establishes that the nation will protect "all Spaniards and the peoples of Spain in the exercise of human rights, their cultures and traditions, languages and institutions.

Spanish ("español")— recognised in the constitution as Castilian ("castellano")—is the official language of the entire country, and it is the right and duty of every Spaniard to know the language. The constitution also establishes that "the other Spanish languages"—that is, the other languages of Spain—will also be official in their respective autonomous communities in accordance to their Statutes, their organic regional legislations, and that the "richness of the distinct linguistic modalities of Spain represents a patrimony which will be the object of special respect and protection."

The other official languages of Spain, co-official with Spanish are:

As a percentage of the general population of all Spain, Spanish is natively spoken by 74%, Catalan by 17%, Galician by 7% and Basque by 2% of all Spaniards. Occitan is spoken by less than 5,000 people, only in the small region of "Val d'Aran".
Other Romance minority languages, though not official, have special recognition, such as the Astur-Leonese language ("asturianu", "bable" or "llionés") in Asturias and in northwestern Castile and León, and Aragonese ("aragonés") in Aragon.

In the North African Spanish autonomous city of Melilla, Riff Berber is spoken by a significant part of the population. Similarly, in Ceuta Darija Arabic is spoken by a significant percentage of the population. In the tourist areas of the Mediterranean coast and the islands, English and German are widely spoken by tourists, foreign residents, and tourism workers.

State education in Spain is free and compulsory from the age of six to sixteen. The current education system is regulated by the 2006 educational law, LOE ("Ley Orgánica de Educación"), or Fundamental Law for the Education. In 2014, the LOE was partially modified by the newer and controversial LOMCE law ("Ley Orgánica para la Mejora de la Calidad Educativa"), or Fundamental Law for the Improvement of the Education System, commonly called "Ley Wert" (Wert Law). Since 1970 to 2014, Spain has had seven different educational laws (LGE, LOECE, LODE, LOGSE, LOPEG, LOE and LOMCE).

Institución Libre de Enseñanza was an educational project that developed in Spain for the half a century of about 1876–1936 by Francisco Giner de los Ríos and Gumersindo de Azcárate. The institute was inspired by the philosophy of Krausism. Concepción Arenal in feminism and Santiago Ramón y Cajal in neuroscience were in the movement.

The health care system of Spain (Spanish National Health System) is considered one of the best in the world, in 7th position in the ranking elaborated by the World Health Organization. The health care is public, universal and free for any legal citizen of Spain. The total health spending is 9.4% of the GDP, slightly above the average of 9.3% of the OECD.

Roman Catholicism, which has a long history in Spain, remains the dominant religion. Although it no longer has official status by law, in all public schools in Spain students have to choose either a religion or ethics class. Catholicism is the religion most commonly taught, although the teaching of Islam, Judaism, and evangelical Christianity is also recognised in law. According to a June 2016 study by the Spanish Centre for Sociological Research about 70% of Spaniards self-identify as Catholics, 2% other faith, and about 25% identify with no religion. Most Spaniards do not participate regularly in religious services. This same study shows that of the Spaniards who identify themselves as religious, 59% hardly ever or never go to church, 16% go to church some times a year, 9% some time per month and 15% every Sunday or multiple times per week. Recent polls and surveys have revealed that 20% to 27% of the Spanish population is irreligious.
The Spanish constitution enshrines secularism in governance, as well as freedom of religion or belief for all, saying that no religion should have a "state character," while allowing for the state to "cooperate" with religious groups.

There have been four Spanish Popes. Damasus I, Calixtus III, Alexander VI and Benedict XIII. Spanish mysticism provided an important intellectual resource against Protestantism with Carmelites like Teresa of Ávila, a reformist nun and John of the Cross, a priest, taking the lead in their reform movement. Later, they became Doctors of the Church. The Society of Jesus was co-founded by Ignatius of Loyola, whose Spiritual Exercises and movement led to the establishment of hundreds of colleges and universities in the world, including 28 in the United States alone. The Society's co-founder, Francis Xavier, was a missionary who reached India and later Japan. In the 1960s, Jesuits Pedro Arrupe and Ignacio Ellacuría supported the movement of Liberation Theology.

Protestant churches have about 1,200,000 members. There are about 105,000 Jehovah's Witnesses. The Church of Jesus Christ of Latter-day Saints has approximately 46,000 adherents in 133 congregations in all regions of the country and has a temple in the Moratalaz District of Madrid.

A study made by the Union of Islamic Communities of Spain demonstrated that there were about 1,700,000 inhabitants of Muslim background living in Spain , accounting for 3–4% of the total population of Spain. The vast majority was composed of immigrants and descendants originating from Morocco and other African countries. More than 514,000 (30%) of them had Spanish nationality.

The recent waves of immigration have also led to an increasing number of Hindus, Buddhists, Sikhs and Muslims.
After the Reconquista in 1492, Muslims did not live in Spain for centuries. Late 19th-century colonial expansion in northwestern Africa gave a number of residents in Spanish Morocco and Western Sahara full citizenship. Their ranks have since been bolstered by recent immigration, especially from Morocco and Algeria.

Judaism was practically non-existent in Spain from the 1492 expulsion until the 19th century, when Jews were again permitted to enter the country. Currently there are around 62,000 Jews in Spain, or 0.14% of the total population. Most are arrivals in the past century, while some are descendants of earlier Spanish Jews. Approximately 80,000 Jews are thought to have lived in Spain prior to its expulsion. However the Jewish Encyclopedia states the number over 800,000 to be too large and 235,000 as too small: 165,000 is given as expelled as possibly too small in favour or 200,000, and the numbers of converts after the 1391 pogroms as less. Other sources suggest 200,000 converts mostly after the pogroms of 1391 and upwards of 100,000 expelled. Descendants of these Sephardic Jews expelled in 1492 are given Spanish nationality if they request it.

Spain is a Western country. Almost every aspect of Spanish life is permeated by its Roman heritage, making Spain one of the major Latin countries of Europe. Spanish culture is marked by strong historic ties to Catholicism, which played a pivotal role in the country's formation and subsequent identity. Spanish art, architecture, cuisine, and music have been shaped by successive waves of foreign invaders, as well as by the country's Mediterranean climate and geography. The centuries-long colonial era globalised Spanish language and culture, with Spain also absorbing the cultural and commercial products of its diverse empire.

Spain has 47 World Heritage Sites. These include the landscape of Monte Perdido in the Pyrenees, which is shared with France, the Prehistoric Rock Art Sites of the Côa Valley and Siega Verde, which is shared with Portugal, the Heritage of Mercury, shared with Slovenia and the Ancient and Primeval Beech Forests, shared with other countries of Europe. In addition, Spain has also 14 Intangible cultural heritage, or "Human treasures".

The earliest recorded examples of vernacular Romance-based literature date from the same time and location, the rich mix of Muslim, Jewish, and Christian cultures in Muslim Spain, in which Maimonides, Averroes, and others worked, the Kharjas ("Jarchas").

During the Reconquista, the epic poem "Cantar de Mio Cid" was written about a real man—his battles, conquests, and daily life. The Valencian chivalric romance Tirant lo Blanch written in Valencian is also remarkable.

Other major plays from the medieval times were "Mester de Juglaría", "Mester de Clerecía", "Coplas por la muerte de su padre" or "El Libro de buen amor" (The Book of Good Love).

During the Renaissance the major plays are "La Celestina" and "El Lazarillo de Tormes", while many religious literature was created with poets as Luis de León, San Juan de la Cruz, Santa Teresa de Jesús, etc.

The Baroque is the most important period for Spanish culture. We are in the times of the Spanish Empire. The famous "Don Quijote de La Mancha" by Miguel de Cervantes was written in this time. Other writers from the period are: Francisco de Quevedo, Lope de Vega, Calderón de la Barca or Tirso de Molina.

During the Enlightenment we find names such as Leandro Fernández de Moratín, Benito Jerónimo Feijóo, Gaspar Melchor de Jovellanos or Leandro Fernández de Moratín.

During the Romanticism, José Zorrilla created one of the most emblematic figures in European literature in Don Juan Tenorio. Other writers from this period are Gustavo Adolfo Bécquer, José de Espronceda, Rosalía de Castro or Mariano José de Larra.

In Realism we find names such as Benito Pérez Galdós, Emilia Pardo Bazán, Leopoldo Alas (Clarín), Concepción Arenal, Vicente Blasco Ibáñez and Menéndez Pelayo. Realism offered depictions of contemporary life and society 'as they were'. In the spirit of general "Realism", Realist authors opted for depictions of everyday and banal activities and experiences, instead of romanticised or stylised presentations.

The group that has become known as the Generation of 1898 was marked by the destruction of Spain's fleet in Cuba by US gunboats in 1898, which provoked a cultural crisis in Spain. The "Disaster" of 1898 led established writers to seek practical political, economic, and social solutions in essays grouped under the literary heading of "Regeneracionismo". For a group of younger writers, among them Miguel de Unamuno, Pío Baroja, and José Martínez Ruiz (Azorín), the Disaster and its cultural repercussions inspired a deeper, more radical literary shift that affected both form and content. These writers, along with Ramón del Valle-Inclán, Antonio Machado, Ramiro de Maeztu, and Ángel Ganivet, came to be known as the Generation of '98.

The Generation of 1914 or "Novecentismo". The next supposed "generation" of Spanish writers following those of '98 already calls into question the value of such terminology. By the year 1914—the year of the outbreak of the First World War and of the publication of the first major work of the generation's leading voice, José Ortega y Gasset—a number of slightly younger writers had established their own place within the Spanish cultural field.

Leading voices include the poet Juan Ramón Jiménez, the academics and essayists Ramón Menéndez Pidal, Gregorio Marañón, Manuel Azaña, Maria Zambrano, Eugeni d'Ors, Clara Campoamor and Ortega y Gasset, and the novelists Gabriel Miró, Ramón Pérez de Ayala, and Ramón Gómez de la Serna. While still driven by the national and existential questions that obsessed the writers of '98, they approached these topics with a greater sense of distance and objectivity. Salvador de Madariaga, another prominent intellectual and writer, was one of the founders of the College of Europe and the composer of the constitutive manifest of the Liberal International.

The Generation of 1927, where poets Pedro Salinas, Jorge Guillén, Federico García Lorca, Vicente Aleixandre, Dámaso Alonso. All were scholars of their national literary heritage, again evidence of the impact of the calls of "regeneracionistas" and the Generation of 1898 for Spanish intelligence to turn at least partially inwards.

The two main writers in the second half of the 20th century were the Nobel Prize in Literature laureate Camilo José Cela and Miguel Delibes from Generation of '36. Spain is one of the countries with the most number of laureates with the Nobel Prize in Literature, and with Latin American laureates they made the Spanish language literature one of the most laureates of all. The Spanish writers are: José Echegaray, Jacinto Benavente, Juan Ramón Jiménez, Vicente Aleixandre and Camilo José Cela. The Portuguese writer José Saramago, also awarded with the prize, lived for many years in Spain and spoke both Portuguese and Spanish. Saramago was also well known by his Iberist ideas.

The Generation of '50 are also known as the children of the civil war. Rosa Chacel, Gloria Fuertes, Jaime Gil de Biedma, Juan Goytisolo, Carmen Martín Gaite, Ana María Matute, Juan Marsé, Blas de Otero, Gabriel Celaya, Antonio Gamoneda, Rafael Sánchez Ferlosio or Ignacio Aldecoa.

Premio Planeta de Novela and Miguel de Cervantes Prize are the two main awards nowadays in Spanish literature.

Seneca was a philosopher residing in Spain during the time of the Roman Empire.

During Al-Andalus, Muslim, Jewish and Christian philosophies flourished, also Ibn Arabi, Averroes and Maimonides.

In the Middle Ages Ramon Llull flourished in Spain.

Humanist Luis Vives worked in Spain during the Renaissance, as did Francisco de Vitoria (creator of the School of Salamanca and scholar on international law) and Bartolomé de las Casas.

The Enlightenment in Spain arrived later and was less strong than in other European countries, but during the XIX century liberal ideas arrived in Spanish society. At the end of the century, socialist and libertarian ideas also flourished, with thinkers such as Francisco Pi i Margall, Ricardo Mella and Francisco Ferrer Guardia.

In the first half of the 20th century among the most prominent philosophers were Maria Zambrano and José Ortega y Gasset.

Contemporary philosophers include Fernando Savater and Adela Cortina, creator of the term "aporophobia".

Artists from Spain have been highly influential in the development of various European and American artistic movements. Due to historical, geographical and generational diversity, Spanish art has known a great number of influences. The Mediterranean heritage with Greco-Roman and some Moorish and influences in Spain, especially in Andalusia, is still evident today. European influences include Italy, Germany and France, especially during the Renaissance, Spanish Baroque and Neoclassical periods. There are many other autochthonous styles such as the Pre-Romanesque art and architecture, Herrerian architecture or the Isabelline Gothic.

During the Golden Age painters painters working in Spain included El Greco, José de Ribera, Bartolomé Esteban Murillo and Francisco Zurbarán. Also in the Baroque period Diego Velázquez created some of the most famous Spanish portraits, such as "Las Meninas" and "Las Hilanderas".

Francisco Goya painted during a historical period that includes the Spanish Independence War, the fights between liberals and absolutists, and the rise of contemporary nations-states.

Joaquín Sorolla is a well-known modern impressionist painter and there are many important Spanish painters belonging to the modernism art movement, including Pablo Picasso, Salvador Dalí, Juan Gris and Joan Miró.

The Plateresque style extended from beginnings of the 16th century until the last third of the century and its stylistic influence pervaded the works of all great Spanish artists of the time. Alonso Berruguete (Valladolid School) is called the "Prince of Spanish sculpture". His main works were the upper stalls of the choir of the Cathedral of Toledo, the tomb of Cardinal Tavera in the same Cathedral, and the altarpiece of the Visitation in the church of Santa Úrsula in the same locality. Other notable sculptors were Bartolomé Ordóñez, Diego de Siloé, Juan de Juni and Damián Forment.

There were two Schools of special flair and talent: the Seville School, to which Juan Martínez Montañés belonged, whose most celebrated works are the Crucifix in the Cathedral of Seville, another in Vergara, and a Saint John; and the Granada School, to which Alonso Cano belonged, to whom an Immaculate Conception and a Virgin of Rosary, are attributed.

Other notable Andalusian Baroque sculptors were Pedro de Mena, Pedro Roldán and his daughter Luisa Roldán, Juan de Mesa and Pedro Duque Cornejo. In the 20th century the most important Spanish sculptors were Julio González, Pablo Gargallo, Eduardo Chillida, and Pablo Serrano.

Spanish cinema has achieved major international success including Oscars for recent films such as "Pan's Labyrinth" and "Volver". In the long history of Spanish cinema, the great filmmaker Luis Buñuel was the first to achieve world recognition, followed by Pedro Almodóvar in the 1980s (La Movida Madrileña). Mario Camus and Pilar Miró worked together in Curro Jiménez.

Spanish cinema has also seen international success over the years with films by directors like Segundo de Chomón, Florián Rey, Luis García Berlanga, Carlos Saura, Julio Medem, Isabel Coixet, Alejandro Amenábar, Icíar Bollaín and brothers David Trueba and Fernando Trueba.

Actresses Sara Montiel and Penélope Cruz or actor Antonio Banderas are among those who have become Hollywood stars.

International Film Festivals of Valladolid and San Sebastian are the oldest and more relevant in Spain.

Due to its historical and geographical diversity, Spanish architecture has drawn from a host of influences. An important provincial city founded by the Romans and with an extensive Roman era infrastructure, Córdoba became the cultural capital, including fine Arabic style architecture, during the time of the Islamic Umayyad dynasty. Later Arab style architecture continued to be developed under successive Islamic dynasties, ending with the Nasrid, which built its famed palace complex in Granada.

Simultaneously, the Christian kingdoms gradually emerged and developed their own styles; developing a pre-Romanesque style when for a while isolated from contemporary mainstream European architectural influences during the earlier Middle Ages, they later integrated the Romanesque and Gothic streams. There was then an extraordinary flowering of the Gothic style that resulted in numerous instances being built throughout the entire territory. The Mudéjar style, from the 12th to 17th centuries, was developed by introducing Arab style motifs, patterns and elements into European architecture.

The arrival of Modernism in the academic arena produced much of the architecture of the 20th century. An influential style centred in Barcelona, known as modernisme, produced a number of important architects, of which Gaudí is one. The International style was led by groups like GATEPAC. Spain is currently experiencing a revolution in contemporary architecture and like Rafael Moneo, Santiago Calatrava, Ricardo Bofill as well as many others have gained worldwide renown.

Spanish music is often considered abroad to be synonymous with flamenco, a West Andalusian musical genre, which, contrary to popular belief, is not widespread outside that region. Various regional styles of folk music abound in Aragon, Catalonia, Valencia, Castile, the Basque Country, Galicia, Cantabria and Asturias. Pop, rock, hip hop and heavy metal are also popular.

In the field of classical music, Spain has produced a number of noted composers such as Isaac Albéniz, Manuel de Falla and Enrique Granados and singers and performers such as Plácido Domingo, José Carreras, Montserrat Caballé, Alicia de Larrocha, Alfredo Kraus, Pablo Casals, Ricardo Viñes, José Iturbi, Pablo de Sarasate, Jordi Savall and Teresa Berganza. In Spain there are over forty professional orchestras, including the Orquestra Simfònica de Barcelona, Orquesta Nacional de España and the Orquesta Sinfónica de Madrid. Major opera houses include the Teatro Real, the Gran Teatre del Liceu, Teatro Arriaga and the El Palau de les Arts Reina Sofía.

Thousands of music fans also travel to Spain each year for internationally recognised summer music festivals Sónar which often features the top up and coming pop and techno acts, and Benicàssim which tends to feature alternative rock and dance acts. Both festivals mark Spain as an international music presence and reflect the tastes of young people in the country.

Vitoria-Gasteiz jazz festival is one of the main ones on its genre.

The most popular traditional musical instrument, the guitar, originated in Spain. Typical of the north are the traditional bag pipers or "gaiteros", mainly in Asturias and Galicia.

Cibeles Madrid Fashion Week is one of the most important fashion weeks in Europe.

Zara is one of the biggest prêt-a-porter fashion companies in the world.

Fashion designers such as Cristóbal Balenciaga are among the most influential of the 20th century.

Spanish cuisine consists of a great variety of dishes which stem from differences in geography, culture and climate. It is heavily influenced by seafood available from the waters that surround the country, and reflects the country's deep Mediterranean roots. Spain's extensive history with many cultural influences has led to a unique cuisine. In particular, three main divisions are easily identified:

Mediterranean Spain – all such coastal regions, from Catalonia to Andalusia – heavy use of seafood, such as "pescaíto frito" (fried fish); several cold soups like "gazpacho"; and many rice-based dishes like "paella" from Valencia and "arròs negre" (black rice) from Catalonia.

Inner Spain – Castile – hot, thick soups such as the bread and garlic-based "Castilian soup", along with substantial stews such as "cocido madrileño". Food is traditionally conserved by salting, such as Spanish ham, or immersed in olive oil, such as Manchego cheese.

Atlantic Spain – the whole Northern coast, including Asturian, Basque, Cantabrian and Galician cuisine – vegetable and fish-based stews like "caldo gallego" and "marmitako". Also, the lightly cured "lacón" ham. The best known cuisine of the northern countries often rely on ocean seafood, as in the Basque-style cod, albacore or anchovy or the Galician octopus-based "polbo á feira" and shellfish dishes.

While varieties of football have been played in Spain as far back as Roman times, sport in Spain has been dominated by football since the early 20th century. Real Madrid C.F. and FC Barcelona are two of the most successful football clubs in the world. The country's national football team won the UEFA European Football Championship in 1964, 2008 and 2012 and the FIFA World Cup in 2010, and is the first team ever to win three back-to-back major international tournaments.

Basketball, tennis, cycling, handball, futsal, motorcycling and, lately, Formula One also can boast of Spanish champions. Today, Spain is a major world sports powerhouse, especially since the 1992 Summer Olympics that were hosted in Barcelona, which stimulated a great deal of interest in sports in the country. The tourism industry has led to an improvement in sports infrastructure, especially for water sports, golf and skiing. In their respective regions, the traditional games of Basque pelota and Valencian pilota both are popular.

Public holidays celebrated in Spain include a mix of religious (Roman Catholic), national and regional observances. Each municipality is allowed to declare a maximum of 14 public holidays per year; up to nine of these are chosen by the national government and at least two are chosen locally. Spain's National Day ("Fiesta Nacional de España") is 12 October, the anniversary of the Discovery of America and commemorate Our Lady of the Pillar feast, patroness of Aragon and throughout Spain.

There are many festivals and festivities in Spain. Some of them are known worldwide, and every year millions of people from all over the world go to Spain to experience one of these festivals. One of the most famous is San Fermín, in Pamplona. While its most famous event is the "encierro", or the running of the bulls, which happens at 8:00 am from 7 to 14 July, the week-long celebration involves many other traditional and folkloric events. Its events were central to the plot of The Sun Also Rises, by Ernest Hemingway, which brought it to the general attention of English-speaking people. As a result, it has become one of the most internationally renowned fiestas in Spain, with over 1,000,000 people attending every year.

Other festivals include: La Tomatina tomato festival in Buñol, Valencia, the carnivals in the Canary Islands, the Falles in Valencia or the Holy Week in Andalusia and Castile and León.








</doc>
<doc id="59889" url="https://en.wikipedia.org/wiki?curid=59889" title="Royal Spanish Academy">
Royal Spanish Academy

The Royal Spanish Academy (Spanish: , generally abbreviated as RAE) is Spain's official royal institution with a mission to ensure the stability of the Spanish language. It is based in Madrid, Spain, but is affiliated with national language academies in 22 other hispanophone nations through the Association of Academies of the Spanish Language. The RAE's emblem is a fiery crucible, and its motto is ("Cleans, sets, and gives splendor").

The RAE dedicates itself to language planning by applying linguistic prescription aimed at promoting linguistic unity within and between various territories, to ensure a common standard. The proposed language guidelines are shown in a number of works.

The Royal Spanish Academy was founded in 1713, modeled after the Accademia della Crusca (1582), of Italy, and the Académie Française (1635), of France, with the purpose "to fix the voices and vocabularies of the Spanish language with propriety, elegance, and purity". King Philip V approved its constitution on 3 October 1714, placing it under the Crown's protection.

Its aristocratic founder, , Duke of Escalona and Marquess of Villena, described its aims as "to assure that Spanish speakers will always be able to read Cervantes" – by exercising a progressive up-to-date maintenance of the formal language.

The RAE began establishing rules for the orthography of Spanish beginning in 1741 with the first edition of the (spelled from the second edition onwards). The proposals of the Academy became the official norm in Spain by royal decree in 1844, and they were also gradually adopted by the Spanish-speaking countries in the Americas.
Several reforms were introduced in the (1959, New Norms of Prosody and Orthography). Since the establishment of the Association of Academies of the Spanish Language in 1951, the Spanish academy works in close consultation with the other Spanish language academies in its various works and projects. The 1999 Orthography was the first to be edited by the twenty two academies together. The current rules and practical recommendations on spelling are presented in the latest edition of the (2010).

The headquarters, opened in 1894, is located at Calle Felipe IV, 4, in the ward of Jerónimos, next to the Museo del Prado. The Center for the Studies of the Royal Spanish Academy, opened in 2007, is located at Calle Serrano 187–189.

According to Salvador Gutiérrez, an academic numerary of the institution, the Academy does not dictate the rules but studies the language, collects information and presents it. The rules of the language are simply the continued use of expressions, some of which are collected by the Academy. Although he also says that it is important to read and write correctly. 
Article 1 of the statutes of the Royal Spanish Academy, translated from Spanish, says the following:

Members of the Academy are known as (), chosen from among prestigious people within the arts and sciences, including several Spanish-language authors, known as The Immortals (Spanish: ), similarly to their French Academy counterparts. The numeraries (Spanish: "Números)" are elected for life by the other academicians. Each academician holds a seat labeled with a letter from the Spanish alphabet, although upper and lower case letters denote separate seats.

The Academy has included Latin American members from the time of Rafael María Baralt, although some Spanish-speaking countries have their own academies of the language.




</doc>
<doc id="6617003" url="https://en.wikipedia.org/wiki?curid=6617003" title="Carrizosa">
Carrizosa

Carrizosa is a village in Spain.

Iron age funerary vessels were found with painted horizontal stripes. Following the Battle of Alarcos, Muslims reconquered Castile again .
Year 1212. - The victory of Las Navas de Tolosa, leads the Order of Santiago to take over the Campo de Montiel, ending conquest in 1229 to seize the castle of Montiel.
Year 1215. - King Henry I of Castile, gives Carrizosa, among other places, the Count Don Alvaro Nunez de Lara, so that they are repopulated, specifying the assignment in another document dated 1217.
Year 1243. - The Order of Santiago and Alcaraz council clash over ownership of the Campo de Montiel castles, villages and wilds, Carrizosa amongst them.
Year 1387. - Is granted Carrizosa entrusted to Pedro Diaz de Monsalve, first known commander, who kept it until the year 1409. They follow a series of commanders until the parcel is the youngest son of Don Rodrigo Manrique, Enrique Manrique. The population at the time, would be about 90 habitanes . In these years, the castle is definitely abandoned and the population is settled in its current location, on the banks of the river Carrizosa (currently Canyamares ) .
Year 1493. - The finished building the parish church of Santa Catalina, which consisted of a nave of four arches of stone and covered with white pine and reed . On the west side was attached the cemetery .
Year 1515. - The population of Carrizosa, due to poor harvests and epidemics, has fallen to 45 inhabitants. In this year, the painter of Fuenllana, Hernando de Miranda, finished a beautiful altarpiece for the high altar of the parish. In November, visitors come to the Order of Santiago, making relationship of objects held by the church. Another visiting shrines and had Carrizosa is the Virgin of the Holm oak . These views are repeated in subsequent years, until the last visit of which no record, in 1554 .
Year 1575. - Although Carrizosa remained Alhambra village had its own council and directs, he owned four homes in Villanueva de los Infantes. The year 1575 there are two maps made on the occasion of the Topographic Relations of Philip II .
Year 1590. - Carrizosa declares independence as Alhambra village after paying an amount of 578,000 maravedis . In the following year reached a population of 75 inhabitants.
In the following years of the seventeenth century, the commanders of the parcel leased land to people who were in charge of collecting tithes to the neighbors. There are several relationships heritage of the charge made in different years, that account for the possessions they had in the same town or area.
Year 1787. - In the Census of Floridablanca, consists Carrizosa as villa, with ordinary mayor belonging to the military order of Santiago . Its inhabitants were 349, of which 263 were listed as minor and profession. Highlights 38 laborers, 26 farmers and 14 servants.
Year 1811. - During the invasion of Napoleon, the French project a water transfer from the Azuer Ruidera gaps for White Houses, benefiting the mills and crops in the area . The situation in the country and the subsequent defeat of the French troops prevented its implementation.
Year 1900 . - The census of that year gives a population of 1507 inhabitants.
Year 1920 . - Appears smallpox epidemic, which for three years, producing high mortality .
Year 1925 . - Around this year, mounting a power plant in the old mill dam .
Year 1928 . - The new church is completed after being exhumed cemetery and demolish the old annex which had become too small .
Year 1931 . - For a political conflict, a group of women destroys the power station and the town remains without power until the following year, in which it brings Ruidera .
Year 1932 . - Present cemetery is constructed and closed the set in the place where today are the schools.
Year 1960 . - Carrizosa largest census reached its history : 3026 inhabitants
Year 2012. - Óscar Parra de Carrizosa Hold wheel in the town, which will be the last film of actress Sara Montiel. The film features Javier Gurruchaga, Maria Garralón, Arevalo and Beatriz Rico, among others.
The screenwriter and film director from Madrid, carrizoseño origin, Oscar Parra de Carrizosa, published in 1992 a book with the history of the town, ranging from the year 720 AD our days . Available in the National Library of Madrid.

San Anton Carrizosa exist and existed in many traditions associated with the celebration of the feast of this saint on January 17 . Formerly, until about 60 years, men on horseback roamed the streets of the town, competing to collect cakes for San Anton being thrown from balconies and windows. The cake had a similar development of San Blas rolls that comment later.
Another tradition associated with the saint was the fifth . Named for the old system of military service by redeeming one in five, fifth . Posibiblemente also related to some ancient form of protest, tore the curtains fifths of the doors of the houses in the nights leading up to January 17 . The night of 16 January 17 fires were lit in the streets but most of it was the main square of wood which was collected by the fifth . Around the fires ate, drank and danced dances to the sound of the old songs also related to this event .
Another even older tradition and unused for more than half a century was to release a pig (animal traditionally linked to the saint ) by the town's streets fed by all the neighbors. Hence there Carrizosa expression " be like the guttural of San Antón " in reference to street life of it.
Rolls of San Blas and St. Agatha . It is celebrated since ancient saints days 3 and 5 February . Loaves consist of small irregularly shaped oval tending to be made with flour, water, oil, salt and aniseed ( aniseed ) without yeast so the result is a crispy bread . The tradition that once marked should be baked the day before Mass to be blessed saints . It is usually done by religious promise or request for favor agredecimiento the saints ( related to diseases of the throat or voice in the case of San Blas or illnesses related to the chest of St. Agatha ) . Families that are written by the children shared that early morning, tour the houses where the deals.
Tostones . Like the previous case, is a culinary tradition . The croutons are a type of nougat made family so the night of 31 October. Is performed based on molten sugar mixed with almonds and walnuts . Peanuts may also be added . Mixed caramelized sugar and nuts expands on a flat surface and the aid of a mallet or mass expands bottle until a uniform cake . It's called " make croutons " to the design of the meeting fresh and festive family or friends on the occasion of the night of October 31
Feast Festival Matins of family and friends celebrated the morning of December 25 . Traditionally it was made after midnight mass after the breakdown of the prohibition of eating meat that existed on 24 December.

It has some impressive frescoes on the ceiling and altar which depicts the ascension to heaven of Santa Catalina . On the roof, include representations of the four evangelists accompanied by their symbolic animals as the visions of the prophet Ezekiel. Also of extraordinary quality the rest of the frescoes depicting the Last Supper or Jesus as a shepherd. They were painted by Jesus Velasco in 1942.
Source of Mina. Power and water supply pylon old public and animal watering . Its construction and date are unknown. Besides source and trough, was also used to irrigate the gardens antigüemente located below it in places now occupied by the slaughterhouse and other private estates . The monument has undergone several restoration and disastrous attempts that caused the loss of the original stones and the original physiognomy cement and stones using different type. Its name, La Mina, can come cimbrático system that collects water and carries it to the source since there is a well known formwork system when used to collect the waters that flow to the foot of a mountain, as is the case .
House of the commanders in the Plaza Mayor.
Route of the "Via Crucis " . 14 crosses dotted the streets and maintained and decorated by the neighbors . Perfect trip to tour the oldest streets of the village and enjoy the views from the higher streets .

Located at the foot of the hill Castellon where the ruins of the Moorish castle of Carrizosa Peñaflor . It is situated on the banks of the estate of Jaraba. In architecture, rebuilt after the Civil War in 1968, as shown in the map of the Party of Campo de Montiel, 1765.
Shrine of Our Lady of the Holm oak . Located just 1.5 miles from Carrizosa was one of the ancient hermitages Carrizosa had Guestbooks according to the visitors of the Order of Santiago . Currently, the cult belongs to the neighboring town of Villahermosa although over the centuries has remained a huge devotion to this ancient Carrizosa black virgin .
Archaeological remains of the Moorish castle of Peñaflor ( next to the Shrine Virgen del Salido ) .
House work and the Fuenlabrada farm, formerly owned Conde de Leyva and later by her four daughters " the ladies of Fuenlabrada.
Palace of the Dukes of San Fernando de Sevilla

Next is the old farm and mill work Huelma
Stream of the Toriles . Waterfall visible only in rainy years . Primitive landscape with caves, boulders and waterfalls and fountains that appear everywhere.
Alameda Azuer River (river Salido )
Ruins of the House of Oydor and environments. Ruins of old farmhouse which appeared on maps of the Party of Campo de Montiel, 1765 and later. It is located on the old road to La Solana before the intercession Alhambra Road .
Roads and places of the estate of Fuenlabrada.

San Anton : January 17 . It is traditional bonfires in the streets of the town
The May : April 30, in honor of the Virgen del Salido.
May Crosses : 2,3 and 4 May .
San Isidro : May 15 .
Celebrations in Honor of Our Lady of Salido : They are celebrated between 13 and 16 August .
Pilgrimage of the Virgin in the Chapel of the Departed : the last weekend of August.
Hearts of Jesus
The traditional festivals most popular attachment Carrizosa the Cross of May party in honor of the Virgen del Salido.

Cross the May [ edit · edit font ]

The feast of you the Cross of May is held between April 30, 2, 3 and 4 May . During the evening a young Rondalla march through the village singing the Ladies Mays . On day 2 he sings to the Crosses ( altars placed in the houses ) and are held treats. Its origin is pagan feast of excitement as spring and has variations across the Spanish geography . In our region have been held in many localities, but where they remain with all their traditional Carrizosa is .
At Christianized, these parties became an exaltation of the Cross as a symbol of the Passion of Jesus. Basically they consist, in its most traditional, in the ornamentation of a cross -based floral and plant elements ( reminiscent of its ancient meaning ), along with symbols of the Passion ( the nails, the crown of thorns, etc.).
May this celebration really begins on the evening of April 30 and continues through the early morning of May 1, during which, gangs of young men ( and now also wenches ) dedicated their songs to the beloved women ( wives, girlfriends, family ) in what is called " ladies night " . It starts with a traditional song called " in May ", in which, through its various stanzas ( the first of which is to apply for leave of the lady), they will " draw" the perfections of a woman's body :
Noted for their interest altars of Corpus Christi, whose feast is celebrated in June.
Between 13 and 16 August, celebrate the festive in honor of


</doc>
<doc id="22550775" url="https://en.wikipedia.org/wiki?curid=22550775" title="Hispanist">
Hispanist

A Hispanist is a scholar specializing in Hispanic studies, that is Spanish language, literature, linguistics, history, or civilization by foreigners (i.e., non-Spaniards). It was used in an article by Miguel de Unamuno in 1908 referring to 'el hispanista italiano Farinelli', and was discussed at length for the U.S. by Hispanist Richard L. Kagan of Johns Hopkins University.

The work carried out by Hispanists includes translations of literature and they may specialize in certain genres, authors or historical periods of the Iberian Peninsula and Hispanic America.

Publications dealing specifically with Hispanic studies include the "Hispania" quarterly published by the American Association of Teachers of Spanish and Portuguese (AATSP). Richard L. Kagan has edited a volume on Hispanism in the United States and Hispanist historian J.H. Elliot has discussed it in his volume "History in the Making". 


The Spanish-language portal run by the Instituto Cervantes lists over 60 associations of Hispanists around the world, including the following:



</doc>
<doc id="59534773" url="https://en.wikipedia.org/wiki?curid=59534773" title="2019 in Spain">
2019 in Spain

Events of 2019 in Spain.












</doc>
<doc id="16278860" url="https://en.wikipedia.org/wiki?curid=16278860" title="Outline of Italy">
Outline of Italy

The following outline is provided as an overview of and topical guide to Italy:

Italy is a unitary parliamentary republic in South-Central Europe, located primarily upon the Italian Peninsula. It is where Ancient Rome originated as a small agricultural community about the 8th century BC, which spread over the course of centuries into the colossal Roman empire, encompassing the whole Mediterranean Sea and merging the Ancient Greek and Roman cultures into one civilization. This civilization was so influential that parts of it survive in modern law, administration, philosophy and arts, providing the groundwork that the Western world is based upon.


Geography of Italy



Regions of Italy


NUTS of Italy




Comune


Demographics of Italy

Italy shares its north border with:

Independent states surrounded by Italy (otherwise within Italy's borders) include:

Politics of Italy

Government of Italy





Foreign relations of Italy

The Italian Republic is a member of:


Military of Italy

History of Italy

Timeline of Italian history



 

Culture of Italy




Religion in Italy

Sport in Italy

Football in Italy


Economy of Italy



Education in Italy







</doc>
<doc id="58096734" url="https://en.wikipedia.org/wiki?curid=58096734" title="Istrian Demarcation">
Istrian Demarcation

The Istrian Demarcation () is a legal document on the demarcation of territories between neighbouring municipalities in Istria, currently shared by Croatia, Slovenia, and Italy. More precisely, between the possessions of the Patriarchate of Aquileia, the Duke of Gorica and Pazin and the representatives of the Venetian Republic. In addition to the borders that were being agreed upon only in disputed parts of Istria, the Istrian Demarcation also established the payment of fees for the use of forests, vineyards and pastures.

The document was probably the result of separates documents being combined into the Istrian Demarcation in 1325. A commission to map and record the borders was established. The scribes of the commission were three: one to write the data in Latin, one to write it in German and one to write the data in Croatian. The data used by the commission were old documents of individual municipalities or labels found on the trees and rocks that were used for demarcation purposes. In addition, witnesses were questions on memory of the borders under oath.

There are no 14th century originals but two copies are preserved, both in Croatian (one found in Momjan, the other in Kršan). The original text in Croatian was written in the Glagolitic script by a priest called Mikul from Gologorica. The copy from Kršan was written in 1502, and is kept at the University Library in Zagreb. It was written by Levac Križanić. The copy from Momjan was also supposedly written by Kižanić.

The Istrian Demarcation vaguely shows us the social organization, economic conditions and life of the rural communities in medieval Istria. The document was often claimed as unauthentic, mostly because of political reasons. The modern edition was first published by Ante Starčević in 1852.

The language of the document is not very different from other Croatian medieval legal texts, that is, the non-liturgical texts written in the Chakavian literary language with a low degree of Church Slavonic influence. However, even the younger copies retain some older features of Croatian (e.g. the dual form of some nouns) and minor traceas of Church Slavonic (e.g. the use of absolute construction of the dative case). The Demarcation is also famous for referring to the Croatian language as "‘hrvacki’", which is an unusual term for the language: "I ondi gospodin Menart sluga naprid sta i pokaza listi prave v keh se udržahu zapisani razvodi i kunfini meju Sovińakom z Vrhom i Plzetom, ki bihu pisani na let Božjih 1195, ke listi ondi pred nas trih nodari postaviše, keh ta gospoda izibra: jednoga latinskoga, a drugoga nimškoga, a tretoga hrvackoga, da imamo vsaki na svoj orijinal pisat, poimeno od mesta do mesta, kako se niže udrži, po vsoj deželi. I tako mi niže imenovani nodari preda vsu tu gospodu pročtesmo kako se v ńih udrži. I tako onde obe strane se sjediniše i kuntentaše i kordaše i razvodi svojimi zlameniji postaviše, i jednoj i drugoj strani pisaše listi jazikom latinskim i hrvackim, a gospoda sebe shraniše jazikom nemškim."Historian Dražen Vlahov says ‘The Croatian Glagolitic document of the demarcation of the holdings of certain Istrian municipalities, i.e. of their feudal lords (Duke Albrecht of Pazin and Gorica, Patriarch Raimund of Aquileia and Venetian authorities in Istria), also known as the Istrian Demarcation, originally dated 5 May 1325, and written, as stated in the text, in three languages (Latin, German and Croatian) in three originals, written by three selected scribes: a priest called Mikula for Duke Albrecht who wrote in the Croatian language in the Glagolitic script, Mr Pernar from Gorica who wrote in German and Mr Ivan from Krmina who wrote it in Latin. All three specimens are considered original. None of the original documents were preserved but two copies were preserved in cursive Glagolitic in Croatian, three copies in Latin and several copies in Italian. According to experts, the Latin and Italian copies were made on the basis of the translation of the Croatian-Glagolitic specimens.’

Vlahov denies the claims of Milko Kos and Josip Bratulić that the Croatian Glagolitic copy of the Istrian Demarcation, found in Momjan in Istria in 1880, was written by an unknown author on the basis of an unpreserved copy of the notary Levac Križanić. The analysis the manuscripts of both Glagolitic copies of the Istrian Demarcation (one from Momjan and one from Kršan) written in Glagolitic cursive script concludes that both Croatian copies of the document in question were written by the same person, i.e. Levac Križanić, and that the unanalysed copy from Momjan originated before the Kršan copy, probably in 1530s.




</doc>
<doc id="14532" url="https://en.wikipedia.org/wiki?curid=14532" title="Italy">
Italy

Italy ( ), officially the Italian Republic ( ), is a European country consisting of a peninsula delimited by the Alps and surrounded by several islands. Italy is located in south-central Europe, and it is also considered a part of western Europe. A unitary parliamentary republic with its capital in Rome, the country covers a total area of and shares land borders with France, Switzerland, Austria, Slovenia, and the enclaved microstates of Vatican City and San Marino. Italy has a territorial exclave in Switzerland (Campione) and a maritime exclave in Tunisian waters (Lampedusa). With around 60 million inhabitants, Italy is the third-most populous member state of the European Union.

Due to its central geographic location in Southern Europe and the Mediterranean, Italy has historically been home to myriad peoples and cultures. In addition to the various ancient peoples dispersed throughout what is now modern-day Italy, the most predominant being the Indo-European Italic peoples who gave the peninsula its name, beginning from the classical era, Phoenicians and Carthaginians founded colonies mostly in insular Italy, Greeks established settlements in the so-called "Magna Graecia" of Southern Italy, while Etruscans and Celts inhabited central and northern Italy respectively. An Italic tribe known as the Latins formed the Roman Kingdom in the 8th century BC, which eventually became a republic with a government of the Senate and the People. The Roman Republic initially conquered and assimilated its neighbours on the Italian peninsula, eventually expanding and conquering parts of Europe, North Africa and Asia. By the first century BC, the Roman Empire emerged as the dominant power in the Mediterranean Basin and became a leading cultural, political and religious centre, inaugurating the Pax Romana, a period of more than 200 years during which Italy's law, technology, economy, art, and literature developed. Italy remained the homeland of the Romans and the metropole of the empire, whose legacy can also be observed in the global distribution of culture, governments, Christianity and the Latin script.

During the Early Middle Ages, Italy endured the fall of the Western Roman Empire and barbarian invasions, but by the 11th century numerous rival city-states and maritime republics, mainly in the northern and central regions of Italy, rose to great prosperity through trade, commerce and banking, laying the groundwork for modern capitalism. These mostly independent statelets served as Europe's main trading hubs with Asia and the Near East, often enjoying a greater degree of democracy than the larger feudal monarchies that were consolidating throughout Europe; however, part of central Italy was under the control of the theocratic Papal States, while Southern Italy remained largely feudal until the 19th century, partially as a result of a succession of Byzantine, Arab, Norman, Angevin, Aragonese and other foreign conquests of the region. The Renaissance began in Italy and spread to the rest of Europe, bringing a renewed interest in humanism, science, exploration and art. Italian culture flourished, producing famous scholars, artists and polymaths. During the Middle Ages, Italian explorers discovered new routes to the Far East and the New World, helping to usher in the European Age of Discovery. Nevertheless, Italy's commercial and political power significantly waned with the opening of trade routes that bypassed the Mediterranean. Centuries of rivalry and infighting between the Italian city-states, such as the Italian Wars of the 15th and 16th centuries, left Italy fragmented and several Italian states were conquered and further divided by multiple European powers over the centuries.

By the mid-19th century, rising Italian nationalism and calls for independence from foreign control led to a period of revolutionary political upheaval. After centuries of foreign domination and political division, Italy was almost entirely unified in 1861, establishing the Kingdom of Italy as a great power. From the late 19th century to the early 20th century, Italy rapidly industrialised, namely in the north, and acquired a colonial empire, while the south remained largely impoverished and excluded from industrialisation, fuelling a large and influential diaspora. Despite being one of the four main allied powers in World War I, Italy entered a period of economic crisis and social turmoil, leading to the rise of the Italian fascist dictatorship in 1922. Participation in World War II on the Axis side ended in military defeat, economic destruction and the Italian Civil War. Following the liberation of Italy the country abolished their monarchy, established a democratic Republic and enjoyed a prolonged economic boom, becoming a highly developed country.

Today, Italy is considered to be one of the world's most culturally and economically advanced countries, with the world's eighth-largest economy by nominal GDP (third in the Eurozone), sixth-largest national wealth and third-largest central bank gold reserve. It ranks very highly in life expectancy, quality of life, healthcare, and education. The country plays a prominent role in regional and global economic, military, cultural and diplomatic affairs; it is both a regional power and a great power, and is ranked the world's eighth most-powerful military. Italy is a founding and leading member of the European Union and a member of numerous international institutions, including the UN, NATO, the OECD, the OSCE, the WTO, the G7, the G20, the Union for the Mediterranean, the Council of Europe, Uniting for Consensus, the Schengen Area and many more. The country has long been a global centre of art, music, literature, philosophy, science and technology, and fashion, and has greatly influenced and contributed to diverse fields including cinema, cuisine, sports, jurisprudence, banking and business. As a reflection of its cultural wealth, Italy is home to the world's largest number of World Heritage Sites (55), and is the fifth-most visited country.

Hypotheses for the etymology of the name "Italia" are numerous. One is that it was borrowed via Greek from the Oscan "Víteliú" 'land of calves' ("cf." Lat "vitulus" "calf", Umb "vitlo" "calf"). Greek historian Dionysius of Halicarnassus states this account together with the legend that Italy was named after Italus, mentioned also by Aristotle and Thucydides.

According to Antiochus of Syracuse, the term Italy was used by the Greeks to initially refer only to the southern portion of the Bruttium peninsula corresponding to the modern province of Reggio and part of the provinces of Catanzaro and Vibo Valentia in southern Italy. Nevertheless, by his time the larger concept of Oenotria and "Italy" had become synonymous and the name also applied to most of Lucania as well. According to Strabo's "Geographica", before the expansion of the Roman Republic, the name was used by Greeks to indicate the land between the strait of Messina and the line connecting the gulf of Salerno and gulf of Taranto, corresponding roughly to the current region of Calabria. The Greeks gradually came to apply the name "Italia" to a larger region In addition to the "Greek Italy" in the south, historians have suggested the existence of an "Etruscan Italy" covering variable areas of central Italy.

The borders of Roman Italy, "Italia", are better established. Cato's "Origines", the first work of history composed in Latin, described Italy as the entire peninsula south of the Alps. According to Cato and several Roman authors, the Alps formed the "walls of Italy". In 264 BC, Roman Italy extended from the Arno and Rubicon rivers of the centre-north to the entire south. The northern area of Cisalpine Gaul was occupied by Rome in the 220s BC and became considered geographically and "de facto" part of Italy, but remained politically and "de jure" separated. It was legally merged into the administrative unit of Italy in 42 BC by the triumvir Octavian as a ratification of Caesar's unpublished acts ("Acta Caesaris"). The islands of Sardinia, Corsica, Sicily and Malta were added to Italy by Diocletian in 292 AD.

Thousands of Paleolithic-era artifacts have been recovered from Monte Poggiolo and dated to around 850,000 years before the present, making them the oldest evidence of first hominins habitation in the peninsula.
Excavations throughout Italy revealed a Neanderthal presence dating back to the Palaeolithic period some 200,000 years ago, while modern Humans appeared about 40,000 years ago at Riparo Mochi. Archaeological sites from this period include Addaura cave, Altamura, Ceprano, and Gravina in Puglia.

The Ancient peoples of pre-Roman Italy – such as the Umbrians, the Latins (from which the Romans emerged), Volsci, Oscans, Samnites, Sabines, the Celts, the Ligures, the Veneti and many others – were Indo-European peoples, most of them specifically of the Italic group. The main historic peoples of possible non-Indo-European or pre-Indo-European heritage include the Etruscans of central and northern Italy, the Elymians and the Sicani in Sicily, and the prehistoric Sardinians, who gave birth to the Nuragic civilisation. Other ancient populations being of undetermined language families and of possible non-Indo-European origin include the Rhaetian people and Cammuni, known for their rock carvings in Valcamonica, the largest collections of prehistoric petroglyphs in the world. A well-preserved natural mummy known as Ötzi the Iceman, determined to be 5,000 years old (between 3400 and 3100 BCE, Copper Age), was discovered in the Similaun glacier of South Tyrol in 1991.

The first foreign colonizers were the Phoenicians, who initially established colonies and founded various emporiums on the coasts of Sicily and Sardinia. Some of these soon became small urban centres and were developed parallel to the Greek colonies; among the main centres there were the cities of Motya, Zyz (modern Palermo), Soluntum in Sicily and Nora, Sulci, and Tharros in Sardinia.

Between the 17th and the 11th centuries BC Mycenaean Greeks established contacts with Italy and in the 8th and 7th centuries BC a number of Greek colonies were established all along the coast of Sicily and the southern part of the Italian Peninsula, that became known as Magna Graecia. The Greek colonization placed the Italic peoples in contact with democratic government forms and with elevated artistic and cultural expressions.

Rome, a settlement around a ford on the river Tiber in central Italy conventionally founded in 753 BC, was ruled for a period of 244 years by a monarchical system, initially with sovereigns of Latin and Sabine origin, later by Etruscan kings. The tradition handed down seven kings: Romulus, Numa Pompilius, Tullus Hostilius, Ancus Marcius, Tarquinius Priscus, Servius Tullius and Tarquinius Superbus. In 509 BC, the Romans expelled the last king from their city, favouring a government of the Senate and the People (SPQR) and establishing an oligarchic republic.

The Italian Peninsula, named Italia, was consolidated into a single entity during the Roman expansion and conquest of new lands at the expense of the other Italic tribes, Etruscans, Celts, and Greeks. A permanent association with most of the local tribes and cities was formed, and Rome began the conquest of Western Europe, Northern Africa and the Middle East. In the wake of Julius Caesar's rise and death in the first century BC, Rome grew over the course of centuries into a massive empire stretching from Britain to the borders of Persia, and engulfing the whole Mediterranean basin, in which Greek and Roman and many other cultures merged into a unique civilisation. The long and triumphant reign of the first emperor, Augustus, began a golden age of peace and prosperity. Italy remained the metropole of the empire, and as the homeland of the Romans and the territory of the capital, maintained a special status which made it "not a province, but the "Domina" (ruler) of the provinces". More than two centuries of stability followed, during which Italy was referred to as the "rectrix mundi" (queen of the world) and "omnium terrarum parens" (motherland of all lands).

The Roman Empire was among the most powerful economic, cultural, political and military forces in the world of its time, and it was one of the largest empires in world history. At its height under Trajan, it covered 5 million square kilometres. The Roman legacy has deeply influenced the Western civilisation, shaping most of the modern world; among the many legacies of Roman dominance are the widespread use of the Romance languages derived from Latin, the numerical system, the modern Western alphabet and calendar, and the emergence of Christianity as a major world religion. The Indo-Roman trade relations, beginning around the 1st century BCE, testifies to extensive Roman trade in far away regions; many reminders of the commercial trade between the Indian subcontinent and Italy have been found, such as the ivory statuette Pompeii Lakshmi from the ruins of Pompeii.

In a slow decline since the third century AD, the Empire split in two in 395 AD. The Western Empire, under the pressure of the barbarian invasions, eventually dissolved in 476 AD when its last emperor, Romulus Augustulus, was deposed by the Germanic chief Odoacer. The Eastern half of the Empire survived for another thousand years.

After the fall of the Western Roman Empire, Italy fell under the power of Odoacer's kingdom, and, later, was seized by the Ostrogoths, followed in the 6th century by a brief reconquest under Byzantine Emperor Justinian. The invasion of another Germanic tribe, the Lombards, late in the same century, reduced the Byzantine presence to the rump realm of the Exarchate of Ravenna and started the end of political unity of the peninsula for the next 1,300 years. Invasions of the peninsula caused a chaotic succession of barbarian kingdoms and the so-called "dark ages". The Lombard kingdom was subsequently absorbed into the Frankish Empire by Charlemagne in the late 8th century. The Franks also helped the formation of the Papal States in central Italy. Until the 13th century, Italian politics was dominated by the relations between the Holy Roman Emperors and the Papacy, with most of the Italian city-states siding with the former (Ghibellines) or with the latter (Guelphs) from momentary convenience.
The Germanic Emperor and the Roman Pontiff became the universal powers of medieval Europe. However, the conflict for the investiture controversy (a conflict over two radically different views of whether secular authorities such as kings, counts, or dukes, had any legitimate role in appointments to ecclesiastical offices) and the clash between Guelphs and Ghibellines led to the end of the Imperial-feudal system in the north of Italy where city-states gained independence. It was during this chaotic era that Italian towns saw the rise of a peculiar institution, the medieval commune. Given the power vacuum caused by extreme territorial fragmentation and the struggle between the Empire and the Holy See, local communities sought autonomous ways to maintain law and order. The investiture controversy was finally resolved by the Concordat of Worms. In 1176 a league of city-states, the Lombard League, defeated the German emperor Frederick Barbarossa at the Battle of Legnano, thus ensuring effective independence for most of northern and central Italian cities.

Italian city-states such as Milan, Florence and Venice played a crucial innovative role in financial development, devising the main instruments and practices of banking and the emergence of new forms of social and economic organization. In coastal and southern areas, the maritime republics grew to eventually dominate the Mediterranean and monopolise trade routes to the Orient. They were independent thalassocratic city-states, though most of them originated from territories once belonging to the Byzantine Empire. All these cities during the time of their independence had similar systems of government in which the merchant class had considerable power. Although in practice these were oligarchical, and bore little resemblance to a modern democracy, the relative political freedom they afforded was conducive to academic and artistic advancement. The four best known maritime republics were Venice, Genoa, Pisa and Amalfi; the others were Ancona, Gaeta, Noli, and Ragusa. Each of the maritime republics had dominion over different overseas lands, including many Mediterranean islands (especially Sardinia and Corsica), lands on the Adriatic, Aegean, and Black Sea (Crimea), and commercial colonies in the Near East and in North Africa. Venice maintained enormous tracts of land in Greece, Cyprus, Istria and Dalmatia until as late as the mid-17th century.

Venice and Genoa were Europe's main gateway to trade with the East, and a producer of fine glass, while Florence was a capital of silk, wool, banks and jewellery. The wealth such business brought to Italy meant that large public and private artistic projects could be commissioned. The republics were heavily involved in the Crusades, providing support and transport, but most especially taking advantage of the political and trading opportunities resulting from these wars. Italy first felt huge economic changes in Europe which led to the commercial revolution: the Republic of Venice was able to defeat the Byzantine Empire and finance the voyages of Marco Polo to Asia; the first universities were formed in Italian cities, and scholars such as Thomas Aquinas obtained international fame; Frederick of Sicily made Italy the political-cultural centre of a reign that temporarily included the Holy Roman Empire and the Kingdom of Jerusalem; capitalism and banking families emerged in Florence, where Dante and Giotto were active around 1300.

In the south, Sicily had become an Islamic emirate in the 9th century, thriving until the Italo-Normans conquered it in the late 11th century together with most of the Lombard and Byzantine principalities of southern Italy. Through a complex series of events, southern Italy developed as a unified kingdom, first under the House of Hohenstaufen, then under the Capetian House of Anjou and, from the 15th century, the House of Aragon. In Sardinia, the former Byzantine provinces became independent states known in Italian as Judicates, although some parts of the island fell under Genoese or Pisan rule until the eventual Aragonese annexation in the 15th century. The Black Death pandemic of 1348 left its mark on Italy by killing perhaps one third of the population. However, the recovery from the plague led to a resurgence of cities, trade and economy which allowed the bloom of Humanism and Renaissance, that later spread to Europe.

Italy was the birthplace and heart of the Renaissance during the 1400s and 1500s. The Italian Renaissance marked the transition from the medieval period to the modern age as Europe recovered, economically and culturally, from the crises of the Late Middle Ages and entered the Early Modern Period. The Italian polities were now regional states effectively ruled by Princes, "de facto" monarchs in control of trade and administration, and their courts became major centres of Arts and Sciences. The Italian princedoms represented a first form of modern states as opposed to feudal monarchies and multinational empires. The princedoms were led by political dynasties and merchant families such as the Medici in Florence, the Visconti and Sforza in the Duchy of Milan, the Doria in the Republic of Genoa, the Mocenigo and Barbarigo in the Republic of Venice, the Este in Ferrara, and the Gonzaga in Mantua. The Renaissance was therefore a result of the great wealth accumulated by Italian merchant cities combined with the patronage of its dominant families. Italian Renaissance exercised a dominant influence on subsequent European painting and sculpture for centuries afterwards, with artists such as Leonardo da Vinci, Brunelleschi, Botticelli, Michelangelo, Raphael, Giotto, Donatello, and Titian, and architects such as Filippo Brunelleschi, Leon Battista Alberti, Andrea Palladio, and Donato Bramante.

Following the conclusion of the western schism in favor of Rome at the Council of Constance (1415–1417), the new Pope Martin V returned to the Papal States after a three years-long journey that touched many Italian cities and restored Italy as the sole centre of Western Christianity. During the course of this voyage, the Medici Bank was made the official credit institution of the Papacy and several significant ties were established between the Church and the new political dynasties of the peninsula. The Popes' status as elective monarchs turned the conclaves and consistories of the Renaissance into political battles between the courts of Italy for primacy in the peninsula and access to the immense resources of the Catholic Church. In 1439, Pope Eugenius IV and the Byzantine Emperor John VIII Palaiologos signed a reconciliation agreement between the Catholic Church and the Orthodox Church at the Council of Florence hosted by Cosimo "the old" de Medici. In 1453, Italian forces under Giovanni Giustiniani were sent by Pope Nicholas V to defend the Walls of Constantinople but the decisive battle was lost to the more advanced Turkish army equipped with cannons, and Byzantium fell to Sultan Mehmed II.

The fall of Constantinople led to the migration of Greek scholars and texts to Italy, fueling the rediscovery of Greco-Roman Humanism. Humanist rulers such as Federico da Montefeltro and Pope Pius II worked to establish ideal cities where "man is the measure of all things", and therefore founded Urbino and Pienza respectively. Pico della Mirandola wrote the "Oration on the Dignity of Man", considered the manifesto of Renaissance Humanism, in which he stressed the importance of free will in human beings. The humanist historian Leonardo Bruni was the first to divide human history in three periods: Antiquity, Middle Ages and Modernity. The second consequence of the Fall of Constantinople was the beginning of the Age of Discovery.
Italian explorers and navigators from the dominant maritime republics, eager to find an alternative route to the Indies in order to bypass the Ottoman Empire, offered their services to monarchs of Atlantic countries and played a key role in ushering the Age of Discovery and the European colonization of the Americas. The most notable among them were: Christopher Columbus, colonizer in the name of Spain, who is credited with discovering the New World and the opening of the Americas for conquest and settlement by Europeans; John Cabot, sailing for England, who was the first European to set foot in "New Found Land" and explore parts of the North American continent in 1497; Amerigo Vespucci, sailing for Portugal, who first demonstrated in about 1501 that the New World (in particular Brazil) was not Asia as initially conjectured, but a fourth continent previously unknown to people of the Old World (America is named after him); and Giovanni da Verrazzano, at the service of France, renowned as the first European to explore the Atlantic coast of North America between Florida and New Brunswick in 1524;

Following the fall of Constantinople, the wars in Lombardy came to an end and a defensive alliance known as Italic League was formed between Venice, Naples, Florence, Milan, and the Papacy. Lorenzo "the Magnificent" de Medici was the greatest Florentine patron of the Renaissance and supporter of the Italic League. He notably avoided the collapse of the League in the aftermath of the Pazzi Conspiracy and during the aborted invasion of Italy by the Turks. However, the military campaign of Charles VIII of France in Italy caused the end of the Italic League and initiated the Italian Wars between the Valois and the Habsburgs. During the High Renaissance of the 1500s, Italy was therefore both the main European battleground and the cultural-economic centre of the continent. Popes such as Julius II (1503–1513) fought for the control of Italy against foreign monarchs, others such as Paul III (1534–1549) preferred to mediate between the European powers in order to secure peace in Italy. In the middle of this conflict, the Medici popes Leo X (1513–1521) and Clement VII (1523–1534) opposed the Protestant reformation and advanced the interests of their family. The end of the wars ultimately left northern Italy indirectly subject to the Austrian Habsburgs and Southern Italy under direct Spanish Habsburg rule.

The Papacy remained independent and launched the Counter-reformation. Key events of the period include: the Council of Trent (1545–1563); the excommunication of Elizabeth I (1570) and the Battle of Lepanto (1571), both occurring during the pontificate of Pius V; the construction of the Gregorian observatory, the adoption of the Gregorian calendar, and the Jesuit China mission of Matteo Ricci under Pope Gregory XIII; the French Wars of Religion; the Long Turkish War and the execution of Giordano Bruno in 1600, under Pope Clement VIII; the birth of the Lyncean Academy of the Papal States, of which the main figure was Galileo Galilei (later put on trial); the final phases of the Thirty Years' War (1618–1648) during the pontificates of Urban VIII and Innocent X; and the formation of the last Holy League by Innocent XI during the Great Turkish War

The Italian economy declined during the 1600s and 1700s, as the peninsula was excluded from the rising Atlantic slave trade. Following the European wars of succession of the 18th century, the south passed to a cadet branch of the Spanish Bourbons and the North fell under the influence of the Habsburg-Lorraine of Austria. During the Coalition Wars, northern-central Italy was reorganised by Napoleon in a number of Sister Republics of France and later as a Kingdom of Italy in personal union with the French Empire. The southern half of the peninsula was administered by Joachim Murat, Napoleon's brother-in-law, who was crowned as King of Naples. The 1814 Congress of Vienna restored the situation of the late 18th century, but the ideals of the French Revolution could not be eradicated, and soon re-surfaced during the political upheavals that characterised the first part of the 19th century.

The birth of the Kingdom of Italy was the result of efforts by Italian nationalists and monarchists loyal to the House of Savoy to establish a united kingdom encompassing the entire Italian Peninsula. Following the Congress of Vienna in 1815, the political and social Italian unification movement, or "Risorgimento", emerged to unite Italy consolidating the different states of the peninsula and liberate it from foreign control. A prominent radical figure was the patriotic journalist Giuseppe Mazzini, member of the secret revolutionary society "Carbonari" and founder of the influential political movement Young Italy in the early 1830s, who favoured a unitary republic and advocated a broad nationalist movement. His prolific output of propaganda helped the unification movement stay active.

The most famous member of Young Italy was the revolutionary and general Giuseppe Garibaldi, renowned for his extremely loyal followers, who led the Italian republican drive for unification in Southern Italy. However, the Northern Italy monarchy of the House of Savoy in the Kingdom of Sardinia, whose government was led by Camillo Benso, Count of Cavour, also had ambitions of establishing a united Italian state. In the context of the 1848 liberal revolutions that swept through Europe, an unsuccessful first war of independence was declared on Austria. In 1855, the Kingdom of Sardinia became an ally of Britain and France in the Crimean War, giving Cavour's diplomacy legitimacy in the eyes of the great powers. The Kingdom of Sardinia again attacked the Austrian Empire in the Second Italian War of Independence of 1859, with the aid of France, resulting in liberating Lombardy.

In 1860–1861, Garibaldi led the drive for unification in Naples and Sicily (the Expedition of the Thousand), while the House of Savoy troops occupied the central territories of the Italian peninsula, except Rome and part of Papal States. Teano was the site of the famous meeting of 26 October 1860 between Giuseppe Garibaldi and Victor Emmanuel II, last King of Sardinia, in which Garibaldi shook Victor Emanuel's hand and hailed him as King of Italy; thus, Garibaldi sacrificed republican hopes for the sake of Italian unity under a monarchy. Cavour agreed to include Garibaldi's Southern Italy allowing it to join the union with the Kingdom of Sardinia in 1860. This allowed the Sardinian government to declare a united Italian kingdom on 17 March 1861. Victor Emmanuel II then became the first king of a united Italy, and the capital was moved from Turin to Florence.

In 1866, Victor Emmanuel II allied with Prussia during the Austro-Prussian War, waging the Third Italian War of Independence which allowed Italy to annexe Venetia. Finally, in 1870, as France abandoned its garrisons in Rome during the disastrous Franco-Prussian War to keep the large Prussian Army at bay, the Italians rushed to fill the power gap by taking over the Papal States. Italian unification was completed and shortly afterward Italy's capital was moved to Rome. Victor Emmanuel, Garibaldi, Cavour and Mazzini have been referred as Italy's "Four Fathers of the Fatherland".

The new Kingdom of Italy obtained Great Power status. The Constitutional Law of the Kingdom of Sardinia the Albertine Statute of 1848, was extended to the whole Kingdom of Italy in 1861, and provided for basic freedoms of the new State, but electoral laws excluded the non-propertied and uneducated classes from voting. The government of the new kingdom took place in a framework of parliamentary constitutional monarchy dominated by liberal forces. As Northern Italy quickly industrialised, the South and rural areas of the North remained underdeveloped and overpopulated, forcing millions of people to migrate abroad and fuelling a large and influential diaspora. The Italian Socialist Party constantly increased in strength, challenging the traditional liberal and conservative establishment.

Starting from the last two decades of the 19th century, Italy developed into a colonial power by forcing under its rule Eritrea and Somalia in East Africa, Tripolitania and Cyrenaica in North Africa (later unified in the colony of Libya) and the Dodecanese islands. From 2 November 1899 to 7 September 1901, Italy also participated as part of the Eight-Nation Alliance forces during the Boxer Rebellion in China; on 7 September 1901, a concession in Tientsin was ceded to the country, and on 7 June 1902, the concession was taken into Italian possession and administered by a consul. In 1913, male universal suffrage was adopted. The pre-war period dominated by Giovanni Giolitti, Prime Minister five times between 1892 and 1921, was characterized by the economic, industrial and political-cultural modernization of Italian society.

Italy, nominally allied with the German Empire and the Empire of Austria-Hungary in the Triple Alliance, in 1915 joined the Allies into World War I with a promise of substantial territorial gains, that included western Inner Carniola, former Austrian Littoral, Dalmatia as well as parts of the Ottoman Empire. The country gave a fundamental contribution to the victory of the conflict as one of the "Big Four" top Allied powers. The war was initially inconclusive, as the Italian army got stuck in a long attrition war in the Alps, making little progress and suffering very heavy losses. However, the reorganization of the army and the conscription of the so-called "'99 Boys" ("Ragazzi del '99", all males born in 1899 who were turning 18) led to more effective Italian victories in major battles, such as on Monte Grappa and in a series of battles on the Piave river. Eventually, in October 1918, the Italians launched a massive offensive, culminating in the victory of Vittorio Veneto. The Italian victory marked the end of the war on the Italian Front, secured the dissolution of the Austro-Hungarian Empire and was chiefly instrumental in ending the First World War less than two weeks later.

During the war, more than 650,000 Italian soldiers and as many civilians died and the kingdom went to the brink of bankruptcy. Under the Peace Treaties of Saint-Germain, Rapallo and Rome, Italy gained a permanent seat in the League of Nations's executive council and obtained most of the promised territories, but not Dalmatia (except Zara), allowing nationalists to define the victory as "mutilated". Moreover, Italy annexed the Hungarian harbour of Fiume, that was not part of territories promised at London but had been occupied after the end of the war by Gabriele D'Annunzio.

The socialist agitations that followed the devastation of the Great War, inspired by the Russian Revolution, led to counter-revolution and repression throughout Italy. The liberal establishment, fearing a Soviet-style revolution, started to endorse the small National Fascist Party, led by Benito Mussolini. In October 1922 the Blackshirts of the National Fascist Party attempted a coup named the "March on Rome" which failed but at the last minute, King Victor Emmanuel III refused to proclaim a state of siege and appointed Mussolini prime minister. Over the next few years, Mussolini banned all political parties and curtailed personal liberties, thus forming a dictatorship. These actions attracted international attention and eventually inspired similar dictatorships such as Nazi Germany and Francoist Spain.

In 1935, Mussolini invaded Ethiopia and founded the Italian East Africa, resulting in an international alienation and leading to Italy's withdrawal from the League of Nations; Italy allied with Nazi Germany and the Empire of Japan and strongly supported Francisco Franco in the Spanish civil war. In 1939, Italy annexed Albania, a "de facto" protectorate for decades. Italy entered World War II on 10 June 1940. After initially advancing in British Somaliland, Egypt, the Balkans and eastern fronts, the Italians were defeated in East Africa, Soviet Union and North Africa.

The Armistice of Villa Giusti, which ended fighting between Italy and Austria-Hungary at the end of World War I, resulted in Italian annexation of neighbouring parts of Yugoslavia. During the interwar period, the fascist Italian government undertook a campaign of Italianisation in the areas it annexed, which suppressed Slavic language, schools, political parties, and cultural institutions. During World War II, Italian war crimes included extrajudicial killings and ethnic cleansing by deportation of about 25,000 people, mainly Jews, Croats, and Slovenians, to the Italian concentration camps, such as Rab, Gonars, Monigo, Renicci di Anghiari and elsewhere.
In Italy and Yugoslavia, unlike in Germany, few war crimes were prosecuted. Yugoslav Partisans perpetrated their own crimes during and after the war, including the foibe killings. Meanwhile, about 250,000 Italians and anti-communist Slavs fled to Italy in the Istrian exodus.

An Allied invasion of Sicily began in July 1943, leading to the collapse of the Fascist regime and the fall of Mussolini on 25 July. Mussolini was deposed and arrested by order of King Victor Emmanuel III in co-operation with the majority of the members of the Grand Council of Fascism, which passed a motion of no confidence. On 8 September, Italy signed the Armistice of Cassibile, ending its war with the Allies. The Germans helped by the Italian fascists shortly succeeded in taking control of northern and central Italy. The country remained a battlefield for the rest of the war, as the Allies were slowly moving up from the south.

In the north, the Germans set up the Italian Social Republic (RSI), a Nazi puppet state with Mussolini installed as leader after he was rescued by German paratroopers. Some Italian troops in the south were organized into the Italian Co-belligerent Army, which fought alongside the Allies for the rest of the war, while other Italian troops, loyal to Mussolini and his RSI, continued to fight alongside the Germans in the National Republican Army. As result, the country descended into civil war. Also, the post-armistice period saw the rise of a large anti-fascist resistance movement, the "Resistenza", which fought a guerilla war against the German and RSI forces. In late April 1945, with total defeat looming, Mussolini attempted to escape north, but was captured and summarily executed near Lake Como by Italian partisans. His body was then taken to Milan, where it was hung upside down at a service station for public viewing and to provide confirmation of his demise. Hostilities ended on 29 April 1945, when the German forces in Italy surrendered. Nearly half a million Italians (including civilians) died in the conflict, and the Italian economy had been all but destroyed; per capita income in 1944 was at its lowest point since the beginning of the 20th century.

Italy became a republic after a referendum held on 2 June 1946, a day celebrated since as Republic Day. This was also the first time that Italian women were entitled to vote. Victor Emmanuel III's son, Umberto II, was forced to abdicate and exiled. The Republican Constitution was approved on 1 January 1948. Under the Treaty of Peace with Italy of 1947, most of the Julian March was lost to Yugoslavia and, later, the Free Territory of Trieste was divided between the two states. Italy also lost all of its colonial possessions, formally ending the Italian Empire. In 1950, Italian Somaliland was made a United Nations Trust Territory under Italian administration until 1 July 1960.

Fears of a possible Communist takeover (especially in the United States) proved crucial for the first universal suffrage electoral outcome on 18 April 1948, when the Christian Democrats, under the leadership of Alcide De Gasperi, obtained a landslide victory. Consequently, in 1949 Italy became a member of NATO. The Marshall Plan helped to revive the Italian economy which, until the late 1960s, enjoyed a period of sustained economic growth commonly called the "Economic Miracle". In 1957, Italy was a founding member of the European Economic Community (EEC), which became the European Union (EU) in 1993.

From the late 1960s until the early 1980s, the country experienced the Years of Lead, a period characterised by economic crisis (especially after the 1973 oil crisis), widespread social conflicts and terrorist massacres carried out by opposing extremist groups, with the alleged involvement of US and Soviet intelligence. The Years of Lead culminated in the assassination of the Christian Democrat leader Aldo Moro in 1978 and the Bologna railway station massacre in 1980, where 85 people died.

In the 1980s, for the first time since 1945, two governments were led by non-Christian-Democrat premiers: one republican (Giovanni Spadolini) and one socialist (Bettino Craxi); the Christian Democrats remained, however, the main government party. During Craxi's government, the economy recovered and Italy became the world's fifth largest industrial nation, after it gained the entry into the Group of Seven in the 1970s. However, as a result of his spending policies, the Italian national debt skyrocketed during the Craxi era, soon passing 100% of the country's GDP.

Italy faced several terror attacks between 1992–93 perpetrated by the Sicilian Mafia as a consequence of several life sentences pronounced during the "Maxi Trial", and of the new anti-mafia measures launched by the government. In 1992, two major dynamite attacks killed the judges Giovanni Falcone (23 May in the Capaci bombing) and Paolo Borsellino (19 July in the Via D'Amelio bombing). One year later (May–July 1993), tourist spots were attacked, such as the Via dei Georgofili in Florence, Via Palestro in Milan, and the Piazza San Giovanni in Laterano and Via San Teodoro in Rome, leaving 10 dead and 93 injured and causing severe damage to cultural heritage such as the Uffizi Gallery. The Catholic Church openly condemned the Mafia, and two churches were bombed and an anti-Mafia priest shot dead in Rome. Also in the early 1990s, Italy faced significant challenges, as voters – disenchanted with political paralysis, massive public debt and the extensive corruption system (known as "Tangentopoli") uncovered by the Clean Hands ("Mani Pulite") investigation – demanded radical reforms. The scandals involved all major parties, but especially those in the government coalition: the Christian Democrats, who ruled for almost 50 years, underwent a severe crisis and eventually disbanded, splitting up into several factions. The Communists reorganised as a social-democratic force. During the 1990s and the 2000s, centre-right (dominated by media magnate Silvio Berlusconi) and centre-left coalitions (led by university professor Romano Prodi) alternately governed the country.

Amidst the Great Recession, Berlusconi resigned in 2011, and his conservative government was replaced by the technocratic cabinet of Mario Monti. Following the 2013 general election, the Vice-Secretary of the Democratic Party Enrico Letta formed a new government at the head of a right-left Grand coalition. In 2014, challenged by the new Secretary of the PD Matteo Renzi, Letta resigned and was replaced by Renzi. The new government started important constitutional reforms such as the abolition of the Senate and a new electoral law. On 4 December the constitutional reform was rejected in a referendum and Renzi resigned; the Foreign Affairs Minister Paolo Gentiloni was appointed new Prime Minister.

In the European migrant crisis of the 2010s, Italy was the entry point and leading destination for most asylum seekers entering the EU. From 2013 to 2018, the country took in over 700,000 migrants and refugees, mainly from sub-Saharan Africa, which caused great strain on the public purse and a surge in the support for far-right or eurosceptic political parties. The 2018 general election was characterized by a strong showing of the Five Star Movement and the League and the university professor Giuseppe Conte became the Prime Minister at the head of a populist coalition between these two parties. However, after only fourteen months the League withdrew its support to Conte, who formed a new unprecedented government coalition between the Five Star Movement and the centre-left.

In March 2020, Conte's government imposed a national quarantine as a measure to limit the spread of the coronavirus pandemic in the country. The measures, despite being widely approved by the public opinion, were also described as the largest suppression of constitutional rights in the history of the republic. Later that month, Italy became the country with the highest total number of deaths in the worldwide coronavirus pandemic.

Italy is located in Southern Europe (it is also considered a part of western Europe) between latitudes 35° and 47° N, and longitudes 6° and 19° E. To the north, Italy borders France, Switzerland, Austria, and Slovenia and is roughly delimited by the Alpine watershed, enclosing the Po Valley and the Venetian Plain. To the south, it consists of the entirety of the Italian Peninsula and the two Mediterranean islands of Sicily and Sardinia (the two biggest islands of the Mediterranean), in addition to many smaller islands. The sovereign states of San Marino and the Vatican City are enclaves within Italy, while Campione d'Italia is an Italian exclave in Switzerland.

The country's total area is , of which is land and is water. Including the islands, Italy has a coastline and border of on the Adriatic, Ionian, Tyrrhenian seas (), and borders shared with France (), Austria (), Slovenia () and Switzerland (). San Marino () and Vatican City (), both enclaves, account for the remainder.

Over 35% of the Italian territory is mountainous. The Apennine Mountains form the peninsula's backbone, and the Alps form most of its northern boundary, where Italy's highest point is located on Mont Blanc (Monte Bianco) (). Other worldwide-known mountains in Italy include the Matterhorn (Monte Cervino), Monte Rosa, Gran Paradiso in the West Alps, and Bernina, Stelvio and Dolomites along the eastern side.

The Po, Italy's longest river (), flows from the Alps on the western border with France and crosses the Padan plain on its way to the Adriatic Sea. The Po Valley is the largest plain in Italy, with , and it represents over 70% of the total plain area in the country.

Many elements of the Italian territory are of volcanic origin. Most of the small islands and archipelagos in the south, like Capraia, Ponza, Ischia, Eolie, Ustica and Pantelleria are volcanic islands.
There are also active volcanoes: Mount Etna in Sicily (the largest active volcano in Europe), Vulcano, Stromboli, and Vesuvius (the only active volcano on mainland Europe).

The five largest lakes are, in order of diminishing size: Garda (), Maggiore (, whose minor northern part is Switzerland), Como (), Trasimeno () and Bolsena ().

Although the country includes the Italian peninsula, adjacent islands, and most of the southern Alpine basin, some of Italy's territory extends beyond the Alpine basin and some islands are located outside the Eurasian continental shelf. These territories are the "comuni" of: Livigno, Sexten, Innichen, Toblach (in part), Chiusaforte, Tarvisio, Graun im Vinschgau (in part), which are all part of the Danube's drainage basin, while the Val di Lei constitutes part of the Rhine's basin and the islands of Lampedusa and Lampione are on the African continental shelf.

Four different seas surround the Italian Peninsula in the Mediterranean Sea from three sides: the Adriatic Sea in the east, the Ionian Sea in the south, and the Ligurian Sea and the Tyrrhenian Sea in the west.

Most of the rivers of Italy drain either into the Adriatic Sea, such as the Po, Piave, Adige, Brenta, Tagliamento, and Reno, or into the Tyrrhenian, like the Arno, Tiber and Volturno. The waters from some border municipalities (Livigno in Lombardy, Innichen and Sexten in Trentino-Alto Adige/Südtirol) drain into the Black Sea through the basin of the Drava, a tributary of the Danube, and the waters from the Lago di Lei in Lombardy drain into the North Sea through the basin of the Rhine.

In the north of the country are a number of subalpine moraine-dammed lakes, the largest of which is Garda (). Other well-known subalpine lakes are Lake Maggiore (), whose most northerly section is part of Switzerland, Como (), one of the deepest lakes in Europe, Orta, Lugano, Iseo, and Idro. Other notable lakes in the Italian peninsula are Trasimeno, Bolsena, Bracciano, Vico, Varano and Lesina in Gargano and Omodeo in Sardinia.

The country is situated at the meeting point of the Eurasian Plate and the African Plate, leading to considerable seismic and volcanic activity. There are 14 volcanoes in Italy, four of which are active: Etna, Stromboli, Vulcano and Vesuvius. The last is the only active volcano in mainland Europe and is most famous for the destruction of Pompeii and Herculanum in the eruption in 79 AD. Several islands and hills have been created by volcanic activity, and there is still a large active caldera, the Campi Flegrei north-west of Naples.

The high volcanic and magmatic neogenic activity is subdivided into provinces:

Italy was the first country to exploit geothermal energy to produce electricity. The high geothermal gradient that forms part of the peninsula makes potentially exploitable also other provinces: research carried out in the 1960s and 1970s identifies potential geothermal fields in Lazio and Tuscany, as well as in most volcanic islands.

After its quick industrial growth, Italy took a long time to confront its environmental problems. After several improvements, it now ranks 84th in the world for ecological sustainability. National parks cover about 5% of the country. In the last decade, Italy has become one of the world's leading producers of renewable energy, ranking as the world's fourth largest holder of installed solar energy capacity and the sixth largest holder of wind power capacity in 2010. Renewable energies now make up about 12% of the total primary and final energy consumption in Italy, with a future target share set at 17% for the year 2020.
However, air pollution remains a severe problem, especially in the industrialised north, reaching the tenth highest level worldwide of industrial carbon dioxide emissions in the 1990s. Italy is the twelfth largest carbon dioxide producer.
Extensive traffic and congestion in the largest metropolitan areas continue to cause severe environmental and health issues, even if smog levels have decreased dramatically since the 1970s and 1980s, and the presence of smog is becoming an increasingly rarer phenomenon and levels of sulphur dioxide are decreasing.

Many watercourses and coastal stretches have also been contaminated by industrial and agricultural activity, while because of rising water levels, Venice has been regularly flooded throughout recent years. Waste from industrial activity is not always disposed of by legal means and has led to permanent health effects on inhabitants of affected areas, as in the case of the Seveso disaster. The country has also operated several nuclear reactors between 1963 and 1990 but, after the Chernobyl disaster and a referendum on the issue the nuclear programme was terminated, a decision that was overturned by the government in 2008, planning to build up to four nuclear power plants with French technology. This was in turn struck down by a referendum following the Fukushima nuclear accident.

Deforestation, illegal building developments and poor land-management policies have led to significant erosion all over Italy's mountainous regions, leading to major ecological disasters like the 1963 Vajont Dam flood, the 1998 Sarno and 2009 Messina mudslides.

Italy has the highest level of faunal biodiversity in Europe, with over 57,000 species recorded, representing more than a third of all European fauna. Italy's varied geological structure contributes to its high climate and habitat diversity. The Italian peninsula is in the centre of the Mediterranean Sea, forming a corridor between central Europe and North Africa, and has of coastline. Italy also receives species from the Balkans, Eurasia, the Middle East. Italy's varied geological structure, including the Alps and the Apennines, Central Italian woodlands, and Southern Italian Garigue and Maquis shrubland, also contributes to high climate and habitat diversity.

Italian fauna includes 4,777 endemic animal species, which include the Sardinian long-eared bat, Sardinian red deer, spectacled salamander, brown cave salamander, Italian newt, Italian frog, Apennine yellow-bellied toad, Aeolian wall lizard, Sicilian wall lizard, Italian Aesculapian snake, and Sicilian pond turtle. There are 102 mammals species (most notably the Italian wolf, Marsican brown bear, Pyrenean chamois, Alpine ibex, crested porcupine, Mediterranean monk seal, Alpine marmot, Etruscan shrew, and European snow vole), 516 bird species and 56,213 invertebrate species.

The flora of Italy was traditionally estimated to comprise about 5,500 vascular plant species. However, , 6,759 species are recorded in the "Data bank of Italian vascular flora". Italy is a signatory to the Berne Convention on the Conservation of European Wildlife and Natural Habitats and the Habitats Directive both affording protection to Italian fauna and flora.

Because of the great longitudinal extension of the peninsula and the mostly mountainous internal conformation, the climate of Italy is highly diverse. In most of the inland northern and central regions, the climate ranges from humid subtropical to humid continental and oceanic. In particular, the climate of the Po valley geographical region is mostly continental, with harsh winters and hot summers.

The coastal areas of Liguria, Tuscany and most of the South generally fit the Mediterranean climate stereotype (Köppen climate classification Csa). Conditions on peninsular coastal areas can be very different from the interior's higher ground and valleys, particularly during the winter months when the higher altitudes tend to be cold, wet, and often snowy. The coastal regions have mild winters and warm and generally dry summers, although lowland valleys can be quite hot in summer. Average winter temperatures vary from on the Alps to

Italy has been a unitary parliamentary republic since 2 June 1946, when the monarchy was abolished by a constitutional referendum. The President of Italy ("Presidente della Repubblica"), currently Sergio Mattarella since 2015, is Italy's head of state. The President is elected for a single seven years mandate by the Parliament of Italy and some regional voters in joint session. Italy has a written democratic constitution, resulting from the work of a Constituent Assembly formed by the representatives of all the anti-fascist forces that contributed to the defeat of Nazi and Fascist forces during the Civil War.

Italy has a parliamentary government based on a mixed proportional and majoritarian voting system. The parliament is perfectly bicameral: the two houses, the Chamber of Deputies that meets in Palazzo Montecitorio, and the Senate of the Republic that meets in Palazzo Madama, have the same powers. The Prime Minister, officially President of the Council of Ministers ("Presidente del Consiglio dei Ministri"), is Italy's head of government. The Prime Minister and the cabinet are appointed by the President of the Republic of Italy and must pass a vote of confidence in Parliament to come into office. To remain the Prime Minister has to pass also eventual further votes of confidence or no confidence in Parliament.

The prime minister is the President of the Council of Ministers – which holds effective executive power – and he must receive a vote of approval from it to execute most political activities. The office is similar to those in most other parliamentary systems, but the leader of the Italian government is not authorised to request the dissolution of the Parliament of Italy.

Another difference with similar offices is that the overall political responsibility for intelligence is vested in the President of the Council of Ministers. By virtue of that, the Prime Minister has exclusive power to: co-ordinate intelligence policies, determining the financial resources and strengthening national cyber security; apply and protect State secrets; authorise agents to carry out operations, in Italy or abroad, in violation of the law.

A peculiarity of the Italian Parliament is the representation given to Italian citizens permanently living abroad: 12 Deputies and 6 Senators elected in four distinct overseas constituencies. In addition, the Italian Senate is characterised also by a small number of senators for life, appointed by the President "for outstanding patriotic merits in the social, scientific, artistic or literary field". Former Presidents of the Republic are "ex officio" life senators.

Italy's three major political parties are the Five Star Movement, the Democratic Party and the Lega. During the 2018 general election these three parties won 614 out of 630 seats available in the Chamber of Deputies and 309 out of 315 in the Senate. Berlusconi's Forza Italia which formed a centre-right coalition with Matteo Salvini's Northern League and Giorgia Meloni's Brothers of Italy won most of the seats without getting the majority in parliament. The rest of the seats were taken by Five Star Movement, Matteo Renzi's Democratic Party along with Achammer and Panizza's South Tyrolean People's Party & Trentino Tyrolean Autonomist Party in a centre-left coalition and the independent Free and Equal party.

The Italian judicial system is based on Roman law modified by the Napoleonic code and later statutes. The Supreme Court of Cassation is the highest court in Italy for both criminal and civil appeal cases. The Constitutional Court of Italy ("Corte Costituzionale") rules on the conformity of laws with the constitution and is a post–World War II innovation. Since their appearance in the middle of the 19th century, Italian organised crime and criminal organisations have infiltrated the social and economic life of many regions in Southern Italy, the most notorious of which being the Sicilian Mafia, which would later expand into some foreign countries including the United States. Mafia receipts may reach 9% of Italy's GDP.

A 2009 report identified 610 which have a strong Mafia presence, where 13 million Italians live and 14.6% of the Italian GDP is produced. The Calabrian 'Ndrangheta, nowadays probably the most powerful crime syndicate of Italy, accounts alone for 3% of the country's GDP. However, at 0.013 per 1,000 people, Italy has only the 47th highest murder rate compared to 61 countries and the 43rd highest number of rapes per 1,000 people compared to 64 countries in the world. These are relatively low figures among developed countries.

The Italian law enforcement system is complex, with multiple police forces. The national policing agencies are the Polizia di Stato (State Police), the Arma dei Carabinieri, the Guardia di Finanza (Financial Guard), and the Polizia Penitenziaria (Prison Police), as well as the Guardia Costiera (coast guard police).

The "Polizia di Stato" are a civil police supervised by the Interior Ministry, while the "Carabinieri" is a gendarmerie supervised by the Defense Ministry; both share duties in law enforcement and the maintenance of public order. Within the Carabinieri is a unit devoted to combating environmental crime. The "Guardia di Finanza" is responsible for combating financial crime and white-collar crime, as well as customs. The "Polizia Penitenziaria" are responsible for guarding the prison system. The Corpo Forestale dello Stato (State Forestry Corps) formerly existed as a separate national park ranger agency, but was merged into the Carabinieri in 2016. Although policing in Italy is primarily provided on a national basis, there also exists "Polizia Provinciale" (provincial police) and "Polizia Municipale" (municipal police).

Italy is a founding member of the European Economic Community (EEC), now the European Union (EU), and of NATO. Italy was admitted to the United Nations in 1955, and it is a member and a strong supporter of a wide number of international organisations, such as the Organisation for Economic Co-operation and Development (OECD), the General Agreement on Tariffs and Trade/World Trade Organization (GATT/WTO), the Organization for Security and Co-operation in Europe (OSCE), the Council of Europe, and the Central European Initiative. Its recent or upcoming turns in the rotating presidency of international organisations include the Organization for Security and Co-operation in Europe in 2018, the G7 in 2017 and the EU Council from July to December 2014. Italy is also a recurrent non-permanent member of the UN Security Council, the most recently in 2017.

Italy strongly supports multilateral international politics, endorsing the United Nations and its international security activities. , Italy was deploying 5,296 troops abroad, engaged in 33 UN and NATO missions in 25 countries of the world. Italy deployed troops in support of UN peacekeeping missions in Somalia, Mozambique, and East Timor and provides support for NATO and UN operations in Bosnia, Kosovo and Albania. Italy deployed over 2,000 troops in Afghanistan in support of Operation Enduring Freedom (OEF) from February 2003.

Italy supported international efforts to reconstruct and stabilise Iraq, but it had withdrawn its military contingent of some 3,200 troops by 2006, maintaining only humanitarian operators and other civilian personnel.
In August 2006 Italy deployed about 2,450 troops in Lebanon for the United Nations' peacekeeping mission UNIFIL. Italy is one of the largest financiers of the Palestinian National Authority, contributing €60 million in 2013 alone.

The Italian Army, Navy, Air Force and Carabinieri collectively form the Italian Armed Forces, under the command of the Supreme Defence Council, presided over by the President of Italy. Since 2005, military service is voluntary. In 2010, the Italian military had 293,202 personnel on active duty, of which 114,778 are Carabinieri. Total Italian military spending in 2010 ranked tenth in the world, standing at $35.8 billion, equal to 1.7% of national GDP. As part of NATO's nuclear sharing strategy Italy also hosts 90 United States B61 nuclear bombs, located in the Ghedi and Aviano air bases.

The Italian Army is the national ground defence force, numbering 109,703 in 2008. Its best-known combat vehicles are the Dardo infantry fighting vehicle, the Centauro tank destroyer and the Ariete tank, and among its aircraft the Mangusta attack helicopter, in the last years deployed in EU, NATO and UN missions. It also has at its disposal many Leopard 1 and M113 armoured vehicles.

The Italian Navy in 2008 had 35,200 active personnel with 85 commissioned ships and 123 aircraft. It is a blue-water navy. In modern times the Italian Navy, being a member of the EU and NATO, has taken part in many coalition peacekeeping operations around the world.

The Italian Air Force in 2008 had a strength of 43,882 and operated 585 aircraft, including 219 combat jets and 114 helicopters. A transport capability is guaranteed by a fleet of 27 C-130Js and C-27J Spartan.

An autonomous corps of the military, the Carabinieri are the gendarmerie and military police of Italy, policing the military and civilian population alongside Italy's other police forces. While the different branches of the Carabinieri report to separate ministries for each of their individual functions, the corps reports to the Ministry of Internal Affairs when maintaining public order and security.

Italy is subdivided into 20 regions ("regioni"), five of these regions having a special autonomous status that enables them to enact legislation on some of their local matters. The country is further divided into 14 metropolitan cities ("città metropolitane") and 96 provinces ("province"), which in turn are subdivided in 7,960 municipalities (2018) ("comuni").

Italy has a major advanced capitalist mixed economy, ranking as the third-largest in the Eurozone and the eighth-largest in the world. A founding member of the G7, the Eurozone and the OECD, it is regarded as one of the world's most industrialised nations and a leading country in world trade and exports. It is a highly developed country, with the world's 8th highest quality of life in 2005 and the 26th Human Development Index. The country is well known for its creative and innovative business, a large and competitive agricultural sector (with the world's largest wine production), and for its influential and high-quality automobile, machinery, food, design and fashion industry.

Italy is the world's sixth largest manufacturing country, characterised by a smaller number of global multinational corporations than other economies of comparable size and many dynamic small and medium-sized enterprises, notoriously clustered in several industrial districts, which are the backbone of the Italian industry. This has produced a manufacturing sector often focused on the export of niche market and luxury products, that if on one side is less capable to compete on the quantity, on the other side is more capable of facing the competition from China and other emerging Asian economies based on lower labour costs, with higher quality products. Italy was the world's 7th largest exporter in 2016. Its closest trade ties are with the other countries of the European Union, with whom it conducts about 59% of its total trade. Its largest EU trade partners, in order of market share, are Germany (12.9%), France (11.4%), and Spain (7.4%).

The automotive industry is a significant part of the Italian manufacturing sector, with over 144,000 firms and almost 485,000 employed people in 2015, and a contribution of 8.5% to Italian GDP. Fiat Chrysler Automobiles or FCA is currently the world's seventh-largest auto maker. The country boasts a wide range of acclaimed products, from very compact city cars to luxury supercars such as Maserati, Lamborghini, and Ferrari, which was rated the world's most powerful brand by Brand Finance.

Italy is part of the European single market which represents more than 500 million consumers. Several domestic commercial policies are determined by agreements among European Union (EU) members and by EU legislation. Italy introduced the common European currency, the Euro in 2002. It is a member of the Eurozone which represents around 330 million citizens. Its monetary policy is set by the European Central Bank.

Italy has been hit hard by the Financial crisis of 2007–08, that exacerbated the country's structural problems. Effectively, after a strong GDP growth of 5–6% per year from the 1950s to the early 1970s, and a progressive slowdown in the 1980-90s, the country virtually stagnated in the 2000s. The political efforts to revive growth with massive government spending eventually produced a severe rise in public debt, that stood at over 131.8% of GDP in 2017, ranking second in the EU only after the Greek one. For all that, the largest chunk of Italian public debt is owned by national subjects, a major difference between Italy and Greece, and the level of household debt is much lower than the OECD average.

A gaping North–South divide is a major factor of socio-economic weakness. It can be noted by the huge difference in statistical income between the northern and southern regions and municipalities. The richest province, Alto Adige-South Tyrol, earns 152% of the national GDP per capita, while the poorest region, Calabria, 61%. The unemployment rate (11.1%) stands slightly above the Eurozone average, but the disaggregated figure is 6.6% in the North and 19.2% in the South. The youth unemployment rate (31.7% in March 2018) is extremely high compared to EU standards.

Italy has a strong cooperative sector, with the largest share of the population (4.5%) employed by a cooperative in the EU.

According to the last national agricultural census, there were 1.6 million farms in 2010 (−32.4% since 2000) covering 12.7 million hectares (63% of which are located in Southern Italy). The vast majority (99%) are family-operated and small, averaging only 8 hectares in size. Of the total surface area in agricultural use (forestry excluded), grain fields take up 31%, olive tree orchards 8.2%, vineyards 5.4%, citrus orchards 3.8%, sugar beets 1.7%, and horticulture 2.4%. The remainder is primarily dedicated to pastures (25.9%) and feed grains (11.6%).

Italy is the world's largest wine producer, and one of the leading in olive oil, fruits (apples, olives, grapes, oranges, lemons, pears, apricots, hazelnuts, peaches, cherries, plums, strawberries and kiwifruits), and vegetables (especially artichokes and tomatoes). The most famous Italian wines are probably the Tuscan Chianti and the Piedmontese Barolo. Other famous wines are Barbaresco, Barbera d'Asti, Brunello di Montalcino, Frascati, Montepulciano d'Abruzzo, Morellino di Scansano, and the sparkling wines Franciacorta and Prosecco.

Quality goods in which Italy specialises, particularly the already mentioned wines and regional cheeses, are often protected under the quality assurance labels DOC/DOP. This geographical indication certificate, which is attributed by the European Union, is considered important in order to avoid confusion with low-quality mass-produced ersatz products.

In 2004 the transport sector in Italy generated a turnover of about 119.4 billion euros, employing 935,700 persons in 153,700 enterprises. Regarding the national road network, in 2002 there were of serviceable roads in Italy, including of motorways, state-owned but privately operated by Atlantia. In 2005, about 34,667,000 passenger cars (590 cars per 1,000 people) and 4,015,000 goods vehicles circulated on the national road network.

The national railway network, state-owned and operated by Rete Ferroviaria Italiana (FSI), in 2008 totalled of which is electrified, and on which 4,802 locomotives and railcars run. The main public operator of high-speed trains is Trenitalia, part of FSI. Higher-speed trains are divided into three categories: Frecciarossa () trains operate at a maximum speed of 300 km/h on dedicated high-speed tracks; Frecciargento () trains operate at a maximum speed of 250 km/h on both high-speed and mainline tracks; and Frecciabianca () trains operate on high-speed regional lines at a maximum speed of 200 km/h. Italy has 11 rail border crossings over the Alpine mountains with its neighbouring countries.

Italy is one of the countries with the most vehicles per capita, with 690 per 1000 people in 2010. The national inland waterways network comprised of navigable rivers and channels for various types of commercial traffic in 2012.

Italy's largest airline is Alitalia, which serves 97 destinations (as of October 2019) and also operates a regional subsidiary under the Alitalia CityLiner brand. Italy's second largest airline is Air Italy, which operates a network of domestic, European and long-haul destinations. The country also has regional airlines (such as Air Dolomiti), low-cost carriers (including Ernest Airlines), and Charter and leisure carriers (including Neos, Blue Panorama Airlines and Poste Air Cargo. Major Italian cargo operators are Alitalia Cargo and Cargolux Italia.

Italy is the fifth in Europe by number of passengers by air transport, with about 148 million passengers or about 10% of the European total in 2011. In 2012 there were 130 airports in Italy, including the two hubs of Malpensa International in Milan and Leonardo da Vinci International in Rome. In 2004 there were 43 major seaports, including the seaport of Genoa, the country's largest and second largest in the Mediterranean Sea. In 2005 Italy maintained a civilian air fleet of about 389,000 units and a merchant fleet of 581 ships.

Italy needs to import about 80% of its energy requirements.

Italy does not invest enough to maintain its drinking water supply. The Galli Law, passed in 1993, aimed at raising the level of investment and to improve service quality by consolidating service providers, making them more efficient and increasing the level of cost recovery through tariff revenues. Despite these reforms, investment levels have declined and remain far from sufficient.

Through the centuries, Italy has fostered the scientific community that produced many major discoveries in physics and the other sciences. During the Renaissance Italian polymaths such as Leonardo da Vinci (1452–1519), Michelangelo (1475–1564) and Leon Battista Alberti (1404–1472) made important contributions to a variety of fields, including biology, architecture, and engineering. Galileo Galilei (1564–1642), a physicist, mathematician and astronomer, played a major role in the Scientific Revolution. His achievements include key improvements to the telescope and consequent astronomical observations, and ultimately the triumph of Copernicanism over the Ptolemaic model.

Other astronomers suchs as Giovanni Domenico Cassini (1625–1712) and Giovanni Schiaparelli (1835–1910) made many important discoveries about the Solar System. In mathematics, Joseph Louis Lagrange (born Giuseppe Lodovico Lagrangia, 1736–1813) was active before leaving Italy. Fibonacci (c. 1170 – c. 1250), and Gerolamo Cardano (1501–1576) made fundamental advances in mathematics. Luca Pacioli established accounting to the world. Physicist Enrico Fermi (1901–1954), a Nobel prize laureate, led the team in Chicago that developed the first nuclear reactor and is also noted for his many other contributions to physics, including the co-development of the quantum theory and was one of the key figures in the creation of the nuclear weapon. He, Emilio G. Segrè (1905–1989) who discovered the elements technetium and astatine, and the antiproton), Bruno Rossi (1905–1993) a pioneer in Cosmic Rays and X-ray astronomy) and a number of Italian physicists were forced to leave Italy in the 1930s by Fascist laws against Jews.

Other prominent physicists include: Amedeo Avogadro (most noted for his contributions to molecular theory, in particular the Avogadro's law and the Avogadro constant), Evangelista Torricelli (inventor of barometer), Alessandro Volta (inventor of electric battery), Guglielmo Marconi (inventor of radio), Galileo Ferraris and Antonio Pacinotti, pioneers of the induction motor, Alessandro Cruto, pioneer of light bulb and Innocenzo Manzetti, eclectic pioneer of auto and robotics, Ettore Majorana (who discovered the Majorana fermions), Carlo Rubbia (1984 Nobel Prize in Physics for work leading to the discovery of the W and Z particles at CERN). Antonio Meucci is known for developing a voice-communication device which is often credited as the first telephone. Pier Giorgio Perotto in 1964 designed one of the first desktop programmable calculators, the Programma 101. In biology, Francesco Redi has been the first to challenge the theory of spontaneous generation by demonstrating that maggots come from eggs of flies and he described 180 parasites in details and Marcello Malpighi founded microscopic anatomy, Lazzaro Spallanzani conducted important research in bodily functions, animal reproduction, and cellular theory, Camillo Golgi, whose many achievements include the discovery of the Golgi complex, paved the way to the acceptance of the Neuron doctrine, Rita Levi-Montalcini discovered the nerve growth factor (awarded 1986 Nobel Prize in Physiology or Medicine). In chemistry, Giulio Natta received the Nobel Prize in Chemistry in 1963 for his work on high polymers. Giuseppe Occhialini received the Wolf Prize in Physics for the discovery of the pion or pi-meson decay in 1947. Ennio de Giorgi, a Wolf Prize in Mathematics recipient in 1990, solved Bernstein's problem about minimal surfaces and the 19th Hilbert problem on the regularity of solutions of Elliptic partial differential equations.

Italy is the fifth most visited country in the world, with a total of 52.3 million international arrivals in 2016. The total contribution of travel & tourism to GDP (including wider effects from investment, the supply chain and induced income impacts) was EUR162.7bn in 2014 (10.1% of GDP) and generated 1,082,000 jobs directly in 2014 (4.8% of total employment).

Italy is well known for its cultural and environmental tourist routes and is home to 55 UNESCO World Heritage Sites, the most in the world. Rome is the 3rd most visited city in Europe and the 12th in the world, with 9.4 million arrivals in 2017 while Milan is the 27th worldwide with 6.8 million tourists. In addition, Venice and Florence are also among the world's top 100 destinations.

At the beginning of 2020, Italy had 60,317,116 inhabitants. The resulting population density, at , is higher than that of most Western European countries. However, the distribution of the population is widely uneven. The most densely populated areas are the Po Valley (that accounts for almost a half of the national population) and the metropolitan areas of Rome and Naples, while vast regions such as the Alps and Apennines highlands, the plateaus of Basilicata and the island of Sardinia are very sparsely populated.

The population of Italy almost doubled during the 20th century, but the pattern of growth was extremely uneven because of large-scale internal migration from the rural South to the industrial cities of the North, a phenomenon which happened as a consequence of the Italian economic miracle of the 1950–1960s. High fertility and birth rates persisted until the 1970s, after which they started to decline. The population rapidly aged; by 2010, one in five Italians was over 65 years old, and the country currently has the fifth oldest population in the world, with a median age of 45.8 years. However, in recent years Italy has experienced significant growth in birth rates. The total fertility rate has also climbed from an all-time low of 1.18 children per woman in 1995 to 1.41 in 2008, albeit still below the replacement rate of 2.1 and considerably below the high of 5.06 children born per woman in 1883. Nevertheless, the total fertility rate is expected to reach 1.6–1.8 in 2030.

From the late 19th century until the 1960s Italy was a country of mass emigration. Between 1898 and 1914, the peak years of Italian diaspora, approximately 750,000 Italians emigrated each year. The diaspora concerned more than 25 million Italians and it is considered the biggest mass migration of contemporary times. As a result, today more than 4.1 million Italian citizens are living abroad, while at least 60 million people of full or part Italian ancestry live outside of Italy, most notably in Argentina, Brazil, Uruguay, Venezuela, the United States, Canada, Australia and France.

Source:

In 2016, Italy had about 5.05 million foreign residents, making up 8.3% of the total population. The figures include more than half a million children born in Italy to foreign nationals (second generation immigrants) but exclude foreign nationals who have subsequently acquired Italian citizenship; in 2016, about 201,000 people became Italian citizens, compared to 130,000 in 2014. The official figures also exclude illegal immigrants, who estimated to number at least 670,000 as of 2008.

Starting from the early 1980s, until then a linguistically and culturally homogeneous society, Italy begun to attract substantial flows of foreign immigrants. After the fall of the Berlin Wall and, more recently, the 2004 and 2007 enlargements of the European Union, large waves of migration originated from the former socialist countries of Eastern Europe (especially Romania, Albania, Ukraine and Poland). An equally important source of immigration is neighbouring North Africa (in particular, Morocco, Egypt and Tunisia), with soaring arrivals as a consequence of the Arab Spring. Furthermore, in recent years, growing migration fluxes from Asia-Pacific (notably China and the Philippines) and Latin America have been recorded.

Currently, about one million Romanian citizens (around 10% of them being ethnic Romani people) are officially registered as living in Italy, representing thus the most important individual country of origin, followed by Albanians and Moroccans with about 500,000 people each. The number of unregistered Romanians is difficult to estimate, but the Balkan Investigative Reporting Network suggested in 2007 that there might have been half a million or more.

As of 2010, the foreign born population of Italy was from the following regions: Europe (54%), Africa (22%), Asia (16%), the Americas (8%) and Oceania (0.06%). The distribution of immigrants is largely uneven in Italy: 87% live in the northern and central parts of the country (the most economically developed areas), while only 13% live in the southern half.

Italy's official language is Italian, as stated by the framework law no. 482/1999 and Trentino Alto-Adige's special Statute, which is adopted with a constitutional law. There are an estimated 64 million native Italian speakers and another 21 million who use it as a second language. Italian is often natively spoken in a regional variety, not to be confused with Italy's regional and minority languages; however, the establishment of a national education system led to a decrease in variation in the languages spoken across the country during the 20th century. Standardisation was further expanded in the 1950s and 1960s due to economic growth and the rise of mass media and television (the state broadcaster RAI helped set a standard Italian).

Twelve "historical minority languages" ("minoranze linguistiche storiche") are formally recognised: Albanian, Catalan, German, Greek, Slovene, Croatian, French, Franco-Provençal, Friulian, Ladin, Occitan and Sardinian. Four of these also enjoy a co-official status in their respective region: French in the Aosta Valley; German in South Tyrol, and Ladin as well in some parts of the same province and in parts of the neighbouring Trentino; and Slovene in the provinces of Trieste, Gorizia and Udine. A number of other Ethnologue, ISO and UNESCO languages are not recognised by Italian law. Like France, Italy has signed the European Charter for Regional or Minority Languages, but has not ratified it.

Because of recent immigration, Italy has sizeable populations whose native language is not Italian, nor a regional language. According to the Italian National Institute of Statistics, Romanian is the most common mother tongue among foreign residents in Italy: almost 800,000 people speak Romanian as their first language (21.9% of the foreign residents aged 6 and over). Other prevalent mother tongues are Arabic (spoken by over 475,000 people; 13.1% of foreign residents), Albanian (380,000 people) and Spanish (255,000 people).

In 2017, the proportion of Italians who identified themselves as Roman Catholic Christians was 74.4%. Since 1985, it is no longer officially the state religion.

The Holy See, the episcopal jurisdiction of Rome, contains the central government of the Roman Catholic Church. It is recognised by other subjects of international law as a sovereign entity, headed by the Pope, who is also the Bishop of Rome, with which diplomatic relations can be maintained. Often incorrectly referred to as "the Vatican", the Holy See is not the same entity as the Vatican City State, which came into existence only in 1929.

In 2011, minority Christian faiths in Italy included an estimated 1.5 million Orthodox Christians, or 2.5% of the population; 500,000 Pentecostals and Evangelicals (of whom 400,000 are members of the Assemblies of God), 251,192 Jehovah's Witnesses, 30,000 Waldensians, 25,000 Seventh-day Adventists, 26,925 Latter-day Saints, 15,000 Baptists (plus some 5,000 Free Baptists), 7,000 Lutherans, 4,000 Methodists (affiliated with the Waldensian Church).

One of the longest-established minority religious faiths in Italy is Judaism, Jews having been present in Ancient Rome since before the birth of Christ. Italy has for centuries welcomed Jews expelled from other countries, notably Spain. However, about 20% of Italian Jews were killed during the Holocaust. This, together with the emigration that preceded and followed World War II, has left only around 28,400 Jews in Italy.
Soaring immigration in the last two decades has been accompanied by an increase in non-Christian faiths. There are more than 800,000 followers of faiths originating in the Indian subcontinent with some 70,000 Sikhs with 22 gurdwaras across the country.

The Italian state, as a measure to protect religious freedom, devolves shares of income tax to recognised religious communities, under a regime known as Eight per thousand. Donations are allowed to Christian, Jewish, Buddhist and Hindu communities; however, Islam remains excluded, since no Muslim communities have yet signed a concordat with the Italian state. Taxpayers who do not wish to fund a religion contribute their share to the state welfare system.

Education in Italy is free and mandatory from ages six to sixteen, and consists of five stages: kindergarten ("scuola dell'infanzia"), primary school ("scuola primaria"), lower secondary school ("scuola secondaria di primo grado", upper secondary school ("scuola secondaria di secondo grado") and university ("università").

Primary education lasts eight years. Students are given a basic education in Italian, English, mathematics, natural sciences, history, geography, social studies, physical education and visual and musical arts. Secondary education lasts for five years and includes three traditional types of schools focused on different academic levels: the "liceo" prepares students for university studies with a classical or scientific curriculum, while the "istituto tecnico" and the "Istituto professionale" prepare pupils for vocational education. In 2012, the Italian secondary education was evaluated as slightly below the OECD average, with a strong and steady improvement in science and mathematics results since 2003; however, a wide gap exists between northern schools, which performed significantly better than the national average (among the best in the world in some subjects), and schools in the South, that had much poorer results.

Tertiary education in Italy is divided between public universities, private universities and the prestigious and selective superior graduate schools, such as the Scuola Normale Superiore di Pisa. 33 Italian universities were ranked among the world's top 500 in 2019, the third-largest number in Europe after the United Kingdom and Germany. Bologna University, founded in 1088, is the oldest university in continuous operation, as well as one of the leading academic institutions in Italy and Europe. The Bocconi University, Università Cattolica del Sacro Cuore, LUISS, Polytechnic University of Turin, Polytechnic University of Milan, Sapienza University of Rome, and University of Milan are also ranked among the best in the world.

The Italian state runs a universal public healthcare system since 1978. However, healthcare is provided to all citizens and residents by a mixed public-private system. The public part is the "Servizio Sanitario Nazionale", which is organised under the Ministry of Health and administered on a devolved regional basis. Healthcare spending in Italy accounted for 9.2% of the national GDP in 2012, very close the OECD countries' average of 9.3%. Italy in 2000 ranked as having the world's 2nd best healthcare system, and the world's 2nd best healthcare performance.

Life expectancy in Italy is 80 for males and 85 for females, placing the country 5th in the world for life expectancy. In comparison to other Western countries, Italy has a relatively low rate of adult obesity (below 10%), as there are several health benefits of the Mediterranean diet. The proportion of daily smokers was 22% in 2012, down from 24.4% in 2000 but still slightly above the OECD average. Smoking in public places including bars, restaurants, night clubs and offices has been restricted to specially ventilated rooms since 2005. In 2013, UNESCO added the Mediterranean diet to the Representative List of the Intangible Cultural Heritage of Humanity of Italy (promoter), Morocco, Spain, Portugal, Greece, Cyprus and Croatia.

Divided by politics and geography for centuries until its eventual unification in 1861, Italy's culture has been shaped by a multitude of regional customs and local centres of power and patronage. Italy has had a central role in Western culture for centuries and is still recognised for its cultural traditions and artists. During the Middle Ages and the Renaissance, a number of magnificent courts competed for attracting the best architects, artists and scholars, thus producing a great legacy of monuments, paintings, music and literature. Despite the political and social isolation of these courts, Italy's contribution to the cultural and historical heritage of Europe and the world remain immense.

Italy has more UNESCO World Heritage Sites (55) than any other country in the world, and has rich collections of art, culture and literature from many periods. The country has had a broad cultural influence worldwide, also because numerous Italians emigrated to other places during the Italian diaspora. Furthermore, Italy has, overall, an estimated 100,000 monuments of any sort (museums, palaces, buildings, statues, churches, art galleries, villas, fountains, historic houses and archaeological remains), and according to some estimates the nation is home to half the world's great art treasures.

Italy is known for its considerable architectural achievements, such as the construction of arches, domes and similar structures during ancient Rome, the founding of the Renaissance architectural movement in the late-14th to 16th centuries, and being the homeland of Palladianism, a style of construction which inspired movements such as that of Neoclassical architecture, and influenced the designs which noblemen built their country houses all over the world, notably in the UK, Australia and the US during the late 17th to early 20th centuries.

Along with pre-historic architecture, the first people in Italy to truly begin a sequence of designs were the Greeks and the Etruscans, progressing to classical Roman, then to the revival of the classical Roman era during the Renaissance and evolving into the Baroque era. The Christian concept of a Basilica, a style of church architecture that came to dominate the early Middle Ages, was invented in Rome. They were known for being long, rectangular buildings, which were built in an almost ancient Roman style, often rich in mosaics and decorations. The early Christians' art and architecture was also widely inspired by that of the pagan Romans; statues, mosaics and paintings decorated all their churches. The first significant buildings in the medieval Romanesque style were churches built in Italy during the 800's. Byzantine architecture was also widely diffused in Italy. The Byzantines kept Roman principles of architecture and art alive, and the most famous structure from this period is the Basilica of St. Mark in Venice.

The Romanesque movement, which went from approximately 800 AD to 1100 AD, was one of the most fruitful and creative periods in Italian architecture, when several masterpieces, such as the Leaning Tower of Pisa in the Piazza dei Miracoli, and the Basilica of Sant'Ambrogio in Milan were built. It was known for its usage of the Roman arches, stained glass windows, and also its curved columns which commonly featured in cloisters. The main innovation of Italian Romanesque architecture was the vault, which had never been seen before in the history of Western architecture.

The greatest flowering of Italian architecture took place during the Renaissance. Filippo Brunelleschi made great contributions to architectural design with his dome for the Cathedral of Florence, a feat of engineering that had not been accomplished since antiquity. A popular achievement of Italian Renaissance architecture was St. Peter's Basilica, originally designed by Donato Bramante in the early 16th century. Also, Andrea Palladio influenced architects throughout western Europe with the villas and palaces he designed in the middle and late 16th century; the city of Vicenza, with its twenty-three buildings designed by Palladio, and twenty-four Palladian Villas of the Veneto are listed by UNESCO as part of a World Heritage Site named City of Vicenza and the Palladian Villas of the Veneto.

The Baroque period produced several outstanding Italian architects in the 17th century, especially known for their churches. The most original work of all late Baroque and Rococo architecture is the Palazzina di caccia di Stupinigi, dating back to the 18th century. Luigi Vanvitelli began in 1752 the construction of the Royal Palace of Caserta. In this large complex, the grandiose Baroque style interiors and gardens are opposed to a more sober building envelope. In the late 18th and early 19th centuries Italy was affected by the Neoclassical architectural movement. Everything from villas, palaces, gardens, interiors and art began to be based on Roman and Greek themes.

During the Fascist period, the so-called "Novecento movement" flourished, based on the rediscovery of imperial Rome, with figures such as Gio Ponti and Giovanni Muzio. Marcello Piacentini, responsible for the urban transformations of several cities in Italy and remembered for the disputed Via della Conciliazione in Rome, devised a form of simplified Neoclassicism.

The history of Italian visual arts is significant to the history of Western painting. Roman art was influenced by Greece and can in part be taken as a descendant of ancient Greek painting. Roman painting does have its own unique characteristics. The only surviving Roman paintings are wall paintings, many from villas in Campania, in Southern Italy. Such paintingS can be grouped into four main "styles" or periods and may contain the first examples of trompe-l'œil, pseudo-perspective, and pure landscape.

Panel painting becomes more common during the Romanesque period, under the heavy influence of Byzantine icons. Towards the middle of the 13th century, Medieval art and Gothic painting became more realistic, with the beginnings of interest in the depiction of volume and perspective in Italy with Cimabue and then his pupil Giotto. From Giotto onwards, the treatment of composition by the best painters also became much more free and innovative. The two are considered to be the two great medieval masters of painting in western culture.

The Italian Renaissance is said by many to be the golden age of painting; roughly spanning the 14th through the mid-17th centuries with a significant influence also out of the borders of modern Italy. In Italy artists like Paolo Uccello, Fra Angelico, Masaccio, Piero della Francesca, Andrea Mantegna, Filippo Lippi, Giorgione, Tintoretto, Sandro Botticelli, Leonardo da Vinci, Michelangelo Buonarroti, Raphael, Giovanni Bellini, and Titian took painting to a higher level through the use of perspective, the study of human anatomy and proportion, and through their development of an unprecedented refinement in drawing and painting techniques. Michelangelo was an active sculptor from about 1500 to 1520, and his great masterpieces including his "David", "Pietà", "Moses". Other prominent Renaissance sculptors include Lorenzo Ghiberti, Luca Della Robbia, Donatello, Filippo Brunelleschi and Andrea del Verrocchio.

In the 15th and 16th centuries, the High Renaissance gave rise to a stylised art known as Mannerism. In place of the balanced compositions and rational approach to perspective that characterised art at the dawn of the 16th century, the Mannerists sought instability, artifice, and doubt. The unperturbed faces and gestures of Piero della Francesca and the calm Virgins of Raphael are replaced by the troubled expressions of Pontormo and the emotional intensity of El Greco.

In the 17th century, among the greatest painters of Italian Baroque are Caravaggio, Annibale Carracci, Artemisia Gentileschi, Mattia Preti, Carlo Saraceni and Bartolomeo Manfredi. Subsequently, in the 18th century, Italian Rococo was mainly inspired by French Rococo, since France was the founding nation of that particular style, with artists such as Giovanni Battista Tiepolo and Canaletto. Italian Neoclassical sculpture focused, with Antonio Canova's nudes, on the idealist aspect of the movement.

In the 19th century, major Italian Romantic painters were Francesco Hayez, Giuseppe Bezzuoli and Francesco Podesti. Impressionism was brought from France to Italy by the "Macchiaioli", led by Giovanni Fattori, and Giovanni Boldini; Realism by Gioacchino Toma and Giuseppe Pellizza da Volpedo. In the 20th century, with Futurism, primarily through the works of Umberto Boccioni and Giacomo Balla, Italy rose again as a seminal country for artistic evolution in painting and sculpture. Futurism was succeeded by the metaphysical paintings of Giorgio de Chirico, who exerted a strong influence on the Surrealists and generations of artists to follow like Bruno Caruso and Renato Guttuso.

Formal Latin literature began in 240 BC, when the first stage play was performed in Rome. Latin literature was, and still is, highly influential in the world, with numerous writers, poets, philosophers, and historians, such as Pliny the Elder, Pliny the Younger, Virgil, Horace, Propertius, Ovid and Livy. The Romans were also famous for their oral tradition, poetry, drama and epigrams. In early years of the 13th century, St. Francis of Assisi was considered the first Italian poet by literary critics, with his religious song "Canticle of the Sun".

Another Italian voice originated in Sicily. At the court of Emperor Frederick II, who ruled the Sicilian kingdom during the first half of the 13th century, lyrics modelled on Provençal forms and themes were written in a refined version of the local vernacular. The most important of these poets was the notary Giacomo da Lentini, inventor of the sonnet form, though the most famous early sonneteer was Petrarch.

Guido Guinizelli is considered the founder of the "Dolce Stil Novo", a school that added a philosophical dimension to traditional love poetry. This new understanding of love, expressed in a smooth, pure style, influenced Guido Cavalcanti and the Florentine poet Dante Alighieri, who established the basis of the modern Italian language; his greatest work, the "Divine Comedy", is considered among the foremost literary statements produced in Europe during the Middle Ages; furthermore, the poet invented the difficult "terza rima". The two great writers of the 14th century, Petrarch and Giovanni Boccaccio, sought out and imitated the works of antiquity and cultivated their own artistic personalities. Petrarch achieved fame through his collection of poems, "Il Canzoniere". Petrarch's love poetry served as a model for centuries. Equally influential was Boccaccio's "The Decameron", one of the most popular collections of short stories ever written.

Italian Renaissance authors produced a number of important works. Niccolò Machiavelli's "The Prince" is one of the world's most famous essays on political science and modern philosophy, in which the "effectual truth" is taken to be more important than any abstract ideal. Another important work of the period, Ludovico Ariosto's "Orlando Furioso", continuation of Matteo Maria Boiardo's unfinished romance "Orlando Innamorato", is perhaps the greatest chivalry poem ever written. Baldassare Castiglione's dialogue "The Book of the Courtier" describes the ideal of the perfect court gentleman and of spiritual beauty. The lyric poet Torquato Tasso in "Jerusalem Delivered" wrote a Christian epic, making use of the "ottava rima", with attention to the Aristotelian canons of unity.

Giovanni Francesco Straparola and Giambattista Basile, which have written "The Facetious Nights of Straparola" (1550–1555) and the "Pentamerone" (1634) respectively, printed some of the first known versions of fairy tales in Europe. In the early 17th century, some literary masterpieces were created, such as Giambattista Marino's long mythological poem, "L'Adone". The Baroque period also produced the clear scientific prose of Galileo as well as Tommaso Campanella's "The City of the Sun", a description of a perfect society ruled by a philosopher-priest. At the end of the 17th century, the Arcadians began a movement to restore simplicity and classical restraint to poetry, as in Metastasio's heroic melodramas. In the 18th century, playwright Carlo Goldoni created full written plays, many portraying the middle class of his day.

The Romanticism coincided with some ideas of the "Risorgimento", the patriotic movement that brought Italy political unity and freedom from foreign domination. Italian writers embraced Romanticism in the early 19th century. The time of Italy's rebirth was heralded by the poets Vittorio Alfieri, Ugo Foscolo, and Giacomo Leopardi. The works by Alessandro Manzoni, the leading Italian Romantic, are a symbol of the Italian unification for their patriotic message and because of his efforts in the development of the modern, unified Italian language; his novel "The Betrothed" was the first Italian historical novel to glorify Christian values of justice and Providence, and it has been called the most famous and widely read novel in the Italian language.

In the late 19th century, a realistic literary movement called "Verismo" played a major role in Italian literature; Giovanni Verga and Luigi Capuana were its main exponents. In the same period, Emilio Salgari, writer of action adventure swashbucklers and a pioneer of science fiction, published his "Sandokan" series. In 1883, Carlo Collodi also published the novel "The Adventures of Pinocchio", the most celebrated children's classic by an Italian author and the most translated non-religious book in the world. A movement called Futurism influenced Italian literature in the early 20th century. Filippo Tommaso Marinetti wrote "Manifesto of Futurism", called for the use of language and metaphors that glorified the speed, dynamism, and violence of the machine age.

Modern literary figures and Nobel laureates are Gabriele D'Annunzio from 1889 to 1910, nationalist poet Giosuè Carducci in 1906, realist writer Grazia Deledda in 1926, modern theatre author Luigi Pirandello in 1936, short stories writer Italo Calvino in 1960, poets Salvatore Quasimodo in 1959 and Eugenio Montale in 1975, Umberto Eco in 1980, and satirist and theatre author Dario Fo in 1997.

Prominent Italian philosophers include Cesare Beccaria, Giordano Bruno, Benedetto Croce, Marsilio Ficino, and Giambattista Vico.

Italian theatre can be traced back to the Roman tradition. The theatre of ancient Rome was a thriving and diverse art form, ranging from festival performances of street theatre, nude dancing, and acrobatics, to the staging of Plautus's broadly appealing situation comedies, to the high-style, verbally elaborate tragedies of Seneca. Although Rome had a native tradition of performance, the Hellenization of Roman culture in the 3rd century BCE had a profound and energising effect on Roman theatre and encouraged the development of Latin literature of the highest quality for the stage. As with many other literary genres, Roman dramatists was heavily influenced or tended to adapt from the Greek. For example, Seneca's "Phaedra" was based on that of Euripides, and many of the comedies of Plautus were direct translations of works by Menander.

During the 16th century and on into the 18th century, Commedia dell'arte was a form of improvisational theatre, and it is still performed today. Travelling troupes of players would set up an outdoor stage and provide amusement in the form of juggling, acrobatics and, more typically, humorous plays based on a repertoire of established characters with a rough storyline, called "canovaccio". Plays did not originate from written drama but from scenarios called lazzi, which were loose frameworks that provided the situations, complications, and outcome of the action, around which the actors would improvise. The characters of the "commedia" usually represent fixed social types and stock characters, each of which has a distinct costume, such as foolish old men, devious servants, or military officers full of false bravado. The main categories of these characters include servants, old men, lovers, and captains.

Carlo Goldoni, who wrote a few scenarios starting in 1734, superseded the comedy of masks and the comedy of intrigue by representations of actual life and manners through the characters and their behaviours. He rightly maintained that Italian life and manners were susceptible of artistic treatment such as had not been given them before.

The Teatro di San Carlo in Naples is the oldest continuously active venue for public opera in the world, opening in 1737, decades before both the Milan's La Scala and Venice's La Fenice theatres.

From folk music to classical, music has always played an important role in Italian culture. Instruments associated with classical music, including the piano and violin, were invented in Italy, and many of the prevailing classical music forms, such as the symphony, concerto, and sonata, can trace their roots back to innovations of 16th- and 17th-century Italian music.

Italy's most famous composers include the Renaissance composers Palestrina, Monteverdi and Gesualdo, the Baroque composers Scarlatti, Corelli and Vivaldi, the Classical composers Paisiello, Paganini and Rossini, and the Romantic composers Verdi and Puccini. Modern Italian composers such as Berio and Nono proved significant in the development of experimental and electronic music. While the classical music tradition still holds strong in Italy, as evidenced by the fame of its innumerable opera houses, such as "La Scala" of Milan and "San Carlo" of Naples (the oldest continuously active venue for public opera in the world), and performers such as the pianist Maurizio Pollini and tenor Luciano Pavarotti, Italians have been no less appreciative of their thriving contemporary music scene.

Italy is widely known for being the birthplace of opera. Italian opera was believed to have been founded in the early 17th century, in cities such as Mantua and Venice. Later, works and pieces composed by native Italian composers of the 19th and early 20th centuries, such as Rossini, Bellini, Donizetti, Verdi and Puccini, are among the most famous operas ever written and today are performed in opera houses across the world. La Scala operahouse in Milan is also renowned as one of the best in the world. Famous Italian opera singers include Enrico Caruso and Alessandro Bonci.

Introduced in the early 1920s, jazz took a particularly strong foothold in Italy, and remained popular despite the xenophobic cultural policies of the Fascist regime. Today, the most notable centres of jazz music in Italy include Milan, Rome, and Sicily. Later, Italy was at the forefront of the progressive rock and pop movement of the 1970s, with bands like PFM, Banco del Mutuo Soccorso, Le Orme, Goblin, and Pooh. The same period saw diversification in the cinema of Italy, and Cinecittà films included complex scores by composers including Ennio Morricone, Armando Trovaioli, Piero Piccioni and Piero Umiliani. In the early 1980s, the first star to emerge from the Italian hip hop scene was singer Jovanotti. Popular Italian metal bands such as Rhapsody of Fire, Lacuna Coil, Elvenking, Forgotten Tomb, and Fleshgod Apocalypse are also seen as pioneers of various heavy metal subgenres.

Italy was also an important country in the development of disco and electronic music, with Italo disco, known for its futuristic sound and prominent use of synthesisers and drum machines, being one of the earliest electronic dance genres, as well as European forms of disco aside from Euro disco (which later went on to influence several genres such as Eurodance and Nu-disco). By circa 1988, the genre had merged into other forms of European dance and electronic music, such as Italo house, which blended elements of Italo disco with traditional house music; its sound was generally uplifting, and made strong usage of piano melodies. Some bands of this genre are Black Box, East Side Beat, and 49ers. By the latter half of the 1990s, a subgenre of Eurodance known as Italo dance emerged. Taking influences from Italo disco and Italo house, Italo dance generally included synthesizer riffs, a melodic sound, and the usage of vocoders. Notable Italian DJs and remixers include Gabry Ponte (member of the group Eiffel 65), Benny Benassi, Gigi D'Agostino, and the trio Tacabro.

Producers such as Giorgio Moroder, who won three Academy Awards and four Golden Globes for his music, were highly influential in the development of electronic dance music. Today, Italian pop music is represented annually with the Sanremo Music Festival, which served as inspiration for the Eurovision song contest, and the Festival of Two Worlds in Spoleto. Singers such as Mina, Andrea Bocelli, Grammy winner Laura Pausini, Zucchero, Eros Ramazzotti and Tiziano Ferro have attained international acclaim.

The history of Italian cinema began a few months after the Lumière brothers began motion picture exhibitions. The first Italian film was a few seconds, showing Pope Leo XIII giving a blessing to the camera. The Italian film industry was born between 1903 and 1908 with three companies: the Società Italiana Cines, the Ambrosio Film and the Itala Film. Other companies soon followed in Milan and in Naples. In a short time these first companies reached a fair producing quality, and films were soon sold outside Italy. Cinema was later used by Benito Mussolini, who founded Rome's renowned Cinecittà studio for the production of Fascist propaganda until World War II.

After the war, Italian film was widely recognised and exported until an artistic decline around the 1980s. Notable Italian film directors from this period include Vittorio De Sica, Federico Fellini, Sergio Leone, Pier Paolo Pasolini, Luchino Visconti, Michelangelo Antonioni and Roberto Rossellini; some of these are recognised among the greatest and most influential filmmakers of all time. Movies include world cinema treasures such as "Bicycle Thieves", "La dolce vita", "8½", "The Good, the Bad and the Ugly" and "Once Upon a Time in the West". The mid-1940s to the early 1950s was the heyday of neorealist films, reflecting the poor condition of post-war Italy.

As the country grew wealthier in the 1950s, a form of neorealism known as pink neorealism succeeded, and other film genres, such as sword-and-sandal followed as spaghetti westerns, were popular in the 1960s and 1970s. Actresses such as Sophia Loren, Giulietta Masina and Gina Lollobrigida achieved international stardom during this period. Erotic Italian thrillers, or "giallos", produced by directors such as Mario Bava and Dario Argento in the 1970s, also influenced the horror genre worldwide. In recent years, the Italian scene has received only occasional international attention, with movies like "Life Is Beautiful" directed by Roberto Benigni, "" with Massimo Troisi and "The Great Beauty" directed by Paolo Sorrentino.

The aforementioned Cinecittà studio is today the largest film and television production facility in continental Europe and the centre of the Italian cinema, where many of the biggest box office hits are filmed, and one of the biggest production communities in the world. In the 1950s, the number of international productions being made there led to Rome's being dubbed "Hollywood on the Tiber". More than 3,000 productions have been made on its lot, of which 90 received an Academy Award nomination and 47 of these won it, from some cinema classics to recent rewarded features (such as "Roman Holiday", "Ben-Hur", "Cleopatra", "Romeo and Juliet", "The English Patient", "The Passion of the Christ", and "Gangs of New York").

Italy is the most awarded country at the Academy Awards for Best Foreign Language Film, with 14 awards won, 3 Special Awards and 31 nominations. , Italian films have also won 12 Palmes d'Or (the second-most of any country), 11 Golden Lions and 7 Golden Bears.

The most popular sport in Italy is football. Italy's national football team is one of the world's most successful teams with four FIFA World Cup victories (1934, 1938, 1982 and 2006). Italian clubs have won 48 major European trophies, making Italy the second most successful country in European football. Italy's top-flight club football league is named Serie A and is followed by millions of fans around the world.

Other popular team sports in Italy include volleyball, basketball and rugby. Italy's male and female national volleyball teams are often featured among the world's best. The Italian national basketball team's best results were gold at Eurobasket 1983 and EuroBasket 1999, as well as silver at the Olympics in 2004. Lega Basket Serie A is widely considered one of the most competitive in Europe. Rugby union enjoys a good level of popularity, especially in the north of the country. Italy's national team competes in the Six Nations Championship, and is a regular at the Rugby World Cup. Italy ranks as a tier-one nation by World Rugby. The men's volleyball team won three consecutive World Championships (in 1990, 1994, and 1998) and earned the Olympic silver medal in 1996, 2004, and 2016.

Italy has a long and successful tradition in individual sports as well. Bicycle racing is a very familiar sport in the country. Italians have won the UCI World Championships more than any other country, except Belgium. The Giro d'Italia is a cycling race held every May, and constitutes one of the three Grand Tours, along with the Tour de France and the Vuelta a España, each of which last approximately three weeks. Alpine skiing is also a very widespread sport in Italy, and the country is a popular international skiing destination, known for its ski resorts. Italian skiers achieved good results in Winter Olympic Games, Alpine Ski World Cup, and World Championship. Tennis has a significant following in Italy, ranking as the fourth most practised sport in the country. The Rome Masters, founded in 1930, is one of the most prestigious tennis tournaments in the world. Italian professional tennis players won the Davis Cup in 1976 and the Fed Cup in 2006, 2009, 2010 and 2013. Motorsports are also extremely popular in Italy. Italy has won, by far, the most MotoGP World Championships. Italian Scuderia Ferrari is the oldest surviving team in Grand Prix racing, having competed since 1948, and statistically the most successful Formula One team in history with a record of 232 wins.

Historically, Italy has been successful in the Olympic Games, taking part from the first Olympiad and in 47 Games out of 48. Italian sportsmen have won 522 medals at the Summer Olympic Games, and another 106 at the Winter Olympic Games, for a combined total of 628 medals with 235 golds, which makes them the fifth most successful nation in Olympic history for total medals. The country hosted two Winter Olympics and will host a third (in 1956, 2006, and 2026), and one Summer games (in 1960).

Italian fashion has a long tradition, and is regarded as one most important in the world. Milan, Florence and Rome are Italy's main fashion capitals. According to "Top Global Fashion Capital Rankings" 2013 by Global Language Monitor, Rome ranked sixth worldwide when Milan was twelfth. Major Italian fashion labels, such as Gucci, Armani, Prada, Versace, Valentino, Dolce & Gabbana, Missoni, Fendi, Moschino, Max Mara, Trussardi, and Ferragamo, to name a few, are regarded as among the finest fashion houses in the world. Also, the fashion magazine Vogue Italia, is considered one of the most prestigious fashion magazines in the world.

Italy is also prominent in the field of design, notably interior design, architectural design, industrial design and urban design. The country has produced some well-known furniture designers, such as Gio Ponti and Ettore Sottsass, and Italian phrases such as "Bel Disegno" and "Linea Italiana" have entered the vocabulary of furniture design. Examples of classic pieces of Italian white goods and pieces of furniture include Zanussi's washing machines and fridges, the "New Tone" sofas by Atrium, and the post-modern bookcase by Ettore Sottsass, inspired by Bob Dylan's song "Stuck Inside of Mobile with the Memphis Blues Again". Today, Milan and Turin are the nation's leaders in architectural design and industrial design. The city of Milan hosts Fiera Milano, Europe's largest design fair. Milan also hosts major design and architecture-related events and venues, such as the "Fuori Salone" and the Salone del Mobile, and has been home to the designers Bruno Munari, Lucio Fontana, Enrico Castellani and Piero Manzoni.

The Italian cuisine has developed through centuries of social and political changes, with roots as far back as the 4th century BC. Italian cuisine in itself takes heavy influences, including Etruscan, ancient Greek, ancient Roman, Byzantine, and Jewish. Significant changes occurred with the discovery of the New World with the introduction of items such as potatoes, tomatoes, bell peppers and maize, now central to the cuisine but not introduced in quantity until the 18th century. Italian cuisine is noted for its regional diversity, abundance of difference in taste, and is known to be one of the most popular in the world, wielding strong influence abroad.

The Mediterranean diet forms the basis of Italian cuisine, rich in pasta, fish, fruits and vegetables and characterised by its extreme simplicity and variety, with many dishes having only four to eight ingredients. Italian cooks rely chiefly on the quality of the ingredients rather than on elaborate preparation. Dishes and recipes are often derivatives from local and familial tradition rather than created by chefs, so many recipes are ideally suited for home cooking, this being one of the main reasons behind the ever-increasing worldwide popularity of Italian cuisine, from America to Asia. Ingredients and dishes vary widely by region.

A key factor in the success of Italian cuisine is its heavy reliance on traditional products; Italy has the most traditional specialities protected under EU law. Cheese, cold cuts and wine are a major part of Italian cuisine, with many regional declinations and Protected Designation of Origin or Protected Geographical Indication labels, and along with coffee (especially espresso) make up a very important part of the Italian gastronomic culture. Desserts have a long tradition of merging local flavours such as citrus fruits, pistachio and almonds with sweet cheeses like mascarpone and ricotta or exotic tastes as cocoa, vanilla and cinnamon. Gelato, tiramisù and cassata are among the most famous examples of Italian desserts, cakes and patisserie.

Public holidays celebrated in Italy include religious, national and regional observances. Italy's National Day, the "Festa della Repubblica" ("Republic Day") is celebrated on 2 June each year, and commemorates the birth of the Italian Republic in 1946.

The Saint Lucy's Day, which take place on 13 December, is very popular among children in some Italian regions, where she plays a role similar to Santa Claus. In addition, the Epiphany in Italy is associated with the folkloristic figure of the Befana, a broomstick-riding old woman who, in the night between 5 and 6 January, bringing good children gifts and sweets, and bad ones charcoal or bags of ashes. The Assumption of Mary coincides with "Ferragosto" on 15 August, the summer vacation period which may be a long weekend or most of the month. Each city or town also celebrates a public holiday on the occasion of the festival of the local patron saint, for example: Rome on 29 June (Saints Peter and Paul) and Milan on 7 December (Saint Ambrose).

There are many festivals and festivities in Italy. Some of them include the Palio di Siena horse race, Holy Week rites, Saracen Joust of Arezzo, Saint Ubaldo Day in Gubbio, Giostra della Quintana in Foligno, and the Calcio Fiorentino. In 2013, UNESCO has included among the intangible cultural heritage some Italian festivals and pasos (in Italian "macchine a spalla"), such as the Varia di Palmi, the Macchina di Santa Rosa in Viterbo, the Festa dei Gigli in Nola, and "faradda di li candareri" in Sassari.

Other festivals include the carnivals in Venice, Viareggio, Satriano di Lucania, Mamoiada, and Ivrea, mostly known for its Battle of the Oranges. The prestigious Venice International Film Festival, awarding the "Golden Lion" and held annually since 1932, is the oldest film festival in the world.






</doc>
<doc id="58661631" url="https://en.wikipedia.org/wiki?curid=58661631" title="Meridionalism">
Meridionalism

Meridionalism () is the study and research, started in the 19th century and developed over the 20th century, of the economical and social issues of Southern Italy, started right after the annexation of the former Kingdom of the two Sicilies which resulted in the unification of Italy. The study of these issues was not only meant for theoretical research, but also to find a solution and to bridge the economical, social and cultural gap between North Italy and South Italy. The scholars and academics who carried out research in this field are referred to as Meridionalists ().




</doc>
<doc id="62134519" url="https://en.wikipedia.org/wiki?curid=62134519" title="Che vuoi">
Che vuoi

Che vuoi? (?) alternatively described as ma che vuoi?, ma che dici/ma che stai dicendo? (but what are you saying?) or simply Che? is one of the most well known hand gestures in Italy, with gestures being a very common method of communicative expression in Italy. It is made where the tips of all the fingers of one hand are brought together to form an "upward pointing cone", with the hand then being moved up and down either from the wrist or forearm. The hand can be motionless while performing this hand gesture, or can also be shaken up and down, if the person wants to express impatience. While it is particularly common in Naples, it is a gesture that is widely used in Italy. The meaning is open to interpretation based on context but the frequency and speed of vertical motion indicates the level of frustration of the speaker. As "pinched fingers" an image of the hand held in this gesture was approved as part of Unicode 13 in 2020.


</doc>
<doc id="62772391" url="https://en.wikipedia.org/wiki?curid=62772391" title="Fiat 619">
Fiat 619

The Fiat 619 is a multi-purpose truck, tractor for Semi-trailer trucks, manufactured from 1964 to 1980 and part of the Fiat V.I. family of heavy vehicles. The truck was intended to replace the Fiat 682 in production since 1952, but the manufacturing of the latter continued until 1984, also thanks to the demands of the African market. 

The first generation included two versions: the Fiat 619 N, as a chassis truck, and the Fiat 619 T, as a road tractor. At that time the letters after the number indicated "naphtha" for the N and "tractor and semi-trailer" for the T. Both were equipped with the characteristic Fiat "baffo" cabin with the characteristic moustache ("baffo" in Italian) on the front. The Fiat "baffo" cabin would become the emblem of Fiat VI trucks from 1955 to 1974 and was designed to comply with the new Italian Highway Code of 1952, that imposed new rules on the sizes of Vehicles, in accordance with the Geneva Convention (1949) on international transport. In South America, where it was produced until 1994, the Fiat 619 was nicknamed "pico de loro", that is parrot beak, due to the shape of the cabin.

The second generation was launched (Fiat 619 N1 and Fiat 619 T1) from 1970 and was equipped with the more spacious Fiat H cabin, which was then kept until 1991. Initially equipped with an engine of 12,883 cubic centimeters and 210 horsepower, with the N1 and T1 versions it changed to a propulsion unit of 13,798 cubic centimeters and 260 horsepower.

The two vehicles, built in advance of the European Code which stipulated 19 Tonnes as mass limits for two-axle vehicles and 26 tonnes for three-axle vehicles, were quite popular in Northern Europe. This preferential position of foreign markets ended up defining the Fiat 619 as the export truck.

The 619 T1 was equipped with a diesel engine type 221, straight-six engine, with a power of 208 and a displacement of 12883 cc. All combined with an Eaton Fuller gearbox. The maximum speed was 95 km/h. 


</doc>
<doc id="63102299" url="https://en.wikipedia.org/wiki?curid=63102299" title="Dairy farming in Italy">
Dairy farming in Italy

Dairy farming in Italy is an important industry, both for domestic consumption and for exports. Two of the most well-known ranges of dairy products are "gelato" (ice cream) and a wide variety of cheeses, of which many have Protected Designation of Origin under EU law.

Some of the largest companies in the Italian dairy sector are Parmalat, Auricchio, Cielo (company), Gelati Cecchi, and Sterilgarda.

The production of quintessentially Italian cheeses such as Parmigiano-Reggiano, Grana Padano, and mozzarella, depends significantly on immigrant labour. Starting in the 1990s, Indians have come to dominate the labourforce of the Italian dairy industry in a surprising niche market.

Statistics show that most Indians in Italy tend to settle in the north of the country and work in agriculture. The Po Valley is similar in climate to the Punjab, where most of these Sikh workers are from. Their first jobs tend to be directly with the cows and buffalos, as many come from farming families, but some move on to become cheesemakers, which is better paying.

, which "Politico Europe" describes as Italy's most important farming union, and civic authorities in the region acknowledge that the immigrants are indispensable for agriculture in general and the dairy industry in particular. The dairy workers themselves ("bergamini") tend to belong to the Italian General Confederation of Labour.


</doc>
<doc id="63379362" url="https://en.wikipedia.org/wiki?curid=63379362" title="Canto della Verbena">
Canto della Verbena

"'" (Italian, "Song of the Verbena"), officially "'" ("And while Siena sleeps") and also known simply as "la Verbena", is a traditional song from the city of Siena, Italy. Its common title and lyrics refer to the verbena plant, which, according to local folklore, grew within the Piazza del Campo thanks to the presence of the Bottini di Siena, a series of underground tunnels used to provision the city with water in the mediaeval period. Today, "la Verbena" has become a hymn to the city of Siena, and is popularly sung at sporting events like those of the Palio di Siena and of the local Robur Siena football club.

On 12--13 March 2020, a viral tweet circulated that showed a video of residents, who were under generalised quarantine due to the 2020 coronavirus pandemic, singing "la Verbena" together from their windows. This was one of many reports of community music-making that surfaced in Italy during the quarantine.

The first verse of the traditional lyrics is little-known, sung neither during the "palio di Siena" nor on other occasions. The second verse is most well-known and frequently sung outside of the context of the "palio".

During the "palio", the above version is never sung by the residents of the participating contrades; rather, these lyrics remain as a contrade-neutral version of the song. Lyrical variations set to the same melody very commonly come into existence. Often goliardic, these may be battle hymns for one's own contrade or a taunt for an opposing one. "Ad hoc" versions are also frequently heard, such as those referring to a particular event like a victory or a rival contrade's defeat, or even hymns dedicated to a particular jockey or horse.

A common "palio"-specific hymn is sung as follows:

When a contrade is victorious, the last line may instead be sung: "" ("for strength and for love / you have been purged"), a derisive taunt towards a rival contrade that has just been defeated ("purged").


</doc>
<doc id="29313320" url="https://en.wikipedia.org/wiki?curid=29313320" title="Bibliography of Finland">
Bibliography of Finland

This bibliography of Finland is a list of English-language nonfiction books which have been described by reliable sources as in some way directly relating to the subject of Finland, its history, geography, culture, people, etc.



</doc>
<doc id="43974055" url="https://en.wikipedia.org/wiki?curid=43974055" title="Journal of Finnish Studies">
Journal of Finnish Studies

The Journal of Finnish Studies is a double-blind, peer-reviewed journal that publishes scholarly articles about Finland for an international audience. The journal was established in 1997 at the University of Toronto by Professor Börje Vähämäki. In 2009, the journal moved to Finlandia University, where it was edited by Professor Beth Virtanen until 2011. 

Since January 2011, the "Journal of Finnish Studies" has been edited in the English Department at Sam Houston State University, by Professor Helena Halmari. The co-editor of the journal is Professor and Dean of Arts Hanna Snellman at the University of Helsinki, the associate editor is Dr. Scott Kaukonen at Sam Houston State University, and the assistant editor is Dr. Hilary Virtanen at Finlandia University. Dr. Sheila Embleton from York University serves as the book review editor. The editorial board includes prominent academics both in Finland and internationally. 

The mission of the "Journal of Finnish Studies" is to publish scholarly articles about topics related to Finland and the Finnish diaspora for the international, English-speaking audience. Each volume includes two issues, and the journal also publishes guest-edited theme issues from time to time. 




</doc>
<doc id="10577" url="https://en.wikipedia.org/wiki?curid=10577" title="Finland">
Finland

Finland ( ;  , ), officially the Republic of Finland (, ), is a Nordic country in Northern Europe bordering the Baltic Sea, Gulf of Bothnia, and Gulf of Finland, between Sweden to the west, Russia to the east, Estonia to the south, and north-eastern Norway to the north. The capital and largest city is Helsinki. Other major cities are Espoo, Tampere, Vantaa, Oulu, Turku, Jyväskylä, Lahti and Kuopio.

Finland's population is 5.53 million as of March 2020, the majority of whom live in the central and south of the country and speak Finnish, a Finnic language from the Uralic language family, unrelated to the Scandinavian languages. Finland is the eighth-largest country in Europe and the most sparsely populated country in the European Union. It is a parliamentary republic of 310 municipalities, and includes an autonomous region, the Åland Islands. Over 1.4 million people live in the Greater Helsinki metropolitan area, which produces one third of the country's GDP. Swedish is the second official language of Finland, which is mainly spoken in certain coastal areas and on Åland. A large majority of Finns are members of the Evangelical Lutheran Church.

Finland was inhabited when the last ice age ended, approximately 9000 BC. Comb Ceramic culture introduced pottery 5200 BC and Corded Ware culture coincided with the start of agriculture between 3000 and 2500 BC. The Bronze Age and Iron Age were characterised by extensive contacts with other cultures in the Fennoscandian and Baltic regions. At the time Finland had three main cultural areas – Southwest Finland, Tavastia and Karelia. From the late 13th century, Finland gradually became an integral part of Sweden through the Northern Crusades and the Swedish part-colonisation of coastal Finland, a legacy reflected in the prevalence of the Swedish language and its official status. 

In 1809, Finland was incorporated into the Russian Empire as the autonomous Grand Duchy of Finland. In 1906, Finland became the first European state to grant all adult citizens the right to vote, and the first in the world to give all adult citizens the right to run for public office. Following the 1917 Russian Revolution, Finland declared itself independent. In 1918, the fledgling state was divided by civil war, with the Bolshevik-leaning Red Guard, supported by Soviet Russia, fighting the White Guard, supported by the German Empire. After a brief attempt to establish a kingdom, the country became a republic. During World War II, Finland fought the Soviet Union in the Winter War and the Continuation War and lost some land, but maintained independence.

Finland remained largely an agrarian country until the 1950s. After World War II, the war reparations demanded by the Soviet Union forced Finland to industrialise. The country rapidly developed an advanced economy while building an extensive welfare state based on the Nordic model, resulting in widespread prosperity and a high per capita income. Finland is a top performer in numerous metrics of national performance, including education, economic competitiveness, civil liberties, quality of life, and human development. In 2015, Finland was ranked first in the World Human Capital and the Press Freedom Index and as the most stable country in the world during 2011–2016 in the Fragile States Index, and second in the Global Gender Gap Report. It also ranked first on the World Happiness Report report for 2018 and 2019.

Finland joined the United Nations in 1955 and adopted an official policy of neutrality. The Finno-Soviet Treaty of 1948 gave the Soviet Union some leverage in Finnish domestic politics during the Cold War. Finland joined the OECD in 1969, the NATO Partnership for Peace in 1994, the European Union in 1995, the Euro-Atlantic Partnership Council in 1997, and the Eurozone at its inception in 1999.

The earliest written appearance of the name "Finland" is thought to be on three runestones. Two were found in the Swedish province of Uppland and have the inscription "finlonti" (U 582). The third was found in Gotland. It has the inscription "finlandi" (G 319) and dates back to the 13th century. The name can be assumed to be related to the tribe name "Finns", which is mentioned at first known time AD 98 (disputed meaning).

The name "Suomi" () has uncertain origins, but a candidate for a source is the Proto-Baltic word "*źemē", meaning "land". In addition to the close relatives of Finnish (the Finnic languages), this name is also used in the Baltic languages Latvian and Lithuanian. Alternatively, the Indo-European word *"gʰm-on" "man" (cf. Gothic "guma", Latin "homo") has been suggested, being borrowed as *"ćoma". Earlier theories suggested derivation from "suomaa" (fen land) or "suoniemi" (fen cape), but these are now considered outdated. Some have suggested common etymology with "saame" (Sami, a Finno-Ugric people in Lapland) and "Häme" (a province in the inland), but that theory is uncertain.

The earliest attested use of word "Suomi" is in 811 in the Royal Frankish Annals where it is used as a person name connected to a peace treaty.

In the earliest historical sources, from the 12th and 13th centuries, the term Finland refers to the coastal region around Turku from Perniö to Uusikaupunki. This region later became known as Finland Proper in distinction from the country name Finland. Finland became a common name for the whole country in a centuries-long process that started when the Catholic Church established a missionary diocese in Nousiainen in the northern part of the province of Suomi possibly sometime in the 12th century.

The devastation of Finland during the Great Northern War (1714–1721) and during the Russo-Swedish War (1741–1743) caused Sweden to begin carrying out major efforts to defend its eastern half from Russia. These 18th-century experiences created a sense of a shared destiny that when put in conjunction with the unique Finnish language, led to the adoption of an expanded concept of Finland.

If the archeological finds from Wolf Cave are the result of Neanderthals' activities, the first people inhabited Finland approximately 120,000–130,000 years ago. The area that is now Finland was settled in, at the latest, around 8,500 BC during the Stone Age towards the end of the last glacial period. The artifacts the first settlers left behind present characteristics that are shared with those found in Estonia, Russia, and Norway. The earliest people were hunter-gatherers, using stone tools.

The first pottery appeared in 5200 BC, when the Comb Ceramic culture was introduced. The arrival of the Corded Ware culture in Southern coastal Finland between 3000 and 2500 BC may have coincided with the start of agriculture. Even with the introduction of agriculture, hunting and fishing continued to be important parts of the subsistence economy.

In the Bronze Age permanent all-year-round cultivation and animal husbandry spread, but the cold climate phase slowed the change. Cultures in Finland shared common features in pottery and also axes had similarities but local features existed. Seima-Turbino-phenomenon brought first bronze artifacts to the region and possibly also the Finno-Ugric-Languages. Commercial contacts that had so far mostly been to Estonia started to extend to Scandinavia. Domestic manufacture of bronze artifacts started 1300 BC with Maaninka-type bronze axes. Bronze was imported from Volga region and from Southern Scandinavia.

In the Iron Age population grew especially in Häme and Savo regions. Finland proper was the most densely populated area. Cultural contacts to the Baltics and Scandinavia became more frequent. Commercial contacts in the Baltic Sea region grew and extended during the 8th and 9th centuries.

Main exports from Finland were furs, slaves, castoreum, and falcons to European courts. Imports included silk and other fabrics, jewelry, Ulfberht swords, and, in lesser extent, glass. Production of iron started approximately in 500 BC.

At the end of the 9th century, indigenous artifact culture, especially women's jewelry and weapons, had more common local features than ever before. This has been interpreted to be expressing common Finnish identity which was born from an image of common origin.

An early form of Finnic languages spread to the Baltic Sea region approximately 1900 BC with the Seima-Turbino-phenomenon. Common Finnic language was spoken around Gulf of Finland 2000 years ago. The dialects from which the modern-day Finnish language was developed came into existence during the Iron Age. Although distantly related, the Sami retained the hunter-gatherer lifestyle longer than the Finns. The Sami cultural identity and the Sami language have survived in Lapland, the northernmost province, but the Sami have been displaced or assimilated elsewhere.

The 12th and 13th centuries were a violent time in the northern Baltic Sea. The Livonian Crusade was ongoing and the Finnish tribes such as the Tavastians and Karelians were in frequent conflicts with Novgorod and with each other. Also, during the 12th and 13th centuries several crusades from the Catholic realms of the Baltic Sea area were made against the Finnish tribes. According to historical sources, Danes waged two crusades on Finland, in 1191 and in 1202, and Swedes, possibly the so-called second crusade to Finland, in 1249 against Tavastians and the third crusade to Finland in 1293 against the Karelians. The so-called first crusade to Finland, possibly in 1155, is most likely an unreal event. Also, it is possible that Germans made violent conversion of Finnish pagans in the 13th century. According to a papal letter from 1241, the king of Norway was also fighting against "nearby pagans" at that time.

As a result of the crusades and the colonisation of some Finnish coastal areas with Christian Swedish population during the Middle Ages, Finland gradually became part of the kingdom of Sweden and the sphere of influence of the Catholic Church. Due to the Swedish conquest, the Finnish upper class lost its position and lands to the new Swedish and German nobility and to the Catholic Church. In Sweden even in the 17th and 18th centuries, it was clear that Finland was a conquered country and its inhabitants could be treated arbitrarily. Swedish kings visited Finland rarely and in Swedish contemporary texts Finns were portrayed to be primitive and their language inferior.

Swedish became the dominant language of the nobility, administration, and education; Finnish was chiefly a language for the peasantry, clergy, and local courts in predominantly Finnish-speaking areas. During the Protestant Reformation, the Finns gradually converted to Lutheranism.

In the 16th century, Mikael Agricola published the first written works in Finnish. The first university in Finland, the Royal Academy of Turku, was established in 1640. Finland suffered a severe famine in 1696–1697, during which about one third of the Finnish population died, and a devastating plague a few years later.

In the 18th century, wars between Sweden and Russia twice led to the occupation of Finland by Russian forces, times known to the Finns as the Greater Wrath (1714–1721) and the Lesser Wrath (1742–1743). It is estimated that almost an entire generation of young men was lost during the Great Wrath, due mainly to the destruction of homes and farms, and to the burning of Helsinki. By this time Finland was the predominant term for the whole area from the Gulf of Bothnia to the Russian border.

Two Russo-Swedish wars in twenty-five years served as reminders to the Finnish people of the precarious position between Sweden and Russia. An increasingly vocal elite in Finland soon determined that Finnish ties with Sweden were becoming too costly, and following Russo-Swedish War (1788–1790), the Finnish elite's desire to break with Sweden only heightened.

Even before the war there were conspiring politicians, among them Col G. M. Sprengtporten, who had supported Gustav III's coup in 1772. Sprengporten fell out with the king and resigned his commission in 1777. In the following decade he tried to secure Russian support for an autonomous Finland, and later became an adviser to Catherine II. In the spirit of the notion of Adolf Ivar Arwidsson (1791–1858), "we are not Swedes, we do not want to become Russians, let us therefore be Finns", the Finnish national identity started to become established.

Notwithstanding the efforts of Finland's elite and nobility to break ties with Sweden, there was no genuine independence movement in Finland until the early 20th century. As a matter of fact, at this time the Finnish peasantry was outraged by the actions of their elite and almost exclusively supported Gustav's actions against the conspirators. (The High Court of Turku condemned Sprengtporten as a traitor c. 1793.) The Swedish era ended in the Finnish War in 1809.

On 29 March 1809, having been taken over by the armies of Alexander I of Russia in the Finnish War, Finland became an autonomous Grand Duchy in the Russian Empire until the end of 1917. In 1811, Alexander I incorporated the Russian Vyborg province into the Grand Duchy of Finland. During the Russian era, the Finnish language began to gain recognition. From the 1860s onwards, a strong Finnish nationalist movement known as the Fennoman movement grew. Milestones included the publication of what would become Finland's national epic – the "Kalevala" – in 1835, and the Finnish language's achieving equal legal status with Swedish in 1892.

The Finnish famine of 1866–1868 killed 15% of the population, making it one of the worst famines in European history. The famine led the Russian Empire to ease financial regulations, and investment rose in following decades. Economic and political development was rapid. The gross domestic product (GDP) per capita was still half of that of the United States and a third of that of Britain.

In 1906, universal suffrage was adopted in the Grand Duchy of Finland. However, the relationship between the Grand Duchy and the Russian Empire soured when the Russian government made moves to restrict Finnish autonomy. For example, the universal suffrage was, in practice, virtually meaningless, since the tsar did not have to approve any of the laws adopted by the Finnish parliament. Desire for independence gained ground, first among radical liberals and socialists.

After the 1917 February Revolution, the position of Finland as part of the Russian Empire was questioned, mainly by Social Democrats. Since the head of state was the tsar of Russia, it was not clear who the chief executive of Finland was after the revolution. The Parliament, controlled by social democrats, passed the so-called Power Act to give the highest authority to the Parliament. This was rejected by the Russian Provisional Government which decided to dissolve the Parliament.

New elections were conducted, in which right-wing parties won with a slim majority. Some social democrats refused to accept the result and still claimed that the dissolution of the parliament (and thus the ensuing elections) were extralegal. The two nearly equally powerful political blocs, the right-wing parties and the social democratic party, were highly antagonized.

The October Revolution in Russia changed the geopolitical situation anew. Suddenly, the right-wing parties in Finland started to reconsider their decision to block the transfer of highest executive power from the Russian government to Finland, as the Bolsheviks took power in Russia. Rather than acknowledge the authority of the "Power Act" of a few months earlier, the right-wing government declared independence on 6 December 1917.
On 27 January 1918, the official opening shots of the war were fired in two simultaneous events. The government started to disarm the Russian forces in Pohjanmaa, and the Social Democratic Party staged a coup. The latter gained control of southern Finland and Helsinki, but the white government continued in exile from Vaasa. This sparked the brief but bitter civil war. The Whites, who were supported by Imperial Germany, prevailed over the Reds. After the war, tens of thousands of Reds and suspected sympathizers were interned in camps, where thousands died by execution or from malnutrition and disease. Deep social and political enmity was sown between the Reds and Whites and would last until the Winter War and beyond. The civil war and activist expeditions into Soviet Russia strained Eastern relations.

After a brief experimentation with monarchy, Finland became a presidential republic, with Kaarlo Juho Ståhlberg elected as its first president in 1919. The Finnish–Russian border was determined by the Treaty of Tartu in 1920, largely following the historic border but granting Pechenga () and its Barents Sea harbour to Finland. Finnish democracy did not see any Soviet coup attempts and survived the anti-Communist Lapua Movement. The relationship between Finland and the Soviet Union was tense. Army officers were trained in France, and relations with Western Europe and Sweden were strengthened.

In 1917, the population was 3 million. Credit-based land reform was enacted after the civil war, increasing the proportion of capital-owning population. About 70% of workers were occupied in agriculture and 10% in industry. The largest export markets were the United Kingdom and Germany.

Finland fought the Soviet Union in the Winter War of 1939–1940 after the Soviet Union attacked Finland and in the Continuation War of 1941–1944, following Operation Barbarossa, when Finland aligned with Germany following Germany's invasion of the Soviet Union. For 872 days, the German army, aided indirectly by Finnish forces, besieged Leningrad, the USSR's second-largest city. After resisting a major Soviet offensive in June/July 1944 led to a standstill, Finland reached an armistice with the Soviet Union. This was followed by the Lapland War of 1944–1945, when Finland fought retreating German forces in northern Finland.

The treaties signed in 1947 and 1948 with the Soviet Union included Finnish obligations, restraints, and reparations—as well as further Finnish territorial concessions in addition to those in the Moscow Peace Treaty of 1940. As a result of the two wars, Finland ceded most of Finnish Karelia, Salla, and Petsamo, which amounted to 10% of its land area and 20% of its industrial capacity, including the ports of Vyborg (Viipuri) and the ice-free Liinakhamari (Liinahamari). Almost the whole population, some 400,000 people, fled these areas. The former Finnish territory now constitutes part of Russia's Republic of Karelia. Finland was never occupied by Soviet forces and it retained its independence, but at a loss of about 93,000 soldiers.

Finland rejected Marshall aid, in apparent deference to Soviet desires. However, the United States provided secret development aid and helped the Social Democratic Party, in hopes of preserving Finland's independence. Establishing trade with the Western powers, such as the United Kingdom, and paying reparations to the Soviet Union produced a transformation of Finland from a primarily agrarian economy to an industrialised one. Valmet was founded to create materials for war reparations. After the reparations had been paid off, Finland continued to trade with the Soviet Union in the framework of bilateral trade.
In 1950, 46% of Finnish workers worked in agriculture and a third lived in urban areas. The new jobs in manufacturing, services, and trade quickly attracted people to the towns. The average number of births per woman declined from a baby boom peak of 3.5 in 1947 to 1.5 in 1973. When baby-boomers entered the workforce, the economy did not generate jobs quickly enough, and hundreds of thousands emigrated to the more industrialized Sweden, with emigration peaking in 1969 and 1970. The 1952 Summer Olympics brought international visitors. Finland took part in trade liberalization in the World Bank, the International Monetary Fund and the General Agreement on Tariffs and Trade.

Officially claiming to be neutral, Finland lay in the grey zone between the Western countries and the Soviet Union. The YYA Treaty (Finno-Soviet Pact of Friendship, Cooperation and Mutual Assistance) gave the Soviet Union some leverage in Finnish domestic politics. This was extensively exploited by president Urho Kekkonen against his opponents. He maintained an effective monopoly on Soviet relations from 1956 on, which was crucial for his continued popularity. In politics, there was a tendency of avoiding any policies and statements that could be interpreted as anti-Soviet. This phenomenon was given the name "Finlandization" by the West German press.

Despite close relations with the Soviet Union, Finland maintained a market economy. Various industries benefited from trade privileges with the Soviets, which explains the widespread support that pro-Soviet policies enjoyed among business interests in Finland. Economic growth was rapid in the postwar era, and by 1975 Finland's GDP per capita was the 15th-highest in the world. In the 1970s and '80s, Finland built one of the most extensive welfare states in the world. Finland negotiated with the European Economic Community (EEC, a predecessor of the European Union) a treaty that mostly abolished customs duties towards the EEC starting from 1977, although Finland did not fully join. In 1981, President Urho Kekkonen's failing health forced him to retire after holding office for 25 years.

Finland reacted cautiously to the collapse of the Soviet Union, but swiftly began increasing integration with the West. On 21 September 1990, Finland unilaterally declared the Paris Peace Treaty obsolete, following the German reunification decision nine days earlier.

Miscalculated macroeconomic decisions, a banking crisis, the collapse of its largest trading partner (the Soviet Union), and a global economic downturn caused a deep early 1990s recession in Finland. The depression bottomed out in 1993, and Finland saw steady economic growth for more than ten years. Like other Nordic countries, Finland decentralised its economy since the late 1980s. Financial and product market regulation were loosened. Some state enterprises have been privatized and there have been some modest tax cuts. Finland joined the European Union in 1995, and the Eurozone in 1999. Much of the late 1990s economic growth was fueled by the success of the mobile phone manufacturer Nokia, which held a unique position of representing 80% of the market capitalization of the Helsinki Stock Exchange.

Lying approximately between latitudes 60° and 70° N, and longitudes 20° and 32° E, Finland is one of the world's northernmost countries. Of world capitals, only Reykjavík lies more to the north than Helsinki. The distance from the southernmost point – Hanko in Uusimaa – to the northernmost – Nuorgam in Lapland – is .

Finland has about 168,000 lakes (of area larger than ) and 179,000 islands. Its largest lake, Saimaa, is the fourth largest in Europe. The Finnish Lakeland is the area with the most lakes in the country. The greatest concentration of islands is found in the southwest, in the Archipelago Sea between continental Finland and the main island of Åland.

Much of the geography of Finland is a result of the Ice Age. The glaciers were thicker and lasted longer in Fennoscandia compared with the rest of Europe. Their eroding effects have left the Finnish landscape mostly flat with few hills and fewer mountains. Its highest point, the Halti at , is found in the extreme north of Lapland at the border between Finland and Norway. The highest mountain whose peak is entirely in Finland is Ridnitšohkka at 1,316 m (4,318 ft), directly adjacent to Halti.

The retreating glaciers have left the land with morainic deposits in formations of eskers. These are ridges of stratified gravel and sand, running northwest to southeast, where the ancient edge of the glacier once lay. Among the biggest of these are the three Salpausselkä ridges that run across southern Finland.

Having been compressed under the enormous weight of the glaciers, terrain in Finland is rising due to the post-glacial rebound. The effect is strongest around the Gulf of Bothnia, where land steadily rises about a year. As a result, the old sea bottom turns little by little into dry land: the surface area of the country is expanding by about annually. Relatively speaking, Finland is rising from the sea.

The landscape is covered mostly by coniferous taiga forests and fens, with little cultivated land. Of the total area 10% is lakes, rivers and ponds, and 78% forest. The forest consists of pine, spruce, birch, and other species. Finland is the largest producer of wood in Europe and among the largest in the world. The most common type of rock is granite. It is a ubiquitous part of the scenery, visible wherever there is no soil cover. Moraine or till is the most common type of soil, covered by a thin layer of humus of biological origin. Podzol profile development is seen in most forest soils except where drainage is poor. Gleysols and peat bogs occupy poorly drained areas.

Phytogeographically, Finland is shared between the Arctic, central European, and northern European provinces of the Circumboreal Region within the Boreal Kingdom. According to the WWF, the territory of Finland can be subdivided into three ecoregions: the Scandinavian and Russian taiga, Sarmatic mixed forests, and Scandinavian Montane Birch forest and grasslands. Taiga covers most of Finland from northern regions of southern provinces to the north of Lapland. On the southwestern coast, south of the Helsinki-Rauma line, forests are characterized by mixed forests, that are more typical in the Baltic region. In the extreme north of Finland, near the tree line and Arctic Ocean, Montane Birch forests are common.
Similarly, Finland has a diverse and extensive range of fauna. There are at least sixty native mammalian species, 248 breeding bird species, over 70 fish species, and 11 reptile and frog species present today, many migrating from neighboring countries thousands of years ago.
Large and widely recognized wildlife mammals found in Finland are the brown bear (the national animal), gray wolf, wolverine, and elk. Three of the more striking birds are the whooper swan, a large European swan and the national bird of Finland; the Western capercaillie, a large, black-plumaged member of the grouse family; and the Eurasian eagle-owl. The latter is considered an indicator of old-growth forest connectivity, and has been declining because of landscape fragmentation. The most common breeding birds are the willow warbler, common chaffinch, and redwing. Of some seventy species of freshwater fish, the northern pike, perch, and others are plentiful. Atlantic salmon remains the favourite of fly rod enthusiasts.

The endangered Saimaa ringed seal, one of only three lake seal species in the world, exists only in the Saimaa lake system of southeastern Finland, down to only 390 seals today. It has become the emblem of the Finnish Association for Nature Conservation.

The main factor influencing Finland's climate is the country's geographical position between the 60th and 70th northern parallels in the Eurasian continent's coastal zone. In the Köppen climate classification, the whole of Finland lies in the boreal zone, characterized by warm summers and freezing winters. Within the country, the temperateness varies considerably between the southern coastal regions and the extreme north, showing characteristics of both a maritime and a continental climate. Finland is near enough to the Atlantic Ocean to be continuously warmed by the Gulf Stream. The Gulf Stream combines with the moderating effects of the Baltic Sea and numerous inland lakes to explain the unusually warm climate compared with other regions that share the same latitude, such as Alaska, Siberia, and southern Greenland.

Winters in southern Finland (when mean daily temperature remains below ) are usually about 100 days long, and in the inland the snow typically covers the land from about late November to April, and on the coastal areas such as Helsinki, snow often covers the land from late December to late March. Even in the south, the harshest winter nights can see the temperatures fall to although on coastal areas like Helsinki, temperatures below are rare. Climatic summers (when mean daily temperature remains above ) in southern Finland last from about late May to mid-September, and in the inland, the warmest days of July can reach over . Although most of Finland lies on the taiga belt, the southernmost coastal regions are sometimes classified as hemiboreal.

In northern Finland, particularly in Lapland, the winters are long and cold, while the summers are relatively warm but short. The most severe winter days in Lapland can see the temperature fall down to . The winter of the north lasts for about 200 days with permanent snow cover from about mid-October to early May. Summers in the north are quite short, only two to three months, but can still see maximum daily temperatures above during heat waves. No part of Finland has Arctic tundra, but Alpine tundra can be found at the fells Lapland.

The Finnish climate is suitable for cereal farming only in the southernmost regions, while the northern regions are suitable for animal husbandry.

A quarter of Finland's territory lies within the Arctic Circle and the midnight sun can be experienced for more days the farther north one travels. At Finland's northernmost point, the sun does not set for 73 consecutive days during summer, and does not rise at all for 51 days during winter.

Finland consists of 19 regions, called in Finnish and in Swedish. The regions are governed by regional councils which serve as forums of cooperation for the municipalities of a region. The main tasks of the regions are regional planning and development of enterprise and education. In addition, the public health services are usually organized on the basis of regions. Currently, the only region where a popular election is held for the council is Kainuu. Other regional councils are elected by municipal councils, each municipality sending representatives in proportion to its population.

In addition to inter-municipal cooperation, which is the responsibility of regional councils, each region has a state Employment and Economic Development Centre which is responsible for the local administration of labour, agriculture, fisheries, forestry, and entrepreneurial affairs. The Finnish Defence Forces regional offices are responsible for the regional defence preparations and for the administration of conscription within the region.

Regions represent dialectal, cultural, and economic variations better than the former provinces, which were purely administrative divisions of the central government. Historically, regions are divisions of historical provinces of Finland, areas which represent dialects and culture more accurately.

Six Regional State Administrative Agencies were created by the state of Finland in 2010, each of them responsible for one of the regions called in Finnish and in Swedish; in addition, Åland was designated a seventh region. These take over some of the tasks of the earlier Provinces of Finland ("lääni"/"län"), which were abolished.

The region of Eastern Uusimaa (Itä-Uusimaa) was consolidated with Uusimaa on 1 January 2011.

The fundamental administrative divisions of the country are the municipalities, which may also call themselves towns or cities. They account for half of public spending. Spending is financed by municipal income tax, state subsidies, and other revenue. , there are 310 municipalities, and most have fewer than 6,000 residents.

In addition to municipalities, two intermediate levels are defined. Municipalities co-operate in seventy sub-regions and nineteen regions. These are governed by the member municipalities and have only limited powers. The autonomous province of Åland has a permanent democratically elected regional council. Sami people have a semi-autonomous Sami native region in Lapland for issues on language and culture.

In the following chart, the number of inhabitants includes those living in the entire municipality ("kunta/kommun"), not just in the built-up area. The land area is given in km², and the density in inhabitants per km² (land area). The figures are as of . The capital region – comprising Helsinki, Vantaa, Espoo and Kauniainen – forms a continuous conurbation of over 1.1 million people. However, common administration is limited to voluntary cooperation of all municipalities, e.g. in Helsinki Metropolitan Area Council.

The Constitution of Finland defines the political system; Finland is a parliamentary republic within the framework of a representative democracy. The Prime Minister is the country's most powerful person. The current version of the constitution was enacted on 1 March 2000, and was amended on 1 March 2012. Citizens can run and vote in parliamentary, municipal, presidential and European Union elections.

The head of state of Finland is President of the Republic of Finland (in Finnish: "Suomen tasavallan presidentti"; in Swedish: "Republiken Finlands president"). Finland has had for most of its independence a semi-presidential system, but in the last few decades the powers of the President have been diminished. In constitution amendments, which came into effect in 1991 or 1992 and also with a new drafted constitution of 2000, amended in 2012, the President's position has become primarily a ceremonial office. However, the President still leads the nation's foreign politics together with the Council of State and is the commander-in-chief of the Defence Forces. The position still does entail some powers, including responsibility for foreign policy (excluding affairs related to the European Union) in cooperation with the cabinet, being the head of the armed forces, some decree and pardoning powers, and some appointive powers. Direct, one- or two-stage elections are used to elect the president for a term of six years and for a maximum of two consecutive terms. The current president is Sauli Niinistö; he took office on 1 March 2012. Former presidents were K. J. Ståhlberg (1919–1925), L. K. Relander (1925–1931), P. E. Svinhufvud (1931–1937), Kyösti Kallio (1937–1940), Risto Ryti (1940–1944), C. G. E. Mannerheim (1944–1946), J. K. Paasikivi (1946–1956), Urho Kekkonen (1956–1982), Mauno Koivisto (1982–1994), Martti Ahtisaari (1994–2000), and Tarja Halonen (2000–2012).

The current president was elected from the ranks of the National Coalition Party for the first time since 1946. The presidency between 1946 and the present was instead held by a member of the Social Democratic Party or the Centre Party.

The 200-member unicameral Parliament of Finland (, ) exercises supreme legislative authority in the country. It may alter the constitution and ordinary laws, dismiss the cabinet, and override presidential vetoes. Its acts are not subject to judicial review; the constitutionality of new laws is assessed by the parliament's constitutional law committee. The parliament is elected for a term of four years using the proportional D'Hondt method within a number of multi-seat constituencies through the most open list multi-member districts. Various parliament committees listen to experts and prepare legislation. The speaker of the parliament is Paula Risikko (National Coalition).

Since universal suffrage was introduced in 1906, the parliament has been dominated by the Centre Party (former Agrarian Union), the National Coalition Party, and the Social Democrats. These parties have enjoyed approximately equal support, and their combined vote has totalled about 65–80% of all votes. Their lowest common total of MPs, 121, was reached in the 2011 elections. For a few decades after 1944, the Communists were a strong fourth party. Due to the electoral system of proportional representation, and the relative reluctance of voters to switch their support between parties, the relative strengths of the parties have commonly varied only slightly from one election to another. However, there have been some long-term trends, such as the rise and fall of the Communists during the Cold War; the steady decline into insignificance of the Liberals and its predecessors from 1906 to 1980; and the rise of the Green League since 1983. In the 2011 elections, the Finns Party achieved exceptional success, increasing its representation from 5 to 39 seats, surpassing the Centre Party.

The autonomous province of Åland, which forms a federacy with Finland, elects one member to the parliament, who traditionally joins the parliamentary group of the Swedish People's Party of Finland. (The province also holds elections for its own permanent regional council, and in the 2011 elections, Åland Centre was the largest party.)

The Parliament can be dissolved by a recommendation of the Prime Minister, endorsed by the President. This procedure has never been used, although the parliament was dissolved eight times under the pre-2000 constitution, when this action was the sole prerogative of the president.

After the parliamentary elections on 19 April 2015, the seats were divided among eight parties as follows:

After parliamentary elections, the parties negotiate among themselves on forming a new cabinet (the Finnish Government), which then has to be approved by a simple majority vote in the parliament. The cabinet can be dismissed by a parliamentary vote of no confidence, although this rarely happens (the last time in 1957), as the parties represented in the cabinet usually make up a majority in the parliament.

The cabinet exercises most executive powers, and originates most of the bills that the parliament then debates and votes on. It is headed by the Prime Minister of Finland, and consists of him or her, of other ministers, and of the Chancellor of Justice. The current prime minister is Sanna Marin (Social Democratic Party). Each minister heads his or her ministry, or, in some cases, has responsibility for a subset of a ministry's policy. After the prime minister, the most powerful minister is the minister of finance. The incumbent Minister of Finance is Katri Kulmuni.

As no one party ever dominates the parliament, Finnish cabinets are multi-party coalitions. As a rule, the post of prime minister goes to the leader of the biggest party and that of the minister of finance to the leader of the second biggest.

The judicial system of Finland is a civil law system divided between courts with regular civil and criminal jurisdiction and administrative courts with jurisdiction over litigation between individuals and the public administration. Finnish law is codified and based on Swedish law and in a wider sense, civil law or Roman law. The court system for civil and criminal jurisdiction consists of local courts ("käräjäoikeus", "tingsrätt"), regional appellate courts ("hovioikeus", "hovrätt"), and the Supreme Court ("korkein oikeus", "högsta domstolen"). The administrative branch of justice consists of administrative courts ("hallinto-oikeus", "förvaltningsdomstol") and the Supreme Administrative Court ("korkein hallinto-oikeus", "högsta förvaltningsdomstolen"). In addition to the regular courts, there are a few special courts in certain branches of administration. There is also a High Court of Impeachment for criminal charges against certain high-ranking officeholders.

Around 92% of residents have confidence in Finland's security institutions. The overall crime rate of Finland is not high in the EU context. Some crime types are above average, notably the high homicide rate for Western Europe. A day fine system is in effect and also applied to offenses such as speeding.

Finland has successfully fought against government corruption, which was more common in the 1970s and '80s. For instance, economic reforms and EU membership introduced stricter requirements for open bidding and many public monopolies were abolished. Today, Finland has a very low number of corruption charges; Transparency International ranks Finland as one of the least corrupt countries in Europe.

In 2008, Transparency International criticized the lack of transparency of the system of Finnish political finance. According to GRECO in 2007, corruption should be taken into account in the Finnish system of election funds better. A scandal revolving around campaign finance of the 2007 parliamentary elections broke out in spring 2008. Nine Ministers of Government submitted incomplete funding reports and even more of the members of parliament. The law includes no punishment of false funds reports of the elected politicians.

According to the 2012 constitution, the president (currently Sauli Niinistö) leads foreign policy in cooperation with the government, except that the president has no role in EU affairs.

In 2008, president Martti Ahtisaari was awarded the Nobel Peace Prize. Finland was considered a cooperative model state, and Finland did not oppose proposals for a common EU defence policy. This was reversed in the 2000s, when Tarja Halonen and Erkki Tuomioja made Finland's official policy to resist other EU members' plans for common defence.

Finland has one of the world's most extensive welfare systems, one that guarantees decent living conditions for all residents: Finns, and non-citizens. Since the 1980s the social security has been cut back, but still the system is one of the most comprehensive in the world. Created almost entirely during the first three decades after World War II, the social security system was an outgrowth of the traditional Nordic belief that the state was not inherently hostile to the well-being of its citizens, but could intervene benevolently on their behalf. According to some social historians, the basis of this belief was a relatively benign history that had allowed the gradual emergence of a free and independent peasantry in the Nordic countries and had curtailed the dominance of the nobility and the subsequent formation of a powerful right wing. Finland's history has been harsher than the histories of the other Nordic countries, but not harsh enough to bar the country from following their path of social development.

The Finnish Defence Forces consist of a cadre of professional soldiers (mainly officers and technical personnel), currently serving conscripts, and a large reserve. The standard readiness strength is 34,700 people in uniform, of which 25% are professional soldiers. A universal male conscription is in place, under which all male Finnish nationals above 18 years of age serve for 6 to 12 months of armed service or 12 months of civilian (non-armed) service.
Voluntary post-conscription overseas peacekeeping service is popular, and troops serve around the world in UN, NATO, and EU missions. Approximately 500 women choose voluntary military service every year. Women are allowed to serve in all combat arms including front-line infantry and special forces.
The army consists of a highly mobile field army backed up by local defence units. The army defends the national territory and its military strategy employs the use of the heavily forested terrain and numerous lakes to wear down an aggressor, instead of attempting to hold the attacking army on the frontier.

Finnish defence expenditure per capita is one of the highest in the European Union. The Finnish military doctrine is based on the concept of total defence. The term total means that all sectors of the government and economy are involved in the defence planning. The armed forces are under the command of the Chief of Defence (currently General Jarmo Lindberg), who is directly subordinate to the president in matters related to military command. The branches of the military are the army, the navy, and the air force. The border guard is under the Ministry of the Interior but can be incorporated into the Defence Forces when required for defence readiness.

Even while Finland hasn't joined the North Atlantic Treaty Organization, the country has joined the NATO Response Force, the EU Battlegroup, the NATO Partnership for Peace and in 2014 signed a NATO memorandum of understanding, thus forming a practical coalition. In 2015, the Finland-NATO ties were strengthened with a host nation support agreement allowing assistance from NATO troops in emergency situations. Finland has been an active participant in the Afghanistan and Kosovo. Recently Finland has been more eager to discuss about its current and planned roles in Syria, Iraq and war against ISIL. On 21 December 2012 Finnish military officer Atte Kaleva was reported to have been kidnapped and later released in Yemen for ransom. At first he was reported be a casual Arabic student, however only later it was published that his studies were about jihadists, terrorism, and that he was employed by the military. As response to French request for solidarity, Finnish defence minister commented in November that Finland could and is willing to offer intelligence support.

In May 2015, Finnish Military sent nearly one million letters to all relevant males in the country, informing them about their roles in the war effort. It was globally speculated that Finland was preparing for war—however Finland claimed that this was a standard procedure, yet something never done before in Finnish history. Assistant chief of staff Hannu Hyppönen however said that this is not an isolated case, but bound to the European security dilemma. The NATO Memorandum of Understanding signed earlier bestows an obligation e.g. to report on internal capabilities and the availability thereof to NATO.

The economy of Finland has a per capita output equal to that of other European economies such as those of France, Germany, Belgium, or the UK. The largest sector of the economy is the service sector at 66% of GDP, followed by manufacturing and refining at 31%. Primary production represents 2.9%. With respect to foreign trade, the key economic sector is manufacturing. The largest industries in 2007 were electronics (22%); machinery, vehicles, and other engineered metal products (21.1%); forest industry (13%); and chemicals (11%). The gross domestic product peaked in 2008. , the country's economy is at the 2006 level.

Finland has significant timber, mineral (iron, chromium, copper, nickel, and gold), and freshwater resources. Forestry, paper factories, and the agricultural sector (on which taxpayers spend around 3 billion euros annually) are important for rural residents so any policy changes affecting these sectors are politically sensitive for politicians dependent on rural votes. The Greater Helsinki area generates around one third of Finland's GDP. In a 2004 OECD comparison, high-technology manufacturing in Finland ranked second largest after Ireland. Knowledge-intensive services have also resulted in the smallest and slow-growth sectors – especially agriculture and low-technology manufacturing – being ranked the second largest after Ireland. The overall short-term outlook was good and GDP growth has been above that of many EU peers.

Finland is highly integrated into the global economy, and international trade produces one third of GDP. Trade with the European Union makes up 60% of Finland's total trade. The largest trade flows are with Germany, Russia, Sweden, the United Kingdom, the United States, the Netherlands, and China. Trade policy is managed by the European Union, where Finland has traditionally been among the free trade supporters, except for agricultural policy. Finland is the only Nordic country to have joined the Eurozone.

Finland's climate and soils make growing crops a particular challenge. The country lies between the latitudes 60°N and 70°N, and it has severe winters and relatively short growing seasons that are sometimes interrupted by frost. However, because the Gulf Stream and the North Atlantic Drift Current moderate the climate, Finland contains half of the world's arable land north of 60° north latitude. Annual precipitation is usually sufficient, but it occurs almost exclusively during the winter months, making summer droughts a constant threat. In response to the climate, farmers have relied on quick-ripening and frost-resistant varieties of crops, and they have cultivated south-facing slopes as well as richer bottomlands to ensure production even in years with summer frosts. Most farmland was originally either forest or swamp, and the soil has usually required treatment with lime and years of cultivation to neutralize excess acid and to improve fertility. Irrigation has generally not been necessary, but drainage systems are often needed to remove excess water. Finland's agriculture has been efficient and productive—at least when compared with farming in other European countries.

Forests play a key role in the country's economy, making it one of the world's leading wood producers and providing raw materials at competitive prices for the crucial wood-processing industries. As in agriculture, the government has long played a leading role in forestry, regulating tree cutting, sponsoring technical improvements, and establishing long-term plans to ensure that the country's forests continue to supply the wood-processing industries. To maintain the country's comparative advantage in forest products, Finnish authorities moved to raise lumber output toward the country's ecological limits. In 1984, the government published the Forest 2000 plan, drawn up by the Ministry of Agriculture and Forestry. The plan aimed at increasing forest harvests by about 3% per year, while conserving forestland for recreation and other uses.

Private sector employees amount to 1.8 million, out of which around a third with tertiary education. The average cost of a private sector employee per hour was 25.1 euro in 2004. , average purchasing power-adjusted income levels are similar to those of Italy, Sweden, Germany, and France. In 2006, 62% of the workforce worked for enterprises with less than 250 employees and they accounted for 49% of total business turnover and had the strongest rate of growth. The female employment rate is high. Gender segregation between male-dominated professions and female-dominated professions is higher than in the US. The proportion of part-time workers was one of the lowest in OECD in 1999. In 2013, the 10 largest private sector employers in Finland were Itella, Nokia, OP-Pohjola, ISS, VR, Kesko, UPM-Kymmene, YIT, Metso, and Nordea.

The unemployment rate was 9.4% in 2015, having risen from 8.7% in 2014. Youth unemployment rate rose from 16.5% in 2007 to 20.5% in 2014. A fifth of residents are outside the job market at the age of 50 and less than a third are working at the age of 61. As of today, nearly one million people are living with minimal wages or unemployed not enough to cover their costs of living.

, 2.4 million households reside in Finland. The average size is 2.1 persons; 40% of households consist of a single person, 32% two persons and 28% three or more persons. Residential buildings total 1.2 million, and the average residential space is per person. The average residential property without land costs 1,187 euro per sq metre and residential land 8.6 euro per sq metre. 74% of households had a car. There are 2.5 million cars and 0.4 million other vehicles.

Around 92% have a mobile phone and 83.5% (2009) Internet connection at home. The average total household consumption was 20,000 euro, out of which housing consisted of about 5,500 euro, transport about 3,000 euro, food and beverages excluding alcoholic beverages at around 2,500 euro, and recreation and culture at around 2,000 euro. According to Invest in Finland, private consumption grew by 3% in 2006 and consumer trends included durables, high quality products, and spending on well-being.

In 2017, Finland's GDP reached EUR 224 billion. However, second quarter of 2018 saw a slow economic growth. Unemployment rate fell to a near one-decade low in June, marking private consumption growth much higher.

Finland has the highest concentration of cooperatives relative to its population. The largest retailer, which is also the largest private employer, S-Group, and the largest bank, OP-Group in the country are both cooperatives.

The free and largely privately owned financial and physical Nordic energy markets traded in NASDAQ OMX Commodities Europe and Nord Pool Spot exchanges, have provided competitive prices compared with other EU countries. , Finland has roughly the lowest industrial electricity prices in the EU-15 (equal to France).

In 2006, the energy market was around 90 terawatt hours and the peak demand around 15 gigawatts in winter. This means that the energy consumption per capita is around 7.2 tons of oil equivalent per year. Industry and construction consumed 51% of total consumption, a relatively high figure reflecting Finland's industries. Finland's hydrocarbon resources are limited to peat and wood. About 10–15% of the electricity is produced by hydropower, which is low compared with more mountainous Sweden or Norway. In 2008, renewable energy (mainly hydropower and various forms of wood energy) was high at 31% compared with the EU average of 10.3% in final energy consumption. Russia supplies more than 75% of Finland's oil imports and 100% of total gas imports.

Finland has four privately owned nuclear reactors producing 18% of the country's energy and one research reactor (decommissioned 2018 ) at the Otaniemi campus. The fifth AREVA-Siemens-built reactor – the world's largest at 1600 MWe and a focal point of Europe's nuclear industry – has faced many delays and is currently scheduled to be operational by 2018–2020, a decade after the original planned opening. A varying amount (5–17%) of electricity has been imported from Russia (at around 3 gigawatt power line capacity), Sweden and Norway.

The Onkalo spent nuclear fuel repository is currently under construction at the Olkiluoto Nuclear Power Plant in the municipality of Eurajoki, on the west coast of Finland, by the company Posiva. Energy companies are about to increase nuclear power production, as in July 2010 the Finnish parliament granted permits for additional two new reactors.

The extensive road system is utilized by most internal cargo and passenger traffic. The annual state operated road network expenditure of around 1 billion euro is paid with vehicle and fuel taxes which amount to around 1.5 billion euro and 1 billion euro.

The main international passenger gateway is Helsinki Airport with about 17 million passengers in 2016. Oulu Airport is the second largest, whilst another 25 airports have scheduled passenger services. The Helsinki Airport-based Finnair, Blue1, and Nordic Regional Airlines, Norwegian Air Shuttle sell air services both domestically and internationally. Helsinki has an optimal location for great circle (i.e. the shortest and most efficient) routes between Western Europe and the Far East.

Despite low population density, the Government spends annually around 350 million euro in maintaining of railway tracks. Rail transport is handled by state owned VR, which has 5% passenger market share (out of which 80% are urban trips in Greater Helsinki) and 25% cargo market share. Since 12 December 2010, Karelian Trains, a joint venture between Russian Railways and VR (Finnish Railways), has been running Alstom Pendolino operated high-speed services between Saint Petersburg's Finlyandsky and Helsinki's Central railway stations. These services are branded as "Allegro" trains. The journey from Helsinki to Saint Petersburg takes only three and a half hours. A high-speed rail line is planned between Helsinki and Turku, with a line from the capital to Tampere also proposed.

The majority of international cargo utilizes ports. Port logistics prices are low. Vuosaari Harbour in Helsinki is the largest container port after completion in 2008 and others include Kotka, Hamina, Hanko, Pori, Rauma, and Oulu. There is passenger traffic from Helsinki and Turku, which have ferry connections to Tallinn, Mariehamn, Stockholm and Travemünde. The Helsinki-Tallinn route - one of the busiest passenger sea routes in the world - has also been served by a helicopter line, and the Helsinki-Tallinn Tunnel is proposed to provide rail travel between the two cities.

Finland was rapidly industrialized after World War II, achieving GDP per capita levels equal to that of Japan or the UK in the beginning of the 1970s. Initially, most development was based on two broad groups of export-led industries, the "metal industry" ("metalliteollisuus") and "forest industry" ("metsäteollisuus"). The "metal industry" includes shipbuilding, metalworking, the car industry, engineered products such as motors and electronics, and production of metals (steel, copper and chromium). The world's biggest cruise ships are built in Finnish shipyards. The "forest industry" includes forestry, timber, pulp and paper, and is a logical development based on Finland's extensive forest resources (77% of the area is covered by forest, most of it in renewable use). In the pulp and paper industry, many of the largest companies are based in Finland (Ahlstrom, Metsä Board, and UPM). However, the Finnish economy has diversified, with expansion into fields such as electronics (e.g. Nokia), metrology (Vaisala), transport fuels (Neste), chemicals (Kemira), engineering consulting (Pöyry), and information technology (e.g. Rovio Entertainment, known for Angry Birds), and is no longer dominated by the two sectors of metal and forest industry. Likewise, the structure has changed, with the service sector growing, with manufacturing reducing in importance; agriculture is only a minor part. Despite this, production for export is still more prominent than in Western Europe, thus making Finland more vulnerable to global economic trends.

In an Economist Intelligence Unit report released in September 2011, Finland clinched the second place after the United States on Benchmarking IT Industry Competitiveness 2011 which scored on 6 key indicators: overall business environment, technology infrastructure, human capital, legal framework, public support for industry development, and research and development landscape.

Finnish politicians have often emulated other Nordics and the Nordic model. Nordics have been free-trading and relatively welcoming to skilled migrants for over a century, though in Finland immigration is relatively new. The level of protection in commodity trade has been low, except for agricultural products.

Finland has top levels of economic freedom in many areas. Finland is ranked 16th in the 2008 global Index of Economic Freedom and 9th in Europe. While the manufacturing sector is thriving, the OECD points out that the service sector would benefit substantially from policy improvements.

The 2007 IMD World Competitiveness Yearbook ranked Finland 17th most competitive. The World Economic Forum 2008 index ranked Finland the 6th most competitive. In both indicators, Finland's performance was next to Germany, and significantly higher than most European countries. In the Business competitiveness index 2007–2008 Finland ranked third in the world.

Economists attribute much growth to reforms in the product markets. According to the OECD, only four EU-15 countries have less regulated product markets (UK, Ireland, Denmark and Sweden) and only one has less regulated financial markets (Denmark). Nordic countries were pioneers in liberalizing energy, postal, and other markets in Europe. The legal system is clear and business bureaucracy less than most countries. Property rights are well protected and contractual agreements are strictly honoured. Finland is rated the least corrupt country in the world in the Corruption Perceptions Index and 13th in the Ease of doing business index. This indicates exceptional ease in cross-border trading (5th), contract enforcement (7th), business closure (5th), tax payment (83rd), and low worker hardship (127th).

Finnish law forces all workers to obey the national contracts that are drafted every few years for each profession and seniority level. The agreement becomes universally enforceable provided that more than 50% of the employees support it, in practice by being a member of a relevant trade union. The unionization rate is high (70%), especially in the middle class (AKAVA—80%). A lack of a national agreement in an industry is considered an exception.

In 2017, tourism in Finland grossed approximately €15.0 billion with a 7% increase from the previous year. Of this, €4.6 billion (30%) came from foreign tourism. In 2017, there were 15.2 million overnight stays of domestic tourists and 6.7 million overnight stays of foreign tourists. Much of the sudden growth can be attributed to the globalisation of the country as well as a rise in positive publicity and awareness. While Russia remains the largest market for foreign tourists, the biggest growth came from Chinese markets (35%). Tourism contributes roughly 2.7% to Finland's GDP, making it comparable to agriculture and forestry.

Commercial cruises between major coastal and port cities in the Baltic region, including Helsinki, Turku, Tallinn, Stockholm, and Travemünde, play a significant role in the local tourism industry. By passenger counts, the Port of Helsinki is the busiest port in the world.

Lapland has the highest tourism consumption of any Finnish region. Above the Arctic Circle, in midwinter, there is a polar night, a period when the sun does not rise for days or weeks, or even months, and correspondingly, midnight sun in the summer, with no sunset even at midnight (for up to 73 consecutive days, at the northernmost point). Lapland is so far north that the aurora borealis, fluorescence in the high atmosphere due to solar wind, is seen regularly in the fall, winter, and spring. Finnish Lapland is also locally regarded as the home of Saint Nicholas or Santa Claus, with several theme parks, such as Santa Claus Village and Santa Park in Rovaniemi. 

Tourist attractions in Finland include the natural landscape found throughout the country as well as urban attractions. Finland is covered with thick pine forests, rolling hills, and lakes. Finland contains 40 national parks, from the Southern shores of the Gulf of Finland to the high fells of Lapland. Outdoor activities range from Nordic skiing, golf, fishing, yachting, lake cruises, hiking, and kayaking, among many others. Bird-watching is popular for those fond of avifauna, however hunting is also popular. Elk and hare are common game in Finland. Finland also has urbanised regions with many cultural events and activities. Tourist attractions in Helsinki include the Helsinki Cathedral and the Suomenlinna sea fortress. Olavinlinna in Savonlinna hosts the annual Savonlinna Opera Festival.

The population of Finland is currently about 5.5 million inhabitants and is aging with the birth rate at 10.42 births per 1,000 population per year, or a fertility rate of 1.49 children born per woman, one of the lowest in the world, below the replacement rate of 2.1, it remains considerably below the high of 5.17 children born per woman in 1887. Finland subsequently has one of the oldest populations in the world, with the average age of 42.6 years. Approximately half of voters are estimated to be over 50 years old. Finland has an average population density of 18 inhabitants per square kilometre. This is the third-lowest population density of any European country, behind those of Norway and Iceland, and the lowest population density in the EU. Finland's population has always been concentrated in the southern parts of the country, a phenomenon that became even more pronounced during 20th-century urbanisation. Two of the three largest cities in Finland are situated in the Greater Helsinki metropolitan area—Helsinki and Espoo. Tampere holds the third place while also Helsinki-neighbouring Vantaa is the fourth. Other cities with population over 100,000 are Turku, Oulu, Jyväskylä, Kuopio, and Lahti.

, there were 402,619 people with a foreign background living in Finland (7.3% of the population), most of whom are from Russia, Estonia, Somalia, Iraq and former Yugoslavia. The children of foreigners are not automatically given Finnish citizenship, as Finnish nationality law practices and maintain "jus sanguinis" policy where only children born to at least one Finnish parent are granted citizenship. If they are born in Finland and cannot get citizenship of any other country, they become citizens. Additionally, certain persons of Finnish descent who reside in countries that were once part of Soviet Union, retain the right of return, a right to establish permanent residency in the country, which would eventually entitle them to qualify for citizenship. 387,215 people in Finland in 2018 were born in another country, representing 7% of the population. The 10 largest foreign born groups are (in order) from Russia, Estonia, Sweden, Iraq, Somalia, China, Thailand, Serbia, Vietnam and Turkey.

The immigrant population is growing quite rapidly. By 2035, the three largest cites in Finland will all have a foreign speaking population percentage of over a quarter each, with Helsinki rising to 26%, Espoo to 30% and Vantaa to 34%. The Helsinki region alone will have 437,000 foreign speakers, up by 236,000.

Finnish and Swedish are the official languages of Finland. Finnish predominates nationwide while Swedish is spoken in some coastal areas in the west and south and in the autonomous region of Åland. The native language of 87.3% of the population is Finnish, which is part of the Finnic subgroup of the Uralic languages. The language is one of only four official EU languages not of Indo-European origin. Finnish is closely related to Karelian and Estonian and more remotely to the Sami languages and Hungarian. Swedish is the native language of 5.2% of the population (Swedish-speaking Finns).

The Nordic languages and Karelian are also specially treated in some contexts.

Finnish Romani is spoken by some 5,000–6,000 people; it and Finnish Sign Language are also recognized in the constitution. There are two sign languages: Finnish Sign Language, spoken natively by 4,000–5,000 people, and Finland-Swedish Sign Language, spoken natively by about 150 people. Tatar is spoken by a Finnish Tatar minority of about 800 people whose ancestors moved to Finland mainly during Russian rule from the 1870s to the 1920s.

The Sami language has an official language status in the north, in Lapland or in northern Lapland, where the Sami people predominate, numbering around 7,000 and recognized as an indigenous people. About a quarter of them speak a Sami language as their mother tongue. The Sami languages that are spoken in Finland are Northern Sami, Inari Sami, and Skolt Sami.

The rights of minority groups (in particular Sami, Swedish speakers, and Romani people) are protected by the constitution.

The largest immigrant languages are Russian (1.4%), Estonian (0.9%), Arabic (0.5%), Somali (0.4%) and English (0.4%). English is studied by most pupils as a compulsory subject from the first grade (at seven years of age) in the comprehensive school (in some schools other languages can be chosen instead). German, French, Spanish and Russian can be studied as second foreign languages from the fourth grade (at 10 years of age; some schools may offer other options).

93% of Finns can speak a second language. The figures in this section should be treated with caution, as they come from the official Finnish population register. People can only register one language and so bilingual or multilingual language users' language competencies are not properly included. A citizen of Finland that speaks bilingually Finnish and Swedish will often be registered as a Finnish only speaker in this system. Similarly "old domestic language" is a category applied to some languages and not others for political not linguistic reasons, for example Russian.

With 3.9 million members, the Evangelical Lutheran Church of Finland is one of the largest Lutheran churches in the world and is also by far Finland's largest religious body; at the end of 2019, 68.7% of Finns were members of the church. The Evangelical Lutheran Church of Finland sees its share of the country's population declining by roughly one percent annually in recent years. The decline has been due to both church membership resignations and falling baptism rates. The second largest group, accounting for 26.3% of the population in 2017, has no religious affiliation. The irreligious group rose quickly from just below 13% in the year 2000. A small minority belongs to the Finnish Orthodox Church (1.1%). Other Protestant denominations and the Roman Catholic Church are significantly smaller, as are the Jewish and other non-Christian communities (totalling 1.6%). The Pew Research Center estimated the Muslim population at 2.7% in 2016. The main Lutheran and Orthodox churches are national churches of Finland with special roles such as in state ceremonies and schools.

In 1869, Finland was the first Nordic country to disestablish its Evangelical Lutheran church by introducing the Church Act, followed by the Church of Sweden in 2000. Although the church still maintains a special relationship with the state, it is not described as a state religion in the Finnish Constitution or other laws passed by the Finnish Parliament. Finland's state church was the Church of Sweden until 1809. As an autonomous Grand Duchy under Russia 1809–1917, Finland retained the Lutheran State Church system, and a state church separate from Sweden, later named the Evangelical Lutheran Church of Finland, was established. It was detached from the state as a separate judicial entity when the new church law came to force in 1869. After Finland had gained independence in 1917, religious freedom was declared in the constitution of 1919 and a separate law on religious freedom in 1922. Through this arrangement, the Evangelical Lutheran Church of Finland lost its position as a state church but gained a constitutional status as a national church alongside the Finnish Orthodox Church, whose position however is not codified in the constitution.

In 2016, 69.3% of Finnish children were baptized and 82.3% were confirmed in 2012 at the age of 15, and over 90% of the funerals are Christian. However, the majority of Lutherans attend church only for special occasions like Christmas ceremonies, weddings, and funerals. The Lutheran Church estimates that approximately 1.8% of its members attend church services weekly. The average number of church visits per year by church members is approximately two.

According to a 2010 Eurobarometer poll, 33% of Finnish citizens responded that "they believe there is a God"; 42% answered that "they believe there is some sort of spirit or life force"; and 22% that "they do not believe there is any sort of spirit, God, or life force". According to ISSP survey data (2008), 8% consider themselves "highly religious", and 31% "moderately religious". In the same survey, 28% reported themselves as "agnostic" and 29% as "non-religious".

Life expectancy has increased from 71 years for men and 79 years for women in 1990 to 79 years for men and 84 years for women in 2017. The under-five mortality rate has decreased from 51 per 1,000 live births in 1950 to 2.3 per 1,000 live births in 2017 ranking Finland's rate among the lowest in the world. The fertility rate in 2014 stood at 1.71 children born/per woman and has been below the replacement rate of 2.1 since 1969. With a low birth rate women also become mothers at a later age, the mean age at first live birth being 28.6 in 2014. A 2011 study published in "The Lancet" medical journal found that Finland had the lowest stillbirth rate out of 193 countries, including the UK, France and New Zealand.

There has been a slight increase or no change in welfare and health inequalities between population groups in the 21st century. Lifestyle-related diseases are on the rise. More than half a million Finns suffer from diabetes, type 1 diabetes being globally the most common in Finland. Many children are diagnosed with type 2 diabetes. The number of musculoskeletal diseases and cancers are increasing, although the cancer prognosis has improved. Allergies and dementia are also growing health problems in Finland. One of the most common reasons for work disability are due to mental disorders, in particular depression. Treatment for depression has improved and as a result the historically high suicide rates have declined to 13 per 100 000 in 2017, closer to the North European average. Suicide rates are still among the highest among developed countries in the OECD.

There are 307 residents for each doctor. About 19% of health care is funded directly by households and 77% by taxation.

In April 2012, Finland was ranked 2nd in Gross National Happiness in a report published by The Earth Institute. Since 2012, Finland has every time ranked at least in the top 5 of world's happiest countries in the annual World Happiness Report by the United Nations, as well as ranking as the happiest country in 2018.

Most pre-tertiary education is arranged at municipal level. Even though many or most schools were started as private schools, today only around 3 percent of students are enrolled in private schools (mostly specialist language and international schools), much less than in Sweden and most other developed countries. Pre-school education is rare compared with other EU countries and formal education is usually started at the age of 7. Primary school takes normally six years and lower secondary school three years. Most schools are managed by municipal officials.

The flexible curriculum is set by the Ministry of Education and the Education Board. Education is compulsory between the ages of 7 and 16. After lower secondary school, graduates may either enter the workforce directly, or apply to trade schools or gymnasiums (upper secondary schools). Trade schools offer a vocational education: approximately 40% of an age group choose this path after the lower secondary school. Academically oriented gymnasiums have higher entrance requirements and specifically prepare for Abitur and tertiary education. Graduation from either formally qualifies for tertiary education.

In tertiary education, two mostly separate and non-interoperating sectors are found: the profession-oriented polytechnics and the research-oriented universities. Education is free and living expenses are to a large extent financed by the government through student benefits. There are 15 universities and 24 Universities of Applied Sciences (UAS) in the country. The University of Helsinki is ranked 75th in the Top University Ranking of 2010. The World Economic Forum ranks Finland's tertiary education No. 1 in the world. Around 33% of residents have a tertiary degree, similar to Nordics and more than in most other OECD countries except Canada (44%), United States (38%) and Japan (37%). The proportion of foreign students is 3% of all tertiary enrollments, one of the lowest in OECD, while in advanced programs it is 7.3%, still below OECD average 16.5%.

More than 30% of tertiary graduates are in science-related fields. Forest improvement, materials research, environmental sciences, neural networks, low-temperature physics, brain research, biotechnology, genetic technology, and communications showcase fields of study where Finnish researchers have had a significant impact.

Finland has a long tradition of adult education, and by the 1980s nearly one million Finns were receiving some kind of instruction each year. Forty percent of them did so for professional reasons. Adult education appeared in a number of forms, such as secondary evening schools, civic and workers' institutes, study centres, vocational course centres, and folk high schools. Study centres allowed groups to follow study plans of their own making, with educational and financial assistance provided by the state. Folk high schools are a distinctly Nordic institution. Originating in Denmark in the 19th century, folk high schools became common throughout the region. Adults of all ages could stay at them for several weeks and take courses in subjects that ranged from handicrafts to economics.

Finland is highly productive in scientific research. In 2005, Finland had the fourth most scientific publications per capita of the OECD countries. In 2007, 1,801 patents were filed in Finland.

In addition, 38 percent of Finland's population has a university or college degree, which is among the highest percentages in the world.

In 2010 a new law was enacted considering the universities, which defined that there are 16 of them as they were excluded from the public sector to be autonomous legal and financial entities, however enjoying special status in the legislation. As result many former state institutions were driven to collect funding from private sector contributions and partnerships. The change caused deep rooted discussions among the academic circles.

English language is important in Finnish education. There are a number of degree programs that are taught in English, which attracts thousands of degree and exchange students every year.

In December 2017 the OECD reported that Finnish fathers spend an average of eight minutes a day more with their school-aged children than mothers do.

Written Finnish could be said to have existed since Mikael Agricola translated the New Testament into Finnish during the Protestant Reformation, but few notable works of literature were written until the 19th century and the beginning of a Finnish national Romantic Movement. This prompted Elias Lönnrot to collect Finnish and Karelian folk poetry and arrange and publish them as the "Kalevala", the Finnish national epic. The era saw a rise of poets and novelists who wrote in Finnish, notably Aleksis Kivi, Eino Leino and Johannes Linnankoski. Many writers of the national awakening wrote in Swedish, such as the national poet Johan Ludvig Runeberg and Zachris Topelius.

After Finland became independent, there was a rise of modernist writers, most famously the Finnish-speaking Mika Waltari and Swedish-speaking Edith Södergran. Frans Eemil Sillanpää was awarded the Nobel Prize in Literature in 1939. World War II prompted a return to more national interests in comparison to a more international line of thought, characterized by Väinö Linna. Besides Kalevala and Waltari, the Swedish-speaking Tove Jansson is the most translated Finnish writer. Popular modern writers include Arto Paasilinna, Ilkka Remes, Kari Hotakainen, Sofi Oksanen, and Jari Tervo, while the best novel is annually awarded the prestigious Finlandia Prize.

The visual arts in Finland started to form their individual characteristics in the 19th century, when Romantic nationalism was rising in autonomic Finland. The best known of Finnish painters, Akseli Gallen-Kallela, started painting in a naturalist style, but moved to national romanticism. Finland's best-known sculptor of the 20th century was Wäinö Aaltonen, remembered for his monumental busts and sculptures. Finns have made major contributions to handicrafts and industrial design: among the internationally renowned figures are Timo Sarpaneva, Tapio Wirkkala and Ilmari Tapiovaara. Finnish architecture is famous around the world, and has contributed significantly to several styles internationally, such as Jugendstil (or Art Nouveau), Nordic Classicism and Functionalism. Among the top 20th-century Finnish architects to gain international recognition are Eliel Saarinen and his son Eero Saarinen. Architect Alvar Aalto is regarded as among the most important 20th-century designers in the world; he helped bring functionalist architecture to Finland, but soon was a pioneer in its development towards an organic style. Aalto is also famous for his work in furniture, lamps, textiles and glassware, which were usually incorporated into his buildings.

Much of Finland's classical music is influenced by traditional Karelian melodies and lyrics, as comprised in the "Kalevala". Karelian culture is perceived as the purest expression of the Finnic myths and beliefs, less influenced by Germanic influence than the Nordic folk dance music that largely replaced the kalevaic tradition. Finnish folk music has undergone a roots revival in recent decades, and has become a part of popular music.

The people of northern Finland, Sweden, and Norway, the Sami, are known primarily for highly spiritual songs called joik. The same word sometimes refers to lavlu or vuelie songs, though this is technically incorrect.

The first Finnish opera was written by the German-born composer Fredrik Pacius in 1852. Pacius also wrote the music to the poem "Maamme/Vårt land" (Our Country), Finland's national anthem. In the 1890s Finnish nationalism based on the "Kalevala" spread, and Jean Sibelius became famous for his vocal symphony "Kullervo". He soon received a grant to study "runo singers" in Karelia and continued his rise as the first prominent Finnish musician. In 1899 he composed "Finlandia", which played its important role in Finland gaining independence. He remains one of Finland's most popular national figures and is a symbol of the nation.

Today, Finland has a very lively classical music scene and many of Finland's important composers are still alive, such as Magnus Lindberg, Kaija Saariaho, Kalevi Aho, and Aulis Sallinen. The composers are accompanied by a large number of great conductors such as Esa-Pekka Salonen, Osmo Vänskä, Jukka-Pekka Saraste, and Leif Segerstam. Some of the internationally acclaimed Finnish classical musicians are Karita Mattila, Soile Isokoski, Pekka Kuusisto, Olli Mustonen, and Linda Lampenius.


"Iskelmä" (coined directly from the German word "Schlager", meaning "hit") is a traditional Finnish word for a light popular song. Finnish popular music also includes various kinds of dance music; tango, a style of Argentine music, is also popular. The light music in Swedish-speaking areas has more influences from Sweden. Modern Finnish popular music includes a number of prominent rock bands, jazz musicians, hip hop performers, dance music acts, etc.

During the early 1960s, the first significant wave of Finnish rock groups emerged, playing instrumental rock inspired by groups such as The Shadows. Around 1964, Beatlemania arrived in Finland, resulting in further development of the local rock scene. During the late 1960s and '70s, Finnish rock musicians increasingly wrote their own music instead of translating international hits into Finnish. During the decade, some progressive rock groups such as Tasavallan Presidentti and Wigwam gained respect abroad but failed to make a commercial breakthrough outside Finland. This was also the fate of the rock and roll group Hurriganes. The Finnish punk scene produced some internationally acknowledged names including Terveet Kädet in the 1980s. Hanoi Rocks was a pioneering 1980s glam rock act that inspired the American hard rock group Guns N' Roses, among others.

Many Finnish metal bands have gained international recognition. HIM and Nightwish are some of Finland's most internationally known bands. HIM's 2005 album "Dark Light" went gold in the United States. Apocalyptica are an internationally famous Finnish group who are most renowned for mixing strings-led classical music with classic heavy metal. Other well-known metal bands are Amorphis, Children of Bodom, Ensiferum, Finntroll, Impaled Nazarene, Insomnium, Korpiklaani, Moonsorrow, Reverend Bizarre, Sentenced, Sonata Arctica, Stratovarius, Swallow the Sun, Turisas, Waltari, and Wintersun.

After Finnish hard rock/heavy metal band Lordi won the 2006 Eurovision Song Contest, Finland hosted the competition in 2007. Alternative rock band Poets of the Fall, formed in 2003, have released eight studio albums and have toured widely.

In the film industry, notable directors include Aki Kaurismäki, Mauritz Stiller, Spede Pasanen, and Hollywood film director and producer Renny Harlin. Around twelve feature films are made each year.

Finland's most internationally successful TV shows are the backpacking travel documentary series "Madventures" and the reality TV show "The Dudesons", about four childhood friends who perform stunts and play pranks on each other (in similar vein to the American TV show "Jackass").

Thanks to its emphasis on transparency and equal rights, Finland's press has been rated the freest in the world.

Today, there are around 200 newspapers, 320 popular magazines, 2,100 professional magazines, 67 commercial radio stations, three digital radio channels and one nationwide and five national public service radio channels.

Each year, around 12,000 book titles are published and 12 million records are sold.

Sanoma publishes the newspaper "Helsingin Sanomat" (its circulation of 412,000 making it the largest), the tabloid "Ilta-Sanomat", the commerce-oriented "Taloussanomat" and the television channel Nelonen. The other major publisher Alma Media publishes over thirty magazines, including the newspaper "Aamulehti", tabloid "Iltalehti" and commerce-oriented "Kauppalehti". Worldwide, Finns, along with other Nordic peoples and the Japanese, spend the most time reading newspapers.

Yle, the Finnish Broadcasting Company, operates five television channels and thirteen radio channels in both national languages. Yle is funded through a mandatory television license and fees for private broadcasters. All TV channels are broadcast digitally, both terrestrially and on cable. The commercial television channel MTV3 and commercial radio channel Radio Nova are owned by Nordic Broadcasting (Bonnier and Proventus Industrier).

In regards to telecommunication infrastructure, Finland is the highest ranked country in the World Economic Forum's Network Readiness Index (NRI) – an indicator for determining the development level of a country's information and communication technologies. Finland ranked 1st overall in the 2014 NRI ranking, unchanged from the year before. This is shown in its penetration throughout the country's population. Around 79% of the population use the Internet. Finland had around 1.52 million broadband Internet connections by the end of June 2007 or around 287 per 1,000 inhabitants. All Finnish schools and public libraries have Internet connections and computers and most residents have a mobile phone. Value-added services are rare. In October 2009, Finland's Ministry of Transport and Communications committed to ensuring that every person in Finland would be able to access the Internet at a minimum speed of one megabit-per-second beginning July 2010.

Finnish cuisine is notable for generally combining traditional country fare and "haute cuisine" with contemporary style cooking. Fish and meat play a prominent role in traditional Finnish dishes from the western part of the country, while the dishes from the eastern part have traditionally included various vegetables and mushrooms. Refugees from Karelia contributed to foods in eastern Finland.

Finnish foods often use wholemeal products (rye, barley, oats) and berries (such as bilberries, lingonberries, cloudberries, and sea buckthorn). Milk and its derivatives like buttermilk are commonly used as food, drink, or in various recipes. Various turnips were common in traditional cooking, but were replaced with the potato after its introduction in the 18th century.

According to the statistics, red meat consumption has risen, but still Finns eat less beef than many other nations, and more fish and poultry. This is mainly because of the high cost of meat in Finland.

Finland has the world's highest per capita consumption of coffee. Milk consumption is also high, at an average of about , per person, per year, even though 17% of the Finns are lactose intolerant.

All official holidays in Finland are established by Acts of Parliament. Christian holidays include Christmas, New Year's Day, Epiphany, Easter, Ascension Day, Pentecost, Midsummer Day (St. John's Day), and All Saints' Day, while secular holidays include May Day, Independence Day, New Year's Day, and Midsummer. Christmas is the most extensively celebrated, and at least 24 to 26 December is taken as a holiday.

Various sporting events are popular in Finland. Pesäpallo, resembling baseball, is the national sport of Finland, although the most popular sports in terms of spectators is ice hockey. Ice Hockey World Championships 2016 final Finland-Canada, 69% of Finnish people watched that game on TV. Other popular sports include athletics, cross-country skiing, ski jumping, football, volleyball and basketball. While ice hockey is the most popular sports when it comes to attendance at games, association football is the most played team sport in terms of the number of players in the country and is also the most appreciated sport in Finland.

In terms of medals and gold medals won per capita, Finland is the best performing country in Olympic history. Finland first participated as a nation in its own right at the Olympic Games in 1908, while still an autonomous Grand Duchy within the Russian Empire. At the 1912 Summer Olympics, great pride was taken in the three gold medals won by the original "Flying Finn" Hannes Kolehmainen.

Finland was one of the most successful countries at the Olympic Games before World War II. At the 1924 Summer Olympics, Finland, a nation then of only 3.2 million people, came second in the medal count. In the 1920s and '30s, Finnish long-distance runners dominated the Olympics, with Paavo Nurmi winning a total of nine Olympic gold medals between 1920 and 1928 and setting 22 official world records between 1921 and 1931. Nurmi is often considered the greatest Finnish sportsman and one of the greatest athletes of all time.

For over 100 years, Finnish male and female athletes have consistently excelled at the javelin throw. The event has brought Finland nine Olympic gold medals, five world championships, five European championships, and 24 world records.

In addition to Kolehmainen and Nurmi, some of Finland's most internationally well-known and successful sportspeople are long-distance runners Ville Ritola and Lasse Virén; ski-jumpers Matti Nykänen and Janne Ahonen; cross-country skiers Veikko Hakulinen, Eero Mäntyranta, Marja-Liisa Kirvesniemi and Mika Myllylä; rower Pertti Karppinen; gymnast Heikki Savolainen; professional skateboarder Arto Saari; ice hockey players Kimmo Timonen, Jari Kurri, Teemu Selänne, and Saku Koivu; football players Jari Litmanen and Sami Hyypiä; basketball player Hanno Möttölä; alpine skiers Kalle Palander and Tanja Poutiainen; Formula One world champions Keke Rosberg, Mika Häkkinen and Kimi Räikkönen; four-time World Rally champions Juha Kankkunen and Tommi Mäkinen; and 13-time World Enduro Champion Juha Salminen, seven-time champion Kari Tiainen, and the five-time champions Mika Ahola, biathlete Kaisa Mäkäräinen and Samuli Aro. Finland is also one of the most successful nations in bandy, being the only nation beside Russia and Sweden to win a Bandy World Championship.

The 1952 Summer Olympics were held in Helsinki. Other notable sporting events held in Finland include the 1983 and 2005 World Championships in Athletics.

Finland also has a notable history in figure skating. Finnish skaters have won 8 world championships and 13 junior world cups in synchronized skating, and Finland is considered one of the best countries at the sport.

Some of the most popular recreational sports and activities include floorball, Nordic walking, running, cycling, and skiing (alpine skiing, cross-country skiing, and ski jumping).
Floorball, in terms of registered players, occupies third place after football and ice hockey. According to the Finnish Floorball Federation, floorball is the most popular school, youth, club and workplace sport. , the total number of licensed players reaches 57,400.

Especially since the 2014 FIBA Basketball World Cup, Finland's national basketball team has received widespread public attention. More than 8,000 Finns travelled to Spain to support their team. Overall, they chartered more than 40 airplanes.


Government

Maps

Travel


</doc>
<doc id="45115728" url="https://en.wikipedia.org/wiki?curid=45115728" title="Harvia">
Harvia

Harvia Plc. is a Finnish heater, sauna, spa and a sauna interiors manufacturer. The company’s product offering covers all three sauna types: traditional sauna, steam sauna and infrared sauna. The company is headquartered in Muurame, Central Finland. Harvia’s products are distributed globally through a network of dealers. Harvia shares (“HARVIA”) are listed on the Nasdaq Helsinki Ltd and are registered in the Finnish Book-Entry Register maintained by Euroclear Finland Ltd.

Tapani Harvia made the first Harvia woodburning stove for his own use in 1950. The first woodburning stove for sale was completed in the mid-1950s. By the end of the decade, Harvia's main product had become the sauna stove. The brand of the heaters was Har-Ve. About 500 heaters were sold annually. In 1958 the company was renamed Takomo T Harvia Ky and in 1961 it was renamed Harvia Ky. Harvia manufactured about a thousand heaters a year and employed five people. Risto Harvia (born 1948) joined the company in 1966 and Kullervo Harvia at the end of the decade.

Harvia moved from Jyväskylä to Muurame in 1972 to its own 500 square meter industrial hall. Pertti Harvia and Sari Harvia-Jyllinmaa joined the company in the mid-1970s. Risto Harvia started as CEO in 1977. By the end of the decade, the production had grown to about 6,000 heaters per year with 12 employees.

Harvia Ky changed from being a limited liability company to Harvia Oy in 1980. Tapani Harvia retired and served as Chairman of the Board. In addition to wood-burning stoves, Harvia began manufacturing electric stoves in the early 1980s. In the end of the decade, the company employed 32 people and manufactured approximately 20,000 wood-fired and 5,000 electric heaters per year.


Harvia shares (“HARVIA”) are listed on the Nasdaq Helsinki Ltd and are registered in the Finnish Book-Entry Register maintained by Euroclear Finland Ltd.

Harvia’s registered share capital was EUR 80,000 and the company had issued 18,549,879 fully paid shares (22.3.2018). Harvia has one share series, and each share entitles the holder to one vote at the company’s General Meeting of shareholders. There are no voting restrictions attached to the shares. The shares have no nominal value. Company’s shares belong to a book-entry system.  At the moment the company does not hold any of its own shares. 

Harvia’s governance and management commits to the Finnish Limited Liability Companies Act and Securities Markets Act, as well as the company’s Articles of Association, the charters of the Board of Directors and Audit Committee of Harvia and the rules and regulations of the Helsinki Stock Exchange, rules and guidelines of the Financial Supervisory Authority as well as the Corporate Governance Code for Finnish Listed Companies set by the Securities Market Association 




Harvia does not publish its short-term outlook. However, the company has set long-term targets related to growth, profitability and leverage. The company targets an average annual revenue growth of more than 5%, adjusted operating profit margin of 20% and a net debt/adjusted EBITDA between 1.5x−2.5x. The future impacts of changes in IFRS reporting standards have been excluded in the net debt/adjusted EBITDA ratio target.

Harvia targets a regularly increasing dividend with a bi-annual dividend payout of at least 60 percent of net income, in total.

During the first quarter of 2019, the demand for sauna and spa products remained steady, and thanks to the systematic implementation of our strategy, we were able to improve our competitiveness. In the sauna and spa market, the first quarter of the year is important and now we faced comparison figures from Harvia’s all-time record quarter in 2018. Revenue for the first quarter increased significantly (16.9%) year on year and amounted to EUR 19.3 million. Organic revenue growth was at a good level (7.5%). Part of our growth is attributable to the acquisition of the business operations of Almost Heaven Saunas (AHS) in the United States at the end of 2018.

Our revenue increased especially in the sauna heater market, with strong growth in electric heaters and particularly in wood-burning heaters. Our new and innovative products had a favorable impact on sales development. Our electric sauna heaters Glow, Cilindro Plus, and The Wall as well as the SENTIO by Harvia products have been well received in the market. In the wood-burning heater product group, one of our largest products, Harvia Pro 20, received its long-awaited update, and we began delivering the renewed version to customers in March. We achieved significant growth also in the sauna room product group, largely thanks to the AHS acquisition.

Demand was good and Harvia’s operations were successful in most of our key markets. Nevertheless, the decrease in demand in the German market that we faced at year end 2018 did not recover in the first quarter of the year. This was reflected in the deliveries of electric heaters and particularly control units.

The profitability of our business remained stable and strong. Harvia’s profitability improved significantly year on year, and the Group’s adjusted operating profit grew to EUR 4.0 million (3.2). Our increased sales volumes, attractive product range and continuous improvement of productivity had a positive impact on our profitability. Our adjusted operating profit margin was 20.6%, showing a clear improvement from the comparison period (19.5%). We continued our investments in improving productivity and we completed investments in new advanced machinery during the first quarter.

Taking over the operations of the US-based Almost Heaven Saunas, acquired at the end of 2018, has proceeded as planned. Among other things, we have renewed AHS’s product range and pricing as well as improved the terms and conditions of purchases. Our revenue in North America increased significantly thanks to the acquisition of Almost Heaven Saunas. Additionally, growth among other customers in North America was exceptionally strong.

Our first quarter of the year was strong, and both Harvia’s team and our key customers have done a very good job. We continue to implement our strategy with determination and focus on increasing the value of the average purchase, geographical expansion as well as the continuing improvement of productivity. In addition to organic growth, Harvia is actively looking into possibilities of growth through business acquisitions.


</doc>
<doc id="63047685" url="https://en.wikipedia.org/wiki?curid=63047685" title="Finnish Customs">
Finnish Customs

The Finnish Customs (, ) is the customs service of Finland. It is a government agency steered by the Ministry of Finance. Finnish Customs is a part of the customs system of the European Union. Finnish Customs has around 1 900 employees.

On February 12 1812, the founding of The General Customs Directorate of the Grand Duchy of Finland was approved by Alexander I of Russia as the Grand Duke of Finland. By the 1850s, customs duties' share of total state tax revenue was over 40 percent. The directorate was renamed the Board of Customs in 1881. The customs service of the Grand Duchy of Finland was autonomous from the customs service of the Russian Empire, and thus the transition to the customs service of an independent Finland in 1917 was smooth.

Customs duties formed the backbone of the Finnish state economy until the 1930s, but the fiscal importance of duties has decreased drastically due to the international reduction or elimination of trade barriers since the 1950s. Finland joined the EU and its Customs Union in 1995, but this caused no significant challenges for Finnish Customs.

The tasks of Finnish Customs include the facilitation of the trade in goods, the protection of society and the environment, and the collection of customs duties, charges and taxes on import goods. It also compiles the official statistics on international trade.


</doc>
<doc id="63390227" url="https://en.wikipedia.org/wiki?curid=63390227" title="Fallbacka">
Fallbacka

Fallbacka Farm was located in Tuusula (Finland) in the area where present-day Vantaa, Kerava and Tuusula meet, by the main railway of Finland, between the stations in Korso and Savio. The farm and its house were inhabited by the farmer family Fallström. There is evidence at least dating back to the 18th century. The main building of the farm, which was at the highest point in the area, that hosted the first school in the area, was demolished in the 1960s. Nowadays, in the area are, for example. Rudolf Steiner School in Vantaa and private residences.

Through the area in the east–west direction there is on the Vantaa side Anttila-road which continues on the Tuusula side as Fallbacka-road, which was the old name for Anttila-road before the area was merged with Vantaa. In the north–south direction the area is divided by the Fallbäcken creek (Vallinoja), which over the years has decreased from a stream to a creek.

Since the 1990s, new construction has been added to the area. The area's western and southwestern part has started to be called Fallbäcken (Vallinoja). The part of the former farm that now belongs to Vantaa is now part of the Korso larger area. A large part constitutes a nature conservation area.

The northern part of the area belongs to "Alikeravan kylä" area and the northwestern part of Tuusula.


</doc>
<doc id="655002" url="https://en.wikipedia.org/wiki?curid=655002" title="Philosophy of space and time">
Philosophy of space and time

Philosophy of space and time is the branch of philosophy concerned with the issues surrounding the ontology, epistemology, and character of space and time. While such ideas have been central to philosophy from its inception, the philosophy of space and time was both an inspiration for and a central aspect of early analytic philosophy. The subject focuses on a number of basic issues, including whether time and space exist independently of the mind, whether they exist independently of one another, what accounts for time's apparently unidirectional flow, whether times other than the present moment exist, and questions about the nature of identity (particularly the nature of identity over time).

The earliest recorded Western philosophy of time was expounded by the ancient Egyptian thinker Ptahhotep (c. 2650–2600 BC) who said:

The "Vedas", the earliest texts on Indian philosophy and Hindu philosophy, dating back to the late 2nd millennium BC, describe ancient Hindu cosmology, in which the universe goes through repeated cycles of creation, destruction, and rebirth, with each cycle lasting 4,320,000 years. Ancient Greek philosophers, including Parmenides and Heraclitus, wrote essays on the nature of time.

Incas regarded space and time as a single concept, named pacha (, ).

Plato, in the "Timaeus", identified time with the period of motion of the heavenly bodies, and space as that in which things come to be. Aristotle, in Book IV of his "Physics", defined time as the number of changes with respect to before and after, and the place of an object as the innermost motionless boundary of that which surrounds it.

In Book 11 of St. Augustine's "Confessions", he ruminates on the nature of time, asking, "What then is time? If no one asks me, I know: if I wish to explain it to one that asketh, I know not." He goes on to comment on the difficulty of thinking about time, pointing out the inaccuracy of common speech: "For but few things are there of which we speak properly; of most things we speak improperly, still the things intended are understood." But Augustine presented the first philosophical argument for the reality of Creation (against Aristotle) in the context of his discussion of time, saying that knowledge of time depends on the knowledge of the movement of things, and therefore time cannot be where there are no creatures to measure its passing (Confessions Book XI ¶30; City of God Book XI ch.6).

In contrast to ancient Greek philosophers who believed that the universe had an infinite past with no beginning, medieval philosophers and theologians developed the concept of the universe having a finite past with a beginning, now known as Temporal finitism. The Christian philosopher John Philoponus presented early arguments, adopted by later Christian philosophers and theologians of the form "argument from the impossibility of the existence of an actual infinite", which states:

In the early 11th century, the Muslim physicist Ibn al-Haytham (Alhacen or Alhazen) discussed space perception and its epistemological implications in his "Book of Optics" (1021). He also rejected Aristotle's definition of "topos" ("Physics" IV) by way of geometric demonstrations and defined place as a mathematical spatial extension. His experimental proof of the intro-mission model of vision led to changes in the understanding of the visual perception of space, contrary to the previous emission theory of vision supported by Euclid and Ptolemy. In "tying the visual perception of space to prior bodily experience, Alhacen unequivocally rejected the intuitiveness of spatial perception and, therefore, the autonomy of vision. Without tangible notions of distance and size for correlation, sight can tell us next to nothing about such things."

A traditional realist position in ontology is that time and space have existence apart from the human mind. Idealists, by contrast, deny or doubt the existence of objects independent of the mind. Some anti-realists, whose ontological position is that objects outside the mind do exist, nevertheless doubt the independent existence of time and space.

In 1781, Immanuel Kant published the "Critique of Pure Reason", one of the most influential works in the history of the philosophy of space and time. He describes time as an "a priori" notion that, together with other "a priori" notions such as space, allows us to comprehend sense experience. Kant holds that neither space nor time are substance, entities in themselves, or learned by experience; he holds, rather, that both are elements of a systematic framework we use to structure our experience. Spatial measurements are used to quantify how far apart objects are, and temporal measurements are used to quantitatively compare the interval between (or duration of) events. Although space and time are held to be "transcendentally ideal" in this sense, they are also "empirically real"—that is, not mere illusions.

Some idealist writers, such as J. M. E. McTaggart in "The Unreality of Time", have argued that time is an illusion (see also The flow of time, below).

The writers discussed here are for the most part realists in this regard; for instance, Gottfried Leibniz held that his monads existed, at least independently of the mind of the observer.

The great debate between defining notions of space and time as real objects themselves (absolute), or mere orderings upon actual objects (relational), began between physicists Isaac Newton (via his spokesman, Samuel Clarke) and Gottfried Leibniz in the papers of the Leibniz–Clarke correspondence.

Arguing against the absolutist position, Leibniz offers a number of thought experiments with the purpose of showing that there is contradiction in assuming the existence of facts such as absolute location and velocity. These arguments trade heavily on two principles central to his philosophy: the principle of sufficient reason and the identity of indiscernibles. The principle of sufficient reason holds that for every fact, there is a reason that is sufficient to explain what and why it is the way it is and not otherwise. The identity of indiscernibles states that if there is no way of telling two entities apart, then they are one and the same thing.

The example Leibniz uses involves two proposed universes situated in absolute space. The only discernible difference between them is that the latter is positioned five feet to the left of the first. The example is only possible if such a thing as absolute space exists. Such a situation, however, is not possible, according to Leibniz, for if it were, a universe's position in absolute space would have no sufficient reason, as it might very well have been anywhere else. Therefore, it contradicts the principle of sufficient reason, and there could exist two distinct universes that were in all ways indiscernible, thus contradicting the identity of indiscernibles.

Standing out in Clarke's (and Newton's) response to Leibniz's arguments is the bucket argument: Water in a bucket, hung from a rope and set to spin, will start with a flat surface. As the water begins to spin in the bucket, the surface of the water will become concave. If the bucket is stopped, the water will continue to spin, and while the spin continues, the surface will remain concave. The concave surface is apparently not the result of the interaction of the bucket and the water, since the surface is flat when the bucket first starts to spin, it becomes concave as the water starts to spin, and it remains concave as the bucket stops.

In this response, Clarke argues for the necessity of the existence of absolute space to account for phenomena like rotation and acceleration that cannot be accounted for on a purely relationalist account. Clarke argues that since the curvature of the water occurs in the rotating bucket as well as in the stationary bucket containing spinning water, it can only be explained by stating that the water is rotating in relation to the presence of some third thing—absolute space.

Leibniz describes a space that exists only as a relation between objects, and which has no existence apart from the existence of those objects. Motion exists only as a relation between those objects. Newtonian space provided the absolute frame of reference within which objects can have motion. In Newton's system, the frame of reference exists independently of the objects contained within it. These objects can be described as moving in relation to space itself. For almost two centuries, the evidence of a concave water surface held authority.

Another important figure in this debate is 19th-century physicist Ernst Mach. While he did not deny the existence of phenomena like that seen in the bucket argument, he still denied the absolutist conclusion by offering a different answer as to what the bucket was rotating in relation to: the fixed stars.

Mach suggested that thought experiments like the bucket argument are problematic. If we were to imagine a universe that only contains a bucket, on Newton's account, this bucket could be set to spin relative to absolute space, and the water it contained would form the characteristic concave surface. But in the absence of anything else in the universe, it would be difficult to confirm that the bucket was indeed spinning. It seems equally possible that the surface of the water in the bucket would remain flat.

Mach argued that, in effect, the water experiment in an otherwise empty universe would remain flat. But if another object were introduced into this universe, perhaps a distant star, there would now be something relative to which the bucket could be seen as rotating. The water inside the bucket could possibly have a slight curve. To account for the curve that we observe, an increase in the number of objects in the universe also increases the curvature in the water. Mach argued that the momentum of an object, whether angular or linear, exists as a result of the sum of the effects of other objects in the universe (Mach's Principle).

Albert Einstein proposed that the laws of physics should be based on the principle of relativity. This principle holds that the rules of physics must be the same for all observers, regardless of the frame of reference that is used, and that light propagates at the same speed in all reference frames. This theory was motivated by Maxwell's equations, which show that electromagnetic waves propagate in a vacuum at the speed of light. However, Maxwell's equations give no indication of what this speed is relative to. Prior to Einstein, it was thought that this speed was relative to a fixed medium, called the luminiferous ether. In contrast, the theory of special relativity postulates that light propagates at the speed of light in all inertial frames, and examines the implications of this postulate.

All attempts to measure any speed relative to this ether failed, which can be seen as a confirmation of Einstein's postulate that light propagates at the same speed in all reference frames. Special relativity is a formalization of the principle of relativity that does not contain a privileged inertial frame of reference, such as the luminiferous ether or absolute space, from which Einstein inferred that no such frame exists.

Einstein generalized relativity to frames of reference that were non-inertial. He achieved this by positing the Equivalence Principle, which states that the force felt by an observer in a given gravitational field and that felt by an observer in an accelerating frame of reference are indistinguishable. This led to the conclusion that the mass of an object warps the geometry of the space-time surrounding it, as described in Einstein's field equations.

In classical physics, an inertial reference frame is one in which an object that experiences no forces does not accelerate. In general relativity, an inertial frame of reference is one that is following a geodesic of space-time. An object that moves against a geodesic experiences a force. An object in free fall does not experience a force, because it is following a geodesic. An object standing on the earth, however, will experience a force, as it is being held against the geodesic by the surface of the planet.

Einstein partially advocates Mach's principle in that distant stars explain inertia because they provide the gravitational field against which acceleration and inertia occur. But contrary to Leibniz's account, this warped space-time is as integral a part of an object as are its other defining characteristics, such as volume and mass. If one holds, contrary to idealist beliefs, that objects exist independently of the mind, it seems that relativistics commits them to also hold that space and temporality have exactly the same type of independent existence.

The position of conventionalism states that there is no fact of the matter as to the geometry of space and time, but that it is decided by convention. The first proponent of such a view, Henri Poincaré, reacting to the creation of the new non-Euclidean geometry, argued that which geometry applied to a space was decided by convention, since different geometries will describe a set of objects equally well, based on considerations from his sphere-world.

This view was developed and updated to include considerations from relativistic physics by Hans Reichenbach. Reichenbach's conventionalism, applying to space and time, focuses around the idea of coordinative definition.

Coordinative definition has two major features. The first has to do with coordinating units of length with certain physical objects. This is motivated by the fact that we can never directly apprehend length. Instead we must choose some physical object, say the Standard Metre at the Bureau International des Poids et Mesures (International Bureau of Weights and Measures), or the wavelength of cadmium to stand in as our unit of length. The second feature deals with separated objects. Although we can, presumably, directly test the equality of length of two measuring rods when they are next to one another, we can not find out as much for two rods distant from one another. Even supposing that two rods, whenever brought near to one another are seen to be equal in length, we are not justified in stating that they are always equal in length. This impossibility undermines our ability to decide the equality of length of two distant objects. Sameness of length, to the contrary, must be set by definition.

Such a use of coordinative definition is in effect, on Reichenbach's conventionalism, in the General Theory of Relativity where light is assumed, i.e. not discovered, to mark out equal distances in equal times. After this setting of coordinative definition, however, the geometry of spacetime is set.

As in the absolutism/relationalism debate, contemporary philosophy is still in disagreement as to the correctness of the conventionalist doctrine.

Building from a mix of insights from the historical debates of absolutism and conventionalism as well as reflecting on the import of the technical apparatus of the General Theory of Relativity, details as to the structure of space-time have made up a large proportion of discussion within the philosophy of space and time, as well as the philosophy of physics. The following is a short list of topics.

According to special relativity each point in the universe can have a different set of events that compose its present instant. This has been used in the Rietdijk–Putnam argument to demonstrate that relativity predicts a block universe in which events are fixed in four dimensions.

Bringing to bear the lessons of the absolutism/relationalism debate with the powerful mathematical tools invented in the 19th and 20th century, Michael Friedman draws a distinction between invariance upon mathematical transformation and covariance upon transformation.

Invariance, or symmetry, applies to "objects", i.e. the symmetry group of a space-time theory designates what features of objects are invariant, or absolute, and which are dynamical, or variable.

Covariance applies to "formulations" of theories, i.e. the covariance group designates in which range of coordinate systems the laws of physics hold.

This distinction can be illustrated by revisiting Leibniz's thought experiment, in which the universe is shifted over five feet. In this example the position of an object is seen not to be a property of that object, i.e. location is not invariant. Similarly, the covariance group for classical mechanics will be any coordinate systems that are obtained from one another by shifts in position as well as other translations allowed by a Galilean transformation.

In the classical case, the invariance, or symmetry, group and the covariance group coincide, but they part ways in relativistic physics. The symmetry group of the general theory of relativity includes all differentiable transformations, i.e., all properties of an object are dynamical, in other words there are no absolute objects. The formulations of the general theory of relativity, unlike those of classical mechanics, do not share a standard, i.e., there is no single formulation paired with transformations. As such the covariance group of the general theory of relativity is just the covariance group of every theory.

A further application of the modern mathematical methods, in league with the idea of invariance and covariance groups, is to try to interpret historical views of space and time in modern, mathematical language.

In these translations, a theory of space and time is seen as a manifold paired with vector spaces, the more vector spaces the more facts there are about objects in that theory. The historical development of spacetime theories is generally seen to start from a position where many facts about objects are incorporated in that theory, and as history progresses, more and more structure is removed.

For example, Aristotelian space and time has both absolute position and special places, such as the center of the cosmos, and the circumference. Newtonian space and time has absolute position and is Galilean invariant, but does not have special positions.

With the general theory of relativity, the traditional debate between absolutism and relationalism has been shifted to whether spacetime is a substance, since the general theory of relativity largely rules out the existence of, e.g., absolute positions. One powerful argument against spacetime substantivalism, offered by John Earman is known as the "hole argument".

This is a technical mathematical argument but can be paraphrased as follows:

Define a function "d" as the identity function over all elements over the manifold M, excepting a small neighbourhood H belonging to M. Over H "d" comes to differ from identity by a smooth function.

With use of this function "d" we can construct two mathematical models, where the second is generated by applying "d" to proper elements of the first, such that the two models are identical prior to the time "t"=0, where "t" is a time function created by a foliation of spacetime, but differ after "t"=0.

These considerations show that, since substantivalism allows the construction of holes, that the universe must, on that view, be indeterministic. Which, Earman argues, is a case against substantivalism, as the case between determinism or indeterminism should be a question of physics, not of our commitment to substantivalism.

The problem of the direction of time arises directly from two contradictory facts. Firstly, the fundamental physical laws are time-reversal invariant; if a cinematographic film were taken of any process describable by means of the aforementioned laws and then played backwards, it would still portray a physically possible process. Secondly, our experience of time, at the macroscopic level, is "not" time-reversal invariant. Glasses can fall and break, but shards of glass cannot reassemble and fly up onto tables. We have memories of the past, and none of the future. We feel we can't change the past but can influence the future.

One solution to this problem takes a metaphysical view, in which the direction of time follows from an asymmetry of causation. We know more about the past because the elements of the past are causes for the effect that is our perception. We feel we can't affect the past and can affect the future because we "can't" affect the past and "can" affect the future.

There are two main objections to this view. First is the problem of distinguishing the cause from the effect in a non-arbitrary way. The use of causation in constructing a temporal ordering could easily become circular. The second problem with this view is its explanatory power. While the causation account, if successful, may account for some time-asymmetric phenomena like perception and action, it does not account for many others.

However, asymmetry of causation can be observed in a non-arbitrary way which is not metaphysical in the case of a human hand dropping a cup of water which smashes into fragments on a hard floor, spilling the liquid. In this order, the causes of the resultant pattern of cup fragments and water spill is easily attributable in terms of the trajectory of the cup, irregularities in its structure, angle of its impact on the floor, etc. However, applying the same event in reverse, it is difficult to explain why the various pieces of the cup should fly up into the human hand and reassemble precisely into the shape of a cup, or why the water should position itself entirely within the cup. The causes of the resultant structure and shape of the cup and the encapsulation of the water by the hand within the cup are not easily attributable, as neither hand nor floor can achieve such formations of the cup or water. This asymmetry is perceivable on account of two features: i) the relationship between the agent capacities of the human hand (i.e., what it is and is not capable of and what it is for) and non-animal agency (i.e., what floors are and are not capable of and what they are for) and ii) that the pieces of cup came to possess exactly the nature and number of those of a cup before assembling. In short, such asymmetry is attributable to the relationship between i) temporal direction and ii) the implications of form and functional capacity.

The application of these ideas of form and functional capacity only dictates temporal direction in relation to complex scenarios involving specific, non-metaphysical agency which is not merely dependent on human perception of time. However, this last observation in itself is not sufficient to invalidate the implications of the example for the progressive nature of time in general.

The second major family of solutions to this problem, and by far the one that has generated the most literature, finds the existence of the direction of time as relating to the nature of thermodynamics.

The answer from classical thermodynamics states that while our basic physical theory is, in fact, time-reversal symmetric, thermodynamics is not. In particular, the second law of thermodynamics states that the net entropy of a closed system never decreases, and this explains why we often see glass breaking, but not coming back together.

But in statistical mechanics things become more complicated. On one hand, statistical mechanics is far superior to classical thermodynamics, in that thermodynamic behavior, such as glass breaking, can be explained by the fundamental laws of physics paired with a statistical postulate. But statistical mechanics, unlike classical thermodynamics, is time-reversal symmetric. The second law of thermodynamics, as it arises in statistical mechanics, merely states that it is "overwhelmingly" likely that net entropy will increase, but it is not an absolute law.

Current thermodynamic solutions to the problem of the direction of time aim to find some further fact, or feature of the laws of nature to account for this discrepancy.

A third type of solution to the problem of the direction of time, although much less represented, argues that the laws are not time-reversal symmetric. For example, certain processes in quantum mechanics, relating to the weak nuclear force, are not time-reversible, keeping in mind that when dealing with quantum mechanics time-reversibility comprises a more complex definition. But this type of solution is insufficient because 1) the time-asymmetric phenomena in quantum mechanics are too few to account for the uniformity of macroscopic time-asymmetry and 2) it relies on the assumption that quantum mechanics is the final or correct description of physical processes.

One recent proponent of the laws solution is Tim Maudlin who argues that the fundamental laws of physics are laws of temporal evolution (see Maudlin [2007]). However, elsewhere Maudlin argues: "[the] passage of time is an intrinsic asymmetry in the temporal structure of the world... It is the asymmetry that grounds the distinction between sequences that runs from past to future and sequences which run from future to past" [ibid, 2010 edition, p. 108]. Thus it is arguably difficult to assess whether Maudlin is suggesting that the direction of time is a consequence of the laws or is itself primitive.

The problem of the flow of time, as it has been treated in analytic philosophy, owes its beginning to a paper written by J. M. E. McTaggart, in which he proposes two "temporal series". The first series, which means to account for our intuitions about temporal becoming, or the moving Now, is called the A-series. The A-series orders events according to their being in the past, present or future, "simpliciter" and in comparison to each other. The B-series eliminates all reference to the present, and the associated temporal modalities of past and future, and orders all events by the temporal relations "earlier than" and "later than".

McTaggart, in his paper "The Unreality of Time", argues that time is unreal since a) the A-series is inconsistent and b) the B-series alone cannot account for the nature of time as the A-series describes an essential feature of it.

Building from this framework, two camps of solution have been offered. The first, the A-theorist solution, takes becoming as the central feature of time, and tries to construct the B-series from the A-series by offering an account of how B-facts come to be out of A-facts. The second camp, the B-theorist solution, takes as decisive McTaggart's arguments against the A-series and tries to construct the A-series out of the B-series, for example, by temporal indexicals.

Quantum field theory models have shown that it is possible for theories in two different space-time backgrounds, like AdS/CFT or T-duality, to be equivalent.

According to Presentism, time is an ordering of various realities. At a certain time some things exist and others do not. This is the only reality we can deal with and we cannot for example say that Homer exists because at the present time he does not. An Eternalist, on the other hand, holds that time is a dimension of reality on a par with the three spatial dimensions, and hence that all things—past, present and future—can be said to be just as real as things in the present. According to this theory, then, Homer really "does" exist, though we must still use special language when talking about somebody who exists at a distant time—just as we would use special language when talking about something far away (the very words "near", "far", "above", "below", and such are directly comparable to phrases such as "in the past", "a minute ago", and so on).

The positions on the persistence of objects are somewhat similar. An endurantist holds that for an object to persist through time is for it to exist completely at different times (each instance of existence we can regard as somehow separate from previous and future instances, though still numerically identical with them). A perdurantist on the other hand holds that for a thing to exist through time is for it to exist as a continuous reality, and that when we consider the thing as a whole we must consider an aggregate of all its "temporal parts" or instances of existing. Endurantism is seen as the conventional view and flows out of our pre-philosophical ideas (when I talk to somebody I think I am talking to that person as a complete object, and not just a part of a cross-temporal being), but perdurantists such as David Lewis have attacked this position. They argue that perdurantism is the superior view for its ability to take account of change in objects.

On the whole, Presentists are also endurantists and Eternalists are also perdurantists (and vice versa), but this is not a necessary relation and it is possible to claim, for instance, that time's passage indicates a series of ordered realities, but that objects within these realities somehow exist outside of the reality as a whole, even though the realities as wholes are not related. However, such positions are rarely adopted.




</doc>
<doc id="8994982" url="https://en.wikipedia.org/wiki?curid=8994982" title="Relational space">
Relational space

The relational theory of space is a metaphysical theory according to which space is composed of relations between objects, with the implication that it cannot exist in the absence of matter. Its opposite is the container theory. A relativistic physical theory implies a relational metaphysics, but not the other way round: even if space is composed of nothing but relations between observers and events, it would be conceptually possible for all observers to agree on their measurements, whereas relativity implies they will disagree. Newtonian physics can be cast in relational terms, but Newton insisted, for philosophical reasons, on absolute (container) space. The subject was famously debated by Gottfried Wilhelm Leibniz and a supporter of Newton's in the Leibniz–Clarke correspondence.

An absolute approach can also be applied to time, with, for instance, the implication that there might have been vast epochs of time before the first event.



</doc>
<doc id="19991258" url="https://en.wikipedia.org/wiki?curid=19991258" title="Sociology of space">
Sociology of space

The sociology of space is a sub-discipline of sociology that mostly borrows from theories developed within the discipline of geography, including the sub fields of human geography, economic geography, and feminist geography. The "sociology" of space examines the social and material constitution of spaces. It is concerned with understanding the social practices, institutional forces, and material complexity of how humans and spaces interact. The sociology of space is an inter-disciplinary area of study, drawing on various theoretical traditions including Marxism, postcolonialism, and Science and Technology Studies, and overlaps and encompasses theorists with various academic disciplines such as geography and architecture. Edward T. Hall developed the study of Proxemics which concentrates on the empirical analysis of space in psychology. 

Space is one of the most important concepts within the disciplines of social science as it is fundamental to our understanding of geography. The term "space" has been defined variously by scholars:

In general terms, the Oxford English Dictionary defines space in two ways;

1. A continuous extension viewed with or without reference to the existence of objects within it. 
2. The interval between points or objects viewed as having one, two or three dimensions.

However, the human geographers’ interest is in the objects within the space and their relative position, which involves the description, explanation and prediction of the distribution of phenomena. Thus, the relationships between objects in space is the central of the study.

Michel Foucault defines space as;
“The space in which we live, which draws us out of ourselves, in which the erosion of our lives, our time and our history occurs, the space that claws and gnaws at us, is also, in itself, a heterogeneous space…..we live inside a set of relations.

Nigel Thrift also defines space as;
"The outcome of a series of highly problematic temporary settlements that divide and connect things up into different kinds of collectives which are slowly provided with the meaning which render them durable and sustainable." 

In short, "space" is the social space in which we live and create relationships with other people, societies and surroundings. Space is an outcome of the hard and continuous work of building up and maintaining collectives by bringing different things into alignments. All kinds of different spaces can and therefore do exist which may or may not relate to each other. Thus, through space, we can understand more about social action.

Georg Simmel has been seen as the classical sociologist who was most important to this field. Simmel wrote on "the sociology of space" in his 1908 book "Sociology: Investigations on the Forms of Sociation". His concerns included the process of metropolitanisation and the separation of leisure spaces in modern economic societies.

The category of space long played a subordinate role in sociological theory formation. Only in the late 1980s did it come to be realised that certain changes in society cannot be adequately explained without taking greater account of the spatial components of life. This shift in perspective is referred to as the topological turn. The space concept directs attention to organisational forms of juxtaposition. The focus is on differences between places and their mutual influence. This applies equally for the micro-spaces of everyday life and the macro-spaces at the nation-state or global levels.

The theoretical basis for the growing interest of the social sciences in space was set primarily by English and French-speaking sociologists, philosophers, and human geographers. Of particular importance is Michel Foucault’s essay on “Of Other Spaces”, in which the author proclaims the “age of space”, and Henri Lefebvre’s seminal work “La production de l’espace”. The latter provided the grounding for Marxist spatial theory on which David Harvey, Manuel Castells, Edward Soja, and others have built. Marxist theories of space, which are predicated on a structural, i.e., capitalist or global determinants of spaces and the growing homogenization of space, are confronted by action theoretical conceptions, which stress the importance of the corporeal placing and the perception of spaces as albeit habitually predetermined but subjective constructions. One example is the theory of space of the German sociologist Martina Löw. Approaches deriving from the post-colonialism discourse have attracted greater attention in recent years. Also in contrast to (neo)Marxist concepts of space, British geographer Doreen Massey and German sociologist Helmuth Berking, for instance, emphasise the heterogeneity of local contexts and the place-relatedness of our knowledge about the world.

Martina Löw developed the idea of a "relational" model of space, which focuses on the “orderings” of living entities and social goods, and examines how space is constituted in processes of perception, recall, or ideation to manifest itself as societal structure. From a social theory point of view, it follows on from the theory of structuration proposed by Anthony Giddens, whose concept of the “duality of structure” Löw extends sociological terms into a “duality of space.” The basic idea is that individuals act as social agents (and constitute spaces in the process), but that their action depends on economic, legal, social, cultural, and, finally, spatial structures. Spaces are hence the outcome of action. At the same time, spaces structure action, that is to say spaces can both constrain and enable action.

With respect to the constitution of space, Löw distinguishes analytically between two, generally mutually determining factors: “spacing” and “synthesis.” Spacing refers to the act of placing or the state of being placed of social goods and people in places. According to Löw, however, an ordering created through placings is only effectively constituted as space where the elements that compose it are actively interlinked by people – in processes of perception, ideation, or recall. Löw calls this synthesis. This concept has been empirically tested in studies such as those by Lars Meier (who examined the constitution of space in the everyday life of financial managers in London and Singapore), Cedric Janowicz (who carried out an ethnographical-space sociological study of food supply in the Ghanaian city of Accra), and Silke Streets (who looked at processes of space constitution in the creative industries in Leipzig).

The most important proponent of Marxist spatial theory was Henri Lefebvre. He proposed "social space" to be where the relations of production are reproduced and that dialectical contradictions were spatial rather than temporal. Lefèbvre sees the societal production of space as a dialectical interaction between three factors. Space is constituted:


In Lefebvre’s view of the 1970s, this spatial production resulted in a space of non-reflexive everydayness marked by alienation, dominating through mathematical-abstract concepts of space, and reproduced in spatial practice. Lefebvre sees a line of flight from alienated spatiality in the spaces of representation – in notions of non-alienated, mythical, pre-modern, or artistic visions of space.

Marxist spatial theory was given decisive new impetus by David Harvey, in particular, who was interested in the effects of the transition from Fordism to “flexible accumulation” on the experience of space and time. He shows how various innovations at the economic and technological levels have breached the crisis-prone inflexibility of the Fordist system, thus increasing the turnover rate of capital. This causes a general acceleration of economic cycles. According to Harvey, the result is “time-space compression.” While the feeling for the long term, for the future, for continuity is lost, the relationship between proximity and distance becomes more and more difficult to determine.

Theories of space that are inspired by the post-colonialism discourse focus on the heterogeneity of spaces. According to Doreen Massey, calling a country in Africa a “developing country” is not appropriate, since this expression implies that spatial difference is temporal difference (Massey 1999b). This logic treats such a country not as different but merely as an early version of countries in the “developed” world, a view she condemns as "Eurocentrism." In this vein, Helmuth Berking criticises theories that postulate the increasing homogenisation of the world through globalisation as “globocentrism.” He confronts this with the distinctiveness and importance of local knowledge resources for the production of (different and specific) places. He claims that local contexts form a sort of framework or filter through which global processes and globally circulating images and symbols are appropriated, thus attaining meaning. For instance, the film character Conan the Barbarian is a different figure in radical rightwing circles in Germany than in the black ghettoes of the Chicago Southside, just as McDonald’s means something different in Moscow than in Paris.

Henri Lefebvre (see also Edward Soja) says that (social) space is a (social) product, or a complex social construction (based on values, and the social production of meanings) which affects spatial practices and perceptions. He explains space embraces a multitude of intersection in his great book, “Production of Space”. That means that we need to consider how the various modes of spatial production relate to each other.

He argues that there are three aspects to our spatial existence, which exist in a kind of triad:

1. First Space
"The spatial practice of a society secretes that society's space; it propounds and presupposes it, in a dialectical interaction; it produces it slowly and surely as it masters and appropriates it."

2. Second Space
"Conceptualized space, the space of scientists, planners, urbanists, technocratic subdividers and social engineers, as of a certain type of artist with a scientific bent -- all of whom identify what is lived and what is perceived with what is conceived."

3. Third Space
"Space as directly lived through its associated images and symbols."

Even though there are many disciplines in the study of Human Geography, the most well-known approach is “The third space” formulated by Edward Soja. In unitary theory, there are three approaches; first space, second space and third space. First space is physical space, and spaces are measurable and mappable. The second space is a mental or conceived space which comes from our thinking and ideas. However, the third space is a social space/lived space which is a social product that is a space created by society under oppression or marginalization that want to reclaim the space of inequality and make it into something else. Soja argues that our old ways to thinking about space (first and second space theories) can no longer accommodate the way the world works because he believed that spaces may not be contained within one social category, they may include different aspects of many categories or developed within the boundaries of a number of category. For instance, two different cultures combine together and emerge as a third culture. This third hybrid space displaces the original values that constitute it and set up new values and perspectives that is different from the first two spaces. Thus, the third space theory can explain some of the complexity of poverty, social exclusion and social inclusion, gender and race issues.

In the work of geographer and critical theorist Nigel Thrift, he wrote a rational view of space in which, rather than seeing space being viewed as a container within which the world proceeds, space should be seen as a co-product of these proceedings. He explained about four constructed space in modern human geography. 
There are four different kinds of space according to how modern geography thinks about space. They are 1. Empirical Construction of Space, 2. Unblocking space, 3. Image space and 4. Place Space.

First Space is the empirical construction of space. Empirical space refers to the process whereby the mundane fabric of daily life is constructed. These simple things like, cars, houses, mobiles, computers and roads are very simple but they are great achievements of our daily life and they play very important role in making up who we are today. For example, today’s technology such as GPS did not suddenly come into existence; in fact, it is laid down in the 18th century and developed throughout time. The first space is real and tangible, and it is also known as physical space.
Second space is the unblocking space. This type of space refers to the process whereby routine pathways of interaction as set up around which boundaries are often drawn. The routine may include the movement of office workers, the interaction of drunk teenagers, and the flow of goods, money, people, and information. Unlike the old time in geography when people accepted a space as blocked boundary (Example: A capitalist space, neoliberal space or city space), we began to realize that there is no such thing like boundaries in space. The space of the world is flowing and transforming continuously that it is very difficult to describe in a fixed way. The second space is ideology/conceptual and it is also known as mental space. For example, the second space will explain the behaviors of people from different social class and the social segregation among rich and poor people. 
Third space is the image space that refers to the process whereby the images has produced new kind of space. The images may be in different form and shape; ranging from painting to photograph, from portrait to post card, and from religious theme to entertainment. Nowadays, we are highly influenced by images in many ways and these certain images can tell us new social and cultures values, or something new about how we see the world. Images, symbols and sign do have some kind of spatial expression. 
Fourth space is the place that refers to the process whereby spaces are ordered in ways that open up affective and other embodied potentials. Place space has more meaning than a place, and it can represent as different type of space. This fourth type of space tries to understand that place is a vital actor in bringing up people's lives in certain ways and place will let us to understand all kind of things which are hidden form us..

Andrew Herod mentioned that scale, within human geography, is typically seen in one of the two ways: either as a real material thing which actually exists and is the result of political struggle and/or social process, or as a way of framing our understanding of the world. People’s lives across the globe have been re-scaling by contemporary economic, political, cultural and social processes, such as globalization, in complex ways. As a result, we have seen the creation of supranational political bodies such as the European Union, the devolution of political power from the nation-state to regional political bodies. We have also experienced the increasing homogenization and ‘Americanization’ through the process of globalization while the locals’ tendencies (or counter force)among people who defend traditional ways of life increase around the world .The process of re-scaling people‘s lives and the relationship between the two extremes of our scaled lives- the ‘global’ and the ‘local’ were brought into question.

Until the 1980s, theorizing the concept of ‘scale’ itself was taken for granted although physical and human geographers looked at issues from ‘regional scale’ or‘national scale’. The questions such as whether scale is simply a mental device categorizing and ordering the world or whether scales really exists as material social products, particularly, were debated among materialists and idealists. Some geographers draw upon Immanuel Kant’s idealist philosophy that scales were handy conceptual mechanism for ordering the world while others, by drawing upon Marxist ideas of materialism, argue that scales really exist in the world and they were the real social products. For those idealists based on Kantian‘s inspiration, the ‘global’ is defined by the geologically given limits of the earth and the ‘local’ is defined as a spatial resolution useful for comprehending the process and practices. For materialists, the ‘national’ scale is a scale that had to be actively created through economic and political processes but not a scale existed in a logical hierarchy between global and the regional.

The notion of ‘becoming’ and the focus on the politics of producing scales have been central to materialist arguments concerning the global scale. It is important to recognize that social actors may have to work just as hard to become ‘local’as they have to work to become ‘global’. People paid attention to how transnational corporations have ‘gone global’, how institutions of governance have ‘become’ supranational and how labour unions have sought to ‘globalize’ their operations to match those of an increasingly ‘globalized’ city.

For the scale ‘global’ and ‘local’, Kevin Cox mentioned that moving from the local to the global scale ‘is not a movement from one discrete arena to another’ but a process of developing networks of associations that allow actors to shift between various spaces of engagement. According to his view, ‘scale’ is seen as a process rather than as a fixed entity and, in other words, the global and the local are not static ’arenas’within which social life plays out but are constantly made by social actions. For example, a political organization might attempt to go ‘global’ to engage with actors or opportunities outside of its own space; likewise, a transnational corporation may attempt to ‘go local’ through tailoring its products and operations in different places.

Gibson-Graham (2002) has identified at least six ways in which the relationship between the local and the global is often viewed.

1. The global and the local are seen as interpretive frames for analyzing situations

2. Drawing on Dirlik, Gibson-Graham suggests that in such a representation, the global is ‘something more than the national or regional ..anything other than the local’. Meaning that, the global and the local each derive meaning from what they are not.

3. According to French social theorist Bruno Latour, the local and the global ‘offer different points of view on networks that are by nature neither local nor global, but are more or less long and more or less connected. Also, in Latour’s view, it is impossible to distinguish where the local ends and the global begins.

4. The concept ‘The global is local’ was proposed by Gibson-Graham. For instance, multinational firms are actually ‘multi local‘ rather than ‘global’.

5. The local is global. In this view, the local is an entry point to the world of global flows which encircle the planet.

6. The global and the local are actually the processes rather than the locations. All spaces are the hybrids of global and local; so they are ‘glocal.’

There are some western thoughts that greater size and extensiveness imply domination and superior power, such that the local is often represented as ‘small and relatively powerless, defined and confined by the global’. So, the global is a force and the local is its field of play. However, the local can serve as a powerful scale of political organization; the global is not a scale just controlled by capital – those who challenge capital can also organize globally( Herod, A). There has been the concept ‘Think globally and act locally’ viewed by neoliberals.

For representing how the world is scaled, there are five different and popular metaphors: they are the ladder, concentric circles, Matryoshka nesting dolls, earthworm burrows and tree roots. First, in using such a metaphor of hierarchical ladder, the global as the highest rung on the ladder is seen to be above the local and all other scales. Second, the use of concentric metaphor leaves us with a particular way of conceptualizing the scalar relationship between places. In this second metaphor, the local is seen as a relatively small circle, with the regional as a larger circle encompassing it, while the national and the global scales are still larger circles encompassing the local and the regional. For the hierarchy of Russian Matryoshka nesting dolls, the global can contain other scales but this does not work the other way round; for instance, the local cannot contain the global. For the fourth metaphor concerning with thinking on scale, what French social theorist Bruno Latour argued is that a world of places is ‘networked’ together. Such the metaphor leaves us with an image of scale in which the global and the local are connected together and not totally separated from each other. For the tree roots metaphor similar with the earthworm burrow metaphor, as the earthworm burrows or tree roots penetrating different strata of the soil, it is difficult to determine exactly where one scale ends and another begins. When thinking about the use of metaphor, it should be aware that the choice of metaphor over another is not made on the basis of which is empirically a ‘more accurate’representation of something but, on the basis of how someone is attempting to understand a particular phenomenon.

Such an appreciation of metaphors is important because it suggests that how we talk about scale impacts upon the ways in which we engage socially and politically with our scaled world and that may impact on how we conduct our social, economic and political praxis and so make landscapes ( Herod,A )




</doc>
<doc id="9332507" url="https://en.wikipedia.org/wiki?curid=9332507" title="Leibniz–Clarke correspondence">
Leibniz–Clarke correspondence

The Leibniz–Clarke correspondence was a scientific, theological and philosophical debate conducted in an exchange of letters between the German thinker Gottfried Wilhelm Leibniz and Samuel Clarke, an English supporter of Isaac Newton during the years 1715 and 1716. The exchange began because of a letter Leibniz wrote to Caroline of Ansbach, in which he remarked that Newtonian physics was detrimental to natural theology. Eager to defend the Newtonian view, Clarke responded, and the correspondence continued until the death of Leibniz in 1716.

Although a variety of subjects is touched on in the letters, the main interest for modern readers is in the dispute between the absolute theory of space favoured by Newton and Clarke, and Leibniz's relational approach. Also important is the conflict between Clarke's and Leibniz's opinions on free will and whether God must create the best of all possible worlds.

Leibniz had published only one book on moral matters, the "Theodicée" (1710), and his more metaphysical views had never been exposed to a sufficient extent, so the collected letters were met with interest by their contemporaries. The primary dispute between Leibniz and Newton about the calculus was still fresh in the public's mind and it was taken as a matter of course that it was Newton himself who stood behind Clarke's replies.

The Leibniz-Clarke letters were first published under Clarke's name in the year following Leibniz' death. He wrote a preface, took care of the translation from French, added notes and some of his own writing. In 1720 Pierre Desmaizeaux published a similar volume in a French translation, including quotes from Newton's work. It is quite certain that for both editions the opinion of Newton himself has been sought and Leibniz left at a disadvantage. However the German translation of the correspondence published by Kohler, also in 1720, contained a reply to Clarke's last letter which Leibniz had not been able to answer. The letters have been reprinted in most collections of Leibniz' works and regularly published in stand alone editions.





</doc>
<doc id="32172794" url="https://en.wikipedia.org/wiki?curid=32172794" title="Visual space">
Visual space

Visual space is the experience of space by an aware observer. It is the subjective counterpart of the space of physical objects. There is a long history in philosophy, and later psychology of writings describing visual space, and its relationship to the space of physical objects. A partial list would include René Descartes, Immanuel Kant, Hermann von Helmholtz, William James, to name just a few.

The location and shape of physical objects can be accurately described with the tools of geometry. For practical purposes the space we occupy is Euclidean. It is three-dimensional and measurable using tools such as rulers. It can be quantified using co-ordinate systems like the Cartesian x,y,z, or polar coordinates with angles of elevation, azimuth and distance from an arbitrary origin.

Percepts, the counterparts in the aware observer's conscious experience of objects in physical space, constitute an ordered ensemble or, as Ernst Cassirer explained, Visual Space can not be measured with rulers. Historically philosophers used introspection, and reasoning to described it. With the development of Psychophysics beginning with Gustav Fechner, there has been an effort to develop suitable experimental procedures which allow objective descriptions of visual space including geometric descriptions to be developed and tested. An example illustrates the relationship between the concepts of object and visual space. Two straight lines are presented to an observer who is asked to set them so that they appear parallel. When this has been done, the lines "are" parallel in visual space A comparison is then possible with the actual measured layout of the lines in physical space. Good precision can be achieved using these and other psychophysical procedures in human observers or behavioral ones in trained animals.

The distinction should be made between the "visual field", the area or extent of physical space that is being imaged on the retina, and the perceptual "space" in which visual percepts are located, which we call visual space. Confusion is caused by the use of "" in the German literature for both. There is no doubt that Ewald Hering and his followers meant visual space in their writings.

The fundamental distinction was made by Rudolf Carnap between three kinds of space which he called "formal", "physical" and "perceptual." Mathematicians, for example, deal with ordered structures, ensembles of elements for which rules of logico-deductive relationships hold, limited solely by being not self-contradictory. These are the "formal" spaces. According to Carnap, studying "physical" space means examining the relationship between empirically determined objects. Finally, there is the realm of what students of Kant know as "," immediate sensory experiences, often awkwardly translated as "apperceptions," which belong to "perceptual spaces."

Geometry is the discipline devoted to the study of space and the rules relating the elements to each other. For example, in Euclidean space the Pythagorean theorem provides a rule to compute distances from Cartesian coordinates. In a two-dimensional space of constant curvature, like the surface of a sphere, the rule is somewhat more complex but applies everywhere. On the two-dimensional surface of a football, the rule is more complex still and has different values depending on location. In well-behaved spaces such rules used for measurement and called "Metrics," are classically handled by the mathematics invented by Riemann. Object space belongs to that class.

To the extent that it is reachable by scientifically acceptable probes, visual space as defined is also a candidate for such considerations. The first and remarkably prescient analysis was published by Ernst Mach in 1901. Under the heading "On Physiological as Distinguished from Geometrical Space" Mach states that "Both spaces are threefold manifoldnesses" but the former is "...neither constituted everywhere and in all directions alike, nor infinite in extent, nor unbounded." A notable attempt at a rigorous formulation was made in 1947 by Rudolf Luneburg, who preceded his essay on mathematical analysis of vision by a profound analysis of the underlying principles. When features are sufficiently singular and distinct, there is no problem about a correspondence between an individual item "A" in object space and its correlate "A' " in visual space. Questions can be asked and answered such as "If visual percepts "A',B',C' " are correlates of physical objects "A,B,C," and if "C" lies between "A" and "B", does "C' " lie between "A' " and "B' "?" In this manner, the possibility of visual space being metrical can be approached. If the exercise is successful, a great deal can be said about the nature of the mapping of the physical space on the visual space.

On the basis of fragmentary psychophysical data of previous generations, Luneburg concluded that visual space was hyperbolic with constant curvature, meaning that elements can be moved throughout the space without changing shape. One of Luneburg's major arguments is that, in accord with a common observation, the transformation involving hyperbolic space renders infinity into a dome (the sky). The Luneburg proposition gave rise to discussions and attempts at corroborating experiments, which on the whole did not favor it.

Basic to the problem, and underestimated by Luneburg the mathematician, is the likely success of a mathematically viable formulation of the relationship between objects in physical space and percepts in visual space. Any scientific investigation of visual space is colored by the kind of access we have to it, and the precision, repeatability and generality of measurements. Insightful questions can be asked about the mapping of visual space to object space but answers are mostly limited in the range of their validity. If the physical setting that satisfies the criterion of, say, apparent parallelism varies from observer to observer, or from day to day, or from context to context, so does the geometrical nature of, and hence mathematical formulation for, visual space.

All these arguments notwithstanding, there is a major concordance between the locations of items in object space and their correlates in visual space. It is adequately veridical for us to navigate very effectively in the world, deviations from such a situation are sufficiently notable to warrant special consideration. visual space agnosia is a recognized neurological condition, and the many common distortions, called geometrical-optical illusions, are widely demonstrated but of minor consequence.

Its founder, Gustav Theodor Fechner defined the mission of the discipline of psychophysics as the functional relationship between the mental and material worlds—in this particular case, the visual and object spaces—but he acknowledged an intermediate step, which has since blossomed into the major enterprise of modern neuroscience. In distinguishing between "inner" and "outer" psychophysics, Fechner recognized that a physical stimulus generates a percept by way of an effect on the organism's sensory and nervous systems. Hence, without denying that its essence is the arc between object and percept, the inquiry can concern itself with the neural substrate of visual space.

Two major concepts dating back to the middle of the 19th century set the parameters of the discussion here. Johannes Müller emphasized that what matters in a neural path is the connection it makes, and Hermann Lotze, from psychological considerations, enunciated the principle of . Put together in modern neuroanatomical terms they mean that a nerve fiber from a fixed retinal location instructs its target neurons in the brain about the presence of a stimulus in the location in the eye's visual field that is imaged there. The orderly array of retinal locations is preserved in the passage from the retina to the brain, and provides what is aptly called a "retinotopic" mapping in the primary visual cortex. Thus in the first instance brain activity retains the relative spatial ordering of the objects and lays the foundations for a neural substrate of visual space.

Unfortunately simplicity and transparency ends here. Right at the outset, visual signals are analyzed not only for their position, but also, separately in parallel channels, for many other attributes such as brightness, color, orientation, depth. No single neuron or even neuronal center or circuit represents both the nature of a target feature and its accurate location. The unitary mapping of object space into the coherent visual space without internal contradictions or inconsistencies that we as observer automatically experience, demands concepts of conjoint activity in several parts of the nervous system that is at present beyond the reach of neurophysiological research.

Though the details of the process by which the experience of visual space emerges remain opaque, a startling finding gives hope for future insights. Neural units have been demonstrated in the brain structure called hippocampus that show activity only when the animal is in a specific place in its environment.

Only on an astronomical scale are physical space and its contents interdependent, This major proposition of the general theory of relativity is of no concern in vision. For us, distances in object space are independent of the nature of the objects.

But this is not so simple in visual space. At a minim an observer judges the relative location of a few light points in an otherwise dark visual field, a simplistic extension from object space that enabled Luneburg to make some statements about the geometry of visual space. In a more richly textured visual world, the various visual percepts carry with them prior perceptual associations which often affect their relative spatial disposition. Identical separations in physical space can look quite different ("are quite different" in visual space) depending on the features that demarcate them. This is particularly so in the depth dimension because the apparatus by which values in the third visual dimension are assigned is fundamentally different from that for the height and width of objects.

Even in monocular vision, which physiologically has only two dimensions, cues of size, perspective, relative motion etc. are used to assign depth differences to percepts. Looked at as a mathematical/geometrical problem, expanding a 2-dimensional object manifold into a 3-dimensional visual world is "ill-posed," i.e., not capable of a rational solution, but is accomplished quite effectively by the human observer.

The problem becomes less ill-posed when binocular vision allows actual determination of relative depth by stereoscopy, but its linkage to the evaluation of distance in the other two dimensions is uncertain (see: stereoscopic depth rendition). Hence, the uncomplicated three-dimensional visual space of every-day experience is the product of many perceptual and cognitive layers superimposed on the physiological representation of the physical world of objects.


</doc>
<doc id="53977963" url="https://en.wikipedia.org/wiki?curid=53977963" title="Dynamical dimensional reduction">
Dynamical dimensional reduction

Dynamical dimensional reduction or spontaneous dimensional reduction is the apparent reduction in the number of spacetime dimensions as a function of the distance scale, or conversely the energy scale, with which spacetime is probed. At least within the current level of experimental precision, our universe has three dimensions of space and one of time. However, the idea that the number of dimensions may increase at extremely small length scales was first proposed more than a century ago, and is now fairly commonplace in theoretical physics. Contrary to this, a number of recent results in quantum gravity suggest the opposite behavior, a dynamical reduction of the number of spacetime dimensions at small length scales. 
The phenomenon of dimensional reduction has now been reported in a number of different approaches to quantum gravity. String theory, causal dynamical triangulations, renormalization group approaches, noncommutative geometry, loop quantum gravity and Horava-Lifshitz gravity all find that the dimensionality of spacetime appears to decrease from approximately 4 on large distance scales to approximately 2 on small distance scales. 

The evidence for dimensional reduction has come mainly, although not exclusively, from calculations of the spectral dimension. The spectral dimension is a measure of the effective dimension of a manifold at different resolution scales. Early numerical simulations within the causal dynamical triangulation (CDT) approach to quantum gravity found a spectral dimension of 4.02 ± 0.10 at large distances and 1.80 ± 0.25 at small distances. This result created significant interest in dimensional reduction within the quantum gravity community. A more recent study of the same point in the parameter space of CDT found consistent results, namely 4.05 ± 0.17 at large distances and 1.97 ± 0.27 at small distances.

Currently, there is no consensus on the correct theoretical explanation for the mechanism of dimensional reduction. 

The ubiquity and consistency of dimensional reduction in quantum gravity has driven the search for a theoretical understanding of this phenomenon. Currently, there exist few proposed explanations for the observation of dimensional reduction. 

One proposal is that of scale invariance. There is growing evidence that gravity may be nonperturbatively renormalizable as described by the asymptotic safety program, which requires the existence of a non-Gaussian fixed point at high energies towards which the couplings defining the theory flow. At such a fixed point gravity must be scale invariant, and hence Newton's constant must be dimensionless. Only in 2-dimensional spacetime is Newton's constant dimensionless, and so in this scenario going to higher energies and hence flowing towards the fixed point should correspond to the dimensionality of spacetime reducing to the value 2. This explanation is not entirely satisfying as it does not explain why such a fixed point should exist in the first place.

A second possible explanation for dimensional reduction is that of asymptotic silence. General relativity exhibits so-called asymptotic silence in the vicinity of a spacelike singularity, which is the narrowing or focusing of light cones close to the Planck scale leading to a causal decoupling of nearby spacetime points. In this scenario, each point has a preferred spatial direction, and geodesics see a reduced (1 + 1)-dimensional spacetime.

Dimensional reduction implies a deformation or violation of Lorentz invariance and typically predicts an energy dependent speed of light. Given such radical consequences, an alternative proposal is that dimensional reduction should not be taken literally, but should instead be viewed as a hint of new Planck scale physics.


</doc>
<doc id="34043" url="https://en.wikipedia.org/wiki?curid=34043" title="Wormhole">
Wormhole

A wormhole (or Einstein–Rosen bridge or Einstein–Rosen wormhole) is a speculative structure linking disparate points in spacetime, and is based on a special solution of the Einstein field equations. A wormhole can be visualized as a tunnel with two ends at separate points in spacetime (i.e., different locations, or different points in time, or both.)

Wormholes are consistent with the general theory of relativity, but whether wormholes actually exist remains to be seen. Many scientists postulate wormholes are merely a projection of the 4th dimension, analogous to how a 2D being could experience only part of a 3D object.

A wormhole could connect extremely long distances such as a billion light years or more, short distances such as a few meters, different universes, or different points in time.

For a simplified notion of a wormhole, space can be visualized as a two-dimensional (2D) surface. In this case, a wormhole would appear as a hole in that surface, lead into a 3D tube (the inside surface of a cylinder), then re-emerge at another location on the 2D surface with a hole similar to the entrance. An actual wormhole would be analogous to this, but with the spatial dimensions raised by one. For example, instead of circular holes on a 2D plane, the entry and exit points could be visualized as spheres in 3D space.

Another way to imagine wormholes is to take a sheet of paper and draw two somewhat distant points on one side of the paper. The sheet of paper represents a plane in the spacetime continuum, and the two points represent a distance to be traveled, however theoretically a wormhole could connect these two points by folding that plane so the points are touching. In this way it would be much easier to traverse the distance since the two points are now touching.

In 1928, Hermann Weyl proposed a wormhole hypothesis of matter in connection with mass analysis of electromagnetic field energy; however, he did not use the term "wormhole" (he spoke of "one-dimensional tubes" instead).

American theoretical physicist John Archibald Wheeler (inspired by Weyl's work) coined the term "wormhole" in a 1957 paper co-authored by Charles Misner:

Wormholes have been defined both "geometrically" and "topologically". From a topological point of view, an intra-universe wormhole (a wormhole between two points in the same universe) is a compact region of spacetime whose boundary is topologically trivial, but whose interior is not simply connected. Formalizing this idea leads to definitions such as the following, taken from Matt Visser's "Lorentzian Wormholes" (1996).

Geometrically, wormholes can be described as regions of spacetime that constrain the incremental deformation of closed surfaces. For example, in Enrico Rodrigo's "The Physics of Stargates, "a wormhole is defined informally as: 

The first type of wormhole solution discovered was the Schwarzschild wormhole, which would be present in the Schwarzschild metric describing an "eternal black hole", but it was found that it would collapse too quickly for anything to cross from one end to the other. Wormholes that could be crossed in both directions, known as traversable wormholes, would be possible only if exotic matter with negative energy density could be used to stabilize them.

Schwarzschild wormholes, also known as "Einstein–Rosen bridges" (named after Albert Einstein and Nathan Rosen), are connections between areas of space that can be modeled as vacuum solutions to the Einstein field equations, and that are now understood to be intrinsic parts of the maximally extended version of the Schwarzschild metric describing an eternal black hole with no charge and no rotation. Here, "maximally extended" refers to the idea that the spacetime should not have any "edges": it should be possible to continue this path arbitrarily far into the particle's future or past for any possible trajectory of a free-falling particle (following a geodesic in the spacetime).

In order to satisfy this requirement, it turns out that in addition to the black hole interior region that particles enter when they fall through the event horizon from the outside, there must be a separate white hole interior region that allows us to extrapolate the trajectories of particles that an outside observer sees rising up "away" from the event horizon. And just as there are two separate interior regions of the maximally extended spacetime, there are also two separate exterior regions, sometimes called two different "universes", with the second universe allowing us to extrapolate some possible particle trajectories in the two interior regions. This means that the interior black hole region can contain a mix of particles that fell in from either universe (and thus an observer who fell in from one universe might be able to see light that fell in from the other one), and likewise particles from the interior white hole region can escape into either universe. All four regions can be seen in a spacetime diagram that uses Kruskal–Szekeres coordinates.

In this spacetime, it is possible to come up with coordinate systems such that if a hypersurface of constant time (a set of points that all have the same time coordinate, such that every point on the surface has a space-like separation, giving what is called a 'space-like surface') is picked and an "embedding diagram" drawn depicting the curvature of space at that time, the embedding diagram will look like a tube connecting the two exterior regions, known as an "Einstein–Rosen bridge". Note that the Schwarzschild metric describes an idealized black hole that exists eternally from the perspective of external observers; a more realistic black hole that forms at some particular time from a collapsing star would require a different metric. When the infalling stellar matter is added to a diagram of a black hole's history, it removes the part of the diagram corresponding to the white hole interior region, along with the part of the diagram corresponding to the other universe.

The Einstein–Rosen bridge was discovered by Ludwig Flamm in 1916, a few months after Schwarzschild published his solution, and was rediscovered by Albert Einstein and his colleague Nathan Rosen, who published their result in 1935. However, in 1962, John Archibald Wheeler and Robert W. Fuller published a paper showing that this type of wormhole is unstable if it connects two parts of the same universe, and that it will pinch off too quickly for light (or any particle moving slower than light) that falls in from one exterior region to make it to the other exterior region.

According to general relativity, the gravitational collapse of a sufficiently compact mass forms a singular Schwarzschild black hole. In the Einstein–Cartan–Sciama–Kibble theory of gravity, however, it forms a regular Einstein–Rosen bridge. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. Torsion naturally accounts for the quantum-mechanical, intrinsic angular momentum (spin) of matter. The minimal coupling between torsion and Dirac spinors generates a repulsive spin–spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction prevents the formation of a gravitational singularity. Instead, the collapsing matter reaches an enormous but finite density and rebounds, forming the other side of the bridge.

Although Schwarzschild wormholes are not traversable in both directions, their existence inspired Kip Thorne to imagine traversable wormholes created by holding the "throat" of a Schwarzschild wormhole open with exotic matter (material that has negative mass/energy).

Other non-traversable wormholes include "Lorentzian wormholes" (first proposed by John Archibald Wheeler in 1957), wormholes creating a spacetime foam in a general relativistic spacetime manifold depicted by a Lorentzian manifold, and "Euclidean wormholes" (named after Euclidean manifold, a structure of Riemannian manifold).

The Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary matter vacuum energy, and it has been shown theoretically that quantum field theory allows states where energy can be "arbitrarily" negative at a given point. Many physicists, such as Stephen Hawking, Kip Thorne, and others, argued that such effects might make it possible to stabilize a traversable wormhole. The only known natural process that is theoretically predicted to form a wormhole in the context of general relativity and quantum mechanics was put forth by Leonard Susskind in his ER=EPR conjecture. The quantum foam hypothesis is sometimes used to suggest that tiny wormholes might appear and disappear spontaneously at the Planck scale, and stable versions of such wormholes have been suggested as dark matter candidates. It has also been proposed that, if a tiny wormhole held open by a negative mass cosmic string had appeared around the time of the Big Bang, it could have been inflated to macroscopic size by cosmic inflation.

Lorentzian traversable wormholes would allow travel in both directions from one part of the universe to another part of that same universe very quickly or would allow travel from one universe to another. The possibility of traversable wormholes in general relativity was first demonstrated in a 1973 paper by Homer Ellis and independently in a 1973 paper by K. A. Bronnikov. Ellis analyzed the topology and the geodesics of the Ellis drainhole, showing it to be geodesically complete, horizonless, singularity-free, and fully traversable in both directions. The drainhole is a solution manifold of Einstein's field equations for a vacuum space-time, modified by inclusion of a scalar field minimally coupled to the Ricci tensor with antiorthodox polarity (negative instead of positive). (Ellis specifically rejected referring to the scalar field as 'exotic' because of the antiorthodox coupling, finding arguments for doing so unpersuasive.) The solution depends on two parameters: , which fixes the strength of its gravitational field, and , which determines the curvature of its spatial cross sections. When is set equal to 0, the drainhole's gravitational field vanishes. What is left is the Ellis wormhole, a nongravitating, purely geometric, traversable wormhole.
Kip Thorne and his graduate student Mike Morris, unaware of the 1973 papers by Ellis and Bronnikov, manufactured, and in 1988 published, a duplicate of the Ellis wormhole for use as a tool for teaching general relativity. For this reason, the type of traversable wormhole they proposed, held open by a spherical shell of exotic matter, was from 1988 to 2015 referred to in the literature as a "Morris–Thorne wormhole". Later, other types of traversable wormholes were discovered as allowable solutions to the equations of general relativity, including a variety analyzed in a 1989 paper by Matt Visser, in which a path through the wormhole can be made where the traversing path does not pass through a region of exotic matter. However, in the pure Gauss–Bonnet gravity (a modification to general relativity involving extra spatial dimensions which is sometimes studied in the context of brane cosmology) exotic matter is not needed in order for wormholes to exist—they can exist even with no matter. A type held open by negative mass cosmic strings was put forth by Visser in collaboration with Cramer "et al.", in which it was proposed that such wormholes could have been naturally created in the early universe.

Wormholes connect two points in spacetime, which means that they would in principle allow travel in time, as well as in space. In 1988, Morris, Thorne and Yurtsever worked out how to convert a wormhole traversing space into one traversing time by accelerating one of its two mouths. However, according to general relativity, it would not be possible to use a wormhole to travel back to a time earlier than when the wormhole was first converted into a time "machine". Until this time it could not have been noticed or have been used.

To see why exotic matter is required, consider an incoming light front traveling along geodesics, which then crosses the wormhole and re-expands on the other side. The expansion goes from negative to positive. As the wormhole neck is of finite size, we would not expect caustics to develop, at least within the vicinity of the neck. According to the optical Raychaudhuri's theorem, this requires a violation of the averaged null energy condition. Quantum effects such as the Casimir effect cannot violate the averaged null energy condition in any neighborhood of space with zero curvature, but calculations in semiclassical gravity suggest that quantum effects may be able to violate this condition in curved spacetime. Although it was hoped recently that quantum effects could not violate an achronal version of the averaged null energy condition, violations have nevertheless been found, so it remains an open possibility that quantum effects might be used to support a wormhole.

In some hypotheses where general relativity is modified, it is possible to have a wormhole that does not collapse without having to resort to exotic matter. For example, this is possible with R gravity, a form of () gravity.

The improbability of faster-than-light relative speed only applies locally. Wormholes might allow effective superluminal (faster-than-light) travel by ensuring that the speed of light is not exceeded locally at any time. While traveling through a wormhole, subluminal (slower-than-light) speeds are used. If two points are connected by a wormhole whose length is shorter than the distance between them "outside" the wormhole, the time taken to traverse it could be less than the time it would take a light beam to make the journey if it took a path through the space "outside" the wormhole. However, a light beam traveling through the same wormhole would beat the traveler.

If traversable wormholes exist, they could allow time travel. A proposed time-travel machine using a traversable wormhole would hypothetically work in the following way: One end of the wormhole is accelerated to some significant fraction of the speed of light, perhaps with some advanced propulsion system, and then brought back to the point of origin. Alternatively, another way is to take one entrance of the wormhole and move it to within the gravitational field of an object that has higher gravity than the other entrance, and then return it to a position near the other entrance. For both these methods, time dilation causes the end of the wormhole that has been moved to have aged less, or become "younger", than the stationary end as seen by an external observer; however, time connects differently "through" the wormhole than "outside" it, so that synchronized clocks at either end of the wormhole will always remain synchronized as seen by an observer passing through the wormhole, no matter how the two ends move around. This means that an observer entering the "younger" end would exit the "older" end at a time when it was the same age as the "younger" end, effectively going back in time as seen by an observer from the outside. One significant limitation of such a time machine is that it is only possible to go as far back in time as the initial creation of the machine; It is more of a path through time rather than it is a device that itself moves through time, and it would not allow the technology itself to be moved backward in time.

According to current theories on the nature of wormholes, construction of a traversable wormhole would require the existence of a substance with negative energy, often referred to as "exotic matter". More technically, the wormhole spacetime requires a distribution of energy that violates various energy conditions, such as the null energy condition along with the weak, strong, and dominant energy conditions. However, it is known that quantum effects can lead to small measurable violations of the null energy condition, and many physicists believe that the required negative energy may actually be possible due to the Casimir effect in quantum physics. Although early calculations suggested a very large amount of negative energy would be required, later calculations showed that the amount of negative energy can be made arbitrarily small.

In 1993, Matt Visser argued that the two mouths of a wormhole with such an induced clock difference could not be brought together without inducing quantum field and gravitational effects that would either make the wormhole collapse or the two mouths repel each other, or otherwise prevent information from passing through the wormhole. Because of this, the two mouths could not be brought close enough for causality violation to take place. However, in a 1997 paper, Visser hypothesized that a complex "Roman ring" (named after Tom Roman) configuration of an N number of wormholes arranged in a symmetric polygon could still act as a time machine, although he concludes that this is more likely a flaw in classical quantum gravity theory rather than proof that causality violation is possible.

A possible resolution to the paradoxes resulting from wormhole-enabled time travel rests on the many-worlds interpretation of quantum mechanics.

In 1991 David Deutsch showed that quantum theory is fully consistent (in the sense that the so-called density matrix can be made free of discontinuities) in spacetimes with closed timelike curves. However, later it was shown that such model of closed timelike curve can have internal inconsistencies as it will lead to strange phenomena like distinguishing non-orthogonal quantum states and distinguishing proper and improper mixture. Accordingly, the destructive positive feedback loop of virtual particles circulating through a wormhole time machine, a result indicated by semi-classical calculations, is averted. A particle returning from the future does not return to its universe of origination but to a parallel universe. This suggests that a wormhole time machine with an exceedingly short time jump is a theoretical bridge between contemporaneous parallel universes.

Because a wormhole time-machine introduces a type of nonlinearity into quantum theory, this sort of communication between parallel universes is consistent with Joseph Polchinski's proposal of an Everett phone (named after Hugh Everett) in Steven Weinberg's formulation of nonlinear quantum mechanics.

The possibility of communication between parallel universes has been dubbed interuniversal travel.

Theories of "wormhole metrics" describe the spacetime geometry of a wormhole and serve as theoretical models for time travel. An example of a (traversable) wormhole metric is the following:
first presented by Ellis (see Ellis wormhole) as a special case of the Ellis drainhole.

One type of non-traversable wormhole metric is the Schwarzschild solution (see the first diagram):

The original Einstein–Rosen bridge was described in an article published in July 1935.

For the Schwarzschild spherically symmetric static solution 
where formula_1 is the proper time and formula_2.

If one replaces formula_3 with formula_4 according to formula_5

For the combined field, gravity and electricity, Einstein and Rosen derived the following Schwarzschild static spherically symmetric solution
where formula_6 is the electric charge.

The field equations without denominators in the case when formula_7 can be written

In order to eliminate singularities, if one replaces formula_3 by formula_4 according to the equation:
and with formula_7 one obtains

Wormholes are a common element in science fiction because they allow interstellar, intergalactic, and sometimes even interuniversal travel within human lifetime scales. In fiction, wormholes have also served as a method for time travel.




</doc>
<doc id="177602" url="https://en.wikipedia.org/wiki?curid=177602" title="Outer space">
Outer space

Outer space, or simply space, is the expanse that exists beyond the Earth and between celestial bodies. Outer space is not completely empty—it is a hard vacuum containing a low density of particles, predominantly a plasma of hydrogen and helium, as well as electromagnetic radiation, magnetic fields, neutrinos, dust, and cosmic rays. The baseline temperature of outer space, as set by the background radiation from the Big Bang, is . The plasma between galaxies accounts for about half of the baryonic (ordinary) matter in the universe; it has a number density of less than one hydrogen atom per cubic metre and a temperature of millions of kelvins. Local concentrations of matter have condensed into stars and galaxies. Studies indicate that 90% of the mass in most galaxies is in an unknown form, called dark matter, which interacts with other matter through gravitational but not electromagnetic forces. Observations suggest that the majority of the mass-energy in the observable universe is "dark energy", a type of vacuum energy that is poorly understood. Intergalactic space takes up most of the volume of the universe, but even galaxies and star systems consist almost entirely of empty space.

Outer space does not begin at a definite altitude above the Earth's surface. However, the Kármán line, an altitude of above sea level, is conventionally used as the start of outer space in space treaties and for aerospace records keeping. The framework for international space law was established by the Outer Space Treaty, which entered into force on 10 October 1967. This treaty precludes any claims of national sovereignty and permits all states to freely explore outer space. Despite the drafting of UN resolutions for the peaceful uses of outer space, anti-satellite weapons have been tested in Earth orbit.

Humans began the physical exploration of space during the 20th century with the advent of high-altitude balloon flights. This was followed by manned rocket flights and, then, manned Earth orbit, first achieved by Yuri Gagarin of the Soviet Union in 1961. Due to the high cost of getting into space, manned spaceflight has been limited to low Earth orbit and the Moon. On the other hand, unmanned spacecraft have reached all of the known planets in the Solar System. 

Outer space represents a challenging environment for human exploration because of the hazards of vacuum and radiation. Microgravity also has a negative effect on human physiology that causes both muscle atrophy and bone loss. In addition to these health and environmental issues, the economic cost of putting objects, including humans, into space is very high.

In 350 BCE, Greek philosopher Aristotle suggested that "nature abhors a vacuum", a principle that became known as the "horror vacui". This concept built upon a 5th-century BCE ontological argument by the Greek philosopher Parmenides, who denied the possible existence of a void in space. Based on this idea that a vacuum could not exist, in the West it was widely held for many centuries that space could not be empty. As late as the 17th century, the French philosopher René Descartes argued that the entirety of space must be filled.

In ancient China, the 2nd-century astronomer Zhang Heng became convinced that space must be infinite, extending well beyond the mechanism that supported the Sun and the stars. The surviving books of the Hsüan Yeh school said that the heavens were boundless, "empty and void of substance". Likewise, the "sun, moon, and the company of stars float in the empty space, moving or standing still".

The Italian scientist Galileo Galilei knew that air had mass and so was subject to gravity. In 1640, he demonstrated that an established force resisted the formation of a vacuum. However, it would remain for his pupil Evangelista Torricelli to create an apparatus that would produce a partial vacuum in 1643. This experiment resulted in the first mercury barometer and created a scientific sensation in Europe. The French mathematician Blaise Pascal reasoned that if the column of mercury was supported by air, then the column ought to be shorter at higher altitude where the air pressure is lower. In 1648, his brother-in-law, Florin Périer, repeated the experiment on the Puy de Dôme mountain in central France and found that the column was shorter by three inches. This decrease in pressure was further demonstrated by carrying a half-full balloon up a mountain and watching it gradually expand, then contract upon descent.

In 1650, German scientist Otto von Guericke constructed the first vacuum pump: a device that would further refute the principle of "horror vacui". He correctly noted that the atmosphere of the Earth surrounds the planet like a shell, with the density gradually declining with altitude. He concluded that there must be a vacuum between the Earth and the Moon.

Back in the 15th century, German theologian Nicolaus Cusanus speculated that the Universe lacked a center and a circumference. He believed that the Universe, while not infinite, could not be held as finite as it lacked any bounds within which it could be contained. These ideas led to speculations as to the infinite dimension of space by the Italian philosopher Giordano Bruno in the 16th century. He extended the Copernican heliocentric cosmology to the concept of an infinite Universe filled with a substance he called aether, which did not resist the motion of heavenly bodies. English philosopher William Gilbert arrived at a similar conclusion, arguing that the stars are visible to us only because they are surrounded by a thin aether or a void. This concept of an aether originated with ancient Greek philosophers, including Aristotle, who conceived of it as the medium through which the heavenly bodies move.

The concept of a Universe filled with a luminiferous aether retained support among some scientists until the early 20th century. This form of aether was viewed as the medium through which light could propagate. In 1887, the Michelson–Morley experiment tried to detect the Earth's motion through this medium by looking for changes in the speed of light depending on the direction of the planet's motion. However, the null result indicated something was wrong with the concept. The idea of the luminiferous aether was then abandoned. It was replaced by Albert Einstein's theory of special relativity, which holds that the speed of light in a vacuum is a fixed constant, independent of the observer's motion or frame of reference.

The first professional astronomer to support the concept of an infinite Universe was the Englishman Thomas Digges in 1576. But the scale of the Universe remained unknown until the first successful measurement of the distance to a nearby star in 1838 by the German astronomer Friedrich Bessel. He showed that the star 61 Cygni had a parallax of just 0.31 arcseconds (compared to the modern value of 0.287″). This corresponds to a distance of over 10 light years. In 1917, Heber Curtis noted that novae in spiral nebulae were, on average, 10 magnitudes fainter than galactic novae, suggesting that the former are 100 times further away. The distance to the Andromeda Galaxy was determined in 1923 by American astronomer Edwin Hubble by measuring the brightness of cepheid variables in that galaxy, a new technique discovered by Henrietta Leavitt. This established that the Andromeda galaxy, and by extension all galaxies, lay well outside the Milky Way.

The modern concept of outer space is based on the "Big Bang" cosmology, first proposed in 1931 by the Belgian physicist Georges Lemaître. This theory holds that the universe originated from a very dense form that has since undergone continuous expansion.

The earliest known estimate of the temperature of outer space was by the Swiss physicist Charles É. Guillaume in 1896. Using the estimated radiation of the background stars, he concluded that space must be heated to a temperature of 5–6 K. British physicist Arthur Eddington made a similar calculation to derive a temperature of 3.18 K in 1926. German physicist Erich Regener used the total measured energy of cosmic rays to estimate an intergalactic temperature of 2.8 K in 1933. American physicists Ralph Alpher and Robert Herman predicted 5 K for the temperature of space in 1948, based on the gradual decrease in background energy following the then-new Big Bang theory. The modern measurement of the cosmic microwave background is about 2.7K.

The term "outward space" was used in 1842 by the English poet Lady Emmeline Stuart-Wortley in her poem "The Maiden of Moscow". The expression "outer space" was used as an astronomical term by Alexander von Humboldt in 1845. It was later popularized in the writings of H. G. Wells in 1901. The shorter term "space" is older, first used to mean the region beyond Earth's sky in John Milton's "Paradise Lost" in 1667.

According to the Big Bang theory, the very early Universe was an extremely hot and dense state about 13.8 billion years ago which rapidly expanded. About 380,000 years later the Universe had cooled sufficiently to allow protons and electrons to combine and form hydrogen—the so-called recombination epoch. When this happened, matter and energy became decoupled, allowing photons to travel freely through the continually expanding space. Matter that remained following the initial expansion has since undergone gravitational collapse to create stars, galaxies and other astronomical objects, leaving behind a deep vacuum that forms what is now called outer space. As light has a finite velocity, this theory also constrains the size of the directly observable universe. This leaves open the question as to whether the Universe is finite or infinite.

The present day shape of the universe has been determined from measurements of the cosmic microwave background using satellites like the Wilkinson Microwave Anisotropy Probe. These observations indicate that the spatial geometry of the observable universe is "flat", meaning that photons on parallel paths at one point remain parallel as they travel through space to the limit of the observable universe, except for local gravity. The flat Universe, combined with the measured mass density of the Universe and the accelerating expansion of the Universe, indicates that space has a non-zero vacuum energy, which is called dark energy.

Estimates put the average energy density of the present day Universe at the equivalent of 5.9 protons per cubic meter, including dark energy, dark matter, and baryonic matter (ordinary matter composed of atoms). The atoms account for only 4.6% of the total energy density, or a density of one proton per four cubic meters. The density of the Universe, however, is clearly not uniform; it ranges from relatively high density in galaxies—including very high density in structures within galaxies, such as planets, stars, and black holes—to conditions in vast voids that have much lower density, at least in terms of visible matter. Unlike matter and dark matter, dark energy seems not to be concentrated in galaxies: although dark energy may account for a majority of the mass-energy in the Universe, dark energy's influence is 5 orders of magnitude smaller than the influence of gravity from matter and dark matter within the Milky Way.

Outer space is the closest known approximation to a perfect vacuum. It has effectively no friction, allowing stars, planets, and moons to move freely along their ideal orbits, following the initial formation stage. However, even the deep vacuum of intergalactic space is not devoid of matter, as it contains a few hydrogen atoms per cubic meter. By comparison, the air humans breathe contains about 10 molecules per cubic meter. The low density of matter in outer space means that electromagnetic radiation can travel great distances without being scattered: the mean free path of a photon in intergalactic space is about 10 km, or 10 billion light years. In spite of this, extinction, which is the absorption and scattering of photons by dust and gas, is an important factor in galactic and intergalactic astronomy.

Stars, planets, and moons retain their atmospheres by gravitational attraction. Atmospheres have no clearly delineated upper boundary: the density of atmospheric gas gradually decreases with distance from the object until it becomes indistinguishable from outer space. The Earth's atmospheric pressure drops to about Pa at of altitude, compared to 100,000 Pa for the International Union of Pure and Applied Chemistry (IUPAC) definition of standard pressure. Above this altitude, isotropic gas pressure rapidly becomes insignificant when compared to radiation pressure from the Sun and the dynamic pressure of the solar wind. The thermosphere in this range has large gradients of pressure, temperature and composition, and varies greatly due to space weather.

The temperature of outer space is measured in terms of the kinetic activity of the gas, as it is on Earth. However, the radiation of outer space has a different temperature than the kinetic temperature of the gas, meaning that the gas and radiation are not in thermodynamic equilibrium. All of the observable universe is filled with photons that were created during the Big Bang, which is known as the cosmic microwave background radiation (CMB). (There is quite likely a correspondingly large number of neutrinos called the cosmic neutrino background.) The current black body temperature of the background radiation is about . The gas temperatures in outer space are always at least the temperature of the CMB but can be much higher. For example, the corona of the Sun reaches temperatures over 1.2–2.6 million K.

Magnetic fields have been detected in the space around just about every class of celestial object. Star formation in spiral galaxies can generate small-scale dynamos, creating turbulent magnetic field strengths of around 5–10 μG. The Davis–Greenstein effect causes elongated dust grains to align themselves with a galaxy's magnetic field, resulting in weak optical polarization. This has been used to show ordered magnetic fields exist in several nearby galaxies. Magneto-hydrodynamic processes in active elliptical galaxies produce their characteristic jets and radio lobes. Non-thermal radio sources have been detected even among the most distant, high-z sources, indicating the presence of magnetic fields.

Outside a protective atmosphere and magnetic field, there are few obstacles to the passage through space of energetic subatomic particles known as cosmic rays. These particles have energies ranging from about 10 eV up to an extreme 10 eV of ultra-high-energy cosmic rays. The peak flux of cosmic rays occurs at energies of about 10 eV, with approximately 87% protons, 12% helium nuclei and 1% heavier nuclei. In the high energy range, the flux of electrons is only about 1% of that of protons. Cosmic rays can damage electronic components and pose a health threat to space travelers. According to astronauts, like Don Pettit, space has a burned/metallic odor that clings to their suits and equipment, similar to the scent of an arc welding torch.

Space is a partial vacuum: its different regions are defined by the various atmospheres and "winds" that dominate within them, and extend to the point at which those winds give way to those beyond. Geospace extends from Earth's atmosphere to the outer reaches of Earth's magnetic field, whereupon it gives way to the solar wind of interplanetary space. Interplanetary space extends to the heliopause, whereupon the solar wind gives way to the winds of the interstellar medium. Interstellar space then continues to the edges of the galaxy, where it fades into the intergalactic void.

Geospace is the region of outer space near Earth, including the upper atmosphere and magnetosphere. The Van Allen radiation belts lie within the geospace. The outer boundary of geospace is the magnetopause, which forms an interface between the Earth's magnetosphere and the solar wind. The inner boundary is the ionosphere. The variable space-weather conditions of geospace are affected by the behavior of the Sun and the solar wind; the subject of geospace is interlinked with heliophysics—the study of the Sun and its impact on the planets of the Solar System.

The day-side magnetopause is compressed by solar-wind pressure—the subsolar distance from the center of the Earth is typically 10 Earth radii. On the night side, the solar wind stretches the magnetosphere to form a magnetotail that sometimes extends out to more than 100–200 Earth radii. For roughly four days of each month, the lunar surface is shielded from the solar wind as the Moon passes through the magnetotail.

Geospace is populated by electrically charged particles at very low densities, the motions of which are controlled by the Earth's magnetic field. These plasmas form a medium from which storm-like disturbances powered by the solar wind can drive electrical currents into the Earth's upper atmosphere. Geomagnetic storms can disturb two regions of geospace, the radiation belts and the ionosphere. These storms increase fluxes of energetic electrons that can permanently damage satellite electronics, interfering with shortwave radio communication and GPS location and timing. Magnetic storms can also be a hazard to astronauts, even in low Earth orbit. They also create aurorae seen at high latitudes in an oval surrounding the geomagnetic poles.

Although it meets the definition of outer space, the atmospheric density within the first few hundred kilometers above the Kármán line is still sufficient to produce significant drag on satellites. This region contains material left over from previous manned and unmanned launches that are a potential hazard to spacecraft. Some of this debris re-enters Earth's atmosphere periodically.

Earth's gravity keeps the Moon in orbit at an average distance of . The region outside Earth's atmosphere and extending out to just beyond the Moon's orbit, including the Lagrangian points, is sometimes referred to as cislunar space.

The region of space where Earth's gravity remains dominant against gravitational perturbations from the Sun is called the Hill sphere. This extends well out into translunar space to a distance of roughly 1% of the mean distance from Earth to the Sun, or .

Deep space has different definitions as to where it starts. It has been defined by the United States government and others as any region beyond cislunar space. The International Telecommunication Union responsible for radio communication (including satellites) defines the beginning of deep space at about 5 times that distance ().

Interplanetary space is defined by the solar wind, a continuous stream of charged particles emanating from the Sun that creates a very tenuous atmosphere (the heliosphere) for billions of kilometers into space. This wind has a particle density of 5–10 protons/cm and is moving at a velocity of . Interplanetary space extends out to the heliopause where the influence of the galactic environment starts to dominate over the magnetic field and particle flux from the Sun. The distance and strength of the heliopause varies depending on the activity level of the solar wind. The heliopause in turn deflects away low-energy galactic cosmic rays, with this modulation effect peaking during solar maximum.

The volume of interplanetary space is a nearly total vacuum, with a mean free path of about one astronomical unit at the orbital distance of the Earth. However, this space is not completely empty, and is sparsely filled with cosmic rays, which include ionized atomic nuclei and various subatomic particles. There is also gas, plasma and dust, small meteors, and several dozen types of organic molecules discovered to date by microwave spectroscopy. A cloud of interplanetary dust is visible at night as a faint band called the zodiacal light.

Interplanetary space contains the magnetic field generated by the Sun. There are also magnetospheres generated by planets such as Jupiter, Saturn, Mercury and the Earth that have their own magnetic fields. These are shaped by the influence of the solar wind into the approximation of a teardrop shape, with the long tail extending outward behind the planet. These magnetic fields can trap particles from the solar wind and other sources, creating belts of charged particles such as the Van Allen radiation belts. Planets without magnetic fields, such as Mars, have their atmospheres gradually eroded by the solar wind.

Interstellar space is the physical space within a galaxy beyond the influence each star has upon the encompassed plasma. The contents of interstellar space are called the interstellar medium. Approximately 70% of the mass of the interstellar medium consists of lone hydrogen atoms; most of the remainder consists of helium atoms. This is enriched with trace amounts of heavier atoms formed through stellar nucleosynthesis. These atoms are ejected into the interstellar medium by stellar winds or when evolved stars begin to shed their outer envelopes such as during the formation of a planetary nebula. The cataclysmic explosion of a supernova generates an expanding shock wave consisting of ejected materials that further enrich the medium. The density of matter in the interstellar medium can vary considerably: the average is around 10 particles per m, but cold molecular clouds can hold 10–10 per m.

A number of molecules exist in interstellar space, as can tiny 0.1 μm dust particles. The tally of molecules discovered through radio astronomy is steadily increasing at the rate of about four new species per year. Large regions of higher density matter known as molecular clouds allow chemical reactions to occur, including the formation of organic polyatomic species. Much of this chemistry is driven by collisions. Energetic cosmic rays penetrate the cold, dense clouds and ionize hydrogen and helium, resulting, for example, in the trihydrogen cation. An ionized helium atom can then split relatively abundant carbon monoxide to produce ionized carbon, which in turn can lead to organic chemical reactions.

The local interstellar medium is a region of space within 100 parsecs (pc) of the Sun, which is of interest both for its proximity and for its interaction with the Solar System. This volume nearly coincides with a region of space known as the Local Bubble, which is characterized by a lack of dense, cold clouds. It forms a cavity in the Orion Arm of the Milky Way galaxy, with dense molecular clouds lying along the borders, such as those in the constellations of Ophiuchus and Taurus. (The actual distance to the border of this cavity varies from 60 to 250 pc or more.) This volume contains about 10–10 stars and the local interstellar gas counterbalances the astrospheres that surround these stars, with the volume of each sphere varying depending on the local density of the interstellar medium. The Local Bubble contains dozens of warm interstellar clouds with temperatures of up to 7,000 K and radii of 0.5–5 pc.

When stars are moving at sufficiently high peculiar velocities, their astrospheres can generate bow shocks as they collide with the interstellar medium. For decades it was assumed that the Sun had a bow shock. In 2012, data from Interstellar Boundary Explorer (IBEX) and NASA's Voyager probes showed that the Sun's bow shock does not exist. Instead, these authors argue that a subsonic bow wave defines the transition from the solar wind flow to the interstellar medium. A bow shock is the third boundary of an astrosphere after the termination shock and the astropause (called the heliopause in the Solar System).

Intergalactic space is the physical space between galaxies. Studies of the large scale distribution of galaxies show that the Universe has a foam-like structure, with groups and clusters of galaxies lying along filaments that occupy about a tenth of the total space. The remainder forms huge voids that are mostly empty of galaxies. Typically, a void spans a distance of (10–40) "h" Mpc, where "h" is the Hubble constant in units of .

Surrounding and stretching between galaxies, there is a rarefied plasma that is organized in a galactic filamentary structure. This material is called the intergalactic medium (IGM). The density of the IGM is 5–200 times the average density of the Universe. It consists mostly of ionized hydrogen; i.e. a plasma consisting of equal numbers of electrons and protons. As gas falls into the intergalactic medium from the voids, it heats up to temperatures of 10 K to 10 K, which is high enough so that collisions between atoms have enough energy to cause the bound electrons to escape from the hydrogen nuclei; this is why the IGM is ionized. At these temperatures, it is called the warm–hot intergalactic medium (WHIM). (Although the plasma is very hot by terrestrial standards, 10 K is often called "warm" in astrophysics.) Computer simulations and observations indicate that up to half of the atomic matter in the Universe might exist in this warm–hot, rarefied state. When gas falls from the filamentary structures of the WHIM into the galaxy clusters at the intersections of the cosmic filaments, it can heat up even more, reaching temperatures of 10 K and above in the so-called intracluster medium (ICM).

A spacecraft enters orbit when its centripetal acceleration due to gravity is less than or equal to the centrifugal acceleration due to the horizontal component of its velocity. For a low Earth orbit, this velocity is about ; by contrast, the fastest manned airplane speed ever achieved (excluding speeds achieved by deorbiting spacecraft) was in 1967 by the North American X-15.

To achieve an orbit, a spacecraft must travel faster than a sub-orbital spaceflight. The energy required to reach Earth orbital velocity at an altitude of is about 36 MJ/kg, which is six times the energy needed merely to climb to the corresponding altitude. Spacecraft with a perigee below about are subject to drag from the Earth's atmosphere, which decreases the orbital altitude. The rate of orbital decay depends on the satellite's cross-sectional area and mass, as well as variations in the air density of the upper atmosphere. Below about , decay becomes more rapid with lifetimes measured in days. Once a satellite descends to , it has only hours before it vaporizes in the atmosphere. The escape velocity required to pull free of Earth's gravitational field altogether and move into interplanetary space is about .

There is no clear boundary between Earth's atmosphere and space, as the density of the atmosphere gradually decreases as the altitude increases. There are several standard boundary designations, namely:

In 2009, scientists reported detailed measurements with a Supra-Thermal Ion Imager (an instrument that measures the direction and speed of ions), which allowed them to establish a boundary at above Earth. The boundary represents the midpoint of a gradual transition over tens of kilometers from the relatively gentle winds of the Earth's atmosphere to the more violent flows of charged particles in space, which can reach speeds well over .

The Outer Space Treaty provides the basic framework for international space law. It covers the legal use of outer space by nation states, and includes in its definition of "outer space" the Moon and other celestial bodies. The treaty states that outer space is free for all nation states to explore and is not subject to claims of national sovereignty. It also prohibits the deployment of nuclear weapons in outer space. The treaty was passed by the United Nations General Assembly in 1963 and signed in 1967 by the USSR, the United States of America and the United Kingdom. As of 2017, 105 state parties have either ratified or acceded to the treaty. An additional 25 states signed the treaty, without ratifying it.

Since 1958, outer space has been the subject of multiple United Nations resolutions. Of these, more than 50 have been concerning the international co-operation in the peaceful uses of outer space and preventing an arms race in space. Four additional space law treaties have been negotiated and drafted by the UN's Committee on the Peaceful Uses of Outer Space. Still, there remains no legal prohibition against deploying conventional weapons in space, and anti-satellite weapons have been successfully tested by the US, USSR, China, and in 2019, India The 1979 Moon Treaty turned the jurisdiction of all heavenly bodies (including the orbits around such bodies) over to the international community. However, this treaty has not been ratified by any nation that currently practices manned spaceflight.

In 1976, eight equatorial states (Ecuador, Colombia, Brazil, Congo, Zaire, Uganda, Kenya, and Indonesia) met in Bogotá, Colombia. With their "Declaration of the First Meeting of Equatorial Countries", or "the Bogotá Declaration", they claimed control of the segment of the geosynchronous orbital path corresponding to each country. These claims are not internationally accepted.

For the majority of human history, space was explored by observations made from the Earth's surface—initially with the unaided eye and then with the telescope. Prior to the advent of reliable rocket technology, the closest that humans had come to reaching outer space was through the use of balloon flights. In 1935, the U.S. "Explorer II" manned balloon flight reached an altitude of . This was greatly exceeded in 1942 when the third launch of the German A-4 rocket climbed to an altitude of about . In 1957, the unmanned satellite Sputnik 1 was launched by a Russian R-7 rocket, achieving Earth orbit at an altitude of . This was followed by the first human spaceflight in 1961, when Yuri Gagarin was sent into orbit on Vostok 1. The first humans to escape low-Earth orbit were Frank Borman, Jim Lovell and William Anders in 1968 on board the U.S. Apollo 8, which achieved lunar orbit and reached a maximum distance of from the Earth.

The first spacecraft to reach escape velocity was the Soviet Luna 1, which performed a fly-by of the Moon in 1959. In 1961, Venera 1 became the first planetary probe. It revealed the presence of the solar wind and performed the first fly-by of Venus, although contact was lost before reaching Venus. The first successful planetary mission was the 1962 fly-by of Venus by Mariner 2. The first fly-by of Mars was by Mariner 4 in 1964. Since that time, unmanned spacecraft have successfully examined each of the Solar System's planets, as well their moons and many minor planets and comets. They remain a fundamental tool for the exploration of outer space, as well as observation of the Earth. In August 2012, "Voyager 1" became the first man-made object to leave the Solar System and enter interstellar space.

The absence of air makes outer space an ideal location for astronomy at all wavelengths of the electromagnetic spectrum. This is evidenced by the spectacular pictures sent back by the Hubble Space Telescope, allowing light from more than 13 billion years ago—almost to the time of the Big Bang—to be observed. However, not every location in space is ideal for a telescope. The interplanetary zodiacal dust emits a diffuse near-infrared radiation that can mask the emission of faint sources such as extrasolar planets. Moving an infrared telescope out past the dust increases its effectiveness. Likewise, a site like the Daedalus crater on the far side of the Moon could shield a radio telescope from the radio frequency interference that hampers Earth-based observations.

Unmanned spacecraft in Earth orbit are an essential technology of modern civilization. They allow direct monitoring of weather conditions, relay long-range communications like television, provide a means of precise navigation, and allow remote sensing of the Earth. The latter role serves a wide variety of purposes, including tracking soil moisture for agriculture, prediction of water outflow from seasonal snow packs, detection of diseases in plants and trees, and surveillance of military activities.

The deep vacuum of space could make it an attractive environment for certain industrial processes, such as those requiring ultraclean surfaces. However, like asteroid mining, space manufacturing requires significant investment with little prospect of immediate return. An important factor in the total expense is the high cost of placing mass into Earth orbit: $– per kg in inflation-adjusted dollars, according to a 2006 estimate. Proposed concepts for addressing this issue include non-rocket spacelaunch, momentum exchange tethers, and space elevators.

Interstellar travel for a human crew remains at present only a theoretical possibility. The distances to the nearest stars will require new technological developments and the ability to safely sustain crews for journeys lasting several decades. For example, the Daedalus Project study, which proposed a spacecraft powered by the fusion of Deuterium and He, would require 36 years to reach the nearby Alpha Centauri system. Other proposed interstellar propulsion systems include light sails, ramjets, and beam-powered propulsion. More advanced propulsion systems could use antimatter as a fuel, potentially reaching relativistic velocities.

Despite the harsh environment, several life forms have been found that can withstand extreme space conditions for extended periods. Species of lichen carried on the ESA BIOPAN facility survived exposure for ten days in 2007. Seeds of "Arabidopsis thaliana" and "Nicotiana tabacum" germinated after being exposed to space for 1.5 years. A strain of "bacillus subtilis" has survived 559 days when exposed to low-Earth orbit or a simulated martian environment. The lithopanspermia hypothesis suggests that rocks ejected into outer space from life-harboring planets may successfully transport life forms to another habitable world. A conjecture is that just such a scenario occurred early in the history of the Solar System, with potentially microorganism-bearing rocks being exchanged between Venus, Earth, and Mars.

Even at relatively low altitudes in the Earth's atmosphere, conditions are hostile to the human body. The altitude where atmospheric pressure matches the vapor pressure of water at the temperature of the human body is called the Armstrong line, named after American physician Harry G. Armstrong. It is located at an altitude of around . At or above the Armstrong line, fluids in the throat and lungs boil away. More specifically, exposed bodily liquids such as saliva, tears, and liquids in the lungs boil away. Hence, at this altitude, human survival requires a pressure suit, or a pressurized capsule.

Out in space, sudden exposure of an unprotected human to very low pressure, such as during a rapid decompression, can cause pulmonary barotrauma—a rupture of the lungs, due to the large pressure differential between inside and outside the chest. Even if the subject's airway is fully open, the flow of air through the windpipe may be too slow to prevent the rupture. Rapid decompression can rupture eardrums and sinuses, bruising and blood seep can occur in soft tissues, and shock can cause an increase in oxygen consumption that leads to hypoxia.

As a consequence of rapid decompression, oxygen dissolved in the blood empties into the lungs to try to equalize the partial pressure gradient. Once the deoxygenated blood arrives at the brain, humans lose consciousness after a few seconds and die of hypoxia within minutes. Blood and other body fluids boil when the pressure drops below 6.3 kPa, and this condition is called ebullism. The steam may bloat the body to twice its normal size and slow circulation, but tissues are elastic and porous enough to prevent rupture. Ebullism is slowed by the pressure containment of blood vessels, so some blood remains liquid. Swelling and ebullism can be reduced by containment in a pressure suit. The Crew Altitude Protection Suit (CAPS), a fitted elastic garment designed in the 1960s for astronauts, prevents ebullism at pressures as low as 2 kPa. Supplemental oxygen is needed at to provide enough oxygen for breathing and to prevent water loss, while above pressure suits are essential to prevent ebullism. Most space suits use around 30–39 kPa of pure oxygen, about the same as on the Earth's surface. This pressure is high enough to prevent ebullism, but evaporation of nitrogen dissolved in the blood could still cause decompression sickness and gas embolisms if not managed.

Humans evolved for life in Earth gravity, and exposure to weightlessness has been shown to have deleterious effects on human health. Initially, more than 50% of astronauts experience space motion sickness. This can cause nausea and vomiting, vertigo, headaches, lethargy, and overall malaise. The duration of space sickness varies, but it typically lasts for 1–3 days, after which the body adjusts to the new environment. Longer-term exposure to weightlessness results in muscle atrophy and deterioration of the skeleton, or spaceflight osteopenia. These effects can be minimized through a regimen of exercise. Other effects include fluid redistribution, slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, nasal congestion, sleep disturbance, and puffiness of the face.

For long-duration space travel, radiation can pose an acute health hazard.
Exposure to high-energy, ionizing cosmic rays can result in fatigue, nausea, vomiting, as well as damage to the immune system and changes to the white blood cell count. Over longer durations, symptoms include an increased risk of cancer, plus damage to the eyes, nervous system, lungs and the gastrointestinal tract. On a round-trip Mars mission lasting three years, a large fraction of the cells in an astronaut's body would be traversed and potentially damaged by high energy nuclei. The energy of such particles is significantly diminished by the shielding provided by the walls of a spacecraft and can be further diminished by water containers and other barriers. However, the impact of the cosmic rays upon the shielding produces additional radiation that can affect the crew. Further research is needed to assess the radiation hazards and determine suitable countermeasures.





</doc>
<doc id="27667" url="https://en.wikipedia.org/wiki?curid=27667" title="Space">
Space

Space is the boundless three-dimensional extent in which objects and events have relative position and direction. Physical space is often conceived in three linear dimensions, although modern physicists usually consider it, with time, to be part of a boundless four-dimensional continuum known as spacetime. The concept of space is considered to be of fundamental importance to an understanding of the physical universe. However, disagreement continues between philosophers over whether it is itself an entity, a relationship between entities, or part of a conceptual framework.

Debates concerning the nature, essence and the mode of existence of space date back to antiquity; namely, to treatises like the "Timaeus" of Plato, or Socrates in his reflections on what the Greeks called "khôra" (i.e. "space"), or in the "Physics" of Aristotle (Book IV, Delta) in the definition of "topos" (i.e. place), or in the later "geometrical conception of place" as "space "qua" extension" in the "Discourse on Place" ("Qawl fi al-Makan") of the 11th-century Arab polymath Alhazen. Many of these classical philosophical questions were discussed in the Renaissance and then reformulated in the 17th century, particularly during the early development of classical mechanics. In Isaac Newton's view, space was absolute—in the sense that it existed permanently and independently of whether there was any matter in the space. Other natural philosophers, notably Gottfried Leibniz, thought instead that space was in fact a collection of relations between objects, given by their distance and direction from one another. In the 18th century, the philosopher and theologian George Berkeley attempted to refute the "visibility of spatial depth" in his "Essay Towards a New Theory of Vision". Later, the metaphysician Immanuel Kant said that the concepts of space and time are not empirical ones derived from experiences of the outside world—they are elements of an already given systematic framework that humans possess and use to structure all experiences. Kant referred to the experience of "space" in his "Critique of Pure Reason" as being a subjective "pure "a priori" form of intuition".

In the 19th and 20th centuries mathematicians began to examine geometries that are non-Euclidean, in which space is conceived as "curved", rather than "flat". According to Albert Einstein's theory of general relativity, space around gravitational fields deviates from Euclidean space. Experimental tests of general relativity have confirmed that non-Euclidean geometries provide a better model for the shape of space.

Galilean and Cartesian theories about space, matter and motion are at the foundation of the Scientific Revolution, which is understood to have culminated with the publication of Newton's "Principia" in 1687. Newton's theories about space and time helped him explain the movement of objects. While his theory of space is considered the most influential in Physics, it emerged from his predecessors' ideas about the same.

As one of the pioneers of modern science, Galilei revised the established Aristotelian and Ptolemaic ideas about a geocentric cosmos. He backed the Copernican theory that the universe was heliocentric, with a stationary sun at the center and the planets—including the Earth—revolving around the sun. If the Earth moved, the Aristotelian belief that its natural tendency was to remain at rest was in question. Galilei wanted to prove instead that the sun moved around its axis, that motion was as natural to an object as the state of rest. In other words, for Galilei, celestial bodies, including the Earth, were naturally inclined to move in circles. This view displaced another Aristotelian idea—that all objects gravitated towards their designated natural place-of-belonging.

Descartes set out to replace the Aristotelian worldview with a theory about space and motion as determined by natural laws. In other words, he sought a metaphysical foundation or a mechanical explanation for his theories about matter and motion. Cartesian space was Euclidean in structure—infinite, uniform and flat. It was defined as that which contained matter; conversely, matter by definition had a spatial extension so that there was no such thing as empty space.

The Cartesian notion of space is closely linked to his theories about the nature of the body, mind and matter. He is famously known for his "cogito ergo sum" (I think therefore I am), or the idea that we can only be certain of the fact that we can doubt, and therefore think and therefore exist. His theories belong to the rationalist tradition, which attributes knowledge about the world to our ability to think rather than to our experiences, as the empiricists believe. He posited a clear distinction between the body and mind, which is referred to as the Cartesian dualism.

Following Galilei and Descartes, during the seventeenth century the philosophy of space and time revolved around the ideas of Gottfried Leibniz, a German philosopher–mathematician, and Isaac Newton, who set out two opposing theories of what space is. Rather than being an entity that independently exists over and above other matter, Leibniz held that space is no more than the collection of spatial relations between objects in the world: "space is that which results from places taken together". Unoccupied regions are those that "could" have objects in them, and thus spatial relations with other places. For Leibniz, then, space was an idealised abstraction from the relations between individual entities or their possible locations and therefore could not be continuous but must be discrete.
Space could be thought of in a similar way to the relations between family members. Although people in the family are related to one another, the relations do not exist independently of the people.
Leibniz argued that space could not exist independently of objects in the world because that implies a difference between two universes exactly alike except for the location of the material world in each universe. But since there would be no observational way of telling these universes apart then, according to the identity of indiscernibles, there would be no real difference between them. According to the principle of sufficient reason, any theory of space that implied that there could be these two possible universes must therefore be wrong.
Newton took space to be more than relations between material objects and based his position on observation and experimentation. For a relationist there can be no real difference between inertial motion, in which the object travels with constant velocity, and non-inertial motion, in which the velocity changes with time, since all spatial measurements are relative to other objects and their motions. But Newton argued that since non-inertial motion generates forces, it must be absolute. He used the example of water in a spinning bucket to demonstrate his argument. Water in a bucket is hung from a rope and set to spin, starts with a flat surface. After a while, as the bucket continues to spin, the surface of the water becomes concave. If the bucket's spinning is stopped then the surface of the water remains concave as it continues to spin. The concave surface is therefore apparently not the result of relative motion between the bucket and the water. Instead, Newton argued, it must be a result of non-inertial motion relative to space itself. For several centuries the bucket argument was considered decisive in showing that space must exist independently of matter.

In the eighteenth century the German philosopher Immanuel Kant developed a theory of knowledge in which knowledge about space can be both "a priori" and "synthetic". According to Kant, knowledge about space is "synthetic", in that statements about space are not simply true by virtue of the meaning of the words in the statement. In his work, Kant rejected the view that space must be either a substance or relation. Instead he came to the conclusion that space and time are not discovered by humans to be objective features of the world, but imposed by us as part of a framework for organizing experience.

Euclid's "Elements" contained five postulates that form the basis for Euclidean geometry. One of these, the parallel postulate, has been the subject of debate among mathematicians for many centuries. It states that on any plane on which there is a straight line "L" and a point "P" not on "L", there is exactly one straight line "L" on the plane that passes through the point "P" and is parallel to the straight line "L". Until the 19th century, few doubted the truth of the postulate; instead debate centered over whether it was necessary as an axiom, or whether it was a theory that could be derived from the other axioms. Around 1830 though, the Hungarian János Bolyai and the Russian Nikolai Ivanovich Lobachevsky separately published treatises on a type of geometry that does not include the parallel postulate, called hyperbolic geometry. In this geometry, an infinite number of parallel lines pass through the point "P". Consequently, the sum of angles in a triangle is less than 180° and the ratio of a circle's circumference to its diameter is greater than pi. In the 1850s, Bernhard Riemann developed an equivalent theory of elliptical geometry, in which no parallel lines pass through "P". In this geometry, triangles have more than 180° and circles have a ratio of circumference-to-diameter that is less than pi.

Although there was a prevailing Kantian consensus at the time, once non-Euclidean geometries had been formalised, some began to wonder whether or not physical space is curved. Carl Friedrich Gauss, a German mathematician, was the first to consider an empirical investigation of the geometrical structure of space. He thought of making a test of the sum of the angles of an enormous stellar triangle, and there are reports that he actually carried out a test, on a small scale, by triangulating mountain tops in Germany.

Henri Poincaré, a French mathematician and physicist of the late 19th century, introduced an important insight in which he attempted to demonstrate the futility of any attempt to discover which geometry applies to space by experiment. He considered the predicament that would face scientists if they were confined to the surface of an imaginary large sphere with particular properties, known as a sphere-world. In this world, the temperature is taken to vary in such a way that all objects expand and contract in similar proportions in different places on the sphere. With a suitable falloff in temperature, if the scientists try to use measuring rods to determine the sum of the angles in a triangle, they can be deceived into thinking that they inhabit a plane, rather than a spherical surface. In fact, the scientists cannot in principle determine whether they inhabit a plane or sphere and, Poincaré argued, the same is true for the debate over whether real space is Euclidean or not. For him, which geometry was used to describe space was a matter of convention. Since Euclidean geometry is simpler than non-Euclidean geometry, he assumed the former would always be used to describe the 'true' geometry of the world.

In 1905, Albert Einstein published his special theory of relativity, which led to the concept that space and time can be viewed as a single construct known as "spacetime". In this theory, the speed of light in a vacuum is the same for all observers—which has the result that two events that appear simultaneous to one particular observer will not be simultaneous to another observer if the observers are moving with respect to one another. Moreover, an observer will measure a moving clock to tick more slowly than one that is stationary with respect to them; and objects are measured to be shortened in the direction that they are moving with respect to the observer.

Subsequently, Einstein worked on a general theory of relativity, which is a theory of how gravity interacts with spacetime. Instead of viewing gravity as a force field acting in spacetime, Einstein suggested that it modifies the geometric structure of spacetime itself. According to the general theory, time goes more slowly at places with lower gravitational potentials and rays of light bend in the presence of a gravitational field. Scientists have studied the behaviour of binary pulsars, confirming the predictions of Einstein's theories, and non-Euclidean geometry is usually used to describe spacetime.

In modern mathematics spaces are defined as sets with some added structure. They are frequently described as different types of manifolds, which are spaces that locally approximate to Euclidean space, and where the properties are defined largely on local connectedness of points that lie on the manifold. There are however, many diverse mathematical objects that are called spaces. For example, vector spaces such as function spaces may have infinite numbers of independent dimensions and a notion of distance very different from Euclidean space, and topological spaces replace the concept of distance with a more abstract idea of nearness.

Space is one of the few fundamental quantities in physics, meaning that it cannot be defined via other quantities because nothing more fundamental is known at the present. On the other hand, it can be related to other fundamental quantities. Thus, similar to other fundamental quantities (like time and mass), space can be explored via measurement and experiment.

Today, our three-dimensional space is viewed as embedded in a four-dimensional spacetime, called Minkowski space (see special relativity). The idea behind space-time is that time is hyperbolic-orthogonal to each of the three spatial dimensions.

Before Einstein's work on relativistic physics, time and space were viewed as independent dimensions. Einstein's discoveries showed that due to relativity of motion our space and time can be mathematically combined into one object–spacetime. It turns out that distances in space or in time separately are not invariant with respect to Lorentz coordinate transformations, but distances in Minkowski space-time along space-time intervals are—which justifies the name.

In addition, time and space dimensions should not be viewed as exactly equivalent in Minkowski space-time. One can freely move in space but not in time. Thus, time and space coordinates are treated differently both in special relativity (where time is sometimes considered an imaginary coordinate) and in general relativity (where different signs are assigned to time and space components of spacetime metric).

Furthermore, in Einstein's general theory of relativity, it is postulated that space-time is geometrically distorted – "curved" – near to gravitationally significant masses.

One consequence of this postulate, which follows from the equations of general relativity, is the prediction of moving ripples of space-time, called gravitational waves. While indirect evidence for these waves has been found (in the motions of the Hulse–Taylor binary system, for example) experiments attempting to directly measure these waves are ongoing at the LIGO and Virgo collaborations. LIGO scientists reported the first such direct observation of gravitational waves on 14 September 2015.

Relativity theory leads to the cosmological question of what shape the universe is, and where space came from. It appears that space was created in the Big Bang, 13.8 billion years ago and has been expanding ever since. The overall shape of space is not known, but space is known to be expanding very rapidly due to the cosmic inflation.

The measurement of "physical space" has long been important. Although earlier societies had developed measuring systems, the International System of Units, (SI), is now the most common system of units used in the measuring of space, and is almost universally used.

Currently, the standard space interval, called a standard meter or simply meter, is defined as the distance traveled by light in a vacuum during a time interval of exactly 1/299,792,458 of a second. This definition coupled with present definition of the second is based on the special theory of relativity in which the speed of light plays the role of a fundamental constant of nature.

Geography is the branch of science concerned with identifying and describing places on Earth, utilizing spatial awareness to try to understand why things exist in specific locations. Cartography is the mapping of spaces to allow better navigation, for visualization purposes and to act as a locational device. Geostatistics apply statistical concepts to collected spatial data of Earth to create an estimate for unobserved phenomena.

Geographical space is often considered as land, and can have a relation to ownership usage (in which space is seen as property or territory). While some cultures assert the rights of the individual in terms of ownership, other cultures will identify with a communal approach to land ownership, while still other cultures such as Australian Aboriginals, rather than asserting ownership rights to land, invert the relationship and consider that they are in fact owned by the land. Spatial planning is a method of regulating the use of space at land-level, with decisions made at regional, national and international levels. Space can also impact on human and cultural behavior, being an important factor in architecture, where it will impact on the design of buildings and structures, and on farming.

Ownership of space is not restricted to land. Ownership of airspace and of waters is decided internationally. Other forms of ownership have been recently asserted to other spaces—for example to the radio bands of the electromagnetic spectrum or to cyberspace.

Public space is a term used to define areas of land as collectively owned by the community, and managed in their name by delegated bodies; such spaces are open to all, while private property is the land culturally owned by an individual or company, for their own use and pleasure.

Abstract space is a term used in geography to refer to a hypothetical space characterized by complete homogeneity. When modeling activity or behavior, it is a conceptual tool used to limit extraneous variables such as terrain.

Psychologists first began to study the way space is perceived in the middle of the 19th century. Those now concerned with such studies regard it as a distinct branch of psychology. Psychologists analyzing the perception of space are concerned with how recognition of an object's physical appearance or its interactions are perceived, see, for example, visual space.

Other, more specialized topics studied include amodal perception and object permanence. The perception of surroundings is important due to its necessary relevance to survival, especially with regards to hunting and self preservation as well as simply one's idea of personal space.

Several space-related phobias have been identified, including agoraphobia (the fear of open spaces), astrophobia (the fear of celestial space) and claustrophobia (the fear of enclosed spaces).

The understanding of three-dimensional space in humans is thought to be learned during infancy using unconscious inference, and is closely related to hand-eye coordination. The visual ability to perceive the world in three dimensions is called depth perception.

Space has been studied in the social sciences from the perspectives of Marxism, feminism, postmodernism, postcolonialism, urban theory and critical geography. These theories account for the effect of the history of colonialism, transatlantic slavery and globalization on our understanding and experience of space and place. The topic has garnered attention since the 1980s, after the publication of Henri Lefebvre's "The Production of Space ." In this book, Lefebvre applies Marxist ideas about the production of commodities and accumulation of capital to discuss space as a social product. His focus is on the multiple and overlapping social processes that produce space.

In his book "The Condition of Postmodernity," David Harvey describes what he terms the "time-space compression." This is the effect of technological advances and capitalism on our perception of time, space and distance. Changes in the modes of production and consumption of capital affect and are affected by developments in transportation and technology. These advances create relationships across time and space, new markets and groups of wealthy elites in urban centers, all of which annihilate distances and affect our perception of linearity and distance.

In his book "Thirdspace," Edward Soja describes space and spatiality as an integral and neglected aspect of what he calls the "trialectics of being," the three modes that determine how we inhabit, experience and understand the world. He argues that critical theories in the Humanities and Social Sciences study the historical and social dimensions of our lived experience, neglecting the spatial dimension. He builds on Henri Lefebvre's work to address the dualistic way in which humans understand space—as either material/physical or as represented/imagined. Lefebvre's "lived space" and Soja's "thridspace" are terms that account for the complex ways in which humans understand and navigate place, which "firstspace" and "Secondspace" (Soja's terms for material and imagined spaces respectively) do not fully encompass.

Postcolonial theorist Homi Bhabha's concept of Third Space is different from Soja's Thirdspace, even though both terms offer a way to think outside the terms of a binary logic. Bhabha's Third Space is the space in which hybrid cultural forms and identities exist. In his theories, the term hybrid describes new cultural forms that emerge through the interaction between colonizer and colonized.


</doc>
<doc id="208999" url="https://en.wikipedia.org/wiki?curid=208999" title="Non-renewable resource">
Non-renewable resource

A non-renewable resource (also called a finite resource) is a natural resource that cannot be readily replaced by natural means at a quick enough pace to keep up with consumption . An example is carbon-based fossil fuel. The original organic matter, with the aid of heat and pressure, becomes a fuel such as oil or gas. Earth minerals and metal ores, fossil fuels (coal, petroleum, natural gas) and groundwater in certain aquifers are all considered non-renewable resources, though individual elements are always conserved (except in nuclear reactions).

Conversely, resources such as timber (when harvested sustainably) and wind (used to power energy conversion systems) are considered renewable resources, largely because their localized replenishment can occur within time frames meaningful to humans as well.

Earth minerals and metal ores are examples of non-renewable resources. The metals themselves are present in vast amounts in Earth's crust, and their extraction by humans only occurs where they are concentrated by natural geological processes (such as heat, pressure, organic activity, weathering and other processes) enough to become economically viable to extract. These processes generally take from tens of thousands to millions of years, through plate tectonics, tectonic subsidence and crustal recycling.

The localized deposits of metal ores near the surface which can be extracted economically by humans are non-renewable in human time-frames. There are certain rare earth minerals and elements that are more scarce and exhaustible than others. These are in high demand in manufacturing, particularly for the electronics industry.

Natural resources such as coal, petroleum (crude oil) and natural gas take thousands of years to form naturally and cannot be replaced as fast as they are being consumed. Eventually it is considered that fossil-based resources will become too costly to harvest and humanity will need to shift its reliance to other sources of energy such as solar or wind power, see renewable energy.

An alternative hypothesis is that carbon based fuel is virtually inexhaustible in human terms, if one includes all sources of carbon-based energy such as methane hydrates on the sea floor, which are vastly greater than all other carbon based fossil fuel resources combined. These sources of carbon are also considered non-renewable, although their rate of formation/replenishment on the sea floor is not known. However their extraction at economically viable costs and rates has yet to be determined.

At present, the main energy source used by humans is non-renewable fossil fuels. Since the dawn of internal combustion engine technologies in the 19th century, petroleum and other fossil fuels have remained in continual demand. As a result, conventional infrastructure and transport systems, which are fitted to combustion engines, remain prominent throughout the globe.

The modern-day fossil fuel economy is widely criticized for its lack of renewability, as well as being a contributor to climate change.

In 1987, the World Commission on Environment and Development (WCED) classified fission reactors that produce more fissile nuclear fuel than they consume (i.e. breeder reactors) among conventional renewable energy sources, such as solar and falling water. The American Petroleum Institute likewise does not consider conventional nuclear fission as renewable, but rather that breeder reactor nuclear power fuel is considered renewable and sustainable, noting that radioactive waste from used spent fuel rods remains radioactive and so has to be very carefully stored for up to a thousand years. With the careful monitoring of radioactive waste products also being required upon the use of other renewable energy sources, such as geothermal energy.

The use of nuclear technology relying on fission requires Naturally occurring radioactive material as fuel. Uranium, the most common fission fuel, is present in the ground at relatively low concentrations and mined in 19 countries. This mined uranium is used to fuel energy-generating nuclear reactors with fissionable uranium-235 which generates heat that is ultimately used to power turbines to generate electricity.

As of 2013 only a few kilograms (picture available) of uranium have been extracted from the ocean in pilot programs and it is also believed that the uranium extracted on an industrial scale from the seawater would constantly be replenished from uranium leached from the ocean floor, maintaining the seawater concentration at a stable level. In 2014, with the advances made in the efficiency of seawater uranium extraction, a paper in the journal of "Marine Science & Engineering" suggests that with, light water reactors as its target, the process would be economically competitive if implemented on a large scale.

Nuclear power provides about 6% of the world's energy and 13–14% of the world's electricity. Nuclear energy production is associated with potentially dangerous radioactive contamination as it relies upon unstable elements. In particular, nuclear power facilities produce about 200,000 metric tons of low and intermediate level waste (LILW) and 10,000 metric tons of high level waste (HLW) (including spent fuel designated as waste) each year worldwide.

Issues entirely separate from the question of the sustainability of nuclear fuel, relate to the use of nuclear fuel and the high-level radioactive waste the nuclear industry generates that if not properly contained, is highly hazardous to people and wildlife. The United Nations (UNSCEAR) estimated in 2008 that average annual human radiation exposure includes 0.01 millisievert (mSv) from the legacy of past atmospheric nuclear testing plus the Chernobyl disaster and the nuclear fuel cycle, along with 2.0 mSv from natural radioisotopes and 0.4 mSv from cosmic rays; all exposures vary by location. natural uranium in some inefficient reactor nuclear fuel cycles, becomes part of the nuclear waste "once through" stream, and in a similar manner to the scenario were this uranium remained naturally in the ground, this uranium emits various forms of radiation in a decay chain that has a half-life of about 4.5 billion years, the storage of this unused uranium and the accompanying fission reaction products have raised public concerns about risks of leaks and containment, however the knowledge gained from studying the Natural nuclear fission reactor in Oklo Gabon, has informed geologists on the proven processes that kept the waste from this 2 billion year old natural nuclear reactor that operated for hundreds of thousands of years.

Natural resources, known as renewable resources, are replaced by natural processes and forces persistent in the natural environment. There are intermittent and reoccurring renewables, and recyclable materials, which are utilized during a cycle across a certain amount of time, and can be harnessed for any number of cycles.

The production of goods and services by manufacturing products in economic systems creates many types of waste during production and after the consumer has made use of it. The material is then either incinerated, buried in a landfill or recycled for reuse. Recycling turns materials of value that would otherwise become waste into valuable resources again.
In the natural environment water, forests, plants and animals are all renewable resources, as long as they are adequately monitored, protected and conserved. Sustainable agriculture is the cultivation of plant and animal materials in a manner that preserves plant and animal ecosystems and that can improve soil health and soil fertility over the long term. The overfishing of the oceans is one example of where an industry practice or method can threaten an ecosystem, endanger species and possibly even determine whether or not a fishery is sustainable for use by humans. An unregulated industry practice or method can lead to a complete resource depletion.
The renewable energy from the sun, wind, wave, biomass and geothermal energies are based on renewable resources. Renewable resources such as the movement of water (hydropower, tidal power and wave power), wind and radiant energy from geothermal heat (used for geothermal power) and solar energy (used for solar power) are practically infinite and cannot be depleted, unlike their non-renewable counterparts, which are likely to run out if not used sparingly.

The potential wave energy on coastlines can provide 1/5 of world demand. Hydroelectric power can supply 1/3 of our total energy global needs. Geothermal energy can provide 1.5 more times the energy we need. There is enough wind to power the planet 30 times over, wind power could power all of humanity's needs alone. Solar currently supplies only 0.1% of our world energy needs, but there is enough out there to power humanity's needs 4,000 times over, the entire global projected energy demand by 2050.

Renewable energy and energy efficiency are no longer niche sectors that are promoted only by governments and environmentalists. The increasing levels of investment and that more of the capital is from conventional financial actors, both suggest that sustainable energy has become mainstream and the future of energy production, as non-renewable resources decline. This is reinforced by climate change concerns, nuclear dangers and accumulating radioactive waste, high oil prices, peak oil and increasing government support for renewable energy. These factors are commercializing renewable energy, enlarging the market and growing demand, the adoption of new products to replace obsolete technology and the conversion of existing infrastructure to a renewable standard.

In economics, a non-renewable resource is defined as goods, where greater consumption today implies less consumption tomorrow. David Ricardo in his early works analysed the pricing of exhaustible resources, where he argued that the price of a mineral resource should increase over time. He argued that the spot price is always determined by the mine with the highest cost of extraction, and mine owners with lower extraction costs benefit from a differential rent. The first model is defined by Hotelling's rule, which is a 1931 economic model of non-renewable resource management by Harold Hotelling. It shows that efficient exploitation of a nonrenewable and nonaugmentable resource would, under otherwise stable conditions, lead to a depletion of the resource. The rule states that this would lead to a net price or "Hotelling rent" for it that rose annually at a rate equal to the rate of interest, reflecting the increasing scarcity of the resources.
The Hartwick's rule provides an important result about the sustainability of welfare in an economy that uses non-renewable source.



</doc>
<doc id="858937" url="https://en.wikipedia.org/wiki?curid=858937" title="Spatial memory">
Spatial memory

In cognitive psychology and neuroscience, spatial memory is a form of memory responsible for the recording of information about one's environment and spatial orientation. For example, a person's spatial memory is required in order to navigate around a familiar city, just as a rat's spatial memory is needed to learn the location of food at the end of a maze. It is often argued that in both humans and animals, spatial memories are summarized as a cognitive map.

Spatial memory has representations within working, short-term memory and long-term memory. Research indicates that there are specific areas of the brain associated with spatial memory. Many methods are used for measuring spatial memory in children, adults, and animals.

Short-term memory (STM) can be described as a system allowing one to temporarily store and manage information that is necessary to complete complex cognitive tasks. Tasks which employ short-term memory include learning, reasoning, and comprehension. Spatial memory is a cognitive process that enables a person to remember different locations as well as spatial relations between objects. This allows one to remember where an object is in relation to another object; for instance, allowing someone to navigate through a familiar city. Spatial memories are said to form after a person has already gathered and processed sensory information about her or his environment.

Working memory (WM) can be described as a limited capacity system that allows one to temporarily store and process information. This temporary store enables one to complete or work on complex tasks while being able to keep information in mind. For instance, the ability to work on a complicated mathematical problem utilizes one's working memory.

One highly influential theory of WM is the Baddeley and Hitch multi-component model of working memory. The most recent version of this model suggests that there are four subcomponents to WM, namely the phonological loop; the visuo-spatial sketchpad; the central executive; and the episodic buffer. One component of this model, the visuo-spatial sketchpad, is said to be responsible for the temporary storage, maintenance, and manipulation of both visual and spatial information.

In contrast to the multi-component model, some researchers believe that STM should be viewed as a unitary construct. In this respect, visual, spatial, and verbal information are thought to be organized by levels of representation rather than the type of store to which they belong. Within the literature, it is suggested that further research into the fractionation of STM and WM be explored. However, much of the research into the visuo-spatial memory construct have been conducted in accordance to the paradigm advanced by Baddeley and Hitch.

Research into the exact function of the visuo-spatial sketchpad has indicated that both spatial short-term memory and working memory are dependent on executive resources and are not entirely distinct. For instance, performance on a working memory but not on a short-term memory task was affected by articulatory suppression suggesting that impairment on the spatial task was caused by the concurrent performance on a task that had extensive use of executive resources. Results have also found that performances were impaired on STM and WM tasks with executive suppression. This illustrates how, within the visuo-spatial domain, both STM and WM require similar utility of the central executive.
Additionally, during a spatial visualisation task (which is related to executive functioning and not STM or WM) concurrent executive suppression impaired performance indicating that the effects were due to common demands on the central executive and not short-term storage. The researchers concluded with the explanation that the central executive employs cognitive strategies enabling participants to both encode and maintain mental representations during short-term memory tasks.

Although studies suggest that the central executive is intimately involved in a number of spatial tasks, the exact way in which they are connected remains to be seen.

Spatial memory recall is built upon a hierarchical structure. That is to say that people remember the general layout of a particular space and then "cue target locations" within that spatial set. This paradigm includes an ordinal scale of features that an individual must attend to in order to inform his or her cognitive map. Recollection of spatial details is a top-down procedure that requires an individual to recall the superordinate features of a cognitive map, followed by the ordinate and subordinate features. Thus, two spatial features are prominent in navigating a path: general layout and landmark orienting (Kahana et al., 2006).

People are not only capable of learning about the spatial layout of their surroundings, but they can also piece together novel routes and new spatial relations through inference. Yet, this field has traditionally been hampered by confounding variables, such as cost and the potential for previous exposure to an experimental environment. Thankfully, technological leaps have opened a new, albeit virtual, world to psychologists.

A cognitive map is "a mental model of objects' spatial configuration that permits navigation along optimal path between arbitrary pairs of points." This mental map is built upon two fundamental bedrocks: layout, also known as route knowledge, and landmark orientation. Layout is potentially the first method of navigation that people learn to utilize; its workings reflect our most basic understandings of the world.

Hermer and Spelke (1994) determined that when toddlers begin to walk, around eighteen months, they navigate by their sense of the world's layout. Indeed, it would seem that a sojourning toddler's world is a place of axial lines and contrasting boundaries. McNamara, Hardy and Hirtle identified region membership as a major building block of anyone's cognitive map (1989). Specifically, region membership is defined by any kind of boundary, whether physical, perceptual or subjective (McNamara et al., 1989). Boundaries are among the most basic and endemic qualities in the world around us. These boundaries are nothing more than axial lines which are a feature that people are biased towards when relating to space; for example one axial line determinant is gravity (McNamara & Shelton, 2001; Kim & Penn, 2004). Axial lines aid everyone in apportioning our perceptions into regions. This parceled world idea is further supported items by the finding that items that get recalled together are more likely than not to also be clustered within the same region of one's larger cognitive map. Clustering shows that people tend to chunk information together according to smaller layouts within a larger cognitive map.

Boundaries, though, are not the only determinants of layout. Clustering also demonstrates another important property of our relation to spatial conceptions. This is that spatial recall is a hierarchical process. When someone recalls an environment or navigates terrain, that person implicitly recalls the overall layout at first. Then, due to the concept's "rich correlational structure", a series of associations become activated. Eventually the resulting cascade of activations will awaken the particular details that correspond with the region being recalled. This is how people encode many entities from varying ontological levels, such as the location of a stapler; in a desk; which is in the office .. Alas, layout has its flaws too. One can recall from only one at region at a time (a bottleneck).

A bottleneck in a person's cognitive navigational system could be disastrous, for instance if there were need for a sudden detour on a long road trip. And yet, people are still capable of getting place to place functionally. Lack of experience in a locale, or simply sheer size, can disorient one's mental layout, especially in a large and unfamiliar place with lots of overwhelming stimuli. In these environments people are still able to orient themselves, and even find their way around using landmarks. This ability to "prioritize objects and regions in complex scenes for selection (and) recognition" was labeled by Chun and Jiang in 1998. Landmarks give people guidance by activating "learned associations between the global context and target locations." Mallot and Gillner (2000) showed that subjects learned an association between a specific landmark and the direction of a turn, thereby furthering the relationship between associations and landmarks. Shelton and McNamara (2001) succinctly summed up why landmarks, as markers, are so helpful: "location...cannot be described without making reference to the orientation of the observer."

It is fairly clear that people use both the layout of a particular space, as well as the presence of orienting landmarks in order to navigate. Yet, psychologists have yet to explain whether layout affects landmarks or if landmarks determine the boundaries of a layout. Thus, this concept suffers from a chicken and the egg paradox. In fact, McNamara has found that subjects use "clusters of landmarks as intrinsic frames of reference," which only confuses the issue further.

People perceive objects in their environment relative to other objects in that same environment. In other words, landmarks and layout are complementary systems for spatial recall. However, it is unknown how these two systems interact when both types of information are available. Thus, we have to make certain assumptions about the interaction between these two systems. For example, cognitive maps are not "absolute" but rather, as anyone can attest, are "used to provide a default...(which) modulated according to...task demands." Psychologists also think that cognitive maps are instance based, which accounts for "discriminative matching to past experience."

These assumptions could soon be validated. Advances in virtual reality technology have pried open the door to this enigmatic field. Now experimenters find themselves creating scenarios that were impossible to imagine fifteen years ago. Virtual reality affords experimenters the luxury of extreme control over their test environment. Any variable can be manipulated, including things that would not be possible in reality.

During one recent study researchers designed three different virtual towns, each of which had its own "unique road layout and a unique set of five stores." However, the overall footprint of the different maps was exactly the same size, "80 sq. units." In this experiment, participants had to partake in two different sets of trials.

First, participants were assigned two of the three virtual landscapes and tasked with the role of a taxi driver. The participant's avatar picked up a total of 25 passengers and dropped them off at random locations throughout the map. Researchers measured the participants' "delivery path length" in both sets of the first trial. They found that there was a "clear decrease in path length with increased number of deliveries in a town." The participants' improved route length correlated with the increasing amount of experience they got driving in the virtual towns. However, as one would expect, the knowledge that participants gleaned from the first town did not transfer, or aid them, in their travels around the second town. Thus, Newman et al. (2006) inferred that the participants "formed a survey representation of each town." That is they drove around enough of the town to infer the general layout of the rest of it.

The second set of trials involved the same task for the participants but with some changes to the context of the environment. Researchers added two more maps and made them smaller. The second map set in this trial was standardized with the same layout and landmarks, meaning that all participants would experience this map on their second run. Moreover, three of the five maps "replaced some set of the landmarks with novel landmarks while the layout of target locations remain(ed) unchanged." The other two maps were either "identical" to the second traversed landscape of the second trial or both the landmarks and their relative locations were changed. Again, Newman et al. measured "excess path length," specifically focusing on the excess distance traveled on the first delivery of the second map in the second trial, where they believed that spatial learning would transfer mostly strongly from the first map. They found that the group that encountered altered building representations in the same locations had the highest level of layout knowledge transfer from the first town.

The results of the first trial showed that people are capable of learning the spatial layout of an interactive environment. The second trial showed that despite a change in landmark presence, participants were still able to "find novel shortest routes." Because participants did not travel across previously memorized routes and still performed well according to the "excess path length" standard, Newman et al. concluded that these results evidenced "some higher-order survey representation of the environment."

A study conducted at the University of Maryland compared the effect of different levels of immersion on spatial memory recall. In the study, 40 participants used both a traditional desktop and a head-mounted display to view two environments, a medieval town, and an ornate palace, where they memorized two sets of 21 faces presented as 3D portraits. After viewing these 21 faces for 5 minutes, followed by a brief rest period, the faces in the virtual environments were replaced with numbers and participants recalled which face was at each location. The study found on average, those who used the head-mounted display recalled the faces 8.8% more accurately, and with a greater confidence. The participants state that leveraging their innate vestibular and proprioceptive senses with the head-mounted display and mapping aspects of the environment relative to their body, elements that are absent with the desktop, was key to their success.

Within the literature there is evidence that experts in a particular field are able to perform memory tasks in accordance with their skills at an exceptional level. The level of skill displayed by experts has also been said to exceed the limits of the normal capacity of both STM and WM. It is believed that because experts have an enormous amount of prelearned and task-specific knowledge, they are able to encode information in a more efficient way.

An interesting study investigating taxi drivers' memory for streets in Helsinki, Finland, examined the role of prelearned spatial knowledge. This study compared experts to a control group to determine how this prelearned knowledge in their skill domain allows them to overcome the capacity limitations of STM and WM. The study used four levels of spatial randomness:

The results of this study indicate that the taxi drivers' (experts') recall of streets was higher in both the route order condition and the map order condition than in the two random conditions. This indicates that the experts were able to use their prelearned spatial knowledge to organize the information in such a way that they surpassed STM and WM capacity limitations. The organization strategy that the drivers employed is known as chunking. Additionally, the comments made by the experts during the procedure point towards their use of route knowledge in completing the task. To ensure that it was in fact spatial information that they were encoding, the researchers also presented lists in alphabetical order and semantic categories. However, the researchers found that it was in fact spatial information that the experts were chunking, allowing them to surpass the limitations of both visuo-spatial STM and WM.

Within the literature it has been found that certain species of paridae and corvidae (such as the black-capped chickadee and the scrub jay) are able to use spatial memory to remember where, when and what type of food they have cached. Recent studies with rats and squirrels, have also suggested that they are able to use spatial memory to locate previously hidden food. Experiments using the radial maze have allowed researchers to control for a number of variables, such as the type of food hidden, the locations where the food is hidden, the retention interval, as well as any odour cues that could skew results of memory research. In particular, studies have indicated that rats have memory for where they have hidden food and what type of food they have hidden. This is shown in retrieval behaviour, such that the rats are selective in going more often to the arms of the maze where they have previously hidden preferred food than to arms with less preferred food or where no food was hidden.
Thus, the evidence for the spatial memory of some species of animals, such as rats, indicates that they do use spatial memory to locate and retrieve hidden food stores.

During a study using GPS tracking to see where domestic cats go when their owners let them outside, has shown that cats have excellent spatial memory. Some of the cats in the study demonstrated exceptional long term spatial memory. One of such cats, that usually travelled no further than to from its home, had unexpectedly travelled some from its home. Researchers initially thought this to be a GPS malfunction, but soon discovered that the cat's owners went out of town that weekend, and that the house the cat went to was the owner's old house. The owners and the cat hadn't lived in that house for well over a year.

Logie (1995) proposed that the visuo-spatial sketchpad is broken down into two subcomponents, one visual and one spatial. These are the visual cache and the inner scribe, respectively. The visual cache is a temporary visual store including such dimensions as colour and shape. Conversely, the inner scribe is a rehearsal mechanism for visual information and is responsible for information concerning movement sequences. Although a general lack of consensus regarding this distinction has been noted in the literature, there is a growing amount of evidence that the two components are separate and serve different functions.

Visual memory is responsible for retaining visual shapes and colours (i.e., what), whereas spatial memory is responsible for information about locations and movement (i.e., where). This distinction is not always straightforward since part of visual memory involves spatial information and vice versa. For example, memory for object shapes usually involves maintaining information about the spatial arrangement of the features which define the object in question.

In practice the two systems work together in some capacity but different tasks have been developed to highlight the unique abilities involved in either visual or spatial memory. For example, the visual patterns test (VPT) measures visual span whereas the Corsi Blocks Task measures spatial span. Correlational studies of the two measures suggest a separation between visual and spatial abilities, due to a lack of correlation found between them in both healthy and brain damaged patients.

Support for the division of visual and spatial memory components is found through experiments using the dual-task paradigm. A number of studies have shown that the retention of visual shapes or colours (i.e., visual information) is disrupted by the presentation of irrelevant pictures or dynamic visual noise. Conversely, the retention of location (i.e., spatial information) is disrupted only by spatial tracking tasks, spatial tapping tasks, and eye movements. For example, participants completed both the VPT and the Corsi Blocks Task in a selective interference experiment. During the retention interval of the VPT, the subject viewed irrelevant pictures (e.g., avant-garde paintings). The spatial interference task required participants to follow, by touching the stimuli, an arrangement of small wooden pegs which were concealed behind a screen. Both the visual and spatial spans were shortened by their respective interference tasks, confirming that the Corsi Blocks Task relates primarily to spatial working memory.

There are a variety of tasks that psychologists use to measure spatial memory on adults, children and animal models. These tasks allow professionals to identify cognitive irregularities in adults and children and allows researchers to administer varying types of drugs and or lesions in participants and measure the consequential effects on spatial memory.

Also known as the Corsi Span Test, this psychological test is commonly used to determine the visual-spatial memory span and the implicit visual-spatial learning abilities of an individual. Participants sit with nine wooden 3x3-cm blocks fastened before them on a 25- x 30-cm baseboard in a standard random order. The experiment taps onto the blocks a sequence pattern which participants must then replicate. The blocks are numbered on the experimenters' side to allow for efficient pattern demonstration. The sequence length increases each trial until the participant is no longer able to replicate the pattern correctly. The test can be used to measure both short-term and long-term spatial memory, depending on the length of time between test and recall.

The test was created by Canadian neuropsychologist Phillip Corsi, who modeled it after Hebb's digit span task by replacing the numerical test items with spatial ones. On average, most participants achieve a span of five items on the Corsi span test and seven on the digit span task.

This is similar to the Corsi block tapping test but regarded as a more pure test of visual short-term recall. Participants are presented with a series of matrix patterns that have half their cells coloured and the other half blank. The matrix patterns are arranged in a way that is difficult to code verbally, forcing the participant to rely on visual spatial memory. Beginning with a small 2 x 2 matrix, participants copy the matrix pattern from memory into an empty matrix. The matrix patterns are increased in size and complexity at a rate of two cells until the participant's ability to replicate them breaks down. On average, participants' performance tends to break down at sixteen cells.

This task is designed to measure spatial memory abilities in children. The experimenter asks the participant to visualize a blank matrix with a little man. Through a series of directional instructions such as forwards, backwards, left or right, the experimenter guides the participant's little man on a pathway throughout the matrix. At the end, the participant is asked to indicate on a real matrix where the little man that he or she visualized finished. The length of the pathway varies depending on the level of difficulty (1-10) and the matrices themselves may vary in length from 2 x 2 cells to 6 x 6.

These are intended for measuring spatial ability in children. With this test, an experimenter presents the participant with a drawing of a maze with a picture of a man in the centre. While the participant watches, the experimenter uses his or her finger to trace a pathway from the opening of the maze to the drawing of the man. The participant is then expected to replicate the demonstrated pathway through the maze to the drawing of the man. Mazes vary in complexity as difficulty increases.

First pioneered by Olton and Samuelson in 1976, the radial arm maze is designed to test the spatial memory capabilities of rats. Mazes are typically designed with a centre platform and a varying number of arms branching off with food placed at the ends. The arms are usually shielded from each other in some way but not to the extent that external cues cannot be used as reference points.

In most cases, the rat is placed in the center of the maze and needs to explore each arm individually to retrieve food while simultaneously remembering which arms it has already pursued. The maze is set up so the rat is forced to return to the center of the maze before pursuing another arm. Measures are usually taken to prevent the rat from using its olfactory senses to navigate such as placing extra food throughout the bottom of the maze.

The Morris water navigation task is a classic test for studying spatial learning and memory in rats and was first developed in 1981 by Richard G. Morris for whom the test is named. The subject is placed in a round tank of translucent water with walls that are too high for it to climb out and water that is too deep for it to stand in. Additionally, the walls of the tank are decorated with visual cues to serve as reference points. The rat must swim around the pool until by chance it discovers just below the surface the hidden platform onto which it can climb.

Typically, rats swim around the edge of the pool first before venturing out into the center in a meandering pattern before stumbling upon the hidden platform. However, as time spent in the pool increases experience, the amount of time needed to locate the platform decreases, with veteran rats swimming directly to the platform almost immediately after being placed in the water.

The hippocampus provides animals with a spatial map of their environment. It stores information regarding non-egocentric space (egocentric means in reference to one's body position in space) and therefore supports viewpoint independence in spatial memory. This means that it allows for viewpoint manipulation from memory. It is however, important for long-term spatial memory of allocentric space (reference to external cues in space). Maintenance and retrieval of memories are thus relational or context dependent. The hippocampus makes use of reference and working memory and has the important role of processing information about spatial locations.

Blocking plasticity in this region results in problems in goal-directed navigation and impairs the ability to remember precise locations. Amnesic patients with damage to the hippocampus cannot learn or remember spatial layouts and patients having undergone hippocampal removal are severely impaired in spatial navigation. Monkeys with lesions to this area cannot learn object-place associations and rats also display spatial deficits by not reacting to spatial change. In addition, rats with hippocampal lesions were shown to have temporally ungraded (time-independent) retrograde amnesia that is resistant to recognition of a learned platform task only when the entire hippocampus is lesioned, but not when it is partially lesioned. Deficits in spatial memory are also found in spatial discrimination tasks.
Large differences in spatial impairment are found among the dorsal and ventral hippocampus. Lesions to the ventral hippocampus have no effect on spatial memory, while the dorsal hippocampus is required for retrieval, processing short-term memory and transferring memory from the short term to longer delay periods. Infusion of amphetamine into the dorsal hippocampus has also been shown to enhance memory for spatial locations learned previously. These findings indicate that there is a functional dissociation between the dorsal and ventral hippocampus.

Hemispheric differences within the hippocampus are also observed. A study on London taxi drivers, asked drivers to recall complex routes around the city as well as famous landmarks for which the drivers had no knowledge of their spatial location. This resulted in an activation of the right hippocampus solely during recall of the complex routes which indicates that the right hippocampus is used for navigation in large scale spatial environments.

The hippocampus is known to contain two separate memory circuits. One circuit is used for recollection-based place recognition memory and includes the entorhinal-CA1 system, while the other system, consisting of the hippocampus trisynaptic loop (entohinal-dentate-CA3-CA1) is used for place recall memory and facilitation of plasticity at the entorhinal-dentate synapse in mice is sufficient to enhance place recall.

Place cells are also found in the hippocampus.

The parietal cortex encodes spatial information using an egocentric frame of reference. It is therefore involved in the transformation of sensory information coordinates into action or effector coordinates by updating the spatial representation of the body within the environment. As a result, lesions to the parietal cortex produce deficits in the acquisition and retention of egocentric tasks, whereas minor impairment is seen among allocentric tasks.

Rats with lesions to the anterior region of the posterior parietal cortex reexplore displaced objects, while rats with lesions to the posterior region of the posterior parietal cortex displayed no reaction to spatial change.

Parietal cortex lesions are also known to produce temporally ungraded retrograde amnesia.

The dorsalcaudal medial entorhinal cortex (dMEC) contains a topographically organized map of the spatial environment made up of grid cells. This brain region thus transforms sensory input from the environment and stores it as a durable allocentric representation in the brain to be used for path integration.

The entorhinal cortex contributes to the processing and integration of geometric properties and information in the environment. Lesions to this region impair the use of distal but not proximal landmarks during navigation and produces a delay-dependent deficit in spatial memory that is proportional to the length of the delay. Lesions to this region are also known to create retention deficits for tasks learned up to 4 weeks but not 6 weeks prior to the lesions.

Memory consolidation in the entorhinal cortex is achieved through extracellular signal-regulated kinase activity.

The medial prefrontal cortex processes egocentric spatial information. It participates in the processing of short-term spatial memory used to guide planned search behavior and is believed to join spatial information with its motivational significance. The identification of neurons that anticipate expected rewards in a spatial task support this hypothesis. The medial prefrontal cortex is also implicated in the temporal organization of information.

Hemisphere specialization is found in this brain region. The left prefrontal cortex preferentially processes categorical spatial memory including source memory (reference to spatial relationships between a place or event), while the right prefrontal cortex preferentially processes coordinate spatial memory including item memory (reference to spatial relationships between features of an item).

Lesions to the medial prefrontal cortex impair the performance of rats on a previously trained radial arm maze, however, rats can gradually improve to the level of the controls as a function of experience. Lesions to this area also cause deficits on delayed nonmatching-to-positions tasks and impairments in the acquisition of spatial memory tasks during training trials.

The retrosplenial cortex is involved in the processing of allocentric memory and geometric properties in the environment. Inactivation of this region accounts for impaired navigation in the dark and thus it is implicated to be involved in the process of path integration.

Lesions to the retrosplenial cortex consistently impair tests of allocentric memory, while sparing egocentric memory. Animals with lesions to the caudal retrosplenial cortex show impaired performance on a radial arm maze only when the maze is rotated to remove their reliance on intramaze cues. 
In humans, damage to the retrosplenial cortex results in topographical disorientation. Most cases involve damage to the right retrosplenial cortex and include Brodmann area 30. Patients are often impaired at learning new routes and at navigating through familiar environments. However, most patients usually recover within 8 weeks.

The retrosplenial cortex preferentially processes spatial information in the right hemisphere.

The perirhinal cortex is associated with both spatial reference and spatial working memory. It processes relational information of environmental cues and locations.

Lesions in the perirhinal cortex account for deficits in reference memory and working memory, and increase the rate of forgetting of information during training trials of the Morris water maze. This accounts for the impairment in the initial acquisition of the task. Lesions also cause impairment on an object location task and reduce habituation to a novel environment.

Spatial memories are formed after an animal gathers and processes sensory information about its surroundings (especially vision and proprioception). In general, mammals require a functioning hippocampus (particularly area CA1) in order to form and process memories about space. There is some evidence that human spatial memory is strongly tied to the right hemisphere of the brain.

Spatial learning requires both NMDA and AMPA receptors, consolidation requires NMDA receptors, and the retrieval of spatial memories requires AMPA receptors. In rodents, spatial memory has been shown to covary with the size of a part of the hippocampal mossy fiber projection.

The function of NMDA receptors varies according to the subregion of the hippocampus. NMDA receptors are required in the CA3 of the hippocampus when spatial information needs to be reorganized, while NMDA receptors in the CA1 are required in the acquisition and retrieval of memory after a delay, as well as in the formation of CA1 place fields. Blockade of the NMDA receptors prevents induction of long-term potentiation and impairs spatial learning.

The CA3 of the hippocampus plays an especially important role in the encoding and retrieval of spatial memories. The CA3 is innervated by two afferent paths known as the perforant path (PPCA3) and the dentate gyrus (DG)-mediated mossy fibers (MFs). The first path is regarded as the retrieval index path while the second is concerned with encoding.

Topographical disorientation is a cognitive disorder that results in the individual being unable to orient his or herself in the real or virtual environment. Patients also struggle with spatial information dependant tasks. These problems could possibly be the result of a disruption in the ability to access one's cognitive map, a mental representation of the surrounding environment or the inability to judge objects' location in relation to one's self.

Developmental Topographical Disorientation (DTD) is diagnosed when patients have shown an inability to navigate even familiar surroundings since birth and show no apparent neurological causes for this deficiency such as lesioning or brain damage. DTD is a relatively new disorder and can occur in varying degrees of severity.

Topographical Disorientation in Mild Cognitive Impairment: A Voxel-Based Morphometry Study was done to see if Topographical Disorientation had an effect on individuals who had mild cognitive impairment. The study was done by recruiting forty-one patients diagnosed with MCI and 24 healthy control individuals. The standards that were set for this experiment were:
(TD) was assessed clinically in all participants. Neurological and neuropsychological evaluations were determined by a magnetic imaging scan which was performed on each participant. Voxel-based morphometry was used to compare patterns of gray-matter atrophy between patients with and without TD, and a group of normal controls. The outcome of the experiment was that they found TD in 17 out of the 41 MCI patients (41.4%). The functional abilities were significantly impaired in MCI patients with TD compared to in MCI patients without TD and that the presence of TD in MCI patients is associated with loss of gray matter in the medial temporal regions, including the hippocampus.

Research with rats indicates that spatial memory may be adversely affected by neonatal damage to the hippocampus in a way that closely resembles schizophrenia. Schizophrenia is thought to stem from neurodevelopmental problems shortly after birth.

Rats are commonly used as models of schizophrenia patients. Experimenters create lesions in the ventral hippocampal area shortly after birth, a procedure known as neonatal ventral hippocampal lesioning(NVHL). Adult rats who with NVHL show typical indicators of schizophrenia such as hypersensitivity to psychostimulants, reduced social interactions and impaired prepulse inhibition, working memory and set-shifting. Similar to schizophrenia, impaired rats fail to use environmental context in spatial learning tasks such as showing difficulty completing the radial arm maze and the Moris water maze.

Recent research on spatial memory and wayfinding in an article by Ishikawa "et al." in 2008 revealed that using a GPS moving map device reduces an individual's navigation abilities when compared to other participants who were using maps or had previous experience on the route with a guide. GPS moving map devices are frequently set up to allow the user to only see a small detailed close-up of a particular segment of the map which is constantly updated. In comparison, maps usually allow the user to see the same view of the entire route from departure to arrival. Other research has shown that individuals who use GPS travel more slowly overall compared to map users who are faster. GPS users stop more frequently and for a longer period of time whereas map users and individuals using past experience as a guide travel on more direct routes to reach their goal.

Endonuclease VIII-like 1 (NEIL1) is a DNA repair enzyme that is widely expressed throughout the brain. NEIL1 is a DNA glycosylase that initiates the first step in base excision repair by cleaving bases damaged by reactive oxygen species and then introducing a DNA strand break via an associated lyase reaction. This enzyme recognizes and removes oxidized DNA bases including formamidopyrimidine, thymine glycol, 5-hydroxyuracil and 5-hydroxycytosine. NEIL1 promotes short-term spatial memory retention. Mice lacking NEIL1 have impaired short-term spatial memory retention in a water maze test.

Nonverbal learning disability is characterized by normal verbal abilities but impaired visuospatial abilities. Problem areas for children with nonverbal learning disability are arithmetic, geometry, and science. Impairments in spatial memory is implicated in nonverbal learning disorder and other learning difficulties.

Arithmetic word problems involve written text containing a set of data followed by one or more questions and require the use of the four basic arithmetic operations (addition, subtraction, multiplication, or division). Researchers suggest that successful completion of arithmetic word problems involves spatial working memory (involved in building schematic representations) which facilitates the creation of spatial relationships between objects. Creating spatial relationships between objects is an important part of solving word problems because mental operations and transformations are required.

For example, consider the following question: "A child builds three towers using red and white coloured blocks of the same size.
The lowest tower has 14 blocks; the highest has 7 more blocks. The intermediate tower has three blocks less than the highest one. How many blocks are in each of the three towers?" To solve the question, it is necessary to maintain incoming information (i.e., the text) and integrate it with previous information (such as knowledge for arithmetic operations). The individual must also select relevant (i.e., the spatial relationship between the blocks) and inhibit irrelevant information (i.e., the colours and textures of the blocks) and simultaneously build a mental representation of the problem.

Researchers investigated the role of spatial memory and visual memory in the ability to complete arithmetic word problems. Children in the study completed the Corsi Block Task (forward and backward series) and a spatial matrix task, as well as a visual memory task called the house recognition test. Poor problem-solvers were impaired on the Corsi Block Tasks and the spatial matrix task, but performed normally on the house recognition test when compared to normally achieving children. The experiment demonstrated that poor problem solving is related specifically to deficient processing of spatial information.

Sleep has been found to benefit spatial memory, by enhancing hippocampal-dependent memory consolidation. Hippocampal areas activated in route-learning are reactivated during subsequent sleep (NREM sleep in particular). It was demonstrated in a particular study that the actual extent of reactivation during sleep correlated with the improvement in route retrieval and thus memory performance the following day. Thus, the study established the idea that sleep enhances the systems-level process of consolidation that consequently enhances/improves behavioural performance. Furthermore, a period of wakefulness has no effect on stabilizing memory traces, in comparison to a period of sleep. Sleep after the first post-training night, i.e. on the second night, does not benefit spatial memory consolidation further. Therefore, sleeping in the first post-training night e.g. after learning a route, is most important.

Sleep deprivation and sleep has also been a researched association. Sleep deprivation actually hinders memory performance improvement due to an active disruption of spatial memory consolidation. Therefore, spatial memory is enhanced by a period of sleep.




</doc>
<doc id="62980659" url="https://en.wikipedia.org/wiki?curid=62980659" title="GGSE-4">
GGSE-4

The Gravity Gradient Stabilization Experiment (GGSE-4) was a technology satellite launched in 1967. This was ostensibly the fourth in a series that developed designs and deployment techniques later applied to the NOSS/Whitecloud reconnaissance satellites.

GGSE-4 was launched by the U.S. Airforce from Vandenberg Air Force Base atop a Thor Agena-D rocket. 
GGSE-4 remained operational from 1967 through 1972.

It is alleged that the real name of GGSE-4 was POPPY 5B or POPPY 5b and that it was a U.S. National Reconnaissance Office satellite designed to collect signals intelligence; POPPY 5B was part of a 7-satellite mission. A partial subset of information about POPPY was declassified in 2005.

Other sources say that GGSE-4 weighed only 10 pounds but that it was attached to the much larger Poppy 5, which would have weighed 85 kg and featured an 18-meter boom.
It is further alleged that GGSE-4's mass is not at all like GGSE-1's mass and that GGSE'4 weighs 85 kg.

On , GGSE-4 was expected to pass as closely as 12 meters from IRAS, another un-deorbited satellite left aloft. IRAS was launched in 1983 and abandoned after a 10-month mission. The 14.7-kilometer per second pass had an estimated risk of collision of 5%. Further complications arose from the fact that GGSE-4 was outfitted with an 18 meter long stabilization boom that was in an unknown orientation and may have struck the satellite even if the spacecraft's main body did not. Initial observations from amateur astronomers seemed to indicate that both satellites had survived the pass, with the California-based debris tracking organization LeoLabs later confirming that they had detected no new tracked debris following the incident.

Gravity Gradient Stabilization Experiment (GGSE-1)


</doc>
<doc id="62996247" url="https://en.wikipedia.org/wiki?curid=62996247" title="The Lowe Files">
The Lowe Files

The Lowe Files is a non-fiction series on the A&E that looks at unsolved mysteries within the United States. The show is hosted by Rob Lowe and his sons John Owen Lowe and Edward Matthew Lowe.

The series currently has aired 9 episodes. The series was cancelled after one season.




</doc>
<doc id="920295" url="https://en.wikipedia.org/wiki?curid=920295" title="British Museum algorithm">
British Museum algorithm

The British Museum algorithm is a general approach to finding a solution by checking all possibilities one by one, beginning with the smallest. The term refers to a conceptual, not a practical, technique where the number of possibilities is enormous.

Newell, Shaw, and Simon 
called this procedure the British Museum algorithm 




</doc>
<doc id="200877" url="https://en.wikipedia.org/wiki?curid=200877" title="Maze generation algorithm">
Maze generation algorithm

Maze generation algorithms are automated methods for the creation of mazes.

A maze can be generated by starting with a predetermined arrangement of cells (most commonly a rectangular grid but other arrangements are possible) with wall sites between them. This predetermined arrangement can be considered as a connected graph with the edges representing possible wall sites and the nodes representing cells. The purpose of the maze generation algorithm can then be considered to be making a subgraph in which it is challenging to find a route between two particular nodes.

If the subgraph is not connected, then there are regions of the graph that are wasted because they do not contribute to the search space. If the graph contains loops, then there may be multiple paths between the chosen nodes. Because of this, maze generation is often approached as generating a random spanning tree. Loops, which can confound naive maze solvers, may be introduced by adding random edges to the result during the course of the algorithm.

The animation shows the maze generation steps for a 
graph that is not on a rectangular grid.
First, the computer creates a random planar graph G
shown in blue, and its dual F
shown in yellow. Second, computer traverses F using a chosen
algorithm, such as a depth-first search, coloring the path red.
During the traversal, whenever a red edge crosses over a blue edge,
the blue edge is removed.
Finally, when all vertices of F have been visited, F is erased
and two edges from G, one for the entrance and one for the exit, are removed.

This algorithm is a randomized version of the depth-first search algorithm. Frequently implemented with a stack, this approach is one of the simplest ways to generate a maze using a computer. Consider the space for a maze being a large grid of cells (like a large chess board), each cell starting with four walls. Starting from a random cell, the computer then selects a random neighbouring cell that has not yet been visited. The computer removes the wall between the two cells and marks the new cell as visited, and adds it to the stack to facilitate backtracking. The computer continues this process, with a cell that has no unvisited neighbours being considered a dead-end. When at a dead-end it backtracks through the path until it reaches a cell with an unvisited neighbour, continuing the path generation by visiting this new, unvisited cell (creating a new junction). This process continues until every cell has been visited, causing the computer to backtrack all the way back to the beginning cell. We can be sure every cell is visited.

As given above this algorithm involves deep recursion which may cause stack overflow issues on some computer architectures. The algorithm can be rearranged into a loop by storing backtracking information in the maze itself. This also provides a quick way to display a solution, by starting at any given point and backtracking to the beginning.

Mazes generated with a depth-first search have a low branching factor and contain many long corridors, because the algorithm explores as far as possible along each branch before backtracking.

The depth-first search algorithm of maze generation is frequently implemented using backtracking. This can be described with a following recursive routine:


which is invoked once for any initial cell in the area.

A disadvantage of this approach is a large depth of recursion – in the worst case, the routine may need to recur on every cell of the area being processed, which may exceed the maximum recursion stack depth in many environments. As a solution, the same bactracking method can be implemented with an explicit stack, which is usually allowed to grow much bigger with no harm.


This algorithm is a randomized version of Kruskal's algorithm.


There are several data structures that can be used to model the sets of cells. An efficient implementation using a disjoint-set data structure can perform each union and find operation on two sets in nearly constant amortized time (specifically, formula_1 time; formula_2 for any plausible value of formula_3), so the running time of this algorithm is essentially proportional to the number of walls available to the maze.

It matters little whether the list of walls is initially randomized or if a wall is randomly chosen from a nonrandom list, either way is just as easy to code.

Because the effect of this algorithm is to produce a minimal spanning tree from a graph with equally weighted edges, it tends to produce regular patterns which are fairly easy to solve.

This algorithm is a randomized version of Prim's algorithm.


It will usually be relatively easy to find the way to the starting cell, but hard to find the way anywhere else.

Note that simply running classical Prim's on a graph with random edge weights would create mazes stylistically identical to Kruskal's, because they are both minimal spanning tree algorithms. Instead, this algorithm introduces stylistic variation because the edges closer to the starting point have a lower effective weight.

Although the classical Prim's algorithm keeps a list of edges, for maze generation we could instead maintain a list of adjacent cells. If the randomly chosen cell has multiple edges that connect it to the existing maze, select one of these edges at random. This will tend to branch slightly more than the edge-based version above.

All the above algorithms have biases of various sorts: depth-first search is biased toward long corridors, while Kruskal's/Prim's algorithms are biased toward many short dead ends. Wilson's algorithm, on the other hand, generates an "unbiased" sample from the uniform distribution over all mazes, using loop-erased random walks.

We begin the algorithm by initializing the maze with one cell chosen arbitrarily. Then we start at a new cell chosen arbitrarily, and perform a random walk until we reach a cell already in the maze—however, if at any point the random walk reaches its own path, forming a loop, we erase the loop from the path before proceeding. When the path reaches the maze, we add it to the maze. Then we perform another loop-erased random walk from another arbitrary starting cell, repeating until all cells have been filled.

This procedure remains unbiased no matter which method we use to arbitrarily choose starting cells. So we could always choose the first unfilled cell in (say) left-to-right, top-to-bottom order for simplicity.

Mazes can be created with "recursive division", an algorithm which works as follows: Begin with the maze's space with no walls. Call this a chamber. Divide the chamber with a randomly positioned wall (or multiple walls) where each wall contains a randomly positioned passage opening within it. Then recursively repeat the process on the subchambers until all chambers are minimum sized. This method results in mazes with long straight walls crossing their space, making it easier to see which areas to avoid.

For example, in a rectangular maze, build at random points two walls that are perpendicular to each other. These two walls divide the large chamber into four smaller chambers separated by four walls. Choose three of the four walls at random, and open a one cell-wide hole at a random point in each of the three. Continue in this manner recursively, until every chamber has a width of one cell in either of the two directions.

Other algorithms exist that require only enough memory to store one line of a 2D maze or one plane of a 3D maze. Eller's algorithm prevents loops by storing which cells in the current line are connected through cells in the previous lines, and never removes walls between any two cells already connected. The Sidewinder algorithm starts with an open passage along the entire the top row, and subsequent rows consist of shorter horizontal passages with one connection to the passage above. The Sidewinder algorithm is trivial to solve from the bottom up because it has no upward dead ends. Given a starting width, both algorithm create perfect mazes of unlimited height.

Most maze generation algorithms require maintaining relationships between cells within it, to ensure the end result will be solvable. Valid simply connected mazes can however be generated by focusing on each cell independently. A binary tree maze is a standard orthogonal maze where each cell always has a passage leading up or leading left, but never both. To create a binary tree maze, for each cell flip a coin to decide whether to add a passage leading up or left. Always pick the same direction for cells on the boundary, and the end result will be a valid simply connected maze that looks like a binary tree, with the upper left corner its root. As with Sidewinder, the binary tree maze has no dead ends in the directions of bias. 

A related form of flipping a coin for each cell is to create an image using a random mix of forward slash and backslash characters. This doesn't generate a valid simply connected maze, but rather a selection of closed loops and unicursal passages. (The manual for the Commodore 64 presents a BASIC program using this algorithm, but using PETSCII diagonal line graphic characters instead for a smoother graphic appearance.)

Certain types of cellular automata can be used to generate mazes. Two well-known such cellular automata, Maze and Mazectric, have rulestrings B3/S12345 and B3/S1234. In the former, this means that cells survive from one generation to the next if they have at least one and at most five neighbours. In the latter, this means that cells survive if they have one to four neighbours. If a cell has exactly three neighbours, it is born. It is similar to Conway's Game of Life in that patterns that do not have a living cell adjacent to 1, 4, or 5 other living cells in any generation will behave identically to it. However, for large patterns, it behaves very differently from Life.

For a random starting pattern, these maze-generating cellular automata will evolve into complex mazes with well-defined walls outlining corridors. Mazecetric, which has the rule B3/S1234 has a tendency to generate longer and straighter corridors compared with Maze, with the rule B3/S12345. Since these cellular automaton rules are deterministic, each maze generated is uniquely determined by its random starting pattern. This is a significant drawback since the mazes tend to be relatively predictable.

Like some of the graph-theory based methods described above, these cellular automata typically generate mazes from a single starting pattern; hence it will usually be relatively easy to find the way to the starting cell, but harder to find the way anywhere else.




</doc>
<doc id="390562" url="https://en.wikipedia.org/wiki?curid=390562" title="Tomasulo algorithm">
Tomasulo algorithm

Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution and enables more efficient use of multiple execution units. It was developed by Robert Tomasulo at IBM in 1967 and was first implemented in the IBM System/360 Model 91’s floating point unit.

The major innovations of Tomasulo’s algorithm include register renaming in hardware, reservation stations for all execution units, and a common data bus (CDB) on which computed values broadcast to all reservation stations that may need them. These developments allow for improved parallel execution of instructions that would otherwise stall under the use of scoreboarding or other earlier algorithms.

Robert Tomasulo received the Eckert–Mauchly Award in 1997 for his work on the algorithm.

The following are the concepts necessary to the implementation of Tomasulo's Algorithm:

The Common Data Bus (CDB) connects reservation stations directly to functional units. According to Tomasulo it "preserves precedence while encouraging concurrency". This has two important effects:

Instructions are issued sequentially so that the effects of a sequence of instructions, such as exceptions raised by these instructions, occur in the same order as they would on an in-order processor, regardless of the fact that they are being executed out-of-order (i.e. non-sequentially). 

Tomasulo's Algorithm uses register renaming to correctly perform out-of-order execution. All general-purpose and reservation station registers hold either a real value or a placeholder value. If a real value is unavailable to a destination register during the issue stage, a placeholder value is initially used. The placeholder value is a tag indicating which reservation station will produce the real value. When the unit finishes and broadcasts the result on the CDB, the placeholder will be replaced with the real value.

Each functional unit has a single reservation station. Reservation stations hold information needed to execute a single instruction, including the operation and the operands. The functional unit begins processing when it is free and when all source operands needed for an instruction are real.

Practically speaking, there may be exceptions for which not enough status information about an exception is available, in which case the processor may raise a special exception, called an "imprecise" exception. Imprecise exceptions cannot occur in in-order implementations, as processor state is changed only in program order (see RISC Pipeline Exceptions).

Programs that experience "precise" exceptions, where the specific instruction that took the exception can be determined, can restart or re-execute at the point of the exception. However, those that experience "imprecise" exceptions generally cannot restart or re-execute, as the system cannot determine the specific instruction that took the exception.

The three stages listed below are the stages through which each instruction passes from the time it is issued to the time its execution is complete.


In the issue stage, instructions are issued for execution if all operands and reservation stations are ready or else they are stalled. Registers are renamed in this step, eliminating WAR and WAW hazards.


In the execute stage, the instruction operations are carried out. Instructions are delayed in this step until all of their operands are available, eliminating RAW hazards. Program correctness is maintained through effective address calculation to prevent hazards through memory.


In the write Result stage, ALU operations results are written back to registers and store operations are written back to memory.

The concepts of reservation stations, register renaming, and the common data bus in Tomasulo's algorithm presents significant advancements in the design of high-performance computers.

Reservation stations take on the responsibility of waiting for operands in the presence of data dependencies and other inconsistencies such as varying storage access time and circuit speeds, thus freeing up the functional units. This improvement overcomes long floating point delays and memory accesses. In particular the algorithm is more tolerant of cache misses. Additionally, programmers are freed from implementing optimized code. This is a result of the common data bus and reservation station working together to preserve dependencies as well as encouraging concurrency.

By tracking operands for instructions in the reservation stations and register renaming in hardware the algorithm minimizes read-after-write (RAW) and eliminates write-after-write (WAW) and Write-after-Read (WAR) computer architecture hazards. This improves performance by reducing wasted time that would otherwise be required for stalls.

An equally important improvement in the algorithm is the design is not limited to a specific pipeline structure. This improvement allows the algorithm to be more widely adopted by multiple-issue processors. Additionally, the algorithm is easily extended to enable branch speculation. 

Tomasulo's algorithm, outside of IBM, was unused for several years after its implementation in the System/360 Model 91 architecture. However, it saw a vast increase in usage during the 1990s for 3 reasons:

Many modern processors implement dynamic scheduling schemes that are derivative of Tomasulo’s original algorithm, including popular Intel x86-64 chips.




</doc>
<doc id="1551981" url="https://en.wikipedia.org/wiki?curid=1551981" title="Medical algorithm">
Medical algorithm

A medical algorithm is any computation, formula, statistical survey, nomogram, or look-up table, useful in healthcare. Medical algorithms include decision tree approaches to healthcare treatment (e.g., if symptoms A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty. A medical prescription is also a type of medical algorithm.

Medical algorithms are part of a broader field which is usually fit under the aims of medical informatics and medical decision-making. Medical decisions occur in several areas of medical activity including medical test selection, diagnosis, therapy and prognosis, and automatic control of medical equipment.

In relation to logic-based and artificial neural network-based clinical decision support systems, which are also computer applications used in the medical decision-making field, algorithms are less complex in architecture, data structure and user interface. Medical algorithms are not necessarily implemented using digital computers. In fact, many of them can be represented on paper, in the form of diagrams, nomographs, etc.

A wealth of medical information exists in the form of published medical algorithms. These algorithms range from simple calculations to complex outcome predictions. Most clinicians use only a small subset routinely.

Examples of medical algorithms are:

A common class of algorithms are embedded in guidelines on the choice of treatments produced by many national, state, financial and local healthcare organisations and provided as knowledge resources for day to day use and for induction of new physicians. A field which has gained particular attention is the choice of medications for psychiatric conditions. In the United Kingdom, guidelines or algorithms for this have been produced by most of the circa 500 primary care trusts, substantially all of the circa 100 secondary care psychiatric units and many of the circa 10 000 general practices. In the US, there is a national (federal) initiative to provide them for all states, and by 2005 six states were adapting the approach of the Texas Medication Algorithm Project or otherwise working on their production.

A grammar—the Arden syntax—exists for describing algorithms in terms of medical logic modules. An approach such as this should allow exchange of MLMs between doctors and establishments, and enrichment of the common stock of tools.

The intended purpose of medical algorithms is to improve and standardize decisions made in the delivery of medical care. Medical algorithms assist in standardizing selection and application of treatment regimens, with algorithm automation intended to reduce potential introduction of errors. Some attempt to predict the outcome, for example critical care scoring systems.

Computerized health diagnostics algorithms can provide timely clinical decision support, improve adherence to evidence-based guidelines, and be a resource for education and research.

Medical algorithms based on best practice can assist everyone involved in delivery of standardized treatment via a wide range of clinical care providers. Many are presented as protocols and it is a key task in training to ensure people step outside the protocol when necessary. In our present state of knowledge, generating hints and producing guidelines may be less satisfying to the authors, but more appropriate.

In common with most science and medicine, algorithms whose contents are not wholly available for scrutiny and open to improvement should be regarded with suspicion.

Computations obtained from medical algorithms should be compared with, and tempered by, clinical knowledge and physician judgment.



</doc>
<doc id="2935699" url="https://en.wikipedia.org/wiki?curid=2935699" title="Run-time algorithm specialization">
Run-time algorithm specialization

In computer science, run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. The methodology originates in the field of automated theorem proving and, more specifically, in the Vampire theorem prover project.

The idea is inspired by the use of partial evaluation in optimising program translation. 
Many core operations in theorem provers exhibit the following pattern.
Suppose that we need to execute some algorithm formula_1 in a situation where a value of formula_2 "is fixed for potentially many different values of" formula_3. In order to do this efficiently, we can try to find a specialization of formula_4 for every fixed formula_2, i.e., such an algorithm formula_6, that executing formula_7 is equivalent to executing formula_1.

The specialized algorithm may be more efficient than the generic one, since it can "exploit some particular properties" of the fixed value formula_2. Typically, formula_7 can avoid some operations that formula_1 would have to perform, if they are known to be redundant for this particular parameter formula_2. 
In particular, we can often identify some tests that are true or false for formula_2, unroll loops and recursion, etc.

The key difference between run-time specialization and partial evaluation is that the values of formula_2 on which formula_4 is specialised are not known statically, so the "specialization takes place at run-time".

There is also an important technical difference. Partial evaluation is applied to algorithms explicitly represented as codes in some programming language. At run-time, we do not need any concrete representation of formula_4. We only have to "imagine" formula_4 "when we program" the specialization procedure.
All we need is a concrete representation of the specialized version formula_6. This also means that we cannot use any universal methods for specializing algorithms, which is usually the case with partial evaluation. Instead, we have to program a specialization procedure for every particular algorithm formula_4. An important advantage of doing so is that we can use some powerful "ad hoc" tricks exploiting peculiarities of formula_4 and the representation of formula_2 and formula_3, which are beyond the reach of any universal specialization methods.

The specialized algorithm has to be represented in a form that can be interpreted.
In many situations, usually when formula_7 is to be computed on many values formula_3 in a row, we can write formula_6 as a code of a special abstract machine, and we often say that formula_2 is "compiled". 
Then the code itself can be additionally optimized by answer-preserving transformations that rely only on the semantics of instructions of the abstract machine.

Instructions of the abstract machine can usually be represented as records. One field of such a record stores an integer tag that identifies the instruction type, other fields may be used for storing additional parameters of the instruction, for example a pointer to another
instruction representing a label, if the semantics of the instruction requires a jump. All instructions of a code can be stored in an array, or list, or tree.

Interpretation is done by fetching instructions in some order, identifying their type
and executing the actions associated with this type. 
In C or C++ we can use a switch statement to associate 
some actions with different instruction tags. 
Modern compilers usually compile a switch statement with integer labels from a narrow range rather efficiently by storing the address of the statement corresponding to a value formula_27 in the formula_27-th cell of a special array. One can exploit this
by taking values for instruction tags from a small interval of integers.

There are situations when many instances of formula_2 are intended for long-term storage and the calls of formula_1 occur with different formula_3 in an unpredictable order.
For example, we may have to check formula_32 first, then formula_33, then formula_34, and so on.
In such circumstances, full-scale specialization with compilation may not be suitable due to excessive memory usage. 
However, we can sometimes find a compact specialized representation formula_35
for every formula_2, that can be stored with, or instead of, formula_2. 
We also define a variant formula_38 that works on this representation 
and any call to formula_1 is replaced by formula_40, intended to do the same job faster.





</doc>
<doc id="8286430" url="https://en.wikipedia.org/wiki?curid=8286430" title="Adaptive algorithm">
Adaptive algorithm

An adaptive algorithm is an algorithm that changes its behavior at the time it is run, based on information available and on "a priori" defined reward mechanism (or criterion). Such information could be the story of recently received data, information on the available computational resources, or other run-time acquired (or "a priori" known) information related to the environment in which it operates.

Among the most used adaptive algorithms is the Widrow-Hoff’s least mean squares (LMS), which represents a class of stochastic gradient-descent algorithms used in adaptive filtering and machine learning. In adaptive filtering the LMS is used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired and the actual signal).

For example, stable partition, using no additional memory is "O"("n" lg "n") but given "O"("n") memory, it can be "O"("n") in time. As implemented by the C++ Standard Library, codice_1 is adaptive and so it acquires as much memory as it can get (up to what it would need at most) and applies the algorithm using that available memory. Another example is adaptive sort, whose behavior changes upon the presortedness of its input.

An example of an adaptive algorithm in radar systems is the constant false alarm rate (CFAR) detector.

In machine learning and optimization, many algorithms are adaptive or have adaptive variants, which usually means that the algorithm parameters are automatically adjusted according to statistics about the optimisation thus far (e.g. the rate of convergence). Examples include adaptive simulated annealing, adaptive coordinate descent, AdaBoost, and adaptive quadrature.

In data compression, adaptive coding algorithms such as Adaptive Huffman coding or Prediction by partial matching can take a stream of data as input, and adapt their compression technique based on the symbols that they have already encountered.

In signal processing, the Adaptive Transform Acoustic Coding (ATRAC) codec used in MiniDisc recorders is called "adaptive" because the window length (the size of an audio "chunk") can change according to the nature of the sound being compressed, to try to achieve the best-sounding compression strategy.



</doc>
<doc id="145555" url="https://en.wikipedia.org/wiki?curid=145555" title="XOR swap algorithm">
XOR swap algorithm

In computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable. "Distinct" means that the variables are stored at different, non-overlapping, memory addresses as the algorithm would set a single aliased value to zero; the actual values of the variables do not have to be different.

Conventional swapping requires the use of a temporary storage variable. Using the XOR swap algorithm, however, no temporary storage is needed. The algorithm is as follows:
X := X XOR Y
Y := Y XOR X
X := X XOR Y
The algorithm typically corresponds to three machine-code instructions. Since XOR is a commutative operation, X XOR Y can be replaced with Y XOR X in any of the lines. When coded in assembly language, this commutativity is often exercised in the second line:

In the above System/370 assembly code sample, R1 and R2 are distinct registers, and each operation leaves its result in the register named in the first argument. Using x86 assembly, values X and Y are in registers eax and ebx (respectively), and places the result of the operation in the first register.

However, the algorithm fails if "x" and "y" use the same storage location, since the value stored in that location will be zeroed out by the first XOR instruction, and then remain zero; it will not be "swapped with itself". This is "not" the same as if "x" and "y" have the same values. The trouble only comes when "x" and "y" use the same storage location, in which case their values must already be equal. That is, if "x" and "y" use the same storage location, then the line:

X := X XOR Y

sets "x" to zero (because "x" = "y" so X XOR Y is zero) "and" sets "y" to zero (since it uses the same storage location), causing "x" and "y" to lose their original values.

The binary operation XOR over bit strings of length formula_1 exhibits the following properties (where formula_2 denotes XOR):


Suppose that we have two distinct registers codice_1 and codice_2 as in the table below, with initial values "A" and "B" respectively. We perform the operations below in sequence, and reduce our results using the properties listed above.

As XOR can be interpreted as binary addition and a pair of bits can be interpreted as a vector in a two-dimensional vector space over the field with two elements, the steps in the algorithm can be interpreted as multiplication by 2×2 matrices over the field with two elements. For simplicity, assume initially that "x" and "y" are each single bits, not bit vectors.

For example, the step:

X := X XOR Y

which also has the implicit:

Y := Y

corresponds to the matrix formula_9 as
The sequence of operations is then expressed as:
(working with binary values, so formula_12), which expresses the elementary matrix of switching two rows (or columns) in terms of the transvections (shears) of adding one element to the other.

To generalize to where X and Y are not single bits, but instead bit vectors of length "n", these 2×2 matrices are replaced by 2"n"×2"n" block matrices such as formula_13

These matrices are operating on "values," not on "variables" (with storage locations), hence this interpretation abstracts away from issues of storage location and the problem of both variables sharing the same storage location.

A C function that implements the XOR swap algorithm:
The code first checks if the addresses are distinct. Otherwise, if they were equal, the algorithm would fold to a triple *x ^= *x resulting in zero.

The XOR swap algorithm can also be defined with a macro:
In most practical scenarios, the trivial swap algorithm using a temporary register is more efficient. Limited situations in which XOR swapping may be practical include:


Because these situations are rare, most optimizing compilers do not generate XOR swap code.

Most modern compilers can optimize away the temporary variable in the three-way swap, in which case it will use the same amount of memory and the same number of registers as the XOR swap and is at least as fast, and often faster. In addition to that, the XOR swap is completely opaque to anyone unfamiliar with the technique.

On modern CPU architectures, the XOR technique can be slower than using a temporary variable to do swapping. At least on recent x86 CPUs, both by AMD and Intel, moving between registers regularly incurs zero latency. (This is called MOV-elimination.) Even if there is not any architectural register available to use, the codice_3 instruction will be at least as fast as the three XORs taken together. Another reason is that modern CPUs strive to execute instructions in parallel via instruction pipelines. In the XOR technique, the inputs to each operation depend on the results of the previous operation, so they must be executed in strictly sequential order, negating any benefits of instruction-level parallelism.

The XOR swap is also complicated in practice by aliasing. If an attempt is made to XOR-swap the contents of some location with itself, the result is that the location is zeroed out and its value lost. Therefore, XOR swapping must not be used blindly in a high-level language if aliasing is possible.

Similar problems occur with call by name, as in Jensen's Device, where swapping codice_4 and codice_5 via a temporary variable yields incorrect results due to the arguments being related: swapping via codice_6 changes the value for codice_4 in the second statement, which then results in the incorrect i value for codice_5 in the third statement.

The underlying principle of the XOR swap algorithm can be applied to any operation meeting criteria L1 through L4 above. Replacing XOR by addition and subtraction gives a slightly different, but largely equivalent, formulation:

Unlike the XOR swap, this variation requires that the underlying processor or programming language uses a method such as modular arithmetic or bignums to guarantee that the computation of codice_9 cannot cause an error due to integer overflow. Therefore, it is seen even more rarely in practice than the XOR swap.

However, the implementation of codice_10 above in the C programming language always works even in case of integer overflow, since, according to the C standard, addition and subtraction of unsigned integers follow the rules of modular arithmetic, i. e. are done in the cyclic group formula_14 where formula_15 is the number of bits of codice_11. Indeed, the correctness of the algorithm follows from the fact that the formulas formula_16 and formula_17 hold in any abelian group. This is actually a generalization of the proof for the XOR swap algorithm: XOR is both the addition and subtraction in the abelian group formula_18 (which is the direct sum of "s" copies of formula_19).

This doesn't hold when dealing with the codice_12 type (the default for codice_13). Signed integer overflow is an undefined behavior in C and thus modular arithmetic is not guaranteed by the standard (a standard-conforming compiler might optimize out such code, which leads to incorrect results).



</doc>
<doc id="15641067" url="https://en.wikipedia.org/wiki?curid=15641067" title="Super-recursive algorithm">
Super-recursive algorithm

In computability theory, super-recursive algorithms are a generalization of ordinary algorithms that are more powerful, that is, compute more than Turing machines. The term was introduced by Mark Burgin, whose book "Super-recursive algorithms" develops their theory and presents several mathematical models. Turing machines and other mathematical models of conventional algorithms allow researchers to find properties of recursive algorithms and their computations. In a similar way, mathematical models of super-recursive algorithms, such as inductive Turing machines, allow researchers to find properties of super-recursive algorithms and their computations.

Burgin, as well as other researchers (including Selim Akl, Eugene Eberbach, Peter Kugel, Jan van Leeuwen, Hava Siegelmann, Peter Wegner, and Jiří Wiedermann) who studied different kinds of super-recursive algorithms and contributed to the theory of super-recursive algorithms, have argued that super-recursive algorithms can be used to disprove the Church-Turing thesis, but this point of view has been criticized within the mathematical community and is not widely accepted.

Burgin (2005: 13) uses the term recursive algorithms for algorithms that can be implemented on Turing machines, and uses the word "algorithm" in a more general sense. Then a super-recursive class of algorithms is "a class of algorithms in which it is possible to compute functions not computable by any Turing machine" (Burgin 2005: 107).

Super-recursive algorithms are closely related to hypercomputation 
in a way similar to the relationship between ordinary computation and ordinary algorithms. Computation is a process, while an algorithm is a finite constructive description of such a process. Thus a super-recursive algorithm defines a "computational process (including processes of input and output) that cannot be realized by recursive algorithms." (Burgin 2005: 108). A more restricted definition demands that hypercomputation solves a supertask (see Copeland 2002; Hagar and Korolev 2007).

Super-recursive algorithms are also related to algorithmic schemes, which are more general than super-recursive algorithms. Burgin argues (2005: 115) that it is necessary to make a clear distinction between super-recursive algorithms and those algorithmic schemes that are not algorithms. Under this distinction, some types of hypercomputation are obtained by super-recursive algorithms, e.g., inductive Turing machines, while other types of hypercomputation are directed by algorithmic schemas, e.g., infinite time Turing machines. This explains how works on super-recursive algorithms are related to hypercomputation and vice versa. According to this argument, super-recursive algorithms are just one way of defining a hypercomputational process.

Examples of super-recursive algorithms include (Burgin 2005: 132):

Examples of algorithmic schemes include:


For examples of practical super-recursive algorithms, see the book of Burgin.

Inductive Turing machines implement an important class of super-recursive algorithms. An inductive Turing machine is a definite list of well-defined instructions for completing a task which, when given an initial state, will proceed through a well-defined series of successive states, eventually giving the final result. The difference between an inductive Turing machine and an ordinary Turing machine is that an ordinary Turing machine must stop when it has obtained its result, while in some cases an inductive Turing machine can continue to compute after obtaining the result, without stopping. Kleene called procedures that could run forever without stopping by the name "calculation procedure or algorithm" (Kleene 1952:137). Kleene also demanded that such an algorithm must eventually exhibit "some object" (Kleene 1952:137). Burgin argues that this condition is satisfied by inductive Turing machines, as their results are exhibited after a finite number of steps. The reason that inductive Turing machines cannot be instructed to halt when their final output is produced is that in some cases inductive Turing machines may not be able to tell at which step the result has been obtained.

Simple inductive Turing machines are equivalent to other models of computation such as general Turing machines of Schmidhuber, trial and error predicates of Hilary Putnam, limiting partial recursive functions of Gold, and trial-and-error machines of Hintikka and Mutanen (1998). More advanced inductive Turing machines are much more powerful. There are hierarchies of inductive Turing machines that can decide membership in arbitrary sets of the arithmetical hierarchy (Burgin 2005). In comparison with other equivalent models of computation, simple inductive Turing machines and general Turing machines give direct constructions of computing automata that are thoroughly grounded in physical machines. In contrast, trial-and-error predicates, limiting recursive functions, and limiting partial recursive functions present only syntactic systems of symbols with formal rules for their manipulation. Simple inductive Turing machines and general Turing machines are related to limiting partial recursive functions and trial-and-error predicates as Turing machines are related to partial recursive functions and lambda calculus.

The non-halting computations of inductive Turing machines should not be confused with infinite-time computations (see, for example, Potgieter 2006). First, some computations of inductive Turing machines do halt. As in the case of conventional Turing machines, some halting computations give the result, while others do not. Even if it does not halt, an inductive Turing machine produces output from time to time. If this output stops changing, it is then considered the result of the computation.

There are two main distinctions between ordinary Turing machines and simple inductive Turing machines. The first distinction is that even simple inductive Turing machines can do much more than conventional Turing machines. The second distinction is that a conventional Turing machine will always determine (by coming to a final state) when the result is obtained, while a simple inductive Turing machine, in some cases (such as when "computing" something that cannot be computed by an ordinary Turing machine), will not be able to make this determination.

A symbol sequence is computable in the limit if there is a finite, possibly non-halting program on a universal Turing machine that incrementally outputs every symbol of the sequence. This includes the dyadic expansion of π but still excludes most of the real numbers, because most cannot be described by a finite program. Traditional Turing machines with a write-only output tape cannot edit their previous outputs; generalized Turing machines, according to Jürgen Schmidhuber, can edit their output tape as well as their work tape. He defines the constructively describable symbol sequences as those that have a finite, non-halting program running on a generalized Turing machine, such that any output symbol eventually converges, that is, it does not change any more after some finite initial time interval. Schmidhuber (2000, 2002) uses this approach to define the set of formally describable or constructively computable universes or constructive theories of everything. Generalized Turing machines and simple inductive Turing machines are two classes of super-recursive algorithms that are the closest to recursive algorithms (Schmidhuber 2000).

The Church–Turing thesis in recursion theory relies on a particular definition of the term "algorithm". Based on definitions that are more general than the one commonly used in recursion theory, Burgin argues that super-recursive algorithms, such as inductive Turing machines disprove the Church–Turing thesis. He proves furthermore that super-recursive algorithms could theoretically provide even greater efficiency gains than using quantum algorithms.

Burgin's interpretation of super-recursive algorithms has encountered opposition in the mathematical community. One critic is logician Martin Davis, who argues that Burgin's claims have been well understood "for decades". Davis states, 
Davis disputes Burgin's claims that sets at level formula_1 of the arithmetical hierarchy can be called computable, saying






</doc>
<doc id="3578575" url="https://en.wikipedia.org/wiki?curid=3578575" title="Randomization function">
Randomization function

In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm.

Randomizing functions are related to random number generators and hash functions, but have somewhat different requirements and uses, and often need specific algorithms.

Randomizing functions are used to turn algorithms that have good expected performance for "random" inputs, into algorithms that have the same performance for "any" input.

For example, consider a sorting algorithm like quicksort, which has small expected running time when the input items are presented in random order, but is very slow when they are presented in certain unfavorable orders. A randomizing function from the integers 1 to "n" to the integers 1 to "n" can be used to rearrange the "n" input items in "random" order, before calling that algorithm. This modified (randomized) algorithm will have small expected running time, whatever the input order.

In theory, randomization functions are assumed to be truly random, and yield an unpredictably different function every time the algorithm is executed. The randomization technique would not work if, at every execution of the algorithm, the randomization function always performed the same mapping, or a mapping entirely determined by some externally observable parameter (such as the program's startup time). With such a "pseudo-randomization" function, one could in principle construct a sequence of calls such that the function would always yield a "bad" case for the underlying deterministic algorithm. For that sequence of calls, the average cost would be closer to the worst-case cost, rather than the average cost for random inputs.

In practice, however, the main concern is that some "bad" cases for the deterministic algorithm may occur in practice much more often than it would be predicted by chance. For example, in a naive variant of quicksort, the worst case is when the input items are already sorted — which is a very common occurrence in many applications. For such algorithms, even a fixed pseudo-random permutation may be good enough. Even though the resulting "pseudo-randomized" algorithm would still have as many "bad" cases as the original, they will be certain peculiar orders that would be quite unlikely to arise in real applications. So, in practice one often uses randomization functions that are derived from pseudo-random number generators, preferably seeded with external "random" data such as the program's startup time.

The uniformity requirements for a randomizing function are usually much weaker than those of hash functions and pseudo-random generators. The minimum requirement is that it maps any input of the deterministic algorithm into a "good" input with a sufficiently high probability. (However, analysis is usually simpler if the randomizing function implements each possible mapping with uniform probability.)


</doc>
<doc id="22509875" url="https://en.wikipedia.org/wiki?curid=22509875" title="Simulation algorithms for atomic DEVS">
Simulation algorithms for atomic DEVS

Given an atomic DEVS model, simulation algorithms are methods to generate the model's legal behaviors which are trajectories not to reach to illegal states. (see Behavior of DEVS). [Zeigler84] originally introduced the algorithms that handle time variables related to "lifespan" formula_1 and "elapsed time" formula_2 by introducing two other time variables, "last event time", formula_3, and "next event time" formula_4 with the following relations: 
formula_5

and

formula_6

where formula_7 denotes the "current time". And the "remaining time",

formula_9, apparently formula_10.

Since the behavior of a given atomic DEVS model can be defined in two different views depending on the total state and the external transition function (refer to Behavior of DEVS), the simulation algorithms are also introduced in two different views as below.

Regardless of two different views of total states, algorithms for initialization and internal transition cases are commonly defined as below.

As addressed in Behavior of Atomic DEVS, when DEVS receives an input event, right calling formula_24, the last event time,formula_11 is set by the current time,formula_14, thus the elapsed timeformula_27 becomes zero because formula_28.

Notice that as addressed in Behavior of Atomic DEVS, depending on the value of formula_36 return by formula_37, last event time,formula_11, and next event time,formula_12,consequently, elapsed time, formula_27, and lifespanformula_12, are updated (if formula_42) or preserved (if formula_43).




</doc>
<doc id="22513037" url="https://en.wikipedia.org/wiki?curid=22513037" title="Simulation algorithms for coupled DEVS">
Simulation algorithms for coupled DEVS

Given a coupled DEVS model, simulation algorithms are methods to generate the model's "legal" behaviors, which are a set of trajectories not to reach illegal states. (see behavior of a Coupled DEVS model.) [Zeigler84] originally introduced the algorithms that handle time variables related to "lifespan" formula_1 and "elapsed time" formula_2 by introducing two other time variables, "last event time", formula_3, and "next event time" formula_4 with the following relations: formula_5

and

formula_6

where formula_7 denotes the "current time". And the "remaining time",

formula_9, apparently formula_10.
Based on these relationships, the algorithms to simulate the behavior of a given Coupled DEVS are written as follows.

 DEVS-coordinator




</doc>
<doc id="505526" url="https://en.wikipedia.org/wiki?curid=505526" title="HAKMEM">
HAKMEM

HAKMEM, alternatively known as AI Memo 239, is a February 1972 "memo" (technical report) of the MIT AI Lab containing a wide variety of hacks, including useful and clever algorithms for mathematical computation, some number theory and schematic diagrams for hardware — in Guy L. Steele's words, "a bizarre and eclectic potpourri of technical trivia".
Contributors included about two dozen members and associates of the AI Lab. The title of the report is short for "hacks memo", abbreviated to six upper case characters that would fit in a single PDP-10 machine word (using a six-bit character set).

HAKMEM is notable as an early compendium of algorithmic technique, particularly for its practical bent, and as an illustration of the wide-ranging interests of AI Lab people of the time, which included almost anything other than AI research.

HAKMEM contains original work in some fields, notably continued fractions.




</doc>
<doc id="14609233" url="https://en.wikipedia.org/wiki?curid=14609233" title="Holographic algorithm">
Holographic algorithm

In computer science, a holographic algorithm is an algorithm that uses a holographic reduction. A holographic reduction is a constant-time reduction that maps solution fragments many-to-many such that the sum of the solution fragments remains unchanged. These concepts were introduced by Leslie Valiant, who called them "holographic" because "their effect can be viewed as that of producing interference patterns among the solution fragments". The algorithms are unrelated to laser holography, except metaphorically. Their power comes from the mutual cancellation of many contributions to a sum, analogous to the interference patterns in a hologram.

Holographic algorithms have been used to find polynomial-time solutions to problems without such previously known solutions for special cases of satisfiability, vertex cover, and other graph problems. They have received notable coverage due to speculation that they are relevant to the P versus NP problem and their impact on computational complexity theory. Although some of the general problems are #P-hard problems, the special cases solved are not themselves #P-hard, and thus do not prove FP = #P.

Holographic algorithms have some similarities with quantum computation, but are completely classical.

Holographic algorithms exist in the context of Holant problems, which generalize counting constraint satisfaction problems (#CSP). A #CSP instance is a hypergraph "G"=("V","E") called the constraint graph. Each hyperedge represents a variable and each vertex formula_1 is assigned a constraint formula_2 A vertex is connected to an hyperedge if the constraint on the vertex involves the variable on the hyperedge. The counting problem is to compute
which is a sum over all variable assignments, the product of every constraint, where the inputs to the constrain formula_4 are the variables on the incident hyperedges of formula_1.

A Holant problem is like a #CSP except the input must be a graph, not a hypergraph. Restricting the class of input graphs in this way is indeed a generalization. Given a #CSP instance, replace each hyperedge "e" of size "s" with a vertex "v" of degree "s" with edges incident to the vertices contained in "e". The constraint on "v" is the equality function of arity "s". This identifies all of the variables on the edges incident to "v", which is the same effect as the single variable on the hyperedge "e".

In the context of Holant problems, the expression in (1) is called the Holant after a related exponential sum introduced by Valiant.

A standard technique in complexity theory is a many-one reduction, where an instance of one problem is reduced to an instance of another (hopefully simpler) problem.
However, holographic reductions between two computational problems preserve the sum of solutions without necessarily preserving correspondences between solutions. For instance, the total number of solutions in both sets can be preserved, even though individual problems do not have matching solutions. The sum can also be weighted, rather than simply counting the number of solutions, using linear basis vectors.

It is convenient to consider holographic reductions on bipartite graphs. A general graph can always be transformed it into a bipartite graph while preserving the Holant value. This is done by replacing each edge in the graph by a path of length 2, which is also known as the 2-stretch of the graph. To keep the same Holant value, each new vertex is assigned the binary equality constraint.

Consider a bipartite graph "G"=("U","V","E") where the constraint assigned to every vertex formula_6 is formula_7 and the constraint assigned to every vertex formula_8 is formula_4. Denote this counting problem by formula_10 If the vertices in "U" are viewed as one large vertex of degree |"E"|, then the constraint of this vertex is the tensor product of formula_7 with itself |"U"| times, which is denoted by formula_12 Likewise, if the vertices in "V" are viewed as one large vertex of degree |"E"|, then the constraint of this vertex is formula_13 Let the constraint formula_7 be represented by its weighted truth table as a row vector and the constraint formula_4 be represented by its weighted truth table as a column vector. Then the Holant of this constraint graph is simply formula_16

Now for any complex 2-by-2 invertible matrix "T" (the columns of which are the linear basis vectors mentioned above), there is a holographic reduction between formula_17 and formula_18 To see this, insert the identity matrix formula_19 in between formula_20 to get
Thus, formula_17 and formula_25 have exactly the same Holant value for every constraint graph. They essentially define the same counting problem.

Let "G" be a graph. There is a 1-to-1 correspondence between the vertex covers of "G" and the independent sets of "G". For any set "S" of vertices of "G", "S" is a vertex cover in "G" if and only if the complement of "S" is an independent set in "G". Thus, the number of vertex covers in "G" is exactly the same as the number of independent sets in "G".

The equivalence of these two counting problems can also be proved using a holographic reduction. For simplicity, let "G" be a 3-regular graph. The 2-stretch of "G" gives a bipartite graph "H"=("U","V","E"), where "U" corresponds to the edges in "G" and "V" corresponds to the vertices in "G". The Holant problem that naturally corresponds to counting the number of vertex covers in "G" is formula_26 The truth table of OR as a row vector is (0,1,1,1). The truth table of EQUAL as a column vector is formula_27. Then under a holographic transformation by formula_28
which is formula_35 the Holant problem that naturally corresponds to counting the number of independent sets in "G".

As with any type of reduction, a holographic reduction does not, by itself, yield a polynomial time algorithm. In order to get a polynomial time algorithm, the problem being reduced to must also have a polynomial time algorithm. Valiant's original application of holographic algorithms used a holographic reduction to a problem where every constraint is realizable by matchgates, which he had just proved is tractable by a further reduction to counting the number of perfect matchings in a planar graph. The latter problem is tractable by the FKT algorithm, which dates to the 1960s.

Soon after, Valiant found holographic algorithms with reductions to matchgates for #Pl-Rtw-Mon-3CNF and #Pl-3/2Bip-VC. These problems may appear somewhat contrived, especially with respect to the modulus. Both problems were already known to be #P-hard when ignoring the modulus and Valiant supplied proofs of #P-hardness modulo 2, which also used holographic reductions. Valiant found these two problems by a computer search that looked for problems with holographic reductions to matchgates. He called their algorithms "accidental algorithms", saying "when applying the term accidental to an algorithm we intend to point out that the algorithm arises from satisfying an apparently onerous set of constraints." The "onerous" set of constraints in question are polynomial equations that, if satisfied, imply the existence of a holographic reduction to matchgate realizable constraints.

After several years of developing (what is known as) matchgate signature theory, Jin-Yi Cai and Pinyan Lu were able to explain the existence of Valiant's two accidental algorithms. These two problems are just special cases of two much larger families of problems: #Pl-Rtw-Mon-kCNF and #Pl-k/2Bip-VC for any positive integer "k". The modulus 7 is just the third Mersenne number and Cai and Lu showed that these types of problems with parameter "k" can be solved in polynomial time exactly when the modulus is the "k"th Mersenne number by using holographic reductions to matchgates and the Chinese remainder theorem.

Around the same time, Jin-Yi Cai, Pinyan Lu and Mingji Xia gave the first holographic algorithm that did not reduce to a problem that is tractable by matchgates. Instead, they reduced to a problem that is tractable by Fibonacci gates, which are symmetric constraints whose truth tables satisfy a recurrence relation similar to one that defines the Fibonacci numbers. They also used holographic reductions to prove that certain counting problems are #P-hard. Since then, holographic reductions have been used extensively as ingredients in both polynomial time algorithms and proofs of #P-hardness.


</doc>
<doc id="23868049" url="https://en.wikipedia.org/wiki?curid=23868049" title="Sequential algorithm">
Sequential algorithm

In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel. The term is primarily used to contrast with "concurrent algorithm" or "parallel algorithm;" most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many distributed algorithms are both concurrent and parallel – and thus "sequential" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.

"Sequential algorithm" may also refer specifically to an algorithm for decoding a convolutional code.



</doc>
<doc id="8756788" url="https://en.wikipedia.org/wiki?curid=8756788" title="One-pass algorithm">
One-pass algorithm

In computing, a one-pass algorithm is a streaming algorithm which reads its input exactly once, in order, without unbounded buffering. A one-pass algorithm generally requires "O"("n") (see 'big O' notation) time and less than "O"("n") storage (typically "O"(1)), where "n" is the size of the input.

Basically one-pass algorithm operates as follows:

Given any list as an input:

Given a list of numbers:

Given a list of symbols from an alphabet of "k" symbols, given in advance.

Given any list as an input:

Given a list of numbers:


</doc>
<doc id="417534" url="https://en.wikipedia.org/wiki?curid=417534" title="Algorism">
Algorism

Algorism is the technique of performing basic arithmetic by writing numbers in place value form and applying a set of memorized rules and facts to the digits. One who practices algorism is known as an algorist. This positional notation system largely superseded earlier calculation systems that used a different set of symbols for each numerical magnitude, such as Roman numerals, and in some cases required a device such as an abacus.

The word "algorism" comes from the name Al-Khwārizmī (c. 780–850), a Muslim Persian mathematician, astronomer, geographer and scholar in the House of Wisdom in Baghdad, whose name means "the native of Khwarezm", a city that was part of the Greater Iran during his era and now is in modern-day Uzbekistan He wrote a treatise in Arabic language in the 9th century, which was translated into Latin in the 12th century under the title "Algoritmi de numero Indorum". This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through his other book, the Algebra. In late medieval Latin, "algorismus", the corruption of his name, simply meant the "decimal number system" that is still the meaning of modern English algorism. During the 17th century, the French form for the word –but not its meaning– was changed to "algorithm", following the model of the word "logarithm", this form alluding to the ancient Greek . English adopted the French very soon afterwards, but it wasn't until the late 19th century that "algorithm" took on the meaning that it has in modern English. In English, it was first used about 1230 and then by Chaucer in 1391. Another early use of the word is from 1240, in a manual titled "Carmen de Algorismo" composed by Alexandre de Villedieu. It begins thus:

which translates as:
The word "algorithm" also derives from "algorism", a generalization of the meaning to any set of rules specifying a computational procedure. Occasionally "algorism" is also used in this generalized meaning, especially in older texts.

Starting with the integer arithmetic developed in India using base 10 notation, Al-Khwārizmī along with other mathematicians in medieval Islam, both Iranian and Arabic, documented new arithmetic methods and made many other contributions to decimal arithmetic (see the articles linked below). These included the concept of the decimal fractions as an extension of the notation, which in turn led to the notion of the decimal point. This system was popularized in Europe by Leonardo of Pisa, now known as Fibonacci.



</doc>
<doc id="12242679" url="https://en.wikipedia.org/wiki?curid=12242679" title="Ping-pong scheme">
Ping-pong scheme

Algorithms said to employ a Ping-Pong scheme exist in different fields of software engineering. They are characterized by an alternation between two entities. In the examples described below, these entities are communication partners, network paths or file blocks.

In most database management systems durable database transactions are supported through a log file. However, multiple writes to the same page of that file can produce a slim chance of data loss. Assuming for simplicity that the log file is organized in pages whose size matches the block size of its underlying medium, the following problem can occur:

If the very last page of the log file is only partially filled with data and has to be written to permanent storage in this state, the very same page will have to be overwritten during the next write operation. If a crash happens during that later write operation, previously stored log data may be lost.

The Ping-Pong scheme described in "Transaction Processing" eliminates this problem by alternately writing the contents of said (logical) last page to two different physical pages inside the log file (the actual last page "i" and its empty successor "i+1"). Once said logical log page is no longer the last page (i.e. it is completely filled with log data), it is written one last time to the regular physical position ("i") inside the log file.

This scheme requires the usage of time stamps for each page in order to distinguish the most recent version of the logical last page one from its predecessor.

A functionality which lets a computer A find out whether a computer B is reachable and responding is built into the Internet Control Message Protocol (ICMP). Through an "echo request" Computer A asks B to send back an "Echo response". These two messages are also sometimes called "ping" and "pong".

In Routing, a Ping-Pong scheme is a simple algorithm for distributing data packets across
two paths.

If you had two paths codice_1 and codice_2, then the algorithm
would randomly start with one of the paths and then switch back and forth 
between the two.

If you were to get the next path from a function call, it would look like
this in Python:


</doc>
<doc id="201154" url="https://en.wikipedia.org/wiki?curid=201154" title="Divide-and-conquer algorithm">
Divide-and-conquer algorithm

In computer science, divide and conquer is an algorithm design paradigm based on multi-branched recursion. A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.

This divide-and-conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. the Karatsuba algorithm), finding the closest pair of points, syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFT).

Understanding and designing divide-and-conquer algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These divide-and-conquer complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.

The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.

The divide-and-conquer paradigm is often used to find an optimal solution of a problem. Its basic idea is to decompose a given problem into two or more similar, but simpler, subproblems, to solve them in turn, and to compose their solutions to solve the given problem. Problems of sufficient simplicity are solved directly. 
For example, to sort a given list of "n" natural numbers, split it into two lists of about "n"/2 numbers each, sort each of them in turn, and interleave both results appropriately to obtain the sorted version of the given list (see the picture). This approach is known as the merge sort algorithm.

The name "divide and conquer" is sometimes applied to algorithms that reduce each problem to only one sub-problem, such as the binary search algorithm for finding a record in a sorted list (or its analog in numerical computing, the bisection algorithm for root finding). These algorithms can be implemented more efficiently than general divide-and-conquer algorithms; in particular, if they use tail recursion, they can be converted into simple loops. Under this broad definition, however, every algorithm that uses recursion or loops could be regarded as a "divide-and-conquer algorithm". Therefore, some authors consider that the name "divide and conquer" should be used only when each problem may generate two or more subproblems. The name decrease and conquer has been proposed instead for the single-subproblem class.

An important application of divide and conquer is in optimization, where if the search space is reduced ("pruned") by a constant factor at each step, the overall algorithm has the same asymptotic complexity as the pruning step, with the constant depending on the pruning factor (by summing the geometric series); this is known as prune and search.

Early examples of these algorithms are primarily decrease and conquer – the original problem is successively broken down into "single" subproblems, and indeed can be solved iteratively.

Binary search, a decrease-and-conquer algorithm where the subproblems are of roughly half the original size, has a long history. While a clear description of the algorithm on computers appeared in 1946 in an article by John Mauchly, the idea of using a sorted list of items to facilitate searching dates back at least as far as Babylonia in 200 BC. Another ancient decrease-and-conquer algorithm is the Euclidean algorithm to compute the greatest common divisor of two numbers by reducing the numbers to smaller and smaller equivalent subproblems, which dates to several centuries BC.

An early example of a divide-and-conquer algorithm with multiple subproblems is Gauss's 1805 description of what is now called the Cooley–Tukey fast Fourier transform (FFT) algorithm, although he did not analyze its operation count quantitatively, and FFTs did not become widespread until they were rediscovered over a century later.

An early two-subproblem D&C algorithm that was specifically developed for computers and properly analyzed is the merge sort algorithm, invented by John von Neumann in 1945.

Another notable example is the algorithm invented by Anatolii A. Karatsuba in 1960 that could multiply two "n"-digit numbers in formula_1 operations (in Big O notation). This algorithm disproved Andrey Kolmogorov's 1956 conjecture that formula_2 operations would be required for that task.

As another example of a divide-and-conquer algorithm that did not originally involve computers, Donald Knuth gives the method a post office typically uses to route mail: letters are sorted into separate bags for different geographical areas, each of these bags is itself sorted into batches for smaller sub-regions, and so on until they are delivered. This is related to a radix sort, described for punch-card sorting machines as early as 1929.

Divide and conquer is a powerful tool for solving conceptually difficult problems: all it requires is a way of breaking the problem into sub-problems, of solving the trivial cases and of combining sub-problems to the original problem. Similarly, decrease and conquer only requires reducing the problem to a single smaller problem, such as the classic Tower of Hanoi puzzle, which reduces moving a tower of height "n" to moving a tower of height "n" − 1.

The divide-and-conquer paradigm often helps in the discovery of efficient algorithms. It was the key, for example, to Karatsuba's fast multiplication method, the quicksort and mergesort algorithms, the Strassen algorithm for matrix multiplication, and fast Fourier transforms.

In all these examples, the D&C approach led to an improvement in the asymptotic cost of the solution.
For example, if (a) the base cases have constant-bounded size, the work of splitting the problem and combining the partial solutions is proportional to the problem's size "n", and (b) there is a bounded number "p" of sub-problems of size ~ "n"/"p" at each stage, then the cost of the divide-and-conquer algorithm will be O("n" log"n").

Divide-and-conquer algorithms are naturally adapted for execution in multi-processor machines, especially shared-memory systems where the communication of data between processors does not need to be planned in advance, because distinct sub-problems can be executed on different processors.

Divide-and-conquer algorithms naturally tend to make efficient use of memory caches. The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower main memory. An algorithm designed to exploit the cache in this way is called "cache-oblivious", because it does not contain the cache size as an explicit parameter.
Moreover, D&C algorithms can be designed for important algorithms (e.g., sorting, FFTs, and matrix multiplication) to be "optimal" cache-oblivious algorithms–they use the cache in a probably optimal way, in an asymptotic sense, regardless of the cache size. In contrast, the traditional approach to exploiting the cache is "blocking", as in loop nest optimization, where the problem is explicitly divided into chunks of the appropriate size—this can also use the cache optimally, but only when the algorithm is tuned for the specific cache size(s) of a particular machine.

The same advantage exists with regards to other hierarchical storage systems, such as NUMA or virtual memory, as well as for multiple levels of cache: once a sub-problem is small enough, it can be solved within a given level of the hierarchy, without accessing the higher (slower) levels.

In computations with rounded arithmetic, e.g. with floating-point numbers, a divide-and-conquer algorithm may yield more accurate results than a superficially equivalent iterative method. For example, one can add "N" numbers either by a simple loop that adds each datum to a single variable, or by a D&C algorithm called pairwise summation that breaks the data set into two halves, recursively computes the sum of each half, and then adds the two sums. While the second method performs the same number of additions as the first, and pays the overhead of the recursive calls, it is usually more accurate.

Divide-and-conquer algorithms are naturally implemented as recursive procedures. In that case, the partial sub-problems leading to the one currently being solved are automatically stored in the procedure call stack. A recursive function is a function that calls itself within its definition.

Divide-and-conquer algorithms can also be implemented by a non-recursive program that stores the partial sub-problems in some explicit data structure, such as a stack, queue, or priority queue. This approach allows more freedom in the choice of the sub-problem that is to be solved next, a feature that is important in some applications — e.g. in breadth-first recursion and the branch-and-bound method for function optimization. This approach is also the standard solution in programming languages that do not provide support for recursive procedures.

In recursive implementations of D&C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise the execution may fail because of stack overflow. D&C algorithms that are time-efficient often have relatively small recursion depth. For example, the quicksort algorithm can be implemented so that it never requires more than formula_3 nested recursive calls to sort formula_4 items.

Stack overflow may be difficult to avoid when using recursive procedures, since many compilers assume that the recursion stack is a contiguous area of memory, and some allocate a fixed amount of space for it. Compilers may also save more information in the recursion stack than is strictly necessary, such as return address, unchanging parameters, and the internal variables of the procedure. Thus, the risk of stack overflow can be reduced by minimizing the parameters and internal variables of the recursive procedure or by using an explicit stack structure.

In any recursive algorithm, there is considerable freedom in the choice of the "base cases", the small subproblems that are solved directly in order to terminate the recursion.

Choosing the smallest or simplest possible base cases is more elegant and usually leads to simpler programs, because there are fewer cases to consider and they are easier to solve. For example, an FFT algorithm could stop the recursion when the input is a single sample, and the quicksort list-sorting algorithm could stop when the input is the empty list; in both examples there is only one base case to consider, and it requires no processing.

On the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively, resulting in a hybrid algorithm. This strategy avoids the overhead of recursive calls that do little or no work, and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion. A general procedure for a simple hybrid recursive algorithm is "short-circuiting the base case", also known as "arm's-length recursion". In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking whether it is null, checking null before recursing; this avoids half the function calls in some algorithms on binary trees. Since a D&C algorithm eventually reduces each problem or sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low. Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.

Thus, for example, many library implementations of quicksort will switch to a simple loop-based insertion sort (or similar) algorithm once the number of items to be sorted is sufficiently small. Note that, if the empty list were the only base case, sorting a list with "n" entries would entail maximally "n" quicksort calls that would do nothing but return immediately. Increasing the base cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more generally a base case larger than 2 is typically used to reduce the fraction of time spent in function-call overhead or stack manipulation.

Alternatively, one can employ large base cases that still use a divide-and-conquer algorithm, but implement the algorithm for predetermined set of fixed sizes where the algorithm can be completely unrolled into code that has no recursion, loops, or conditionals (related to the technique of partial evaluation). For example, this approach is used in some efficient FFT implementations, where the base cases are unrolled implementations of divide-and-conquer FFT algorithms for a set of fixed sizes. Source-code generation methods may be used to produce the large number of separate base cases desirable to implement this strategy efficiently.

The generalized version of this idea is known as recursion "unrolling" or "coarsening", and various techniques have been proposed for automating the procedure of enlarging the base case.

For some problems, the branched recursion may end up evaluating the same sub-problem many times over. In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique commonly known as memoization. Followed to the limit, it leads to bottom-up divide-and-conquer algorithms such as dynamic programming and chart parsing.



</doc>
<doc id="26754386" url="https://en.wikipedia.org/wiki?curid=26754386" title="Randomized rounding">
Randomized rounding

Within computer science and operations research,
many combinatorial optimization problems are computationally intractable to solve exactly (to optimality).
Many such problems do admit fast (polynomial time) approximation algorithms—that is, algorithms that are guaranteed to return an approximately optimal solution given any input.

Randomized rounding

is a widely used approach for designing and analyzing such approximation algorithms. 
The basic idea is to use the probabilistic method
to convert an optimal solution of a relaxation
of the problem into an approximately optimal solution to the original problem.

The basic approach has three steps:

(Although the approach is most commonly applied with linear programs,
other kinds of relaxations are sometimes used.
For example, see Goeman's and Williamson's semi-definite programming-based
Max-Cut approximation algorithm.)

The challenge in the first step is to choose a suitable integer linear program.
Familiarity with linear programming is required, in particular, familiarity with
how to model problems using linear programs and integer linear programs.
But, for many problems, there is a natural integer linear program that works well,
such as in the Set Cover example below. (The integer linear program should have a small
integrality gap;
indeed randomized rounding is often used to prove bounds on integrality gaps.)

In the second step, the optimal fractional solution can typically be computed
in polynomial time
using any standard linear programming algorithm.

In the third step, the fractional solution must be converted into an integer solution
(and thus a solution to the original problem).
This is called "rounding" the fractional solution.
The resulting integer solution should (provably) have cost
not much larger than the cost of the fractional solution.
This will ensure that the cost of the integer solution
is not much larger than the cost of the optimal integer solution.

The main technique used to do the third step (rounding) is to use randomization,
and then to use probabilistic arguments to bound the increase in cost due to the rounding
(following the probabilistic method from combinatorics).
There, probabilistic arguments are used to show the existence of discrete structures with
desired properties. In this context, one uses such arguments to show the following:

Finally, to make the third step computationally efficient,
one either shows that formula_3 approximates formula_1
with high probability (so that the step can remain randomized)
or one derandomizes the rounding step,
typically using the method of conditional probabilities.
The latter method converts the randomized rounding process
into an efficient deterministic process that is guaranteed
to reach a good outcome.

The randomized rounding step differs from most applications of the probabilistic method in two respects:

The following example illustrates how randomized rounding can be used to design an approximation algorithm for the Set Cover problem.

Fix any instance formula_14 of set cover over a universe formula_15.

For step 1, let IP be the standard integer linear program for set cover for this instance.

For step 2, let LP be the linear programming relaxation of IP,
and compute an optimal solution formula_16 to LP
using any standard linear programming algorithm.

(The feasible solutions to LP are the vectors formula_1
that assign each set formula_18
a non-negative weight formula_19,
such that, for each element formula_20,
formula_3 "covers" formula_22
-- the total weight assigned to the sets containing formula_22
is at least 1, that is,
The optimal solution formula_16
is a feasible solution whose cost
is as small as possible.)
Note that any set cover formula_27 for formula_28
gives a feasible solution formula_1
(where formula_30 for formula_31,
formula_32 otherwise).
The cost of this formula_27 equals the cost of formula_1, that is,
In other words, the linear program LP is a relaxation
of the given set-cover problem.

Since formula_16 has minimum cost among feasible solutions to the LP,
"the cost of formula_16 is a lower bound on the cost of the optimal set cover".

Here is a description of the third step—the rounding step,
which must convert the minimum-cost fractional set cover formula_16
into a feasible integer solution formula_3 (corresponding to a true set cover).

The rounding step should produce an formula_3 that, with positive probability,
has cost within a small factor of the cost of formula_16.
Then (since the cost of formula_16 is a lower bound on the cost of the optimal set cover),
the cost of formula_3 will be within a small factor of the optimal cost.

As a starting point, consider the most natural rounding scheme:

With this rounding scheme,
the expected cost of the chosen sets is at most formula_48,
the cost of the fractional cover.
This is good. Unfortunately the coverage is not good.
When the variables formula_49 are small,
the probability that an element formula_22 is not covered is about

So only a constant fraction of the elements will be covered in expectation.

To make formula_3 cover every element with high probability,
the standard rounding scheme
first "scales up" the rounding probabilities
by an appropriate factor formula_53.
Here is the standard rounding scheme:

Scaling the probabilities up by formula_59
increases the expected cost by formula_59,
but makes coverage of all elements likely.
The idea is to choose formula_59 as small
as possible so that all elements are provably
covered with non-zero probability.
Here is a detailed analysis.

(Note: with care the formula_65
can be reduced to formula_67.)

The output formula_3 of the random rounding scheme has the desired properties
as long as none of the following "bad" events occur:

The expectation of each formula_75 is at most formula_76.
By linearity of expectation,
the expectation of formula_69
is at most formula_78.
Thus, by Markov's inequality, the probability of the first bad event
above is at most formula_79.

For the remaining bad events (one for each element formula_22), note that,
since formula_81 for any given element formula_22,
the probability that formula_22 is not covered is

(This uses the inequality formula_85,
which is strict for formula_86.)

Thus, for each of the formula_87 elements,
the probability that the element is not covered is less than formula_88.

By the naive union bound,
the probability that one of the formula_89 bad events happens
is less than formula_90.
Thus, with positive probability there are no bad events
and formula_3 is a set cover of cost at most formula_71.
QED

The lemma above shows the "existence" of a set cover
of cost formula_93).
In this context our goal is an efficient approximation algorithm,
not just an existence proof, so we are not done.

One approach would be to increase formula_59
a little bit, then show that the probability of success is at least, say, 1/4.
With this modification, repeating the random rounding step a few times
is enough to ensure a successful outcome with high probability.

That approach weakens the approximation ratio.
We next describe a different approach that yields
a deterministic algorithm that is guaranteed to
match the approximation ratio of the existence proof above.
The approach is called the method of conditional probabilities.

The deterministic algorithm emulates the randomized rounding scheme:
it considers each set formula_44 in turn,
and chooses formula_96.
But instead of making each choice "randomly" based on formula_16,
it makes the choice "deterministically", so as to
"keep the conditional probability of failure, given the choices so far, below 1".

We want to be able to set each variable formula_75 in turn
so as to keep the conditional probability of failure below 1.
To do this, we need a good bound on the conditional probability of failure.
The bound will come by refining the original existence proof.
That proof implicitly bounds the probability of failure
by the expectation of the random variable
where
is the set of elements left uncovered at the end.

The random variable formula_101 may appear a bit mysterious,
but it mirrors the probabilistic proof in a systematic way.
The first term in formula_101 comes from applying Markov's inequality
to bound the probability of the first bad event (the cost is too high).
It contributes at least 1 to formula_101 if the cost of formula_3 is too high.
The second term
counts the number of bad events of the second kind (uncovered elements).
It contributes at least 1 to formula_101 if formula_3 leaves any element uncovered.
Thus, in any outcome where formula_101 is less than 1,
formula_3 must cover all the elements
and have cost meeting the desired bound from the lemma.
In short, if the rounding step fails, then formula_109.
This implies (by Markov's inequality) that
"formula_110 is an upper bound on the probability of failure."
Note that the argument above is implicit already in the proof of the lemma,
which also shows by calculation that formula_111.

To apply the method of conditional probabilities,
we need to extend the argument to bound the "conditional" probability of failure
as the rounding step proceeds.
Usually, this can be done in a systematic way,
although it can be technically tedious.

So, what about the "conditional" probability of failure as the rounding step iterates through the sets?
Since formula_109 in any outcome where the rounding step fails,
by Markov's inequality, the "conditional" probability of failure
is at most the "conditional" expectation of formula_101.

Next we calculate the conditional expectation of formula_101,
much as we calculated the unconditioned expectation of formula_101 in the original proof.
Consider the state of the rounding process at the end of some iteration formula_116.
Let formula_117 denote the sets considered so far
(the first formula_116 sets in formula_28).
Let formula_120 denote the (partially assigned) vector formula_3
(so formula_122 is determined only if formula_123).
For each set formula_124,
let formula_125
denote the probability with which formula_75 will be set to 1.
Let formula_127 contain the not-yet-covered elements.
Then the conditional expectation of formula_101,
given the choices made so far, that is, given formula_120, is

Note that formula_131 is determined only after iteration formula_116.

To keep the conditional probability of failure below 1,
it suffices to keep the conditional expectation of formula_101 below 1.
To do this, it suffices to keep the conditional expectation of formula_101 from increasing.
This is what the algorithm will do.
It will set formula_75 in each iteration to ensure that
(where formula_137).

In the formula_116th iteration,
how can the algorithm set formula_139
to ensure that formula_140?
It turns out that it can simply set formula_139
so as to "minimize" the resulting value of formula_142.

To see why, focus on the point in time when iteration formula_116 starts.
At that time, formula_144 is determined,
but formula_142 is not yet determined
--- it can take two possible values depending on how formula_139
is set in iteration formula_116.
Let formula_148 denote the value of formula_149.
Let formula_150 and formula_151,
denote the two possible values of formula_142,
depending on whether formula_139 is set to 0, or 1, respectively.
By the definition of conditional expectation,
Since a weighted average of two quantities
is always at least the minimum of those two quantities,
it follows that
Thus, setting formula_139
so as to minimize the resulting value of
formula_131
will guarantee that
formula_158.
This is what the algorithm will do.

In detail, what does this mean?
Considered as a function of formula_139
formula_131
is a linear function of formula_139,
and the coefficient of formula_139 in that function is

Thus, the algorithm should set formula_139 to 0 if this expression is positive,
and 1 otherwise. This gives the following algorithm.

input: set system formula_28, universe formula_15, cost vector formula_167

output: set cover formula_3 (a solution to the standard integer linear program for set cover)

The algorithm ensures that the conditional expectation of formula_101,
formula_185, does not increase at each iteration.
Since this conditional expectation is initially less than 1 (as shown previously),
the algorithm ensures that the conditional expectation stays below 1.
Since the conditional probability of failure
is at most the conditional expectation of formula_101,
in this way the algorithm
ensures that the conditional probability of failure stays below 1.
Thus, at the end, when all choices are determined,
the algorithm reaches a successful outcome.
That is, the algorithm above returns a set cover formula_3
of cost at most formula_183 times
the minimum cost of any (fractional) set cover.

In the example above, the algorithm was guided by the conditional expectation of a random variable formula_101.
In some cases, instead of an exact conditional expectation,
an "upper bound" (or sometimes a lower bound)
on some conditional expectation is used instead.
This is called a pessimistic estimator.





</doc>
<doc id="25190127" url="https://en.wikipedia.org/wiki?curid=25190127" title="Reservoir sampling">
Reservoir sampling

Reservoir sampling is a family of randomized algorithms for choosing a simple random sample without replacement of items from a population of unknown size in a single pass over the items. The size of the population is not known to the algorithm and is typically too large to fit all items into main memory. The population is revealed to the algorithm over time, and the algorithm cannot look back at previous items. At any point, the current state of the algorithm must permit extraction of a simple random sample without replacement of size over the part of the population seen so far.

Suppose we see a sequence of items, one at a time. We want to keep ten items in memory, and we want them to be selected at random from the sequence. If we know the total number of items and can access the items arbitrarily, then the solution is easy: select 10 distinct indices between 1 and with equal probability, and keep the -th elements. The problem is that we do not always know the exact in advance.

A simple and popular but slow algorithm, commonly known as "Algorithm R", is due to Alan Waterman.

The algorithm works by maintaining a "reservoir" of size , which initially contains the first items of the input. It then iterates over the remaining items until the input is exhausted. Using one-based array indexing, let formula_1 be the index of the item currently under consideration. The algorithm then generates a random number between (and including) 1 and . If is at most , then the item is selected and replaces whichever item currently occupies the -th position in the reservoir. Otherwise, the item is discarded. In effect, for all , the element of the input is chosen to be included in the reservoir with probability formula_2. Similarly, at each iteration the element of the reservoir array is chosen to be replaced with probability formula_3. It can be shown that when the algorithm has finished executing, each item in the input population has equal probability (i.e., formula_4) of being chosen for the reservoir.

While conceptually simple and easy to understand, this algorithm needs to generate a random number for each item of the input, including the items that are discarded. Its asymptotic running time is thus formula_5. This causes the algorithm to be unnecessarily slow if the input population is large.

"Algorithm L" improves upon this algorithm by computing how many items are discarded before the next item enters the reservoir. The key observation is that this number follows a geometric distribution and can therefore be computed in constant time.

This algorithm computes three random numbers for each item that becomes part of the reservoir, and does not spend any time on items that do not. Its expected running time is thus formula_6, which is optimal. At the same time, it is simple to implement efficiently and does not depend on random deviates from exotic or hard-to-compute distributions.

If we associate with each item of the input a uniformly generated random number, the items with the largest (or, equivalently, smallest) associated values form a simple random sample. A simple reservoir-sampling thus maintains the items with the currently largest associated values in a priority queue.

The expected running time of this algorithm is formula_7 and it is relevant mainly because it can easily be extended to items with weights.

Some applications require items' sampling probabilities to be according to weights associated with each item. For example, it might be required to sample queries in a search engine with weight as number of times they were performed so that the sample can be analyzed for overall impact on user experience. Let the weight of item be formula_8, and the sum of all weights be . There are two ways to interpret weights assigned to each item in the set:

The following algorithm was given by Efraimidis and Spirakis that uses interpretation 1:
ReservoirSample(S[1..?])
This algorithm is identical to the algorithm given in Reservoir Sampling with Random Sort except for the generation of the items' keys. The algorithm is equivalent to assigning each item a key formula_12 where is the random number and then selecting the items with the largest keys. Equivalently, a more numerically stable formulation of this algorithm computes the keys as formula_13 and select the items with the "smallest" keys.

Following algorithm was given by M. T. Chao uses interpretation 2:
WeightedReservoir-Chao(S[1..n], R[1..k])

For each item, its relative weight is calculated and used to randomly decide if the item will be added into the reservoir. If the item is selected, then one of the existing items of the reservoir is uniformly selected and replaced with the new item. The trick here is that, if the probabilities of all items in the reservoir are already proportional to their weights, then by selecting uniformly which item to replace, the probabilities of all items remain proportional to their weight after the replacement.

Suppose one wanted to draw random cards from a deck of cards.
A natural approach would be to shuffle the deck and then take the top cards.
In the general case, the shuffle also needs to work even if the number of cards in the deck is not known in advance, a condition which is satisfied by the inside-out version of the Fisher–Yates shuffle:
Shuffle(S[1..n], R[1..n])
Note that although the rest of the cards are shuffled, only the first are important in the present context.
Therefore, the array need only track the cards in the first positions while performing the shuffle, reducing the amount of memory needed.
Truncating to length , the algorithm is modified accordingly:

ReservoirSample(S[1..n], R[1..k])
Since the order of the first cards is immaterial, the first loop can be removed and can be initialized to be the first items of the input.
This yields "Algorithm R".

Probabilities of selection of the reservoir methods are discussed in Chao (1982) and Tillé (2006). While the first-order selection probabilities are equal to formula_4 (or, in case of Chao's procedure, to an arbitrary set of unequal probabilities), the second order selection probabilities depend on the order in which the records are sorted in the original reservoir. The problem is overcome by the cube sampling method of Deville and Tillé (2004).

Reservoir sampling makes the assumption that the desired sample fits into main memory, often implying that is a constant independent of . In applications where we would like to select a large subset of the input list (say a third, i.e. formula_15), other methods need to be adopted. Distributed implementations for this problem have been proposed.


</doc>
<doc id="219861" url="https://en.wikipedia.org/wiki?curid=219861" title="In-place algorithm">
In-place algorithm

In computer science, an in-place algorithm is an algorithm which transforms input using no auxiliary data structure. However a small amount of extra storage space is allowed for auxiliary variables. The input is usually overwritten by the output as the algorithm executes. In-place algorithm updates input sequence only through replacement or swapping of elements. An algorithm which is not in-place is sometimes called not-in-place or out-of-place.

In-place can have slightly different meanings. In its strictest form, the algorithm can only have a constant amount of extra space, counting everything including function calls and pointers. However, this form is very limited as simply having an index to a length n array requires O(log "n") bits. More broadly, in-place means that the algorithm does not use extra space for manipulating the input but may require a small though nonconstant extra space for its operation. Usually, this space is O(log "n"), though sometimes anything in O("n") is allowed. Note that space complexity also has varied choices in whether or not to count the index lengths as part of the space used. Often, the space complexity is given in terms of the number of indices or pointers needed, ignoring their length. In this article, we refer to total space complexity (DSPACE), counting pointer lengths. Therefore, the space requirements here have an extra log "n" factor compared to an analysis that ignores the length of indices and pointers. 

An algorithm may or may not count the output as part of its space usage. Since in-place algorithms usually overwrite their input with output, no additional space is needed. When writing the output to write-only memory or a stream, it may be more appropriate to only consider the working space of the algorithm. In theory applications such as log-space reductions, it is more typical to always ignore output space (in these cases it is more essential that the output is "write-only").

Given an array codice_1 of "n" items, suppose we want an array that holds the same elements in reversed order and dispose of the original. One seemingly simple way to do this is to create a new array of equal size, fill it with copies from codice_1 in appropriate order and then delete codice_1.

Unfortunately, this requires O("n") extra space for having the arrays codice_1 and codice_5 available simultaneously. Also, allocation and deallocation are often slow operations. Since we no longer need codice_1, we can instead overwrite it with its own reversal using this in-place algorithm which will only need constant number (2) of integers for the auxiliary variables codice_7 and codice_8, no matter how large the array is.

As another example, many sorting algorithms rearrange arrays into sorted order in-place, including: bubble sort, comb sort, selection sort, insertion sort, heapsort, and Shell sort. These algorithms require only a few pointers, so their space complexity is .

Quicksort operates in-place on the data to be sorted. However, quicksort requires stack space pointers to keep track of the subarrays in its divide and conquer strategy. Consequently, quicksort needs additional space. Although this non-constant space technically takes quicksort out of the in-place category, quicksort and other algorithms needing only additional pointers are usually considered in-place algorithms.

Most selection algorithms are also in-place, although some considerably rearrange the input array in the process of finding the final, constant-sized result.

Some text manipulation algorithms such as trim and reverse may be done in-place.

In computational complexity theory, the strict definition of in-place algorithms includes all algorithms with O(1) space complexity, the class DSPACE(1). This class is very limited; it equals the regular languages. In fact, it does not even include any of the examples listed above.

We usually consider algorithms in L, the class of problems requiring O(log "n") additional space, to be in-place. This class is more in line with the practical definition, as it allows numbers of size n as pointers or indices. This expanded definition still excludes quicksort, however, because of its recursive calls. 

Identifying the in-place algorithms with L has some interesting implications; for example, it means that there is a (rather complex) in-place algorithm to determine whether a path exists between two nodes in an undirected graph, a problem that requires O("n") extra space using typical algorithms such as depth-first search (a visited bit for each node). This in turn yields in-place algorithms for problems such as determining if a graph is bipartite or testing whether two graphs have the same number of connected components. See SL for more information.

In many cases, the space requirements for an algorithm can be drastically cut by using a randomized algorithm. For example, say we wish to know if two vertices in a graph of "n" vertices are in the same connected component of the graph. There is no known simple, deterministic, in-place algorithm to determine this, but if we simply start at one vertex and perform a random walk of about 20"n" steps, the chance that we will stumble across the other vertex provided that it is in the same component is very high. Similarly, there are simple randomized in-place algorithms for primality testing such as the Miller-Rabin primality test, and there are also simple in-place randomized factoring algorithms such as Pollard's rho algorithm. See RL and BPL for more discussion of this phenomenon.

Functional programming languages often discourage or don't support explicit in-place algorithms that overwrite data, since this is a type of side effect; instead, they only allow new data to be constructed. However, good functional language compilers will often recognize when an object very similar to an existing one is created and then the old one is thrown away, and will optimize this into a simple mutation "under-the-hood".

Note that it is possible in principle to carefully construct in-place algorithms that don't modify data (unless the data is no longer being used), but this is rarely done in practice. See purely functional data structures.



</doc>
<doc id="416776" url="https://en.wikipedia.org/wiki?curid=416776" title="Timeline of algorithms">
Timeline of algorithms

The following timeline outlines the development of algorithms (mainly "mathematical recipes") since their inception.












</doc>
<doc id="6901703" url="https://en.wikipedia.org/wiki?curid=6901703" title="Algorithm characterizations">
Algorithm characterizations

Algorithm characterizations are attempts to formalize the word algorithm. Algorithm does not have a generally accepted formal definition. Researchers are actively working on this problem. This article will present some of the "characterizations" of the notion of "algorithm" in more detail.

Over the last 200 years the definition of algorithm has become more complicated and detailed as researchers have tried to pin down the term. Indeed, there may be more than one type of "algorithm". But most agree that algorithm has something to do with defining generalized processes for the creation of "output" integers from other "input" integers – "input parameters" arbitrary and infinite in extent, or limited in extent but still variable—by the manipulation of distinguishable symbols (counting numbers) with finite collections of rules that a person can perform with paper and pencil.

The most common number-manipulation schemes—both in formal mathematics and in routine life—are: (1) the recursive functions calculated by a person with paper and pencil, and (2) the Turing machine or its Turing equivalents—the primitive register machine or "counter machine" model, the Random Access Machine model (RAM), the Random access stored program machine model (RASP) and its functional equivalent "the computer".

When we are doing "arithmetic" we are really calculating by the use of "recursive functions" in the shorthand algorithms we learned in grade-school, for example, adding and subtracting.

The proofs that every "recursive function" we can "calculate by hand" we can "compute by machine" and vice versa—note the usage of the words "calculate" versus "compute"—is remarkable. But this equivalence together with the "thesis" (unproven assertion) that this includes "every" calculation/computation indicates why so much emphasis has been placed upon the use of Turing-equivalent machines in the definition of specific algorithms, and why the definition of "algorithm" itself often refers back to "the Turing machine". This is discussed in more detail under Stephen Kleene's characterization.

The following are summaries of the more famous characterizations (Kleene, Markov, Knuth) together with those that introduce novel elements—elements that further expand the definition or contribute to a more precise definition.

There is more consensus on the "characterization" of the notion of "simple algorithm".

All algorithms need to be specified in a formal language, and the "simplicity notion" arises from the simplicity of the language. The Chomsky (1956) hierarchy is a containment hierarchy of classes of formal grammars that generate formal languages. It is used for classifying of programming languages and abstract machines.

From the "Chomsky hierarchy" perspective, if the algorithm can be specified on a simpler language (than unrestricted), it can be characterized by this kind of language, else it is a typical "unrestricted algorithm".

Examples: a "general purpose" macro language, like M4 is unrestricted (Turing complete), but the C preprocessor macro language is not, so any algorithm expressed in "C preprocessor" is a "simple algorithm".

See also Relationships between complexity classes.

The following are the features of a good algorithm;



In early 1870 W. Stanley Jevons presented a "Logical Machine" (Jevons 1880:200) for analyzing a syllogism or other logical form e.g. an argument reduced to a Boolean equation. By means of what Couturat (1914) called a "sort of "logical piano" [,] ... the equalities which represent the premises ... are "played" on a keyboard like that of a typewriter. ... When all the premises have been "played", the panel shows only those constituents whose sum is equal to 1, that is, ... its logical whole. This mechanical method has the advantage over VENN's geometrical method..." (Couturat 1914:75).

For his part John Venn, a logician contemporary to Jevons, was less than thrilled, opining that "it does not seem to me that any contrivances at present known "or likely to be discovered" really deserve the name of logical machines" (italics added, Venn 1881:120). But of historical use to the developing notion of "algorithm" is his explanation for his negative reaction with respect to a machine that "may subserve a really valuable purpose by enabling us to avoid otherwise inevitable labor":
He concludes that "I cannot see that any machine can hope to help us except in the third of these steps; so that it seems very doubtful whether any thing of this sort really deserves the name of a logical engine."(Venn 1881:119–121).

This section is longer and more detailed than the others because of its importance to the topic: Kleene was the first to propose that "all" calculations/computations—of "every" sort, the "totality" of—can "equivalently" be (i) "calculated" by use of five "primitive recursive operators" plus one special operator called the mu-operator, or be (ii) "computed" by the actions of a Turing machine or an equivalent model.

Furthermore, he opined that either of these would stand as a definition of algorithm.

A reader first confronting the words that follow may well be confused, so a brief explanation is in order. "Calculation" means done by hand, "computation" means done by Turing machine (or equivalent). (Sometimes an author slips and interchanges the words). A "function" can be thought of as an "input-output box" into which a person puts natural numbers called "arguments" or "parameters" (but only the counting numbers including 0—the nonnegative integers) and gets out a single nonnegative integer (conventionally called "the answer"). Think of the "function-box" as a little man either calculating by hand using "general recursion" or computing by Turing machine (or an equivalent machine).

"Effectively calculable/computable" is more generic and means "calculable/computable by "some" procedure, method, technique ... whatever...". "General recursive" was Kleene's way of writing what today is called just "recursion"; however, "primitive recursion"—calculation by use of the five recursive operators—is a lesser form of recursion that lacks access to the sixth, additional, mu-operator that is needed only in rare instances. Thus most of life goes on requiring only the "primitive recursive functions."

In 1943 Kleene proposed what has come to be known as Church's thesis:

In a nutshell: to calculate "any" function the only operations a person needs (technically, formally) are the 6 primitive operators of "general" recursion (nowadays called the operators of the mu recursive functions).

Kleene's first statement of this was under the section title "12. Algorithmic theories". He would later amplify it in his text (1952) as follows:

This is not as daunting as it may sound – "general" recursion is just a way of making our everyday arithmetic operations from the five "operators" of the primitive recursive functions together with the additional mu-operator as needed. Indeed, Kleene gives 13 examples of primitive recursive functions and Boolos–Burgess–Jeffrey add some more, most of which will be familiar to the reader—e.g. addition, subtraction, multiplication and division, exponentiation, the CASE function, concatenation, etc., etc.; for a list see Some common primitive recursive functions.
Why general-recursive functions rather than primitive-recursive functions?

Kleene et al. (cf §55 General recursive functions p. 270 in Kleene 1952) had to add a sixth recursion operator called the minimization-operator (written as μ-operator or mu-operator) because Ackermann (1925) produced a hugely growing function—the Ackermann function—and Rózsa Péter (1935) produced a general method of creating recursive functions using Cantor's diagonal argument, neither of which could be described by the 5 primitive-recursive-function operators. With respect to the Ackermann function:
But the need for the mu-operator is a rarity. As indicated above by Kleene's list of common calculations, a person goes about their life happily computing primitive recursive functions without fear of encountering the monster numbers created by Ackermann's function (e.g. super-exponentiation ).

Turing's Thesis hypothesizes the computability of "all computable functions" by the Turing machine model and its equivalents.

To do this in an effective manner, Kleene extended the notion of "computable" by casting the net wider—by allowing into the notion of "functions" both "total functions" and "partial functions". A "total function" is one that is defined "for all natural numbers" (positive integers including 0). A partial function is defined for "some" natural numbers but not all—the specification of "some" has to come "up front". Thus the inclusion of "partial function" extends the notion of function to "less-perfect" functions. Total- and partial-functions may either be calculated by hand or computed by machine.

We now observe Kleene's definition of "computable" in a formal sense:

Thus we have arrived at "Turing's Thesis":

Although Kleene did not give examples of "computable functions" others have. For example, Davis (1958) gives Turing tables for the Constant, Successor and Identity functions, three of the five operators of the primitive recursive functions:

Boolos–Burgess–Jeffrey (2002) give the following as prose descriptions of Turing machines for:

With regards to the counter machine, an abstract machine model equivalent to the Turing machine:

Demonstrations of computability by abacus machine (Boolos–Burgess–Jeffrey (2002)) and by counter machine (Minsky 1967):

The fact that the abacus/counter machine models can simulate the recursive functions provides the proof that: If a function is "machine computable" then it is "hand-calculable by partial recursion". Kleene's Theorem XXIX :

The converse appears as his Theorem XXVIII. Together these form the proof of their equivalence, Kleene's Theorem XXX.

With his Theorem XXX Kleene proves the "equivalence" of the two "Theses"—the Church Thesis and the Turing Thesis. (Kleene can only hypothesize (conjecture) the truth of both thesis – "these he has not proven"):

Thus by Kleene's Theorem XXX: either method of making numbers from input-numbers—recursive functions calculated by hand or computated by Turing-machine or equivalent—results in an ""effectively calculable/computable" function". If we accept the hypothesis that "every" calculation/computation can be done by either method equivalently we have accepted both Kleene's Theorem XXX (the equivalence) and the Church–Turing Thesis (the hypothesis of "every").

The notion of separating out Church's and Turing's theses from the "Church–Turing thesis" appears not only in Kleene (1952) but in Blass-Gurevich (2003) as well. But while there are agreements, there are disagreements too:

Andrey Markov Jr. (1954) provided the following definition of algorithm:

He admitted that this definition "does not pretend to mathematical precision" (p. 1). His 1954 monograph was his attempt to define algorithm more accurately; he saw his resulting definition—his "normal" algorithm—as "equivalent to the concept of a recursive function" (p. 3). His definition included four major components (Chapter II.3 pp. 63ff):

In his Introduction Markov observed that "the entire significance for mathematics" of efforts to define algorithm more precisely would be "in connection with the problem of a constructive foundation for mathematics" (p. 2). Ian Stewart (cf Encyclopædia Britannica) shares a similar belief: "...constructive analysis is very much in the same algorithmic spirit as computer science...". For more see constructive mathematics and Intuitionism.

Distinguishability and Locality: Both notions first appeared with Turing (1936–1937) --

Locality appears prominently in the work of Gurevich and Gandy (1980) (whom Gurevich cites). Gandy's "Fourth Principle for Mechanisms" is "The Principle of Local Causality":

1936: A rather famous quote from Kurt Gödel appears in a "Remark added in proof [of the original German publication] in his paper "On the Length of Proofs" translated by Martin Davis appearing on pp. 82–83 of "The Undecidable". A number of authors—Kleene, Gurevich, Gandy etc. -- have quoted the following:

1963: In a "Note" dated 28 August 1963 added to his famous paper "On Formally Undecidable Propositions" (1931) Gödel states (in a footnote) his belief that "formal systems" have "the characteristic property that reasoning in them, in principle, can be completely replaced by mechanical devices" (p. 616 in van Heijenoort). ". . . due to "A. M. Turing's work a precise and unquestionably adequate definition of the general notion of formal system can now be given [and] a completely general version of Theorems VI and XI is now possible." (p. 616). In a 1964 note to another work he expresses the same opinion more strongly and in more detail.

1964: In a Postscriptum, dated 1964, to a paper presented to the Institute for Advanced Study in spring 1934, Gödel amplified his conviction that "formal systems" are those that can be mechanized:

The * indicates a footnote in which Gödel cites the papers by Alan Turing (1937) and Emil Post (1936) and then goes on to make the following intriguing statement:

Church's definitions encompass so-called "recursion" and the "lambda calculus" (i.e. the λ-definable functions). His footnote 18 says that he discussed the relationship of "effective calculatibility" and "recursiveness" with Gödel but that he independently questioned "effectively calculability" and "λ-definability":

It would appear from this, and the following, that far as Gödel was concerned, the Turing machine was sufficient and the lambda calculus was "much less suitable." He goes on to make the point that, with regards to limitations on human reason, the jury is still out:

Minsky (1967) baldly asserts that "an algorithm is "an effective procedure" and declines to use the word "algorithm" further in his text; in fact his index makes it clear what he feels about "Algorithm, "synonym" for Effective procedure"(p. 311):

Other writers (see Knuth below) use the word "effective procedure". This leads one to wonder: What is Minsky's notion of "an effective procedure"? He starts off with:

But he recognizes that this is subject to a criticism:

His refinement? To "specify, along with the statement of the rules, "the details of the mechanism that is to interpret them"". To avoid the "cumbersome" process of "having to do this over again for each individual procedure" he hopes to identify a "reasonably "uniform" family of rule-obeying mechanisms". His "formulation":

In the end, though, he still worries that "there remains a subjective aspect to the matter. Different people may not agree on whether a certain procedure should be called effective" (p. 107)

But Minsky is undeterred. He immediately introduces "Turing's Analysis of Computation Process" (his chapter 5.2). He quotes what he calls "Turing's "thesis""

After an analysis of "Turing's Argument" (his chapter 5.3)
he observes that "equivalence of many intuitive formulations" of Turing, Church, Kleene, Post, and Smullyan "...leads us to suppose that there is really here an 'objective' or 'absolute' notion. As Rogers [1959] put it:

In his 1967 "Theory of Recursive Functions and Effective Computability" Hartley Rogers' characterizes "algorithm" roughly as "a clerical (i.e., deterministic, bookkeeping) procedure . . . applied to . . . symbolic "inputs" and which will eventually yield, for each such input, a corresponding symbolic "output""(p. 1). He then goes on to describe the notion "in approximate and intuitive terms" as having 10 "features", 5 of which he asserts that "virtually all mathematicians would agree [to]" (p. 2). The remaining 5 he asserts "are less obvious than *1 to *5 and about which we might find less general agreement" (p. 3).

The 5 "obvious" are:

The remaining 5 that he opens to debate, are:

Knuth (1968, 1973) has given a list of five properties that are widely accepted as requirements for an algorithm:


Knuth offers as an example the Euclidean algorithm for determining the greatest common divisor of two natural numbers (cf. Knuth Vol. 1 p. 2).

Knuth admits that, while his description of an algorithm may be intuitively clear, it lacks formal rigor, since it is not exactly clear what "precisely defined" means, or "rigorously and unambiguously specified" means, or "sufficiently basic", and so forth. He makes an effort in this direction in his first volume where he defines "in detail" what he calls the "machine language" for his "mythical MIX...the world's first polyunsaturated computer" (pp. 120ff). Many of the algorithms in his books are written in the MIX language. He also uses tree diagrams, flow diagrams and state diagrams.

"Goodness" of an algorithm, "best" algorithms: Knuth states that "In practice, we not only want algorithms, we want "good" algorithms..." He suggests that some criteria of an algorithm's goodness are the number of steps to perform the algorithm, its "adaptability to computers, its simplicity and elegance, etc." Given a number of algorithms to perform the same computation, which one is "best"? He calls this sort of inquiry "algorithmic analysis: given an algorithm, to determine its performance characteristcis" (all quotes this paragraph: Knuth Vol. 1 p. 7)

Stone (1972) and Knuth (1968, 1973) were professors at Stanford University at the same time so it is not surprising if there are similarities in their definitions (boldface added for emphasis):

Stone is noteworthy because of his detailed discussion of what constitutes an “effective” rule – his robot, or person-acting-as-robot, must have some information and abilities within them, and if not the information and the ability must be provided in "the algorithm":

Furthermore, "...not all instructions are acceptable, because they may require the robot to have abilities beyond those that we consider reasonable.” He gives the example of a robot confronted with the question is “Henry VIII a King of England?” and to print 1 if yes and 0 if no, but the robot has not been previously provided with this information. And worse, if the robot is asked if Aristotle was a King of England and the robot only had been provided with five names, it would not know how to answer. Thus:

After providing us with his definition, Stone introduces the Turing machine model and states that the set of five-tuples that are the machine’s instructions are “an algorithm ... known as a Turing machine program” (p. 9). Immediately thereafter he goes on say that a “"computation" of a Turing machine is "described" by stating:

This precise prescription of what is required for "a computation" is in the spirit of what will follow in the work of Blass and Gurevich.

While a student at Princeton in the mid-1960s, David Berlinski was a student of Alonzo Church (cf p. 160). His year-2000 book "The Advent of the Algorithm: The 300-year Journey from an Idea to the Computer" contains the following definition of algorithm:

A careful reading of Gurevich 2000 leads one to conclude (infer?) that he believes that "an algorithm" is actually "a Turing machine" or "a pointer machine" doing a computation. An "algorithm" is not just the symbol-table that guides the behavior of the machine, nor is it just one instance of a machine doing a computation given a particular set of input parameters, nor is it a suitably programmed machine with the power off; rather "an algorithm is the machine actually doing any computation of which it is capable". Gurevich does not come right out and say this, so as worded above this conclusion (inference?) is certainly open to debate:

In Blass and Gurevich 2002 the authors invoke a dialog between "Quisani" ("Q") and "Authors" (A), using Yiannis Moshovakis as a foil, where they come right out and flatly state:

This use of the word "implementation" cuts straight to the heart of the question. Early in the paper, Q states his reading of Moshovakis:

But the authors waffle here, saying "[L]et's stick to "algorithm" and "machine", and the reader is left, again, confused. We have to wait until Dershowitz and Gurevich 2007 to get the following footnote comment:

Blass and Gurevich describe their work as evolved from consideration of Turing machines and pointer machines, specifically Kolmogorov-Uspensky machines (KU machines), Schönhage Storage Modification Machines (SMM), and linking automata as defined by Knuth. The work of Gandy and Markov are also described as influential precursors.

Gurevich offers a 'strong' definition of an algorithm (boldface added):

The above phrase computation as an evolution of the state differs markedly from the definition of Knuth and Stone—the "algorithm" as a Turing machine program. Rather, it corresponds to what Turing called "the complete configuration" (cf Turing's definition in Undecidable, p. 118) -- and includes "both" the current instruction (state) "and" the status of the tape. [cf Kleene (1952) p. 375 where he shows an example of a tape with 6 symbols on it—all other squares are blank—and how to Gödelize its combined table-tape status].

In Algorithm examples we see the evolution of the state first-hand.

Philosopher Daniel Dennett analyses the importance of evolution as an algorithmic process in his 1995 book "Darwin's Dangerous Idea". Dennett identifies three key features of an algorithm:

It is on the basis of this analysis that Dennett concludes that "According to Darwin, evolution is an algorithmic process". (p. 60).

However, in the previous page he has gone out on a much-further limb. In the context of his chapter titled "Processes as Algorithms", he states:

It is unclear from the above whether Dennett is stating that the physical world by itself and without observers is intrinsically algorithmic (computational) or whether a symbol-processing observer is what is adding "meaning" to the observations.

Daniel Dennett is a proponent of strong artificial intelligence: the idea that the logical structure of an algorithm is sufficient to explain mind. John Searle, the creator of the Chinese room thought experiment, claims that "syntax [that is, logical structure] is by itself not sufficient for semantic content [that is, meaning]" . In other words, the "meaning" of symbols is relative to the mind that is using them; an algorithm—a logical construct—by itself is insufficient for a mind.

Searle cautions those who claim that algorithmic (computational) processes are intrinsic to nature (for example, cosmologists, physicists, chemists, etc.):

An example in Boolos-Burgess-Jeffrey (2002) (pp. 31–32) demonstrates the precision required in a complete specification of an algorithm, in this case to add two numbers: m+n. It is similar to the Stone requirements above.

(i) They have discussed the role of "number format" in the computation and selected the "tally notation" to represent numbers:

(ii) At the outset of their example they specify the machine to be used in the computation as a Turing machine. They have previously specified (p. 26) that the Turing-machine will be of the 4-tuple, rather than 5-tuple, variety. For more on this convention see Turing machine.

(iii) Previously the authors have specified that the tape-head's position will be indicated by a subscript to the "right" of the scanned symbol. For more on this convention see Turing machine. (In the following, boldface is added for emphasis):

This specification is incomplete: it requires the location of where the instructions are to be placed and their format in the machine--

This later point is important. Boolos-Burgess-Jeffrey give a demonstration (p. 36) that the predictability of the entries in the table allow one to "shrink" the table by putting the entries in sequence and omitting the input state and the symbol. Indeed, the example Turing machine computation required only the 4 columns as shown in the table below (but note: these were presented to the machine in "rows"):

Sipser begins by defining '"algorithm" as follows:

Does Sipser mean that "algorithm" is just "instructions" for a Turing machine, or is the combination of "instructions + a (specific variety of) Turing machine"? For example, he defines the two standard variants (multi-tape and non-deterministic) of his particular variant (not the same as Turing's original) and goes on, in his Problems (pages 160-161), to describes four more variants (write-once, doubly infinite tape (i.e. left- and right-infinite), left reset, and "stay put instead of left). In addition, he imposes some constraints. First, the input must be encoded as a string (p. 157) and says of numeric encodings in the context of complexity theory:

van Emde Boas comments on a similar problem with respect to the random access machine (RAM) abstract model of computation sometimes used in place of the Turing machine when doing "analysis of algorithms":
"The absence or presence of multiplicative and parallel bit manipulation operations is of relevance for the correct understanding of some results in the analysis of algorithms.

". . . [T]here hardly exists such as a thing as an "innocent" extension of the standard RAM model in the uniform time measures; either one only has additive arithmetic or one might as well include all reasonable multiplicative and/or bitwise Boolean instructions on small operands." (van Emde Boas, 1990:26)

With regards to a "description language" for algorithms Sipser finishes the job that Stone and Boolos-Burgess-Jeffrey started (boldface added). He offers us three levels of description of Turing machine algorithms (p. 157):



</doc>
<doc id="2842189" url="https://en.wikipedia.org/wiki?curid=2842189" title="Hyphenation algorithm">
Hyphenation algorithm

A hyphenation algorithm is a set of rules, especially one codified for implementation in a computer program, that decides at which points a word can be broken over two lines with a hyphen. For example, a hyphenation algorithm might decide that "impeachment" can be broken as "impeach-ment" or "im-peachment" but not "impe-achment".

One of the reasons for the complexity of the rules of word-breaking is that different "dialects" of English tend to differ on hyphenation: American English tends to work on sound, but British English tends to look to the origins of the word and then to sound. There are also a large number of exceptions, which further complicates matters.

Some rules of thumb can be found in the Major Keary's: "On Hyphenation – Anarchy of Pedantry." Among the algorithmic approaches to hyphenation, the one implemented in the TeX typesetting system is widely used. It is thoroughly documented in the first two volumes of "Computers and Typesetting" and in Francis Mark Liang's dissertation. The aim of Liang's work was to get the algorithm as accurate as he practically could and to keep any exception dictionary small.

In TeX's original hyphenation patterns for American English, the exception list contains only 14 words.

Ports of the TeX hyphenation algorithm are available as libraries for several programming languages, including Haskell, JavaScript, Perl, PostScript, Python, Ruby, C#, and TeX can be made to show hyphens in the log by the command codice_1.

In LaTeX, hyphenation correction can be added by users by using:

The codice_2 command declares allowed hyphenation points in which words is a list of words, separated by spaces, in which each hyphenation point is indicated by a codice_3 character. For example,

declares that in the current job "fortran" should not be hyphenated and that if "ergonomic" must be hyphenated, it will be at one of the indicated points.

However, there are several limits. For example, the stock codice_2 command accepts only ASCII letters by default and so it cannot be used to correct hyphenation for words with non-ASCII characters (like "ä", "é", "ç"), which are very common in almost all languages except English. Simple workarounds exist, however.




</doc>
<doc id="7347241" url="https://en.wikipedia.org/wiki?curid=7347241" title="Spreading activation">
Spreading activation

Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or "activation" and then iteratively propagating or "spreading" that activation out to other nodes linked to the source nodes. Most often these "weights" are real values that decay as activation propagates through the network. When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing.

Spreading activation models are used in cognitive psychology to model the fan out effect.

Spreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents.

As it relates to cognitive psychology, spreading activation is the theory of how the brain iterates through a network of associated ideas to retrieve specific information. The spreading activation theory presents the array of concepts within our memory as cognitive units, each consisting of a node and its associated elements or characteristics, all connected together by edges. A spreading activation network can be represented schematically, in a sort of web diagram with shorter lines between two nodes meaning the ideas are more closely related and will typically be associated more quickly to the original concept. For memory psychology, Spreading activation model means people organize their knowledge of the world based on their personal experience, which is saying those personal experiences form the network of ideas that is the person's knowledge of the world.

When a word (the target) is preceded by an associated word (the prime) in word recognition tasks, participants seem to perform better in the amount of time that it takes them to respond. For instance, subjects respond faster to the word "doctor" when it is preceded by "nurse" than when it is preceded by an unrelated word like "carrot". This semantic priming effect with words that are close in meaning within the cognitive network has been seen in a wide range of tasks given by experimenters, ranging from sentence verification to lexical decision and naming.

As another example, if the original concept is "red" and the concept "vehicles" is primed, they are much more likely to say "fire engine" instead of something unrelated to vehicles, such as "cherries". If instead "fruits" was primed, they would likely name "cherries" and continue on from there. The activation of pathways in the network has everything to do with how closely linked two concepts are by meaning, as well as how a subject is primed.

A directed graph is populated by Nodes[ 1...N ] each having an associated activation value A [ i ] which is a real number in the range [ 0.0 ... 1.0]. A Link[ i, j ] connects source node[ i ] with target node[ j ]. Each edge has an associated weight W [ i, j ] usually a real number in the range [0.0 ... 1.0].

Parameters:

Steps:




</doc>
<doc id="26784161" url="https://en.wikipedia.org/wiki?curid=26784161" title="Manhattan address algorithm">
Manhattan address algorithm

The Manhattan address algorithm refers to the formulas used to estimate the closest east–west cross street for building numbers on north–south avenues in the New York City borough of Manhattan.

To find the approximate number of the closest cross street, divide the building number by a divisor (generally 20) and add (or subtract) the "magic number" from the table below:


</doc>
<doc id="35685954" url="https://en.wikipedia.org/wiki?curid=35685954" title="Generalized distributive law">
Generalized distributive law

The generalized distributive law (GDL) is a generalization of the distributive property which gives rise to a general message passing algorithm. It is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. The law and algorithm were introduced in a semi-tutorial by Srinivas M. Aji and Robert J. McEliece with the same title.

"The distributive law in mathematics is the law relating the operations of multiplication and addition, stated symbolically, formula_1; that is, the monomial factor formula_2 is distributed, or separately applied, to each term of the binomial factor formula_3, resulting in the product formula_4" - Britannica

As it can be observed from the definition, application of distributive law to an arithmetic expression reduces the number of operations in it. In the previous example the total number of operations reduced from three (two multiplications and an addition in formula_4) to two (one multiplication and one addition in formula_6). Generalization of distributive law leads to a large family of fast algorithms. This includes the FFT and Viterbi algorithm.

This is explained in a more formal way in the example below:

formula_7 where formula_8 and formula_9 are real-valued functions, formula_10 and formula_11 (say)

Here we are "marginalizing out" the independent variables (formula_12, formula_13, and formula_14) to obtain the result. When we are calculating the computational complexity, we can see that for each formula_15 pairs of formula_16, there are formula_17 terms due to the triplet formula_18 which needs to take part in the evaluation of formula_19 with each step having one addition and one multiplication. Therefore, the total number of computations needed is formula_20. Hence the asymptotic complexity of the above function is formula_21.

If we apply the distributive law to the RHS of the equation, we get the following:

This implies that formula_23 can be described as a product formula_24 where formula_25 and formula_26

Now, when we are calculating the computational complexity, we can see that there are formula_17 additions in formula_28 and formula_29 each and there are formula_30 multiplications when we are using the product formula_24 to evaluate formula_23. Therefore, the total number of computations needed is formula_33. Hence the asymptotic complexity of calculating formula_34 reduces to formula_35 from formula_36. This shows by an example that applying distributive law reduces the computational complexity which is one of the good features of a "fast algorithm".

Some of the problems that used distributive law to solve can be grouped as follows

1. Decoding algorithms<br>
A GDL like algorithm was used by Gallager's for decoding low density parity-check codes. Based on Gallager's work Tanner introduced the Tanner graph and expressed Gallagers work in message passing form. The tanners graph also helped explain the Viterbi algorithm.

It is observed by Forney that Viterbi's maximum likelihood decoding of convolutional codes also used algorithms of GDL-like generality.

2. Forward-backward algorithm<br>
The forward backward algorithm helped as an algorithm for tracking the states in the markov chain. And this also was used the algorithm of GDL like generality

3. Artificial intelligence<br>
The notion of junction trees has been used to solve many problems in AI. Also the concept of bucket elimination used many of the concepts.

MPF or marginalize a product function is a general computational problem which as special case includes many classical problems such as computation of discrete Hadamard transform, maximum likelihood decoding of a linear code over a memory-less channel, and matrix chain multiplication. The power of the GDL lies in the fact that it applies to situations in which additions and multiplications are generalized.
A commutative semiring is a good framework for explaining this behavior. It is defined over a set formula_37 with operators "formula_38" and "formula_39" where formula_40 and formula_41 are a commutative monoids and the distributive law holds.

Let formula_42 be variables such that formula_43 where formula_44 is a finite set and formula_45. Here formula_46. If formula_47 and formula_48, let
formula_49,
formula_50, 
formula_51, 
formula_52, and
formula_53

Let formula_54 where formula_55. Suppose a function is defined as formula_56, where formula_57 is a commutative semiring. Also, formula_58 are named the "local domains" and formula_59 as the "local kernels".

Now the global kernel formula_60 is defined as :
formula_61

"Definition of MPF problem": For one or more indices formula_62, compute a table of the values of formula_63-"marginalization" of the global kernel formula_64, which is the function formula_65 defined as formula_66

Here formula_67 is the complement of formula_63 with respect to formula_69 and the formula_70 is called the formula_71 "objective function", or the "objective function" at formula_72. It can observed that the computation of the formula_71 objective function in the obvious way needs formula_74 operations. This is because there are formula_75 additions and formula_76 multiplications needed in the computation of the formula_77 objective function. The GDL algorithm which is explained in the next section can reduce this computational complexity.

The following is an example of the MPF problem. 
Let formula_78 and formula_79 be variables such that formula_80 and formula_81. Here formula_82 and formula_83. The given functions using these variables are formula_84 and formula_85 and we need to calculate formula_86 and formula_87 defined as:

Here local domains and local kernels are defined as follows: 
where formula_90 is the formula_91 objective function and formula_87 is the formula_93 objective function.

Consider another example where formula_94 and formula_95 is a real valued function. Now, we shall consider the MPF problem where the commutative semiring is defined as the set of real numbers with ordinary addition and multiplication and the local domains and local kernels are defined as follows:

Now since the global kernel is defined as the product of the local kernels, it is

and the objective function at the local domain formula_97 is

This is the Hadamard transform of the function formula_8. Hence we can see that the computation of Hadamard transform is a special case of the MPF problem. More examples can be demonstrated to prove that the MPF problem forms special cases of many classical problem as explained above whose details can be found at

If one can find a relationship among the elements of a given set formula_100, then one can solve the MPF problem basing on the notion of belief propagation which is a special use of "message passing" technique. The required relationship is that the given set of local domains can be organised into a junction tree. In other words, we create a graph theoretic tree with the elements of formula_100 as the vertices of the tree formula_102, such that for any two arbitrary vertices say formula_103 and formula_104 where formula_105 and there exists an edge between these two vertices, then the intersection of corresponding labels, viz formula_106, is a subset of the label on each vertex on the unique path from formula_103 to formula_104.

For example,

Example 1: Consider the following nine local domains:


For the above given set of local domains, one can organize them into a junction tree as shown below:

Similarly If another set like the following is given

Example 2: Consider the following four local domains:


Then constructing the tree only with these local domains is not possible since this set of values has no common domains which can be placed between any two values of the above set. But however if add the two dummy domains as shown below then organizing the updated set into a junction tree would be possible and easy too.

5.formula_122,formula_123
6.formula_124,formula_123

Similarly for these set of domains, the junction tree looks like shown below:
Input: A set of local domains.
Output: For the given set of domains, possible minimum number of operations that is required to solve the problem is computed. 
So, if formula_103 and formula_104 are connected by an edge in the junction tree, then a message from formula_103 to formula_104 is a set/table of values given by a function: formula_130:formula_131. To begin with all the functions i.e. for all combinations of formula_132 and formula_133 in the given tree, formula_130 is defined to be identically formula_135 and when a particular message is update, it follows the equation given below.

where formula_138 means that formula_139 is an adjacent vertex to formula_103 in tree.

Similarly each vertex has a state which is defined as a table containing the values from the function formula_141, Just like how messages initialize to 1 identically, state of formula_103 is defined to be local kernel formula_143, but whenever formula_144 gets updated, it follows the following equation:

For the given set of local domains as input, we find out if we can create a junction tree, either by using the set directly or by adding dummy domains to the set first and then creating the junction tree, if construction junction is not possible then algorithm output that there is no way to reduce the number of steps to compute the given equation problem, but once we have junction tree, algorithm will have to schedule messages and compute states, by doing these we can know where steps can be reduced, hence will be discusses this below.

There are two special cases we are going to talk about here namely "Single Vertex Problem" in which the objective function is computed at only one vertex formula_146 and the second one is "All Vertices Problem" where the goal is to compute the objective function at all vertices.

Lets begin with the single-vertex problem, GDL will start by directing each edge towards the targeted vertex formula_147. Here messages are sent only in the direction towards the targeted vertex. Note that all the directed messages are sent only once. The messages are started from the leaf nodes(where the degree is 1) go up towards the target vertex formula_147. The message travels from the leaves to its parents and then from there to their parents and so on until it reaches the target vertex formula_147. The target vertex formula_147 will compute its state only when it receives all messages from all its neighbors. Once we have the state, We have got the answer and hence the algorithm terminates.

For Example, Lets consider a junction tree constructed from the set of local domains given above i.e. the set from example 1, Now the Scheduling table for these domains is (where the target vertex is formula_151).

formula_152
formula_153
formula_154
formula_155
formula_156
formula_157
formula_158
formula_159
formula_160
formula_161

Thus the complexity for Single Vertex GDL can be shown as

formula_162 arithmetic operations
Where (Note: The explanation for the above equation is explained later in the article )
formula_163 is the label of formula_164.
formula_165 is the degree of formula_164 (i.e. number of vertices adjacent to v).

To solve the All-Vertices problem, we can schedule GDL in several ways, some of them are parallel implementation where in each round, every state is updated and every message is computed and transmitted at the same time. In this type of implementation the states and messages will stabilizes after number of rounds that is at most equal to the diameter of the tree. At this point all the all states of the vertices will be equal to the desired objective function.

Another way to schedule GDL for this problem is serial implementation where its similar to the Single vertex problem except that we don't stop the algorithm until all the vertices of a required set have not got all the messages from all their neighbors and have compute their state. 
Thus the number of arithmetic this implementation requires is at most formula_167 arithmetic operations.

The key to constructing a junction tree lies in the local domain graph formula_168, which is a weighted complete graph with formula_169 vertices formula_170 i.e. one for each local domain, having the weight of the edge formula_171 defined by
formula_172.
if formula_173, then we say formula_174 is contained informula_175. Denoted by formula_176 (the weight of a maximal-weight spanning tree of formula_168), which is defined by

where "n" is the number of elements in that set. For more clarity and details, please refer to these.

Let formula_179 be a junction tree with vertex set formula_180 and edge set formula_181. In this algorithm, the messages are sent in both the direction on any edge, so we can say/regard the edge set E as set of ordered pairs of vertices. For example, from Figure 1 formula_181 can be defined as follows

NOTE:formula_184 above gives you all the possible directions that a message can travel in the tree.

The schedule for the GDL is defined as a finite sequence of subsets offormula_184. Which is generally represented by 
formula_186{formula_187}, Where formula_188 is the set of messages updated during the formula_189 round of running the algorithm.

Having defined/seen some notations, we will see want the theorem says,
When we are given a schedule formula_190, the corresponding message trellis as a finite directed graph with Vertex set of formula_191, in which a typical element is denoted by formula_192 for formula_193, Then after completion of the message passing, state at vertex formula_104 will be the formula_195 objective defined in

and <nowiki>iff</nowiki> there is a path from formula_197 to formula_198

Here we try to explain the complexity of solving the MPF problem in terms of the number of mathematical operations required for the calculation. i.e. We compare the number of operations required when calculated using the normal method (Here by normal method we mean by methods that do not use message passing or junction trees in short methods that do not use the concepts of GDL)and the number of operations using the generalized distributive law.

Example: Consider the simplest case where we need to compute the following expression formula_199.

To evaluate this expression naively requires two multiplications and one addition. The expression when expressed using the distributive law can be written as formula_200 a simple optimization that reduces the number of operations to one addition and one multiplication.

Similar to the above explained example we will be expressing the equations in different forms to perform as few operation as possible by applying the GDL.

As explained in the previous sections we solve the problem by using the concept of the junction trees. The optimization obtained by the use of these trees is comparable to the optimization obtained by solving a semi group problem on trees. For example, to find the minimum of a group of numbers we can observe that if we have a tree and the elements are all at the bottom of the tree, then we can compare the minimum of two items in parallel and the resultant minimum will be written to the parent. When this process is propagated up the tree the minimum of the group of elements will be found at the root.

The following is the complexity for solving the junction tree using message passing

We rewrite the formula used earlier to the following form. This is the eqn for a message to be sent from vertex "v" to "w"

Similarly we rewrite the equation for calculating the state of vertex v as follows

We first will analyze for the single-vertex problem and assume the target vertex is formula_147 and hence we have one edge from formula_164 to formula_205. 
Suppose we have an edge formula_206 we calculate the message using the message equation. To calculate formula_207 requires

additions and

multiplications.

But there will be many possibilities for formula_212 hence 
formula_213 possibilities for formula_214.
Thus the entire message will need

additions and

multiplications

The total number of arithmetic operations required to send a message towards formula_217along the edges of tree will be

additions and

multiplications.

Once all the messages have been transmitted the algorithm terminates with the computation of state at formula_147 The state computation requires formula_221 more multiplications.
Thus number of calculations required to calculate the state is given as below

additions and

multiplications

Thus the grand total of the number of calculations is

where formula_226 is an edge and its size is defined by formula_227

The formula above gives us the upper bound.

If we define the complexity of the edge formula_226 as

Therefore, formula_225 can be written as

We now calculate the edge complexity for the problem defined in Figure 1 as follows

The total complexity will be formula_240 which is considerably low compared to the direct method. (Here by direct method we mean by methods that do not use message passing. The time taken using the direct method will be the equivalent to calculating message at each node and time to calculate the state of each of the nodes.)

Now we consider the all-vertex problem where the message will have to be sent in both the directions and state must be computed at both the vertexes. This would take formula_241 but by precomputing we can reduce the number of multiplications to formula_242. Here formula_13 is the degree of the vertex. Ex : If there is a set formula_244 with formula_245 numbers. It is possible to compute all the d products of formula_246 of the formula_247 with at most formula_242 multiplications rather than the obvious formula_249. 
We do this by precomputing the quantities 
formula_250 and formula_251 this takes formula_252 multiplications. Then if formula_253 denotes the product of all formula_254 except for formula_255 we have formula_256 and so on will need another formula_257 multiplications making the total formula_258

There is not much we can do when it comes to the construction of the junction tree except that we may have many maximal weight spanning tree and we should choose the spanning tree with the least formula_259 and sometimes this might mean adding a local domain to lower the junction tree complexity.

It may seem that GDL is correct only when the local domains can be expressed as a junction tree. But even in cases where there are cycles and a number of iterations the messages will approximately be equal to the objective function. The experiments on Gallager–Tanner–Wiberg algorithm for low density parity-check codes were supportive of this claim.


</doc>
<doc id="36033877" url="https://en.wikipedia.org/wiki?curid=36033877" title="Bisection (software engineering)">
Bisection (software engineering)

Bisection is a method used in software development to identify change sets that result in a specific behavior change. It is mostly employed for finding the patch that introduced a bug. Another application area is finding the patch that indirectly fixed a bug.

The process of locating the changeset that introduced a specific regression was described as "source change isolation" in 1997 by Brian Ness and Viet Ngo of Cray Research. Regression testing was performed on Cray's compilers in editions comprising one or more changesets. Editions with known regressions could not be validated until developers addressed the problem. Source change isolation narrowed the cause to a single changeset that could then be excluded from editions, unblocking them with respect to this problem, while the author of the change worked on a fix. Ness and Ngo outlined linear search and binary search methods of performing this isolation.

Code bisection has the goal of minimizing the effort to find a specific change set.
It employs a divide and conquer algorithm that
depends on having access to the code history which is usually preserved by
revision control in a code repository.

Code history has the structure of a directed acyclic graph which can be topologically sorted. This makes it possible to use a divide and conquer search algorithm which:

Bisection is in LSPACE having an algorithmic complexity of formula_1 with formula_2 denoting the number of revisions in the search space, and is similar to a binary search.

For code bisection it is desirable that each revision in the search space can be built and tested independently.

For the bisection algorithm to identify a single changeset which caused the behavior being tested to change, the behavior must change monotonically across the search space. For a Boolean function such as a pass/fail test, this means that it only changes once across all changesets between the start and end of the search space.

If there are multiple changesets across the search space where the behavior being tested changes between false and true, then the bisection algorithm will find one of them, but it will not necessarily be the root cause of the change in behavior between the start and the end of the search space. The root cause could be a different changeset, or a combination of two or more changesets across the search space. To help deal with this problem, automated tools allow specific changesets to be ignored during a bisection search.

Although the bisection method can be completed manually, one of its main advantages is that it can be easily automated. It can thus fit into existing test automation processes: failures in exhaustive automated regression tests can trigger automated bisection to localize faults. Ness and Ngo focused on its potential in Cray's continuous delivery-style environment in which the automatically-isolated bad changeset could be automatically excluded from builds.

The revision control systems Git and Mercurial have built-in functionality for code bisection. The user can start a bisection session with a specified range of revisions from which the revision control system proposes a revision to test, the user tells the system whether the revision tested as "good" or "bad", and the process repeats until the specific "bad" revision has been identified. Other revision control systems, such as Bazaar or Subversion, support bisection through plugins or external scripts.

Phoronix Test Suite can do bisection automatically to find performance regressions.



</doc>
<doc id="37606787" url="https://en.wikipedia.org/wiki?curid=37606787" title="Run to completion scheduling">
Run to completion scheduling

Run-to-completion scheduling or nonpreemptive scheduling is a scheduling model in which each task runs until it either finishes, or explicitly yields control back to the scheduler. Run to completion systems typically have an event queue which is serviced either in strict order of admission by an event loop, or by an admission scheduler which is capable of scheduling events out of order, based on other constraints such as deadlines.

Some preemptive multitasking scheduling systems behave as run-to-completion schedulers in regard to scheduling tasks at one particular process priority level, at the same time as those processes still preempt other lower priority tasks and are themselves preempted by higher priority tasks.



</doc>
<doc id="19759995" url="https://en.wikipedia.org/wiki?curid=19759995" title="Kinodynamic planning">
Kinodynamic planning

In robotics and motion planning, kinodynamic planning is a class of problems for which velocity, acceleration, and force/torque bounds must be satisfied, together with kinematic constraints such as avoiding obstacles. The term was coined by Bruce Donald, Pat Xavier, John Canny, and John Reif. Donald et al. developed the first polynomial-time approximation schemes (PTAS) for the problem. By providing a provably polynomial-time ε-approximation algorithm, they resolved a long-standing open problem in optimal control. Their first paper considered time-optimal control ("fastest path") of a point mass under Newtonian dynamics, amidst polygonal (2D) or polyhedral (3D) obstacles, subject to state bounds on position, velocity, and acceleration. Later they extended the technique to many other cases, for example, to 3D open-chain kinematic robots under full Lagrangian dynamics. More recently, many practical heuristic algorithms based on stochastic optimization and iterative sampling were developed, by a wide range of authors, to address the kinodynamic planning problem. These techniques for kinodynamic planning have been shown to work well in practice. However, none of these heuristic techniques can guarantee the optimality of the computed solution (i.e., they have no performance guarantees), and none can be mathematically proven to be faster than the original PTAS algorithms (i.e., none have a provably lower computational complexity).


</doc>
<doc id="17876651" url="https://en.wikipedia.org/wiki?curid=17876651" title="Predictor–corrector method">
Predictor–corrector method

In numerical analysis, predictor–corrector methods belong to a class of algorithms designed to integrate ordinary differential equationsto find an unknown function that satisfies a given differential equation. All such algorithms proceed in two steps: 


When considering the numerical solution of ordinary differential equations (ODEs), a predictor–corrector method typically uses an explicit method for the predictor step and an implicit method for the corrector step.

A simple predictor–corrector method (known as Heun's method) can be constructed from the Euler method (an explicit method) and the trapezoidal rule (an implicit method).

Consider the differential equation

and denote the step size by formula_2.

First, the predictor step: starting from the current value formula_3, calculate an initial guess value formula_4 via the Euler method,

Next, the corrector step: improve the initial guess using trapezoidal rule,

That value is used as the next step.

There are different variants of a predictor–corrector method, depending on how often the corrector method is applied. The Predict–Evaluate–Correct–Evaluate (PECE) mode refers to the variant in the above example:

It is also possible to evaluate the function "f" only once per step by using the method in Predict–Evaluate–Correct (PEC) mode:

Additionally, the corrector step can be repeated in the hope that this achieves an even better approximation to the true solution. If the corrector method is run twice, this yields the PECECE mode:

The PECEC mode has one fewer function evaluation than PECECE mode.

More generally, if the corrector is run "k" times, the method is in P(EC)
or P(EC)E mode. If the corrector method is iterated until it converges, this could be called PE(CE).





</doc>
<doc id="39226029" url="https://en.wikipedia.org/wiki?curid=39226029" title="HCS clustering algorithm">
HCS clustering algorithm

The HCS (Highly Connected Subgraphs) clustering algorithm (also known as the HCS algorithm, and other names such as Highly Connected Clusters/Components/Kernels) is an algorithm based on graph connectivity for cluster analysis. It works by representing the similarity data in a similarity graph, and then finding all the highly connected subgraphs. It does not make any prior assumptions on the number of the clusters. This algorithm was published by Erez Hartuv and Ron Shamir in 1998.

The HCS algorithm gives a clustering solution, which is inherently meaningful in the application domain, since each solution cluster must have diameter 2 while a union of two solution clusters will have diameter 3.

The goal of cluster analysis is to group elements into disjoint subsets, or clusters, based on similarity between elements, so that elements in the same cluster are highly similar to each other (homogeneity), while elements from different clusters have low similarity to each other (separation). Similarity graph is one of the models to represent the similarity between elements, and in turn facilitate generating of clusters. To construct a similarity graph from similarity data, represent elements as vertices, and elicit edges between vertices when the similarity value between them is above some threshold.

In the similarity graph, the more edges exist for a given number of vertices, the more similar such a set of vertices are between each other. In other words, if we try to disconnect a similarity graph by removing edges, the more edges we need to remove before the graph becomes disconnected, the more similar the vertices in this graph. Minimum cut is a minimum set of edges without which the graph will become disconnected.

HCS clustering algorithm finds all the subgraphs with n vertices such that the minimum cut of those subgraphs contain more than n/2 edges, and identifies them as clusters. Such a subgraph is called a Highly Connected Subgraph (HCS). Single vertices are not considered clusters and are grouped into a singletons set S.

Given a similarity graph G(V,E), HCS clustering algorithm will check if it is already highly connected, if yes, returns G, otherwise uses the minimum cut of G to partition G into two subgraphs H and H', and recursively run HCS clustering algorithm on H and H'.

The following animation shows how the HCS clustering algorithm partitions a similarity graph into three clusters.

 function HCS(G(V, E)) is

The step of finding the minimum cut on graph is a subroutine that can be implemented using different algorithms for this problem. See below for an example algorithm for finding minimum cut using randomization.

The running time of the HCS clustering algorithm is bounded by × f(n, m). f(n, m) is the time complexity of computing a minimum cut in a graph with n vertices and m edges, and is the number of clusters found. In many applications N « n.

For fast algorithms for finding a minimum cut in an unweighted graph: 

The clusters produced by the HCS clustering algorithm possess several properties, which can demonstrate the homogeneity and the separation of the solution.

Theorem 1 The diameter of every highly connect graph is at most two.

"Proof:" We know the edges of minimum cut must be greater or equal than the minimum degree of the graph. If the graph G is highly connected, then the edges of the minimum cut must be greater than the number of vertices divided by 2. So the degree of vertices in the highly connected graph G must be greater than half the vertices. Therefore, for any two vertices in this graph G, there must be at least one common neighbor, as the distance between them is two.

Theorem 2 (a) The number of edges in a highly connected subgraph is quadratic. (b) The number of edges removed by each iteration of the HCS algorithm is at most linear.

"Proof:" (For a) From Theorem 1 we know every vertex must have more than half of the total vertices as neighbors. Therefore, the total number of edges in a highly connect subgraph must be at least (/2) × n × 1/2, where we sum all the degrees of each vertex and divide by 2.

(For b) Each iteration HCS algorithm will separate a graph containing n vertices into two subgraphs, so the number of edges between those two components is at most /2.

Theorem 1 and 2a provide a strong indication to the homogeneity, as the only better possibility in terms of the diameter is that every two vertices of a cluster are connected by an edge, which is both too stringent and also a NP-hard problem.

Theorem 2b also indicates separation since the number of edges removed by each iteration of the HCS algorithm is at most linear in the size of the underlying subgraph, contrast to the quadratic number of edges within final clusters.

Singletons adoption: Elements left as singletons by the initial clustering process can be "adopted" by clusters based on similarity to the cluster. If the maximum number of neighbors to a specific cluster is large enough, then it can be added to that cluster.

Removing Low Degree Vertices: When the input graph has vertices with low degrees, it is not worthy to run the algorithm since it is computationally expensive and not informative. Alternatively, a refinement of the algorithm can first remove all vertices with a degree lower than certain threshold.





</doc>
<doc id="39456471" url="https://en.wikipedia.org/wiki?curid=39456471" title="Driver scheduling problem">
Driver scheduling problem

The driver scheduling problem (DSP) is type of problem in operations research and theoretical computer science.

The DSP consists of selecting a set of duties (assignments) for the drivers or pilots of vehicles (e.g., buses, trains, boats, or planes) involved in the transportation of passengers or goods.

This very complex problem involves several constraints related to labour and company rules and also different evaluation criteria and objectives. Being able to solve this problem efficiently can have a great impact on costs and quality of service for public transportation companies. There is a large number of different rules that a feasible duty might be required to satisfy, such as
Operations research has provided optimization models and algorithms that lead to efficient solutions for this problem. Among the most common models proposed to solve the DSP are the Set Covering and Set Partitioning Models (SPP/SCP). In the SPP model, each work piece (task) is covered by only one duty. In the SCP model, it is possible to have more than one duty covering a given work piece.
In both models, the set of work pieces that needs to be covered is laid out in rows, and the set of previously defined feasible duties available for covering specific work pieces is arranged in columns. The DSP resolution, based on either of these models, is the selection of the set of feasible duties that guarantees that there is one (SPP) or more (SCP) duties covering each work piece while minimizing the total cost of the final schedule.


</doc>
<doc id="39093307" url="https://en.wikipedia.org/wiki?curid=39093307" title="Chandy–Misra–Haas algorithm resource model">
Chandy–Misra–Haas algorithm resource model

The Chandy–Misra–Haas algorithm resource model checks for deadlock in a distributed system. It was developed by K. Mani Chandy, Jayadev Misra and Laura M Haas.

Consider the n processes "P", "P", "P", "P", "P", ... ,"P" which are performed in a single system (controller). "P" is locally dependent on "P", if "P" depends on "P", "P" on "P" so on and "P" on "P". That is, if formula_1, then formula_2 is locally dependent on formula_3. If "P" is said to be locally dependent to itself if it is locally dependent on "P" and "P" depends on "P": i.e. if formula_4, then formula_2 is locally dependent on itself.

The algorithm uses a message called probe(i,j,k) to transfer a message from controller of process "P" to controller of process "P". It specifies a message started by process "P" to find whether a deadlock has occurred or not. Every process "P" maintains a boolean array "dependent" which contains the information about the processes that depend on it. Initially the values of each array are all "false".

Before sending, the probe checks whether "P" is locally dependent on itself. If so, a deadlock occurs. Otherwise it checks whether "P", and "P" are in different controllers, are locally dependent and "P" is waiting for the resource that is locked by "P". Once all the conditions are satisfied it sends the probe.

On the receiving side, the controller checks whether "P" is performing a task. If so, it neglects the probe. Otherwise, it checks the responses given "P" to "P" and "dependent"(i) is false. Once it is verified, it assigns true to "dependent"(i). Then it checks whether k is equal to i. If both are equal, a deadlock occurs, otherwise it sends the probe to next dependent process.

In pseudocode, the algorithm works as follows:

 if "P" is locally dependent on itself

 if

"P" initiates deadlock detection. "C" sends the probe saying "P" depends on "P". Once the message is received by "C", it checks whether "P" is idle. "P" is idle because it is locally dependent on "P" and updates "dependent"(2) to True.

As above, "C" sends probe to "C" and "C" sends probe to "C". At "C", "P" is idle so it update "dependent"(1) to True. Therefore, deadlock can be declared.

Consider that there are "m" controllers and "p" process to perform, to declare whether a deadlock has occurred or not, the worst case for controllers and processes must be visited. Therefore, the solution is O(m+p). The time complexity is O(n).


</doc>
<doc id="40129720" url="https://en.wikipedia.org/wiki?curid=40129720" title="Devex algorithm">
Devex algorithm

In applied mathematics, the devex algorithm is a pivot rule for the simplex method developed by Paula M. J. Harris. It identifies the steepest-edge approximately in its search for the optimal solution.


</doc>
<doc id="40338559" url="https://en.wikipedia.org/wiki?curid=40338559" title="Hybrid algorithm">
Hybrid algorithm

A hybrid algorithm is an algorithm that combines two or more other algorithms that solve the same problem, either choosing one (depending on the data), or switching between them over the course of the algorithm. This is generally done to combine desired features of each, so that the overall algorithm is better than the individual components.

"Hybrid algorithm" does not refer to simply combining multiple algorithms to solve a different problem – many algorithms can be considered as combinations of simpler pieces – but only to combining algorithms that solve the same problem, but differ in other characteristics, notably performance.

In computer science, hybrid algorithms are very common in optimized real-world implementations of recursive algorithms, particularly implementations of 
divide and conquer or decrease and conquer algorithms, where the size of the data decreases as one moves deeper in the recursion. In this case, one algorithm is used for the overall approach (on large data), but deep in the recursion, it switches to a different algorithm, which is more efficient on small data. A common example is in sorting algorithms, where the insertion sort, which is inefficient on large data, but very efficient on small data (say, five to ten elements), is used as the final step, after primarily applying another algorithm, such as merge sort or quicksort. Merge sort and quicksort are asymptotically optimal on large data, but the overhead becomes significant if applying them to small data, hence the use of a different algorithm at the end of the recursion. A highly optimized hybrid sorting algorithm is Timsort, which combines merge sort, insertion sort, together with additional logic (including binary search) in the merging logic.

A general procedure for a simple hybrid recursive algorithm is "short-circuiting the base case," also known as "arm's-length recursion." In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking if it is null, checking null before recursing. This is useful for efficiency when the algorithm usually encounters the base case many times, as in many tree algorithms, but is otherwise considered poor style, particularly in academia, due to the added complexity.

Another example of hybrid algorithms for performance reasons are introsort and introselect, which combine one algorithm for fast average performance, falling back on another algorithm to ensure (asymptotically) optimal worst-case performance. Introsort begins with a quicksort, but switches to a heap sort if quicksort is not progressing well; analogously introselect begins with quickselect, but switches to median of medians if quickselect is not progressing well.

Centralized distributed algorithms can often be considered as hybrid algorithms, consisting of an individual algorithm (run on each distributed processor), and a combining algorithm (run on a centralized distributor) – these correspond respectively to running the entire algorithm on one processor, or running the entire computation on the distributor, combining trivial results (a one-element data set from each processor). A basic example of these algorithms are distribution sorts, particularly used for external sorting, which divide the data into separate subsets, sort the subsets, and then combine the subsets into totally sorted data; examples include bucket sort and flashsort.

However, in general distributed algorithms need not be hybrid algorithms, as individual algorithms or combining or communication algorithms may be solving different problems. For example, in models such as MapReduce, the Map and Reduce step solve different problems, and are combined to solve a different, third problem.



</doc>
<doc id="41026219" url="https://en.wikipedia.org/wiki?curid=41026219" title="Pointer jumping">
Pointer jumping

Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs. Pointer jumping allows an algorithm to follow paths with a time complexity that is logarithmic with respect to the length of the longest path. It does this by "jumping" to the end of the path computed by neighbors.

The basic operation of pointer jumping is to replace each neighbor in a pointer structure with its neighbor's neighbor. In each step of the algorithm, this replacement is done for all nodes in the data structure, which can be done independently in parallel. In the next step when a neighbor's neighbor is followed, the neighbor's path already followed in the previous step is added to the node's followed path in a single step. Thus, each step effectively doubles the distance traversed by the explored paths.

Pointer jumping is best understood by looking at simple examples such as list ranking and root finding.

One of the simpler tasks that can be solved by a pointer jumping algorithm is the "list ranking" problem. This problem is defined as follows: given a linked list of nodes, find the distance (measured in the number of nodes) of each node to the end of the list. The distance is defined as follows, for nodes that point to their successor by a pointer called :


This problem can easily be solved in linear time on a sequential machine, but a parallel algorithm can do better: given processors, the problem can be solved in logarithmic time, , by the following pointer jumping algorithm:

The pointer jumping occurs in the last line of the algorithm, where each node's pointer is reset to skip the node's direct successor. It is assumed, as in common in the PRAM model of computation, that memory access are performed in lock-step, so that each memory fetch is performed before each memory store; otherwise, processors may clobber each other's data, producing inconsistencies.

The following diagram follows how the parallel list ranking algorithm uses pointer jumping for a linked list with 11 elements. As the algorithm describes, the first iteration starts initialized with all ranks set to 1 except those with a null pointer for . The first iteration looks at immediate neighbors. Each subsequent iteration jumps twice as far as the previous.

Analyzing the algorithm yields a logarithmic running time. The initialization loop takes constant time, because each of the processors performs a constant amount of work, all in parallel. The inner loop of the main loop also takes constant time, as does (by assumption) the termination check for the loop, so the running time is determined by how often this inner loop is executed. Since the pointer jumping in each iteration splits the list into two parts, one consisting of the "odd" elements and one of the "even" elements, the length of the list pointed to by each processor's is halved in each iteration, which can be done at most time before each list has a length of at most one.

Following a path in a graph is an inherently serial operation, but pointer jumping reduces the total amount of work by following all paths simultaneously and sharing results among dependent operations. Pointer jumping iterates and finds a "successor" — a vertex closer to the tree root — each time. By following successors computed for other vertices, the traversal down each path can be doubled every iteration, which means that the tree roots can be found in logarithmic time.

Pointer doubling operates on an array successor with an entry for every vertex in the graph. Each successor["i"] is initialized with the parent index of vertex "i" if that vertex is not a root or to "i" itself if that vertex is a root. At each iteration, each successor is updated to its successor's successor. The root is found when the successor's successor points to itself.

The following pseudocode demonstrates the algorithm.

The following image provides an example of using pointer jumping on a small forest. On each iteration the successor points to the vertex following one more successor. After two iterations, every vertex points to its root node.

Although the name pointer jumping would come later, JáJá attributes the first uses of the technique in early parallel graph algorithms and list ranking. The technique has been described with other names such as shortcutting, but by the 1990s textbooks on parallel algorithms consistently used the term pointer jumping. Today, pointer jumping is considered a software design pattern for operating on recursive data types in parallel.

As a technique for following linked paths, graph algorithms are a natural fit for pointer jumping. Consequently, several parallel graph algorithms utilizing pointer jumping have been designed. These include algorithms for finding the roots of a forest of rooted trees, connected components, minimum spanning trees, and biconnected components. However, pointer jumping has also shown to be useful in a variety of other problems including computer vision, image compression, and Bayesian inference.


</doc>
<doc id="40543215" url="https://en.wikipedia.org/wiki?curid=40543215" title="Rendezvous hashing">
Rendezvous hashing

Rendezvous or highest random weight (HRW) hashing is an algorithm that allows clients to achieve distributed agreement on a set of formula_1 options out of a possible set of formula_2 options. A typical application is when clients need to agree on which sites (or proxies) objects are assigned to.

Rendezvous hashing is more general than consistent hashing, which becomes a special case (for formula_3) of rendezvous hashing.

Rendezvous hashing was invented by David Thaler and Chinya Ravishankar at the University of Michigan in 1996. Consistent hashing appeared a year later in the literature. One of the first applications of rendezvous hashing was to enable multicast clients on the Internet (in contexts such as the MBONE) to identify multicast rendezvous points in a distributed fashion. It was used in 1998 by Microsoft's Cache Array Routing Protocol (CARP) for distributed cache coordination and routing. Some Protocol Independent Multicast routing protocols use rendezvous hashing to pick a rendezvous point.

Given its simplicity and generality, rendezvous hashing has been applied in a wide variety of applications, including mobile caching, router design, secure key establishment, and sharding and distributed databases.

Rendezvous hashing solves the distributed hash table problem: How can a set of clients, given an object formula_4, agree on where in a set of formula_2 sites (servers, say) to place formula_4? Each client is to select a site independently, but all clients must end up picking the same site. This is non-trivial if we add a "minimal disruption" constraint, and require that only objects mapping to a removed site may be reassigned to other sites.

The basic idea is to give each site formula_7 a score (a "weight") for each object formula_8, and assign the object to the highest scoring site. All clients first agree on a hash function formula_9. For object formula_8, the site formula_7 is defined to have weight formula_12. HRW assigns formula_8 to the site formula_14 whose weight formula_15 is the largest. Since formula_9 is agreed upon, each client can independently compute the weights formula_17 and pick the largest. If the goal is distributed formula_1-agreement, the clients can independently pick the sites with the formula_1 largest hash values.

If a site formula_20 is added or removed, only the objects mapping to formula_20 are remapped to different sites, satisfying the minimal disruption constraint above. The HRW assignment can be computed independently by any client, since it depends only on the identifiers for the set of sites formula_22 and the object being assigned.

HRW easily accommodates different capacities among sites. If site formula_23 has twice the capacity of the other sites, we simply represent formula_23 twice in the list, say, as formula_25. Clearly, twice as many objects will now map to formula_23 as to the other sites.

It might first appear sufficient to treat the "n" sites as buckets in a hash table and hash the object name "O" into this table. However, if any of the sites fails or is unreachable, the hash table size changes, requiring all objects to be remapped. This massive disruption makes such direct hashing unworkable. Under rendezvous hashing, however, clients handle site failures by picking the site that yields the next largest weight. Remapping is required only for objects currently mapped to the failed site, and disruption is minimal.

Rendezvous hashing has the following properties:


Consistent hashing operates by mapping sites uniformly and randomly to points on a unit circle called tokens. Objects are also mapped to the unit circle and placed in the site whose token is the first encountered traveling clockwise from the object's location. When a site is removed, the objects it owns are transferred to the site owning the next token encountered moving clockwise. Provided each site is mapped to a large number (100–200, say) of tokens this will reassign objects in a relatively uniform fashion among the remaining sites.

If sites are mapped to points on the circle randomly by hashing 200 variants of the site ID, say, the assignment of any object requires storing or recalculating 200 hash values for each site. However, the tokens associated with a given site can be precomputed and stored in a sorted list, requiring only a single application of the hash function to the object, and a binary search to compute the assignment. Even with many tokens per site, however, the basic version of consistent hashing may not balance objects uniformly over sites, since when a site is removed each object assigned to it is distributed only over as many other sites as the site has tokens (say 100–200).

Variants of consistent hashing (such as Amazon's Dynamo) that use more complex logic to distribute tokens on the unit circle offer better load balancing than basic consistent hashing, reduce the overhead of adding new sites, and reduce metadata overhead and offer other benefits.

In contrast, rendezvous hashing (HRW) is much simpler conceptually and in practice. It also distributes objects uniformly over all sites, given a uniform hash function. Unlike consistent hashing, HRW requires no precomputing or storage of tokens. An object formula_8 is placed into one of formula_2 sites formula_22 by computing the formula_2 hash values formula_31 and picking the site formula_23 that yields the highest hash value. If a new site formula_33 is added, new object placements or requests will compute formula_34 hash values, and pick the largest of these. If an object already in the system at formula_23 maps to this new site formula_33, it will be fetched afresh and cached at formula_33. All clients will henceforth obtain it from this site, and the old cached copy at formula_38 will ultimately be replaced by the local cache management algorithm. If formula_23 is taken offline, its objects will be remapped uniformly to the remaining formula_40 sites.

Variants of the HRW algorithm, such as the use of a skeleton (see below), can reduce the formula_41 time for object location to formula_42, at the cost of less global uniformity of placement. When formula_2 is not too large, however, the formula_41 placement cost of basic HRW is not likely to be a problem. HRW completely avoids all the overhead and complexity associated with correctly handling multiple tokens for each site and associated metadata.

Rendezvous hashing also has the great advantage that it provides simple solutions to other important problems, such as distributed formula_1-agreement.

Consistent hashing can be reduced to an instance of HRW by an appropriate choice of a two-place hash function. From the site identifier formula_20 the simplest version of consistent hashing computes a list of token positions, e.g., formula_47 where formula_48 hashes values to locations on the unit circle. Define the two place hash function formula_49 to be formula_50 where formula_51 denotes the distance along the unit circle from formula_52 to formula_53 (since formula_51 has some minimal non-zero value there is no problem translating this value to a unique integer in some bounded range). This will duplicate exactly the assignment produced by consistent hashing.

It is not possible, however, to reduce HRW to consistent hashing (assuming the number of tokens per site is bounded), since HRW potentially reassigns the objects from a removed site to an unbounded number of other sites.

In the standard implementation of rendezvous hashing, every node receives a statically equal proportion of the keys. This behavior, however, is undesirable when the nodes have different capacities for processing or holding their assigned keys. For example, if one of the nodes had twice the storage capacity as the others, it would be beneficial if the algorithm could take this into account such that this more powerful node would receive twice the number of keys as each of the others.

A straightforward mechanism to handle this case is to assign two virtual locations to this node, so that if either of that larger node's virtual locations has the highest hash, that node receives the key. But this strategy does not work when the relative weights are not integer multiples. For example, if one node had 42% more storage capacity, it would require adding many virtual nodes in different proportions, leading to greatly reduced performance. Several modifications to rendezvous hashing have been proposed to overcome this limitation.

The Cache Array Routing Protocol (CARP) is a 1998 IETF draft that describes a method for computing "load factors" which can be multiplied by each node's hash score to yield an arbitrary level of precision for weighting nodes differently. However, one disadvantage of this approach is that when any node's weight is changed, or when any node is added or removed, all the load factors must be re-computed and relatively scaled. When the load factors change relative to one another, it triggers movement of keys between nodes whose weight was not changed, but whose load factor did change relative to other nodes in the system. This results in excess movement of keys.

Controlled replication under scalable hashing or CRUSH is an extension to RUSH that improves upon rendezvous hashing by constructing a tree where a pseudo-random function (hash) is used to navigate down the tree to find which node is ultimately responsible for a given key. It permits perfect stability for adding nodes however it is not perfectly stable when removing or re-weighting nodes, with the excess movement of keys being proportional to the height of the tree.

The CRUSH algorithm is used by the ceph data storage system to map data objects to the nodes responsible for storing them.

When formula_2 is extremely large, a skeleton-based variant can improve running time. This approach creates a virtual hierarchical structure (called a "skeleton"), and achieves formula_42 running time by applying HRW at each level while descending the hierarchy. The idea is to first choose some constant formula_57 and organize the formula_2 sites into formula_59 clusters formula_60 Next, build a virtual hierarchy by choosing a constant formula_61 and imagining these formula_62 clusters placed at the leaves of a tree formula_63 of virtual nodes, each with fanout formula_64.

In the accompanying diagram, the cluster size is formula_65, and the skeleton fanout is formula_66. Assuming 108 sites (real nodes) for convenience, we get a three-tier virtual hierarchy. Since formula_66, each virtual node has a natural numbering in octal. Thus, the 27 virtual nodes at the lowest tier would be numbered formula_68 in octal (we can, of course, vary the fanout at each level - in that case, each node will be identified with the corresponding mixed-radix number).

Instead of applying HRW to all 108 real nodes, we can first apply HRW to the 27 lowest-tier virtual nodes, selecting one. We then apply HRW to the four real nodes in its cluster, and choose the winning site. We only need formula_69 hashes, rather than 108. If we apply this method starting one level higher in the hierarchy, we would need formula_70 hashes to get to the winning site. The figure shows how, if we proceed starting from the root of the skeleton, we may successively choose the virtual nodes formula_71, formula_72, and formula_73, and finally end up with site 74.

We can start at any level in the virtual hierarchy, not just at the root. Starting lower in the hierarchy requires more hashes, but may improve load distribution in the case of failures. Also, the virtual hierarchy need not be stored, but can be created on demand, since the virtual nodes names are simply prefixes of base-formula_64 (or mixed-radix) representations. We can easily create appropriately sorted strings from the digits, as required. In the example, we would be working with the strings formula_75 (at tier 1), formula_76 (at tier 2), and formula_77 (at tier 3). Clearly, formula_63 has height formula_79, since formula_57 and formula_64 are both constants. The work done at each level is formula_82, since formula_64 is a constant.

For any given object, it is clear that the method chooses each cluster, and hence each of the formula_2 sites, with equal probability. If the site finally selected is unavailable, we can select a different site within the same cluster, in the usual manner. Alternatively, we could go up one or more tiers in the skeleton and select an alternate from among the sibling virtual nodes at that tier, and once again descend the hierarchy to the real nodes, as above.

The value of formula_57 can be chosen based on factors like the anticipated failure rate and the degree of desired load balancing. A higher value of formula_57 leads to less load skew in the event of failure at the cost of higher search overhead.

The choice formula_87 is equivalent to non-hierarchical rendezvous hashing. In practice, the hash function formula_9 is very cheap, so formula_89 can work quite well unless formula_2 is very high.

In 2005, Christian Schindelhauer and Gunnar Schomaker described a logarithmic method for re-weighting hash scores in a way that does not require relative scaling of load factors when a node's weight changes or when nodes are added or removed. This enabled the dual benefits of perfect precision when weighting nodes, along with perfect stability, as only a minimum number of keys needed to be remapped to new nodes.

A similar logarithm-based hashing strategy is used to assign data to storage nodes in Cleversafe's data storage system, now IBM Cloud Object Storage.

Implementation is straightforward once a hash function formula_9 is chosen (the original work on the HRW method makes a hash function recommendation). Each client only needs to compute a hash value for each of the formula_2 sites, and then pick the largest. This algorithm runs in formula_41 time. If the hash function is efficient, the formula_41 running time is not a problem unless formula_2 is very large.

Python code implementing a weighted rendezvous hash:

Example outputs of WRH:



</doc>
<doc id="22074859" url="https://en.wikipedia.org/wiki?curid=22074859" title="Maze solving algorithm">
Maze solving algorithm

There are a number of different maze solving algorithms, that is, automated methods for the solving of mazes. The random mouse, wall follower, Pledge, and Trémaux's algorithms are designed to be used inside the maze by a traveler with no prior knowledge of the maze, whereas the dead-end filling and shortest path algorithms are designed to be used by a person or computer program that can see the whole maze at once.

Mazes containing no loops are known as "simply connected", or "perfect" mazes, and are equivalent to a "tree" in graph theory. Thus many maze solving algorithms are closely related to graph theory. Intuitively, if one pulled and stretched out the paths in the maze in the proper way, the result could be made to resemble a tree.

This is a trivial method that can be implemented by a very unintelligent robot or perhaps a mouse. It is simply to proceed following the current passage until a junction is reached, and then to make a random decision about the next direction to follow. Although such a method would always eventually find the right solution, this algorithm can be extremely slow.

The best-known rule for traversing mazes is the "wall follower", also known as either the "left-hand rule" or the "right-hand rule". If the maze is "simply connected", that is, all its walls are connected together or to the maze's outer boundary, then by keeping one hand in contact with one wall of the maze the solver is guaranteed not to get lost and will reach a different exit if there is one; otherwise, the algorithm will return to the entrance having traversed every corridor next to that connected section of walls at least once.

Another perspective into why wall following works is topological. If the walls are connected, then they may be deformed into a loop or circle. Then wall following reduces to walking around a circle from start to finish. To further this idea, notice that by grouping together connected components of the maze walls, the boundaries between these are precisely the solutions, even if there is more than one solution (see figures on the right).

If the maze is not simply-connected (i.e. if the start or endpoints are in the center of the structure surrounded by passage loops, or the pathways cross over and under each other and such parts of the solution path are surrounded by passage loops), this method will not reach the goal.

Another concern is that care should be taken to begin wall-following at the entrance to the maze. If the maze is not simply-connected and one begins wall-following at an arbitrary point inside the maze, one could find themselves trapped along a separate wall that loops around on itself and containing no entrances or exits. Should it be the case that wall-following begins late, attempt to mark the position in which wall-following began. Because wall-following will always lead you back to where you started, if you come across your starting point a second time, you can conclude the maze is not simply-connected, and you should switch to an alternative wall not yet followed. See the "Pledge Algorithm", below, for an alternative methodology.

Wall-following can be done in 3D or higher-dimensional mazes if its higher-dimensional passages can be projected onto the 2D plane in a deterministic manner. For example, if in a 3D maze "up" passages can be assumed to lead Northwest, and "down" passages can be assumed to lead southeast, then standard wall following rules can apply. However, unlike in 2D, this requires that the current orientation is known, to determine which direction is the first on the left or right.

Disjoint mazes can be solved with the wall follower method, so long as the entrance and exit to the maze are on the outer walls of the maze. If however, the solver starts inside the maze, it might be on a section disjoint from the exit, and wall followers will continually go around their ring. The Pledge algorithm (named after Jon Pledge of Exeter) can solve this problem.

The Pledge algorithm, designed to circumvent obstacles, requires an arbitrarily chosen direction to go toward, which will be preferential. When an obstacle is met, one hand (say the right hand) is kept along the obstacle while the angles turned are counted (clockwise turn is positive, counter-clockwise turn is negative). When the solver is facing the original preferential direction again, and the angular sum of the turns made is 0, the solver leaves the obstacle and continues moving in its original direction.

The hand is removed from the wall only when both "sum of turns made" and "current heading" are at zero. This allows the algorithm to avoid traps shaped like an upper case letter "G". Assuming the algorithm turns left at the first wall, one gets turned around a full 360 degrees by the walls. An algorithm that only keeps track of "current heading" leads into an infinite loop as it leaves the lower rightmost wall heading left and runs into the curved section on the left hand side again. The Pledge algorithm does not leave the rightmost wall due to the "sum of turns made" not being zero at that point (note 360 degrees is not equal to 0 degrees). It follows the wall all the way around, finally leaving it heading left outside and just underneath the letter shape.

This algorithm allows a person with a compass to find their way from any point inside to an outer exit of any finite two-dimensional maze, regardless of the initial position of the solver. However, this algorithm will not work in doing the reverse, namely finding the way from an entrance on the outside of a maze to some end goal within it.

Trémaux's algorithm, invented by Charles Pierre Trémaux, is an efficient method to find the way out of a maze that requires drawing lines on the floor to mark a path, and is guaranteed to work for all mazes that have well-defined passages, but it is not guaranteed to find the shortest route.

A path from a junction is either unvisited, marked once or marked twice. The algorithm works according to the following rules:

The "turn around and return" rule effectively transforms any maze with loops into a simply connected one; whenever we find a path that would close a loop, we treat it as a dead end and return. Without this rule, it is possible to cut off one's access to still-unexplored parts of a maze if, instead of turning back, we arbitrarily follow another path.

When you finally reach the solution, paths marked exactly once will indicate a way back to the start. If there is no exit, this method will take you back to the start where all paths are marked twice.
In this case each path is walked down exactly twice, once in each direction. The resulting walk is called a bidirectional double-tracing.

Essentially, this algorithm, which was discovered in the 19th century, has been used about a hundred years later as depth-first search.

Dead-end filling is an algorithm for solving mazes that fills all dead ends, leaving only the correct ways unfilled. It can be used for solving mazes on paper or with a computer program, but it is not useful to a person inside an unknown maze since this method looks at the entire maze at once. The method is to 1) find all of the dead-ends in the maze, and then 2) "fill in" the path from each dead-end until the first junction is met. Note that some passages won't become parts of dead end passages until other dead ends are removed first. A video of dead-end filling in action can be seen here: .

Dead-end filling cannot accidentally "cut off" the start from the finish since each step of the process preserves the topology of the maze. Furthermore, the process won't stop "too soon" since the end result cannot contain any dead-ends. Thus if dead-end filling is done on a perfect maze (maze with no loops), then only the solution will remain. If it is done on a partially braid maze (maze with some loops), then every possible solution will remain but nothing more. 

If given an omniscient view of the maze, a simple recursive algorithm can tell one how to get to the end. The algorithm will be given a starting X and Y value. If the X and Y values are not on a wall, the method will call itself with all adjacent X and Y values, making sure that it did not already use those X and Y values before. If the X and Y values are those of the end location, it will save all the previous instances of the method as the correct path. Here is a sample code in Java:
The maze-routing algorithm is a low overhead method to find the way between any two locations of the maze. The algorithm is initially proposed for chip multiprocessors (CMPs) domain and guarantees to work for any grid-based maze. In addition to finding paths between two location of the grid (maze), the algorithm can detect when there is no path between the source and destination. Also, the algorithm is to be used by an inside traveler with no prior knowledge of the maze with fixed memory complexity regardless of the maze size; requiring 4 variables in total for finding the path and detecting the unreachable locations. Nevertheless, the algorithm is not to find the shortest path.

Maze-routing algorithm uses the notion of Manhattan distance (MD) and relies on the property of grids that the MD increments/decrements "exactly" by 1 when moving from one location to any 4 neighboring locations. Here is the pseudocode without the capability to detect unreachable locations.

When a maze has multiple solutions, the solver may want to find the shortest path from start to finish. There are several algorithms to find shortest paths, most of them coming from graph theory. One such algorithm finds the shortest path by implementing a breadth-first search, while another, the A* algorithm, uses a heuristic technique. The breadth-first search algorithm uses a queue to visit cells in increasing distance order from the start until the finish is reached. Each visited cell needs to keep track of its distance from the start or which adjacent cell nearer to the start caused it to be added to the queue. When the finish location is found, follow the path of cells backwards to the start, which is the shortest path. The breadth-first search in its simplest form has its limitations, like finding the shortest path in weighted graphs.




</doc>
<doc id="41457976" url="https://en.wikipedia.org/wiki?curid=41457976" title="METIS">
METIS

METIS is a software package for graph partitioning that implements various multilevel algorithms. METIS' multilevel approach has three phases and comes with several algorithms for each phase:
The final partition computed during the third phase (the refined partition projected onto G) is a partition of the original graph.



</doc>
<doc id="13830115" url="https://en.wikipedia.org/wiki?curid=13830115" title="Jump-and-Walk algorithm">
Jump-and-Walk algorithm

Jump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations). Surprisingly, the algorithm does not need any preprocessing or complex data structures except some simple representation of the triangulation itself. The predecessor of Jump-and-Walk was due to Lawson (1977) and Green and Sibson (1978), which picks a random starting point S and then walks from S toward the query point Q one triangle at a time. But no theoretical analysis was known for these predecessors until after mid-1990s.

Jump-and-Walk picks a small group of sample points and starts the walk from the sample point which is the closest to Q until the simplex containing Q is found. The algorithm was a folklore in practice for some time, and the formal presentation of the algorithm and the analysis of its performance on 2D random Delaunay triangulation was done by Devroye, Mucke and Zhu in mid-1990s (the paper appeared in Algorithmica, 1998). The analysis on 3D random Delaunay triangulation was done by Mucke, Saias and Zhu (ACM Symposium of Computational Geometry, 1996). In both cases, a boundary condition was assumed, namely, Q must be slightly away from the boundary of the convex domain where the vertices of the random Delaunay triangulation are drawn. In 2004, Devroye, Lemaire and Moreau showed that in 2D the boundary condition can be withdrawn (the paper appeared in Computational Geometry: Theory and Applications, 2004).

Jump-and-Walk has been used in many famous software packages, e.g., QHULL, Triangle and CGAL.

 


</doc>
<doc id="42360188" url="https://en.wikipedia.org/wiki?curid=42360188" title="Algorithmic logic">
Algorithmic logic

Algorithmic logic is a calculus of programs which allows the expression of semantic properties of programs by appropriate logical formulas. It provides a framework that enables proving the formulas from the axioms of program constructs such as assignment, iteration and composition instructions and from the axioms of the data structures in question see , .

The following diagram helps to locate algorithmic logic among other logics.
formula_1
The formalized language of algorithmic logic (and of algorithmic theories of various data structures) contains three types of well formed expressions: "Terms" - i.e. expressions denoting operations on elements of data structures, 
"formulas" - i.e. expressions denoting the relations among elements of data structures, "programs" - i.e. algorithms - these expressions describe the computations.
For semantics of terms and formulas consult pages on first order logic and Tarski's semantic. The meaning of a program formula_2 is the set of possible computations of the program.

Algorithmic logic is one of many logics of programs.
Another logic of programs is dynamic logic, see dynamic logic, .



</doc>
<doc id="42563034" url="https://en.wikipedia.org/wiki?curid=42563034" title="KiSAO">
KiSAO

The Kinetic Simulation Algorithm Ontology (KiSAO) supplies information about existing algorithms available for the simulation of systems biology models, their characterization and interrelationships. KiSAO is part of the BioModels.net project and of the COMBINE initiative.

KiSAO consists of three main branches:
The elements of each algorithm branch are linked to characteristic and parameter branches using "has characteristic" and "has parameter" relationships accordingly. The algorithm branch itself is hierarchically structured using relationships which denote that the descendant algorithms were derived from, or specify, more general ancestors.



</doc>
<doc id="42923391" url="https://en.wikipedia.org/wiki?curid=42923391" title="Kleene's algorithm">
Kleene's algorithm

In theoretical computer science, in particular in formal language theory, Kleene's algorithm transforms a given nondeterministic finite automaton (NFA) into a regular expression. 
Together with other conversion algorithms, it establishes the equivalence of several description formats for regular languages. Alternative presentations of the same method include the "elimination method" attributed to Brzozowski and McCluskey, the algorithm of McNaughton and Yamada, and the use of Arden's lemma.

According to Gross and Yellen (2004), the algorithm can be traced back to Kleene (1956). A presentation of the algorithm in the case of deterministic finite automata (DFAs) is given in Hopcroft and Ullman (1979). The presentation of the algorithm for NFAs below follows Gross and Yellen (2004).

Given a nondeterministic finite automaton "M" = ("Q", Σ, δ, "q", "F"), with "Q" = { "q"...,"q" } its set of states, the algorithm computes 
Here, "going through a state" means entering "and" leaving it, so both "i" and "j" may be higher than "k", but no intermediate state may.
Each set "R" is represented by a regular expression; the algorithm computes them step by step for "k" = -1, 0, ..., "n". Since there is no state numbered higher than "n", the regular expression "R" represents the set of all strings that take "M" from its start state "q" to "q". If "F" = { "q"...,"q" } is the set of accept states, the regular expression "R" | ... | "R" represents the language accepted by "M".

The initial regular expressions, for "k" = -1, are computed as follows for "i"≠"j":
and as follows for "i"="j":

In other words, "R" mentions all letters that label a transition from "i" to "j", and we also include ε in the case where "i"="j".

After that, in each step the expressions "R" are computed from the previous ones by

Another way to understand the operation of the algorithm is as an "elimination method", where the states from 0 to "n" are successively removed: when state "k" is removed, the regular expression "R", which describes the words that label a path from state "i">"k" to state "j">"k", is rewritten into "R" so as to take into account the possibility of going via the "eliminated" state "k".

By induction on "k", it can be shown that the length of each expression "R" is at most (4(6"s"+7) - 4) symbols, where "s" denotes the number of characters in Σ.
Therefore, the length of the regular expression representing the language accepted by "M" is at most (4(6"s"+7)"f" - "f" - 3) symbols, where "f" denotes the number of final states.
This exponential blowup is inevitable, because there exist families of DFAs for which any equivalent regular expression must be of exponential size.

In practice, the size of the regular expression obtained by running the algorithm can be very different depending on the order in which the states are considered by the procedure, i.e., the order in which they are numbered from 0 to "n".

The automaton shown in the picture can be described as "M" = ("Q", Σ, δ, "q", "F") with

Kleene's algorithm computes the initial regular expressions as

After that, the "R" are computed from the "R" step by step for "k" = 0, 1, 2.
Kleene algebra equalities are used to simplify the regular expressions as much as possible.




Since "q" is the start state and "q" is the only accept state, the regular expression "R" denotes the set of all strings accepted by the automaton.



</doc>
<doc id="45194398" url="https://en.wikipedia.org/wiki?curid=45194398" title="Domain reduction algorithm">
Domain reduction algorithm

Domain reduction algorithms are algorithms used to reduce constraints and degrees of freedom in order to provide solutions for partial differential equations.


</doc>
<doc id="35457560" url="https://en.wikipedia.org/wiki?curid=35457560" title="RNA22">
RNA22

Rna22 is a pattern-based algorithm for the discovery of microRNA target sites and the corresponding heteroduplexes.

The algorithm is conceptually distinct from other methods for predicting in that it does "not" use experimentally validated heteroduplexes for training, instead relying only on the sequences of
known mature miRNAs that are found in the public databases. The key idea of rna22 is that the reverse complement of any salient sequence features that one can identify in mature microRNA sequences (using pattern discovery techniques) should allow one to identify candidate microRNA target sites in a sequence of interest: rna22 makes use of the Teiresias algorithm to discover such salient features. Once a candidate microRNA target site has been located, the targeting microRNA can be identified with the help of any of several algorithms able to compute RNA:RNA heteroduplexes. A new version (v2.0) of the algorithm is now available: v2.0-beta adds probability estimates to each prediction, gives users the ability to choose the sensitivity/specificity settings on-the-fly, is significantly faster than the original, and can be accessed through http://cm.jefferson.edu/rna22/Interactive/.

Rna22 neither relies on nor imposes any cross-organism conservation constraints to filter out unlikely candidates; this gives it the ability to discover microRNA binding sites that may not be conserved in phylogenetically proximal organisms. Also, as mentioned above, rna22 can identify putative microRNA binding sites without needing to know the identity of the targeting microRNA. A notable property of rna22 is that it does "not" require the presence of the exact reverse complement of a microRNA's seed in a putative target permitting bulges and G:U wobbles in the seed region of the heteroduplex. Lastly, the algorithm has been shown to achieve high signal-to-noise ratio.

Use of rna22 led to the discovery of "non-canonical" microRNA targets in the coding regions of the mouse "Nanog", "Oct4" and "Sox2". Most of these targets are not conserved in the human orthologues of these three transcription factors even though they reside in the coding region of the corresponding mRNAs. Moreover, most of these targets contain G:U wobbles, one or more bulges, or both, in the seed region of the heteroduplex. In addition to coding regions, rna22 has helped discover non-canonical targets in 3'UTRs.

A recent study examined the problem of non-canonical miRNA targets using molecular dynamics simulations of the crystal structure of the Argonaute-miRNA:mRNA ternary complex. The study found that several kinds of modifications, including combinations of multiple G:U wobbles and mismatches in the seed region, are admissible and result in only minor structural fluctuations that do not affect the stability of the ternary complex. The study also showed that the findings of the molecular dynamics simulation are supported by HITS-CLIP (CLIP-seq) data. These results suggest that "bona fide" miRNA targets transcend the canonical seed-model in turn making target prediction tools like rna22 an ideal choice for exploring the newly augmented spectrum of miRNA targets.


</doc>
<doc id="44995795" url="https://en.wikipedia.org/wiki?curid=44995795" title="AVT Statistical filtering algorithm">
AVT Statistical filtering algorithm

AVT Statistical filtering algorithm is an approach to improving quality of raw data collected from various sources. It is most effective in cases when there is inband noise present. In those cases AVT is better at filtering data then, band-pass filter or any digital filtering based on variation of.

Conventional filtering is useful when signal/data has different frequency than noise and signal/data is separated/filtered by frequency discrimination of noise. Frequency discrimination filtering is done using Low Pass, High Pass and Band Pass filtering which refers to relative frequency filtering criteria target for such configuration. Those filters are created using passive and active components and sometimes are implemented using software algorithms based on Fast Fourier transform (FFT).

AVT filtering is implemented in software and its inner working is based on statistical analysis of raw data.

When signal frequency/(useful data distribution frequency) coincides with noise frequency/(noisy data distribution frequency) we have inband noise. In this situations frequency discrimination filtering does not work since the noise and useful signal are indistinguishable and where AVT excels. To achieve filtering in such conditions there are several methods/algorithms available which are briefly described below.



AVT algorithm stands for Antonyan Vardan Transform and its implementation explained below.

This algorithm is based on amplitude discrimination and can easily reject any noise that is not like actual signal, otherwise statistically different then 1 standard deviation of the signal. Note that this type of filtering can be used in situations where the actual environmental noise is not known in advance.

Using a system that has signal value of 1 and has noise added at 0.1% and 1% levels will simplify quantification of algorithm performance. The R script is used to create pseudo random noise added to signal and analyze the results of filtering using several algorithms. Please refer to "Reduce Inband Noise with the AVT Algorithm" article for details.
This graphs show that AVT algorithm provides best results compared with Median and Averaging algorithms while using data sample size of 32, 64 and 128 values. Note that this graph was created by analyzing random data array of 10000 values. Sample of this data is graphically represented below.

In some situations better results can be obtained by cascading several stages of AVT filtering. This will produce singular constant value which can be used for equipment that has known stable characteristics like thermometers, thermistors and other slow acting sensors.

This is useful for detecting minute signals that are close to background noise level.



</doc>
<doc id="44308703" url="https://en.wikipedia.org/wiki?curid=44308703" title="Flajolet–Martin algorithm">
Flajolet–Martin algorithm

The Flajolet–Martin algorithm is an algorithm for approximating the number of distinct elements in a stream with a single pass and space-consumption logarithmic in the maximal number of possible distinct elements in the stream (the count-distinct problem). The algorithm was introduced by Philippe Flajolet and G. Nigel Martin in their 1984 article "Probabilistic Counting Algorithms for Data Base Applications". Later it has been refined in "LogLog counting of large cardinalities" by Marianne Durand and Philippe Flajolet, and "HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm" by Philippe Flajolet et al.

In their 2010 article "An optimal algorithm for the distinct elements problem", Daniel M. Kane, Jelani Nelson and David P. Woodruff give an improved algorithm, which uses nearly optimal space and has optimal "O"(1) update and reporting times.

Assume that we are given a hash function formula_1 that maps input formula_2 to integers in the range formula_3, and where the outputs are sufficiently uniformly distributed. Note that the set of integers from 0 to formula_4 corresponds to the set of binary strings of length formula_5. For any non-negative integer formula_6, define formula_7 to be the formula_8-th bit in the binary representation of formula_6, such that:

We then define a function formula_11 that outputs the position of the least-significant set bit in the binary representation of formula_6:

where formula_14. Note that with the above definition we are using 0-indexing for the positions. For example, formula_15, since the least significant bit is a 1 (0th position), and formula_16, since the least significant bit is at the 3rd position. At this point, note that under the assumption that the output of our hash function is uniformly distributed, then the probability of observing a hash output ending with formula_17 (a one, followed by formula_8 zeroes) is formula_19, since this corresponds to flipping formula_8 heads and then a tail with a fair coin.

Now the Flajolet–Martin algorithm for estimating the cardinality of a multiset formula_21 is as follows:

The idea is that if formula_33 is the number of distinct elements in the multiset formula_21, then formula_35 is accessed approximately formula_36 times, formula_37 is accessed approximately formula_38 times and so on. Consequently, if formula_39, then formula_40 is almost certainly 0, and if formula_41, then formula_40 is almost certainly 1. If formula_43, then formula_40 can be expected to be either 1 or 0.

The correction factor formula_32 is found by calculations, which can be found in the original article.

A problem with the Flajolet–Martin algorithm in the above form is that the results vary significantly. A common solution has been to run the algorithm multiple times with formula_8 different hash functions and combine the results from the different runs. One idea is to take the mean of the formula_8 results together from each hash function, obtaining a single estimate of the cardinality. The problem with this is that averaging is very susceptible to outliers (which are likely here). A different idea is to use the median, which is less prone to be influences by outliers. The problem with this is that the results can only take form formula_31, where formula_27 is integer. A common solution is to combine both the mean and the median: Create formula_50 hash functions and split them into formula_8 distinct groups (each of size formula_52). Within each group use the median for aggregating together the formula_52 results, and finally take the mean of the formula_8 group estimates as the final estimate.

The 2007 HyperLogLog algorithm splits the multiset into subsets and estimates their cardinalities, then it uses the harmonic mean to combine them into an estimate for the original cardinality.



</doc>
<doc id="46900918" url="https://en.wikipedia.org/wiki?curid=46900918" title="Xulvi-Brunet–Sokolov algorithm">
Xulvi-Brunet–Sokolov algorithm

Xulvi-Brunet and Sokolov’s algorithm generates networks with chosen degree correlations. This method is based on link rewiring, in which the desired degree is governed by parameter ρ. By varying this single parameter it is possible to generate networks from random (when ρ = 0) to perfectly assortative or disassortative (when ρ = 1). This algorithm allows to keep network’s degree distribution unchanged when changing the value of ρ.

In assortative networks, well-connected nodes are likely to be connected to other highly connected nodes. Social networks are examples of assortative networks. This means that an assortative network has the property that almost all nodes with the same degree are linked only between themselves.

The Xulvi-Brunet–Sokolov algorithm for this type of networks is the following. 
In a given network, two links connecting four different nodes are chosen randomly. These nodes are ordered by their degrees. Then, with probability ρ, the links are randomly rewired in such a way that one link connects the two nodes with the smaller degrees and the other connects the two nodes with the larger degrees. If one or both of these links already existed in the network, the step is discarded and is repeated again. Thus, there will be no self-connected nodes or multiple links connecting the same two nodes. Different degrees of assortativity of a network can be achieved by changing the parameter ρ. 
Assortative networks are characterized by highly connected groups of nodes with similar degree. As assortativity grows, the average path length and clustering coefficient increase.

In disassortative networks, highly connected nodes tend to connect to less-well-connected nodes with larger probability than in uncorrelated networks. Examples of such networks include biological networks.
The Xulvi-Brunet and Sokolov’s algorithm for this type of networks is similar to the one for assortative networks with one minor change. As before, two links of four nodes are randomly chose and the nodes are ordered with respect to their degrees. However, in this case, the links are rewired (with probability p) such that one link connects the highest connected node with the node with the lowest degree and the other link connects the two remaining nodes randomly with probability 1 − ρ. Similarly, if the new links already existed, the previous step is repeated. This algorithm does not change the degree of nodes and thus the degree distribution of the network.


</doc>
<doc id="46953393" url="https://en.wikipedia.org/wiki?curid=46953393" title="Zassenhaus algorithm">
Zassenhaus algorithm

In mathematics, the Zassenhaus algorithm
is a method to calculate a basis for the intersection and sum of two subspaces of a vector space.
It is named after Hans Zassenhaus, but no publication of this algorithm by him is known. It is used in computer algebra systems.

Let be a vector space and , two finite-dimensional subspaces of with the following spanning sets:
and
Finally, let formula_3 be linearly independent vectors so that formula_4 and formula_5 can be written as
and

The algorithm computes the base of the sum formula_8 and a base of the intersection formula_9.

The algorithm creates the following block matrix of size formula_10:

Using elementary row operations, this matrix is transformed to the row echelon form. Then, it has the following shape:
Here, formula_13 stands for arbitrary numbers, and the vectors 
formula_14 for every formula_15 and formula_16 for every formula_17 are nonzero.

Then formula_18 with
is a basis of formula_20
and formula_21 with
is a basis of formula_9.

First, we define formula_24 to be the projection to the first component.

Let
formula_25
Then formula_26 and
formula_27.

Also, formula_28 is the kernel of formula_29, the projection restricted to .
Therefore, formula_30.

The Zassenhaus Algorithm calculates a basis of . In the first columns of this matrix, there is a basis formula_31 of formula_20.

The rows of the form formula_33 (with formula_34) are obviously in formula_28. Because the matrix is in row echelon form, they are also linearly independent.
All rows which are different from zero (formula_36 and formula_33) are a basis of , so there are formula_38 such formula_39s. Therefore, the formula_39s form a basis of formula_9.

Consider the two subspaces formula_42 and formula_43 of the vector space formula_44.

Using the standard basis, we create the following matrix of dimension formula_45:

Using elementary row operations, we transform this matrix into the following matrix:

Therefore,
formula_49 is a basis of formula_20, and
formula_51 is a basis of formula_9.



</doc>
<doc id="46877898" url="https://en.wikipedia.org/wiki?curid=46877898" title="Chinese Whispers (clustering method)">
Chinese Whispers (clustering method)

Chinese Whispers is a clustering method used in network science named after the famous whispering game. Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by Chris Biemann and Sven Teresniak in 2005. The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.

Chinese Whispers is a hard partitioning, randomized, flat clustering (no hierarchical relations between clusters) method. The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can only belong to one cluster at a given moment. The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese Whispers is time linear which means that it is extremely fast even if the number of nodes and links are very high in the network.

The algorithm works in the following way in an undirected unweighted graph:

The predetermined threshold for the number of the iterations is needed because it is possible that process does not converge. On the other hand in a network with approximately 10000 nodes the clusters does not change significantly after 40-50 iterations even if there is no convergence.

The main strength of Chinese Whispers lies in its time linear property. Because of the processing time increases linearly with the number of nodes, the algorithm is capable of identifying communities in a network very fast. For this reason Chinese Whispers is a good tool to analyze community structures in graph with a very high number of nodes. The effectiveness of the method increases further if the network has the small world property.

On the other hand because the algorithm is not deterministic in the case of small node number the resulting clusters often significantly differ from each other. The reason for this is that in the case of a small network it matters more from which node the iteration process starts while in large networks the relevance of starting points disappears. For this reason for small graphs other clustering methods are recommended.

Chinese Whispers is used in many subfield of network science. Most frequently it is mentioned in the context of natural language processing problems. On the other hand the algorithm is applicable to any kind of community identification problem which is related to a network framework. Chinese Whispers is available for personal use as an extension package for Gephi which is an open source program designed for network analysis.



</doc>
<doc id="47341174" url="https://en.wikipedia.org/wiki?curid=47341174" title="Collaborative diffusion">
Collaborative diffusion

Collaborative Diffusion is a type of pathfinding algorithm which uses the concept of "antiobjects", objects within a computer program that function opposite to what would be conventionally expected. Collaborative Diffusion is typically used in video games, when multiple agents must path towards a single target agent. For example, the ghosts in Pac-Man. In this case, the background tiles serve as antiobjects, carrying out the necessary calculations for creating a path and having the foreground objects react accordingly, whereas having foreground objects be responsible for their own pathing would be conventionally expected.

Collaborative Diffusion is favored for its efficiency over other pathfinding algorithms, such as A*, when handling multiple agents. Also, this method allows elements of competition and teamwork to easily be incorporated between tracking agents. Notably, the time taken to calculate paths remains constant as the number of agents increases.


</doc>
<doc id="10140499" url="https://en.wikipedia.org/wiki?curid=10140499" title="Algorithm engineering">
Algorithm engineering

Algorithm engineering focuses on the design, analysis, implementation, optimization, profiling and experimental evaluation of computer algorithms, bridging the gap between algorithm theory and practical applications of algorithms in software engineering.
It is a general methodology for algorithmic research.

In 1995, a report from an NSF-sponsored workshop "with the purpose of assessing the current goals and directions of the Theory of Computing (TOC) community" identified the slow speed of adoption of theoretical insights by practitioners as an important issue and suggested measures to
But also, promising algorithmic approaches have been neglected due to difficulties in mathematical analysis.
The term "algorithm engineering" was first used with specificity in 1997, with the first Workshop on Algorithm Engineering (WAE97), organized by Giuseppe F. Italiano.

Algorithm engineering does not intend to replace or compete with algorithm theory, but tries to enrich, refine and reinforce its formal approaches with experimental algorithmics (also called empirical algorithmics).

This way it can provide new insights into the efficiency and performance of algorithms in cases where

Some researchers describe algorithm engineering's methodology as a cycle consisting of algorithm design, analysis, implementation and experimental evaluation, joined by further aspects like machine models or realistic inputs.
They argue that equating algorithm engineering with experimental algorithmics is too limited, because viewing design and analysis, implementation and experimentation as separate activities ignores the crucial feedback loop between those elements of algorithm engineering.

While specific applications are outside the methodology of algorithm engineering, they play an important role in shaping realistic models of the problem and the underlying machine, and supply real inputs and other design parameters for experiments.

Compared to algorithm theory, which usually focuses on the asymptotic behavior of algorithms, algorithm engineers need to keep further requirements in mind: Simplicity of the algorithm, implementability in programming languages on real hardware, and allowing code reuse.
Additionally, constant factors of algorithms have such a considerable impact on real-world inputs that sometimes an algorithm with worse asymptotic behavior performs better in practice due to lower constant factors.

Some problems can be solved with heuristics and randomized algorithms in a simpler and more efficient fashion than with deterministic algorithms. Unfortunately, this makes even simple randomized algorithms "difficult to analyze because there are subtle dependencies to be taken into account".

Huge semantic gaps between theoretical insights, formulated algorithms, programming languages and hardware pose a challenge to efficient implementations of even simple algorithms, because small implementation details can have rippling effects on execution behavior.
The only reliable way to compare several implementations of an algorithm is to spend an considerable amount of time on tuning and profiling, running those algorithms on multiple architectures, and looking at the generated machine code.

See: Experimental algorithmics

Implementations of algorithms used for experiments differ in significant ways from code usable in applications.
While the former prioritizes fast prototyping, performance and instrumentation for measurements during experiments, the latter requires "thorough testing, maintainability, simplicity, and tuning for particular classes of inputs".

Stable, well-tested algorithm libraries like LEDA play an important role in technology transfer by speeding up the adoption of new algorithms in applications. 
Such libraries reduce the required investment and risk for practitioners, because it removes the burden of understanding and implementing the results of academic research.

Two main conferences on Algorithm Engineering are organized annually, namely:

The 1997 Workshop on Algorithm Engineering (WAE'97) was held in Venice (Italy) on September 11–13, 1997. The Third International Workshop on Algorithm Engineering (WAE'99) was held in London, UK in July 1999.
The first Workshop on Algorithm Engineering and Experimentation (ALENEX99) was held in Baltimore, Maryland on January 15–16, 1999. It was sponsored by DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science (at Rutgers University), with additional support from SIGACT, the ACM Special Interest Group on Algorithms and Computation Theory, and SIAM, the Society for Industrial and Applied Mathematics.


</doc>
<doc id="47108875" url="https://en.wikipedia.org/wiki?curid=47108875" title="Jumble algorithm">
Jumble algorithm

Each clue in a Jumble word puzzle is a word that has been “jumbled” by permuting the letters of each word to make an anagram. A dictionary of such anagrams may be used to solve puzzles or verify that a jumbled word is unique when creating puzzles.

Algorithms have been designed to solve Jumbles, using a dictionary. Common algorithms work by printing all words that can be formed from a set of letters. The solver then chooses the right word.

First algorithm:


Second algorithm:


Algorithm to find the permutations of J:


J(1)J(2)

J(2)J(1)


J(1)J(2)J(3)

J(1)J(3)J(2)

J(3)J(1)J(2)

J(2)J(1)J(3)

J(2)J(3)J(1)

J(3)J(2)J(1)


Though the algorithm looks complex it is easy to program.

Douglas Hofstadter developed a program called Jumbo that tries to solve Jumble problems as a human mind would.
The program doesn't rely on a dictionary and doesn't try to find real English words, but rather words that could be English, exploiting a database of plausibilities for various combinations of letters.
Letters are combined non-deterministically, following a strategy inspired by chemical reactions and free associations.


</doc>
<doc id="46900869" url="https://en.wikipedia.org/wiki?curid=46900869" title="Label propagation algorithm">
Label propagation algorithm

Label propagation is a semi-supervised machine learning algorithm that assigns labels to previously unlabeled data points. At the start of the algorithm, a (generally small) subset of the data points have labels (or classifications). These labels are propagated to the unlabeled points throughout the course of the algorithm.

Within complex networks, real networks tend to have community structure. Label propagation is an algorithm for finding communities. In comparison with other algorithms label propagation has advantages in its running time and amount of a priori information needed about the network structure (no parameter is required to be known beforehand). The disadvantage is that it produces no unique solution, but an aggregate of many solutions.

At initial condition, the nodes carry a label that denotes the community they belong to. Membership in a community changes based on the labels that the neighboring nodes possess. This change is subject to the maximum number of labels within one degree of the nodes. Every node is initialized with a unique label, then the labels diffuse through the network. Consequently, densely connected groups reach a common label quickly. When many such dense (consensus) groups are created throughout the network, they continue to expand outwards until it is impossible to do so.

The process has 5 steps:

1. Initialize the labels at all nodes in the network. For a given node x, C (0) = x.

2. Set t = 1.

3. Arrange the nodes in the network in a random order and set it to X.

4. For each x ∈ X chosen in that specific order, let C(t) = f(C(t), ...,C(t),C (t − 1), ...,C (t − 1)). f here returns the label occurring with the highest frequency among neighbours. Select a label at random if there are multiple highest frequency labels.

5. If every node has a label that the maximum number of their neighbours have, then stop the algorithm. Else, set t = t + 1 and go to (3).

In contrast with other algorithms label propagation can result in various community structures from the same initial condition. The range of solutions can be narrowed if some nodes are given preliminary labels while others are held unlabelled. Consequently, unlabelled nodes will be more likely to adapt to the labelled ones. For a more accurate finding of communities, Jaccard’s index is used to aggregate multiple community structures, containing all important information.




</doc>
<doc id="38090349" url="https://en.wikipedia.org/wiki?curid=38090349" title="EdgeRank">
EdgeRank

EdgeRank is the name commonly given to the algorithm that Facebook uses to determine what articles should be displayed in a user's News Feed. As of 2011, Facebook has stopped using the EdgeRank system and uses a machine learning algorithm that, as of 2013, takes more than 100,000 factors into account.

EdgeRank was developed and implemented by Serkan Piantino.

In 2010, a simplified version of the EdgeRank algorithm was presented as:

where:


Some of the methods that Facebook uses to adjust the parameters are proprietary and not available to the public.

EdgeRank and its successors have a broad impact on what users actually see out of what they ostensibly follow: for instance, the selection can produce a filter bubble (if users are exposed to updates which confirm their opinions etc.) or alter people's mood (if users are shown a disproportionate amount of positive or negative updates).

As a result, for Facebook pages, the typical engagement rate is less than 1% (or less than 0.1% for the bigger ones) and organic reach 10% or less for most non-profits.

As a consequence, for pages it may be nearly impossible to reach any significant audience without paying to promote their content.




</doc>
<doc id="46493377" url="https://en.wikipedia.org/wiki?curid=46493377" title="The Algorithm Auction">
The Algorithm Auction

The Algorithm Auction is the world’s first auction of computer algorithms. Created by Ruse Laboratories, the initial auction featured seven lots and was held at the Cooper Hewitt, Smithsonian Design Museum on March 27, 2015.

Five lots were physical representations of famous code or algorithms, including a signed, handwritten copy of the original Hello, World! C program by its creator Brian Kernighan on dot-matrix printer paper, a printed copy of 5,000 lines of Assembly code comprising the earliest known version of Turtle Graphics, signed by its creator Hal Abelson, a necktie containing the six-line qrpff algorithm capable of decrypting content on a commercially produced DVD video disc, and a pair of drawings representing OKCupid’s original Compatibility Calculation algorithm, signed by the company founders. The qrpff lot sold for $2,500.

Two other lots were “living algorithms,” including a set of JavaScript tools for building applications that are accessible to the visually impaired and the other is for a program that converts lines of software code into music. Winning bidders received, along with artifacts related to the algorithms, a full intellectual property license to use, modify, or open-source the code. All lots were sold, with Hello World receiving the most bids.

Exhibited alongside the auction lots were a facsimile of the Plimpton 322 tablet on loan from Columbia University, and Nigella, an art-world facing computer virus named after Nigella Lawson and created by cypherpunk and hacktivist Richard Jones.

Sebastian Chan, Director of Digital & Emerging Media at the Cooper–Hewitt, attended the event remotely from Milan, Italy via a Beam Pro telepresence robot.

Following the auction, the Museum of Modern Art held a salon titled "The Way of the Algorithm" highlighting algorithms as "a ubiquitous and indispensable component of our lives."


</doc>
<doc id="48786651" url="https://en.wikipedia.org/wiki?curid=48786651" title="Communication-avoiding algorithms">
Communication-avoiding algorithms

Communication-Avoiding Algorithms minimize movement of data within a memory hierarchy for improving its running-time and energy consumption. These minimize the total of two costs (in terms of time and energy): arithmetic and communication. Communication, in this context refers to moving data, either between levels of memory or between multiple processors over a network. It is much more expensive than arithmetic.

Consider the following running-time model:
⇒ Total running time = γ*(no. of FLOPs) + β*(no. of words)

From the fact that β » γ as measured in time and energy, communication cost dominates computation cost. Technological trends indicate that the relative cost of communication is increasing on a variety of platforms, from cloud computing to supercomputers to mobile devices. The report also predicts that gap between DRAM access time and FLOPs will increase 100x over coming decade to balance power usage between processors and DRAM.

Energy consumption increases by orders of magnitude as we go higher in the memory hierarchy. United States president Barack Obama cited Communication-Avoiding Algorithms in the FY 2012 Department of Energy budget request to Congress: "“New Algorithm Improves Performance and Accuracy on Extreme-Scale Computing Systems. On modern computer architectures, communication between processors takes longer than the performance of a floating point arithmetic operation by a given processor. ASCR researchers have developed a new method, derived from commonly used linear algebra methods, to minimize communications between processors and the memory hierarchy, by reformulating the communication patterns specified within the algorithm. This method has been implemented in the TRILINOS framework, a highly-regarded suite of software, which provides functionality for researchers around the world to solve large scale, complex multi-physics problems.”"

Communication-Avoiding algorithms are designed with the following objectives:

The following simple example demonstrates how these are achieved.

Let A, B and C be square matrices of order n x n. The following naive algorithm implements C = C + A * B:

Arithmetic cost (time-complexity): n² (2n-1) for sufficiently large n or O(n³).

Rewriting this algorithm with communication cost labelled at each step

Fast memory may be defined as the local processor memory (CPU cache) of size M and slow memory may be defined as the DRAM.

Communication cost (reads/writes): n³ + 3n² or O(n³)

Since total running time = γ*O(n³) + β*O(n³) and β » γ the communication cost is dominant. The Blocked (Tiled) Matrix Multiplication algorithm reduces this dominant term.

Consider A,B,C to be n/b-by-n/b matrices of b-by-b sub-blocks where b is called the block size; assume 3 b-by-b blocks fit in fast memory.

Communication cost: 2n³/b + 2n² reads/writes « 2n³ arithmetic cost

Making b as large possible:
3b ≤ M 
We achieve the following communication lowerbound:
3n/M + 2n or Ω(no. of FLOPs / M )

Most of the approaches investigated in the past to address this problem rely on scheduling or tuning techniques that aim at overlapping communication with computation. However, this approach can lead to an improvement of at most a factor of two. Ghosting is a different technique for reducing communication, in which a processor stores and computes redundantly data from neighboring processors for future computations. Cache-oblivious algorithms represent a different approach introduced in 1999 for Fast Fourier Transforms, and then extended to graph algorithms, dynamic programming, etc. They were also applied to several operations in linear algebra as dense LU and QR factorizations. The design of architecture specific algorithms is another approach that can be used for reducing the communication in parallel algorithms, and there are many examples in the literature of algorithms that are adapted to a given communication topology.


</doc>
<doc id="48768665" url="https://en.wikipedia.org/wiki?curid=48768665" title="Non-malleable code">
Non-malleable code

The notion of non-malleable codes was introduced in 2010 by Dziembowski, Pietrzak, and Wichs, for relaxing the notion of error-correction and error-detection. Informally, a code is non-malleable if the message contained in a modified code-word is either the original message, or a completely unrelated value. Non-malleable codes provide a useful and meaningful security guarantee in situations where traditional error-correction and error-detection is impossible; for example, when the attacker can completely overwrite the encoded message. Although such codes do not exist if the family of "tampering functions" F is completely unrestricted, they are known to exist for many broad tampering families F.

To know the operation schema of non-malleable code, we have to have a knowledge of the basic experiment it based on. The following is the three step method of tampering experiment.

The tampering experiment can be used to model several interesting real-world settings, such as data transmitted over a noisy channel, or adversarial tampering of data stored in the memory of a physical device. Having this experimental base, we would like to build special encoding/decoding procedures formula_12, which give us some meaningful guarantees about the results of the above tampering experiment, for large and interesting families formula_13 of tampering functions. The following are several possibilities for the type of guarantees that we may hope for.

One very natural guarantee, called error-correction, would be to require that for any tampering function and any "source-message s", the tampering experiment always produces the correct decoded message formula_14.

A weaker guarantee, called error-detection, requires that the tampering-experiment always results in either the correct value formula_14 or a special symbol formula_16 indicating that tampering has been detected. This notion of error-detection is a weaker guarantee than error-correction, and achievable for larger F of tampering functions.

A non-malleable code ensures that either the tampering experiment results in a correct decoded-message formula_14, or the decoded-message formula_10 is completely independent of and unrelated to the "source-message" formula_1. In other word, the notion of non-malleability for codes is similar, in spirit, to notions of non-malleability for cryptographic primitives (such as encryption2, commitments and zero-knowledge proofs), introduced by the seminal work of Dolev, Dwork and Naor.

Compared to error correction or error detection, the "right" formalization of non-malleable codes is somewhat harder to define. Let formula_20 be a random variable for the value of the decoded-message, which results when we run the tampering experiment with source-message formula_1 and tampering-function formula_22, over the randomness of the encoding procedure. Intuitively, we wish to say that the distribution of formula_20 is independent of the encoded message formula_1. Of course, we also want to allow for the case where the tampering experiment results in formula_14 (for example, if the tampering function is identity), which clearly depends on formula_1.

Thus, we require that for every tampering-function formula_5, there exists a distribution formula_28 which outputs either concrete values formula_10 or a special same formula_30 symbol, and faithfully models the distribution of formula_20 for all formula_1 in the following sense: for every source message formula_1, the distributions of formula_20 and formula_28 are statistically close when the formula_30 symbol is interpreted as formula_1. That is, formula_28 correctly simulates the "outcome" of the tampering-experiment with a function formula_5 without knowing the source-messages formula_1, but it is allowed some ambiguity by outputting a same formula_30 symbol to indicate that the decoded-message should be the same as the source-message, without specifying what the exact value is. The fact that formula_28 depends on only formula_22 and not on formula_1, shows that the outcome of formula_20 is independent of formula_1, exempting equality.

Notice that non-malleability is a weaker guarantee than error correction/detection; the latter ensure that any change in the code-word can be corrected or at least detected by the decoding procedure, whereas the former does allow the message to be modified, but only to an unrelated value. However, when studying error correction/detection we usually restrict ourselves to limited forms of tampering which preserve some notion of distance (e.g., usually hamming distance) between the original and tampered code-word. 
For example, it is already impossible to achieve error correction/detection for the simple family of functions formula_47 which, for every constant formula_6, includes a "constant" function formula_49 that maps all inputs to formula_6. There is always some function in formula_47 that maps everything to a valid code-word formula_6. In contrast, it is trivial to construct codes that are non-malleable w.r.t formula_47, as the output of a constant function is clearly independent of its input. The prior works on non-malleable codes show that one can construct non-malleable codes for highly complex tampering function families formula_13 for which error correction/detection can not be achievable.

As one very concrete example, we study non-malleability with respect to the family of functions formula_22 which specify, for each bit of the code-word formula_3, whether to keep it as is, flip it, set it to 0, set it to 1. That is, each bit of the code-word is modified arbitrarily but independently of the value of the other bits of the code-word. We call this the “bit-wise independent tampering” family formula_57. Note that this family contains constant functions formula_47 and constant-error functions formula_59 as subsets. Therefore, as we have mentioned, error-correction and error-detection cannot be achieved w.r.t. this family. Nevertheless, the following can show an efficient non-malleable code for this powerful family.

With formula_57 we denote the family which contains all tampering functions that tamper every bit independently. Formally, this family contains all functions <math>f_i: \left\


</doc>
<doc id="49589765" url="https://en.wikipedia.org/wiki?curid=49589765" title="Kunstweg">
Kunstweg

Bürgi's Kunstweg is a set of algorithms invented by Jost Bürgi at the end of the 16th century. They can be used for the calculation of sines to an arbitrary precision. Bürgi used these algorithms to calculate a Canon Sinuum, a table of sines in steps of 2 arc seconds. It is thought that this table had 8 sexagesimal places. Some authors have speculated that this table only covered the range from 0 to 45 degrees, but nothing seems to support this claim. Such tables were extremely important for navigation at sea. Johannes Kepler called the Canon Sinuum the most precise known table of sines (reference?). Bürgi explained his algorithms in his work Fundamentum Astronomiae which he presented to Emperor Rudolf II. in 1592.

The principles of iterative sine table calculation through the Kunstweg are as follows: cells in a column sum up the values of the two previous cells in the same column. The final cell's value is divided by two, and the next iteration starts. Finally, the values of the last column get normalized. Rather accurate approximations of sines are obtained after few iterations.

As recently as 2015, Folkerts et al. showed that this simple process converges indeed towards the true sines. According to Folkerts, this was the first step towards difference calculus.


</doc>
<doc id="49914674" url="https://en.wikipedia.org/wiki?curid=49914674" title="Online optimization">
Online optimization

Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). These kind of problems are denoted as online problems and are seen as opposed to the classical optimization problems where complete information is assumed (offline). The research on online optimization can be distinguished into online problems where multiple decisions are made sequentially based on a piece-by-piece input and those where a decision is made only once. A famous online problem where a decision is made only once is the Ski rental problem. In general, the output of an online algorithm is compared to the solution of a corresponding offline algorithm which is necessarily always optimal and knows the entire input in advance (competitive analysis).

In many situations, present decisions (for example, resources allocation) must be made with incomplete knowledge of the future or distributional assumptions on the future are not reliable. In such cases, online optimization can be used, which is different from other approaches such as robust optimization, stochastic optimization and Markov decision processes.

A problem exemplifying the concepts of online algorithms is the Canadian traveller problem. The goal of this problem is to minimize the cost of reaching a target in a weighted graph where some of the edges are unreliable and may have been removed from the graph. However, that an edge has been removed ("failed") is only revealed to "the traveller" when they reach one of the edge's endpoints. The worst case for this problem is simply that all of the unreliable edges fail and the problem reduces to the usual shortest path problem. An alternative analysis of the problem can be made with the help of competitive analysis. For this method of analysis, the offline algorithm knows in advance which edges will fail and the goal is to minimize the ratio between the online and offline algorithms' performance. This problem is PSPACE-complete.

There are many formal problems that offer more than one "online algorithm" as solution:


</doc>
<doc id="32612385" url="https://en.wikipedia.org/wiki?curid=32612385" title="Hindley–Milner type system">
Hindley–Milner type system

A Hindley–Milner (HM) type system is a classical type system for the lambda calculus with parametric polymorphism. It is also known as Damas–Milner or Damas–Hindley–Milner. It was first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.

Among HM's more notable properties are its completeness and its ability to infer the most general type of a given program without programmer-supplied type annotations or other hints. Algorithm W is an efficient type inference method that performs in almost linear time with respect to the size of the source, making it practically useful to type large programs. HM is preferably used for functional languages. It was first implemented as part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably with type class constraints like those in Haskell.

One and the same thing can be used for many purposes. A chair might be used to support a sitting person but also as a ladder to stand on while changing a light bulb or as a clothes valet. Beside having particular material qualities, which make a chair usable as such, it also has the particular designation for its use. When no chair is at hand, other things might be used as a seat, and so the designation of a thing can be changed as fast as one can turn an empty bottle crate upside down to change its purpose from a container to that of a support.

Different uses of physically near-identical things are usually accompanied by giving those things different names to emphasize the intended purpose. Depending on the use, seamen have a dozen or more words for a rope though it might materially be the same thing. The same in everyday language, where a leash indicates a use different to a line.

In computer science, this practice of naming things by its intended use is put to an extreme called "typing" and the names or expressions called "types":


Beside structuring objects, (data) types serve as means to validate that these objects are used as intended. Much like a bucket that could only be worn as a helmet or used to contain water at a time, a particular arrangement of bytes designated for one purpose might exclude other possible uses.

In programming, these uses are expressed as "functions" or "procedures" which serve the role of verbs in natural language. As an example for typing verbs, an English dictionary might define "gift" as "to give someone something", indicating that the object must be a person and the indirect object a physical thing. In programming, "someone" and "something" would be called types. Using a physical thing in the place of "someone" would be indicated as a programming error by a type checker.

Beside checking, one can use the types in this example to gain knowledge about an unknown word. Reading the sentence "Mary gifts John a bilber" the types could be used to conclude that a "bilber" is likely a physical thing. This activity and conclusion is called "type inference". As the story unfolds, more and more information about the unknown "bilber" may be gained, and eventually enough details become known to form a complete image of that kind of thing.

The type inference method designed by Hindley and Milner does just this for programming languages. The advantage of type inference over type checking is that it allows a more natural and dense style of programming. Instead of starting a program text with a glossary defining what a bilber and everything else is, one can distribute this information over the text simply by using the yet undefined words and let a program collect all the details about them. The method works for both nouns (data types) and for verbs (functions types). As a consequence, a programmer can proceed without ever mentioning types at all, while still having the full support of a type checker that validates their writing. When reading a program, the programmer can use type inference to query the full definition of anything named in the program whenever needed.

Historically, type inference to this extent was developed for a particular group of programming languages, called functional languages. These started in 1958 with Lisp, a programming language based on the lambda calculus and that compares well with modern scripting languages like Python or Lua. Lisp was mainly used for computer science research, often for symbol manipulation purposes where large, tree-like data structures were common.

Data in Lisp is dynamically typed and the types are only available to some degree while running a program. Debugging type errors was no less of a concern than it is with modern script languages. But, being completely untyped, i.e. written without any explicit type information, maintaining large programs written in Lisp soon became a problem because the many complicated types of the data were mentioned only in the program documentation and comments at best.

Thus, the need to have a Lisp-like language with machine-checkable types became more and more pressing. At some point, programming language development faced two challenges:


As an example, polymorphically constructing the list "(1 2)" of two numbers would mean writing:

This example was quite typical. Every third word a type, monotonously serving the type checker in every step. This worsens when the types become more complex. Then, the methods to be expressed in code become buried in types.

To handle this issue, effective methods for type inference were the subject of research, and Hindley–Milner's method was one of them. Their method was first used in ML (1973) and is also used in an extended form in Haskell (1990). The HM type inference method is strong enough to infer types not only for expressions, but for whole programs including the procedures and local definitions, providing a type-less style of programming.

The following text gives an impression of the resulting programming style for the quicksort procedure in Haskell:

Though all of the functions in the above example need type parameters, types are nowhere mentioned. The code is statically type-checked even though the type of the function defined is unknown and must be inferred to type-check the applications in the body.

Over the years, other programming languages added their own version of parametric types. C++ templates were introduced in 1998 and Java introduced generics in 2004. As programming with type parameters became more common, problems similar to the ones sketched for Lisp surfaced in imperative languages too, perhaps not as pressing as it was for the functional languages. As a consequence, these languages obtained support for some type inference techniques, for instance "auto" in C++11 (2014). Typically, the stronger type inference methods developed for functional programming cannot easily be integrated in the imperative languages, as their type systems' features are in part incompatible. However, through the support of additional techniques, it is actually possible to provide Haskell- and ML-style type inference even for a language like C which was designed decades ago, without any consideration for such a mechanism.

Before presenting the HM type system and related algorithms, the following sections make some features of HM more formal and precise.

In a typing, an expression E is opposed to a type T, formally written as E : T. Usually a typing only makes sense within some context, which is omitted here.

In this setting, the following questions are of particular interest:


For the simply typed lambda calculus, all three questions are decidable. The situation is not as comfortable when more expressive types are allowed. Additionally, the simply typed lambda calculus makes the types of the parameters of each function explicit, while they are not needed in HM. While HM is a method for type inference, it can be used also for type checking and answer the first question. To do that, a type is first inferred from E and then compared with the type wanted. The third question becomes of interest when looking at recursively-defined functions at the end of this article.

In the simply typed lambda calculus, types formula_1 are either atomic type constants or function types of form formula_2. Such types are "monomorphic". Typical examples are the types used in arithmetic values:

Contrary to this, the untyped lambda calculus is neutral to typing at all, and many of its functions can be meaningfully applied to all type of arguments. The trivial example is the identity function

which simply returns whatever value it is applied to. Less trivial examples include parametric types like lists.

While polymorphism in general means that operations accept values of more than one type, the polymorphism used here is parametric. One finds the notation of "type schemes" in the literature, too, emphasizing the parametric nature of the polymorphism. Additionally, constants may be typed with (quantified) type variables. E.g.:

Polymorphic types can become monomorphic by consistent substitution of their variables. Examples of monomorphic "instances" are:

More generally, types are polymorphic when they contain type variables, while types without them are monomorphic.

Contrary to the type systems used for example in Pascal (1970) or C (1972), which only support monomorphic types, HM is designed with emphasis on parametric polymorphism. The successors of the languages mentioned, like C++ (1985), focused on different types of polymorphism, namely subtyping in connection with object-oriented programming and overloading. While subtyping is incompatible with HM, a variant of systematic overloading is available in the HM-based type system of Haskell.

When extending the type inference for the simply-typed lambda calculus towards polymorphism, one has to define when deriving an instance of a value is admissible. Ideally, this would be allowed with any use of a bound variable, as in:

Unfortunately, type inference in polymorphic lambda calculus is not decidable. Instead, HM provides a "let-polymorphism" of the form

restricting the binding mechanism in an extension of the expression syntax. Only values bound in a let construct are subject to instantiation, i.e. are polymorphic, while the parameters in lambda-abstractions are treated as being monomorphic.

The remainder of the article is more technical as it has to present the HM method as it is handled in the literature. It proceeds as follows:


The same description of the deduction system is used throughout, even for the two algorithms, to make the various forms in which the HM method is presented directly comparable.

The type system can be formally described by syntax rules that fix a language for the expressions, types, etc. The presentation here of such a syntax is not too formal, in that it is written down not to study the surface grammar, but rather the depth grammar, and leaves some syntactical details open. This form of presentation is usual. Building on this, type rules are used to define how expressions and types are related. As before, the form used is a bit liberal.

The expressions to be typed are exactly those of the lambda calculus extended with a let-expression as shown in the adjacent table. Parentheses can be used to disambiguate an expression. The application is left-binding and binds stronger than abstraction or the let-in construct.

Types are syntactically split into two groups, monotypes and polytypes.

Monotypes always designate a particular type. Monotypes formula_4 are syntactically represented as terms.

Examples of monotypes include type constants like formula_5 or formula_6, and parametric types like formula_7. The latter types are examples of "applications" of type functions, for example, from the set
formula_8, 
where the superscript indicates the number of type parameters. The complete set of type functions formula_9 is arbitrary in HM, except that it "must" contain at least formula_10, the type of functions. It is often written in infix notation for convenience. For example, a function mapping integers to strings has type formula_11. Again, parentheses can be used to disambiguate a type expression. The application binds stronger than the infix arrow, which is right-binding.

Type variables are admitted as monotypes. Monotypes are not to be confused with monomorphic types, which exclude variables and allow only ground terms.

Two monotypes are equal if they have identical terms.

"Polytypes" (or "type schemes") are types containing variables bound by one or more for-all quantifiers, e.g. formula_12.

A function with polytype formula_12 can map "any" value of the same type to itself,
and the identity function is a value for this type.

As another example, formula_14 is the type of a function mapping all finite sets to integers. A function which returns the cardinality of a set would be a value of this type.

Quantifiers can only appear top level. For instance, a type formula_15 is excluded by the syntax of types. Also monotypes are included in the polytypes, thus a type has the general form formula_16, where formula_4 is a monotype.

Equality of polytypes is up to reordering the quantification and renaming the quantified variables (formula_18-conversion). Further, quantified variables not occurring in the monotype can be dropped.

To meaningfully bring together the still disjoint parts (syntax expressions and types) a third part is needed: context. Syntactically, a context is a list of pairs formula_19, called assignments, assumptions or bindings, each pair stating that value variable formula_20has type formula_21. All three parts combined give a "typing judgment" of the form formula_22, stating that under assumptions formula_23, the expression formula_24 has type formula_25.

In a type formula_26, the symbol formula_27 is the quantifier binding the type variables formula_28 in the monotype formula_4. The variables formula_28 are called "quantified" and any occurrence of a quantified type variable in formula_4 is called "bound" and all unbound type variables in formula_4 are called "free". Additionally to the quantification formula_27 in polytypes, type variables can also be bound by occurring in the context, but with the inverse effect on the right hand side of the formula_34. Such variables then behave like type constants there. Finally, a type variable may legally occur unbound in a typing, in which case they are implicitly all-quantified.

The presence of both bound and unbound type variables is a bit uncommon in programming languages. Often, all type variables are implicitly treated all-quantified. For instance, one does not have clauses with free variables in Prolog. Likely in Haskell, where all type variables implicitly occur quantified, i.e. a Haskell type codice_4 means formula_12 here. Related and also very uncommon is the binding effect of the right hand side formula_25 of the assignments.

Typically, the mixture of both bound and unbound type variables originate from the use of free variables in an expression. The constant function K = formula_37 provides an example. It has the monotype formula_38. One can force polymorphism by formula_39. Herein, formula_40 has the type formula_41. The free monotype variable formula_18 originates from the type of the variable formula_43 bound in the surrounding scope. formula_44 has the type formula_45. One could imagine the free type variable formula_18 in the type of formula_40 be bound by the formula_48 in the type of formula_44. But such a scoping cannot be expressed in HM. Rather, the binding is realized by the context.

Polymorphism means that one and the same expression can have (perhaps
infinitely) many types. But in this type system, these types are not completely
unrelated, but rather orchestrated by the parametric polymorphism.

As an example, the identity formula_50 can have formula_51 as its type as well as
formula_52 or formula_53 and many others, but not formula_54. The most general type for this function is
formula_55, while the
others are more specific and can be derived from the general one by consistently
replacing another type for the "type parameter", i.e. the quantified
variable formula_18. The counter-example fails because the
replacement is not consistent.

The consistent replacement can be made formal by applying a substitution formula_57 to the term of a type formula_4, written formula_59. As the example suggests, substitution is not only strongly related to an order, that expresses that a type is more or less special, but also with the all-quantification which allows the substitution to be applied.
Formally, in HM, a type formula_25 is 
more general than formula_61, formally formula_62 if some quantified variable in formula_25 is
consistently substituted such that one gains formula_61 as shown in the side bar.
This order is part of the type definition of the type system.

While substituting a monomorphic (ground) type for a quantified variable is
straight forward, substituting a polytype has some pitfalls caused by the
presence of free variables. Most particularly, unbound variables must not be
replaced. They are treated as constants here. Additionally, quantifications can only occur top-level. Substituting a parametric type,
one has to lift its quantors. The table on the right makes the rule precise.

Alternatively, consider an equivalent notation for the polytypes without
quantors in which quantified variables are represented by a different set of
symbols. In such a notation, the specialization reduces to plain consistent
replacement of such variables.

The relation formula_65 is a partial order
and formula_66 is its smallest element.

While specialization of a type scheme is one use of the order, it plays a
crucial second role in the type system. Type inference with polymorphism
faces the challenge of summarizing all possible types an expression may have.
The order guarantees that such a summary exists as the most general type
of the expression.

The type order defined above can be extended to typings because the implied all-quantification of typings enables consistent replacement:
Contrary to the specialisation rule, this is not part of the definition, but like the implicit all-quantification rather a consequence of the type rules defined next.
Free type variables in a typing serve as placeholders for possible refinement. The binding effect of the environment to free type
variables on the right hand side of formula_34 that prohibits their substitution in the specialisation rule is again
that a replacement has to be consistent and would need to include the whole typing.

The syntax of HM is carried forward to the syntax of the inference rules that form the body of the formal system, by using the typings as judgments. Each of the rules define what conclusion could be drawn from what premises. Additionally to the judgments, some extra conditions introduced above might be used as premises, too.

A proof using the rules is a sequence of judgments such that all premises are listed before a conclusion. The examples below show a possible format of proofs. From left to right, each line shows the conclusion, the formula_69 of the rule applied and the premises, either by referring to an earlier line (number) if the premise is a judgment or by making the predicate explicit.

The side box shows the deduction rules of the HM type system. One can roughly divide the rules into two groups:

The first four rules formula_70 (variable or function access), formula_71 ("application", i.e. function call with one parameter), formula_72 ("abstraction", i.e. function declaration) and formula_73 (variable declaration) are centered around the syntax, presenting one rule for each of the expression forms. Their meaning is obvious at the first glance, as they decompose each expression, prove their sub-expressions and finally combine the individual types found in the premises to the type in the conclusion.

The second group is formed by the remaining two rules formula_74 and formula_75.
They handle specialization and generalization of types. While the rule formula_74 should be clear from the section on specialization above, formula_75 complements the former, working in the opposite direction. It allows generalization, i.e. to quantify monotype variables not bound in the context.
The following two examples exercise the rule system in action. Since both the expression and the type are given, they are a type-checking use of the rules.

Example: A proof for formula_78 where formula_79,
could be written

Example: To demonstrate generalization,
formula_81
is shown below:

Not visible immediately, the rule set encodes a regulation under which circumstances a type might be generalized or not by a slightly varying use of mono- and polytypes in the rules formula_72 and formula_73. Remember that formula_25 and formula_4 denote poly- and monotypes respectively.

In rule formula_72, the value variable of the parameter of the function formula_88 is added to the context with a monomorphic type through the premise formula_89, while in the rule formula_73, the variable enters the environment in polymorphic form formula_91. Though in both cases the presence of formula_43 in the context prevents the use of the generalisation rule for any free variable in the assignment, this regulation forces the type of parameter formula_43 in a formula_94-expression to remain monomorphic, while in a let-expression, the variable could be introduced polymorphic, making specializations possible.

As a consequence of this regulation, formula_95 cannot be typed,
since the parameter formula_40 is in a monomorphic position, while formula_97 has type formula_98, because formula_40 has been introduced in a let-expression and is treated polymorphic therefore.

The generalisation rule is also worth for closer look. Here, the all-quantification implicit in the premise formula_100 is simply moved to the right hand side of formula_101 in the conclusion. This is possible, since formula_18 does not occur free in the context. Again, while this makes the generalisation rule plausible, it is not really a consequence. Vis versa, the generalisation rule is part of the definition of HM's type system and the implicit all-quantification a consequence.

Now that the deduction system of HM is at hand, one could present an algorithm and validate it with respect to the rules.
Alternatively, it might be possible to derive it by taking a closer look on how the rules interact and proof are
formed. This is done in the remainder of this article focusing on the possible decisions one can make while proving a typing.

Isolating the points in a proof, where no decision is possible at all,
the first group of rules centered around the syntax leaves no choice since
to each syntactical rule corresponds a unique typing rule, which determines
a part of the proof, while between the conclusion and the premises of these
fixed parts chains of formula_74 and formula_75
could occur. Such a chain could also exist between the conclusion of the
proof and the rule for topmost expression. All proofs must have
the so sketched shape.

Because the only choice in a proof with respect of rule selection are the
formula_74 and formula_75 chains, the
form of the proof suggests the question whether it can be made more precise,
where these chains might be needed. This is in fact possible and leads to a
variant of the rules system with no such rules.

A contemporary treatment of HM uses a purely syntax-directed rule system due to
Clement
as an intermediate step. In this system, the specialization is located directly after the original formula_70 rule
and merged into it, while the generalization becomes part of the formula_73 rule. There the generalization is
also determined to always produce the most general type by introducing the function formula_109, which quantifies
all monotype variables not bound in formula_23.

Formally, to validate, that this new rule system formula_111 is equivalent to the original formula_101, one has
to show that formula_113, which falls apart into two sub-proofs:


While consistency can be seen by decomposing the rules formula_73 and formula_70
of formula_111 into proofs in formula_101, it is likely visible that formula_111 is incomplete, as
one cannot show formula_121 in formula_111, for instance, but only
formula_123. An only slightly weaker version of completeness is provable


implying, one can derive the principal type for an expression in formula_111 allowing us to generalize the proof in the end.

Comparing formula_101 and formula_111, now only monotypes appear in the judgments of all rules. Additionally, the shape of any possible proof with the deduction system is now identical to the shape of the expression (both seen as trees). Thus the expression fully determines the shape of the proof. In formula_101 the shape would likely be determined with respect to all rules except formula_74 and formula_75, which allow building arbitrarily long branches (chains) between the other nodes.

Now that the shape of the proof is known, one is already close to formulating a type inference algorithm.
Because any proof for a given expression must have the same shape, one can assume the monotypes in the
proof's judgements to be undetermined and consider how to determine them.

Here, the substitution (specialisation) order comes into play. Although at the first glance one cannot determine the types locally, the hope is that it is possible to refine them with the help of the order while traversing the proof tree, additionally assuming, because the resulting algorithm is to become an inference method, that the type in any premise will be determined as the best possible. And in fact, one can, as looking at the rules of formula_111 suggests:


The first premise forces the outcome of the inference to be of the form formula_143.

The second premise requires that the inferred type is equal to formula_4 of the first premise. Now there are two possibly different types, perhaps with open type variables, at hand to compare and to make equal if it is possible. If it is, a refinement is found, and if not, a type error is detected again. An effective method is known to "make two terms equal" by substitution, Robinson's Unification in combination with the so-called Union-Find algorithm.

To briefly summarize the union-find algorithm, given the set of all types in a proof, it allows one to group them together into equivalence classes by means of a formula_146
procedure and to pick a representative for each such class using a formula_147 procedure. Emphasizing the word procedure in the sense of side effect, we're clearly leaving the realm of logic in order to prepare an effective algorithm. The representative of a formula_148 is determined such that, if both formula_149 and formula_150 are type variables then the representative is arbitrarily one of them, but while uniting a variable and a term, the term becomes the representative. Assuming an implementation of union-find at hand, one can formulate the unification of two monotypes as follows:

Now having a sketch of an inference algorithm at hand, a more formal presentation is given in the next section. It is described in Milner P. 370 ff. as algorithm J.

The presentation of Algorithm J is a misuse of the notation of logical rules, since it includes side effects but allows a direct comparison with formula_111 while expressing an efficient implementation at the same time. The rules now specify a procedure with parameters formula_152 yielding formula_4 in the conclusion where the execution of the premises proceeds from left to right.

The procedure formula_154 specializes the polytype formula_25 by copying the term and replacing the bound type variables consistently by new monotype variables. 'formula_156' produces a new monotype variable. Likely, formula_109 has to copy the type introducing new variables for the quantification to avoid unwanted captures. Overall, the algorithm now proceeds by always making the most general choice leaving the specialization to the unification, which by itself produces the most general result. As noted above, the final result formula_4 has to be generalized to formula_109 in the end, to gain the most general type for a given expression.

Because the procedures used in the algorithm have nearly O(1) cost, the overall cost of the algorithm is close to linear in the size of the expression for which a type is to be inferred. This is in strong contrast to many other attempts to derive type inference algorithms, which often came out to be NP-hard, if not undecidable with respect to termination. Thus the HM performs as well as the best fully informed type-checking algorithms can. Type-checking here means that an algorithm does not have to find a proof, but only to validate a given one.

Efficiency is slightly reduced because the binding of type variables in the context has to be maintained to allow computation of formula_109 and enable an occurs check to prevent the building of recursive types during formula_161.
An example of such a case is formula_162, for which no type can be derived using HM. Practically, types are only small terms and do not build up expanding structures. Thus, in complexity analysis, one can treat comparing them as a constant, retaining O(1) costs.

In the previous section, while sketching the algorithm its proof was hinted at with metalogical argumentation. While this leads to an efficient algorithm J, it is
not clear whether the algorithm properly reflects the deduction systems D or S
which serve as a semantic base line.

The most critical point in the above argumentation is the refinement of monotype
variables bound by the context. For instance, the algorithm boldly changes the
context while inferring e.g. formula_163,
because the monotype variable added to the context for the parameter formula_40 later needs to be refined
to formula_165 when handling application.
The problem is that the deduction rules do not allow such a refinement.
Arguing that the refined type could have been added earlier instead of the
monotype variable is an expedient at best.

The key to reaching a formally satisfying argument is to properly include
the context within the refinement. Formally,
typing is compatible with substitution of free type variables.

To refine the free variables thus means to refine the whole typing.

From there, a proof of algorithm J leads to algorithm W, which only makes the
side effects imposed by the procedure formula_167 explicit by
expressing its serial composition by means of the substitutions
formula_168. The presentation of algorithm W in the sidebar still makes use of side effects
in the operations set in italic, but these are now limited to generating
fresh symbols. The form of judgement is formula_169,
denoting a function with a context and expression as parameter producing a monotype together with
a substitution. formula_170 is a side-effect free version
of formula_167 producing a substitution which is the most general unifier.

While algorithm W is normally considered to be "the" HM algorithm and is
often directly presented after the rule system in literature, its purpose is
described by Milner on P. 369 as follows:

While he considered W more complicated and less efficient, he presented it 
in his publication before J. It has its merits when side effects are unavailable or unwanted.
By the way, W is also needed to prove completeness, which is factored by him into the soundness proof.

Before formulating the proof obligations, a deviation between the rules systems
D and S and the algorithms presented needs to be emphasized.

While the development above sort of misused the monotypes as "open" proof variables, the possibility that proper monotype variables might be harmed was sidestepped by introducing fresh variables and hoping for the best. But there's a catch: One of the promises made was that these fresh variables would be "kept in mind" as such. This promise is not fulfilled by the algorithm.

Having a context formula_172, the expression formula_173
cannot be typed in either formula_101 or formula_111, but the algorithms come up with
the type formula_176, where W additionally delivers the substitution formula_177,
meaning that the algorithm fails to detect all type errors. This omission can easily be fixed by more carefully distinguishing proof
variables and monotype variables.

The authors were well aware of the problem but decided not to fix it. One might assume a pragmatic reason behind this.
While more properly implementing the type inference would have enabled the algorithm to deal with abstract monotypes,
they were not needed for the intended application where none of the items in a preexisting context have free
variables. In this light, the unneeded complication was dropped in favor of a simpler algorithm.
The remaining downside is that the proof of the algorithm with respect to the rule system is less general and can only be made
for contexts with formula_178 as a side condition.

formula_179

The side condition in the completeness obligation addresses how the deduction may give many types, while the algorithm always produces one. At the same time, the side condition demands that the type inferred is actually the most general.

To properly prove the obligations one needs to strengthen them first to allow activating the substitution lemma threading the substitution formula_180 through formula_111 and formula_182. From there, the proofs are by induction over the expression.

Another proof obligation is the substitution lemma itself, i.e. the substitution of the typing, which finally establishes the all-quantification. The later cannot formally be proven, since no such syntax is at hand.

To make programming practical recursive functions are needed.
A central property of the lambda calculus is that recursive definitions
are not directly available, but can instead be expressed with a fixed point combinator.
But unfortunately, the fixpoint combinator cannot be formulated in a typed version
of the lambda calculus without having a disastrous effect on the system as outlined
below.

The original paper shows recursion can be realized by a combinator
formula_183. A possible recursive definition could thus be formulated as
formula_184.

Alternatively an extension of the expression syntax and an extra typing rule is possible:

where
basically merging formula_72 and formula_73 while including the recursively defined
variables in monotype positions where they occur to the left of the formula_190 but as polytypes to the right of it. This
formulation perhaps best summarizes the essence of let-polymorphism.

While the above is straightforward it does come at a price.

Type theory connects lambda calculus computation and logic.
The easy modification above has effects on both:


Programs in simply typed lambda calculus are guaranteed to always terminate. Moreover, they
are even guaranteed to terminate under any evaluation strategy, be it top down, bottom up, breadth first, whatever. The same is true for expressions that have types in HM. It is well-known that separating
terminating from non-terminating programs is most difficult, and especially in lambda calculus,
which is so expressive that it can formulate recursion with just a few symbols. Thus the initial
inability of HM to provide recursive functions was not an omission, but a feature. Adding
recursion enables normal programming but the guarantee is not longer valid.

Another reading of the typing is given by the Curry–Howard isomorphism. Here
the types are interpreted as logical expressions. Let's look at the type of the fixpoint combinator from this
perspective, assuming the variables to have logical value:
But this is invalid.
Adding an invalid axiom will break the logic in the sense that
every formula can then be shown to be true in it, e.g. formula_192.
Thus the ability to distinguish even two simple things is no longer given. Everything is the same and
collapses into 42. The fixpoint
combinator that came in so handy above also plays a role in Curry's paradox.

Logic aside, does this matter for typing programs? It does. Since one is now able to formulate
non-terminating functions, one can make a function that would return whatever one wants but never really returns:
In practical programming such a function can come in handy when breaking out of a computation,
like with codice_5 in C, while silencing the type checker in
the current branch by returning essentially nothing but with a suitable type.

Less desirable is that the type checker (type inferencer) now succeeds with a type for a function that in fact never returns any value, like formula_194. The function "would" return a value of this type, but it "cannot" because no terminating function with this type exists. The type checker's claim that everything is ok thus has to be taken with a grain of salt. The types might only be "claimed" to be checked, but the program can still be typed wrong. Only if all functions are terminating does formula_18 in the logic above have a "true" value, and the assertions of the type checker become strong again.

Overloading means, that different functions still can be defined and used with the same name. Most programming languages at least provide overloading with the built-in arithmetic operations (+,<,etc.), to allow the programmer to write arithmetic expressions in the same form, even for different numerical types like codice_6 or codice_7. Because a mixture of these different types within the same expression also demands for implicit conversion, overloading especially for these operations is often built into the programming language itself. In some languages, this feature is generalized and made available to the user, e.g. in C++.

While ad-hoc overloading has been avoided in functional programming for the computation costs both in type checking and inference, a means to systematise overloading has been introduced that resembles both in form and naming to object oriented programming, but works one level upwards. "Instances" in this systematic are not objects (i.e. on value level), but rather types.
The quicksort example mentioned in the introduction uses the overloading in the orders, having the following type annotation in Haskell:

Herein, the type codice_8 is not only polymorphic, but also restricted to be an instance of some type class codice_9, that provides the order predicates codice_10 and codice_11 used in the functions body. The proper implementations of these predicates are then passed to quicksorts as additional parameters, as soon as quicksort is used on more concrete types providing a single implementation of the overloaded function quickSort.

Because the "classes" only allow a single type as their argument, the resulting type system can still provide inference. Additionally, the type classes can then be equipped with some kind of overloading order allowing one to arrange the classes as a lattice.

Parametric polymorphism implies that types themselves are passed as parameters as if they were proper values. Passed as arguments into a proper functions as in the introduction, but also into "type functions" as in the "parametric" type constants, leads to the question how to more properly type types themselves. A meta type, the "type of types" would be useful to create an even more expressive type system.

Though this would be a straight forward extension, unfortunately, only unification is not longer decidable in the presence of meta types, rendering type inference impossible in this extend of generality.
Additionally, assuming a type of all types that includes itself as type leads into a paradox, as in the set of all sets, so one must proceed in steps of levels of abstraction.
Research in second order lambda calculus, one step upwards, showed, that type inference is undecidable in this generality.

Parts of one extra level has been introduced into Haskell named kind, where it is used helping to type monads. Kinds are left implicit, working behind the scenes in the inner mechanics of the extended type system.

Attempts to combine subtyping and type inference have caused quite some frustration. While type inference is needed in object-oriented programming for the same reason as in functional languages, methods like HM cannot be made going for this purpose. It is not difficult to set up a type system with subtyping enabling object-oriented style, as e.g. Cardelli


Such objects would be immutable in a functional language context, but the type system would enable object-oriented programming style and the type inference method could be reused in imperative languages.

The subtyping rule for the record types is:
Syntatically, record expressions would have form
and have a type rule leading to the above type.
Such record values could then be used the same way as objects in object-oriented programming.




</doc>
