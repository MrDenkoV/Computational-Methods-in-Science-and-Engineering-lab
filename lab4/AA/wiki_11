<doc id="2917649" url="https://en.wikipedia.org/wiki?curid=2917649" title="Speech">
Speech

Speech is human vocal communication using language. Each language uses phonetic combinations of vowel and consonant sounds that form the sound of its words (that is, all English words sound different from all French words, even if they are the same word, e.g., "role" or "hotel"), and using those words in their semantic character as words in the lexicon of a language according to the syntactic constraints that govern lexical words' function in a sentence. In speaking, speakers perform many different intentional speech acts, e.g., informing, declaring, asking, persuading, directing, and can use enunciation, intonation, degrees of loudness, tempo, and other non-representational or paralinguistic aspects of vocalization to convey meaning. In their speech speakers also unintentionally communicate many aspects of their social position such as sex, age, place of origin (through accent), physical states (alertness and sleepiness, vigor or weakness, health or illness), psychic states (emotions or moods), physico-psychic states (sobriety or drunkenness, normal consciousness and trance states), education or experience, and the like.

Although people ordinarily use speech in dealing with other persons (or animals), when people swear they do not always mean to communicate anything to anyone, and sometimes in expressing urgent emotions or desires they use speech as a quasi-magical cause, as when they encourage a player in a game to do or warn them not to do something. There are also many situations in which people engage in solitary speech. People talk to themselves sometimes in acts that are a development of what some psychologists (e.g., Lev Vygotsky) have maintained is the use in thinking of silent speech in an interior monologue to vivify and organize cognition, sometimes in the momentary adoption of a dual persona as self addressing self as though addressing another person. Solo speech can be used to memorize or to test one's memorization of things, and in prayer or in meditation (e.g., the use of a mantra).

Researchers study many different aspects of speech: speech production and speech perception of the sounds used in a language, speech repetition, speech errors, the ability to map heard spoken words onto the vocalizations needed to recreate them, which plays a key role in children's enlargement of their vocabulary, and what different areas of the human brain, such as Broca's area and Wernicke's area, underlie speech. Speech is the subject of study for linguistics, cognitive science, communication studies, psychology, computer science, speech pathology, otolaryngology, and acoustics. 
Speech compares with written language, which may differ in its vocabulary, syntax, and phonetics from the spoken language, a situation called diglossia.

The evolutionary origins of speech are unknown and subject to much debate and speculation. While animals also communicate using vocalizations, and trained apes such as Washoe and Kanzi can use simple sign language, no animals' vocalizations are articulated phonemically and syntactically, and do not constitute speech.

Speech production is a multi-step process by which thoughts are generated into spoken utterances. Production involves the selection of appropriate words and the appropriate form of those words from the lexicon and morphology, and the organization of those words through the syntax. Then, the phonetic properties of the words are retrieved and the sentence is uttered through the articulations associated with those phonetic properties.

In linguistics (articulatory phonetics), articulation refers to how the tongue, lips, jaw, vocal cords, and other speech organs used to produce sounds are used to make sounds. Speech sounds are categorized by manner of articulation and place of articulation. Place of articulation refers to where the airstream in the mouth is constricted. Manner of articulation refers to the manner in which the speech organs interact, such as how closely the air is restricted, what form of airstream is used (e.g. pulmonic, implosive, ejectives, and clicks), whether or not the vocal cords are vibrating, and whether the nasal cavity is opened to the airstream. The concept is primarily used for the production of consonants, but can be used for vowels in qualities such as voicing and nasalization. For any place of articulation, there may be several manners of articulation, and therefore several homorganic consonants.

Normal human speech is pulmonic, produced with pressure from the lungs, which creates phonation in the glottis in the larynx, which is then modified by the vocal tract and mouth into different vowels and consonants. However humans can pronounce words without the use of the lungs and glottis in alaryngeal speech, of which there are three types: esophageal speech, pharyngeal speech and buccal speech (better known as Donald Duck talk).

Speech production is a complex activity, and as a consequence errors are common, especially in children. Speech errors come in many forms and are often used to provide evidence to support hypotheses about the nature of speech. As a result, speech errors are often used in the construction of models for language production and child language acquisition. For example, the fact that children often make the error of over-regularizing the -ed past tense suffix in English (e.g. saying 'singed' instead of 'sang') shows that the regular forms are acquired earlier. Speech errors associated with certain kinds of aphasia have been used to map certain components of speech onto the brain and see the relation between different aspects of production: for example, the difficulty of expressive aphasia patients in producing regular past-tense verbs, but not irregulars like 'sing-sang' has been used to demonstrate that regular inflected forms of a word are not individually stored in the lexicon, but produced from affixation of the base form.

Speech perception refers to the processes by which humans can interpret and understand the sounds used in language. The study of speech perception is closely linked to the fields of phonetics and phonology in linguistics and cognitive psychology and perception in psychology. Research in speech perception seeks to understand how listeners recognize speech sounds and use this information to understand spoken language. Research into speech perception also has applications in building computer systems that can recognize speech, as well as improving speech recognition for hearing- and language-impaired listeners.

Speech perception is categorical, in that people put the sounds they hear into categories rather than perceiving them as a spectrum. People are more likely to be able to hear differences in sounds across categorical boundaries than within them. A good example of this is voice onset time (VOT). For example, Hebrew speakers, who distinguish voiced /b/ from voiceless /p/, will more easily detect a change in VOT from -10 ( perceived as /b/ ) to 0 ( perceived as /p/ ) than a change in VOT from +10 to +20, or -10 to -20, despite this being an equally large change on the VOT spectrum.

In speech repetition, speech being heard is quickly turned from sensory input into motor instructions needed for its immediate or delayed vocal imitation (in phonological memory). This type of mapping plays a key role in enabling children to expand their spoken vocabulary. Masur (1995) found that how often children repeat novel words versus those they already have in their lexicon is related to the size of their lexicon later on, with young children who repeat more novel words having a larger lexicon later in development. Speech repetition could help facilitate the acquisition of this larger lexicon.

There are several organic and psychological factors that can affect speech. Among these are:


The classical or Wernicke-Geschwind model of the language system in the brain focuses on Broca's area in the inferior prefrontal cortex, and Wernicke's area in the posterior superior temporal gyrus on the dominant hemisphere of the brain (typically the left hemisphere for language). In this model, a linguistic auditory signal is first sent from the auditory cortex to Wernicke's area. The lexicon is accessed in Wernicke's area, and these words are sent via the arcuate fasciculus to Broca's area, where morphology, syntax, and instructions for articulation are generated. This is then sent from Broca's area to the motor cortex for articulation.

Paul Broca identified an approximate region of the brain in 1861 which, when damaged in two of his patients, caused severe deficits in speech production, where his patients were unable to speak beyond a few monosyllabic words. This deficit, known as Broca's or expressive aphasia, is characterized by difficulty in speech production where speech is slow and labored, function words are absent, and syntax is severely impaired, as in telegraphic speech. In expressive aphasia, speech comprehension is generally less affected except in the comprehension of grammatically complex sentences. Wernicke's area is named after Carl Wernicke, who in 1874 proposed a connection between damage to the posterior area of the left superior temporal gyrus and aphasia, as he noted that not all aphasic patients had suffered damage to the prefrontal cortex. Damage to Wernicke's area produces Wernicke's or receptive aphasia, which is characterized by relatively normal syntax and prosody but severe impairment in lexical access, resulting in poor comprehension and nonsensical or jargon speech.

Modern models of the neurological systems behind linguistic comprehension and production recognize the importance of Broca's and Wernicke's areas, but are not limited to them nor solely to the left hemisphere. Instead, multiple streams are involved in speech production and comprehension. Damage to the left lateral sulcus has been connected with difficulty in processing and producing morphology and syntax, while lexical access and comprehension of irregular forms (e.g. eat-ate) remain unaffected.
Moreover, the circuits involved in human speech comprehension dynamically adapt with learning, for example, by becoming more efficient in terms of processing time when listening to familiar messages such as learned verses.





</doc>
<doc id="1034781" url="https://en.wikipedia.org/wiki?curid=1034781" title="Vulgarity">
Vulgarity

Vulgarity is the quality of being common, coarse, or unrefined. This judgement may refer to language, visual art, social class, or social climbers. John Bayley claims the term can never be self-referential, because to be aware of vulgarity is to display a degree of sophistication which thereby elevates the subject above the vulgar.

From the fifteenth to seventeenth centuries, "vulgar" simply described the common language or vernacular of a country. From the mid-seventeenth century onward, it began to take on a pejorative aspect: "having a common and offensively mean character, coarsely commonplace; lacking in refinement or good taste; uncultured; ill bred". 

In the Victorian age, vulgarity broadly described many sorts of activity, such as wearing ostentatious clothing, and other similarly subtle aspects of behavior. In a George Eliot novel, one character could be vulgar for talking about money, a second because he criticizes the first for doing so, and a third for being fooled by the excessive refinement of the second. The effort to avoid vulgar phrasing could leave characters at a loss for words. In George Meredith's "Beauchamp's Career", an heiress does not wish to make the commonplace statement that she is "engaged", nor "betrothed", "affianced", or "plighted". Though such words are not vulgarity in the vulgar sense, they nonetheless could stigmatize the user as a member of a socially inferior class. Even favored euphemisms such as "toilet" eventually become stigmatized like the words they replace (the so-called euphemism treadmill), and currently favored words serve as a sort of "cultural capital".

Vulgarity, in the sense of vulgar speech, can refer to language which is offensive or obscene.

The word most associated with the verbal form of vulgarity is "cursing." However, there are many subsections of vulgar words. American psychologist Timothy Jay classifies "dirty words" because it "allows people interested in language to define the different types of reference or meaning that dirty words employ. One can see that what is considered taboo or obscene revolves around a few dimensions of human experience that there is a logic behind dirty word usage." One of the most commonly used vulgar terms in the English language is "fuck".

Curse words, or cursing, have been recognized by religious organizations as being able to cause actual mental and physical harm. More recently such words have separated themselves from their religious meanings, and it is doubtful that those who use curse words imagine the words will bring actual mental or physical harm. Both parties are aware that the cursing is simply an expression, and the ones receiving the curse words or phrase are aware they are being targeted.

Religious curses generally; depending on the listener, location, and their culture, among other things; include the words "damn", "God", and "hell", while words such as "fuck", "shit", and "bitch" are generally seen as non-religious curses in the western world.


</doc>
<doc id="32977" url="https://en.wikipedia.org/wiki?curid=32977" title="Writing">
Writing

Writing is a medium of human communication that involves the representation of a language with symbols. While not all languages utilize a writing system, those with systems of inscriptions can complement and extend capacities of spoken language by enabling the creation of durable forms of speech that can be transmitted across space (e.g., correspondence) and stored over time (e.g., libraries or other public records). It has also been observed that the activity of writing itself can have knowledge-transforming effects, since it allows humans to externalize their thinking in forms that are easier to reflect on and potentially rework. 

Writing systems are not themselves human languages (with the debatable exception of computer languages) but are means of rendering a language in a readable form. Writing relies on many of the same semantic structures as the speech it represents, such as lexicon and syntax, with the added dependency of a system of symbols to represent that language's phonology and morphology. The result of the activity of writing is called a "text", and the interpreter or activator of this text is called a "reader".

As human societies emerged, collective motivations for the development of writing were driven by pragmatic exigencies like keeping history, maintaining culture, codifying knowledge through curricula and lists of texts deemed to contain foundational knowledge (e.g., "The Canon of Medicine") or artistically exceptional (e.g., a literary canon), organizing and governing societies through the formation of legal systems, census records, contracts, deeds of ownership, taxation, trade agreements, treaties, and so on. For H.G. Wells, writing "made the growth of states larger than the old city states possible. It made a continuous historical consciousness possible. The command of the priest or king and his seal could go far beyond his sight and voice and could survive his death". For example, around the 4th millennium BC, the complexity of trade and administration in Mesopotamia outgrew human memory, and writing became a more dependable method of recording and presenting transactions in a permanent form. In both ancient Egypt and Mesoamerica, on the other hand, writing may have evolved through calendric and political necessities for recording historical and environmental events.

Individual motivations for writing include improvised additional capacity for the limitations of human memory (e.g., to-do lists, recipes, reminders, logbooks, maps, the proper sequence for a complicated task or important ritual), dissemination of ideas (as in an essay, monograph, broadside, petition, or manifesto), imaginative narratives and other forms of storytelling, personal or business correspondence, and lifewriting (e.g., a diary or journal).

The major writing systems—methods of inscription—broadly fall into five categories: logographic, syllabic, alphabetic, featural, and ideographic (symbols for ideas). A sixth category, pictographic, is insufficient to represent language on its own, but often forms the core of logographies.

A logogram is a written character which represents a word or morpheme. A vast number of logograms are needed to write Chinese characters, cuneiform, and Mayan, where a glyph may stand for a morpheme, a syllable, or both—("logoconsonantal" in the case of hieroglyphs). Many logograms have an ideographic component (Chinese "radicals", hieroglyphic "determiners"). For example, in Mayan, the glyph for "fin", pronounced "ka", was also used to represent the syllable "ka" whenever the pronunciation of a logogram needed to be indicated, or when there was no logogram. In Chinese, about 90% of characters are compounds of a semantic (meaning) element called a "radical" with an existing character to indicate the pronunciation, called a "phonetic". However, such phonetic elements complement the logographic elements, rather than vice versa.

The main logographic system in use today is Chinese characters, used with some modification for the various languages or dialects of China, Japan, and sometimes in Korean despite the fact that in South and North Korea, the phonetic Hangul system is mainly used.

A syllabary is a set of written symbols that represent (or approximate) syllables. A glyph in a syllabary typically represents a consonant followed by a vowel, or just a vowel alone, though in some scripts more complex syllables (such as consonant-vowel-consonant, or consonant-consonant-vowel) may have dedicated glyphs. Phonetically related syllables are not so indicated in the script. For instance, the syllable "ka" may look nothing like the syllable "ki", nor will syllables with the same vowels be similar.

Syllabaries are best suited to languages with a relatively simple syllable structure, such as Japanese. Other languages that use syllabic writing include the Linear B script for Mycenaean Greek; Sequoyan, Ndjuka, an English-based creole language of Surinam; and the Vai script of Liberia. Most logographic systems have a strong syllabic component. Ethiopic, though technically an abugida, has fused consonants and vowels together to the point where it is learned as if it were a syllabary.

An alphabet is a set of symbols, each of which represents or historically represented a phoneme of the language. In a perfectly phonological alphabet, the phonemes and letters would correspond perfectly in two directions: a writer could predict the spelling of a word given its pronunciation, and a speaker could predict the pronunciation of a word given its spelling.

As languages often evolve independently of their writing systems, and writing systems have been borrowed for languages they were not designed for, the degree to which letters of an alphabet correspond to phonemes of a language varies greatly from one language to another and even within a single language.

In most of the writing systems of the Middle East, it is usually only the consonants of a word that are written, although vowels may be indicated by the addition of various diacritical marks. Writing systems based primarily on marking the consonant phonemes alone date back to the hieroglyphs of ancient Egypt. Such systems are called "abjads", derived from the Arabic word for "alphabet".

In most of the alphabets of India and Southeast Asia, vowels are indicated through diacritics or modification of the shape of the consonant. These are called "abugidas". Some abugidas, such as Ethiopic and Cree, are learned by children as syllabaries, and so are often called "syllabics". However, unlike true syllabaries, there is not an independent glyph for each syllable.

Sometimes the term "alphabet" is restricted to systems with separate letters for consonants and vowels, such as the Latin alphabet, although abugidas and abjads may also be accepted as alphabets. Because of this use, Greek is often considered to be the first alphabet.

A featural script notates in an internally consistent way the building blocks of the phonemes that make up a language. For instance, all sounds pronounced with the lips ("labial" sounds) may have some element in common. In the Latin alphabet, this is accidentally the case with the letters "b" and "p"; however, labial "m" is completely dissimilar, and the similar-looking "q" and "d" are not labial. In Korean hangul, however, all four labial consonants are based on the same basic element, but in practice, Korean is learned by children as an ordinary alphabet, and the featural elements tend to pass unnoticed.

Another featural script is SignWriting, the most popular writing system for many sign languages, where the shapes and movements of the hands and face are represented iconically. Featural scripts are also common in fictional or invented systems, such as J.R.R. Tolkien's Tengwar.

Historians draw a sharp distinction between prehistory and history, with history defined by the advent of writing. The cave paintings and petroglyphs of prehistoric peoples can be considered precursors of writing, but they are not considered true writing because they did not represent language directly.

Writing systems develop and change based on the needs of the people who use them. Sometimes the shape, orientation, and meaning of individual signs changes over time. By tracing the development of a script, it is possible to learn about the needs of the people who used the script as well as how the script changed over time.

The many tools and writing materials used throughout history include stone tablets, clay tablets, bamboo slats, papyrus, wax tablets, vellum, parchment, paper, copperplate, styluses, quills, ink brushes, pencils, pens, and many styles of lithography. The Incas used knotted cords known as quipu (or khipu) for keeping records.

The typewriter and various forms of word processors have subsequently become widespread writing tools, and various studies have compared the ways in which writers have framed the experience of writing with such tools as compared with the pen or pencil.

A stone slab with 3,000-year-old writing, known as the Cascajal Block, was discovered in the Mexican state of Veracruz and is an example of the oldest script in the Western Hemisphere, preceding the oldest Zapotec writing by approximately 500 years. It is thought to be Olmec.

Of several pre-Columbian scripts in Mesoamerica, the one that appears to have been best developed, and the only one to be deciphered, is the Maya script. The earliest inscription identified as Maya dates to the 3rd century BC. Maya writing used logograms complemented by a set of syllabic glyphs, somewhat similar in function to modern Japanese writing.
In 2001, archaeologists discovered that there was a civilization in Central Asia that used writing c. 2000 BC. An excavation near Ashgabat, the capital of Turkmenistan, revealed an inscription on a piece of stone that was used as a stamp seal.
The earliest surviving examples of writing in China—inscriptions on so-called "oracle bones", tortoise plastrons and ox scapulae used for divination—date from around 1200 BC in the late Shang dynasty. A small number of bronze inscriptions from the same period have also survived.
Historians have found that the type of media used had an effect on what the writing was documenting and how it was used.

In 2003, archaeologists reported discoveries of isolated tortoise-shell carvings dating back to the 7th millennium BC, but whether or not these symbols are related to the characters of the later oracle-bone script is disputed.

The earliest known hieroglyphic date back to the second half of the 4th millennium BC, such as the clay labels of a Predynastic ruler called "Scorpion I" (Naqada IIIA period, c. 32nd century BC) recovered at Abydos (modern Umm el-Qa'ab) in 1998 or the Narmer Palette, dating to c. 3100 BC, and several recent discoveries that may be slightly older, though these glyphs were based on a much older artistic rather than written tradition. The hieroglyphic script was logographic with phonetic adjuncts that included an effective alphabet. The world's oldest deciphered sentence was found on a seal impression found in the tomb of Seth-Peribsen at Umm el-Qa'ab, which dates from the Second Dynasty (28th or 27th century BC). There are around 800 hieroglyphs dating back to the Old Kingdom, Middle Kingdom and New Kingdom Eras. By the Greco-Roman period, there are more than 5,000.

Writing was very important in maintaining the Egyptian empire, and literacy was concentrated among an educated elite of scribes. Only people from certain backgrounds were allowed to train to become scribes, in the service of temple, pharaonic, and military authorities, resulting in only 1 percent of the population that could write. The hieroglyph system was always difficult to learn, but in later centuries was purposely made even more so, as this preserved the scribes' status.

The world's oldest known alphabet appears to have been developed by Canaanite turquoise miners in the Sinai desert around the mid-19th century BC. Around 30 crude inscriptions have been found at a mountainous Egyptian mining site known as Serabit el-Khadem. This site was also home to a temple of Hathor, the "Mistress of turquoise". A later, two line inscription has also been found at Wadi el-Hol in Central Egypt. Based on hieroglyphic prototypes, but also including entirely new symbols, each sign apparently stood for a consonant rather than a word: the basis of an alphabetic system. It was not until the 12th to 9th centuries, however, that the alphabet took hold and became widely used.

Over the centuries, three distinct Elamite scripts developed.
Proto-Elamite is the oldest known writing system from Iran. In use only for a brief time (c. 3200–2900 BC), clay tablets with Proto-Elamite writing have been found at different sites across Iran. The Proto-Elamite script is thought to have developed from early cuneiform (proto-cuneiform). The Proto-Elamite script consists of more than 1,000 signs and is thought to be partly logographic.

Linear Elamite is a writing system attested in a few monumental inscriptions in Iran. It was used for a very brief period during the last quarter of the 3rd millennium BC. It is often claimed that Linear Elamite is a syllabic writing system derived from Proto-Elamite, although this cannot be proven since Linear-Elamite has not been deciphered. Several scholars have attempted to decipher the script, most notably and .

The Elamite cuneiform script was used from about 2500 to 331 BC, and was adapted from the Akkadian cuneiform. The Elamite cuneiform script consisted of about 130 symbols, far fewer than most other cuneiform scripts.

Cretan hieroglyphs are found on artifacts of Crete (early-to-mid-2nd millennium BC, MM I to MM III, overlapping with Linear A from MM IIA at the earliest). Linear B, the writing system of the Mycenaean Greeks, has been deciphered while Linear A has yet to be deciphered. The sequence and the geographical spread of the three overlapping, but distinct writing systems can be summarized as follows (beginning date refers to first attestations, the assumed origins of all scripts lie further back in the past): Cretan hieroglyphs were used in Crete from c. 1625 to 1500 BC; Linear A was used in the Aegean Islands (Kea, Kythera, Melos, Thera), and the Greek mainland (Laconia) from c. 18th century to 1450 BC; and Linear B was used in Crete (Knossos), and mainland (Pylos, Mycenae, Thebes, Tiryns) from c. 1375 to 1200 BC.
Indus script refers to short strings of symbols associated with the Indus Valley Civilization (which spanned modern-day Pakistan and North India) used between 2600 and 1900 BC. In spite of many attempts at decipherments and claims, it is as yet undeciphered. The term 'Indus script' is mainly applied to that used in the mature Harappan phase, which perhaps evolved from a few signs found in early Harappa after 3500 BC, and was followed by the mature Harappan script. The script is written from right to left, and sometimes follows a boustrophedonic style. Since the number of principal signs is about 400–600, midway between typical logographic and syllabic scripts, many scholars accept the script to be logo-syllabic (typically syllabic scripts have about 50–100 signs whereas logographic scripts have a very large number of principal signs). Several scholars maintain that structural analysis indicates that an agglutinative language underlies the script.

While neolithic writing is a current research topic, conventional history assumes that the writing process first evolved from economic necessity in the ancient Near East. Writing most likely began as a consequence of political expansion in ancient cultures, which needed reliable means for transmitting information, maintaining financial accounts, keeping historical records, and similar activities. Around the 4th millennium BC, the complexity of trade and administration outgrew the power of memory, and writing became a more dependable method of recording and presenting transactions in a permanent form.

The invention of the first writing systems is roughly contemporary with the beginning of the Bronze Age of the late 4th millennium BC. The Sumerian archaic cuneiform script and the Egyptian hieroglyphs are generally considered the earliest writing systems, both emerging out of their ancestral proto-literate symbol systems from 3400 to 3200 BC with earliest coherent texts from about 2600 BC. It is generally agreed that Sumerian writing was an independent invention; however, it is debated whether Egyptian writing was developed completely independently of Sumerian, or was a case of cultural diffusion.
Archaeologist Denise Schmandt-Besserat determined the link between previously uncategorized clay "tokens", the oldest of which have been found in the Zagros region of Iran, and the first known writing, Mesopotamian cuneiform. In approximately 8000 BC, the Mesopotamians began using clay tokens to count their agricultural and manufactured goods. Later they began placing these tokens inside large, hollow clay containers (bulla, or globular envelopes) which were then sealed. The quantity of tokens in each container came to be expressed by impressing, on the container's surface, one picture for each instance of the token inside. They next dispensed with the tokens, relying solely on symbols for the tokens, drawn on clay surfaces. To avoid making a picture for each instance of the same object (for example: 100 pictures of a hat to represent 100 hats), they 'counted' the objects by using various small marks. In this way the Sumerians added "a system for enumerating objects to their incipient system of symbols".

The original Mesopotamian writing system was derived around 3200 BC from this method of keeping accounts. By the end of the 4th millennium BC, the Mesopotamians were using a triangular-shaped stylus pressed into soft clay to record numbers. This system was gradually augmented with using a sharp stylus to indicate what was being counted by means of pictographs. Round-stylus and sharp-stylus writing was gradually replaced by writing using a wedge-shaped stylus (hence the term cuneiform), at first only for logograms, but by the 29th century BC also for phonetic elements. Around 2700 BC, cuneiform began to represent syllables of spoken Sumerian. About that time, Mesopotamian cuneiform became a general purpose writing system for logograms, syllables, and numbers. This script was adapted to another Mesopotamian language, the East Semitic Akkadian (Assyrian and Babylonian) around 2600 BC, and then to others such as Elamite, Hattian, Hurrian and Hittite. Scripts similar in appearance to this writing system include those for Ugaritic and Old Persian. With the adoption of Aramaic as the 'lingua franca' of the Neo-Assyrian Empire (911–609 BC), Old Aramaic was also adapted to Mesopotamian cuneiform. The last cuneiform scripts in Akkadian discovered thus far date from the 1st century AD.
The Proto-Sinaitic script, in which Proto-Canaanite is believed to have been first written, is attested as far back as the 19th century BC. The Phoenician writing system was adapted from the Proto-Canaanite script sometime before the 14th century BC, which in turn borrowed principles of representing phonetic information from Egyptian hieroglyphs. This writing system was an odd sort of syllabary in which only consonants are represented. This script was adapted by the Greeks, who adapted certain consonantal signs to represent their vowels. The Cumae alphabet, a variant of the early Greek alphabet, gave rise to the Etruscan alphabet and its own descendants, such as the Latin alphabet and Runes. Other descendants from the Greek alphabet include Cyrillic, used to write Bulgarian, Russian and Serbian, among others. The Phoenician system was also adapted into the Aramaic script, from which the Hebrew and the Arabic scripts are descended.

The Tifinagh script (Berber languages) is descended from the Libyco-Berber script, which is assumed to be of Phoenician origin.

In many parts of the world, writing has become an even more important part of daily life as digital technologies have helped connect individuals from across the globe through systems such as e-mail and social media. Such technologies have brought substantial amounts of routine reading and writing into most modern workplaces. In the United States, for example, the ability to read and write is necessary for most jobs, and multiple programs are in place to aid both children and adults in improving their literacy skills. For example, the emergence of the writing center and community-wide literacy councils aim to help students and community members sharpen their writing skills. These resources, and many more, span across different age groups in order to offer each individual a better understanding of their language and how to express themselves via writing in order to perhaps improve their socioeconomic status.

Other parts of the world have seen an increase in writing abilities as a result of programs such as the World Literacy Foundation and International Literacy Foundation, as well as a general push for increased global communication.




</doc>
<doc id="60316903" url="https://en.wikipedia.org/wiki?curid=60316903" title="Symbolic language (art)">
Symbolic language (art)

In art, symbolic language refers to the use of characters or images to represent concepts. Symbolic language in art uses imagery to communicate meaning by displaying an accessible concept, the "signifier", to represent a signified "concept".

Symbolic language in art may be used figuratively, to reference ideas and "convey concepts in terms of images", as when images and positioning of objects such as flowers or animals are used to signify cultural concepts. . 

Symbolic language in art may be used more literally, as in floriography, where arrangements of flowers are decoded with the help of special dictionaries, enabling communication of secret, unspoken information as a form of cryptography.

Similarly, in religious iconography, symbolic languages may be developed to communicate between believers in a hostile environment, with progressive teachings providing increasing access to hidden meanings in the images.




</doc>
<doc id="25572785" url="https://en.wikipedia.org/wiki?curid=25572785" title="Kuuk Thaayorre language">
Kuuk Thaayorre language

Kuuk Thaayorre (Thayore) is a Paman language spoken in the settlement Pormpuraaw on the western part of the Cape York Peninsula, Queensland in Australia by the Thaayorre people. As of 2006, 250 of the 350 ethnic Thaayorre speak the language. It is in a robust position compared to many indigenous Australian languages, as it is still being acquired by children and used in daily interaction.

It is closely related to the Ogh-Undjan and more distantly related to the Uw languages, Uw Olkola. Kuuk Yak is either a dialect or closely related.

Speakers of the Kuuk Thaayorre language are able to recall the names of a couple of dialects, such as "Kuuk Thaayunth", "Kuuk Thayem" and "Kuuk Thanon", but today there is only little dialectal difference and the language is becoming more and more uniform as the number of speakers goes down. The so-called Kuuk Yak language may be a dialect of Kuuk Thaayorre, but may be a closely related language as well. Barry Alpher is currently trying to document the language in order to understand its genetic affiliation.

As with many other Australian languages, there is a long list of alternative spellings of "Kuuk Thaayorre". The name itself, "Kuuk Thaayorre", means "the Thaayorre" language in the language itself, "kuuk" meaning "language" and "Thaayorre" being their ethnonym.

Other names include "Kuuk Thaayoore", "Kuktayor", "Kukudayore", "Gugudayor", "Koko Daiyuri", "Koko Taiyor", "Kokkotaijari", "Kokotaiyari", "Thayorre", "Thaayore", "Thayore", "Tayore", "Taior", "Taiol", "Da:jor" and "Behran".

Kuuk Thaayorre has 5 vowels:

All of the vowels above have a long counterpart. In addition, one of the rhotics may be syllabic.

Kuuk Thaayorre has 16 consonants:

The maximal syllable structure is CVCCC, and four-consonant clusters are not uncommon. Unusually, sequences of consecutive /r/ and /ɻ/ are licit.

Unlike in many Australian languages, monosyllables of all word classes are frequent in Kuuk Thaayorre.

A clause in Kuuk Thaayorre can be as small as a single predicate constituent. Any arguments that a predicate subcategorises for can be omitted. Predicating constituents include verbs, adjectives, nouns, demonstrative pronouns, and locative adverbs.

Kuuk Thaayorre is on the whole a nonconfigurational language at the level of the clause, although for complex clauses there are constraints on the ordering of the main clause and the dependent clause. Within a clause noun phrases have intricate structure.

The irregular form of the ergative morpheme makes it a clear suffix, rather than an enclitic; however, it is borne on the last nominal in the noun phrase. This makes Kuuk Thaayorre an example of a language displaying affixation to phrases. Ergative marking has the pragmatic function of displaying the degree of expectedness of the subject.

There are multiple "inclusory" constructions, i.e. those referring to a superset while simultaneously focussing on a subset (these are found in many IE languages, e.g. German "Wit Scilling" 1.du Sc. "Sc. and I"). One of these is a set of single-word inclusory pronouns encoding both superset and subset.

Kuuk Thaayorre is noted for its thoroughgoing use of sixteen words for absolute cardinal directions instead of words with relative senses ("ahead", "left", etc.) as is familiar in most languages. Speakers of Kuuk Thaayorre show a correspondingly greater skill in navigational ability than speakers of languages like English, and always know the exact direction of their facing. When asked to arrange a sequence of pictures in temporal order, speakers consistently arrange them so that time runs east to west, regardless of their own bodily orientation. They are also able to point to cardinal directions with very high accuracy.


</doc>
<doc id="26860" url="https://en.wikipedia.org/wiki?curid=26860" title="Syntax">
Syntax

In linguistics, syntax () is the set of rules, principles, and processes that govern the structure of sentences (sentence structure) in a given language, usually including word order. The term "syntax" is also used to refer to the study of such principles and processes. The goal of many syntacticians is to discover the syntactic rules common to all languages.

The word "syntax" comes from Ancient Greek: "coordination", which consists of "syn", "together", and "táxis", "an ordering".

One basic description of a language's syntax is the sequence in which the subject (S), verb (V), and object (O) usually appear in sentences. Over 85% of languages usually place the subject first, either in the sequence SVO or the sequence SOV. The other possible sequences are VSO, VOS, OVS, and OSV, the last three of which are rare. In most generative theories of syntax, these surface differences arise from a more complex clausal phrase structure, and each order may be compatible with multiple derivations.

The "Aṣṭādhyāyī" of Pāṇini (c. 4th century BC in Ancient India), is often cited as an example of a premodern work that approaches the sophistication of a modern syntactic theory (as works on grammar were written long before modern syntax came about). In the West, the school of thought that came to be known as "traditional grammar" began with the work of Dionysius Thrax.

For centuries, a framework known as (first expounded in 1660 by Antoine Arnauld in a book of the same title) dominated work in syntax: as its basic premise the assumption that language is a direct reflection of thought processes and therefore there is a single, most natural way to express a thought.

However, in the 19th century, with the development of historical-comparative linguistics, linguists began to realize the sheer diversity of human language and to question fundamental assumptions about the relationship between language and logic. It became apparent that there was no such thing as the most natural way to express a thought, and therefore logic could no longer be relied upon as a basis for studying the structure of language.

The Port-Royal grammar modeled the study of syntax upon that of logic. (Indeed, large parts of the Port-Royal Logic were copied or adapted from the "Grammaire générale".) Syntactic categories were identified with logical ones, and all sentences were analyzed in terms of "subject – copula – predicate". Initially, this view was adopted even by the early comparative linguists such as Franz Bopp.

The central role of syntax within theoretical linguistics became clear only in the 20th century, which could reasonably be called the "century of syntactic theory" as far as linguistics is concerned. (For a detailed and critical survey of the history of syntax in the last two centuries, see the monumental work by Giorgio Graffi (2001).)

There are a number of theoretical approaches to the discipline of syntax. One school of thought, founded in the works of Derek Bickerton, sees syntax as a branch of biology, since it conceives of syntax as the study of linguistic knowledge as embodied in the human mind. Other linguists (e.g., Gerald Gazdar) take a more Platonistic view, since they regard syntax to be the study of an abstract formal system. Yet others (e.g., Joseph Greenberg) consider syntax a taxonomical device to reach broad generalizations across languages.

Dependency grammar is an approach to sentence structure where syntactic units are arranged according to the dependency relation, as opposed to the constituency relation of phrase structure grammars. Dependencies are directed links between words. The (finite) verb is seen as the root of all clause structure and all the other words in the clause are either directly or indirectly dependent on this root. Some prominent dependency-based theories of syntax are:


Lucien Tesnière (1893–1954) is widely seen as the father of modern dependency-based theories of syntax and grammar. He argued vehemently against the binary division of the clause into subject and predicate that is associated with the grammars of his day (S → NP VP) and which remains at the core of most phrase structure grammars. In the place of this division, he positioned the verb as the root of all clause structure.

Categorial grammar is an approach that attributes the syntactic structure not to rules of grammar, but to the properties of the syntactic categories themselves. For example, rather than asserting that sentences are constructed by a rule that combines a noun phrase (NP) and a verb phrase (VP) (e.g., the phrase structure rule S → NP VP), in categorial grammar, such principles are embedded in the category of the head word itself. So the syntactic category for an intransitive verb is a complex formula representing the fact that the verb acts as a function word requiring an NP as an input and produces a sentence level structure as an output. This complex category is notated as (NP\S) instead of V. NP\S is read as "a category that searches to the left (indicated by \) for an NP (the element on the left) and outputs a sentence (the element on the right)." The category of transitive verb is defined as an element that requires two NPs (its subject and its direct object) to form a sentence. This is notated as (NP/(NP\S)) which means "a category that searches to the right (indicated by /) for an NP (the object), and generates a function (equivalent to the VP) which is (NP\S), which in turn represents a function that searches to the left for an NP and produces a sentence."

Tree-adjoining grammar is a categorial grammar that adds in partial tree structures to the categories.

Theoretical approaches to syntax that are based upon probability theory are known as stochastic grammars. One common implementation of such an approach makes use of a neural network or connectionism.

Functionalist models of grammar study the form–function interaction by performing a structural and a functional analysis.


The hypothesis of generative grammar is that language is a biological structure. The difference between structural–functional and generative models is that, in generative grammar, the object is placed into the verb phrase. Generative grammar is meant to be used to describe all human language and to predict whether any given utterance in a hypothetical language would sound correct to a speaker of that language (versus constructions which no human language would use). This approach to language was pioneered by Noam Chomsky. Most generative theories (although not all of them) assume that syntax is based upon the constituent structure of sentences. Generative grammars are among the theories that focus primarily on the form of a sentence, rather than its communicative function.

Among the many generative theories of linguistics, the Chomskyan theories are:

Other theories that find their origin in the generative paradigm are:

The Cognitive Linguistics framework stems from generative grammar, but adheres to evolutionary rather than Chomskyan linguistics. Cognitive models often recognise the generative assumption that the object belongs to the verb phrase. Cognitive frameworks include:







</doc>
<doc id="208041" url="https://en.wikipedia.org/wiki?curid=208041" title="Franglais">
Franglais

Franglais (; also Frenglish ) is a French blend referring initially to the pretentious overuse of English words by French-speakers, and afterwards, to diglossia or the macaronic mixture of the French () and English () languages.

The word "Franglais" was first attested in French in 1959, but it was popularised by the academic, novelist, and critic René Étiemble in his denunciation of the overuse of English words in French, "Parlez-vous franglais?" published in 1964. Earlier than the French term was the English label "Frenglish" first recorded in 1937. Other colloquial blends for French influenced English include: "Franglish" (recorded from 1967), "Frenchlish" (1974), and "Fringlish" (1982).

In English, "Franglais" means a combination of English and French. It evokes the linguistic concepts of mixed language and barbarism. Reasons for this blend could be caused by lexical gaps, native bilingualism, populations trying to imitate a language where they have no fluency (sometimes known as creoles/pidgins), or humorous intent. Franglais usually consists of either filling in gaps in one's knowledge of French with English words, using false friends, or speaking French which (although ostensibly "French") would not be understood by a French speaker who does not also have a knowledge of English (for example, by using a literal translation of English idiomatic phrases).

Some examples of Franglais are:

Franglais may also mean a diplomatic compromise, such as the abbreviation "UTC" for Coordinated Universal Time.

Chaucer's Prioress knew nothing of the French of Paris, but only that of Stratford-atte-Bow ('Cockney French'). Similar mixtures occur in the later stages of Law French, such as the famous defendant who "ject un brickbat a le dit Justice, que narrowly mist" ("threw a at the said Justice, which narrowly missed").

Another example in English literature is found in "Henry V" by William Shakespeare. In Act 3, Scene 4, a French princess is trying to learn English, but unfortunately, "foot" as pronounced by her maid sounds too much like (vulgar French for 'semen', or 'to have sexual intercourse' when used as a verb) and "gown" like (French for 'cunt', also used to mean 'idiot'). She decides that English is too obscene.

A literary example of the delight in occurs in Robert Surtees' "Jorrocks' Jaunts and Jollities":

The 19th-century American writer Mark Twain, in "Innocents Abroad", included the following letter to a Parisian landlord:
According to Chapman Pincher, one of Winston Churchill's family recounted how the latter, in response to obstinacy from General de Gaulle in a meeting during de Gaulle's wartime exile in London, told him, "Si vous m’opposerez je vous get riderai!"

The humourist Miles Kington wrote a regular column "Let's Parler Franglais" which was published in the British magazine "Punch" in the late 1970s. These columns were collected into a series of books: "Let's Parler Franglais", "Let's Parler Franglais Again!", "Parlez-vous Franglais?", "Let's Parler Franglais One More Temps", "The Franglais Lieutenant's Woman and Other Literary Masterpieces".

A somewhat different tack was taken in Luis van Rooten's "Mots d'Heures: Gousses, Rames: The D'Antin Manuscript". Here, English nursery rhymes are written with nonsensical French phrases meant to recall the sounds of the English words, and the resulting French texts are presented as a historical manuscript and given a pseudo-learned commentary.

Another classic is Jean Loup Chiflet's "Sky My Husband! Ciel Mon Mari!" which is a literal translation of French into English. However, in this context, the correct translation of is 'heavens...!'

In Monty Python's 1975 movie "Monty Python and the Holy Grail", the French castle guard (John Cleese) orders, when King Arthur (Graham Chapman) doesn't want to go away, his fellow guards to "Fetchez la vache.". The other French guards respond with " and he repeats "Fetchez la vache!"". The guards finally get it: fetch ('the cow'), which they then catapult at the Britons.

In French, refers to the use of English words sometimes deemed unwelcome borrowings or bad slang. An example would be (also ), which is used in many French dialects which have no synonym; however Canadians would use ' ('the end of the week') instead, although in France refers to the end of the work week, i.e. Thursday and Friday. also refers to nouns coined from Anglo-Saxon roots or from recent English loanwords (themselves not always Anglo-Saxon in origin), often by adding "-ing" at the end of a popular word—e.g., ('a car park or parking lot' is alternatively ' in Canadian French, although means 'the action of parking or the state of being parked' in European French); ('a campsite'); and ('shampoo', but pronounced , not ), which has been standardized and has appeared on many French hair-care product labels since at least the 1960s. A few words which have entered French are derived from English roots but are not found at all in English, such as ('a makeover'), and ('a rugby player'). Others are based on misunderstandings of English words, e.g.: "un " meaning 'a jog or a run' rather than 'a pediment'; meaning 'a tram', not 'a tram-track'. Still others are based on misapprehensions of English punctuation, e.g. (with the apostrophe in both singular and plural) meaning 'a lapel pin'; or word order, e.g. meaning 'a walkie-talkie' (hand-held, two-way radio). For those who do not speak English, such words may be believed to exist as such in English. However, in Canada, where both English and French are spoken, expressions such as and are not used.)

Some examples of Franglais are in fact imagined or examples of words being adopted from one language into another in the opposite direction of what many people believe. People who have no linguistic training or don't bother to consult dictionaries tend to create and perpetuate such urban legends about Franglais. For example, many numismatists think that the French spelling of the English term piedfort results from an imagined reintroduction of an English misspelling. In fact, the spelling "piéfort" is found in French dictionaries as an alternative of and even as the only spelling given in the 1932-1935 edition of the Dictionnaire de l'Académie française. and the etymology derived by professional linguists and shown in these dictionaries shows the change in spelling happened within French.

Owing to the worldwide popularity of the Internet, relatively new English words have been introduced into French (e.g. and , referring to either e-mail or an e-mail address). An equivalent for the English word "e-mail" derived from French roots was coined in Quebec French and promoted by Quebec government: "" (from ), and this term is now widely used there. The Académie française has also suggested the use of the abbreviation (from ) as an analogy with the abbreviation for 'telephone', to be used before an e-mail address; however the term, which roughly approximates the English pronunciation of "mail", is now used more broadly in France than that prescribed usage. Another example from French is the word . The equivalent of the English verb "to look at" in French is but the noun "a look" (i.e. the way that something looks or is styled) has become in French, such that the sentence "This Pepsi can has a new look" in French would be "".

After World War II, a backlash began in France over the growing use of English there. "Corruption of the national language" was perceived by some to be tantamount to an attack on the identity of the country itself. During this period, ever greater imports of American products led to the increasingly widespread use of some English phrases in French. Measures taken to slow this trend included government censorship of comic strips and financial support for the French film and French-language dubbing industries. Despite public policies against the spread of English, Franglais is gaining popularity in both writing and speaking.

In recent years, English expressions are increasingly present in French mass media:

Most telecommunication and Internet service providers use English and Franglais expressions in their product names and advertising campaigns. The leading operator, France Télécom, has dropped the accents in its corporate logo. In recent years, it has changed its product names with trendier expressions such as Business Talk, Live-Zoom, Family Talk. France Télécom's mobile telecommunications subsidiary Orange SA runs a franchise retail network called "mobistores". Its Internet subsidiary, formerly known as Wanadoo (inspired by the American slang expression "wanna do") provides a popular triple play service through its "Livebox" cable modem. The second-largest Internet service provider in France is Free, which offers its "freebox". Set-top boxes that are offered by many other providers are also following this trend (e.g. Neuf-box, Alice-box, etc.) and the word "box" by itself is gradually ending up referring to these set-top boxes.

SNCF, the state-owned railway company, has recently introduced a customer fidelity program called S'Miles. Meanwhile, Air France has renamed its Fréquence Plus frequent flyer program to Flying Blue. The Paris transportation authority RATP has also recently introduced a contactless smartcard ticketing system (like the Oyster card in London) called NaviGO.

Public authorities such as the Académie française and the Conseil supérieur de la langue française generally propose alternative words for anglicisms. The acceptance of such words varies considerably; for example, and existed before the English words "computer" and "software" reached France, so they are accepted (even outside France in the case of ). On the other hand, failed to replace "weekend" or "" (the latter being in current usage in Canada). The word "courriel", equivalent of "e-mail", coined and used in French-speaking Canada is gaining popularity in written European French. However, most French Internet users generally speak about "mail" without the prefix "e-". Note that English words are often shorter, and they are usually coined first (the French alternatives are generally thought of only after the original word has already been coined, and then they are debated at length before coming into use). This is partly why they tend to stay in use.

Alternative words proposed by the Académie française are sometimes poorly received by an aware (often technical) audience and unclear to a non-technical audience. The proposed terms may be ambiguous (often because they are coined based on phonetics, thus hiding their etymology) which results in nonsense (e.g. for CD-RW (literally 'rewritable CD-ROMs', despite "ROM" meaning 'read-only memory'). Some words are considered uncool (for example, adding the initial "t-" to "chat" to form (in accordance with French phonetics) or rendering "DVD" as (reproducing the French pronunciation of the letters in the initialism).

The use of English expressions is very common in the youth language, which combines them with wordplay. The letter "j" is thus sometimes humorously pronounced as in English in words such as ('youth'), rendered as /dʒœns/ and thus written , to refer to this trend.

Quebec is the only French majority province in Canada and the only "de jure" (but not "de facto") monolingual jurisdiction. New Brunswick is officially bilingual, and the other provinces, while mostly English-speaking, are not officially English-only.

When a speaker uses calques and loanwords in speech which includes English or French words and grammatical structures in a combination, it is sometimes referred to as Franglais, or a mixed language. The Montreal Gazette has examined this so-called "linguistic mosaic".

Quebec French has longstanding borrowings from English due to the historical coexistence of two speech communities within Quebec (and especially around Montreal). Likewise, Quebec English, the language of the English-speaking minority, has borrowed many French words such as "dépanneur" ('convenience store'), "autoroute" ('highway'), "stage" ('internship'), "circular" ('flyer', from the word , a circulated pamphlet), and many others . These are permanent and longstanding features of local usage, rather than the recent slangish improvisation by any speaker or affinity group with poor knowledge of the other language.
Some words are attributed to what is called Joual (French pronunciation: [ʒwal]): the name given by some to linguistic features of what is known as basilectal dialect of French when it is placed on Post-creole continuum.

These expressions have mainly become part of a common tongue/ born out of mutual concession to each other. In fact, the substantial bilingual community in and around Montreal will occasionally refer to Franglais, usually after it is pointed out by an observer that someone has used various French and English words, expressions or propositions in the same sentence, a surprisingly common occurrence in various spoken registers.

Canadian French is French as it is spoken in Canada. Scholars debate to what extent language mixture can be distinguished from other mechanisms, such as code-switching, substrata, or lexical borrowing. A mixed language arises in a population which is fluent in both languages.

The word "Franglais" refers to the long-standing and stable mixes of English and French spoken in some towns, cities, and rural areas of other Canadian provinces: New Brunswick, Nova Scotia, Ontario, Manitoba, and Newfoundland. Such mixing is used in the northern regions of Maine (U.S.) (see Chiac and Acadian French). It has been asserted that this mix uses approximately equal proportions of each language (except in Newfoundland), although it is more likely to be understood by a French-speaker, since it usually uses English words in French pronunciation and grammar.

Franglais is commonly spoken in French immersion schools in Ontario and Alberta, as well as in DSFM (Division scolaire Franco-Manitobaine) schools in Manitoba, where most students speak either French or English as their first or preferred language, yet they know school-related terms in French specifically (e.g. "Let's go to la bibliothèque", instead of "Let's go to the library"). As many French immersion classes and French schools have a strict "French-only" policy, such Franglais is used discreetly between students, or out of class.

Because of bilingual product packaging, speakers and readers may form new pronunciations that become terms. For example, someone may pronounce the words on a package of strong cheddar and call it "old fort".

Franglais, in the sense of mistaken usage by second-language speakers, occurs across Canada. An example of an anglicism turned Franglais is the mistranslation of English phrases into French by students unaware of the Canadian French term. For example, a hot dog is sometimes called "un chien chaud" when the French term is simply . (However, the Quebec government has itself promoted using expressions such as for 'hot dog', and for 'hamburger', neither of which has gained widespread acceptance.) In some ways, confusion over which expression is more correct, and the emphasis that many immersion schools place on eliminating anglicisms from students' vocabulary, has promoted the use of Franglais. Franglais can also slowly creep into use from mispronunciation and misspelling by many bilingual Canadians. Common mistakes that immersion or bilingual students propagate include incorrect inflection and stresses on syllables, incorrect doubling of consonants, strange vowel combinations in their spelling and using combinations of prefixes and suffixes from English.

Recently, Canadian youth culture (especially in British Columbia and southeastern Ontario) purposely uses Franglais for its comical or euphemistic characteristics, for example, in replacing English swear words with French ones. Some English-speaking Canadians euphemistically use the (i.e., religious words such as as expletives) rather than swearing in English.

There is a particular form of Franglish which consists in the adoption of English words with alternative meanings to their usage in actual English.

These are words like ('a scramble', 'a rush', 'a strong effort'), or ('a tan', 'the act of sunbathing'), made by adding the English ending "-ing" to a verb from French (e.g. 'to force' or 'to tan') to form a new noun. These are slang or informal at best, and not widely accepted.

Another type of false anglicism comes from the shortening of an English name, keeping only the first word (while the important word for English speakers is the last word, impossible to remove). For example, to designate a dress suit, the word is used by the French (and some other languages), borrowed ultimately English 'smoking jacket'. Yet the British use "dinner jacket" and Americans "tuxedo" (or "tux"); "smoking" does not exist in English other than as a form of the verb "to smoke", and as the gerund "smoking", referring to the act of smoking something or giving off smoke. Another example is the French term for 'clapperboard' as used in filmmaking.

They are either French constructions mimicking the English rules, or shifts of meaning which affect borrowings.

Cameroon has substantial English and French-speaking populations as a legacy of its colonial past as British Southern Cameroons and French Cameroun. Despite linguistically segregated education since independence, many younger Cameroonians in urban centres have formed a version of Franglais/Franglish from English, French and Cameroonian Pidgin English known as Camfranglais or Frananglais. Many educational authorities disapprove of Frananglais in Cameroon and have banned it in their schools. Nevertheless, the youth-culture argot has gained in popularity and has a growing music scene.

Franglais is spoken in London, due to its population of French speakers from France, Africa, Southeast Asia, and the Caribbean.

Franglais also thrives in communities where imperfect English–French bilingualism is common. The United Nations Office at Geneva is so named in an imitation of the French , rather than the expected ""in" Geneva".

Another example is provided by the civil servants in European Union institutions (European Parliament, European Commission, European Court of Justice), based in bilingual Brussels (French and Dutch) and Luxembourg City (Luxembourgish and German). They often work in English, but are surrounded by a French-speaking environment, which influences their English (e.g. "I'm a stagiaire at the Commission and I'm looking for another stage in a consultancy", referring to internships).





</doc>
<doc id="50594391" url="https://en.wikipedia.org/wiki?curid=50594391" title="Polyglossia">
Polyglossia

Polyglossia (pronunciation: /ˌpɒlɪˈɡlɒsɪə/)is a noun that refers to the coexistence of multiple languages (or distinct varieties of the same language) in one society or area. It was first used in the 1970s in the Academic journal "International Migration Review".
Polyglossia is a useful term for describing situations where more than two distinct varieties are used for clearly distinct purposes.



</doc>
<doc id="12563101" url="https://en.wikipedia.org/wiki?curid=12563101" title="Speech production">
Speech production

Speech production is the process by which thoughts are translated into speech. This includes the selection of words, the organization of relevant grammatical forms, and then the articulation of the resulting sounds by the motor system using the vocal apparatus. Speech production can be spontaneous such as when a person creates the words of a conversation, reactive such as when they name a picture or read aloud a written word, or imitative, such as in speech repetition. Speech production is not the same as language production since language can also be produced manually by signs.

In ordinary fluent conversation people pronounce roughly four syllables, ten or twelve phonemes and two to three words out of their vocabulary (that can contain 10 to 100 thousand words) each second. Errors in speech production are relatively rare occurring at a rate of about once in every 900 words in spontaneous speech. Words that are commonly spoken or learned early in life or easily imagined are quicker to say than ones that are rarely said, learnt later in life, or are abstract.

Normally speech is created with pulmonary pressure provided by the lungs that generates sound by phonation through the glottis in the larynx that then is modified by the vocal tract into different vowels and consonants. However speech production can occur without the use of the lungs and glottis in alaryngeal speech by using the upper parts of the vocal tract. An example of such alaryngeal speech is Donald Duck talk.

The vocal production of speech may be associated with the production of hand gestures that act to enhance the comprehensibility of what is being said.

The development of speech production throughout an individual's life starts from an infant's first babble and is transformed into fully developed speech by the age of five. The first stage of speech doesn't occur until around age one (holophrastic phase). Between the ages of one and a half and two and a half the infant can produce short sentences (telegraphic phase). After two and a half years the infant develops systems of lemmas used in speech production. Around four or five the child's lemmas are largely increased, this enhances the child's production of correct speech and they can now produce speech like an adult. An adult now develops speech in four stages: Activation of lexical concepts, select lemmas needed, morphologically and phonologically encode speech, and the word is phonetically encoded.

The production of spoken language involves three major levels of processing: conceptualization, formulation, and articulation.

The first is the processes of conceptualization or conceptual preparation, in which the intention to create speech links a desired concept to the particular spoken words to be expressed. Here the preverbal intended messages are formulated that specify the concepts to be expressed.

The second stage is formulation in which the linguistic form required for the expression of the desired message is created. Formulation includes grammatical encoding, morpho-phonological encoding, and phonetic encoding. Grammatical encoding is the process of selecting the appropriate syntactic word or lemma. The selected lemma then activates the appropriate syntactic frame for the conceptualized message. Morpho-phonological encoding is the process of breaking words down into syllables to be produced in overt speech. Syllabification is dependent on the preceding and proceeding words, for instance:
"I-com-pre-hend" vs. "I-com-pre-hen-dit".
The final part of the formulation stage is phonetic encoding. This involves the activation of articulatory gestures dependent on the syllables selected in the morpho-phonological process, creating an articulatory score as the utterance is pieced together and the order of movements of the vocal apparatus is completed.

The third stage of speech production is articulation, which is the execution of the articulatory score by the lungs, glottis, larynx, tongue, lips, jaw and other parts of the vocal apparatus resulting in speech.

The motor control for speech production in right handed people depends mostly upon areas in the left cerebral hemisphere. These areas include the bilateral supplementary motor area, the left posterior inferior frontal gyrus, the left insula, the left primary motor cortex and temporal cortex. There are also subcortical areas involved such as the basal ganglia and cerebellum. The cerebellum aids the sequencing of speech syllables into fast, smooth and rhythmically organized words and longer utterances.

Speech production can be affected by several disorders:



Until the late 1960s research on speech was focused on comprehension. As researchers collected greater volumes of speech error data, they began to investigate the psychological processes responsible for the production of speech sounds and to contemplate possible processes for fluent speech. Findings from speech error research were soon incorporated into speech production models. Evidence from speech error data supports the following conclusions about speech production.

Some of these ideas include:


Models of speech production must contain specific elements to be viable. These include the elements from which speech is composed, listed below. The accepted models of speech production discussed in more detail below all incorporate these stages either explicitly or implicitly, and the ones that are now outdated or disputed have been criticized for overlooking one or more of the following stages.

The attributes of accepted speech models are:

a) a conceptual stage where the speaker abstractly identifies what they wish to express.

b) a syntactic stage where a frame is chosen that words will be placed into, this frame is usually sentence structure.

c) a lexical stage where a search for a word occurs based on meaning. Once the word is selected and retrieved, information about it becomes available to the speaker involving phonology and morphology.

d) a phonological stage where the abstract information is converted into a speech like form.

e) a phonetic stage where instructions are prepared to be sent to the muscles of articulation.

Also, models must allow for forward planning mechanisms, a buffer, and a monitoring mechanism.

Following are a few of the influential models of speech production that account for or incorporate the previously mentioned stages and include information discovered as a result of speech error studies and other disfluency data, such as tip-of-the-tongue research.

The Utterance Generator Model was proposed by Fromkin (1971). It is composed of six stages and was an attempt to account for the previous findings of speech error research. The stages of the Utterance Generator Model were based on possible changes in representations of a particular utterance. The first stage is where a person generates the meaning they wish to convey. The second stage involves the message being translated onto a syntactic structure. Here, the message is given an outline. The third stage proposed by Fromkin is where/when the message gains different stresses and intonations based on the meaning. The fourth stage Fromkin suggested is concerned with the selection of words from the lexicon. After the words have been selected in Stage 4, the message undergoes phonological specification. The fifth stage applies rules of pronunciation and produces syllables that are to be outputted. The sixth and final stage of Fromkin's Utterance Generator Model is the coordination of the motor commands necessary for speech. Here, phonetic features of the message are sent to the relevant muscles of the vocal tract so that the intended message can be produced. Despite the ingenuity of Fromkin's model, researchers have criticized this interpretation of speech production. Although The Utterance Generator Model accounts for many nuances and data found by speech error studies, researchers decided it still had room to be improved.

A more recent (than Fromkin's) attempt to explain speech production was published by Garrett in 1975. Garrett also created this model by compiling speech error data. There are many overlaps between this model and the Fromkin model from which it was based, but he added a few things to the Fromkin model that filled some of the gaps being pointed out by other researchers. The Garrett Fromkin models both distinguish between three levels—a conceptual level, and sentence level, and a motor level. These three levels are common to contemporary understanding of Speech Production.

In 1994, Dell proposed a model of the lexical network that became fundamental in the understanding of the way speech is produced. This model of the lexical network attempts to symbolically represent the lexicon, and in turn, explain how people choose the words they wish to produce, and how those words are to be organized into speech. Dell's model was composed of three stages, semantics, words, and phonemes. The words in the highest stage of the model represent the semantic category. (In the image, the words representing semantic category are winter, footwear, feet, and snow represent the semantic categories of boot and skate.) The second level represents the words that refer to the semantic category (In the image, boot and skate). And, the third level represents the phonemes ( syllabic information including onset, vowels, and codas).

Levelt further refined the lexical network proposed by Dell. Through the use of speech error data, Levelt recreated the three levels in Dell's model. The conceptual stratum, the top and most abstract level, contains information a person has about ideas of particular concepts. The conceptual stratum also contains ideas about how concepts relate to each other. This is where word selection would occur, a person would choose which words they wish to express. The next, or middle level, the lemma-stratum, contains information about the syntactic functions of individual words including tense and function. This level functions to maintain syntax and place words correctly into sentence structure that makes sense to the speaker. The lowest and final level is the form stratum which, similarly to the Dell Model, contains syllabic information. From here, the information stored at the form stratum level is sent to the motor cortex where the vocal apparatus are coordinated to physically produce speech sounds.

The physical structure of the human nose, throat, and vocal cords allows for the productions of many unique sounds, these areas can be further broken down into places of articulation. Different sounds are produced in different areas, and with different muscles and breathing techniques. Our ability to utilize these skills to create the various sounds needed to communicate effectively is essential to our speech production. Speech is a psychomotor activity. Speech between two people is a conversation - they can be casual, formal, factual, or transactional, and the language structure/ narrative genre employed differs depending upon the context. Affect is a significant factor that controls speech, manifestations that disrupt memory in language use due to affect include feelings of tension, states of apprehension, as well as physical signs like nausea. Language level manifestations that affect brings could be observed with the speaker's hesitations, repetitions, false starts, incompletion, syntactic blends, etc. Difficulties in manner of articulation can contribute to speech difficulties and impediments. It is suggested that infants are capable of making the entire spectrum of possible vowel and consonant sounds. IPA has created a system for understanding and categorizing all possible speech sounds, which includes information about the way in which the sound is produced, and where the sounds is produced. This is extremely useful in the understanding of speech production because speech can be transcribed based on sounds rather than spelling, which may be misleading depending on the language being spoken. Average speaking rates are in the 120 to 150 words per minute (wpm) range, and same is the recommended guidelines for recording audiobooks. As people grow accustomed to a particular language they are prone to lose not only the ability to produce certain speech sounds, but also to distinguish between these sounds.

Articulation, often associated with speech production, is how people physically produce speech sounds. For people who speak fluently, articulation is automatic and allows 15 speech sounds to be produced per second.

An effective articulation of speech include the following elements – fluency, complexity, accuracy, and comprehensibility.


Before even producing a sound, infants imitate facial expressions and movements. Around 7 months of age, infants start to experiment with communicative sounds by trying to coordinate producing sound with opening and closing their mouths.

Until the first year of life infants cannot produce coherent words, instead they produce a reoccurring babbling sound. Babbling allows the infant to experiment with articulating sounds without having to attend to meaning. This repeated babbling starts the initial production of speech. Babbling works with object permanence and understanding of location to support the networks of our first lexical items or words. The infant’s vocabulary growth increases substantially when they are able to understand that objects exist even when they are not present.

The first stage of meaningful speech does not occur until around the age of one. This stage is the holophrastic phase. The holistic stage refers to when infant speech consists of one word at a time (i.e. papa).

The next stage is the telegraphic phase. In this stage infants can form short sentences (i.e., Daddy sit, or Mommy drink). This typically occurs between the ages of one and a half and two and a half years old. This stage is particularly noteworthy because of the explosive growth of their lexicon. During this stage, infants must select and match stored representations of words to the specific perceptual target word in order to convey meaning or concepts. With enough vocabulary, infants begin to extract sound patterns, and they learn to break down words into phonological segments, increasing further the number of words they can learn. At this point in an infant's development of speech their lexicon consists of 200 words or more and they are able to understand even more than they can speak.
When they reach two and a half years their speech production becomes increasingly complex, particularly in its semantic structure. With a more detailed semantic network the infant learns to express a wider range of meanings, helping the infant develop a complex conceptual system of lemmas.

Around the age of four or five the child lemmas have a wide range of diversity, this helps them select the right lemma needed to produce correct speech. Reading to infants enhances their lexicon. At this age, children who have been read to and are exposed to more uncommon and complex words have 32 million more words than a child who is linguistically impoverished. At this age the child should be able to speak in full complete sentences, similar to an adult.




</doc>
<doc id="18342" url="https://en.wikipedia.org/wiki?curid=18342" title="Outline of law">
Outline of law

Law ("article link") is the set of rules and principles (laws) by which a society is governed, through enforcement by governmental authorities. Law is also the field that concerns the creation and administration of laws, and includes any and all legal systems.

Law can be described as all of the following:




Public law







History of law






Sources of law

Legislatures

Courts

Prisons




</doc>
<doc id="25389252" url="https://en.wikipedia.org/wiki?curid=25389252" title="Index of law articles">
Index of law articles

This collection of lists of law topics collects the names of topics related to law. Everything related to law, even quite remotely, should be included on the alphabetical list, and on the appropriate topic lists. All links on topical lists should also appear in the main alphabetical listing. The process of creating lists is ongoing – these lists are neither complete nor up-to-date – if you see an article that should be listed but is not (or one that shouldn't be listed as legal but is), please update the lists accordingly. You may also want to include talk page banners on the relevant pages.

"a posteriori" –
"ab extra" –
"ab initio" –
Abandoned property –
Abandonment (legal) –
Abduction –
Abet –
Abeyance –
Abolitionism in the United States –
Abolitionism –
Abortion –
Abortion, legal and moral issues –
Abrogate –
Abstention doctrine –
Abstract –
Abstract of judgment –
Abstract of title –
Abuse of discretion –
Abuse of process –
Abut –
Acceleration clause –
Accept –
Acceptance –
Acceptance of service –
Accessory –
Accommodation –
Accomplice –
Accord and satisfaction –
Account stated –
Accountability –
Accounting period –
Accounting reference date –
Accounts payable –
Accounts receivable –
Accrue –
Accusation –
Accused –
Acknowledge –
Acknowledgement of service –
Acknowledgment –
Acquis –
Acquit –
Acquittal –
Act of God –
Act of Parliament –
Action –
Actionable –
Actual controversy –
Actual malice –
Actual notice –
"actus reus" –
"ad colligenda bona" –
"ad hoc" –
"ad idem" –
"ad infinitum" –
"ad litem" –
"ad quod damnum" –
"ad seriatim" –
"ad valorem" –
Addendum –
Adeem –
Ademption –
Adequate remedy –
Adhesion contract –
Adjourn –
Adjournment sine die –
Adjournment in contemplation of dismissal –
Adjudication –
Adjusted basis –
Adjuster –
Administer –
Administration –
administration order –
Administrative hearing –
Administrative law –
Administrative law judge –
Administrative Procedure Act (Japan) –
Administrative Procedure Act (United States) –
Administrator –
Administrator –
Admiralty –
Admiralty actions –
Admiralty court –
Admiralty law –
Admissible evidence –
Admission against interest –
Admission of evidence –
Admission (law) –
Admission to bail –
Admission to the bar –
Adopt –
Adoption –
Adultery –
Advance directive –
Adversary system –
Adverse –
Adverse interest –
Adverse party –
Adverse possession –
Adverse witness –
Advisory opinion –
Advocate –
Affiant –
Affidavit –
Affirm –
Affirmative action –
Affirmative defense –
Affix –
Affreightment –
After-acquired property –
Age discrimination –
Age of consent –
Age of majority –
Agency –
Agency agreement –
Agent –
Agent for acceptance of service –
Aggravated assault –
Agreed statement –
Aid and abet –
Aleatory –
Alias –
Alibi –
Alien –
Alienation –
Alienation of affections –
Alimony –
All the estate I own –
Allegation –
Allege –
Allocation questionnaire –
Allocatur –
Allocution –
Allodial –
Alluvion –
Alodium –
"alter ego" –
Alternate director –
Alternative dispute resolution –
Alternative Minimum Tax –
Alternative pleading –
ALWD Citation Manual –
Ambiguity –
Ambulance chasing –
Amelioration Act 1798 –
Amended complaint –
Amended pleading –
American Academy of Appellate Lawyers –
American Arbitration Association –
American Bar Association –
American Civil Liberties Union –
American Civil Rights Movement –
American Declaration of the Rights and Duties of Man –
American Depositary Receipt –
American Law Institute –
"amicus curiae" –
Amnesty –
Amnesty International –
Amortization –
An eye for an eye –
Ancillary administration –
Ancillary jurisdiction –
Ancillary relief –
Animal rights –
Animal rights by country or territory –
Animal law –
Animal Law Review –
Animal Legal Defense Fund –
Animal trial –
Animal Welfare Act of 1966 –
"animus nocendi" –
"Animus revertendi" –
annual general meeting –
Annul –
Annulment –
Anomie –
Answer (law) –
Antecedent (law) –
Antenuptial (prenuptial) agreement –
"Antejuramentum" –
Anticipatory breach –
Antidisestablishmentarianism –
Antinomianism –
Antitrust –
Antitrust laws –
Apartheid –
Ape personhood –
Apparent authority –
Appeal –
Appeal bond –
Appeals court –
Appear –
Appellant –
Appellate court –
Appellate review –
Appellee –
Appraiser –
Appreciate –
Appreciation –
Apprenticeship 
Approach the bench 
Appurtenances 
Appurtenant 
Arbitrary 
Arbitration –
Arbitration award –
Arbitrator –
"arguendo" –
Argumentative –
Arm's length –
Arraign –
Arraignment –
Arrears –
Arrest –
Arrest warrant –
Arson –
Article I and Article III tribunals –
Articles of Association –
Articles of impeachment –
Articles of Incorporation –
Articles of War –
As is –
Asharite –
Assault –
Asset –
Assignment (law) –
Assigned risk –
Assignee –
Assignment for benefit of creditors –
Assigns –
Assisted person –
Assize Court –
Associate justice –
Association –
Assumption of risk –
Asylum and Immigration Tribunal –
Asylum seeker –
At will –
At will employment –
Attachment –
Attachment of earnings –
Attempt –
Attestation clause –
Attorney at law (or attorney-at-law) –
Attorney general –
Attorney of record –
Attorney's advertising –
Attorney's fee –
Attorney's work product –
Attorney–client privilege –
Attorney-in-fact –
Attractive nuisance doctrine –
Audit –
Auditor –
Australian Constitution –
Australian Constitutional history –
Australian copyright law –
Authorised share capital –
Authoritarianism –
Authority –
Authorize –
Automatic stay –
Autrefois acquit –
Avulsion –
Ayatollah

B.C.L. –
Babylonian law –
Bachelor of Civil Law –
Bachelor of Laws –
Bachelor of Legal Letters –
Back-to-back life sentences –
Bad debt –
Bad faith –
Bail –
Bail bond –
Bail bondsman –
Bail schedule –
Bailee –
Bailiff –
Bailment –
Bailor –
Bait and switch –
Balance due –
Balance sheet –
Ban –
Bank –
Bankrupt –
Bankruptcy –
Bankruptcy court –
Bankruptcy proceedings –
Bankruptcy remote –
Bar –
Bar association –
Bar council –
Bar examination –
Bare trust –
Bargain and sale deed –
Barratry (admiralty law) –
Barratry (common law) –
Barrister –
Basic Law of various jurisdictions –
Battery –
Beach bum trust provision –
Bearer paper –
Belief –
Bench –
Bench memorandum –
Bench trial –
Bench warrant –
Beneficial interest –
Beneficial use –
Beneficiary –
Beneficiary (trust) –
Benefit of counsel –
Bequeath –
Bequest –
Berne three-step test –
Best evidence rule –
Best Interests of the Child –
Bestiality –
Beyond a reasonable doubt –
BFP –
Bias –
Bifurcate –
Bifurcation –
Bigamy –
Bilateral contract –
Bill –
Bill of attainder –
Bill of costs –
Bill of exchange –
Bill of indictment –
Bill of lading –
Bill of particulars –
Bill of rights –
Bill of sale –
Bind over –
Bind over for sentence –
Binding arbitration –
Bioethics –
Black's Law Dictionary –
Blackmail –
Blank endorsement –
Blood libel –
Blue law –
Blue laws –
Blue ribbon jury –
Blue Sky Laws –
Bluebook –
Board of directors –
"bona fide" –
Bona fide purchaser –
"bona vacantia" –
Bond –
Bond for deed –
Booby trap –
Book account –
Book value –
Bootleg recording –
Border control –
Bottomry –
Boycott –
Breach of contract –
Breach of promise –
Breach of the peace –
Breach of warranty –
Breaking and entering –
Bribery –
Bride price –
Brief –
British constitution –
British constitutional law –
British nationality law –
Broker –
Brought to trial –
Building and loan –
Bulk sale –
Bulk sales acts –
Bulk transfer –
Burden –
Burden of proof –
Burgage –
Burglary –
Business –
Business ethics –
Business invitee –
But for rule –
Buy-sell agreement –
Bylaw –
Bylaws –
Bypass trust

Cadastral map –
"cadit quaestio" –
Calendar call –
Caliphate –
Call to the bar –
Calumny –
Campaign finance reform –
Canadian Bill of Rights –
Canadian Charter of Rights and Freedoms –
Caning –
Canon –
Canon law –
Cape (writ) –
"capital" –
Capital account –
Capital assets –
Capital expenditure –
Capital gain –
Capital gain tax –
Capital gains –
Capital investment –
Capital loss –
Capital offense –
Capital punishment –
Capital punishment in the United States –
Capital stock –
Capitalized value –
Capricious –
Carjacking –
Carnal knowledge –
Carrier –
Carrying for hire –
Cartel –
Case –
Case conference –
Case law –
Case law in the United States –
Case number –
Case of first impression –
Case-based reasoning –
Cashier's check –
Casualty insurance –
Casualty loss –
Casuistry –
Catechism –
Categorical Imperative –
Catholic Emancipation –
"cause" –
Cause of action –
"caveat emptor –
Cease and desist order –
Censorship –
Certificate of deposit –
Certificate of incorporation –
Certificate of legal aid costs –
Certificate of title –
Certified check –
"certiorari" –
Writ of Certiorari –
Cessate –
Cestui que trust –
Cestui que use –
"ceteris paribus" –
Chain of title –
Chairman –
Challenge for cause –
Champerty –
Chancellor –
Chancery –
Chancery division –
Change of venue –
Character witness –
Charge –
Charging lien –
Charging order –
Charitable contribution –
Charitable organization –
Charitable remainder trust –
Charitable trust –
Charter –
Chattel –
Chattel mortgage-- Checks and balances –
Cheque –
Chief Justice –
Chief Justice of Canada –
Chief Justice of the United States –
Child –
Child abandonment –
Child abuse –
Child custody –
Child endangerment –
Child neglect –
Child pornography –
Child sexual abuse –
Child support –
Chinese law –
Churning –
Circuit courts –
Circumcision –
Circumstantial evidence –
Citation –
Cite –
Citizen –
Citizen's dividend –
Citizenship –
Civil action –
Civil and social disobedience –
Civil calendar –
Civil code –
Civil Code of Quebec –
Civil commitment –
Civil death –
Civil disobedience –
Civil disorder –
Civil justice reforms –
civil law –
Civil law notary –
Civil liability –
Civil liberties –
Civil penalties –
Civil procedure –
Civil rights –
Civil union –
Claim against a governmental agency –
Claim against an estate –
Claim form –
Claim in bankruptcy –
Claimant –
Class –
Class action –
Class action suit –
Clean hands doctrine –
Cleanup clause –
Clear and convincing evidence –
Clear and present danger –
Clear title –
Clerk –
Close corporation –
Closed shop –
Closing –
Closing argument –
Cloud on title –
Co-trustee –
Code –
Code of Hammurabi –
Code of professional responsibility –
Codefendant –
Codex –
Codicil –
Codification –
Codify –
Coercion –
Cohabitation –
Cohabitation agreement –
Coinsurance –
Collateral –
Collateral attack –
Collateral descendant –
Collateral estoppel –
Collateral Warranty –
Collective agreement –
Collective bargaining agreement –
Collective rights –
Collective trade marks – –
Collusion –
Collusive action –
Color of law –
Color of title –
Comaker –
Comity –
Commencement of action –
Commentaries on the Laws of England –
Commercial frustration –
Commercial law –
Commingling –
Commission of rebellion -
Commissioner of oaths –
Committal –
Commodity status of animals –
Common area –
Common carrier –
Common counts –
Common law –
Common property –
Common purpose –
Common stock –
Common-law marriage –
Commons –
Community patent –
Community property –
Commutation –
Company –
Company seal –
Comparative law –
Comparative negligence –
Comparative responsibility –
Compensatory damages –
Competence –
Complainant –
Complaint –
Complete contract –
Compound interest –
Compound question –
Compounding a felony –
Compounding treason –
Compromise –
Compromise verdict –
Concealed weapon –
Conciliation –
Conclusion of fact –
Conclusion of law –
Concubinage –
Concurrent sentence –
Concurrent sentences –
Concurrent writ –
Condemnation action –
Condition precedent –
Condition subsequent –
Conditional bequest –
Conditional discharge –
Conditional dismissal –
Conditional sale –
Condominium –
Conduct money –
Confederate States Constitution –
Confession (law) –
Confession and avoidance –
Confession of judgment –
Confidence game –
Confidential communication –
Confidential information –
Confidentiality –
Confiscate –
Conflict of interest –
Conflict of law –
Conflict of laws –
Confucianism –
Confusingly similar –
Congregation for the Doctrine of the Faith –
Congressional-executive agreement –
Conscientious objector –
Conscious parallelism –
Conscription –
Consecutive sentence –
Consecutive sentences –
Counsul of Force –
Countries banning non-human ape experimentation –
Consensu –
Consensual crime –
Consensus –
Consensus ad idem –
Consensus decision-making –
Consent –
Consent decree –
Consent judgment –
Consequential damages –
Consequentialism –
Conservatee –
Conservative Judaism –
Conservator (law) –
Consideration –
Consign –
Consignee –
Consignment –
Consortium –
Conspiracy –
Conspirator –
Constable –
Constitution –
Constitution of France –
Constitution of Spain –
Constitutional amendment –
Constitutional charter –
Constitutional Convention (Australia) –
Constitutional Convention (United States) –
Constitutional law –
Constitutional monarchy –
Constitutional rights –
Construction –
Constructive –
Constructive dismissal –
Constructive eviction –
Constructive fraud –
Constructive notice –
Constructive possession –
Constructive trust –
Construe –
Consuetudinary –
Consultancy –
Consultant –
Consumer protection –
Contact –
Contemplation of death –
Contempt of court –
Contingency –
Contingency fee –
Contingent beneficiary –
Contingent fee –
Contingent interest –
Contingent remainder –
Continuance –
Continuing objection –
Continuing trespass –
"contra bonos mores" –
"contra legem" –
Contraband –
Contract –
Contract (canon law) –
Contract of adhesion –
Contract of sale –
Contract theory –
Contractor –
"Contramandatio placiti" -
Contributory negligence –
Controlled substance –
Controlling law –
Controversy –
Conversion –
Conveyancing –
Convict –
Conviction –
Cooperative –
Cooperative housing –
Cop a plea –
Copartner –
Copyhold –
Copyleft –
Copyright –
Copyright infringement –
Copyright law of the European Union –
Copyright misuse –
"coram nobis" –
"coram non judice" –
Coroner –
Corporate governance –
Corporate haven –
Corporate opportunity –
Corporate personhood –
Corporate state –
Corporation –
Corporations law –
"corpus delicti" –
"corpus juris" –
"corpus juris civilis" –
"corpus juris secundum" –
Correlative rights doctrine –
Corroborate –
Corroborating evidence –
Corroboration –
Cost bill –
Cotenancy –
Cotenant –
Council Tax –
Counsel –
Counsellor –
Count –
Counter offer –
Counterclaim –
Counterfeit –
County court –
Coup d'état –
Cour de cassation –
Course of employment –
Court –
Court calendar –
Court costs –
Court docket –
Court of appeal –
Court of Appeal of England and Wales –
Court of Appeal (France) –
Court of Appeals –
Court of customs and patent appeals –
Court of equity –
Court of last resort –
Court of law –
Court of protection –
Court of record –
Court of Session –
Court order –
Court trial –
Court-martial –
Courtesy –
Courtroom –
Courts of England and Wales –
Courts of the United Kingdom –
Covenant (law) –
Covenant not to compete –
Covenant that runs with the land –
Covenants, conditions and restrictions –
Creature of statute –
Credibility –
Credible witness –
Creditor –
Creditor's claim –
Creditor's rights –
Crime –
Crime against humanity –
Crime against nature –
Crime against peace –
Crime of passion –
Criminal –
Criminal attorney –
Criminal calendar –
Criminal conversion –
Criminal justice –
Criminal law –
Criminal negligence –
Criminal procedure –
Critical legal studies –
Cross examination –
Cross-complaint –
Cross-examination –
Crown copyright –
Crown corporation –
Crown Court –
Crown entity –
Crown land –
Cruel and unusual punishment –
Cruelty –
Cruelty to animals –
"cui bono" –
"cuius regio, eius religio" –
Culpability –
Cumis counsel –
Cumulative sentence (disambiguation) –
Cumulative voting –
Curfew –
Customary estate –
Customary law –
Customs –
"custos morum" –
Cut a check –
Cy pres doctrine –
Cyber law –
Cybersquatting

D.A. –
D.B.A. –
D.U.I. –
D.W.I. –
Damages –
Damnation –
Dangerous weapon –
Data protection –
Date rape –
Daubert standard –
Day in court –
"de bonis asportatis" –
"de bonis non administratis" –
"de facto" –
De facto corporation –
"de futuro" –
"de integro" –
"de jure" –
De jure corporation –
"de lege ferenda" –
"de lege lata" –
"de minimis" –
"de novo" –
Deadlock –
Deadlock provision –
Deadly weapon –Death tax –
Death penalty –
Death row –
Death duty –
Debenture –
Debt –
Debt bondage –
Debtor –
Debtor in possession –
Decapitation –
Deceased –
Deceit –
Deception –
Decide! –
Decision –
Decisory oath -
Declarant –
Declaration of Arbroath –
Declaration of independence –
Declaration of mailing –
Declaration of the Independence of New Zealand –
Declaration of the Rights of Man and of the Citizen –
Declaration of trust –
Declaration of war –
Declaration of war by the United States –
Declaratory judgment –
Declaratory relief –
Declared death "in absentia" –
Decree –
Decree absolute –
Decree nisi –
Decriminalization –
Dedication –
Deduction –
Deed –
Deed poll –
"defalcation" –
Defamation –
Default (law) –
Default judgment –
Default rule –
Defeasance –
Defective title –
Defendant –
Defense –
Defense attorney – –
Defense of infancy –
Deficiency judgment –
Defined benefit plan –
Defined contribution plan –
Deforce –
Defraud –
Degree of kinship –
Deliberate –
Deliberation –
Deliberative body –
Delict –
Demand –
Demand note –
Demesne –
Demise –
Democracy –
Demonstrative evidence –
Demurrer –
Denial –
Deobandi –
Deontology –
Department for Constitutional Affairs –
Dependent –
Deportation –
Deposition –
Depreciate –
Depreciation –
Depreciation reserve –
Derivative action –
Derivative work –
Derivatives law -
Descent and distribution –
Desert –
Desertion –
Detailed Assessment –
Devi –
Devil's Advocate –
Devisee –
Devolution –
Devolve –
Devolved government –
"dicta" –
"dictum" –
Digital signature –
Diligence –
Diminished capacity –
Diminished responsibility –
Diminished responsibility in English law –
Diminution in value –
Diplomatic immunity –
Diplomatic recognition –
Direct and proximate cause –
Direct evidence –
Direct examination –
Directed verdict –
Directors register –
Disability –
Disbar –
Disbarment –
Discharge in bankruptcy –
Disciplinary procedure –
Disclaimer –
Discovery –
Discovery of documents –
Discretion –
Discretionary trust –
Discrimination –
Disembowelment –
Disfigure –
Dishonor –
Disinheritance –
Disjunctive allegations –
Dismissal –
Dismissal with prejudice –
Dismissal without prejudice –
Disobbedienti –
Disorderly conduct –
Disorderly house – –
Disposing mind and memory –
Disposition –
Dispossess –
Dispute resolution –
Dissent –
Dissenting opinion –
Dissolution (law) –
Dissolution of corporation –
Dissolution of the Monasteries –
Distinguish –
Distribute –
Distribution of property –
Distributive justice –
District attorney –
District court –
Diversity of citizenship –
Divestiture –
Divestment –
Dividend –
Dividend tax –
Divine Right of Kings –
Division of property –
Divisional court (disambiguation) –
Divorce –
DNA –
Document –
Documentary evidence –
"doli incapax" –
Domestic partner –
Domestic partners –
Domestic relations –
Domestic violence –
Dominant estate –
Dominant tenement –
"Donatio mortis causa" –
Donation –
Donative intent –
Donee –
Doom book –
Double jeopardy –
Double taxation –
Dower –
Dowry –
Draft document –
Drainage law –
Dram shop rule –
Drawer –
Drawing and quartering –
Dreyfus affair –
Driver's license –
Driving under the influence –
Driving while intoxicated –
Droit du seigneur –
Drop dead date –
Drug –
Dubitante –
"duces tecum" –
Due and owing –
Due care –
Due diligence –
Due process –
Due process of law –
Due, owing and unpaid –
Duress –
Duress in English law –
Duty –
Duty of care –
Duty of care in English law –
Duty to warn –
Dying declaration

Early Muslim philosophy –
Earned income tax credit –
Earnest payment –
Easement –
Ecclesia –
Ecclesiastical court –
Ecumenical council –
Edict –
Edict of Fontainebleau –
Edict of Milan –
Edict of Nantes –
Edict of Worms –
"ei incumbit probatio qui" –
Either –
Ejectment –
"ejusdem generis" –
Elder law –
Election of remedies –
Election under the will –
Elective share –
Electoral reform –
Electric chair –
Emancipation –
Emancipation Proclamation –
Embezzlement –
Embezzler –
Emblements –
Emergency –
Eminent domain –
Emolument –
Employee –
Employer –
Employers' liability –
Employment –
Employment contract –
Employment law –
En banc –
Enabling clause –
Enclosure –
Encumbrance –
End user license agreement –
Endowment –
Enfeoff –
Enfeoffment –
Enforcement –
English Bill of Rights –
English law –
Enjoin –
Enjoyment –
Enrolled Bill doctrine –
Entail –
Enter a judgment –
Entertainment law –
Entity –
Entrapment –
Entry of judgment –
Environmental Impact Report –
Environmental impact statement –
Environmental law –
Ephebophilia –
Equal Access Act –
Equal opportunity –
Equal Protection Clause –
Equitable distribution –
Equitable estoppel –
Equitable lien –
Equitable remedy –
Equity (law) –
Equity of redemption –
Equivalent –
"erga omnes" –
"erratum" –
Error –
Escalator clause –
Escape clause –
Escheat –
Escrow –
Escrow account –
Escrow agent –
Escrow instructions –
Espionage –
Esquire –
Essential facilities doctrine –
Establishment clause –
Estate –
Estate by entirety –
Estate in land –
Inheritance tax –
"estoppel" –
"et al." –
"et cetera" –
"et seq" –
Eternity clause
Ethical calculus –
Ethical code –
Ethics –
Ethics in religion –
Ethnic cleansing –
EU Directive 2010/63/EU –
European Convention on Human Rights –
European Court of Human Rights –
European Court of Justice –
European Patent Convention –
European Patent Organisation –
European Union directive –
Directive (EU) –
European Union Law –
European Union regulation –
Regulation (EU) –
Euthanasia –
Evasion of tax –
Evasion of the law –
Eviction –
Evidence –
"ex aequo et bono" –
"ex cathedra" –
"ex delicto" –
"ex facie" –
"ex gratia" –
"ex officio" –
"ex parte" –
"ex post facto" –
Ex post facto law –
"ex rel" –
Examination –
Exception in deed –
Excessive bail –
Excise –
Exclusionary rule –
Excommunication –
Exculpatory –
Excusable neglect –
Excuse –
Execution –
Execution –
Execution warrant –
Executioner –
Executive –
Executive clemency –
Executive privilege –
Executor –
Executory contract –
Executory interest –
"executrix" –
Exegesis –
Exemplary damages –
Exempt –
Exempt employees –
Exempt property –
Exemption –
Exhibit –
exigent circumstances –
Exile –
Expectancy –
Expense –
Expert determination –
Expert testimony –
Expert witness –
Express contract –
Express warranty –
Extension –
Extenuating circumstances –
Extinguishment –
Extortion –
Extradition –
Extrajudicial –
Extraordinary General Meeting –
Extraordinary resolution –
Extreme cruelty –
Extrinsic fraud

FOB (shipping) –
Fabrica –
Fabricate –
Fabula –
Face –
Facere –
Facies –
Facile –
Fact –
Facto –
Factory –
Factum –
Faculties, Court of –
Faculty (instrument) –
Faculty of a college – –
Faculty of Advocates –
Faggot voter –
Fail –
Failure –
Failure of consideration –
Failure of issue –
Faint action –
Fair –
Fair Play Men –
Fair pleader –
Faith –
Faithless servant –
Falang –
Falda –
Faldstool –
Falesia –
Falk-land –
Fall –
Fallo
False action –
False imprisonment –
False pretenses –
False swearing –
Falsehood –
Falsify –
Falsing –
Falsum –
Falsus in uno, falsus in omnibus –
Familia –
Family –
Famosus –
Famosus libelus –
Fanatics –
Fakir –
Farm –
Farmer –
Faro –
Farrier –
Fasti –
Father –
Father-in-law –
Fathom –
Fatuity –
Fatuus –
Faubourg –
Fautor –
Fealty –
Fear –
Feciales –
Federal government –
Fee –
Fee-simple –
Fenian –
Feodal –
Feodal system –
Feodary –
Feodum –
Feoffee –
Feoffment –
Feoh –
Feria –
Feriae –
Ferling –
Ferry –
Ferryman –
Feu (land tenure) –
Feud –
Fishing law –
Flag –
Flag of the United States –
Flagrante delicto –
Flem –
Fleta –
Flight –
Floating capital –
Floor –
Florin –
Flotsam –
Fluctus –
Face amount –
Face value –
Fact –
Factum –
Faculty of law –
Failure of consideration –
Failure of issue –
Fair comment –
Fair dealing –
Fair market value –
Fair trade laws –
Fair use –
Fairness Doctrine –
False arrest –
False Claims Law –
False imprisonment –
False pretenses –
Family –
Family law –
Family court –
Family law –
Family patrimony –
Family purpose doctrine –
Fatwa –
Fault auto insurance system –
Federal Communications Commission –
Federal Constitutional Court (Germany) –
United States federal courts –
Federal judge –
Federal jurisdiction (United States) –
Federal law –
Federal question –
Federal tort claims act –
Federalism –
Fee –
Fee simple –
Fee tail –
Felicific calculus –
Felony –
Felonious –
Felony –
Felony murder rule –
Feme covert –
Feoff –
Feoffee –
Feoffment –
Fertile octogenarian –
Feud –
Feudal land tenure –
Feudal system –
Feudalism –
Fiat –
Fictitious defendants –
"fiduciary" –
Fiduciary duty –
Fiduciary relationship –
Fief –
Fieri –
"fieri facias" –
Fighting words –
File –
Final judgment –
Finder of fact –
Findings of fact –
Fine –
Fiqh –
Firm offer –
First degree murder –
First impression –
First to file and first to invent –
Fixture –
Fixtures –
Flight –
Floating charge –
Floating easement –
FOB –
Fostering –
Foujdar –
Four corners –
Fourierism –
Fox's Libel Act –
Frais –
Franc –
Francia –
Francus –
Frank-marriage –
Franking privilege –
Fraternity –
Fratricide –
Fraud –
Fraus –
Fraxinetum –
Free-bench –
Free and clear –
Free socage –
Free warren –
Freedman –
Freedom –
Freedom of speech –
Freedom of the press –
Free on board –
Freight –
Freighter –
Frenchman –
Frequent –
Frere –
Fresca –
Fresh pursuit –
Fretum Britannicum –
Friend of the court –
Friendly societies –
Friendly suit –
Frigidity –
Frith –
Frivolous –
Frontage –
Frontier –
Fructus industriales –
Fructus naturales –
Fruges –
Fruit –
Folkways –
For value received –
Forbearance –
Force majeure –
Forced heirship –
Forced sale –
Forcible entry –
Foreclosure –
Foreclosure sale –
Foreign corporation –
Forensic –
Forensic medicine –
Forensic testimony –
Forensics –
Foreseeability –
Foreseeable risk –
Forfeit –
Forger –
Forgery –
Formal contract –
Fornication –
"forum conveniens" –
"forum non conveniens" –
Forum shopping –
Foster child –
Four Cardinal Virtues –
Four corners of an instrument –
Franc-tireur –
Franchise –
Franchise tax –
Franchising –
Fraud –
Fraud in the inducement –
Fraudulent conveyance –
Fraudulent trading –
Free and clear –
Free economic zone –
Free on board –
Free port –
Free software license –
Free speech –
Free will –
Freedom of assembly –
Freedom of association –
Freedom of expression –
Freedom of Information Act –
Freedom of religion –
Freedom of speech –
Freedom of speech by country –
Freedom of the press –
Freedom of thought –
Freehold –
French law on secularity and conspicuous religious symbols in schools –
Fresh pursuit –
Friendly suit –
Frisking –
Frivolous –
Frivolous lawsuit –
Fructus naturales –
Fruit of the poisonous tree –
Frustration of purpose –
Fugitive from justice –
Full faith and credit –
Fully paid –
"functus officio" –
Fundamental justice –
Fundamentalism –
Fungible things –
Future interest –
Futuwa –
Fyrd

Gag order –
Gallows –
Game law –
Gaps and gores –
Garnish –
Garnishee –
Garnishment –
Gas chamber –
Gasoline tax –
Gemara –
Gender bias –
General Agreement on Tariffs and Trade –
General appearance –
General assignment –
General Counsel –
General damages –
General denial –
General meeting –
General order –
General partnership –
General plan –
General strike –
General Synod –
Generation skipping –
Geneva Conventions –
Genocide –
German town law –
Gibbet –
Gift –
Gift in contemplation of death –
Gift tax –
Glasnost –
Go bail –
Going concern –
Good cause –
Good faith –
Good governance –
Good samaritan rule –
Good title –
Goods –
Goseibai Shikimoku –
Government –
Government-granted monopoly –
Governmental immunity –
Grace period –
Grand Inquisitor –
Grand jury –
Grand larceny –
Grand theft –
Grandfather clause –
Grandfathered in –
Grandparent visitation –
Grant –
Grant deed –
Grantor-grantee index –
Gratuitous –
"gravamen" –
Great Ape Project –
Green card –
Gross income –
Gross negligence –
Grounds for divorce –
Group boycott –
Group Litigation Order –
Guanxi –
Guarantee –
Guarantees –
Guarantor –
Guaranty –
Guardian –
Guardian "ad litem" –
Guest statute –
Guild –
Guillotine –
Guilt –
Guilty

Habeas corpus –
Habeas corpus ad deliberandum et recipiendum –
Habeas corpus ad faciendum et recipiendum –
Habeas corpus ad prosequendum –
Habeas corpus ad respondendum –
Habeas corpus ad satisfaciendum –
Habeas corpus ad subjiciendum –
Habeas corpus ad testificandum –
Habeas corpus cum causa –
Habitant –
Habitation (see Dwelling) –
Habitual Criminals Act –
Hable –
Hacienda –
Habitable – –
Habitual criminal –
Hadith –
Hague Convention –
Hague-Visby Rules –
Halaal –
Halakha (Jewish law) –
Jewish law (Halakha) –
Half blood –
Halsbury's Laws of England –
Hanafi –
Hanbali –
Hanging –
Haram –
Harass –
Harassment –
Harm reduction –
Harmless error –
Hate speech –
Head of household –
Headnote –
Headright –
Heads of loss -
Health care proxy –
Hearing –
Hearsay –
Hearsay rule –
Heat of passion –
Heir –
Heir apparent –
Heiress –
Heirs –
Heirs of the body –
Hell or high water clause –
Hereditament –
Herem (censure) –
Herem (priestly gift) –
Herem (war or property) –
Heresy –
Hidden asset –
High court judge –
High Court of Australia –
Royal High Court of Bhutan –
High Court of Justice (England and Wales) –
Court of High Commission (ecclesiastical court in England) –
High Court of Fiji –
High Court (Hong Kong) –
High Courts of India, several courts –
High Court (Ireland) –
High Court (Isle of Man) –
High Court of Malaya –
High Court of New Zealand –
High Court of Cassation and Justice (Romania) –
High Court of Justiciary (Scotland) –
High Court of Sabah and Sarawak –
High Court of Singapore –
High Court of South Africa –
Highway –
Highwayman –
Hima –
Himalaya clause –
Hit and run –
Hobby loss –
Hold harmless –
Holder in due course –
Holding –
Holding company –
Holdover tenancy –
Holographic will –
Home Rule –
Home Secretary –
Homestead Act –
Homestead exemption –
Homestead principle –
Hometowned –
Homicide –
Hong Kong trademark law –
Hornbook law –
Hostile environment sexual harassment –
Hostile possession –
Hostile witness –
Hot pursuit –
Hotch-pot –
House counsel –
House of Lords –
Household –
Housing tenure –
Human rights –
Human Rights Committee –
Human rights issues in the United States –
Humanism –
Hung jury –
Hunting Act 2004 –
Hypothecate

Idea-expression divide –
"idem" –
"ignorantia juris non excusat" –
Ijma –
Ijtihad –
Illegal combatant –
Illegal drug trade –
Illegal immigrant –
Illegitimacy –
Illusory promise –
Ilm ar-Rijal –
Imam –
Immediately –
Immigrant visa –
Immigration –
Immigration Appellate Authority –
Immunity –
Impanel –
Impaneling –
Impeach –
Impeachment –
Impleader –
Implied Bill of Rights –
Implied consent –
Implied contract –
Implied covenant of good faith and fair dealing –
Implied terms –
Implied warranty –
Implied warranty of fitness for a particular purpose –
Implied warranty of habitability –
Implied warranty of merchantability –
Importation right –
Impossibility –
Impotence –
Imputation –
"in camera" –
In chambers –
"in curia" –
"in delicto" –
"in esse" –
In fee simple –
"in flagrante delicto" –
"in forma pauperis" –
"in haec verba" –
In kind –
In lieu –
"in limine" –
"in loco parentis" –
"in pari delicto" –
"in personam" –
in pro per –
"in prope persona" –
"in propria persona" –
"in re" –
"in rem" –
"in situ" –
"in terrorem" –
"in terrorem clause" –
"in toto" –
Incapacity –
Incest –
Inchoate offense –
Incidental beneficiary –
Income –
Income tax –
Incompetent evidence –
Incontrovertible evidence –
Incorporation (business) –
Incorporate by reference –
Incorporation (business) –
Incorporeal –
Incriminate –
Incumbrance –
Indecent exposure –
Indefeasible –
Indefeasible estate –
Indemnify –
Indemnity –
Indenture –
Indentured servant –
Independent contractor –
Indeterminacy debate in legal theory –
Indeterminate sentence –
Indictable offence –
Indictable offense –
Indictment –
Indigent –
Indispensable party –
Individual capital –
Individual rights –
Indorse –
Industrial design rights –
Industrial tribunal –
Infancy –
Infant –
Infanticide Act –
Inference –
Information –
Information and belief –
Informed consent –
Infraction –
Infractions –
Infringement –
Ingress –
Inherit –
Inheritance –
Inheritance tax –
Injunction –
Injunctive relief –
Injury –
Inkan –
Innocence –
Innocent –
Inns of Court –
"innuendo" –
Inquest –
Inquisition –
Inquisitor –
Inquisitorial system –
Insanity –
Insanity defense –
Insider –
Insider trading –
Insolvency –
Insolvent –
Inspection of documents –
Installment contract –
Instruction –
Instructional capital –
Insufficient evidence –
Insurance –
Insured –
Insurer –
Intangible property –
Integrated criminal justice information system –
Integration –
Intellectual capital –
Intellectual property –
Intellectual rights –
Intendant of New France –
Intent –
"inter alia" –
"inter se" –
"inter vivos" –
Inter vivos trust –
Interest –
Interference proceeding –
Interim order –
Interlineation –
"interlocutory" –
International Business Companies Act –
Interlocutory decree –
Interlocutory order –
Intermediate sanctions –
Internal affairs doctrine –
International Business Companies Act –
International constitutional law –
International Court of Justice –
International Covenant on Civil and Political Rights –
International crime –
International Criminal Court –
International Criminal Tribunal for Rwanda –
International Criminal Tribunal for the Former Yugoslavia –
International environmental law –
International human rights instruments –
International human rights law –
International law –
International relations –
International trade –
International trade law –
Internment –
Interpleader –
Interrogation –
Interrogatories –
Interstate commerce –
Intertemporal Law –
Intervene –
Intervening cause –
Intervention –
Intestacy –
Intestate –
Intestate succession –
Intoxication –
"intra fauces terra" –
"intra vires" –
Intrinsic fraud –
Inure –
Invasion of privacy –
Inventor –
Inventor's notebook –
Inverse condemnation –
Invest –
Investiture –
Investment –
Invitation to treat –
Invitee –
Involuntary commitment –
"ipse dixit" –
"ipsissima verba" –
"ipso facto" –
Irreconcilable differences –
Irrelevant –
Irreparable damage or injury –
Irresistible impulse –
Islamic Law (Sharia) –
Sharia (Islamic law) –
Islamic philosophy –
Isnad –
Issue –
Issue preclusion –
Issued shares

Juris Doctor (J.D.) –
Jafari –
Jane Doe –
Jaywalking –
Jeopardy –
Jewish principles of faith –
Jewish Theological Seminary of America –
Jim Crow laws –
John Doe –
Joinder –
Joinder of issue –
Joint –
Joint adventure –
Joint and several –
Joint and several liability –
Joint custody –
Joint liability –
Joint property –
Joint tenancy –
Joint tortfeasors –
Joint venture –
Jointure –
Jones act –
Journeyman –
Joyride –
Judge –
Judge advocate –
Judge Advocate General –
Judgment –
Judgment by default –
Judgment debtor –
Judgment in Berlin –
Judgment non obstante veredicto –
Judgment notwithstanding the verdict –
Judgment notwithstanding verdict –
Judicial –
Judicial Committee of the Privy Council –
Judicial discretion –
Judicial economy –
Judicial foreclosure –
Judicial functions of the House of Lords –
Judicial independence –
Judicial interference –
Judicial notice –
Judicial review –
Jump bail –
Junior barrister –
"jurat" –
Jurisdiction –
Jurisdictional amount –
Jurisprudence –
Jurist –
Juror –
Jury –
Jury box –
Jury charge –
Jury fees –
Jury instructions –
Jury nullification –
Jury of one's peers –
Jury panel –
Jury selection –
Jury stress –
Jury tampering –
Jury trial –
"jus ad bellum" –
"jus ad bellum" –
"jus civile" –
"jus cogens" –
"jus commune" –
"jus gentium" –
"jus inter gentes" –
"jus naturale" –
"jus primae noctis" –
"jus sanguines" –
"jus sanguinis" –
"jus soli" –
Just cause –
Just compensation –
Just war –
Justice –
Justice of the Peace –
Justiciable –
Justifiable homicide –
Justification –
Juvenile –
Juvenile court –
Juvenile delinquent

Kangaroo court –
Karaites –
Karma –
Kosher law –
Kellogg-Briand Pact –
Kidnapping –
King's Bench –
Kinshasa Declaration on Great Apes –
Know-how –
Kollel

Labor and materials –
Labor law –
labor union –
Laches –
"lacunae" –
Land use –
Land value tax –
Landlady –
Landlocked –
Landlord –
Landlord and tenant –
Landlord and Tenant Act –
Landlord's lien –
Lapse –
Larceny –
Last antecedent rule –
Last clear chance –
Last will and testament –
Latent defect –
Law –
Law and economics –
Law and literature –
Law and motion calendar –
Law basic topics –
Law book –
Law dictionary –
Law French –
Law lords –
Law of admiralty –
Law of Canada –
Law of costs –
Law of Ireland –
Law library –
Law of obligations –
Law of the case –
Law of the land –
Law of the Russian Federation –
Law of the Sea –
Law of the Soviet Union –
Law of the United Kingdom –
Law of the United States –
Law of treaties –
Law school –
Law Society –
Laws of war –
Lawsuit –
Lawyer –
Lay a foundation –
Lay assessor –
Laïcité –
Leading –
Leading question –
Leading the witness –
Lease –
Lease and release –
Leasehold –
Legal –
Legal abuse –
Legal action –
Legal advertising –
Legal age –
Legal aid –
Legal Aid Society –
Legal code –
Legal consequences of marriage and civil partnership in the United Kingdom –
Legal custody –
Legal debate –
Legal dualism –
Legal entity –
Artificial person –
Legal fiction –
Legal formalism –
Legal history –
Legal instrument –
Legal Latin –
Legal lexicography –
Legal personal representative –
Legal positivism –
Legal pluralism –
Legal realism –
Legal separation –
Legal status of animals in Canada –
Legal technicality –
Legal tender –
Legal translation –
Legalese –
Legalism (Western philosophy) –
Legalism (Chinese philosophy) –
Legalism (theology) –
Legalization –
Legatee –
Legislation –
Legislature –
Legitimacy (family law) –
Legitimacy (political science) –
"legitime" –
Lemon law –
Lessee –
Lesser crime –
Lesser included offenses –
Lesser-included offense –
Let –
Lethal injection –
Letter of credit –
Letter of marque –
Letter of wishes –
Letters –
Letters of administration –
Letters patent –
Letters testamentary –
Leverage –
Leviticus –
"lex lata" –
"lex scripta" –
Liable –
Libel –
Libel per se –
Libertarian theories of law –
Liberty –
Licence –
License –
Licensee –
Lie detector test –
Lien –
Lienor –
Life –
Life estate –
Life without possibility of parole –
Limitation of actions –
Limitations clause, Constitution of Canada –
Limited company –
Limited jurisdiction –
Limited liability –
Limited liability company –
Limited partner –
Limited partnership –
Line of succession –
Lineal descendant –
Lineup –
Liquidate –
Liquidated damages –
Liquidation –
Liquidator (law) –
"lis pendens" –
List of Roman laws –
Listed building –
Literary property –
Litigant –
Litigation –
Litigious –
Liturgy –
Livery –
Livery of seizin –
"living trust" –
Living will –
LL.B. –
LL.M. –
Loanshark –
Lockout –
"locus delicti" –
"locus in quo" –
Loiter (law) –
Long cause –
Long vacation –
Long-arm statute –
Lord Chancellor –
Lord Chancellor's Department –
Lord Chief Justice –
Lord Chief Justice of England and Wales –
Lord Justice General –
Lord Justice of appeal –
Lord Keeper of the Great Seal –
Lord President of the Council –
Lord Steward –
Loss of consortium –
Loss of use –
Lost volume seller –
Lower court –
Lübeck law

M'Naghten Rules –
Madhhab –
Madrassa –
Magdeburg rights –
Magdeburg law –
Magistrate –
"magna carta" –
Mail box rule –
Maim –
Maintenance –
Maintenance –
Maintenance –
Majority –
Mala fides –
"male fide" –
Malfeasance –
Malice aforethought –
Malicious prosecution –
Maliki –
Malpractice –
"malum in se" –
"malum prohibitum" –
"mandamus" –
Writ of mandamus –
Mandate (criminal law) –
Mandate (international law) –
Mandate of Heaven –
Mandatory joinder –
Mandatory sentence –
Mann act –
Manorialism –
Manslaughter –
Manslaughter in English law –
Manumission –
Manusmriti –
"mare clausum" –
"mare liberum" –
Marital deduction –
Marital life estate –
Marital rights –
Maritime law –
Marked for identification –
Market value –
Marketable title –
Marriage –
Marriageable age –
Marshal –
Martial law –
Mask work –
Masoretes –
Masoretic Text –
Masorti –
Massachusetts trust –
Master –
Master and servant –
Master of Laws –
Master of the Rolls –
Master of the Rolls in Ireland –
Materiality –
Material witness –
Matrimonial regime –
Matter –
Maturity –
Maxims –
Maxims of equity –
Maxims of law –
May –
Mayhem –
Mechanic's lien –
Mechanics lien –
Mediation –
Mediator –
Medical directive –
Medical ethics –
Medieval Inquisition –
Meet and confer –
Meeting of the minds –
Meforshim –
Megan's Law –
Memorandum –
Memorandum of Association –
"mens rea" –
Mental cruelty –
Mental health law –
Mental suffering –
Mercantile law –
Merchantable –
Merger –
Mesne –
"mesne assignment" –
Mesne profits –
Messuage –
Metes and bounds –
Military alliance –
Military dictatorship –
Military law –
Military tribunal –
Militia –
Mining claim –
Ministerial act –
minor –
Minutes –
Miranda warning –
Mirror wills –
Misappropriation –
Mischief –
Misdemeanor –
Misfeasance –
Mishnah Berurah –
Mishnah –
Hebrew law (Mishpat Ivri) –
Mishpat Ivri (Hebrew law) –
Misjoinder –
Misnomer –
Misprision of a felony –
Misprision of treason –
Misrepresentation –
Mistake of law –
Mistrial –
Mitigating circumstances –
Mitigating factors –
Mitzvah –
Mock trial –
Modern Islamic philosophy –
"modus operandi" –
Moiety title –
Monarch –
Money laundering –
Monopoly –
Monopoly on the legitimate use of physical force –
Month-to-month –
Monument –
Moot court –
Moot point –
Mootness –
Mopery –
Moral absolutism –
Moral certainty –
Moral code –
Moral core –
Moral relativism –
Moral rights –
Moral turpitude –
Moral universalism –
Morality –
Moratorium –
Mores –
Morganatic marriage –
Mortgage law –
Mortgagee –
Mortgagor –
Motion –
Motion for a summary judgment –
Motion for more definite statement –
Motion for directed verdict –
Motion for dismissal –
Motion for summary judgment –
"motion in limine" –
Motion to dismiss –
Motion to Strike –
Motion to suppress –
Motion to suppress evidence –
Motive –
Motor vehicle exception--Motor vehicle theft –
Movant –
Mujtahid –
Mullah –
Multiple citizenship –
Multiplicity of suits –
Municipal –
Muniment of title –
Murder –
Murder in English law –
Muslim dietary laws –
Mutation –
"mutatis mutandis" –
Mutiny –
Mutual wills

N.O.V. –
Name change –
Named plaintiffs –
Napoleonic code –
Narcotic –
National Insurance contributions –
National Labor Relations Board –
National trade union center –
Nationality –
Natural law –
Natural person –
Natural resource law –
"ne exeat" –
Necessary party –
Negative pledge –
Negative pregnant –
Negligence –
Negligence "per se" –
Negligent –
Negotiable instrument –
Negotiation –
"nemo dat quod non habet" –
"nemo judex in sua causa" –
Neutral country –
Next friend –
Next of kin –
Night and Fog prisoner –
"nihil dicit" –
"nisi prius" –
No contest –
No fault divorce –
No fault insurance –
No-par stock –
Noble Eightfold Path –
"nolle prosequi" –
"nolo contendere" –
Nominal damages –
Nominal party –
Nominal value –
Nominee –
"non compos mentis" –
"non constat" –
"non est factum" –
"non liquet" –
"non obstante verdicto" –
Non-binding arbitration –
Non-conforming use –
Non-contestability clause –
Non-disclosure agreement –
Non-executive director –
Non-feasance –
Non-profit corporation –
Non-profit organization –
Non-suit –
Nonimmigrant visa –
Nonviolence –
Not guilty –
Not guilty by reason of insanity –
Not-for-profit corporation –
"nota bene" –
Notary public –
Notice –
Notice of appeal –
Notice of default –
Notice to quit –
Notorious possession –
Notwithstanding clause (Canadian Constitution) –
Novation –
Nuisance –
"nulla bona" –
"nulla poena sine lege" –
Nullity (conflict) –
"nullum crimen, nulla poena sine praevia lege poenali" –
"nunc pro tunc" –
Nuremberg Code –
Nuremberg Trials

O.R. –
O.S.C. –
Oath –
"obiter dicta" is plural; see the singular "obiter dictum" –
Object –
Objectivist philosophy –
Obligation –
Obligations of confidentiality –
Obligee –
Obligor –
Obscene –
Obscenity –
Obstruction of justice –
Occupancy –
Occupant –
Occupational disease –
Occupational hazard –
Occupy the field –
Of counsel –
Offender –
Offer of proof –
Offeree –
Offeror –
Officer of the court –
Officers of a corporation –
Official –
Official misconduct –
Official receiver –
Official Solicitor –
Officious intermeddler –
Offshore corporation –
Ombudsman –
Omission –
Omnibus clause –
On all fours –
"onus probandi" –
Open adoption –
Open court –
Open-source license –
Opening statement –
Operation of law –
"opinio juris sive necessitatis" –
Opinion –
Oppression remedy –
Oral argument –
Oral contract –
Oral examination –
Oral law –
Order –
Order in Council –
Order to show cause –
Ordinary (officer) –
Ordinary course of business –
Ordinary resolution –
Ordinary shares –
Organized crime –
Original jurisdiction –
Original sin –
Originating application –
Orphan –
Ostensible agent –
Ostensible authority –
Out of court –
Out-of-pocket expenses –
Outlaw –
Output contract –
Over-the-counter drug –
Overcharge –
Overt act –
Owe –
Own –
Own recognizance –
Owner –
Owner-occupier –
Ownership

"pacta sunt servanda" –
Pain and suffering –
Palimony –
Panderer –
"par delictum" –
Paralegal –
Paramount title –
Paraphilia –
Pardon –
"parens patriae" –
Parent –
Parent company –
Pari delicto –
Pari passu –
Paris Convention for the Protection of Industrial Property –
Parish –
Parliament –
Parliamentary procedure –
Parliamentary supremacy –
Parliamentary system –
Parody –
Parol –
Parol evidence rule –
Parole –
Parquet –
Partial breach –
Partial verdict –
Particulars –
Partition –
Partner –
Partnership –
Party –
Party of the first part –
Party of the second part –
Party wall –
Passenger –
Passing off –
Patent –
Patent ambiguity –
Patent Cooperation Treaty –
Patent infringement –
Patent pending –
Patentability –
Patently unreasonable –
Paternity –
Paternity suit –
Patient –
Patrimony of affectation –
Patronage –
Pay as you earn (paye) –
Payable –
Payee –
Payor –
Peace bond –
Peaceable possession –
Peculation –
Pecuniary –
Pedophilia –
Peeping tom –
Peer group –
Peerage –
Peer review –
Penal –
Penal code –
Penal colony –
Penal law –
Penal notice –
Penal transportation –
Penalty phase –
Penance –
Pendent jurisdiction –
"pendente lite" –
Pension plan –
Pension scheme –
People's Republic of China's trademark law –
"per capita" –
"per curiam" –
"per diem" –
"per minas" –
"per pro" –
"per quod" –
"per stirpes" –
Peremptory challenge –
Peremptory challenges –
Peremptory norm –
Peremptory writ of mandate –
Perfect –
Perfection (law) –
Perform –
Performance –
Perjurer –
Perjury –
Permanent Court of Arbitration –
Permanent injunction –
Permissive –
Perpetuity –
Person –
Person having ordinary skill in the art –
"persona non grata" –
Personal effects –
Personal jurisdiction –
Personal property –
Personal recognizance –
Personal representative –
Personal service –
Personality rights –
Personalty –
Perversion –
Petit jury –
Petition –
Petition for probate –
Petition to make special –
Petitioner –
Petty larceny –
Petty offenses –
Philosophy of law –
Physical custody –
Physician-patient privilege –
Picketing –
Pierce the corporate veil –
Piercing the corporate veil –
Pilferage –
Pillory –
Pimp –
Piracy –
Plagiarism –
Plain error –
Plain view doctrine –
Plaint note –
Plaint number –
Plaintiff –
Plc –
Plea –
Plea bargain –
Plea in abatement –
Plead –
Pleading –
Pleadings –
Plenary authority –
Police –
Police brutality –
Police oppression –
Police powers (United States constitutional law) –
Police state –
Corruption –
Political prisoner –
Political question –
Political science –
Poll tax (disambiguation) –
Polyandry –
Polygamy –
Bigamy –
Polygraph –
Pornography –
Port of entry –
Positive law –
Possession –
Possession of stolen goods –
Possession proceedings –
Possessory –
Possessory interest –
Possibility of a reverter –
"post mortem" –
Postdated check –
Pot –
Pour over will –
Poverty law –
Power –
Power of appointment –
Power of arrest –
Power of attorney –
Practice –
Practice Direction –
Practice of law –
Praemunire –
"praetor peregrinus" –
Pre-emption rights –
Precedent –
Preemption of state and local laws in the United States –
Preemptive right –
Preference –
Preferential creditor –
Preferred dividend –
Preferred stock –
Pregnant denial –
Preliminary hearing –
Preliminary injunction –
Premeditation –
Premises –
Prenuptial agreement –
Preponderance of the evidence –
Prerogative writ –
Prescription drug –
Prescriptive easement –
President of the family division –
Presiding judge –
Presumption –
Presumption of innocence –
Pretermitted heir –
Pretrial discovery –
Price fixing –
"prima facie" –
Prima facie case –
Prima impressionis –
Prime suspect –
"primogeniture" –
Prior restraint –
Prison –
Prisoner of war –
Privacy –
Private bill –
Private carrier –
Private company –
Private Express Statutes –
Private international law –
Private law –
Private nuisance –
Private parts –
Private property –
Private road –
Privateer –
Privilege (evidence) –
Privilege (legal ethics) –
Privilege against self incrimination –
Privileged communication –
Privity –
Privy Council –
Privy Council of Sweden –
"pro bono" –
"pro bono publico" –
"pro forma" –
"pro hac vice" –
"pro per" –
"pro se" –
"pro tanto" –
"pro tem" –
"pro tempore" –
Probable cause –
Probate –
Probation –
Probative –
Probative value –
Procedendo –
Procedural defense –
Procedural justice –
Procedural law –
Procedure –
Proceeding –
Process –
Process server –
Proctor –
Product liability –
Professional corporation –
Professional negligence –
Proffer –
Prohibition –
Writ of prohibition –
Promise –
Promissory estoppel –
Promissory note –
Property –
Property damage –
Property law –
Property tax –
"propria persona" –
Proprietary rights –
Proprietor –
Prosecute –
Prosecution –
Prosecutor –
Prostitute –
Prostitution –
Protective custody –
Protective order –
Protest –
Protocol –
Provisional remedy –
Proximate cause –
Prudent man rule –
Public –
Public administrator –
Public benefit corporation –
Public company –
Public corporation (disambiguation) –
Public defender –
Public domain –
Public figure –
Public limited company –
Public nuisance –
Public order –
Public property –
Public record –
Public trust doctrine –
Public trustee –
Public use –
Public utility –
Publication –
Publici juris –
Publish –
Puffery –
Puisne judge –
Punitive damages –
Putative father –
Putative father registry

"quaere" –
"quantum meruit" –
Quash –
"quasi" –
Quasi community property –
Quasi contract –
Quasi corporation –
Quasi in rem –
Quasi-contract –
Quasi-criminal –
Quasi-delict –
Quasi-judicial –
Queen's bench –
Queen's Privy Council for Canada –
Queens bench division –
Queen's counsel –
Question of fact –
Question of law –
"qui tam action" –
"quid pro quo" –
"quid pro quo sexual harassment" –
Quiet enjoyment –
Quiet title action –
Quitclaim deed –
Quitrent –
"quo warranto" –
Quorum –
Quotient verdict –
Qur'an

Rabbi –
Rabbinic literature –
Rabbinical Assembly –
Race to the courthouse –
Racial discrimination –
Racial segregation –
Racism –
Racketeer influenced corrupt organization (RICO) statute –
Racketeering –
Radical transparency –
Ransom –
Rape –
Ratification –
Ratify –
"ratio decidendi" –
"ratio scripta" –
Rational basis –
"Ratum sed non consummatum" –
Real estate –
Real estate investment trust –
Real party in interest –
Real property –
Realty –
Reasonable –
Reasonable care –
Reasonable doubt –
Reasonable man doctrine –
Reasonable time –
Rebbe –
"rebus sic stantibus" –
Rebuttable presumption –
Rebuttal –
Recapture –
Receipt –
Receivership –
Recharacterisation –
Recidivist –
Reciprocal discovery –
Reckless –
Reckless disregard –
Reckless driving –
Recklessness –
Recognisance –
Reconstructionist Judaism –
Reconveyance –
Recorder –
Recording acts –
Recoupment –
Recover –
Recoverable –
Recusal –
Rectification (law) –
Recuse –
Redemption (bonds) –
Redemption of shares –
Redemption value –
Redetermination –
Redirect examination –
Redundancy –
Reentry –
Referee –
Referendum –
Reform Judaism –
Refugee –
Refundable tax credit –
Registered office –
Registered trade mark –
Registration statement –
Registry of deeds –
Regulation –
Regulations –
Regulatory taking –
Rehearing –
Reichstag Fire Decree –
Reid technique –
Release –
Release on one's own recognizance –
Relevancy –
Relief –
Religion and heterosexuality –
Religion and homosexuality –
Religious law –
Remainder –
Remainderman –
Remand (court procedure) –
Remittitur –
Rent –
Rent control –
Rental value –
Reorganization –
Repair –
Repeal –
Repentance –
Replevin –
Reply brief –
Reports –
Repossess –
Represent –
Representation –
Reprisal –
Reputation –
Requirements contract –
"res adjudicata" –
"res gestae" –
"res ipsa loquitur" –
"res judicata" –
"res nulis" –
"res publica christiana" –
Resale –
Rescind –
Rescission –
Rescue doctrine –
Reservation –
Reserved decision –
Resident –
Resident alien –
Residuary bequest –
Residuary estate –
Residuary legatee –
Residue –
Resistance movement –
Resisting arrest –
Resolution –
Resolution of disputes –
"respondeat superior" –
Responsa –
Responsibility –
Restatement of the law –
Restitution –
Restorative justice –
Restraining order –
Restraint of trade –
Restraint on alienation –
Restrictive covenant –
Result –
Resulting trust –
Retaining lien –
Retention of title clause –
Retire –
Retraction –
Retrial –
Retributive justice –
Return of service –
Revenue ruling –
Reversible error –
Reversion –
Review –
Revocable living trust –
Revocation –
Revoke –
RICO –
Right of audience –
Right of eminent domain –
Right of survivorship –
Right of the first night –
Right-of-way –
Right to privacy –
Right to silence –
Right-to-work laws –
Rights –
Riot –
Riot control agent –
Riparian –
Riparian rights –
Risk –
Risk of loss –
Ritual –
Roadside test –
Robbery –
Robert's Rules of Order –
Rocket docket –
Rogatory letters –
Roman Forum –
Roman Inquisition –
Roman law –
Room –
Royal Assent –
Royal Charter –
Royal Commission –
Royal Courts of Justice –
Royal Prerogative –
Royal Warrant –
Royalties –
Rule –
Rule against perpetuities –
Rule by decree –
Rule in Allhusen v Whittell –
Rule in Re Atkinson –
Rule in Bartlett v Barclays Bank –
Rule in Clayton's Case –
Rule in Dearle v Hall –
Rule in Dumpor's Case –
Rule in Howe v Earl of Dartmouth –
Rule in Saunders v Vautier –
Rule in Shelley's Case –
Rule in Wild's Case –
Rule of law –
Rulemaking –
Rules of evidence –
Ruling –
Rum-running –
Running with the land –
Ruse of war

Sabotage –
Sacred text –
Salafi –
Sales tax –
Samaritan Pentateuch –
Same-sex marriage –
Sanctions –
Sanhedrin –Sasine –
Satyagraha –
Save harmless –
Savings and loan –
Scapegoat –
School of law –
Sciens –
"scienter" –
"scire facias" –
Scope of employment –
Scots law – 
Scrivener –
Scutage –
"se defendendo" –
Seal –
Sealed verdict –
Sealing of records –
Search and seizure –
Search warrant –
Second degree murder –
Secondary boycott –
Secret police –
Secret rebate –
Secret tribunal –
Secret trust –
Secretary of State for the Home Department –
Secularism –
Secured creditor –
Secured transaction –
Security –
Security agreement –
Security deposit –
Security for costs –
Security interest –
Security of tenure –
Sedition –
Seduction –
Seigniorage –
Seised –
Seisin –
Seized –
Seizure –
Self-dealing –
Self-defense –
Self-determination –
Self-help –
Self-incrimination –
Seller –
Semble –
Semicha –
Senior lien –
Sentence (law) –
Separate property –
Separation –
Separation of church and state –
Separation of powers –
Separatism –
Septuagint –
Serf –
"seriatim" –
Servant –
Service –
Service by fax –
Service by mail –
Service by publication –
Service mark –
Service of process –
Services –
Servient estate –
Set-aside –
Set-off –
Setting –
Settle –
Settlement –
Settlement agreement –
Settlor –
Seven deadly sins –
Severable contract –
Several liability –
Sex offender –
Sex offender registries in the United States –
Sex tourism –
Sex worker –
Sex-related court cases –
Sexual abuse –
Sexual assault –
Sexual discrimination –
Sexual harassment –
Sexual morality –
Sexual norm –
Shafi'i –
Shaikh –
Shall –
Shame –
Share –
Share capital –
Share certificate –
Shareholder –
Shareholders agreement –
Shareholders' agreement –
Shareholders' derivative action –
Shareholders' meeting –
Sharia law –Sharp practice –
Shepardize –
Sheriff –
Sheriff's sale –
Shield laws –
Shifting the burden of proof –
Shoplifting –
Short cause –
Shortening time –
Show cause order –
Shulkhan Arukh –
Sick pay –
Sidebar –
Sign –
Signature –
Signing bonus –
Silk –
Simple trust –
Simultaneous death act –
Sin –
Sin-offering –
"sine die" –
"sine qua non" –
Single life annuity –
Situated ethics –
Situational ethics –
"situs (law)" –
Slander –
Slander of title –
Slavery –
Slavery at common law –
Small claims court –
Small claims track –
Smuggling –
Socage –
Social capital –
Social control –
Social justice –
Socialist law –
Society for Animal Protective Legislation –
Sodomy –
Sodomy law –
Software license –
Software patent –
Copyright infringement of software –
Sole proprietorship –
Solicitation –
Solicitor –
Solitary confinement –
Solvency –
Solvent –
Sound mind and memory –
Sounds in –
Southern Poverty Law Center –
Sovereign immunity –
Sovereignty –
Spanish Constitution of 1978 –
Spanish Inquisition –
Speaking demurrer –
Special administrator –
Special appearance –
Special damages –
Special master –
Special prosecutor –
Special resolution –
Special verdict –
Specific bequest –
Specific devise –
Specific finding –
Specific legacy –
Specific performance –
Speculative damages –
Speed limit –
Speed trap –
Speedy trial –
Spendthrift clause –
Spendthrift trust –
Spoliation of evidence –
Spontaneous exclamation –
Spot zoning –
Spousal abuse –
Spousal support –
Springing interest –
Squatter –
Squatting –
Stakeholder –
Stamp duty –
Standard form contract –
Standard of care –
Standing –
Star Chamber –
Star chamber proceedings –
"stare decisis" –
State action –
State of domicile –
State of Emergency –
State religion –
State-owned enterprise –
Stationhouse bail –
Statism –
Status conference –
Statute –
Statute of frauds –
Statute of limitations –
Statutes of fraud –
Statutes of limitations –
Statutory Instrument –
Statutory law –
Statutory offer of settlement –
Statutory rape –
Stay away order –
Stay of execution –
Stay of proceedings –
Stet –
Stipendiary magistrate –
Stipulation –
Stock –
Stock certificate –
Stock in trade –
Stock option –
Stockholder –
Stockholders' derivative action –
Stoning –
Stop and frisk –
Strata title –
Strategic lawsuits against public participation –
Straw deed –
Straw man –
Street –
Strict construction –
Strict liability –
Strike –
Strike action –
Structure –
"sua sponte" –
"sub judice" –
"sub modo" –
"sub nomine" –
"sub silentio" –
Sub-tenant –
Subchapter S corporation –
Subcontractor –
Sublease –
Sublet –
Submitted –
Subordination –
Subordination agreement –
Subornation of perjury –
"subpoena" –
"subpoena ad testificum" –
"subpoena duces tecum" –
Subrogation –
Subrogee –
Subrogor –
Subscribe –
Subscribers –
Subsidiary company –
Substantial performance –
Substantive law –
Substituted service –
Substitution of attorney –
Succession –
Successive sentences –
Suffering –
Suffrage –
Suggestion of death -
"sui generis" –
Suicide –
Suitor –
Sum certain –
Summary adjudication of issues –
Summary assessment –
Summary dismissal –
Summary judgment –
Summary offence –
Summation –
Summing –
Summons –
Sunnah –
Superior court –
"supersedeas" –
Superseding cause –
Suppression of evidence –
Supremacy clause –
Supreme court –
Supreme Court of Canada –
Supreme Court of India –
Supreme Court of judicature –
Supreme Court of New Zealand –
Surety –
Surplusage –
Surrebutal –
Surrender –
Surrogate court –
Survivorship –
Suspended sentence –
Sustain –
Syndicate –
Synod –
Synthetic lease

T.R.O. –
Table A –
Tacking (law) –
Tainted evidence –
Taking the fifth –
Tallage –
Talmud –
Tangible personal property –
Tangible property –
Taqlid –
Targeting civilians –
Targum –
Tax –
Tax avoidance –
Tax costs –
Tax credit –
Tax deduction –
Tax evasion –
Tax haven –
Tax law –
Tax sale –
Tax treaty –
Taxation in the United States –
Taxation of costs –
Temporary injunction –
Temporary insanity –
Ten Commandments –
Tenancy –
Tenancy at sufferance –
Tenancy at will –
Tenancy by the entirety –
Tenancy in common –
Tenement –
Tentative trust –
Tenure –
Terms and conditions of employment –
Terms and conditions of purchase –
Terms and conditions of sale –
Terms of disparagement –
"terra nullius" –
Territorial integrity –
Terrorism –
Test Act –
Testacy –
Testamentary –
Testamentary capacity –
Testamentary disposition –
Testamentary trust –
Testate –
Testator –
Testatrix –
Testify –
Testimony –
Texas Declaration of Independence –
Crown –
Old Bailey –
The problem of evil –
Theft –
Theocracy –
Third-party beneficiary –
Thirty-day notice –
Three strikes law –
Three theological virtues –
Tide lands –
Time is of the essence –
Time served –
Timeshare –
Tipstaff –
Tithe –
Title –
Title abstract –
Title insurance –
Title report –
Title search –
Toll –
Toll bridge –
Toll road –
Tontine –
Tools of trade –
Torah –
Torah study –
Torrens title –
Tort –
Tort claims act –
Tortfeasor –
Tortious –
Torture –
Tosafists –
Tosefta –
Total depravity –
Totalitarian democracy –
Totalitarianism –
Totten doctrine –
Totten trust –
Tracing (law) –
Trade –
Trade fixture –
Trade name –
Trade secret –
Trade union –
Trade-Related aspects of Intellectual Property rights –
Trademark –
Trademarks registry –
Tragedy of the commons –
Transfer agent –
Transfer in contemplation of death –
Transfer of shares –
Transferred intent –
Transparency –
Treason –
Treasure trove –
Treasury security –
Treasury stock –
Treaty –
Treaty of Waitangi –
Treble damages –
Trespass –
Trial –
Trial advocacy –
Trial by combat –
Trial by ordeal –
Trial court –
Trial de novo –
Trial "in absentia" –
Tribunal –
Tribute –
Trier of fact –
"trinoda necessitas" –
Triple net lease –
Truancy –
True bill –
Trust law –
Trust fund –
Trust instrument –
Trustee –
Trustee in bankruptcy –
Trustor –
Trusts and estates –
Truth in Lending Act –
Try title –
Turn state's evidence –
Twelve Tables –
Twinkie defense

Uberrima fides –
UCC-1 –
Ulema –
Ultimate fact –
"ultra vires" –
Ultrahazardous activity –
Unclean hands –
Unconscionable –
Unconstitutional –
Under the influence –
Underground Railroad –
Underwrite –
Underwriter –
Underwriting agreement –
Undisclosed principal –
Undivided interest –
Undue influence –
Unfair competition –
Unfair dismissal –
Unfree labour –
Unified estate and gift tax –
Uniform Code of Military Justice –
Uniform Commercial Code –
Uniform reciprocal enforcement of support act –
Unilateral contract –
Uninsured motorist clause –
Unissued stock –
Unitary state –
United Nations Charter –
United Nations Convention Against Torture –
United States bankruptcy court –
United States Bill of Rights –
United States Code –
United States Constitution –
United States constitutional law –
United States court of appeals –
United States Declaration of Independence –
United States Department of Justice –
United States district court –
United States Federal Income Tax Personal Exemption –
United States federal judicial circuit –
United States federal judicial district –
United States Office of the Independent Counsel –
United States Patent and Trademark Office –
United States prison population –
United States Supreme Court –
United States tax reform –
Trademark Law (United States) –
Universal Declaration on Animal Welfare –
Universal Declaration of Human Rights –
Universal jurisdiction –
Unjust enrichment –
Unjust taking –
Unlawful –
Unlawful assembly –
Unlawful detainer –
"uno flatu" –
Unofficial law –
Unreasonable search and seizure –
Unspecified claim –
Use tax –
Usucaption –
Usufruct –
Usurious –
Usury –
"uti possidetis" –
Utilitarianism –
Utility (patent)

Vacate –
"Vacatio legis" –
Valid claim –
Valuable consideration –
Variance –
Vehicular manslaughter –
"vel non" –
Vendee –
Vendor –
"venire" –
Venue (law) –
Verdict –
Vest –
Vested –
Vested remainder –
Vested right –
Vexatious litigation –
Vicarious liability –
Vice-Chancellor (UK legal system) –
"vice versa" –
"vide" –
"videlicet" –
Vienna Convention on Diplomatic Relations –
Vienna Convention on the Law of Treaties –
Vigilante –
Violence –
Virginia Declaration of Rights –
Virtue ethics –
Virtue jurisprudence –
Visitation right –
"viz." –
Void –
Void for vagueness –
Void marriage –
Voidable –
Voidable marriage –
Voir dire –
Volens –
Voluntary association –
Voluntary bankruptcy –
Voting trust –
Vulgate

Wage execution –
Wahhabism –
Waive –
Waiver –
Walking possession –
Waqf –
War crime –
War Crimes Law (Belgium) –
War on Drugs –
War Powers Resolution –
War reparations –
Ward (legal) –
Ward of court –
Wardship –
Warrant (legal) –
Warrant of committal –
Warrant of delivery –
Warrant of execution –
Warrant of possession –
Warranty –
Warranty deed –
Waste –
Watered stock –
Weimar constitution –
West American Digest System –
Wet reckless –
Whiplash (medicine) –
Whistleblower –
White collar crime –
Widow –
Widow's election –
Widower –
Will –
Will contest –
Willful –
Willfully –
Winding up –
Window tax –
Wiretap –
Witchhunt –
Witness –
Witness stand –
Witness statement –
Words of art –
Work stoppage –
Workers' compensation –
Workers' compensation acts –
Workmen's compensation –
World Declaration on Great Apes –
World Intellectual Property Organization –
World Trade Organization –
Writ –
Writ of attachment –
Writ of coram nobis –
Writ of execution –
Writ of mandate –
Wrongful death –
Wrongful discharge –
Wrongful dismissal –
Wrongful termination –
Wrongful trading

Nothing Law related seems to begin with X, if you find something please add it!

Yellow Dog contract –
Yeshiva –
Your honor –
Youthful offender

Zoning


</doc>
<doc id="18949668" url="https://en.wikipedia.org/wiki?curid=18949668" title="Law">
Law

Law is commonly understood as a system of rules that are created and enforced through social or governmental institutions to regulate conduct, although its precise definition is a matter of longstanding debate. It has been variously described as a science and the art of justice. State-enforced laws can be made by a collective legislature or by a single legislator, resulting in statutes, by the executive through decrees and regulations, or established by judges through precedent, normally in common law jurisdictions. Private individuals can create legally binding contracts, including arbitration agreements that may elect to accept alternative arbitration to the normal court process. The formation of laws themselves may be influenced by a constitution, written or tacit, and the rights encoded therein. The law shapes politics, economics, history and society in various ways and serves as a mediator of relations between people.

Legal systems vary between countries, with their differences analysed in comparative law. In civil law jurisdictions, a legislature or other central body codifies and consolidates the law. In common law systems, judges make binding case law through precedent, although on occasion case law may be overturned by a higher court or the legislature. Historically, religious law influenced secular matters, and is still used in some religious communities. Sharia law based on Islamic principles is used as the primary legal system in several countries, including Iran and Saudi Arabia.

Law's scope can be divided into two domains. Public law concerns government and society, including constitutional law, administrative law, and criminal law. Private law deals with legal disputes between individuals and/or organisations in areas such as contracts, property, torts/delicts and commercial law. This distinction is stronger in civil law countries, particularly those with a separate system of administrative courts; by contrast, the public-private law divide is less pronounced in common law jurisdictions.

Law provides a source of scholarly inquiry into legal history, philosophy, economic analysis and sociology. Law also raises important and complex issues concerning equality, fairness, and justice.

The philosophy of law is commonly known as jurisprudence. Normative jurisprudence asks "what should law be?", while analytic jurisprudence asks "what is law?" John Austin's utilitarian answer was that law is "commands, backed by threat of sanctions, from a sovereign, to whom people have a habit of obedience". Natural lawyers on the other side, such as Jean-Jacques Rousseau, argue that law reflects essentially moral and unchangeable laws of nature. The concept of "natural law" emerged in ancient Greek philosophy concurrently and in connection with the notion of justice, and re-entered the mainstream of Western culture through the writings of Thomas Aquinas, notably his "Treatise on Law".

Hugo Grotius, the founder of a purely rationalistic system of natural law, argued that law arises from both a social impulse—as Aristotle had indicated—and reason. Immanuel Kant believed a moral imperative requires laws "be chosen as though they should hold as universal laws of nature". Jeremy Bentham and his student Austin, following David Hume, believed that this conflated the "is" and what "ought to be" problem. Bentham and Austin argued for law's positivism; that real law is entirely separate from "morality". Kant was also criticised by Friedrich Nietzsche, who rejected the principle of equality, and believed that law emanates from the will to power, and cannot be labeled as "moral" or "immoral".

In 1934, the Austrian philosopher Hans Kelsen continued the positivist tradition in his book the "Pure Theory of Law". Kelsen believed that although law is separate from morality, it is endowed with "normativity", meaning we ought to obey it. While laws are positive "is" statements (e.g. the fine for reversing on a highway "is" €500); law tells us what we "should" do. Thus, each legal system can be hypothesised to have a basic norm ("Grundnorm") instructing us to obey. Kelsen's major opponent, Carl Schmitt, rejected both positivism and the idea of the rule of law because he did not accept the primacy of abstract normative principles over concrete political positions and decisions. Therefore, Schmitt advocated a jurisprudence of the exception (state of emergency), which denied that legal norms could encompass all of the political experience.

Later in the 20th century, H. L. A. Hart attacked Austin for his simplifications and Kelsen for his fictions in "The Concept of Law". Hart argued law is a system of rules, divided into primary (rules of conduct) and secondary ones (rules addressed to officials to administer primary rules). Secondary rules are further divided into rules of adjudication (to resolve legal disputes), rules of change (allowing laws to be varied) and the rule of recognition (allowing laws to be identified as valid). Two of Hart's students continued the debate: In his book "Law's Empire", Ronald Dworkin attacked Hart and the positivists for their refusal to treat law as a moral issue. Dworkin argues that law is an "interpretive concept", that requires judges to find the best fitting and most just solution to a legal dispute, given their constitutional traditions. Joseph Raz, on the other hand, defended the positivist outlook and criticised Hart's "soft social thesis" approach in "The Authority of Law". Raz argues that law is authority, identifiable purely through social sources and without reference to moral reasoning. In his view, any categorisation of rules beyond their role as authoritative instruments in mediation are best left to sociology, rather than jurisprudence.

There have been several attempts to produce "a universally acceptable definition of law". In 1972, Baron Hampstead suggested that no such definition could be produced. McCoubrey and White said that the question "what is law?" has no simple answer. Glanville Williams said that the meaning of the word "law" depends on the context in which that word is used. He said that, for example, "early customary law" and "municipal law" were contexts where the word "law" had two different and irreconcilable meanings. Thurman Arnold said that it is obvious that it is impossible to define the word "law" and that it is also equally obvious that the struggle to define that word should not ever be abandoned. It is possible to take the view that there is no need to define the word "law" (e.g. "let's forget about generalities and get down to cases").

One definition is that law is a system of rules and guidelines which are enforced through social institutions to govern behaviour. In "The Concept of Law" Hart argued law is a "system of rules"; Austin said law was "the command of a sovereign, backed by the threat of a sanction"; Dworkin describes law as an "interpretive concept" to achieve justice in his text titled "Law's Empire"; and Raz argues law is an "authority" to mediate people's interests. Holmes said, "The prophecies of what the courts will do in fact, and nothing more pretentious, are what I mean by the law." In his "Treatise on Law" Aquinas argues that law is a rational ordering of things which concern the common good that is promulgated by whoever is charged with the care of the community. This definition has both positivist and naturalist elements.

The history of law links closely to the development of civilization. Ancient Egyptian law, dating as far back as 3000 BC, was based on the concept of Ma'at and characterised by tradition, rhetorical speech, social equality and impartiality. By the 22nd century BC, the ancient Sumerian ruler Ur-Nammu had formulated the first law code, which consisted of casuistic statements ("if … then ..."). Around 1760 BC, King Hammurabi further developed Babylonian law, by codifying and inscribing it in stone. Hammurabi placed several copies of his law code throughout the kingdom of Babylon as stelae, for the entire public to see; this became known as the Codex Hammurabi. The most intact copy of these stelae was discovered in the 19th century by British Assyriologists, and has since been fully transliterated and translated into various languages, including English, Italian, German, and French.

The Old Testament dates back to 1280 BC and takes the form of moral imperatives as recommendations for a good society. The small Greek city-state, ancient Athens, from about the 8th century BC was the first society to be based on broad inclusion of its citizenry, excluding women and the slave class. However, Athens had no legal science or single word for "law", relying instead on the three-way distinction between divine law ("thémis"), human decree ("nomos") and custom ("díkē"). Yet Ancient Greek law contained major constitutional innovations in the development of democracy.

Roman law was heavily influenced by Greek philosophy, but its detailed rules were developed by professional jurists and were highly sophisticated. Over the centuries between the rise and decline of the Roman Empire, law was adapted to cope with the changing social situations and underwent major codification under Theodosius II and Justinian I. Although codes were replaced by custom and case law during the Early Middle Ages, Roman law was rediscovered around the 11th century when medieval legal scholars began to research Roman codes and adapt their concepts to the canon law, giving birth to the "jus commune". Latin legal maxims (called brocards) were compiled for guidance. In medieval England, royal courts developed a body of precedent which later became the common law. A Europe-wide Law Merchant was formed so that merchants could trade with common standards of practice rather than with the many splintered facets of local laws. The Law Merchant, a precursor to modern commercial law, emphasised the freedom to contract and alienability of property. As nationalism grew in the 18th and 19th centuries, the Law Merchant was incorporated into countries' local law under new civil codes. The Napoleonic and German Codes became the most influential. In contrast to English common law, which consists of enormous tomes of case law, codes in small books are easy to export and easy for judges to apply. However, today there are signs that civil and common law are converging. EU law is codified in treaties, but develops through "de facto" precedent laid down by the European Court of Justice.

Ancient India and China represent distinct traditions of law, and have historically had independent schools of legal theory and practice. The "Arthashastra", probably compiled around 100 AD (although it contains older material), and the "Manusmriti" (c. 100–300 AD) were foundational treatises in India, and comprise texts considered authoritative legal guidance. Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia. During the Muslim conquests in the Indian subcontinent, sharia was established by the Muslim sultanates and empires, most notably Mughal Empire's Fatawa-e-Alamgiri, compiled by emperor Aurangzeb and various scholars of Islam. After British colonialism, the Hindu tradition, along with Islamic law, was supplanted by the common law when India became part of the British Empire. Malaysia, Brunei, Singapore and Hong Kong also adopted the common law. The eastern Asia legal tradition reflects a unique blend of secular and religious influences. Japan was the first country to begin modernising its legal system along western lines, by importing parts of the French, but mostly the German Civil Code. This partly reflected Germany's status as a rising power in the late 19th century. Similarly, traditional Chinese law gave way to westernisation towards the final years of the Qing Dynasty in the form of six private law codes based mainly on the Japanese model of German law. Today Taiwanese law retains the closest affinity to the codifications from that period, because of the split between Chiang Kai-shek's nationalists, who fled there, and Mao Zedong's communists who won control of the mainland in 1949. The current legal infrastructure in the People's Republic of China was heavily influenced by Soviet Socialist law, which essentially inflates administrative law at the expense of private law rights. Due to rapid industrialisation, today China is undergoing a process of reform, at least in terms of economic, if not social and political, rights. A new contract code in 1999 represented a move away from administrative domination. Furthermore, after negotiations lasting fifteen years, in 2001 China joined the World Trade Organization.

In general, legal systems can be split between civil law and common law systems. Modern scholars argue that the significance of this distinction has progressively declined; the numerous legal transplants, typical of modern law, result in the sharing by modern legal systems of many features traditionally considered typical of either common law or civil law. The term "civil law", referring to the civilian legal system originating in continental Europe, should not be confused with "civil law" in the sense of the common law topics distinct from criminal law and public law. 

The third type of legal system—accepted by some countries without separation of church and state—is religious law, based on scriptures. The specific system that a country is ruled by is often determined by its history, connections with other countries, or its adherence to international standards. The sources that jurisdictions adopt as authoritatively binding are the defining features of any legal system. Yet classification is a matter of form rather than substance since similar rules often prevail.

Civil law is the legal system used in most countries around the world today. In civil law the sources recognised as authoritative are, primarily, legislation—especially codifications in constitutions or statutes passed by government—and custom. Codifications date back millennia, with one early example being the Babylonian "Codex Hammurabi". Modern civil law systems essentially derive from legal codes issued by Byzantine Emperor Justinian I in the 6th century, which were rediscovered by 11th century Italy. Roman law in the days of the Roman Republic and Empire was heavily procedural, and lacked a professional legal class. Instead a lay magistrate, "iudex", was chosen to adjudicate. Decisions were not published in any systematic way, so any case law that developed was disguised and almost unrecognised. Each case was to be decided afresh from the laws of the State, which mirrors the (theoretical) unimportance of judges' decisions for future cases in civil law systems today. From 529–534 AD the Byzantine Emperor Justinian I codified and consolidated Roman law up until that point, so that what remained was one-twentieth of the mass of legal texts from before. This became known as the "Corpus Juris Civilis". As one legal historian wrote, "Justinian consciously looked back to the golden age of Roman law and aimed to restore it to the peak it had reached three centuries before." The Justinian Code remained in force in the East until the fall of the Byzantine Empire. Western Europe, meanwhile, relied on a mix of the Theodosian Code and Germanic customary law until the Justinian Code was rediscovered in the 11th century, and scholars at the University of Bologna used it to interpret their own laws. Civil law codifications based closely on Roman law, alongside some influences from religious laws such as canon law, continued to spread throughout Europe until the Enlightenment; then, in the 19th century, both France, with the "Code Civil", and Germany, with the "Bürgerliches Gesetzbuch", modernised their legal codes. Both these codes influenced heavily not only the law systems of the countries in continental Europe (e.g. Greece), but also the Japanese and Korean legal traditions. Today, countries that have civil law systems range from Russia and Turkey to most of Central and Latin America. 

"Socialist law" refers to the legal systems in socialist and formerly socialist states such as the former Soviet Union and the People's Republic of China. Academic opinion is divided on whether it is a separate system from civil law, given major deviations based on Marxist-Leninist ideology, such as subordinating the judiciary to the executive ruling party.

In common law legal systems, decisions by courts are explicitly acknowledged as "law" on equal footing with statutes adopted through the legislative process and with regulations issued by the executive branch. The "doctrine of precedent", or "stare decisis" (Latin for "to stand by decisions") means that decisions by higher courts bind lower courts, and future decisions of the same court, to assure that similar cases reach similar results. In contrast, in "civil law" systems, legislative statutes are typically more detailed, and judicial decisions are shorter and less detailed, because the judge or barrister is only writing to decide the single case, rather than to set out reasoning that will guide future courts.

Common law originated from England and has been inherited by almost every country once tied to the British Empire (except Malta, Scotland, the U.S. state of Louisiana, and the Canadian province of Quebec). In medieval England, the Norman conquest the law varied-shire-to-shire, based on disparate tribal customs. The concept of a "common law" developed during the reign of Henry II during the late 12th century, when Henry appointed judges that had authority to create an institutionalised and unified system of law "common" to the country. The next major step in the evolution of the common law came when King John was forced by his barons to sign a document limiting his authority to pass laws. This "great charter" or "Magna Carta" of 1215 also required that the King's entourage of judges hold their courts and judgments at "a certain place" rather than dispensing autocratic justice in unpredictable places about the country. A concentrated and elite group of judges acquired a dominant role in law-making under this system, and compared to its European counterparts the English judiciary became highly centralised. In 1297, for instance, while the highest court in France had fifty-one judges, the English Court of Common Pleas had five. This powerful and tight-knit judiciary gave rise to a systematised process of developing common law.

However, the system became overly systematised—overly rigid and inflexible. As a result, as time went on, increasing numbers of citizens petitioned the King to override the common law, and on the King's behalf the Lord Chancellor gave judgment to do what was equitable in a case. From the time of Sir Thomas More, the first lawyer to be appointed as Lord Chancellor, a systematic body of equity grew up alongside the rigid common law, and developed its own Court of Chancery. At first, equity was often criticised as erratic, that it varied according to the length of the Chancellor's foot. Over time, courts of equity developed solid principles, especially under Lord Eldon. In the 19th century in England, and in 1937 in the U.S., the two systems were merged.

In developing the common law, academic writings have always played an important part, both to collect overarching principles from dispersed case law, and to argue for change. William Blackstone, from around 1760, was the first scholar to collect, describe, and teach the common law. But merely in describing, scholars who sought explanations and underlying structures slowly changed the way the law actually worked.

Religious law is explicitly based on religious precepts. Examples include the Jewish Halakha and Islamic Sharia—both of which translate as the "path to follow"—while Christian canon law also survives in some church communities. Often the implication of religion for law is unalterability, because the word of God cannot be amended or legislated against by judges or governments. However, a thorough and detailed legal system generally requires human elaboration. For instance, the Quran has some law, and it acts as a source of further law through interpretation, "Qiyas" (reasoning by analogy), "Ijma" (consensus) and precedent. This is mainly contained in a body of law and jurisprudence known as Sharia and Fiqh respectively. Another example is the Torah or Old Testament, in the Pentateuch or Five Books of Moses. This contains the basic code of Jewish law, which some Israeli communities choose to use. The Halakha is a code of Jewish law that summarizes some of the Talmud's interpretations. Nevertheless, Israeli law allows litigants to use religious laws only if they choose. Canon law is only in use by members of the Catholic Church, the Eastern Orthodox Church and the Anglican Communion.

Canon law (from Greek "kanon", a 'straight measuring rod, ruler') is a set of ordinances and regulations made by ecclesiastical authority (Church leadership), for the government of a Christian organisation or church and its members. It is the internal ecclesiastical law governing the Catholic Church (both the Latin Church and the Eastern Catholic Churches), the Eastern Orthodox and Oriental Orthodox churches, and the individual national churches within the Anglican Communion. The way that such church law is legislated, interpreted and at times adjudicated varies widely among these three bodies of churches. In all three traditions, a canon was originally a rule adopted by a church council; these canons formed the foundation of canon law.

The Catholic Church has the oldest continuously functioning legal system in the western world, predating the evolution of modern European civil law and common law systems. The 1983 Code of Canon Law governs the Latin Church "sui juris". The Eastern Catholic Churches, which developed different disciplines and practices, are governed by the "Code of Canons of the Eastern Churches". The canon law of the Catholic Church influenced the common law during the medieval period through its preservation of Roman law doctrine such as the presumption of innocence.

Until the 18th century, Sharia law was practiced throughout the Muslim world in a non-codified form, with the Ottoman Empire's Mecelle code in the 19th century being a first attempt at codifying elements of Sharia law. Since the mid-1940s, efforts have been made, in country after country, to bring Sharia law more into line with modern conditions and conceptions. In modern times, the legal systems of many Muslim countries draw upon both civil and common law traditions as well as Islamic law and custom. The constitutions of certain Muslim states, such as Egypt and Afghanistan, recognise Islam as the religion of the state, obliging legislature to adhere to Sharia. Saudi Arabia recognises Quran as its constitution, and is governed on the basis of Islamic law. Iran has also witnessed a reiteration of Islamic law into its legal system after 1979. During the last few decades, one of the fundamental features of the movement of Islamic resurgence has been the call to restore the Sharia, which has generated a vast amount of literature and affected world politics.

There are distinguished methods of legal reasoning (applying the law) and methods of interpreting (construing) the law. The former are legal syllogism, which holds sway in civil law legal systems, analogy, which is present in common law legal systems, especially in the US, and argumentative theories that occur in both systems. The latter are different rules (directives) of legal interpretation such as directives of linguistic interpretation, teleological interpretation or systemic interpretation as well as more specific rules, for instance, golden rule or mischief rule. There are also many other arguments and cannons of interpretation which altogether make statutory interpretation possible.

Law professor and former United States Attorney General Edward H. Levi noted that the "basic pattern of legal reasoning is reasoning by example" - that is, reasoning by comparing outcomes in cases resolving similar legal questions. In a U.S. Supreme Court case regarding procedural efforts taken by a debt collection company to avoid errors, Justice Sotomayor cautioned that "legal reasoning is not a mechanical or strictly linear process".

Jurimetrics is the formal application of quantitative methods, especially probability and statistics, to legal questions. The use of statistical methods in court cases and law review articles has grown massively in importance in the last few decades.

The main institutions of law in industrialised countries are independent courts, representative parliaments, an accountable executive, the military and police, bureaucratic organisation, the legal profession and civil society itself. John Locke, in his "Two Treatises of Government", and Baron de Montesquieu in "The Spirit of the Laws", advocated for a separation of powers between the political, legislature and executive bodies. Their principle was that no person should be able to usurp all powers of the state, in contrast to the absolutist theory of Thomas Hobbes' "Leviathan". Sun Yat-sen's Five Power Constitution for the Republic of China took the separation of powers further by having two additional branches of government - a Control Yuan for auditing oversight and an Examination Yuan to manage the employment of public officials.

Max Weber and others reshaped thinking on the extension of state. Modern military, policing and bureaucratic power over ordinary citizens' daily lives pose special problems for accountability that earlier writers such as Locke or Montesquieu could not have foreseen. The custom and practice of the legal profession is an important part of people's access to justice, whilst civil society is a term used to refer to the social institutions, communities and partnerships that form law's political basis.

A judiciary is a number of judges mediating disputes to determine outcome. Most countries have systems of appeal courts, with an apex court as the ultimate judicial authority. In the United States, this authority is the Supreme Court; in Australia, the High Court; in the UK, the Supreme Court; in Germany, the "Bundesverfassungsgericht"; and in France, the "Cour de Cassation". For most European countries the European Court of Justice in Luxembourg can overrule national law, when EU law is relevant. The European Court of Human Rights in Strasbourg allows citizens of the Council of Europe member states to bring cases relating to human rights issues before it.
Some countries allow their highest judicial authority to overrule legislation they determine to be unconstitutional. For example, in "Brown v. Board of Education", the United States Supreme Court nullified many state statutes that had established racially segregated schools, finding such statutes to be incompatible with the Fourteenth Amendment to the United States Constitution.

A judiciary is theoretically bound by the constitution, just as all other government bodies are. In most countries judges may only interpret the constitution and all other laws. But in common law countries, where matters are not constitutional, the judiciary may also create law under the doctrine of precedent. The UK, Finland and New Zealand assert the ideal of parliamentary sovereignty, whereby the unelected judiciary may not overturn law passed by a democratic legislature.

In communist states, such as China, the courts are often regarded as parts of the executive, or subservient to the legislature; governmental institutions and actors exert thus various forms of influence on the judiciary. In Muslim countries, courts often examine whether state laws adhere to the Sharia: the Supreme Constitutional Court of Egypt may invalidate such laws, and in Iran the Guardian Council ensures the compatibility of the legislation with the "criteria of Islam".

Prominent examples of legislatures are the Houses of Parliament in London, the Congress in Washington D.C., the Bundestag in Berlin, the Duma in Moscow, the Parlamento Italiano in Rome and the "Assemblée nationale" in Paris. By the principle of representative government people vote for politicians to carry out "their" wishes. Although countries like Israel, Greece, Sweden and China are unicameral, most countries are bicameral, meaning they have two separately appointed legislative houses.

In the 'lower house' politicians are elected to represent smaller constituencies. The 'upper house' is usually elected to represent states in a federal system (as in Australia, Germany or the United States) or different voting configuration in a unitary system (as in France). In the UK the upper house is appointed by the government as a house of review. One criticism of bicameral systems with two elected chambers is that the upper and lower houses may simply mirror one another. The traditional justification of bicameralism is that an upper chamber acts as a house of review. This can minimise arbitrariness and injustice in governmental action.

To pass legislation, a majority of the members of a legislature must vote for a bill (proposed law) in each house. Normally there will be several readings and amendments proposed by the different political factions. If a country has an entrenched constitution, a special majority for changes to the constitution may be required, making changes to the law more difficult. A government usually leads the process, which can be formed from Members of Parliament (e.g. the UK or Germany). However, in a presidential system, the government is usually formed by an executive and his or her appointed cabinet officials (e.g. the United States or Brazil).

The executive in a legal system serves as the centre of political authority of the State. In a parliamentary system, as with Britain, Italy, Germany, India, and Japan, the executive is known as the cabinet, and composed of members of the legislature. The executive is led by the head of government, whose office holds power under the confidence of the legislature. Because popular elections appoint political parties to govern, the leader of a party can change in between elections.

The head of state is apart from the executive, and symbolically enacts laws and acts as representative of the nation. Examples include the President of Germany (appointed by members of federal and state legislatures), the Queen of the United Kingdom (an hereditary office), and the President of Austria (elected by popular vote). The other important model is the presidential system, found in the United States and in Brazil. In presidential systems, the executive acts as both head of state and head of government, and has power to appoint an unelected cabinet. Under a presidential system, the executive branch is separate from the legislature to which it is not accountable.

Although the role of the executive varies from country to country, usually it will propose the majority of legislation, and propose government agenda. In presidential systems, the executive often has the power to veto legislation. Most executives in both systems are responsible for foreign relations, the military and police, and the bureaucracy. Ministers or other officials head a country's public offices, such as a foreign ministry or defence ministry. The election of a different executive is therefore capable of revolutionising an entire country's approach to government.

While military organisations have existed as long as government itself, the idea of a standing police force is a relatively modern concept. For example, Medieval England's system of traveling criminal courts, or assizes, used show trials and public executions to instill communities with fear to maintain control. The first modern police were probably those in 17th-century Paris, in the court of Louis XIV, although the Paris Prefecture of Police claim they were the world's first uniformed policemen.

Max Weber famously argued that the state is that which controls the monopoly on the legitimate use of force. The military and police carry out enforcement at the request of the government or the courts. The term failed state refers to states that cannot implement or enforce policies; their police and military no longer control security and order and society moves into anarchy, the absence of government.

The etymology of "bureaucracy" derives from the French word for "office" ("bureau") and the Ancient Greek for word "power" ("kratos"). Like the military and police, a legal system's government servants and bodies that make up its bureaucracy carry out the directives of the executive. One of the earliest references to the concept was made by Baron de Grimm, a German author who lived in France. In 1765 he wrote,

The real spirit of the laws in France is that bureaucracy of which the late Monsieur de Gournay used to complain so greatly; here the offices, clerks, secretaries, inspectors and "intendants" are not appointed to benefit the public interest, indeed the public interest appears to have been established so that offices might exist.

Cynicism over "officialdom" is still common, and the workings of public servants is typically contrasted to private enterprise motivated by profit. In fact private companies, especially large ones, also have bureaucracies. Negative perceptions of "red tape" aside, public services such as schooling, health care, policing or public transport are considered a crucial state function making public bureaucratic action the locus of government power.

Writing in the early 20th century, Max Weber believed that a definitive feature of a developed state had come to be its bureaucratic support. Weber wrote that the typical characteristics of modern bureaucracy are that officials define its mission, the scope of work is bound by rules, and management is composed of career experts who manage top down, communicating through writing and binding public servants' discretion with rules.

A corollary of the rule of law is the existence of a legal profession sufficiently autonomous to invoke the authority of the independent judiciary; the right to assistance of a barrister in a court proceeding emanates from this corollary—in England the function of barrister or advocate is distinguished from legal counselor. As the European Court of Human Rights has stated, the law should be adequately accessible to everyone and people should be able to foresee how the law affects them.

In order to maintain professionalism, the practice of law is typically overseen by either a government or independent regulating body such as a bar association, bar council or law society. Modern lawyers achieve distinct professional identity through specified legal procedures (e.g. successfully passing a qualifying examination), are required by law to have a special qualification (a legal education earning the student a Bachelor of Laws, a Bachelor of Civil Law, or a Juris Doctor degree. Higher academic degrees may also be pursued. Examples include a Master of Laws, a Master of Legal Studies, a Bar Professional Training Course or a Doctor of Laws.), and are constituted in office by legal forms of appointment (being admitted to the bar). There are few titles of respect to signify famous lawyers, such as Esquire, to indicate barristers of greater dignity, and Doctor of law, to indicate a person who obtained a PhD in Law.

Many Muslim countries have developed similar rules about legal education and the legal profession, but some still allow lawyers with training in traditional Islamic law to practice law before personal status law courts. In China and other developing countries there are not sufficient professionally trained people to staff the existing judicial systems, and, accordingly, formal standards are more relaxed.

Once accredited, a lawyer will often work in a law firm, in a chambers as a sole practitioner, in a government post or in a private corporation as an internal counsel. In addition a lawyer may become a legal researcher who provides on-demand legal research through a library, a commercial service or freelance work. Many people trained in law put their skills to use outside the legal field entirely.

Significant to the practice of law in the common law tradition is the legal research to determine the current state of the law. This usually entails exploring case-law reports, legal periodicals and legislation. Law practice also involves drafting documents such as court pleadings, persuasive briefs, contracts, or wills and trusts. Negotiation and dispute resolution skills (including ADR techniques) are also important to legal practice, depending on the field.

The Classical republican concept of "civil society" dates back to Hobbes and Locke. Locke saw civil society as people who have "a common established law and judicature to appeal to, with authority to decide controversies between them." German philosopher Georg Wilhelm Friedrich Hegel distinguished the "state" from "civil society" ("bürgerliche Gesellschaft") in "Elements of the Philosophy of Right".

Hegel believed that civil society and the state were polar opposites, within the scheme of his dialectic theory of history. The modern dipole state–civil society was reproduced in the theories of Alexis de Tocqueville and Karl Marx. Nowadays in post-modern theory civil society is necessarily a source of law, by being the basis from which people form opinions and lobby for what they believe law should be. As Australian barrister and author Geoffrey Robertson QC wrote of international law,

... one of its primary modern sources is found in the responses of ordinary men and women, and of the non-governmental organizations which many of them support, to the human rights abuses they see on the television screen in their living rooms.

Freedom of speech, freedom of association and many other individual rights allow people to gather, discuss, criticise and hold to account their governments, from which the basis of a deliberative democracy is formed. The more people are involved with, concerned by and capable of changing how political power is exercised over their lives, the more acceptable and legitimate the law becomes to the people. The most familiar institutions of civil society include economic markets, profit-oriented firms, families, trade unions, hospitals, universities, schools, charities, debating clubs, non-governmental organisations, neighbourhoods, churches, and religious associations. There is no clear legal definition of the civil society, and of the institutions it includes. Most of the institutions and bodies who try to give a list of institutions (such as the European Economic and Social Committee) exclude the political parties.
All legal systems deal with the same basic issues, but jurisdictions categorise and identify their legal topics in different ways. A common distinction is that between "public law" (a term related closely to the state, and including constitutional, administrative and criminal law), and "private law" (which covers contract, tort and property). In civil law systems, contract and tort fall under a general law of obligations, while trusts law is dealt with under statutory regimes or international conventions. International, constitutional and administrative law, criminal law, contract, tort, property law and trusts are regarded as the "traditional core subjects", although there are many further disciplines.

International law can refer to three things: public international law, private international law or conflict of laws and the law of supranational organisations.

Constitutional and administrative law govern the affairs of the state. Constitutional law concerns both the relationships between the executive, legislature and judiciary and the human rights or civil liberties of individuals against the state. Most jurisdictions, like the United States and France, have a single codified constitution with a bill of rights. A few, like the United Kingdom, have no such document. A "constitution" is simply those laws which constitute the body politic, from statute, case law and convention. A case named "Entick v Carrington" illustrates a constitutional principle deriving from the common law. Mr Entick's house was searched and ransacked by Sheriff Carrington. When Mr Entick complained in court, Sheriff Carrington argued that a warrant from a Government minister, the Earl of Halifax, was valid authority. However, there was no written statutory provision or court authority. The leading judge, Lord Camden, stated that,

The great end, for which men entered into society, was to secure their property. That right is preserved sacred and incommunicable in all instances, where it has not been taken away or abridged by some public law for the good of the whole ... If no excuse can be found or produced, the silence of the books is an authority against the defendant, and the plaintiff must have judgment.

The fundamental constitutional principle, inspired by John Locke, holds that the individual can do anything except that which is forbidden by law, and the state may do nothing except that which is authorised by law. Administrative law is the chief method for people to hold state bodies to account. People can sue an agency, local council, public service, or government ministry for judicial review of actions or decisions, to ensure that they comply with the law, and that the government entity observed required procedure. The first specialist administrative court was the "Conseil d'État" set up in 1799, as Napoleon assumed power in France.

Criminal law, also known as penal law, pertains to crimes and punishment. It thus regulates the definition of and penalties for offences found to have a sufficiently deleterious social impact but, in itself, makes no moral judgment on an offender nor imposes restrictions on society that physically prevent people from committing a crime in the first place. Investigating, apprehending, charging, and trying suspected offenders is regulated by the law of criminal procedure. The paradigm case of a crime lies in the proof, beyond reasonable doubt, that a person is guilty of two things. First, the accused must commit an act which is deemed by society to be criminal, or "actus reus" (guilty act). Second, the accused must have the requisite malicious intent to do a criminal act, or "mens rea" (guilty mind). However, for so called "strict liability" crimes, an "actus reus" is enough. Criminal systems of the civil law tradition distinguish between intention in the broad sense ("dolus directus" and "dolus eventualis"), and negligence. Negligence does not carry criminal responsibility unless a particular crime provides for its punishment.
Examples of crimes include murder, assault, fraud and theft. In exceptional circumstances defences can apply to specific acts, such as killing in self defence, or pleading insanity. Another example is in the 19th-century English case of "R v Dudley and Stephens", which tested a defence of "necessity". The "Mignonette", sailing from Southampton to Sydney, sank. Three crew members and Richard Parker, a 17-year-old cabin boy, were stranded on a raft. They were starving and the cabin boy was close to death. Driven to extreme hunger, the crew killed and ate the cabin boy. The crew survived and were rescued, but put on trial for murder. They argued it was necessary to kill the cabin boy to preserve their own lives. Lord Coleridge, expressing immense disapproval, ruled, "to preserve one's life is generally speaking a duty, but it may be the plainest and the highest duty to sacrifice it." The men were sentenced to hang, but public opinion was overwhelmingly supportive of the crew's right to preserve their own lives. In the end, the Crown commuted their sentences to six months in jail.

Criminal law offences are viewed as offences against not just individual victims, but the community as well. The state, usually with the help of police, takes the lead in prosecution, which is why in common law countries cases are cited as ""The People" v ..." or ""R" (for Rex or Regina) v ...". Also, lay juries are often used to determine the guilt of defendants on points of fact: juries cannot change legal rules. Some developed countries still condone capital punishment for criminal activity, but the normal punishment for a crime will be imprisonment, fines, state supervision (such as probation), or community service. Modern criminal law has been affected considerably by the social sciences, especially with respect to sentencing, legal research, legislation, and rehabilitation. On the international field, 111 countries are members of the International Criminal Court, which was established to try people for crimes against humanity.

Contract law concerns enforceable promises, and can be summed up in the Latin phrase "pacta sunt servanda" (agreements must be kept). In common law jurisdictions, three key elements to the creation of a contract are necessary: offer and acceptance, consideration and the intention to create legal relations. In "Carlill v Carbolic Smoke Ball Company" a medical firm advertised that its new wonder drug, the smokeball, would cure people's flu, and if it did not, the buyers would get £100. Many people sued for their £100 when the drug did not work. Fearing bankruptcy, Carbolic argued the advert was not to be taken as a serious, legally binding offer. It was an invitation to treat, mere puffery, a gimmick. But the Court of Appeal held that to a reasonable man Carbolic had made a serious offer, accentuated by their reassuring statement, "£1000 is deposited". Equally, people had given good consideration for the offer by going to the "distinct inconvenience" of using a faulty product. "Read the advertisement how you will, and twist it about as you will", said Lord Justice Lindley, "here is a distinct promise expressed in language which is perfectly unmistakable".

"Consideration" indicates the fact that all parties to a contract have exchanged something of value. Some common law systems, including Australia, are moving away from the idea of consideration as a requirement. The idea of estoppel or "culpa in contrahendo", can be used to create obligations during pre-contractual negotiations. 

Civil law jurisdictions treat contracts differently in a number of respects, with a more interventionist role for the state in both the formation and enforcement of contracts. Compared to common law jurisdictions, civil law systems incorporate more mandatory terms into contracts, allow greater latitude for courts to interpret and revise contract terms and impose a stronger duty of good faith, but are also more likely to enforce penalty clauses and specific performance of contracts. They also do not require consideration for a contract to be binding. In France, an ordinary contract is said to form simply on the basis of a "meeting of the minds" or a "concurrence of wills". Germany has a special approach to contracts, which ties into property law. Their 'abstraction principle' ("Abstraktionsprinzip") means that the personal obligation of contract forms separately from the title of property being conferred. When contracts are invalidated for some reason (e.g. a car buyer is so drunk that he lacks legal capacity to contract) the contractual obligation to pay can be invalidated separately from the proprietary title of the car. Unjust enrichment law, rather than contract law, is then used to restore title to the rightful owner.

Certain civil wrongs are grouped together as torts under common law systems and delicts under civil law systems. To have acted tortiously, one must have breached a duty to another person, or infringed some pre-existing legal right. A simple example might be accidentally hitting someone with a cricket ball. Under the law of negligence, the most common form of tort, the injured party could potentially claim compensation for their injuries from the party responsible. The principles of negligence are illustrated by "Donoghue v Stevenson". A friend of Mrs Donoghue ordered an opaque bottle of ginger beer (intended for the consumption of Mrs Donoghue) in a café in Paisley. Having consumed half of it, Mrs Donoghue poured the remainder into a tumbler. The decomposing remains of a snail floated out. She claimed to have suffered from shock, fell ill with gastroenteritis and sued the manufacturer for carelessly allowing the drink to be contaminated. The House of Lords decided that the manufacturer was liable for Mrs Donoghue's illness. Lord Atkin took a distinctly moral approach, and said,

The liability for negligence ... is no doubt based upon a general public sentiment of moral wrongdoing for which the offender must pay ... The rule that you are to love your neighbour becomes in law, you must not injure your neighbour; and the lawyer's question, Who is my neighbour? receives a restricted reply. You must take reasonable care to avoid acts or omissions which you can reasonably foresee would be likely to injure your neighbour.

This became the basis for the four principles of negligence: (1) Mr Stevenson owed Mrs Donoghue a duty of care to provide safe drinks (2) he breached his duty of care (3) the harm would not have occurred but for his breach and (4) his act was the proximate cause of her harm. Another example of tort might be a neighbour making excessively loud noises with machinery on his property. Under a nuisance claim the noise could be stopped. Torts can also involve intentional acts, such as assault, battery or trespass. A better known tort is defamation, which occurs, for example, when a newspaper makes unsupportable allegations that damage a politician's reputation. More infamous are economic torts, which form the basis of labour law in some countries by making trade unions liable for strikes, when statute does not provide immunity.

Property law governs ownership and possession. Real property, sometimes called 'real estate', refers to ownership of land and things attached to it. Personal property, refers to everything else; movable objects, such as computers, cars, jewelry or intangible rights, such as stocks and shares. A right "in rem" is a right to a specific piece of property, contrasting to a right "in personam" which allows compensation for a loss, but not a particular thing back. Land law forms the basis for most kinds of property law, and is the most complex. It concerns mortgages, rental agreements, licences, covenants, easements and the statutory systems for land registration. Regulations on the use of personal property fall under intellectual property, company law, trusts and commercial law. An example of a basic case of most property law is "Armory v Delamirie" [1722]. A chimney sweep's boy found a jewel encrusted with precious stones. He took it to a goldsmith to have it valued. The goldsmith's apprentice looked at it, sneakily removed the stones, told the boy it was worth three halfpence and that he would buy it. The boy said he would prefer the jewel back, so the apprentice gave it to him, but without the stones. The boy sued the goldsmith for his apprentice's attempt to cheat him. Lord Chief Justice Pratt ruled that even though the boy could not be said to own the jewel, he should be considered the rightful keeper ("finders keepers") until the original owner is found. In fact the apprentice and the boy both had a right of "possession" in the jewel (a technical concept, meaning evidence that something "could" belong to someone), but the boy's possessory interest was considered better, because it could be shown to be first in time. Possession may be nine tenths of the law, but not all.

This case is used to support the view of property in common law jurisdictions, that the person who can show the best claim to a piece of property, against any contesting party, is the owner. By contrast, the classic civil law approach to property, propounded by Friedrich Carl von Savigny, is that it is a right good against the world. Obligations, like contracts and torts, are conceptualised as rights good between individuals. The idea of property raises many further philosophical and political issues. Locke argued that our "lives, liberties and estates" are our property because we own our bodies and mix our labour with our surroundings.

Equity is a body of rules that developed in England separately from the "common law". The common law was administered by judges and barristers. The Lord Chancellor on the other hand, as the King's keeper of conscience, could overrule the judge-made law if he thought it equitable to do so. This meant equity came to operate more through principles than rigid rules. For instance, whereas neither the common law nor civil law systems allow people to split the ownership from the control of one piece of property, equity allows this through an arrangement known as a 'trust'. 'Trustees' control property, whereas the 'beneficial' (or 'equitable') ownership of trust property is held by people known as 'beneficiaries'. Trustees owe duties to their beneficiaries to take good care of the entrusted property. In the early case of "Keech v Sandford" [1722] a child had inherited the lease on a market in Romford, London. Mr Sandford was entrusted to look after this property until the child matured. But before then, the lease expired. The landlord had (apparently) told Mr Sandford that he did not want the child to have the renewed lease. Yet the landlord was happy (apparently) to give Mr Sandford the opportunity of the lease instead. Mr Sandford took it. When the child (now Mr Keech) grew up, he sued Mr Sandford for the profit that he had been making by getting the market's lease. Mr Sandford was meant to be trusted, but he put himself in a position of conflict of interest. The Lord Chancellor, Lord King, agreed and ordered Mr Sandford should disgorge his profits. He wrote,

Of course, Lord King LC was worried that trustees might exploit opportunities to use trust property for themselves instead of looking after it. Business speculators using trusts had just recently caused a stock market crash. Strict duties for trustees made their way into company law and were applied to directors and chief executive officers. Another example of a trustee's duty might be to invest property wisely or sell it. This is especially the case for pension funds, the most important form of trust, where investors are trustees for people's savings until retirement. But trusts can also be set up for charitable purposes, famous examples being the British Museum or the Rockefeller Foundation.

Law spreads far beyond the core subjects into virtually every area of life. Three categories are presented for convenience, though the subjects intertwine and overlap.






In the 18th century Adam Smith presented a philosophical foundation for explaining the relationship between law and economics. The discipline arose partly out of a critique of trade unions and U.S. antitrust law. The most influential proponents, such as Richard Posner and Oliver Williamson and the so-called Chicago School of economists and lawyers including Milton Friedman and Gary Becker, are generally advocates of deregulation and privatisation, and are hostile to state regulation or what they see as restrictions on the operation of free markets.

The most prominent economic analyst of law is 1991 Nobel Prize winner Ronald Coase, whose first major article, "The Nature of the Firm" (1937), argued that the reason for the existence of firms (companies, partnerships, etc.) is the existence of transaction costs. Rational individuals trade through bilateral contracts on open markets until the costs of transactions mean that using corporations to produce things is more cost-effective. His second major article, "The Problem of Social Cost" (1960), argued that if we lived in a world without transaction costs, people would bargain with one another to create the same allocation of resources, regardless of the way a court might rule in property disputes. Coase used the example of a nuisance case named "Sturges v Bridgman", where a noisy sweetmaker and a quiet doctor were neighbours and went to court to see who should have to move. Coase said that regardless of whether the judge ruled that the sweetmaker had to stop using his machinery, or that the doctor had to put up with it, they could strike a mutually beneficial bargain about who moves that reaches the same outcome of resource distribution. Only the existence of transaction costs may prevent this. So the law ought to pre-empt what "would" happen, and be guided by the most efficient solution. The idea is that law and regulation are not as important or effective at helping people as lawyers and government planners believe. Coase and others like him wanted a change of approach, to put the burden of proof for positive effects on a government that was intervening in the market, by analysing the costs of action.

Sociology of law is a diverse field of study that examines the interaction of law with society and overlaps with jurisprudence, philosophy of law, social theory and more specialised subjects such as criminology. The institutions of social construction, social norms, dispute processing and legal culture are key areas for inquiry in this knowledge field. Sociology of law is sometimes seen as a sub-discipline of sociology, but its ties to the academic discipline of law are equally strong, and it is best seen as a transdisciplinary and multidisciplinary study focused on the theorisation and empirical study of legal practices and experiences as social phenomena. In the United States the field is usually called law and society studies; in Europe it is more often referred to as socio-legal studies. At first, jurists and legal philosophers were suspicious of sociology of law. Kelsen attacked one of its founders, Eugen Ehrlich, who sought to make clear the differences and connections between positive law, which lawyers learn and apply, and other forms of 'law' or social norms that regulate everyday life, generally preventing conflicts from reaching barristers and courts. Contemporary research in sociology of law is much concerned with the way that law is developing outside discrete state jurisdictions, being produced through social interaction in many different kinds of social arenas, and acquiring a diversity of sources of (often competing or conflicting) authority in communal networks existing sometimes within nation states but increasingly also transnationally.

Around 1900 Max Weber defined his "scientific" approach to law, identifying the "legal rational form" as a type of domination, not attributable to personal authority but to the authority of abstract norms. Formal legal rationality was his term for the key characteristic of the kind of coherent and calculable law that was a precondition for modern political developments and the modern bureaucratic state. Weber saw this law as having developed in parallel with the growth of capitalism. Another leading sociologist, Émile Durkheim, wrote in his classic work "The Division of Labour in Society" that as society becomes more complex, the body of civil law concerned primarily with restitution and compensation grows at the expense of criminal laws and penal sanctions. Other notable early legal sociologists included Hugo Sinzheimer, Theodor Geiger, Georges Gurvitch and Leon Petrażycki in Europe, and William Graham Sumner in the U.S.






</doc>
<doc id="16366" url="https://en.wikipedia.org/wiki?curid=16366" title="Jurisprudence">
Jurisprudence

Jurisprudence or legal theory is the theoretical study of law. Scholars of jurisprudence seek to explain the nature of law in its most general form and provide a deeper understanding of legal reasoning, legal systems, legal institutions, and the role of law in society.

Modern jurisprudence began in the 18th century and was focused on the first principles of natural law, civil law, and the law of nations. General jurisprudence can be divided into categories both by the type of question scholars seek to answer and by the theories of jurisprudence, or schools of thought, regarding how those questions are best answered. Contemporary philosophy of law, which deals with general jurisprudence, addresses problems internal to law and legal systems and problems of law as a social institution that relates to the larger political and social context in which it exists.

This article addresses three distinct branches of thought in general jurisprudence. Ancient natural law is the idea that there are rational objective limits to the power of legislative rulers. The foundations of law are accessible through reason, and it is from these laws of nature that human laws gain whatever force they have. Analytic jurisprudence (Clarificatory jurisprudence) rejects natural law's fusing of what law is and what it ought to be. It espouses the use of a neutral point of view and descriptive language when referring to aspects of legal systems. It encompasses such theories of jurisprudence as "legal positivism", which holds that there is no necessary connection between law and morality and that the force of law comes from basic social facts; and "legal realism", which argues that the real-world practice of law determines what law is, the law having the force that it does because of what legislators, lawyers, and judges do with it. Normative jurisprudence is concerned with "evaluative" theories of law. It deals with what the goal or purpose of law is, or what moral or political theories provide a foundation for the law. It not only addresses the question "What is law?", but also tries to determine what the proper function of law should be, or what sorts of acts should be subject to legal sanctions, and what sorts of punishment should be permitted.

The English word is derived from the Latin, "iurisprudentia". "Juris" is the genitive form of "jus" meaning law, and "prudentia" meaning prudence (also: discretion, foresight, forethought, circumspection). It refers to the exercise of good judgment, common sense, and caution, especially in the conduct of practical matters. The word first appeared in written English in 1628, at a time when the word "prudence" meant knowledge of, or skill in, a matter. It may have entered English via the French "jurisprudence", which appeared earlier.

Ancient Indian jurisprudence is mentioned in various Dharmaśāstra texts, starting with the Dharmasutra of Bhodhayana.

In Ancient China, the Daoists, Confucians, and Legalists all had competing theories of jurisprudence.

Jurisprudence in Ancient Rome had its origins with the ("periti")—experts in the "jus" "mos maiorum" (traditional law), a body of oral laws and customs.

Praetors established a working body of laws by judging whether or not singular cases were capable of being prosecuted either by the edicta, the annual pronunciation of prosecutable offense, or in extraordinary situations, additions made to the edicta. An iudex would then prescribe a remedy according to the facts of the case.

The sentences of the iudex were supposed to be simple interpretations of the traditional customs, but—apart from considering what traditional customs applied in each case—soon developed a more equitable interpretation, coherently adapting the law to newer social exigencies. The law was then adjusted with evolving "institutiones" (legal concepts), while remaining in the traditional mode. Praetors were replaced in the 3rd century BC by a laical body of "prudentes". Admission to this body was conditional upon proof of competence or experience.

Under the Roman Empire, schools of law were created, and practice of the law became more academic. From the early Roman Empire to the 3rd century, a relevant body of literature was produced by groups of scholars, including the Proculians and Sabinians. The scientific nature of the studies was unprecedented in ancient times.

After the 3rd century, "juris prudentia" became a more bureaucratic activity, with few notable authors. It was during the Eastern Roman Empire (5th century) that legal studies were once again undertaken in depth, and it is from this cultural movement that Justinian's Corpus Juris Civilis was born.

In its general sense, natural law theory may be compared to both state-of-nature law and general law understood on the basis of being analogous to the laws of physical science. Natural law is often contrasted to positive law which asserts law as the product of human activity and human volition.

Another approach to natural-law jurisprudence generally asserts that human law must be in response to compelling reasons for action. There are two readings of the natural-law jurisprudential stance.

Notions of an objective moral order, external to human legal systems, underlie natural law. What is right or wrong can vary according to the interests one is focused on. John Finnis, one of the most important of modern natural lawyers, has argued that the maxim "an unjust law is no law at all" is a poor guide to the classical Thomist position.

Strongly related to theories of natural law are classical theories of justice, beginning in the West with Plato's Republic.

Aristotle is often said to be the father of natural law. Like his philosophical forefathers Socrates and Plato, Aristotle posited the existence of natural justice or natural right ("dikaion physikon", "δικαίον φυσικόν", Latin "ius naturale"). His association with natural law is largely due to how he was interpreted by Thomas Aquinas. This was based on Aquinas' conflation of natural law and natural right, the latter of which Aristotle posits in Book V of the "Nicomachean Ethics" (Book IV of the "Eudemian Ethics"). Aquinas's influence was such as to affect a number of early translations of these passages, though more recent translations render them more literally.

Aristotle's theory of justice is bound up in his idea of the golden mean. Indeed, his treatment of what he calls "political justice" derives from his discussion of "the just" as a moral virtue derived as the mean between opposing vices, just like every other virtue he describes. His longest discussion of his theory of justice occurs in "Nicomachean Ethics" and begins by asking what sort of mean a just act is. He argues that the term "justice" actually refers to two different but related ideas: general justice and particular justice. When a person's actions toward others are completely virtuous in all matters, Aristotle calls them "just" in the sense of "general justice"; as such, this idea of justice is more or less coextensive with virtue. "Particular" or "partial justice", by contrast, is the part of "general justice" or the individual virtue that is concerned with treating others equitably.

Aristotle moves from this unqualified discussion of justice to a qualified view of political justice, by which he means something close to the subject of modern jurisprudence. Of political justice, Aristotle argues that it is partly derived from nature and partly a matter of convention. This can be taken as a statement that is similar to the views of modern natural law theorists. But it must also be remembered that Aristotle is describing a view of morality, not a system of law, and therefore his remarks as to nature are about the grounding of the morality enacted as law, not the laws themselves.

The best evidence of Aristotle's having thought there was a natural law comes from the "Rhetoric", where Aristotle notes that, aside from the "particular" laws that each people has set up for itself, there is a "common" law that is according to nature. The context of this remark, however, suggests only that Aristotle thought that it could be rhetorically advantageous to appeal to such a law, especially when the "particular" law of one's own city was adverse to the case being made, not that there actually was such a law. Aristotle, moreover, considered certain candidates for a universally valid, natural law to be wrong. Aristotle's theoretical paternity of the natural law tradition is consequently disputed.

Thomas Aquinas is the foremost classical proponent of natural theology, and the father of the Thomistic school of philosophy, for a long time the primary philosophical approach of the Roman Catholic Church. The work for which he is best known is the "Summa Theologica". One of the thirty-five Doctors of the Church, he is considered by many Catholics to be the Church's greatest theologian. Consequently, many institutions of learning have been named after him.

Aquinas distinguished four kinds of law: eternal, natural, divine, and human:


Natural law is based on "first principles":

"... this is the first precept of the law, that good is to be done and promoted, and evil is to be avoided. All other precepts of the natural law are based on this ..."

The desires to live and to procreate are counted by Aquinas among those basic (natural) human values on which all other human values are based.

Francisco de Vitoria was perhaps the first to develop a theory of "ius gentium" (the rights of peoples), and thus is an important figure in the transition to modernity. He extrapolated his ideas of legitimate sovereign power to international affairs, concluding that such affairs ought to be determined by forms respecting of the rights of all and that the common good of the world should take precedence before the good of any single state. This meant that relations between states ought to pass from being justified by force to being justified by law and justice. Some scholars have upset the standard account of the origins of International law, which emphasises the seminal text "De iure belli ac pacis" by Grotius, and argued for Vitoria and, later, Suárez's importance as forerunners and, potentially, founders of the field. Others, such as Koskenniemi, have argued that none of these humanist and scholastic thinkers can be understood to have founded international law in the modern sense, instead placing its origins in the post-1870 period.

Francisco Suárez, regarded as among the greatest scholastics after Aquinas, subdivided the concept of "ius gentium". Working with already well-formed categories, he carefully distinguished "ius inter gentes" from "ius intra gentes". "Ius inter gentes" (which corresponds to modern international law) was something common to the majority of countries, although, being positive law, not natural law, it was not necessarily universal. On the other hand, "ius intra gentes", or civil law, is specific to each nation.

Writing after World War II, Lon L. Fuller defended a secular and procedural form of natural law. He emphasised that the (natural) law must meet certain formal requirements (such as being impartial and publicly knowable). To the extent that an institutional system of social control falls short of these requirements, Fuller argued, we are less inclined to recognise it as a system of law, or to give it our respect. Thus, the law must have a morality that goes beyond the societal rules under which laws are made.

Sophisticated positivist and natural law theories sometimes resemble each other and may have certain points in common. Identifying a particular theorist as a positivist or a natural law theorist sometimes involves matters of emphasis and degree, and the particular influences on the theorist's work. The natural law theorists of the distant past, such as Aquinas and John Locke made no distinction between analytic and normative jurisprudence, while modern natural law theorists, such as John Finnis, who claim to be positivists, still argue that law is moral by nature. In his book "Natural Law and Natural Rights" (1980, 2011), John Finnis provides a restatement of natural law doctrine.

Analytic, or "clarificatory", jurisprudence means taking a neutral point of view and using descriptive language when referring to various aspects of legal systems. This was a philosophical development that rejected natural law's fusing of what law is and what it ought to be. David Hume argued, in "A Treatise of Human Nature", that people invariably slip from describing what the world "is" to asserting that we therefore "ought" to follow a particular course of action. But as a matter of pure logic, one cannot conclude that we "ought" to do something merely because something "is" the case. So analysing and clarifying the way the world "is" must be treated as a strictly separate question from normative and evaluative questions of what "ought" to be done.

The most important questions of analytic jurisprudence are: "What are laws?"; "What is "the" law?"; "What is the relationship between law and power/sociology?"; and "What is the relationship between law and morality?" Legal positivism is the dominant theory, although there is a growing number of critics who offer their own interpretations.

Historical jurisprudence came to prominence during the debate on the proposed codification of German law. In his book "On the Vocation of Our Age for Legislation and Jurisprudence", Friedrich Carl von Savigny argued that Germany did not have a legal language that would support codification because the traditions, customs, and beliefs of the German people did not include a belief in a code. Historicists believe that law originates with society.

An effort to systematically to inform jurisprudence from sociological insights developed from the beginning of the twentieth century, as sociology began to establish itself as a distinct social science, especially in the United States and in continental Europe. In Germany, Austria and France, the work of the "free law" theorists (e.g. Ernst Fuchs, Hermann Kantorowicz, Eugen Ehrlich and Francois Geny) encouraged the use of sociological insights in the development of legal and juristic theory. The most internationally influential advocacy for a "sociological jurisprudence" occurred in the United States, where, throughout the first half of the twentieth century, Roscoe Pound, for many years the Dean of Harvard Law School, used this term to characterise his legal philosophy. In the United States, many later writers followed Pound's lead or developed distinctive approaches to sociological jurisprudence. In Australia, Julius Stone strongly defended and developed Pound's ideas. In the 1930s, a significant split between the sociological jurists and the American legal realists emerged. In the second half of the twentieth century, sociological jurisprudence as a distinct movement declined as jurisprudence came more strongly under the influence of analytical legal philosophy; but with increasing criticism of dominant orientations of legal philosophy in English-speaking countries in the present century, it has attracted renewed interest. Increasingly, its contemporary focus is on providing theoretical resources for jurists to aid their understanding of new types of regulation (for example, the diverse kinds of developing transnational law) and the increasingly important interrelations of law and culture, especially in multicultural Western societies.

Legal positivism is the view that the content of law is dependent on social facts and that a legal system's existence is not constrained by morality. Within legal positivism, theorists agree that law's content is a product of social facts, but theorists disagree whether law's validity can be explained by incorporating moral values. Legal positivists who argue against the incorporation of moral values to explain law's validity are labeled exclusive (or hard) legal positivists. Joseph Raz's legal positivism is an example of exclusive legal positivism. Legal positivists who argue that law's validity can be explained by incorporating moral values are labeled inclusive (or soft) legal positivists. The legal positivist theories of HLA Hart and Jules Coleman are examples of inclusive legal positivism.

Hobbes was a social contractarian and believed that the law had peoples' tacit consent. He believed that society was formed from a state of nature to protect people from the state of war that would exist otherwise. In "Leviathan", Hobbes argues that without an ordered society life would be "solitary, poor, nasty, brutish and short." It is commonly said that Hobbes's views on human nature were influenced by his times. The English Civil War and the Cromwellian dictatorship had taken place; and, in reacting to that, Hobbes felt that absolute authority vested in a monarch, whose subjects obeyed the law, was the basis of a civilized society.

John Austin and Jeremy Bentham were early legal positivists who sought to provide a descriptive account of law that describes the law as it is. Austin explained the descriptive focus for legal positivism by saying, "The existence of law is one thing; its merit and demerit another. Whether it be or be not is one enquiry; whether it be or be not conformable to an assumed standard, is a different enquiry." For Austin and Bentham, a society is governed by a sovereign who has de facto authority. Through the sovereign's authority come laws, which for Austin and Bentham are commands backed by sanctions for non-compliance. Along with Hume, Bentham was an early and staunch supporter of the utilitarian concept, and was an avid prison reformer, advocate for democracy, and firm atheist. Bentham's views about law and jurisprudence were popularized by his student John Austin. Austin was the first chair of law at the new University of London, from 1829. Austin's utilitarian answer to "what is law?" was that law is "commands, backed by threat of sanctions, from a sovereign, to whom people have a habit of obedience". HLA Hart criticized Austin and Bentham's early legal positivism because the command theory failed to account for individual's compliance with the law.

Hans Kelsen is considered one of the prominent jurists of the 20th century and has been highly influential in Europe and Latin America, although less so in common-law countries. His Pure Theory of Law describes law as "binding norms", while at the same time refusing to evaluate those norms. That is, "legal science" is to be separated from "legal politics". Central to the Pure Theory of Law is the notion of a "basic norm" ("Grundnorm")'—a hypothetical norm, presupposed by the jurist, from which all "lower" norms in the hierarchy of a legal system, beginning with constitutional law, are understood to derive their authority or the extent to which they are binding. Kelsen contends that the extent to which legal norms are binding, their specifically "legal" character, can be understood without tracing it ultimately to some suprahuman source such as God, personified Nature or—of great importance in his time—a personified State or Nation.

In the English-speaking world, the most influential legal positivist of the twentieth century was HLA Hart, professor of jurisprudence at Oxford University. Hart argued that the law should be understood as a system of social rules. In "The Concept of Law", Hart rejected Kelsen's views that sanctions were essential to law and that a normative social phenomenon, like law, cannot be grounded in non-normative social facts.

Hart claimed that law is the union primary rules and secondary rules. Primary rules require individuals to act or not act in certain ways and create duties for the governed to obey. Secondary rules are rules that confer authority to create new primary rules or modify existing ones. Secondary rules are divided into rules of adjudication (how to resolve legal disputes), rules of change (how laws are amended), and the rule of recognition (how laws are identified as valid). The validity of a legal system comes from the "rule of recognition," which is a customary practice of officials (especially barristers and judges) who identify certain acts and decisions as sources of law. In 1981, Neil MacCormick wrote a pivotal book on Hart (second edition published in 2008), which further refined and offered some important criticisms that led MacCormick to develop his own theory (the best example of which is his "Institutions of Law", 2007). Other important critiques include those of Ronald Dworkin, John Finnis, and Joseph Raz.

In recent years, debates on the nature of law have become increasingly fine-grained. One important debate is within legal positivism. One school is sometimes called "exclusive legal positivism" and is associated with the view that the legal validity of a norm can never depend on its moral correctness. A second school is labeled "inclusive legal positivism", a major proponent of which is Wil Waluchow, and is associated with the view that moral considerations , but do not necessarily, determine the legal validity of a norm.

Joseph Raz's theory of legal positivism argues against the incorporation of moral values to explain law's validity. In Raz's 1979 book "The Authority of Law", he criticised what he called the "weak social thesis" to explain law. He formulates the weak social thesis as "(a) Sometimes the identification of some laws turn on moral arguments, but also with, (b) In all legal systems the identification of some law turns on moral argument." Raz argues that law's authority is identifiable purely through social sources, without reference to moral reasoning. This view he calls "the sources thesis." Raz suggests that any categorisation of rules beyond their role as authority is better left to sociology than to jurisprudence. Some philosophers used to contend that positivism was the theory that held that there was "no necessary connection" between law and morality; but influential contemporary positivists—including Joseph Raz, John Gardner, and Leslie Green—reject that view. As Raz points out, it is a necessary truth that there are vices that a legal system cannot possibly have (for example, it cannot commit rape or murder).

Legal realism is the view that a theory of law should be descriptive and account for the reasons why judges decide cases as they do. Legal realism had some affinities with the sociology of law and sociological jurisprudence. The essential tenet of legal realism is that all law is made by humans and thus should account for reasons besides legal rules that led to a legal decision.

There are two separate schools of legal realism: American legal realism and Scandinavian legal realism. American legal realism grew out of the writings of Oliver Wendell Holmes. At the start of Holmes's "The Common Law", he claims that “[t]he life of the law has not been logic: it has been experience.” This view was a reaction to legal formalism that was popular the time due to the Christopher Columbus Langdell. Holmes's writings on jurisprudence also laid the foundations for the predictive theory of law. In his article "The Path of the Law," Holmes argues that "the object of [legal] study...is prediction, the prediction of the incidence of the public force through the instrumentality of the courts."

For the American legal realists of the early twentieth century, legal realism sought to describe the way judges decide cases. For legal realists such as Jerome Frank, judges start with the facts before them and then move to legal principles. Before legal realism, theories of jurisprudence turned this method around where judges were thought to begin with legal principles and then look to facts.

It has become common today to identify Justice Oliver Wendell Holmes, Jr., as the main precursor of American Legal Realism (other influences include Roscoe Pound, Karl Llewellyn, and Justice Benjamin Cardozo). Karl Llewellyn, another founder of the U.S. legal realism movement, similarly believed that the law is little more than putty in the hands of judges who are able to shape the outcome of cases based on their personal values or policy choices.

The Scandinavian school of legal realism argued that law can be explained through the empirical methods used by social scientists. Prominent Scandinavian legal realists are Alf Ross, Axel Hägerström, and Karl Olivecrona. Scandinavian legal realists also took a naturalist approach to law.

Despite its decline in popularity, legal realism continues to influence a wide spectrum of jurisprudential schools today, including critical legal studies, feminist legal theory, critical race theory, sociology of law, and law and economics.

Critical legal studies are a new theory of jurisprudence that has developed since the 1970s. The theory can generally be traced to American legal realism and is considered "the first movement in legal theory and legal scholarship in the United States to have espoused a committed Left political stance and perspective". It holds that the law is largely contradictory, and can be best analyzed as an expression of the policy goals of a dominant social group.

Karl Popper originated the theory of critical rationalism. According to Reinhold Zippelius many advances in law and jurisprudence take place by operations of critical rationalism. He writes, "daß die Suche nach dem Begriff des Rechts, nach seinen Bezügen zur Wirklichkeit und nach der Gerechtigkeit experimentierend voranschreitet, indem wir Problemlösungen versuchsweise entwerfen, überprüfen und verbessern" (that we empirically search for solutions to problems, which harmonise fairly with reality, by projecting, testing and improving the solutions).

American legal philosopher Ronald Dworkin's legal theory attacks legal positivists that separate law's content from morality. In his book "Law's Empire", Dworkin argued that law is an "interpretive" concept that requires barristers to find the best-fitting and most just solution to a legal dispute, given their constitutional traditions. According to him, law is not entirely based on social facts, but includes the best moral justification for the institutional facts and practices that form a society's legal tradition. It follows from Dworkin's view that one cannot know whether a society has a legal system in force, or what any of its laws are, until one knows some truths about the moral justifications of the social and political practices of that society. It is consistent with Dworkin's view—in contrast with the views of legal positivists or legal realists—that in a society may know what its laws are, because no-one may know the best moral justification for its practices.

Interpretation, according to Dworkin's "integrity theory of law", has two dimensions. To count as an interpretation, the reading of a text must meet the criterion of "fit". Of those interpretations that fit, however, Dworkin maintains that the correct interpretation is the one that portrays the practices of the community in their best light, or makes them "the best that they can be". But many writers have doubted whether there a single best moral justification for the complex practices of any given community, and others have doubted whether, even if there is, it should be counted as part of the law of that community.

Consequences of the operation of legal rules or legal procedures—or of the behavior of legal actors (such as lawyers and judges)—may be either beneficial (therapeutic) or harmful (anti-therapeutic) to people. Therapeutic jurisprudence ("TJ") studies law as a social force (or agent) and uses social science methods and data to study the extent to which a legal rule or practice affects the psychological well-being of the people it impacts.

In addition to the question, "What is law?", legal philosophy is also concerned with normative, or "evaluative" theories of law. What is the goal or purpose of law? What moral or political theories provide a foundation for the law? What is the proper function of law? What sorts of acts should be subject to punishment, and what sorts of punishment should be permitted? What is justice? What rights do we have? Is there a duty to obey the law? What value has the rule of law? Some of the different schools and leading thinkers are discussed below.

Aretaic moral theories, such as contemporary virtue ethics, emphasize the role of character in morality. Virtue jurisprudence is the view that the laws should promote the development of virtuous character in citizens. Historically, this approach has been mainly associated with Aristotle or Thomas Aquinas. Contemporary virtue jurisprudence is inspired by philosophical work on virtue ethics.

Deontology is the "theory of duty or moral obligation". The philosopher Immanuel Kant formulated one influential deontological theory of law. He argued that any rule we follow must be able to be universally applied, i.e. we must be willing for everyone to follow that rule. A contemporary deontological approach can be found in the work of the legal philosopher Ronald Dworkin.

Utilitarianism is the view that the laws should be crafted so as to produce the best consequences for the greatest number of people. Historically, utilitarian thinking about law has been associated with the philosopher Jeremy Bentham. John Stuart Mill was a pupil of Bentham's and was the torch bearer for utilitarian philosophy throughout the late nineteenth century. In contemporary legal theory, the utilitarian approach is frequently championed by scholars who work in the law and economics tradition.

John Rawls was an American philosopher; a professor of political philosophy at Harvard University; and author of "A Theory of Justice" (1971), "Political Liberalism", "", and "The Law of Peoples". He is widely considered one of the most important English-language political philosophers of the 20th century. His theory of justice uses a method called "original position" to ask us which principles of justice we would choose to regulate the basic institutions of our society if we were behind a "veil of ignorance". Imagine we do not know who we are—our race, sex, wealth, status, class, or any distinguishing feature—so that we would not be biased in our own favour. Rawls argued from this "original position" that we would choose exactly the same political liberties for everyone, like freedom of speech, the right to vote, and so on. Also, we would choose a system where there is only inequality because that produces incentives enough for the economic well-being of all society, especially the poorest. This is Rawls's famous "difference principle". Justice is fairness, in the sense that the fairness of the original position of choice guarantees the fairness of the principles chosen in that position.

There are many other normative approaches to the philosophy of law, including critical legal studies and libertarian theories of law.





</doc>
<doc id="62789474" url="https://en.wikipedia.org/wiki?curid=62789474" title="Designated Professional Body">
Designated Professional Body

According to the UK Financial Conduct Authority (FCA), a dedicated professional body is one designated by the Treasury under section 326 of the Act (Designation of professional bodies) for the purposes of the Act (Provision of Financial Services by Members of the Professions).

The following professional bodies have been designated in the Financial Services and Markets Act 2000 (Designated Professional Bodies) Order 2001 (SI 2001/1226), the Financial Services and Markets Act 2000 (Designated Professional Bodies) (Amendment) Order 2004 (SI 2004/3352) and the Financial Services and Markets Act 2000 (Designated Professional Bodies) (Amendment) Order 2006 (SI 2006/58):


Under Section 325(4) of the FSMA, Designated Professional Bodies are required to cooperate with the FCA in a number of ways, including information sharing, in order for the FCA to be able to perform its functions. 


</doc>
<doc id="61386565" url="https://en.wikipedia.org/wiki?curid=61386565" title="Nixon v. Warner Communications">
Nixon v. Warner Communications

Nixon v. Warner Communications, , was a case in which the SCOTUS held that neither the common law right of access to judicial records, nor the First Amendment to the U.S. Constitution guarantee of freedom of the press, nor the Sixth Amendment to the U.S. Constitution guarantee of a public trial compelled the release of tapes from the custody of the District Court.



</doc>
<doc id="2216647" url="https://en.wikipedia.org/wiki?curid=2216647" title="Misrepresentation">
Misrepresentation

In common law jurisdictions, a misrepresentation is an untrue or misleading statement of fact made during negotiations by one party to another, the statement then inducing that other party to enter into a contract. The misled party may normally rescind the contract, and sometimes may be awarded damages as well (or instead of rescission).

The law of misrepresentation is an amalgam of contract and tort; and its sources are common law, equity and statute. The common law was amended by the Misrepresentation Act 1967. The general principle of misrepresentation has been adopted by the USA and various Commonwealth countries, e.g. India.

A "representation" is a pre-contractual statement made during negotiations. If a representation has been incorporated into the contract as a term, then the normal remedies for breach of contract apply. Factors that determine whether or not a representation has become a term include:

Otherwise, an action may lie in misrepresentation, and perhaps in the torts of negligence and deceit also. Although a suit for breach of contract is relatively straightforward, there are advantages in bringing a parallel suit in misrepresentation, because whereas repudiation is available only for breach of condition, rescission is "prima facie" available for all misreps, subject to the provisions of s.2 of the Misrepresentation Act 1967, and subject to the inherent limitations of an equitable remedy.

For a misrepresentation to occur, especially a negligent misrepresentation, the following elements need to be satisified.

There is no general duty of disclosure in English contract law, and one is normally not obliged to say anything. Ordinary contracts do not require "good faith" as such, and mere compliance with the law is sufficient. However in particular relationships silence may form the basis of an actionable misrepresentation:


To amount to a misrepresentation, the statement must be untrue or seriously misleading. A statement which is "technically true" but which gives a misleading impression is deemed an "untrue statement". If a misstatement is made and later the representor finds that it is false, it becomes fraudulent unless the representer updates the other party. If the statement is true at the time, but becomes untrue due to a change in circumstances, the representor must update the original statement. Actionable misrepresentations must be misstatements of fact or law: misstatements of opinion or intention are not deemed statements of fact; but if one party appears to have specialist knowledge of the topic, his "opinions" may be considered actionable misstatements of fact. For example, false statements made by a seller regarding the quality or nature of the property that the seller has may constitute misrepresentation.

Statements of opinion are usually insufficient to amount to a misrepresentation as it would be unreasonable to treat personal opinions as "facts", as in "Bisset v Wilkinson". 

Exceptions can arise where opinions may be treated as "facts":

- where an opinion is expressed yet this opinion is not actually held by the representor,

- where it is implied that the representor has facts on which to base the opinion,

- where one party should have known facts on which such an opinion would be based.

Statements of intention do not constitute misrepresentations should they fail to come to fruition, since the time the statements were made they can not be deemed either true or false. However, an action can be brought if the intention never actually existed, as in "Edgington v Fitzmaurice".

For many years, statements of law were deemed incapable of amounting to misrepresentations because the law is "equally accessible by both parties" and is "...as much the business of the plaintiff as of [the defendants] to know what the law [is].". This view has changed, and it is now accepted that statements of law may be treated as akin to statements of fact. As stated by Lord Denning "...the distinction between law and fact is illusory".

An action in misrepresentation can only be brought by the misled party, or "representee". This means that only those who were an intended recipient of the representation may sue, as in "Peek v Gurney", where the plaintiff sued the directors of a company for indemnity. The action failed because it was found that the plaintiff was not a representee (an intended party to the representation) and accordingly misrepresentation could not be a protection.

It is not necessary for the representation to have been be received directly; it is sufficient that the representation was made to another party with the intention that it would become known to a subsequent party and ultimately acted upon by them. However, it IS essential that the untruth originates from the defendant.

The misled party must show that he relied on the misstatement and was induced into the contract by it.

In "Attwood v Small", the seller, Small, made false claims about the capabilities of his mines and steelworks. The buyer, Attwood, said he would verify the claims before he bought, and he employed agents who declared that Small's claims were true. The House of Lords held that Attwood could not rescind the contract, as he did not rely on Small but instead relied on his agents. "Edgington v Fitzmaurice" confirmed further that a misrepresentation need not be the sole cause of entering a contract, for a remedy to be available, so long as it is an influence.

A party induced by a misrepresentation is not obliged to check its veracity. In "Redgrave v Hurd" Redgrave, an elderly solicitor told Hurd, a potential buyer, that the practice earned £300 pa. Redgrave said Hurd could inspect the accounts to check the claim, but Hurd did not do so. Later, having signed a contract to join Redgrave as a partner, Hurd discovered the practice generated only £200 pa, and the accounts verified this figure. Lord Jessel MR held that the contract could be rescinded for misrepresentation, because Redgrave had made a misrepresentation, adding that Hurd was entitled to rely on the £300 statement.

By contrast, in "Leaf v International Galleries", where a gallery sold painting after wrongly saying it was a Constable, Lord Denning held that while there was neither breach of contract nor operative mistake, there "was" a misrepresentation; but, five years having passed, the buyer's right to rescind had lapsed. This suggests that, having relied on a misrepresentation, the misled party has the onus to discover the truth "within a reasonable time". In "Doyle v Olby" [1969], a party misled by a fraudulent misrepresentation was deemed NOT to have affirmed even after more than a year.

Within trade and commerce, the law regarding misrepresentation is dealt with by the Australian Consumer Law, under Section 18 and 29 of this code, the ACL calls contractual misrepresentations as "misleading and deceptive conduct" and imposes a prohibition. The ACL provides for remedies, such as damages, injunctions, rescission of the contract, and other measures.

In England, the common law was codified and amended by the Misrepresentation Act 1967. (Although short and apparently succinct, the 1967 Act is widely regarded as a confusing and poorly drafted statute which has caused a number of difficulties, especially in relation to the basis of the award of damages. It was mildly amended by the Unfair Contract Terms Act 1977 and in 2012, but it escaped the attention of the consolidating Consumer Rights Act 2015).

Prior to the Misrepresentation Act 1967, the common law deemed that there were two categories of misrepresentation: fraudulent and innocent. The effect of the act is primarily to create a new category by dividing innocent misrepresentation into two separate categories: negligent and "wholly" innocent; and it goes on to state the remedies in respect of each of the three categories. The point of the three categories is that the law recognises that the defendant may have been blameworthy to a greater or lesser extent; and the relative degrees of blameworthiness lead to differing remedies for the claimant.

Once misrepresentation has been proven, it is presumed to be "negligent misrepresentation", the default category. It then falls to the claimant to prove that the defendant's culpability was more serious and that the misrepresentation was fraudulent. Conversely, the defendant may try to show that his misrepresentation was innocent.




Negligent misstatement is not strictly part of the law of misrepresentation, but is a tort based upon the 1964 "obiter dicta" in "Hedley Byrne v Heller" where the House of Lords found that a negligently-made statement (if relied upon) could be actionable provided a "special relationship" existed between the parties.

Subsequently in "Esso Petroleum Co Ltd v Mardon", Lord Denning transported this tort into contract law, stating the rule as:

"...if a man, who has or professes to have special knowledge or skill, makes a representation by virtue thereof to another…with the intention of inducing him to enter into a contract with him, he is under a duty to use reasonable care to see that the representation is correct, and that the advice, information or opinion is reliable'.

Depending on the type of misrepresentation, remedies such as recission, or damages, or a combination of both may be available. Tortious liability may also be considered. Several countries, such as Australia have a statutory schema which deals with misrepresentations under consumer law.

Entitlement to rescission of the contract, but not damages

Entitlement to damages or rescission of the contract

Entitlement to damages, or rescission of the contract

A contract vitiated by misrepresentation is voidable and not void "ab initio". The misled party may either (i) rescind, or (ii) affirm and continue to be bound. If the claimant chooses to rescind, the contract will still be deemed to have been valid up to the time it was avoided, so any transactions with a third party remains valid, and the third party will retain good title. Rescission can be effected either by informing the representor or by requesting an order from the court. Rescission is an equitable remedy which is not always available. Rescission requires the parties to be restored to their former positions; so if this is not possible, rescission is unavailable.

A misled party who, knowing of the misrepresentation, fails to take steps to avoid the contract will be deemed to have affirmed through "laches", as in "Leaf v International Galleries"; and the claimant will be estopped from rescinding. The time limit for taking such steps varies depending on the type of misrepresentation. In cases of fraudulent misrepresentation, the time limit runs until when the misrepresentation ought to have been discovered, whereas in innocent misrepresentation, the right to rescission may lapse even before the represent can reasonably be expected to know about it.

Sometimes, third party rights may intervene and render rescission impossible. Say, if A misleads B and contracts to sell a house to him, and B later sells to C, the courts are unlikely to permit rescission as that would unfair impinge upon C.

Under Misrepresentations Act 1967 s. 2(2) of the Misrepresentation Act 1967, the court has discretion to award damages instead of rescission, "if of opinion that it would be equitable to do so, having regard to the nature of the misrepresentation and the loss that would be caused by it if the contract were upheld, as well as to the loss that rescission would cause to the other party."

"Damages" are monetary compensation for loss. In contract and tort, damages will be awarded if the breach of contract (or breach of duty) causes foreseeable loss.


Given the relative lack of blameworthiness of a non-fraudulent defendant (who is at worst merely careless, and at best may honestly "believe on reasonable grounds" that he told the truth) for many years lawyers presumed that for these two categories, damages would be on a contract/tort basis requiring reasonable foreseeability of loss.

In 1991, "Royscot Trust Ltd v Rogerson" changed all that. The court gave a literal interpretation of s.2 (which, to paraphrase, provides that where a person has been misled by a negligent misrepresentation then, if the misrepresentor would be liable to damages had the representation been made fraudulently, the defendant "shall be so liable"). The phrase shall be so liable was read literally to mean "liable as in fraudulent misrepresentation". So, under the Misrepresentation Act 1967, damages for negligent misrepresentation are calculated as if the defendant had been fraudulent, even if he has been merely careless. Although this was almost certainly not the intention of Parliament, no changes to the law have been made to address this discrepancy: the Consumer Rights Act 2015 left the 1967 Act intact. This is known as the fiction of fraud and also extends to tortious liability.

S.2 does not specify how "damages in lieu" should be determined, and interpretation of the statute is up to the courts.

Misrepresentation is one of several vitiating factors that can affect the validity of a contract. Other vitiating factors include:





</doc>
<doc id="44368785" url="https://en.wikipedia.org/wiki?curid=44368785" title="Military impostor">
Military impostor

A military impostor is a person who makes false claims about their military service in civilian life. This includes claims by people that have never been in the military as well as lies or embellishments by genuine veterans. Some individuals who do this also wear privately obtained uniforms or medals which were never officially issued to them.

In British military slang, such impostors are called "Walts", based on James Thurber's fictional character, Walter Mitty, who daydreamed of being a war hero. In the United States since the early 2000s, the term "stolen valor" has become popular slang for this kind of behavior, so named for the 1998 book of that name. Other terms include "fake warriors", "military phonies", "medal cheats",and "military posers".

Lying about military service or wearing a uniform or medals that were not earned is criminalized in some circumstances, especially if done with the goal of obtaining money or any other kind of tangible benefit, though laws vary by country.

Military impostors engage in a broad range of deceptive behaviors, all intended to achieve recognition from others. An impostor may make verbal statements, written claims, or create deceptive impressions through actions, such as wearing a uniform, rank insignia, unit symbols, medals, or patches. 

Generally impostors fall into two broad categories: civilians who have never been in any branch of the military, and real veterans who make false claims exaggerating their experiences or accomplishments. Impostors in the latter category may claim any of the following: 


While many individuals outright fabricate some or all of their military service history, others employ equivocation tactics or similarly misleading language that avoids making a technically false statement, but still gives a deceptive impression. A common example is stating one was in a branch of the military during a specific war. In many contexts, such a statement "implies" that the speaker was deployed to a combat zone, even if in reality never left their home country. A similar misleading statement is boasting about being a member of a branch or unit that is well known for its combat prowess and heroic achievements, when the speaker was purely in a logistical role without any combat experience. Imposters also frequently claim to be part of "classified" operations as an excuse for why they cannot provide details or, when confronted, why there is no record of their actions or service.

Historically, when military record-keeping was less accurate than it is now, some men falsely claimed to be war veterans to obtain military pensions. Such men added a few years to their ages and claimed service in obscure units. Most did not make extravagant claims, because they were seeking money, not public attention that might expose them. There were numerous U.S. media reports in the 1950s of men claiming to be Confederate veterans over 110 years old, and most articles debunked these stories, saying the men had exaggerated their ages and made fraudulent pension claims years earlier and then found themselves in the spotlight after the last genuine Civil War veterans died off. Walter Williams, noted below, is considered one of these impostors, though some people continue to believe his claim.

In the modern world, reasons for posing as a member of the military or exaggerating one's service record vary, but the intent is almost always about gaining the respect and admiration of others. Philosophy professor Verna V. Gehring describes such people as "virtue impostors," in that they don't necessarily adopt the identity of another person, but instead adopt a false history for themselves to impersonate virtues and characteristics. Many are only motivated by social recognition, attempting to exploit the reverence and respect for veterans in their country. These individuals often become absorbed in a fantasy of being a veteran that they attempt to live out in real life, sometimes even inserting themselves into public events or ceremonies, or volunteering for interviews with journalists about their alleged experiences. Others are motivated by more direct gains, such as impressing employers, casting directors, audiences, investors, voters in political campaigns or romantic interests.

Occasionally impostors use their claims in an attempt to intimidate others, such as claiming to be a trained sniper or ex-special forces, or use their fabricated experiences as a pretense of authority for their opinions on political matters. False claims of military service are also used by panhandlers to increase donations, sometimes coupled with real or fake injuries that are implied to be combat-related.

Military imposters are frequently caught and exposed due to mistakes and inconsistencies in their story or behavior. For example, they may be too young or too old to have been in the war they say they were or too young for the rank they claim to be, might inadvertently profess to have been in two different places at once, or might state factually incorrect information about the war they allegedly were part of. Among imposters that wear uniforms, they often make mistakes about the placement of patches, insignia and medals, and may have some from the wrong branch or from old campaigns they could not possibly have taken part in. Real veterans often can spot mistakes more readily, especially if they were part of the same branch the imposter claims to have served in.

Some countries have ways of verifying military service and certain claims within it. In the United States, any real veteran that has been separated from the military for any reason has a DD Form 214 they can present, which indicates their branch, rank, unit, MOS/AFSC, awards, and other information. Alternatively, requests can also be made to the National Personnel Records Center using the Freedom of Information Act (FOIA) to verify service. Other claims can be verified against public lists, such as recipients of the Medal of Honor or the prisoner of war list from the Vietnam War. Several websites are specifically devoted to verifying the claims of alleged military imposters, and if discovered to have lied, proceed to shame the perpetrator publicly.

Accusations do occasionally backfire, with real veterans accused of being imposters. Doug Sterner, a Vietnam War veteran who catalogs military awards, and "Stolen Valor" author B.G. Burkett, note that some modern veterans have become hypersensitive to imposters, leading to vigilantism or even turning detection into a "hunting game." A common error is placing too much emphasis on the neatness of a uniform or certain quirks about how it's worn, which is not necessarily compelling when a veteran is older and has been out of the service for several decades. Another is making too many inferences based on older regulations, such as gender restrictions that were in place in the past. Even FOIA requests to the National Personnel Records Center, considered the most thorough type of verification for US veterans, are not perfect and sometimes fail to find a record even if the veteran is genuine. Sterner states, "There’s some people that feel good about confronting people, and making themselves look big by trying to take them down. But when they do that, they’re going to make mistakes."

Laws vary between countries regarding false statements about military service and/or wearing of uniforms or medals.

Under the Australia's Defence Act, 1903, as amended, it is a federal crime to falsely claim to be a returned soldier, sailor or airman. It is also a crime to wear any service decoration one has not earned.

In Canada, section 419 of the Criminal Code makes it a crime to wear a uniform from the Canadian Forces without authority as well as any awards or marks not earned. It additionally makes it a crime to possess any fraudulent discharge papers, commissions, warrants or military ID, including those that are forged, altered or belong to someone else.

In the United Kingdom, it was an offense under the Army Act 1955 to wear real or replica military decorations with intent to deceive. However, this law was superseded by the Armed Forces Act 2006, which lacks this prohibition.

It is still a crime in the UK for a civilian to wear a uniform of the armed forces without authorization under the Uniforms Act 1894, and false claims of military service used to obtain money or other enrichment are prosecuted under the general crime of fraud. In November 2016, the Defence Select Committee recommended making the wearing of unearned medals a criminal offence punishable by up to six months imprisonment.

In the United States, the Stolen Valor Act of 2013 makes it a federal offense to falsely claim to have received any of several major military awards with the intention of obtaining money, property, or other tangible benefits. There are additional laws criminalizing the altering or forging of discharge documents, and attempting to obtain veteran's benefits from the government.


Cyrus Hamilton, a character in John Steinbeck's novel "East of Eden", loses his leg in the first and only action he saw during the U.S. Civil War. He subsequently creates an entire military career, encompassing nearly every battle of the war, and stating that he was a personal advisor to President Lincoln.




</doc>
<doc id="49025267" url="https://en.wikipedia.org/wiki?curid=49025267" title="Military call sign">
Military call sign

Military call signs are call signs assigned as unique identifiers to military communications. In wartime, monitoring an adversary's communications can be a valuable form of intelligence. Consistent call signs can aid in this monitoring, so in wartime, military units often employ tactical call signs and sometimes change them at regular intervals. In peacetime, some military stations will use fixed call signs in the international series.

The United States Army uses fixed station call signs which begin with "W", such as WAR, used by U.S. Army Headquarters. Tactical call signs are often assigned to a company sized unit or higher. For example, the collective "Checkmate" might be assigned to an entire company and thus "Checkmate Red 6" would be the first platoon leader (platoons are red for first, white for second, blue for third, black for headquarters, and often additional colors for other portions under the command), "Checkmate white 6" to the second platoon leader, etc. "Checkmate 6" is the Company Commander and "Checkmate 6 Romeo" is the commander's radio-telephone operator (Romeo the NATO phonetic of the letter R). Under NATO conventions, 6 is designated the commander or leader, 5 the second-in-command or executive officer, 7 the chief NCO. Also, companies often have the letter they are designated by ('A', 'B', 'C' or 'D') be the first letter of their call sign. This means a 'C' Company could potentially have 'Checkmate' as its call sign. One specific call sign used Army wide is DUSTOFF, dating back to the first dedicated Air Ambulance unit in Vietnam. 

Fixed call signs for the United States Air Force stations begin with "A", such as AIR, used by USAF Headquarters.
The USAF also uses semi-fixed identifiers consisting of a name followed by a two or three digit number. The name is assigned to a unit on a semi-permanent basis; they change only when the U.S. Department of Defense goes to DEFCON 3. For example, JAMBO 51 would be assigned to a particular B-52 aircrew of the 5th Bomb Wing, while NODAK 1 would be an F-16 fighter with the North Dakota Air National Guard. Individual military pilots or other flight officers usually adopt a personal aviator call sign.

The most recognizable call sign of this type is "Air Force One", used when any Air Force aircraft is transporting the U.S. President. Similarly, when the President is flown in a U.S. Marine Corps helicopter, the call sign is Marine One. When then-president George W. Bush, a former Air National Guard fighter pilot, was flown to the aircraft carrier USS "Abraham Lincoln" in a Navy S-3B Viking, it was the first use of the "Navy One" call sign.

The United States Navy, United States Marine Corps, and United States Coast Guard use a mixture of tactical call signs and international call signs, with ships beginning with the letter "N". For example, the carrier USS "John F. Kennedy" had the call sign NJFK for unclassified and navigation communications with other vessels, but uses tactical call signs that vary with its mission. 

Navy and Marine Corps aircraft will typically use a stateside naming convention based on the aircraft tail code, identifying the squadron or air wing assigned, and a three digit number painted on the aircraft's nose known as the MODEX. Examples might be "November Lima Two-Zero-One" or "Navy November Lima Two-Zero-One." Another option would be to use a unit callsign from the publication known as JANAP 119, such as "Old Nick Two-Zero-One." A Marine Corps aircraft might use a call sign like "Marine Delta November One-Zero-Two" or "Shamrock One-Zero-Two." Other tactical call signs may be employed as mission necessities dictate.

Coast Guard aircraft callsigns are almost always the word "Coast Guard" and the 4-digit aircraft number, e.g., "Coast Guard Six-Five-Seven-Niner," although other call signs may be used for special operations such as counter-narcotics interdiction. 

In tactical situations, the Marine Corps utilizes call signs naming conventions similar to the Army's.

In May 2019, United States Navy announced new procedures for assigning call signs to pilots in training to avoid potentially racist names.

Tactical voice communications ("combat net radio") use a system of call signs of the form "letter-digit-digit". Within a standard infantry battalion these characters represent companies, platoons and sections respectively, so that 3 Section, 1 Platoon of B Company might be F13. In addition, a suffix following the initial call sign can denote a specific individual or grouping within the designated call sign, so F13C would be the Charlie fire team. Unused suffixes can be used for other call signs that do not fall into the standard call sign matrix, for example the unused 33A call sign is used to refer to the Company Sergeant Major.

The letter part of the call sign is not the company's letter (B vs F in the above example). Instead, letter designations are randomly assigned using BATCO sheets, and appear on CEIs (communication electronic instructions), and change along with the BATCO codes every 24 hours. This, together with frequency changes and voice procedure aimed at making every unit sound the same, protects the military against simple traffic analysis and eavesdropping. Other radio users, like B20, do not fit into the standard battalion model but are also assigned a call sign for protection.

The controller of each net has the call sign 0 ("zero"). There may also be a second controller - either a backup station or a commander who has delegated communication tasks to a signaller but may occasionally wish to speak in person - using the call sign 0A ("zero alpha").

Earlier systems used a series of appointment titles to identify users and individuals, "Sunray", for instance, referring to the appropriate leader. Most appointment titles are no longer used by the British Army, but titles such as "Sunray" and (Sunray) Minor are still used. Several other armed forces still use appointment titles, including the Australian, New Zealand, and Canadian armies.


</doc>
<doc id="2726726" url="https://en.wikipedia.org/wiki?curid=2726726" title="Military logistics">
Military logistics

Military logistics is the discipline of planning and carrying out the movement, supply, and maintenance of military forces. In its most comprehensive sense, it is those aspects or military operations that deal with:

The word "logistics" is derived from the Greek adjective "logistikos" meaning "skilled in calculating". The first administrative use of the word was in Roman and Byzantine times when there was a military administrative official with the title "Logista". At that time, the word apparently implied a skill involved in numerical computations.

Historically supplies for an army were first acquired by foraging or looting, especially in the case of food and fodder, although if traveling through a desolated region or staying in one place for too long resources could quickly be exhausted. A second method was for the army to bring along what was needed, whether by ships, pack animals, wagons or carried on the backs of the soldiers themselves. This allowed the army some measure of self-sufficiency, and up through to the 19th century most of the ammunition a soldier needed for an entire campaign could be carried on their person. However, this method led to an extensive baggage train which could slow down the army's advance and the development of faster-firing weapons soon outpaced an army's ability to supply itself. Starting with the Industrial Revolution new technological, technical and administrative advances led to a third method, that of maintaining supplies in a rear area and transporting them to the front. This led to a "logistical revolution" which began in the 20th century and drastically improved the capabilities of modern armies while making them highly dependent on this new system.

Through the medieval period (the 5th to 15th century in Europe), soldiers were responsible for supplying themselves, either through foraging, looting, or purchases. Even so, military commanders often provided their troops with food and supplies, but this would be provided in lieu of the soldiers' wages, or soldiers would be expected to pay for it from their wages, either at cost or even with a profit.

In 1294, the same year John II de Balliol of Scotland refused to support Edward I of England's planned invasion of France, Edward I implemented a system in Wales and Scotland where sheriffs would acquire foodstuffs, horses and carts from merchants with compulsory sales at prices fixed below typical market prices under the Crown's rights of prise and purveyance. These goods would then be transported to Royal Magazines in southern Scotland and along the Scottish border where English conscripts under his command could purchase them. This continued during the First War of Scottish Independence which began in 1296, though the system was unpopular and was ended with Edward I's death in 1307.

Starting under the rule of Edward II in 1307 and ending under the rule of Edward III in 1337, the English instead used a system where merchants would be asked to meet armies with supplies for the conscripts to purchase. This led to discontent as the merchants saw an opportunity to profiteer, forcing conscripts to pay well above normal market prices for food.

As Edward III went to war with France in the Hundred Years' War (starting in 1337), the English returned to a practice of foraging and looting to meet their logistical needs. This practice lasted throughout the course of war, extending through the remainder of Edward III's reign into the reign of Henry VI.

Starting in the late sixteenth century armies in Europe greatly increased in size, upwards of 100,000 or more in some cases. This increase in size came not just in the number of actual soldiers but also camp followers—anywhere from half to one and a half the size of the army itself—and the size of the baggage train—averaging one wagon for every fifteen men. However, very little state support was provided to these massive armies, the vast majority of which consisted of mercenaries. Beyond being paid for their service by the state—an act which bankrupted even the Spanish Empire on several occasions—these soldiers and their commanders were forced to provide everything for themselves. If permanently assigned to a town or city with a working marketplace, or traveling along a well-established military route, supplies could be easily bought locally with intendants overseeing the exchanges. In other cases an army traveling in friendly territory could expect to be followed by sutlers—although their supply stocks were small and subject to price gouging—or a commissioner could be sent ahead to a town to make arraignments, including quartering if necessary.

When operating in enemy territory an army was forced to plunder the local countryside for supplies, a historical tradition meant to allow war to be conducted at the enemy's expense. However, with the increase in army sizes this reliance on plunder became a major problem, as many decisions regarding where an army could move or fight were made based not on strategic objectives but whether a given area was capable of supporting the soldiers' needs. Sieges in particular were affected by this, both for any army attempting to lay siege to a location or coming to its relief. Unless a military commander was able to implement some sort of regular resupply, a fortress or town with a devastated countryside could be effectively immune to either operation.

Conversely, armies of this time had little need to maintain lines of communication while on the move, except insofar as it was necessary to recruit more soldiers, and thus could not be cut off from non-existent supply bases. Although this theoretically granted armies freedom of movement, the need for plunder prevented any sort of sustained, purposeful advance. Many armies were further restricted to following waterways due to the fact that what supplies they were forced to carry could be more easily transported by boat. Artillery in particular was reliant of this method of travel, since even a modest number of cannons of the period required hundreds of horses to pull overland and traveled at half the speed of the rest of the army.

By the seventeenth century, the French under Secretary of State for War Michel Le Tellier began a series of military reforms to address some of the issues which had plagued armies in the previous century. Besides ensuring that soldiers were more regularly paid and combating the corruption and inefficiencies of private contractors, Le Tellier devised formulas to calculate the exact amount of supplies necessary for a given campaign, created standardized contracts for dealing with commercial suppliers, and formed a permanent vehicle-park manned by army specialists whose job was to carry a few days' worth of supplies while accompanying the army during campaigns. With these arrangements there was a gradual increase in the use of magazines which could provide a more regular flow of supply via convoys. While the concepts of magazines and convoys was not new at this time, prior to the increase in army sizes there had rarely been cause to implement them.

Despite these changes, French armies still relied on plunder for a majority of their needs while on the move. Magazines were created for specific campaigns and any surplus was immediately sold for both monetary gain and to lessen the tax burden. The vehicles used to form convoys were contracted out from commercial interests or requisitioned from local stockpiles. In addition, given warfare of this era's focus on fortified towns and an inability to establish front lines or exert a stabilizing control over large areas, these convoys often needed armies of their own to provide escort. The primary benefits of these reforms was to supply an army during a siege. This was borne out in the successful campaign of 1658 when the French army at no point was forced to end a siege on account of supplies, including the Siege of Dunkirk.

Le Tellier's son Louvois would continue his father's reforms after assuming his position. The most important of these was to guarantee free daily rations for the soldiers, amounting to two pounds of bread or hardtack a day. These rations were supplemented as circumstances allowed by a source of protein such as meat or beans; soldiers were still responsible for purchasing these items out-of-pocket but they were often available at below-market prices or even free at the expense of the state. He also made permanent a system of magazines which were overseen by local governors to ensure they were fully stocked. Some of these magazines were dedicated to providing frontier towns and fortresses several months' worth of supplies in the event of a siege, while the rest were dedicated to supporting French armies operating in the field.

With these reforms French armies enjoyed one of the best logistical systems in Europe, however there were still severe restrictions on its capabilities. Only a fraction of an army's supply needs could be met by the magazines, requiring that it continue to use plunder. In particular this was true for perishable goods or those too bulky to store and transport such as fodder. The administration and transportation of supplies remained inadequate and subject to the deprivations of private contractors. The primary aim of this system was still to keep an army supplied while conducting a siege, a task for which it succeeded, rather than increase its freedom of movement.

The British were seriously handicapped in the American Revolutionary War by the need to ship all supplies across the Atlantic, since the Americans prevented most local purchases. The British found a solution after the war by creating the infrastructure and the experience needed to manage an empire. London reorganized the management of the supply of military food and transport that was completed in 1793–94 when the naval Victualling and Transport Boards undertook those responsibilities. It built upon experience learned from the supply of the very-long-distance Falklands garrison (1767–72) to systematize needed shipments to distant places such as Australia, Nova Scotia, and Sierra Leone. This new infrastructure allowed Britain to launch large expeditions to the Continent during the French Revolutionary War and to develop a global network of colonial garrisons.

Before the Napoleonic wars, military supply was based on contracts with private companies, looting and requisition (legal taking of whatever the army needed, with minimal compensation). Napoleon made logistical operations a major part of French strategy. During the Ulm Campaign in 1805, the French army of 200,000 men had no need for time-consuming efforts to scour the countryside for supplies and live off the land, as it was well provided for by France's German allies. France's ally, the Electorate of Bavaria, turned the city of Augsburg into a gigantic supply center, allowing the Grande Armée, generously replenished with food, shoes and ammunition, to quickly invade Austria after the decisive French victory at Ulm. Napoleon left nothing to chance, requesting the Bavarians to prepare in advance a specified amount of food at certain cities such as Würzburg and Ulm, for which the French reimbursed them. When French demands proved excessive for the German principalities, the French army used a system of vouchers to requisition supplies and keep the rapid French advance going. The agreements with French allies permitted the French to obtain huge quantities of supplies within a few days' notice. Napoleon built up a major supply magazine at Passau, with barges transporting supplies down the Danube to Vienna to maintain the French army prior to the Battle of Austerlitz in combat readiness. In 1807, Napoleon created the first "military train" regiments—units entirely dedicated to the supply and the transport of equipment.

The French system fared poorly in the face of a guerrilla warfare that targeted supply lines during the Peninsular War in Spain, and the British blockade of Spanish ports. The need to supply a besieged Barcelona made it impossible to control the province and ended French plans to incorporate Catalonia into Napoleon's Empire.

The first theoretical analysis of this was by the Swiss writer, Antoine-Henri Jomini, who studied the Napoleonic wars. In 1838, he devised a theory of war based on the trinity of "strategy", "tactics", and "logistics".

Railways and steamboats revolutionized logistics by the mid-19th century.

In the American Civil War (1861–65), both armies used railways extensively, for transport of personnel, supplies, horses and mules, and heavy field pieces. Both tried to disrupt the enemy's logistics by destroying trackage and bridges. Military railways were built specifically for supporting armies in the field.

During the Seven Weeks War of 1866, railways enabled the swift mobilization of the Prussian Army, but the problem of moving supplies from the end of rail lines to units at the front resulted in nearly 18,000 tons trapped on trains unable to be unloaded to ground transport. The Prussian use of railways during the Franco-Prussian War is often cited as a prime example of logistic modernizations, but the advantages of maneuver were often gained by abandoning supply lines that became hopelessly congested with rear-area traffic.

With the expansion of military conscription and reserve systems in the decades leading up to the 20th century, the potential size of armies increased substantially, while the industrialization of firepower (bolt-action rifles with higher rate-of-fire, larger and more artillery, plus machine guns) was starting to multiply the potential amount munitions each required. Military logistical systems, however, continued to rely on 19th century technology.

When World War I started, the capabilities of rail and horse-drawn supply were stretched to their limits. Where the stalemate of trench warfare took hold, special narrow gauge trench railways were built to extend the rail network to the front lines. The great size of the German Army proved too much for its railways to support except while immobile. Tactical successes like Operation Michael devolved into operational failures where logistics failed to keep up with the army's advance over shell-torn ground.

On the seas, the British blockade of Germany kept a stranglehold on raw materials, goods, and food needed to support Germany's war efforts, and is considered one of the key elements in the eventual Allied victory in the war. At the same time, Germany's unrestricted submarine warfare showed the vulnerability of shipping lanes despite Allied naval superiority.

The mechanization of warfare, starting at the tail end of World War I, added increasing ammo, fuel, and maintenance needs of tanks and other combat vehicles to the burden on military logistics. The growing needs of more powerful and numerous military ships and aircraft increased this burden even further. On the other hand, mechanization also brought trucks to logistics; though they generally require better roads and bridges, trucks are much faster and far more efficient than fodder-bound horse-drawn transport. While many nations, including Germany, continued to rely on wagons to some extent, the US and UK readily switched to trucks wherever possible.

Military logistics played a significant role in many World War II operations, especially ones far from industrial centers, from the Finnish Lapland to the Burma Campaign, limiting the size and movement of any military forces. In the North African Campaign, with a lack of rail, few roads, and hot-dry climate, attacks and advances were timed as much by logistics as enemy actions. Poor logistics, in the form of Russia's vast distances and its state of road and rail networks, contributed to the fate of Germany's invasion of the USSR: despite many battlefield victories, the campaign lost momentum before the gates of Moscow.

Breaking the logistics supply line became a major target for airpower; a single fighter aircraft could attack dozens of supply vehicles at a time by strafing down a road, many miles behind the front line. Air superiority became critical for almost any major offensive in good weather. Allied air forces took out German-controlled bridges and rail infrastructure throughout northern France to help ensure the success of the Normandy landings, but after the breakout from Normandy, this now limited the Allies' own logistics. In response, the Red Ball Express was organized—a massive truck convoy system to supply the advance towards Germany. During the Battle of Stalingrad, Supplying by air, called an airbridge, was attempted by Germany to keep its surrounded 6th Army supplied, but they lacked sufficient air transport. Allied airbridges were more successful, in the Burma Campaign, and in "The Hump" to resupply the Chinese war effort. (A few years after the war, the Berlin Air Lift was successful in supplying the whole non-Soviet half of the city.)

At sea, the Battle of the Atlantic began in the first days of the war and continued to the end. German surface raiders and U-boats targeted vital Allied cargo ship convoys supplying English, US, and Russian forces, and became more effective than in World War I. Technological improvements in both U-boats and anti-submarine warfare raced to out-do each other for years, with the Allies eventually keeping losses to U-boats in check.

Logistics was a major challenge for the American war effort, since wartime material had to be supplied across either the Atlantic or the even wider Pacific Ocean. Germany undertook an aggressive U-boat campaign against American logistics on the Atlantic, but the Japanese neglected to attack shipping in the Pacific, using their submarines to fight alongside the surface Navy in large-scale battles.

Long logistical distances dominated the Pacific War. For the attack on Pearl Harbor, the Japanese required numerous oiler ships to refuel the attacking fleet at sea on-route. Massive numbers of transports, including thousands of US Liberty ships, were required to sustain the Allied forces fighting back towards the Japanese homeland. As in the Atlantic, submarine warfare accounted for more losses than naval battles, with over 1,200 merchant ships sank.

Logistics, occasionally referred to as "combat service support", must address highly uncertain conditions. While perfect forecasts are rarely possible, forecast models can reduce uncertainty about what supplies or services will be needed, where and when they will be needed, or the best way to provide them.

Ultimately, responsible officials must make judgments on these matters, sometimes using intuition and scientifically weighing alternatives as the situation requires and permits. Their judgments must be based not only upon professional knowledge of the numerous aspects of logistics itself but also upon an understanding of the interplay of closely related military considerations such as strategy, tactics, intelligence, training, personnel, and finance.

However, case studies have shown that more quantitative, statistical analysis are often a significant improvement on human judgment. One such recent example is the use of Applied Information Economics by the Office of Naval Research and the Marine Corps for forecasting bulk fuel requirements for the battlefield.

In major military conflicts, logistics matters are often crucial in deciding the overall outcome of wars. For instance, tonnage war—the bulk sinking of cargo ships—was a crucial factor in World War II. The successful Allied anti-submarine campaign and the failure of the German Navy to sink enough cargo in the Battle of the Atlantic allowed Britain to stay in the war and the ability to maintain a Mediterranean supply chain allowed the maintenance of the second front against the Nazis in North Africa; by contrast, the successful U.S. submarine campaign against Japanese maritime shipping across Asian waters effectively crippled its economy and its military production capabilities and the Axis were unable to consistently maintain a supply chain to their North African forces with on average 25% fewer supplies than required being landed and critical fuel shortages dictating strategic decisions. In a tactical scale, in the Battle of Ilomantsi, the Soviets had an overwhelming numerical superiority in guns and men, but managed to fire only 10,000 shells against the Finnish 36,000 shells, eventually being forced to abandon their heavy equipment and flee the battlefield, resulting in a Finnish victory. One reason for this was the successful Finnish harassment of Soviet supply lines.

More generally, protecting one's own supply lines and attacking those of an enemy is a fundamental military strategy; an example of this as a purely logistical campaign for the military means of implementing strategic policy was the Berlin Airlift.

Military logistics has pioneered a number of techniques that have since become widely deployed in the commercial world. Operations research grew out of WWII military logistics efforts. Likewise, military logistics borrows from methods first introduced to the commercial world.

The Kargil Conflict in 1999 between India and Pakistan also referred to as Operation Vijay (Victory in Hindi) is one of the most recent examples of high altitude warfare in mountainous terrain that posed significant logistical problems for the combating sides. The Stallion which forms the bulk of the Indian Army's logistical vehicles proved its reliability and serviceability with 95% operational availability during the operation.

Geographic "distance" is a key factor in military affairs. The shorter the distance, the greater the ease with which force can be brought to bear upon an opponent. This is because it is easier to undertake the supply of logistics to a force on the ground as well as engage in bombardment. The importance of distance is demonstrated by the Loss of Strength Gradient devised by Kenneth Boulding. This shows the advantage of supply that is forward based.

The United States Military logistics support is grouped into 10 classes of supply:
Supply chain management in military logistics often deals with a number of variables in predicting cost, deterioration, consumption, and future demand. The US Military's categorical supply classification was developed in such a way that categories of supply with similar consumption variables are grouped together for planning purposes. For instance peacetime consumption of ammunition and fuel will be considerably less than wartime consumption of these items, whereas other classes of supply such as subsistence and clothing have a relatively consistent consumption rate regardless of war or peace. Troops will always require uniform and food. More troops will require equally more uniforms and food.

In the table above, each class of supply has a consumer. Some classes of supply have a linear demand relationship—as more troops are added more supply items are needed—as more equipment is used more fuel and ammo is consumed. Other classes of supply must consider a third variable besides usage and quantity: time. As equipment ages more and more repair parts are needed over time, even when usage and quantity stays consistent. By recording and analyzing these trends over time and applying to future scenarios, the US military can accurately supply troops with the items necessary at the precise moment they are needed. History has shown that good logistical planning creates a lean and efficient fighting force. Lack thereof can lead to a clunky, slow, and ill-equipped force with too much or too little supply.



Notes
Bibliography



</doc>
<doc id="40953958" url="https://en.wikipedia.org/wiki?curid=40953958" title="Public opinion of militaries">
Public opinion of militaries

Militaries and their troops are held in high regard in the United States, where military officers are considered to have one of the most prestigious jobs.

While military support is very high in most countries, there is variation. While 10% of Canadians viewed the military as "not at all favorable," only 3% of Britons had a "low" or "very low" view of the military. 65% of Russians believe their military, which is the second largest in the world, does their job "just about always" or "most of the time." 

In the United States, 89% of white Americans had a "very" or "somewhat" favorable opinion of the military, compared to 77% of Latinos and 72% of blacks.

In the United States, public opinion of the military was very low during the Vietnam War. The public perception increased considerably between the early 1970s to the late 1990s, with the exception of briefly after the Gulf War, when support was around 60%, the percentage of Americans who said they had a "very favorable" view of the US military hovered between 20% and 30%. By 2007, 47% claimed a "very favorite" view of the military and 84% expressed a "favorable" view.

Opposition to the Iraq War was comparable to opposition to the Vietnam War, but unlike the Vietnam War, opposition to the Iraq War did not correlate with a significant decrease in public opinion of military personnel themselves.



</doc>
<doc id="1376" url="https://en.wikipedia.org/wiki?curid=1376" title="Army">
Army

An army (from Latin "arma" "arms, weapons" via Old French "armée", "armed" [feminine]), ground force or land force is a fighting force that fights primarily on land. In the broadest sense, it is the land-based military branch, service branch or armed service of a nation or state. It may also include aviation assets by possessing an army aviation component. Within a national military force, the word army may also mean a field army.

In some countries, such as France and China, the term "army", especially in its plural form "armies", has the broader meaning of armed forces as a whole, while retaining the colloquial sense of land forces. To differentiate the colloquial army from the formal concept of military force, the term is qualified, for example in France the land force is called "Armée de terre", meaning Land Army, and the air force is called "Armée de l'Air", meaning Air Army. The naval force, although not using the term "army", is also included in the broad sense of the term "armies" — thus the French Navy is an integral component of the collective French Armies (French Armed Forces) under the Ministry of the Armies. A similar pattern is seen in China, with the People's Liberation Army (PLA) being the overall military, the "actual army" being the PLA Ground Force, and so forth for the PLA Air Force, the PLA Navy, and other branches.

The current largest army in the world, by number of active troops, is the PLA Ground Force of China with 1,600,000 active troops and 510,000 reserve personnel followed by the Indian Army with 1,129,000 active troops and 960,000 reserve personnel.

By convention, irregular military is understood in contrast to regular armies which grew slowly from personal bodyguards or elite militia. Regular in this case refers to standardized doctrines, uniforms, organizations, etc. Regular military can also refer to full-time status (standing army), versus reserve or part-time personnel. Other distinctions may separate statutory forces (established under laws such as the National Defence Act), from de facto "non-statutory" forces such as some guerrilla and revolutionary armies. Armies may also be expeditionary (designed for overseas or international deployment) or fencible (designed for – or restricted to – homeland defence)

India's armies were among the first in the world. The first recorded battle, the Battle of the Ten Kings, happened when an Hindu Aryan king named Sudas defeated an alliance of ten kings and their supportive chieftains. During the Iron Age, the Maurya and Nanda Empires had the largest armies in the world, the peak being approximately over 600,000 Infantry, 30,000 Cavalry, 8,000 War-Chariots and 9,000 War Elephants not including tributary state allies. In the Gupta age, large armies of longbowmen were recruited to fight off invading horse archer armies. Elephants, pikemen and cavalry were other featured troops.

In Rajput times, the main piece of equipment was iron or chain-mail armour, a round shield, either a curved blade or a straight-sword, a chakra disc and a katar dagger.

The states of China raised armies for at least 1000 years before the Spring and Autumn Annals. By the Warring States period, the crossbow had been perfected enough to become a military secret, with bronze bolts which could pierce any armor. Thus any political power of a state rested on the armies and their organization. China underwent political consolidation of the states of Han (韓), Wei (魏), Chu (楚), Yan (燕), Zhao (趙) and Qi (齊), until by 221 BCE, Qin Shi Huang (秦始皇帝), the first emperor of the Qin dynasty, attained absolute power. This first emperor of China could command the creation of a Terracotta Army to guard his tomb in the city of Xi'an (西安), as well as a realignment of the Great Wall of China to strengthen his empire against insurrection, invasion and incursion.

Sun Tzu's "The Art of War" remains one of China's Seven Military Classics, even though it is two thousand years old. Since no political figure could exist without an army, measures were taken to ensure only the most capable leaders could control the armies. Civil bureaucracies (士大夫) arose to control the productive power of the states, and their military power.

The Spartan Army was one of the earliest known professional armies. Boys were sent to a barracks at the age of seven or eight to train for becoming a soldier. At the age of thirty they were released from the barracks and allowed to marry and have a family. After that, men devoted their lives to war until their retirement at the age of 60. Unlike other civilizations, whose armies had to disband during the planting and harvest seasons, the Spartan serfs or "helots", did the manual labor.

This allowed the Spartans to field a full-time army with a campaign season that lasted all year. The Spartan Army was largely composed of hoplites, equipped with arms and armor nearly identical to each other. Each hoplite bore the Spartan emblem and a scarlet uniform. The main pieces of this armor were a round shield, a spear and a helmet.

The Roman Army had its origins in the citizen army of the Republic, which was staffed by citizens serving mandatory duty for Rome. Reforms turned the army into a professional organization which was still largely filled by citizens, but these citizens served continuously for 25 years before being discharged.

The Romans were also noted for making use of auxiliary troops, non-Romans who served with the legions and filled roles that the traditional Roman military could not fill effectively, such as light skirmish troops and heavy cavalry. After their service in the army they were made citizens of Rome and then their children were citizens also. They were also given land and money to settle in Rome. In the Late Roman Empire, these auxiliary troops, along with foreign mercenaries, became the core of the Roman Army; moreover, by the time of the Late Roman Empire tribes such as the Visigoths were paid to serve as mercenaries.

In the earliest Middle Ages it was the obligation of every aristocrat to respond to the call to battle with his own equipment, archers, and infantry. This decentralized system was necessary due to the social order of the time, but could lead to motley forces with variable training, equipment and abilities. The more resources the noble had access to, the better his troops would be.

Initially, the words "knight" and "noble" were used interchangeably as there was not generally a distinction between them. While the nobility did fight upon horseback, they were also supported by lower class citizens – and mercenaries and criminals – whose only purpose was participating in warfare because, most often than not, they held brief employment during their lord's engagement. As the Middle Ages progressed and feudalism developed in a legitimate social and economic system, knights started to develop into their own class with a minor caveat: they were still in debt to their lord. No longer primarily driven by economic need, the newly established vassal class were, instead, driven by fealty and chivalry.

As central governments grew in power, a return to the citizen armies of the classical period also began, as central levies of the peasantry began to be the central recruiting tool. England was one of the most centralized states in the Middle Ages, and the armies that fought in the Hundred Years' War were, predominantly, composed of paid professionals.

In theory, every Englishman had an obligation to serve for forty days. Forty days was not long enough for a campaign, especially one on the continent.

Thus the scutage was introduced, whereby most Englishmen paid to escape their service and this money was used to create a permanent army. However, almost all high medieval armies in Europe were composed of a great deal of paid core troops, and there was a large mercenary market in Europe from at least the early 12th century.

As the Middle Ages progressed in Italy, Italian cities began to rely mostly on mercenaries to do their fighting rather than the militias that had dominated the early and high medieval period in this region. These would be groups of career soldiers who would be paid a set rate. Mercenaries tended to be effective soldiers, especially in combination with standing forces, but in Italy they came to dominate the armies of the city states. This made them considerably less reliable than a standing army. Mercenary-on-mercenary warfare in Italy also led to relatively bloodless campaigns which relied as much on maneuver as on battles.

In 1439 the French legislature, known as the Estates General (French: "états généraux"), passed laws that restricted military recruitment and training to the king alone. There was a new tax to be raised known as the "taille" that was to provide funding for a new Royal army. The mercenary companies were given a choice of either joining the Royal army as "compagnies d'ordonnance" on a permanent basis, or being hunted down and destroyed if they refused. France gained a total standing army of around 6,000 men, which was sent out to gradually eliminate the remaining mercenaries who insisted on operating on their own. The new standing army had a more disciplined and professional approach to warfare than its predecessors. The reforms of the 1440s, eventually led to the French victory at Castillon in 1453, and the conclusion of the Hundred Years' War. By 1450 the companies were divided into the field army, known as the "grande ordonnance" and the garrison force known as the "petite ordonnance".

First nation states lacked the funds needed to maintain standing forces, so they tended to hire mercenaries to serve in their armies during wartime. Such mercenaries typically formed at the ends of periods of conflict, when men-at-arms were no longer needed by their respective governments.

The veteran soldiers thus looked for other forms of employment, often becoming mercenaries. Free Companies would often specialize in forms of combat that required longer periods of training that was not available in the form of a mobilized militia.

As late as the 1650s, most troops were mercenaries. However, after the 17th century, most states invested in better disciplined and more politically reliable permanent troops. For a time mercenaries became important as trainers and administrators, but soon these tasks were also taken by the state. The massive size of these armies required a large supporting force of administrators.

The newly centralized states were forced to set up vast organized bureaucracies to manage these armies, which some historians argue is the basis of the modern bureaucratic state. The combination of increased taxes and increased centralisation of government functions caused a series of revolts across Europe such as the Fronde in France and the English Civil War.

In many countries, the resolution of this conflict was the rise of absolute monarchy. Only in England and the Netherlands did representative government evolve as an alternative. From the late 17th century, states learned how to finance wars through long term low interest loans from national banking institutions. The first state to master this process was the Dutch Republic. This transformation in the armies of Europe had great social impact. The defense of the state now rested on the commoners, not on the aristocrats.

However, aristocrats continued to monopolise the officer corps of almost all early modern armies, including their high command. Moreover, popular revolts almost always failed unless they had the support and patronage of the noble or gentry classes. The new armies, because of their vast expense, were also dependent on taxation and the commercial classes who also began to demand a greater role in society. The great commercial powers of the Dutch and English matched much larger states in military might.

As any man could be quickly trained in the use of a musket, it became far easier to form massive armies. The inaccuracy of the weapons necessitated large groups of massed soldiers. This led to a rapid swelling of the size of armies. For the first time huge masses of the population could enter combat, rather than just the highly skilled professionals.

It has been argued that the drawing of men from across the nation into an organized corps helped breed national unity and patriotism, and during this period the modern notion of the nation state was born. However, this would only become apparent after the French Revolutionary Wars. At this time, the "levée en masse" and conscription would become the defining paradigm of modern warfare.

Before then, however, most national armies were in fact composed of many nationalities. In Spain armies were recruited from all the Spanish European territories including Spain, Italy, Wallonia (Walloon Guards) and Germany. The French recruited some soldiers from Germany, Switzerland as well as from Piedmont. Britain recruited Hessian and Hanovrian troops until the late 18th century. Irish Catholics made careers for themselves in the armies of many Catholic European states.

Prior to the English Civil War in England, the monarch maintained a personal bodyguard of Yeomen of the Guard and the Honourable Corps of Gentlemen at Arms, or "gentlemen pensioners", and a few locally raised companies to garrison important places such as Berwick on Tweed or Portsmouth (or Calais before it was recaptured by France in 1558).

Troops for foreign expeditions were raised upon an "ad hoc" basis. Noblemen and professional regular soldiers were commissioned by the monarch to supply troops, raising their quotas by indenture from a variety of sources. On January 26, 1661 Charles II issued the Royal Warrant that created the genesis of what would become the British Army, although the Scottish and English Armies would remain two separate organizations until the unification of England and Scotland in 1707. The small force was represented by only a few regiments.

After the American Revolutionary War the Continental Army was quickly disbanded as part of the Americans' distrust of standing armies, and irregular state militias became the sole ground army of the United States, with the exception of one battery of artillery guarding West Point's arsenal. Then First American Regiment was established in 1784. However, because of continuing conflict with Native Americans, it was soon realized that it was necessary to field a trained standing army. The first of these, the Legion of the United States, was established in 1791.

Until 1733 the common soldiers of Prussian Army consisted largely of peasantry recruited or impressed from Brandenburg–Prussia, leading many to flee to neighboring countries. To halt this trend, Frederick William I divided Prussia into regimental cantons. Every youth was required to serve as a soldier in these recruitment districts for three months each year; this met agrarian needs and added extra troops to bolster the regular ranks.

Russian tsars before Peter I of Russia maintained professional hereditary musketeer corps (streltsy in Russian) that were highly unreliable and undisciplined. In times of war the armed forces were augmented by peasants. Peter I introduced a modern regular army built on German model, but with a new aspect: officers not necessarily from nobility, as talented commoners were given promotions that eventually included a noble title at the attainment of an officer's rank. Conscription of peasants and townspeople was based on quota system, per settlement. Initially it was based on the number of households, later it was based on the population numbers. The term of service in the 18th century was for life. In 1793 it was reduced to 25 years. In 1834 it was reduced to 20 years plus 5 years in reserve and in 1855 to 12 years plus 3 years of reserve.

The first Ottoman standing army were Janissaries. They replaced forces that mostly comprised tribal warriors ("ghazis") whose loyalty and morale could not always be trusted. The first Janissary units were formed from prisoners of war and slaves, probably as a result of the sultan taking his traditional one-fifth share of his army's booty in kind rather than cash.

From the 1380s onwards, their ranks were filled under the "devşirme" system, where feudal dues were paid by service to the sultan. The "recruits" were mostly Christian youths, reminiscent of mamluks.

China organized the Manchu people into the Eight Banner system in the early 17th century. Defected Ming armies formed the Green Standard Army. These troops enlisted voluntarily and for long terms of service.

Conscription allowed the French Republic to form the "Grande Armée", what Napoleon Bonaparte called "the nation in arms", which successfully battled European professional armies.

Conscription, particularly when the conscripts are being sent to foreign wars that do not directly affect the security of the nation, has historically been highly politically contentious in democracies.

Canada also had a political dispute over conscription during World War II. Similarly, mass protests against conscription to fight the Vietnam War occurred in several countries in the late 1960s.

In developed nations, the increasing emphasis on technological firepower and better-trained fighting forces, the sheer unlikelihood of a conventional military assault on most developed nations, as well as memories of the contentiousness of the Vietnam War experience, make mass conscription unlikely in the foreseeable future.

Russia, as well as many other nations, retains mainly a conscript army. There is also a very rare "citizen army" as used in Switzerland (see Military of Switzerland).

Western armies are usually subdivided as follows:

A field army is composed of a headquarters, army troops, a variable number of corps, typically between three and four, and a variable number of divisions, also between three and four. A battle is influenced at the Field Army level by transferring divisions and reinforcements from one corps to another to increase the pressure on the enemy at a critical point. Field armies are controlled by a general or lieutenant general.

A particular army can be named or numbered to distinguish it from military land forces in general. For example, the First United States Army and the Army of Northern Virginia. In the British Army it is normal to spell out the ordinal number of an army (e.g. First Army), whereas lower formations use figures (e.g. 1st Division).

Armies (as well as army groups and theaters) are large formations which vary significantly between armed forces in size, composition, and scope of responsibility.

In the Soviet Red Army and the Soviet Air Force, "Armies" could vary in size, but were subordinate to an Army Group-sized "front" in wartime. In peacetime, a Soviet army was usually subordinate to a military district. Viktor Suvorov's "Inside the Soviet Army" describes how Cold War era Soviet military districts were actually composed of a front headquarters and a military district headquarters co-located for administration and deception ('maskirovika') reasons.



</doc>
<doc id="7378204" url="https://en.wikipedia.org/wiki?curid=7378204" title="Security forces">
Security forces

Security Forces are statutory organisations with internal security mandates. In the legal context of several nations, the term has variously denoted police and military units working in concert, or the role of military and paramilitary forces (such as gendarmerie) tasked with the internal provision of public security. 

Examples of formally designated security forces include:




</doc>
<doc id="58764523" url="https://en.wikipedia.org/wiki?curid=58764523" title="Aristeus">
Aristeus

Aristeus (), son of Adeimantus (Ἀδείμαντος; "Adeímantos"), was a Corinthian general who commanded the expedition to Potidaea in 432 BC. After the Athenians broke a truce with the Corinthians at Sybota, his primary goal was to defend Potidaea from an Athenian attack. He then went on to defend the Corinthian colony from Athens during the Battle of Potidaea in 432 BC, until he was left with no option but to leave the colony with the Chalcidians. In 430 BC he traveled to Thrace with Spartan envoys where they were discovered by Athenians and brought to Athens, by Athenian ambassadors, where they were promptly killed without a trial. After Aristeus' death, Athens seized Potidaea in 430/429 BC during the Peloponnesian War, the battles of Sybota and Potidaea being two main catalysts for the war.

In 432 BC, Aristeus was appointed as commander of the Corinthian military expedition for the relief of their colony Potidaea, which had just seceded from Athens. He was able to recruit Corinthian volunteers and mercenaries from the rest of the Peloponnesus to fight alongside him due to his popularity, both domestically and in Potidaea. After the Battle of Sybota, in which the Corinthians fought a combined force of Athenians and Corcyraeans in 433 BC, Corinth was furious with the Athenians for fighting alongside Corcyraeans during a time of truce. 

The Athenians became worried that Potidaea would revolt against Athens and immediately sought to win over Corinth. Forty days after the revolt in Potidaea, Aristeus, along with 1600 hoplites and 400 light troops, arrived in Thrace and shortly thereafter encamped at Olynthus, where they prepared for battle. 

Meanwhile Athens, informed that Aristeus and his troops were on their way to defend Potidaea, sent 2000 hoplites along with 40 ships to prepare to oppose him. Under the command of the general Callias and four of his colleagues, the Athenian troops first traveled to Macedonia, where they besieged Pydna. It was at this time that Athens, realizing the revolt in Potidaea and Aristeus' intention to defend it, forced Perdiccas, the Macedonian king, to return to his alliance with them and march on Potidaea. With the alliance with Perdiccas formalized, Athens withdrew its troops from Macedonia, via Beroea and Strepsa, and marched on Potidaea with 3000 hoplites and 600 Macedonian horsemen, with 70 ships following closely along the coast. After three days of advancing on Potidaea by short marches, the Athenian forces encamped at Gigonus. 

In their preparation for the Athenian attack on their city, the citizens of Potidaea and the Peloponnesians encamped at Olynthus and chose Aristeus as general of all the infantry. Perdiccas, who had deputed Iolaus as his general, left the alliance with Athens, returned to that with Potidaea, and was made commander of the allied cavalry. Aristeus' next move was to keep his infantrymen on the isthmus to await the Athenian attack. The Chalcidians and the other allies waited nearby, while the 200 cavalry from Perdiccas stayed in Olynthus to attack the Athenian rear, in case of an attack on Aristeus' infantry. By doing this, Aristeus placed the Athenian advance between the allied forces on the isthmus and at Olynthus. Knowing that Aristeus' infantry posed a threat to the Athenian advance, Callias and his colleagues sent away the Macedonian horsemen and a group of allies to prevent an attack from Olynthus, while the rest of the Athenians marched on Potidaea. 

Shortly after the Athenians arrived at the isthmus, the armies engaged. Aristeus' infantry, which consisted of Corinthians and other chosen men, routed the Athenian advance and pursued them for a considerable distance. However, the remaining army of Potidaeans and Peloponnesians met a different fate; they were routed by the Athenians and were forced to take refuge behind fortifications. When he returned from pursuing the Athenians, Aristeus realized the defeat of the rest of the army and was left to choose whether to go to Olynthus or to Potidaea. Once his men were assembled, Aristeus led them along the breakwater to Potidaea, while being attacked. Some perished, but most made it safely.

Seeing this occur and battle signals raised, Potidaean allies from Olynthus began to advance to provide aid. The Macedonian horsemen successfully countered those troops. Predicting an Athenian victory, the Potidaeans retreated back within their walls, while the Macedonians, having seen the battle signals taken down, retreated to the Athenians, leaving no cavalry on either side. When the battle concluded, the Athenians set up a trophy and, under truce, allowed the Potidaeans to collect their dead, which numbered nearly 300. The Athenians lost 150 men, including their general, Callias.

After the battle of Potidaea ended, Athenians built a wall on the Macedonian side of the isthmus, consolidating their forces there and leaving the Pallene side unmanned. Under the command of Phormio, 1600 hoplites were sent from Athens for reinforcement. Upon their arrival at Pallene, a new Athenian headquarters was constructed at Aphytis. Phormio's troops defeated the Potidaeans in the field, built a wall on the Pallene side, and deployed Athenian ships around the peninsula, thus besieging Potidaea by land and sea. Aristeus recognized that the peninsula was unsalvageable and instructed the remaining troops, except for 500 of them, to sail away. He attempted to convince his troops to let him stay in Potidaea but was not successful. After evading Athenian ships, he sailed away. Although the conflict was all but finished, Aristeus remained with the Chalcidians and successfully ambushed Athenians near Aphytis. In an attempt to buy mercenaries to fight for him, he was in communication with the Peloponnesus. Phormio ended the siege of Potidaea by sending his 1600 hoplites to destroy Chalcidice and Bottica. Athenians and Peloponnesians continued to debate their claim to Potidaea; their conflict on the isthmus would be a precursor to the Peloponnesian War.

In the summer of 430 BC, Aristeus, along with a group of Spartans—including Aneristus, Nicolaus and Protodamus, Timagoras from Tegea and Pollis from Argos—traveled to Thrace to meet Sitalces, the son of the former Thracian king, Teres I, in an attempt to persuade the king to supply funds and betray his alliance to Athens. In particular, they wanted Sitalces' assistance to march yet again on Potidaea, which was occupied by Athenian forces, and to join them in the war against Athens. Athenian ambassadors happened to be with Sitalces at the time, and they convinced his son, Sadocus, to seize Aristeus and the other individuals as they traveled through Thrace to the vessel in which they were to cross the Hellespont. It followed that Aristeus and the other individuals were handed over to the Athenian ambassadors and brought to Athens. When they arrived back in Athens, the Athenians were aware that Aristeus had commanded the Corinthians and others at Potidaea and refused to give them a trial, being afraid that if he escaped, Aristeus would cause them to suffer again. Aristeus and the other envoys were killed immediately and cast into a pit, a familiar mode of death that the Spartans themselves were known for in the Peloponnesian War. In 430 BC, Potidaea was seized by Athens.


</doc>
<doc id="167928" url="https://en.wikipedia.org/wiki?curid=167928" title="Republican guard">
Republican guard

A republican guard, sometimes called a national guard, is a state organization of a country (often a republic) which typically serves to protect the head of state and the government, and thus is often synonymous with a presidential guard. The term is derived from the original French Gendarmerie unit. Several other countries also have adopted the term and have active guard units.





</doc>
<doc id="365009" url="https://en.wikipedia.org/wiki?curid=365009" title="National Guard">
National Guard

National Guard is the name used by a wide variety of current and historical uniformed organizations in different countries. The original National Guard was formed during the French Revolution around a cadre of defectors from the French Guards.

National Guard may refer to:







</doc>
<doc id="562759" url="https://en.wikipedia.org/wiki?curid=562759" title="Defence Forces">
Defence Forces

Defence Force(s) or Defense Force(s) is the title of the armed forces of certain countries. 



At the start of the Cold War, the United States Air Force had established the Aerospace Defense Command. It was broken into three different regions:


In South Africa under apartheid the nominally independent Bantustans had their own forces, separately from the South African Defence Force:

A number of football clubs related to defence forces are also named such, for example:


</doc>
<doc id="92357" url="https://en.wikipedia.org/wiki?curid=92357" title="Military">
Military

A military is a heavily armed, highly organised force primarily intended for warfare, also known collectively as armed forces. It is typically officially authorized and maintained by a sovereign state, with its members identifiable by their distinct military uniform. It may consist of one or more military branches such as an army, navy, air force, space force, marines, or coast guard. The main task of the military is usually defined as defence of the state and its interests against external armed threats.

In broad usage, the terms "armed forces" and "military" are often treated as synonymous, although in technical usage a distinction is sometimes made in which a country's armed forces may include both its military and other paramilitary forces. There are various forms of irregular military forces, not belonging to a recognized state; though they share many attributes with regular military forces, they are less often referred to as simply "military".

A nation's military may function as a discrete social subculture, with dedicated infrastructure such as military housing, schools, utilities, logistics, hospitals, legal services, food production, finance, and banking services. Beyond warfare, the military may be employed in additional sanctioned and non-sanctioned functions within the state, including internal security threats, population control, the promotion of a political agenda, emergency services and reconstruction, protecting corporate economic interests, social ceremonies and national honor guards.

The profession of soldiering as part of a military is older than recorded history itself. Some of the most enduring images of classical antiquity portray the power and feats of its military leaders. The Battle of Kadesh in 1274 BC was one of the defining points of Pharaoh Ramses II's reign, and his monuments commemorate it in bas-relief. A thousand years later, the first emperor of unified China, Qin Shi Huang, was so determined to impress the gods with his military might that he had himself buried with an army of terracotta soldiers.
The Romans paid considerable attention to military matters, leaving to posterity many treatises and writings on the subject, as well as many lavishly carved triumphal arches and victory columns.

The first recorded use of the word military in English, spelled , was in 1582. It comes from the Latin "militaris" (from Latin "miles", meaning "soldier") through French, but is of uncertain etymology, one suggestion being derived from "*mil-it-" – going in a body or mass. 

As a noun, the military usually refers generally to a country's armed forces, or sometimes, more specifically, to the senior officers who command them. In general, it refers to the physicality of armed forces, their personnel, equipment, and the physical area which they occupy.

As an adjective, military originally referred only to soldiers and soldiering, but it soon broadened to apply to land forces in general, and anything to do with their profession. The names of both the Royal Military Academy (1741) and United States Military Academy (1802) reflect this. However, at about the time of the Napoleonic Wars, 'military' began to be used in reference to armed forces as a whole, such as 'military service', 'military intelligence', and 'military history'. As such, it now connotes any activity performed by armed force personnel.

Military history is often considered to be the history of all conflicts, not just the history of the state militaries. It differs somewhat from the history of war, with military history focusing on the people and institutions of war-making, while the history of war focuses on the evolution of war itself in the face of changing technology, governments, and geography.

Military history has a number of facets. One main facet is to learn from past accomplishments and mistakes, so as to more effectively wage war in the future. Another is to create a sense of military tradition, which is used to create cohesive military forces. Still, another may be to learn to prevent wars more effectively. Human knowledge about the military is largely based on both recorded and oral history of military conflicts (war), their participating armies and navies and, more recently, air forces.

There are two types of military history, although almost all texts have elements of both: descriptive history, that serves to chronicle conflicts without offering any statements about the causes, nature of conduct, the ending, and effects of a conflict; and analytical history, that seeks to offer statements about the causes, nature, ending, and aftermath of conflicts – as a means of deriving knowledge and understanding of conflicts as a whole, and prevent repetition of mistakes in future, to suggest better concepts or methods in employing forces, or to advocate the need for new technology.

Despite the growing importance of military technology, military activity depends above all on people. For example, in 2000 the British Army declared: "Man is still the first weapon of war."

The military organization is characterized by a strict command hierarchy divided by military rank, with ranks normally grouped (in descending order of authority) as officers (e.g. Colonel), non-commissioned officers (e.g. Sergeant), and personnel at the lowest rank (e.g. Private Soldier). While senior officers make strategic decisions, subordinated military personnel (soldiers, sailors, marines, or airmen) fulfil them. Although rank titles vary by military branch and country, the rank hierarchy is common to all state armed forces worldwide.

In addition to their rank, personnel occupy one of many trade roles, which are often grouped according to the nature of the role's military task on combat operations: combat roles (e.g. infantry), combat support roles (e.g. combat engineers), and combat service support roles (e.g. logistical support).

Personnel may be recruited or conscripted, depending on the system chosen by the state. Most military personnel are males; the minority proportion of female personnel varies internationally (approximately 3% in India, 10% in the UK, 13% in Sweden, 16% in the US, and 27% in South Africa). While two-thirds of states now recruit or conscript only adults, as of 2017 50 states still relied partly on children under the age of 18 (usually aged 16 or 17) to staff their armed forces.

Whereas recruits who join as officers tend to be upwardly-mobile, most enlisted personnel have a childhood background of relative socio-economic deprivation. For example, after the US suspended conscription in 1973, "the military disproportionately attracted African American men, men from lower-status socioeconomic backgrounds, men who had been in nonacademic high school programs, and men whose high school grades tended to be low".

The obligations of military employment are many. Full-time military employment normally requires a minimum period of service of several years; between two and six years is typical of armed forces in Australia, the UK and the US, for example, depending on role, branch, and rank. Some armed forces allow a short discharge window, normally during training, when recruits may leave the armed force as of right. Alternatively, part-time military employment, known as reserve service, allows a recruit to maintain a civilian job while training under military discipline at weekends; he or she may be called out to deploy on operations to supplement the full-time personnel complement. After leaving the armed forces, recruits may remain liable for compulsory return to full-time military employment in order to train or deploy on operations.

Military law introduces offences not recognised by civilian courts, such as absence without leave (AWOL), desertion, political acts, malingering, behaving disrespectfully, and disobedience (see, for example, offences against military law in the United Kingdom). Penalties range from a summary reprimand to imprisonment for several years following a court martial. Certain fundamental rights are also restricted or suspended, including the freedom of association (e.g. union organizing) and freedom of speech (speaking to the media). Military personnel in some countries have a right of conscientious objection if they believe an order is immoral or unlawful, or cannot in good conscience carry it out.

Personnel may be posted to bases in their home country or overseas, according to operational need, and may be deployed from those bases on exercises or operations anywhere in the world. During peacetime, when military personnel are generally stationed in garrisons or other permanent military facilities, they mostly conduct administrative tasks, training and education activities, technology maintenance, and recruitment.

Initial training conditions recruits for the demands of military life, including preparedness to injure and kill other people, and to face mortal danger without fleeing. It is a physically and psychologically intensive process which resocializes recruits for the unique nature of military demands. For example:

The next requirement comes as a fairly basic need for the military to identify possible threats it may be called upon to face. For this purpose, some of the commanding forces and other military, as well as often civilian personnel participate in identification of these threats. This is at once an organisation, a system and a process collectively called military intelligence (MI).

The difficulty in using military intelligence concepts and military intelligence methods is in the nature of the secrecy of the information they seek, and the clandestine nature that intelligence operatives work in obtaining what may be plans for a conflict escalation, initiation of combat, or an invasion.

An important part of the military intelligence role is the military analysis performed to assess military capability of potential future aggressors, and provide combat modelling that helps to understand factors on which comparison of forces can be made. This helps to quantify and qualify such statements as: "China and India maintain the largest armed forces in the World" or that "the U.S. Military is considered to be the world's strongest".
Although some groups engaged in combat, such as militants or resistance movements, refer to themselves using military terminology, notably 'Army' or 'Front', none have had the structure of a national military to justify the reference, and usually have had to rely on support of outside national militaries. They also use these terms to conceal from the MI their true capabilities, and to impress potential ideological recruits.

Having military intelligence representatives participate in the execution of the national defence policy is important, because it becomes the first respondent and commentator on the policy expected strategic goal, compared to the realities of identified threats. When the intelligence reporting is compared to the policy, it becomes possible for the national leadership to consider allocating resources over and above the officers and their subordinates military pay, and the expense of maintaining military facilities and military support services for them.

Defense economics is the financial and monetary efforts made to resource and sustain militaries, and to finance military operations, including war.

The process of allocating resources is conducted by determining a military budget, which is administered by a military finance organisation within the military. Military procurement is then authorised to purchase or contract provision of goods and services to the military, whether in peacetime at a permanent base, or in a combat zone from local population.

Capability development, which is often referred to as the military 'strength', is arguably one of the most complex activities known to humanity; because it requires determining: strategic, operational, and tactical capability requirements to counter the identified threats; strategic, operational, and tactical doctrines by which the acquired capabilities will be used; identifying concepts, methods, and systems involved in executing the doctrines; creating design specifications for the manufacturers who would produce these in adequate quantity and quality for their use in combat; purchase the concepts, methods, and systems; create a forces structure that would use the concepts, methods, and systems most effectively and efficiently; integrate these concepts, methods, and systems into the force structure by providing military education, training, and practice that preferably resembles combat environment of intended use; create military logistics systems to allow continued and uninterrupted performance of military organisations under combat conditions, including provision of health services to the personnel, and maintenance for the equipment; the services to assist recovery of wounded personnel, and repair of damaged equipment; and finally, post-conflict demobilisation, and disposal of war stocks surplus to peacetime requirements.

Development of military doctrine is perhaps the more important of all capability development activities, because it determines how military forces were, and are used in conflicts, the concepts and methods used by the command to employ appropriately military skilled, armed and equipped personnel in achievement of the tangible goals and objectives of the war, campaign, battle, engagement, action or a duel. The line between strategy and tactics is not easily blurred, although deciding which is being discussed had sometimes been a matter of personal judgement by some commentators, and military historians. The use of forces at the level of organisation between strategic and tactical is called operational mobility.

There have been attempts to produce a military strength index: this is an example taken from a Credit Suisse report in September 2015. The factors under consideration for that military strength indicator and their total weights were: number of active personnel in the armed forces (5%), tanks (10%), attack helicopters (15%), aircraft (20%), aircraft carriers (25%), and submarines (25%). It was practically impossible to make an estimation of the actual training of the armed forces.

These were the results:

Because most of the concepts and methods used by the military, and many of its systems are not found in commercial branches, much of the material is researched, designed, developed, and offered for inclusion in arsenals by military science organisations within the overall structure of the military. Military scientists are therefore found to interact with all Arms and Services of the armed forces, and at all levels of the military hierarchy of command.

Although concerned with research into military psychology, particularly combat stress and how it affect troop morale, often the bulk of military science activities is directed at military intelligence technology, military communications, and improving military capability through research. The design, development, and prototyping of weapons, military support equipment, and military technology in general, is also an area in which lots of effort is invested – it includes everything from global communication networks and aircraft carriers to paint and food.

Possessing military capability is not sufficient if this capability cannot be deployed for, and employed in combat operations. To achieve this, military logistics are used for the logistics management and logistics planning of the forces military supply chain management, the consumables, and capital equipment of the troops.

Although mostly concerned with the military transport, as a means of delivery using different modes of transport; from military trucks, to container ships operating from permanent military base, it also involves creating field supply dumps at the rear of the combat zone, and even forward supply points in specific unit's Tactical Area of Responsibility.

These supply points are also used to provide military engineering services, such as the recovery of defective and derelict vehicles and weapons, maintenance of weapons in the field, the repair and field modification of weapons and equipment; and in peacetime, the life-extension programmes undertaken to allow continued use of equipment. One of the most important role of logistics is the supply of munitions as a primary type of consumable, their storage, and disposal.

The primary reason for the existence of the military is to engage in combat, should it be required to do so by the national defence policy, and to win. This represents an organisational goal of any military, and the primary focus for military thought through military history. How victory is achieved, and what shape it assumes, is studied by most, if not all, military groups on three levels.

Military strategy is the management of forces in wars and military campaigns by a commander-in-chief, employing large military forces, either national and allied as a whole, or the component elements of armies, navies and air forces; such as army groups, naval fleets, and large numbers of aircraft. Military strategy is a long-term projection of belligerents' policy, with a broad view of outcome implications, including outside the concerns of military command. Military strategy is more concerned with the supply of war and planning, than management of field forces and combat between them. The scope of strategic military planning can span weeks, but is more often months or even years.

Operational mobility is, within warfare and military doctrine, the level of command which coordinates the minute details of tactics with the overarching goals of strategy. A common synonym is operational art.

The operational level is at a scale bigger than one where line of sight and the time of day are important, and smaller than the strategic level, where production and politics are considerations. Formations are of the operational level if they are able to conduct operations on their own, and are of sufficient size to be directly handled or have a significant impact at the strategic level. This concept was pioneered by the German army prior to and during the Second World War. At this level, planning and duration of activities takes from one week to a month, and are executed by Field Armies and Army Corps and their naval and air equivalents.

Military tactics concerns itself with the methods for engaging and defeating the enemy in direct combat. Military tactics are usually used by units over hours or days, and are focused on the specific, close proximity tasks and objectives of squadrons, companies, battalions, regiments, brigades, and divisions, and their naval and air force equivalents.

One of the oldest military publications is "The Art of War", by the Chinese philosopher Sun Tzu. Written in the 6th century BCE, the 13-chapter book is intended as military instruction, and not as military theory, but has had a huge influence on Asian military doctrine, and from the late 19th century, on European and United States military planning. It has even been used to formulate business tactics, and can even be applied in social and political areas.

The Classical Greeks and the Romans wrote prolifically on military campaigning. Among the best-known Roman works are Julius Caesar's commentaries on the Gallic Wars, and the Roman Civil war – written about 50 BC.

Two major works on tactics come from the late Roman period: "Taktike Theoria" by Aelianus Tacticus, and "De Re Militari" ('On military matters') by Vegetius. "Taktike Theoria" examined Greek military tactics, and was most influential in the Byzantine world and during the Golden Age of Islam.

"De Re Militari" formed the basis of European military tactics until the late 17th century. Perhaps its most enduring maxim is "Igitur qui desiderat pacem, praeparet bellum" (let he who desires peace prepare for war).

Due to the changing nature of combat with the introduction of artillery in the European Middle Ages, and infantry firearms in the Renaissance, attempts were made to define and identify those strategies, grand tactics, and tactics that would produce a victory more often than that achieved by the Romans in praying to the gods before the battle.

Later this became known as military science, and later still, would adopt the scientific method approach to the conduct of military operations under the influence of the Industrial Revolution thinking. In his seminal book "On War", the Prussian Major-General and leading expert on modern military strategy, Carl von Clausewitz defined military strategy as 'the employment of battles to gain the end of war'. According to Clausewitz: 
strategy forms the plan of the War, and to this end it links together the series of acts which are to lead to the final decision, that is to say, it makes the plans for the separate campaigns and regulates the combats to be fought in each.

Hence, Clausewitz placed political aims above military goals, ensuring civilian control of the military. Military strategy was one of a triumvirate of 'arts' or 'sciences' that governed the conduct of warfare, the others being: military tactics, the execution of plans and manoeuvring of forces in battle, and maintenance of an army.

The meaning of military tactics has changed over time; from the deployment and manoeuvring of entire land armies on the fields of ancient battles, and galley fleets; to modern use of small unit ambushes, encirclements, bombardment attacks, frontal assaults, air assaults, hit-and-run tactics used mainly by guerrilla forces, and, in some cases, suicide attacks on land and at sea. Evolution of aerial warfare introduced its own air combat tactics. Often, military deception, in the form of military camouflage or misdirection using decoys, is used to confuse the enemy as a tactic.

A major development in infantry tactics came with the increased use of trench warfare in the 19th and 20th centuries. This was mainly employed in World War I in the Gallipoli campaign, and the Western Front. Trench warfare often turned to a stalemate, only broken by a large loss of life, because, in order to attack an enemy entrenchment, soldiers had to run through an exposed 'no man's land' under heavy fire from their opposing entrenched enemy.

As with any occupation, since the ancient times, the military has been distinguished from other members of the society by their tools, the military weapons, and military equipment used in combat. When Stone Age humans first took a sliver of flint to tip the spear, it was the first example of applying technology to improve the weapon.

Since then, the advances made by human societies, and that of weapons, has been irretrievably linked. Stone weapons gave way to Bronze Age weapons, and later, the Iron Age weapons. With each technological change, was realised some tangible increase in military capability, such as through greater effectiveness of a sharper edge in defeating leather armour, or improved density of materials used in manufacture of weapons.

On land, the first really significant technological advance in warfare was the development of the ranged weapons, and notably, the sling. The next significant advance came with the domestication of the horses and mastering of equestrianism.
Arguably, the greatest invention that affected not just the military, but all society, after adoption of fire, was the wheel, and its use in the construction of the chariot. There were no advances in military technology, until, from the mechanical arm action of a slinger, the Greeks, Egyptians, Romans, Persians, Chinese, etc., developed the siege engines. The bow was manufactured in increasingly larger and more powerful versions, to increase both the weapon range, and armour penetration performance. These developed into the powerful composite and recurve bows, and crossbows of Ancient China. These proved particularly useful during the rise of cavalry, as horsemen encased in ever-more sophisticated armour came to dominate the battlefield.

Somewhat earlier, in medieval China, gunpowder had been invented, and was increasingly used by the military in combat. The use of gunpowder in the early vase-like mortars in Europe, and advanced versions of the long bow and cross bow, which all had armour-piercing arrowheads, that put an end to the dominance of the armoured knight. After the long bow, which required great skill and strength to use, the next most significant technological advance was the musket, which could be used effectively, with little training. In time, the successors to muskets and cannon, in the form of rifles and artillery, would become core battlefield technology.

As the speed of technological advances accelerated in civilian applications, so too warfare became more industrialised. The newly invented machine gun and repeating rifle redefined firepower on the battlefield, and, in part, explains the high casualty rates of the American Civil War. The next breakthrough was the conversion of artillery parks from the muzzle loading guns, to the quicker loading breech loading guns with recoiling barrel that allowed quicker aimed fire and use of a shield. The widespread introduction of low smoke (smokeless) propellant powders since the 1880s also allowed for a great improvement of artillery ranges.

The development of breech loading had the greatest effect on naval warfare, for the first time since the Middle Ages, altering the way weapons are mounted on warships, and therefore naval tactics, now divorced from the reliance on sails with the invention of the internal combustion. A further advance in military naval technology was the design of the submarine, and its weapon, the torpedo.

Main battle tanks, and other heavy equipment such as armoured fighting vehicles, military aircraft, and ships, are characteristic to organised military forces.

During World War I, the need to break the deadlock of trench warfare saw the rapid development of many new technologies, particularly tanks. Military aviation was extensively used, and bombers became decisive in many battles of World War II, which marked the most frantic period of weapons development in history. Many new designs, and concepts were used in combat, and all existing technologies of warfare were improved between 1939 and 1945.

During the war, significant advances were made in military communications through increased use of radio, military intelligence through use of the radar, and in military medicine through use of penicillin, while in the air, the guided missile, jet aircraft, and helicopters were seen for the first time. Perhaps the most infamous of all military technologies was the creation of the atomic bomb, although the exact effects of its radiation were unknown until the early 1950s. Far greater use of military vehicles had finally eliminated the cavalry from the military force structure.
After World War II, with the onset of the Cold War, the constant technological development of new weapons was institutionalised, as participants engaged in a constant 'arms race' in capability development. This constant state of weapons development continues into the present, and remains a constant drain on national resources, which some blame on the military-industrial complex.

The most significant technological developments that influenced combat have been the guided missiles, which can be used by all branches of the armed services. More recently, information technology, and its use in surveillance, including space-based reconnaissance systems, have played an increasing role in military operations.

The impact of information warfare that focuses on attacking command communication systems, and military databases, has been coupled with the new development in military technology, has been the use of robotic systems in intelligence combat, both in hardware and software applications.

Recently, there has also been a particular focus towards the use of renewable fuels for running military vehicles on. Unlike fossil fuels, renewable fuels can be produced in any country, creating a strategic advantage. The US military has already committed itself to have 50% of its energy consumption come from alternative sources.

For much of military history, the armed forces were considered to be for use by the heads of their societies, until recently, the crowned heads of states. In a democracy or other political system run in the public interest, it is a public force.

The relationship between the military and the society it serves is a complicated and ever-evolving one. Much depends on the nature of the society itself, and whether it sees the military as important, as for example in time of threat or war, or a burdensome expense typified by defence cuts in time of peace.

One difficult matter in the relation between military and society is control and transparency. In some countries, limited information on military operations and budgeting is accessible for the public. However transparency in the military sector is crucial to fight corruption. This showed the Government Defence Anti-corruption Index Transparency International UK published in 2013.

Militaries often function as societies within societies, by having their own military communities, economies, education, medicine, and other aspects of a functioning civilian society. Although a 'military' is not limited to nations in of itself as many private military companies (or PMC's) can be used or 'hired' by organisations and figures as security, escort, or other means of protection; where police, agencies, or militaries are absent or not trusted.

Militarist ideology is the society's social attitude of being best served, or being a beneficiary of a government, or guided by concepts embodied in the military "culture, doctrine, system", or "leaders".

Either because of the cultural memory, national history, or the potentiality of a military threat, the militarist argument asserts that a civilian population is dependent upon, and thereby subservient to the needs and goals of its military for continued independence. Militarism is sometimes contrasted with the concepts of comprehensive national power, soft power and hard power.

Most nations have separate military laws which regulate conduct in war and during peacetime. An early exponent was Hugo Grotius, whose "On the Law of War and Peace" (1625) had a major impact of the humanitarian approach to warfare development. His theme was echoed by Gustavus Adolphus.

Ethics of warfare have developed since 1945, to create constraints on the military treatment of prisoners and civilians, primarily by the Geneva Conventions; but rarely apply to use of the military forces as internal security troops during times of political conflict that results in popular protests and incitement to popular uprising.

International protocols restrict the use, or have even created international bans on some types of weapons, notably weapons of mass destruction (WMD). International conventions define what constitutes a war crime, and provides for war crimes prosecution. Individual countries also have elaborate codes of military justice, an example being the United States' Uniform Code of Military Justice that can lead to court martial for military personnel found guilty of war crimes.

Military actions are sometimes argued to be justified by furthering a humanitarian cause, such as disaster relief operations, or in defence of refugees. The term military humanism is used to refer to such actions.




</doc>
<doc id="62161738" url="https://en.wikipedia.org/wiki?curid=62161738" title="Alabama Military Academy">
Alabama Military Academy

Alabama Military Academy, also known as the Fort McClellan Army National Guard Training Center, is a National Guard training school for officer candidates at Fort McClellan, Alabama. It was established in 1957 and has the motto "It shall be done". Alumni include Tulsi Gabbard.

The training center celebrated Fort McClellan's 100th anniversary in 2017.

Cadets participate in a six-mile march at Talladega Speedway.

United States Representative Tulsi Gabbard was the first woman to finish as the distinguished honor graduate in the Academy's 50-year history. 

On February 13, 2009, comedian Sacha Baron Cohen fooled guard officers into allowing him to participate in training at the Alabama Military Academy at Fort McClellan. The officers were led to believe that Cohen was a reporter making a German TV documentary. The incident ended when an Alabama cadet recognized Cohen. Guard spokesperson Staff Sergeant Katrina Timmons stated on March 16, 2009 about the incident, "It's an embarrassment to the Alabama National Guard. Since then we have put in protocols to make sure this doesn't happen again."


</doc>
<doc id="62891155" url="https://en.wikipedia.org/wiki?curid=62891155" title="Separation of military and police roles">
Separation of military and police roles

The separation of military and police roles is the principle by which the military and law enforcement perform clearly differentiated duties and do not interfere with each other's areas of discipline. Whereas the military's purpose is to fight wars, law enforcement is meant to enforce domestic law. Neither is trained specifically to do the other's job. Military and law enforcement differ, sometimes fundamentally, in areas such as source of authority, training in use of force, training in investigation and prosecution, and training in enforcing laws and ensuring civil liberties.

Even "gray area" threats like drug trafficking, organized crime or terrorism will require sophisticated investigative skills and adherence to procedures for building a case as well as close collaboration with prosecutorial and judicial authorities. These are arguably traits solely attributed to law enforcement officers. Military personnel, on the other hand, are trained to defend the national territory from foreign military threats and are equipped with weapons designed to kill the enemy, rather than to stun or disable. The presence of a heavily-armed military standing in for the law enforcement personnel may reassure anxious civilians, but is at best partial and short-term.

In the United States, the 1878 Posse Comitatus Act place limits (but does not absolutely forbid) on the federal government in dispatching federal military personnel and federalized National Guard forces to enforce domestic policies.

There are instances of the military being called into action (such as during national emergencies caused by natural disasters or civil disorders) and efficiently saving lives and restoring order, such as the 1992 Los Angeles riots and Hurricane Katrina. On the other hand, there have also been instances where the use of military in a domestic role has gone wrong, such as the Kent State shootings.

In the 1980s, Congress began discussing a broader military role, but Ronald Reagan's Secretary of Defense, Frank Carlucci, testified that "I remain absolutely opposed to the assignment of a law enforcement mission to the Department of Defense. I am even more firmly opposed to any relaxation of the Posse Comitatus restrictions on the use of the military to search, seize and arrest. I have discussed this matter with the President and other senior members of his Cabinet, and I can report that these views are shared throughout this Administration."



</doc>
<doc id="11697735" url="https://en.wikipedia.org/wiki?curid=11697735" title="Civil Guard">
Civil Guard

Civil Guard refers to various policing organisations:


Historic Civil Guards now abolished:



</doc>
<doc id="63419599" url="https://en.wikipedia.org/wiki?curid=63419599" title="Special Troops Training Center">
Special Troops Training Center

The Special Troops Training Center (CFTS) is a specialized military school of the Algerian land forces.

The Special Troops Training Center (CFTS), is located in the town of Biskra, 405 km south-east of Algiers.

Originally the CFTS was created under the supervision of the Special Troops Superior School (ESTS), however it became completely autonomous from January 1994.

The ESTS is in charge of training officers and non-commissioned officers, while the CFTS is responsible for training men of the rank of paratroopers of the Algerian army.

Also nearby are the 1st combat helicopter regiment, the 32nd Algerian Air Force Transport Squadron and the 12th parachute commando regiment (12th RPC).

The FSTC offers training courses in order to obtain the following certificates :


First phase: basic training (common core) of 2 months duration, the second phase: 4-month specialty training course


First phase: basic training (core curriculum) of 2 months. the second phase: specialization training taking place in three 3 months. This phase completes the first one. It is devoted to the aptitudes enabling the djoundi (soldier) to accomplish the elementary acts of his speciality.

This school thus trains the future soldiers and corporals of the paratroopers commandos of the Algerian army.

The school has at its disposal:

Classrooms equipped with pedagogical means

Instructional sketches and sectional material of various rifles, ammunition, grenades and instructional ammunition

Various courses (combat, obstacle course, nautical, stress, psychological...)

Several sports halls and multisports halls

Replica of an aircraft for parachute jumping training

Several exercise fields for practical combat training...

Several firing ranges

Transport vehicles

Parachute jump simulator


</doc>
<doc id="26530571" url="https://en.wikipedia.org/wiki?curid=26530571" title="Mental operations">
Mental operations

Mental operations are operations that affect mental contents. Initially, operations of reasoning have been the object of logic alone. Pierre Janet was one of the first to use the concept in psychology. Mental operations have been investigated at a developmental level by Jean Piaget, and from a psychometric perspective by J. P. Guilford. There is also a cognitive approach to the subject, as well as a systems view of it.

Since Antiquity, mental operations, more precisely, formal operations of reasoning have been the object of logic.

In 1903, Pierre Janet described two types of mental operations:
Jean Piaget differentiated a preoperational stage, and operational stages of cognitive development, on the basis of presence of mental operations as an adaptation tool.

J. P. Guilford's Structure of Intellect model described up to 180 different intellectual abilities organized along three dimensions—Operations, Content, and Products.

According to 
most logicians, the three primary mental operations are apprehension (understanding), judgement, and inference.

Apprehension is the mental operation by which an idea is formed in the mind. If you were to think of a sunset or a baseball, the action of forming that picture in your mind is apprehension. The verbal expression of apprehension is called a term.

Judgment is the mental operation by which we predicate something of a subject. Were you to think, "That sunset is beautiful" or "Baseball is the all-American sport" is to make a judgment. The verbal expression of judgment is the statement (or proposition).

Inference (or reasoning) is the mental operation by which we draw conclusions from other information. If you were to think, "I like to look at that sunset, because I enjoy beautiful things, and that sunset is beautiful" you would be reasoning. The verbal expression of reasoning is the logical argument.

Jean Piaget identifies several mental operations of the concrete operational stage of cognitive development:


Piaget also describes a formal operational stage, with formal operations of abstract thinking: hypothesizing, hypothesis testing, and deduction.

According to J. P. Guilford's Structure of Intellect (SI) theory, an individual's performance on intelligence tests can be traced back to the underlying mental abilities or factors of intelligence. SI theory comprises multiple intellectual abilities organized along three dimensions—Operations, Content, and Products.


SI includes six operations or general intellectual processes:
Cognition—The ability to understand, comprehend, discover, and become aware of information.
Memory recording—The ability to encode information.
Memory retention—The ability to recall information.
Divergent production—The ability to generate multiple solutions to a problem; creativity.
Convergent production—The ability to deduce a single solution to a problem; rule-following or problem-solving.
Evaluation—The ability to judge whether or not information is accurate, consistent, or valid.


SI includes five broad areas of information to which the human intellect applies the six operations:
Visual—Information perceived through seeing.
Auditory—Information perceived through hearing.
Kinesthetic -through actions
Symbolic—Information perceived as symbols or signs that have no meaning by themselves; e.g., Arabic numerals or the letters of an alphabet.
Semantic—Information perceived in words or sentences, whether oral, written, or silently in one's mind.
Behavioral—Information perceived as acts of people.


As the name suggests, this dimension contains results of applying particular operations to specific contents. The SI model includes six products, in increasing complexity:
Units—Single items of knowledge.
Classes—Sets of units sharing common attributes.
Relations—Units linked as opposites or in associations, sequences, or analogies.
Systems—Multiple relations interrelated to comprise structures or networks.
Transformations—Changes, perspectives, conversions, or mutations to knowledge.
Implications—Predictions, inferences, consequences, or anticipations of knowledge.

Therefore, according to Guilford there are 6 x 5 x 6 = 180 intellectual abilities or factors. Each ability stands for a particular operation in a particular content area and results in a specific product, such as Comprehension of Figural Units or Evaluation of Semantic Implications.

Following on the footsteps of Silvio Ceccato, Giulio Benedetti describes several types of mental operations:

Taking into account all mental processes, the following types of mental operations have been described:



</doc>
<doc id="1740474" url="https://en.wikipedia.org/wiki?curid=1740474" title="Primacy of mind">
Primacy of mind

A belief in the primacy of mind is . In this view the mind or soul is not only primary as an explanation of reality, but is the only conceivable explanation, as nothing so subtle and sublime as reason and morality could possibly emerge from matter and motion, the primary elements of scientific explanation. In his book "Darwin's Dangerous Idea", Daniel Dennett states that a central aspect of Judeo-Christian and Islamic cosmogony is that, in the beginning, there was "something with Mind—'a cogitative Being,'".



</doc>
<doc id="28297" url="https://en.wikipedia.org/wiki?curid=28297" title="Soul">
Soul

The soul, in many religious, philosophical, and mythological traditions, is the incorporeal essence of a living being. Soul or psyche (Ancient Greek: ψυχή "psykhḗ", of ψύχειν "psýkhein", "to breathe") comprises the mental abilities of a living being: reason, character, feeling, consciousness, memory, perception, thinking, etc. Depending on the philosophical system, a soul can either be mortal or immortal.

Greek philosophers, such as Socrates, Plato, and Aristotle, understood that the soul (ψυχή "psūchê") must have a logical faculty, the exercise of which was the most divine of human actions. At his defense trial, Socrates even summarized his teaching as nothing other than an exhortation for his fellow Athenians to excel in matters of the psyche since all bodily goods are dependent on such excellence ("Apology" 30a–b).

In Judaism and in Christianity, only human beings have immortal souls (although immortality is disputed within Judaism and the concept of immortality may have been influenced by Plato). For example, the Catholic theologian Thomas Aquinas attributed "soul" ("anima") to all organisms but argued that only human souls are immortal. Other religions (most notably Hinduism and Jainism) hold that all living things from the smallest bacterium to the largest of mammals are the souls themselves (Atman, jiva) and have their physical representative (the body) in the world. The actual self is the soul, while the body is only a mechanism to experience the karma of that life. Thus if we see a tiger then there is a self-conscious identity residing in it (the soul), and a physical representative (the whole body of the tiger, which is observable) in the world. Some teach that even non-biological entities (such as rivers and mountains) possess souls. This belief is called animism.

The Modern English word "soul", derived from Old English "sáwol, sáwel", was first attested in the 8th century poem "Beowulf" v. 2820 and in the Vespasian Psalter 77.50 . It is cognate with other German and Baltic terms for the same idea, including Gothic "saiwala", Old High German "sêula, sêla", Old Saxon "sêola", Old Low Franconian "sêla, sîla", Old Norse "sála" and Lithuanian "siela". Deeper etymology of the Germanic word is unclear.

The original concept behind the Germanic root is thought to mean “"coming from or belonging to the sea" (or "lake")”, because of the Germanic and pre-Celtic belief in souls emerging from and returning to sacred lakes, Old Saxon "sêola" (soul) compared to Old Saxon "sêo" (sea).

The Koine Greek Septuagint uses ("psyche") to translate Hebrew ("nephesh"), meaning "life, vital breath", and specifically refers to a mortal, physical life, but in English it is variously translated as "soul, self, life, creature, person, appetite, mind, living being, desire, emotion, passion"; an example can be found in :

The Koine Greek word ("psychē"), "life, spirit, consciousness", is derived from a verb meaning "to cool, to blow", and hence refers to the breath, as opposed to ("soma"), meaning "body". "Psychē" occurs juxtaposed to , as seen in :

Paul the Apostle used ψυχή ("psychē") and ("pneuma") specifically to distinguish between the Jewish notions of ("nephesh") and "ruah" (spirit) (also in the Septuagint, e.g. = = "" = "the Spirit of God").

In the ancient Egyptian religion, an individual was believed to be made up of various elements, some physical and some spiritual. Similar ideas are found in ancient Assyrian and Babylonian religion. Kuttamuwa, an 8th-century BCE royal official from Sam'al, ordered an inscribed stele erected upon his death. The inscription requested that his mourners commemorate his life and his afterlife with feasts "for my soul that is in this stele". It is one of the earliest references to a soul as a separate entity from the body. The basalt stele is tall and wide. It was uncovered in the third season of excavations by the Neubauer Expedition of the Oriental Institute in Chicago, Illinois.

The Bahá'í Faith affirms that "the soul is a sign of God, a heavenly gem whose reality the most learned of men hath failed to grasp, and whose mystery no mind, however acute, can ever hope to unravel". Bahá'u'lláh stated that the soul not only continues to live after the physical death of the human body, but is, in fact, immortal. Heaven can be seen partly as the soul's state of nearness to God; and hell as a state of remoteness from God. Each state follows as a natural consequence of individual efforts, or the lack thereof, to develop spiritually. Bahá'u'lláh taught that individuals have no existence prior to their life here on earth and the soul's evolution is always towards God and away from the material world.

Buddhism teaches the principle of impermanence, that all things are in a constant state of flux: all is changing, and no permanent state exists by itself. This applies to human beings as much as to anything else in the cosmos. Thus, a human being has no permanent self. According to this doctrine of "anatta" (Pāli; Sanskrit: "anātman") – "no-self" or "no soul" – the words "I" or "me" do not refer to any fixed thing. They are simply convenient terms that allow us to refer to an ever-changing entity.

The "anatta" doctrine is not a kind of materialism. Buddhism does not deny the existence of "immaterial" entities, and it (at least traditionally) distinguishes bodily states from mental states. Thus, the conventional translation of "anatta" as "no-soul" can be confusing. If the word "soul" simply refers to an incorporeal component in living things that can continue after death, then Buddhism does not deny the existence of the soul. Instead, Buddhism denies the existence of a permanent entity that remains constant behind the changing corporeal and incorporeal components of a living being. Just as the body changes from moment to moment, so thoughts come and go, and there is no permanent state underlying the mind that experiences these thoughts, as in Cartesianism. Conscious mental states simply arise and perish with no "thinker" behind them. When the body dies, Buddhists believe the incorporeal mental processes continue and are reborn in a new body. Because the mental processes are constantly changing, the being that is reborn is neither entirely different from, nor exactly the same as, the being that died. However, the new being is "continuous" with the being that died – in the same way that the "you" of this moment is continuous with the "you" of a moment before, despite the fact that you are constantly changing.

Buddhist teaching holds that a notion of a permanent, abiding self is a delusion that is one of the causes of human conflict on the emotional, social, and political levels. They add that an understanding of "anatta" provides an accurate description of the human condition, and that this understanding allows us to pacify our mundane desires.

Various schools of Buddhism have differing ideas about what continues after death. The Yogacara school in Mahayana Buddhism said there are Store consciousness which continue to exist after death. In some schools, particularly Tibetan Buddhism, the view is that there are three minds: "very subtle mind", which does not disintegrate in death; "subtle mind", which disintegrates in death and which is "dreaming mind" or "unconscious mind"; and "gross mind", which does not exist when one is "sleeping". Therefore, "gross mind" is less permanent than subtle mind, which does not exist in death. "Very subtle mind", however, does continue, and when it "catches on", or coincides with phenomena, again, a new "subtle mind" emerges, with its own personality/assumptions/habits, and "that" entity experiences karma in the current continuum.

Plants were said to be non-sentient (無情), but Buddhist monks are required to not cut or burn trees, because some sentient beings rely on them. Some Mahayana monks said non-sentient beings such as plants and stones have Buddha-nature.

Certain modern Buddhists, particularly in Western countries, reject—or at least take an agnostic stance toward—the concept of rebirth or reincarnation. Stephen Batchelor discusses this in his book "Buddhism Without Beliefs". Others point to research that has been conducted at the University of Virginia as proof that some people are reborn.

According to a common Christian eschatology, when people die, their souls will be judged by God and determined to go to Heaven or to Hell. Other Christians understand the soul as the life, and believe that the dead are sleeping (Christian conditionalism). This belief is traditionally accompanied by the belief that the unrighteous soul will cease to exist instead of suffering eternally (annihilationism). Believers will inherit eternal life either in Heaven, or in a Kingdom of God on earth, and enjoy eternal fellowship with God.

Although all major branches of Christianity – Catholics, Eastern Orthodox, Oriental Orthodox, Church of the East, Evangelical, and mainline Protestants – teach that Jesus Christ plays a decisive role in the Christian salvation process, the specifics of that role and the part played by individual persons or by ecclesiastical rituals and relationships, is a matter of wide diversity in official church teaching, theological speculation and popular practice. Some Christians believe that if one has not repented of one's sins and has not trusted in Jesus Christ as Lord and Savior, one will go to Hell and suffer eternal damnation or eternal separation from God. Some hold a belief that babies (including the unborn) and those with cognitive or mental impairments who have died will be received into Heaven on the basis of God's grace through the sacrifice of Jesus.

There are also beliefs in universal salvation.

The "origin of the soul" has provided a vexing question in Christianity. The major theories put forward include soul creationism, traducianism, and pre-existence. According to soul creationism, God creates each individual soul created directly, either at the moment of conception or some later time. According to traducianism, the soul comes from the parents by natural generation. According to the preexistence theory, the soul exists before the moment of conception. There have been differing thoughts regarding whether human embryos have souls from conception, or whether there is a point between conception and birth where the fetus acquires a soul, consciousness, and/or personhood. Stances in this question might play a role in judgements on the morality of abortion.

Augustine (354-430), one of western Christianity's most influential early Christian thinkers, described the soul as "a special substance, endowed with reason, adapted to rule the body". Some Christians espouse a trichotomic view of humans, which characterizes humans as consisting of a body ("soma"), soul ("psyche"), and spirit ("pneuma"). However, the majority of modern Bible scholars point out how the concepts of "spirit" and of "soul" are used interchangeably in many biblical passages, and so hold to dichotomy: the view that each human comprises a body and a soul. Paul said that the "body wars against" the soul, "For the word of God is living and active and sharper than any two-edged sword, and piercing as far as the division of soul and spirit" (Heb 4:12 NASB), and that "I buffet my body", to keep it under control.

The present Catechism of the Catholic Church defines the soul as "the innermost aspect of humans, that which is of greatest value in them, that by which they are in God's image described as 'soul' signifies the "spiritual principle" in man". All souls living and dead will be judged by Jesus Christ when he comes back to earth. The Catholic Church teaches that the existence of each individual soul is dependent wholly upon God: "The doctrine of the faith affirms that the spiritual and immortal soul is created immediately by God."

Protestants generally believe in the soul's existence, but fall into two major camps about what this means in terms of an afterlife. Some, following Calvin, believe in the immortality of the soul and conscious existence after death, while others, following Luther, believe in the mortality of the soul and unconscious "sleep" until the resurrection of the dead. Various new religious movements deriving from Adventism—including Christadelphians, Seventh-day Adventists and Jehovah's Witnesses—similarly believe that the dead do not possess a soul separate from the body and are unconscious until the resurrection.

The Church of Jesus Christ of Latter-day Saints teaches that the spirit and body together constitute the Soul of Man (Mankind). "The spirit and the body are the soul of man." Latter-day Saints believe that the soul is the union of a pre-existing, God-made spirit and a temporal body, which is formed by physical conception on earth. After death, the spirit continues to live and progress in the Spirit world until the resurrection, when it is reunited with the body that once housed it. This reuniting of body and spirit results in a perfect soul that is immortal and eternal and capable of receiving a fulness of joy. Latter-day Saint cosmology also describes "intelligences" as the essence of consciousness or agency. These are co-eternal with God, and animate the spirits. The union of a newly-created spirit body with an eternally-existing intelligence constitutes a "spirit birth" and justifies God's title "Father of our spirits".

Some Confucian traditions contrast a spiritual soul with a corporeal soul.

"Ātman" is a Sanskrit word that means inner self or soul. In Hindu philosophy, especially in the Vedanta school of Hinduism, Ātman is the first principle, the "true" self of an individual beyond identification with phenomena, the essence of an individual. In order to attain liberation (moksha), a human being must acquire self-knowledge (atma jnana), which is to realize that one's true self (Ātman) is identical with the transcendent self Brahman.

The six orthodox schools of Hinduism believe that there is Ātman (self, essence) in every being.

In Hinduism and Jainism, a "jiva" (, , alternative spelling "jiwa"; , , alternative spelling "jeev") is a living being, or any entity imbued with a life force.

In Jainism, "jiva" is the immortal essence or soul of a living organism (human, animal, fish or plant etc.) which survives physical death. The concept of "Ajiva" in Jainism means "not soul", and represents matter (including body), time, space, non-motion and motion. In Jainism, a "Jiva" is either "samsari" (mundane, caught in cycle of rebirths) or "mukta" (liberated).

The concept of "jiva" in Jainism is similar to "atman" in Hinduism. However, some Hindu traditions differentiate between the two concepts, with "jiva" considered as individual self, while atman as that which is universal unchanging self that is present in all living beings and everything else as the metaphysical Brahman. The latter is sometimes referred to as "jiva-atman" (a soul in a living body).
According to Brahma Kumaris, the soul is an eternal point of light.

The Quran, the holy book of Islam, distinguishes between the immortal "Rūḥ" (translated as spirit, consciousness, pneuma or "soul") and the mortal "Nafs" (translated as self, ego, psyche or "soul"). The immortal Rūḥ "drives" the mortal Nafs, which comprises temporal desires and perceptions necessary for living.
One of the passages in the Quran that mention Rûh occur in chapter 17 ("The Night Journey"),and in Chapter 39 ("The Troops"):

In Jainism, every living being, from plant or bacterium to human, has a soul and the concept forms the very basis of Jainism. According to Jainism, there is no beginning or end to the existence of soul. It is eternal in nature and changes its form until it attains liberation.

The soul "(Jīva)" is basically categorized in one of two ways based on its present state.

Until the time the soul is liberated from the "saṃsāra" (cycle of repeated birth and death), it gets attached to one of these bodies based on the karma (actions) of the individual soul. Irrespective of which state the soul is in, it has got the same attributes and qualities. The difference between the liberated and non-liberated souls is that the qualities and attributes are manifested completely in case of "siddha" (liberated soul) as they have overcome all the karmic bondages whereas in case of non-liberated souls they are partially exhibited. Souls who rise victorious over wicked emotions while still remaining within physical bodies are referred to as arihants.

Concerning the Jain view of the soul, Virchand Gandhi said

The Hebrew terms "nefesh" (literally "living being"), "ruach" (literally "wind"), "neshamah" (literally "breath"), "chayah" (literally "life") and "yechidah" (literally "singularity") are used to describe the soul or spirit.

In Judaism the soul was believed to be given by God to Adam as mentioned in Genesis, 

Judaism relates the quality of one's soul to one's performance of the commandments ("mitzvot)" and reaching higher levels of understanding, and thus closeness to God. A person with such closeness is called a "tzadik". Therefore, Judaism embraces the commemoration of the day of one's death, "nahala"/"Yahrtzeit" and not the birthday as a festivity of remembrance, for only toward the end of life's struggles, tests and challenges could human souls be judged and credited for righteousness. Judaism places great importance on the study of the souls.

Kabbalah and other mystic traditions go into greater detail into the nature of the soul. Kabbalah separates the soul into five elements, corresponding to the five worlds:


Kabbalah also proposed a concept of reincarnation, the "gilgul". (See also "nefesh habehamit" the "animal soul".)

The Scientology view is that a person does not have a soul, it is a soul. A person is immortal, and may be reincarnated if they wish. The Scientology term for the soul is "thetan", derived from the Greek word "theta", symbolizing thought. Scientology counselling (called auditing) addresses the soul to improve abilities, both worldly and spiritual.

The belief in soul dualism found throughout most Austronesian shamanistic traditions. The reconstructed Proto-Austronesian word for the "body soul" is "*nawa" ("breath", "life", or "vital spirit"). It is located somewhere in the abdominal cavity, often in the liver or the heart (Proto-Austronesian "*qaCay"). The "free soul" is located in the head. Its names are usually derived from Proto-Austronesian "*qaNiCu" ("ghost", "spirit [of the dead]"), which also apply to other non-human nature spirits. The "free soul" is also referred to in names that literally mean "twin" or "double", from Proto-Austronesian "*duSa" ("two"). A virtuous person is said to be one whose souls are in harmony with each other, while an evil person is one whose souls are in conflict.

The "free soul" is said to leave the body and journey to the spirit world during sleep, trance-like states, delirium, insanity, and death. The duality is also seen in the healing traditions of Austronesian shamans, where illnesses are regarded as a "soul loss" and thus to heal the sick, one must "return" the "free soul" (which may have been stolen by an evil spirit or got lost in the spirit world) into the body. If the "free soul" can not be returned, the afflicted person dies or goes permanently insane.

In some ethnic groups, there can also be more than two souls. Like among the Tagbanwa people, where a person is said to have six souls - the "free soul" (which is regarded as the "true" soul) and five secondary souls with various functions.

Kalbo Inuit groups believe that a person has more than one type of soul. One is associated with respiration, the other can accompany the body as a shadow. In some cases, it is connected to shamanistic beliefs among the various Inuit groups. Also Caribou Inuit groups believed in several types of souls.

The shaman heals within the spiritual dimension by returning 'lost' parts of the human soul from wherever they have gone. The shaman also cleanses excess negative energies, which confuse or pollute the soul.

Sikhism considers soul ("atma") to be part of God (Waheguru). Various hymns are cited from the holy book Guru Granth Sahib (SGGS) that suggests this belief. "God is in the Soul and the Soul is in the God." The same concept is repeated at various pages of the SGGS. For example: "The soul is divine; divine is the soul. Worship Him with love." and "The soul is the Lord, and the Lord is the soul; contemplating the Shabad, the Lord is found."

The "atma" or soul according to Sikhism is an entity or "spiritual spark" or "light" in our body because of which the body can sustain life. On the departure of this entity from the body, the body becomes lifeless – No amount of manipulations to the body can make the person make any physical actions. The soul is the 'driver' in the body. It is the "roohu" or spirit or "atma", the presence of which makes the physical body alive.

Many religious and philosophical traditions support the view that the soul is the ethereal substance – a spirit; a non-material spark – particular to a unique living being. Such traditions often consider the soul both immortal and innately aware of its immortal nature, as well as the true basis for sentience in each living being. The concept of the soul has strong links with notions of an afterlife, but opinions may vary wildly even within a given religion as to what happens to the soul after death. Many within these religions and philosophies see the soul as immaterial, while others consider it possibly material.

According to Chinese traditions, every person has two types of soul called hun and po (魂 and 魄), which are respectively yang and yin. Taoism believes in ten souls, "sanhunqipo" () "three "hun" and seven "po"". A living being that loses any of them is said to have mental illness or unconsciousness, while a dead soul may reincarnate to a disability, lower desire realms, or may even be unable to reincarnate.

In theological reference to the soul, the terms "life" and "death" are viewed as emphatically more definitive than the common concepts of "biological life" and "biological death". Because the soul is said to be transcendent of the "material existence," and is said to have (potentially) eternal life, the death of the soul is likewise said to be an "eternal death". Thus, in the concept of divine judgment, God is commonly said to have options with regard to the dispensation of souls, ranging from Heaven (i.e., angels) to hell (i.e., demons), with various concepts in between. Typically both Heaven and hell are said to be eternal, or at least far beyond a typical human concept of lifespan and time.

According to Louis Ginzberg, soul of Adam is the image of God. Every soul of human also escapes from the body every night, rises up to heaven, and fetches new life thence for the body of man.

In Dada Bhagwan, The Soul is an independent eternal element. The Soul is permanent. In order to experience the Soul you need to attain Self-Realization.

In Brahma Kumaris, human souls are believed to be incorporeal and eternal. God is considered to be the Supreme Soul, with maximum degrees of spiritual qualities, such as peace, love and purity.

In Helena Blavatsky's Theosophy, the soul is the field of our psychological activity (thinking, emotions, memory, desires, will, and so on) as well as of the so-called paranormal or psychic phenomena (extrasensory perception, out-of-body experiences, etc.). However, the soul is not the highest, but a middle dimension of human beings. Higher than the soul is the spirit, which is considered to be the real self; the source of everything we call "good"—happiness, wisdom, love, compassion, harmony, peace, etc. While the spirit is eternal and incorruptible, the soul is not. The soul acts as a link between the material body and the spiritual self, and therefore shares some characteristics of both. The soul can be attracted either towards the spiritual or towards the material realm, being thus the "battlefield" of good and evil. It is only when the soul is attracted towards the spiritual and merges with the Self that it becomes eternal and divine.

Rudolf Steiner claimed classical trichotomic stages of soul development, which interpenetrated one another in consciousness:

In Surat Shabda Yoga, the soul is considered to be an exact replica and spark of the Divine. The purpose of Surat Shabd Yoga is to realize one's True Self as soul (Self-Realisation), True Essence (Spirit-Realisation) and True Divinity (God-Realisation) while living in the physical body.

Similarly, the spiritual teacher Meher Baba held that "Atma, or the soul, is in reality identical with Paramatma the Oversoul – which is one, infinite, and eternal...[and] [t]he sole purpose of creation is for the soul to enjoy the infinite state of the Oversoul consciously."

Eckankar, founded by Paul Twitchell in 1965, defines Soul as the true self; the inner, most sacred part of each person.

The ancient Greeks used the word "ensouled" to represent the concept of being "alive", indicating that the earliest surviving western philosophical view believed that the soul was that which gave the body life. The soul was considered the incorporeal or spiritual "breath" that animates (from the Latin, "anima", cf. "animal") the living organism.

Francis M. Cornford quotes Pindar by saying that the soul sleeps while the limbs are active, but when one is sleeping, the soul is active and reveals "an award of joy or sorrow drawing near" in dreams.

Erwin Rohde writes that an early pre-Pythagorean belief presented the soul as lifeless when it departed the body, and that it retired into Hades with no hope of returning to a body.

Drawing on the words of his teacher Socrates, Plato considered the psyche to be the essence of a person, being that which decides how we behave. He considered this essence to be an incorporeal, eternal occupant of our being. Plato said that even after death, the soul exists and is able to think. He believed that as bodies die, the soul is continually reborn (metempsychosis) in subsequent bodies. However, Aristotle believed that only one part of the soul was immortal namely the intellect ("logos"). The Platonic soul consists of three parts:

The parts are located in different regions of the body:

Plato also compares the three parts of the soul or psyche to a societal caste system. According to Plato's theory, the three-part soul is essentially the same thing as a state's class system because, to function well, each part must contribute so that the whole functions well. Logos keeps the other functions of the soul regulated.

Aristotle (384–322 BCE) defined the soul, or "Psūchê" (ψυχή), as the "first actuality" of a naturally organized body, and argued against its separate existence from the physical body. In Aristotle's view, the primary activity, or full actualization, of a living thing constitutes its soul. For example, the full actualization of an eye, as an independent organism, is to see (its purpose or final cause). Another example is that the full actualization of a human being would be living a fully functional human life in accordance with reason (which he considered to be a faculty unique to humanity). For Aristotle, the soul is the organization of the form and matter of a natural being which allows it to strive for its full actualization. This organization between form and matter is necessary for any activity, or functionality, to be possible in a natural being. Using an artifact (non-natural being) as an example, a house is a building for human habituation, but for a house to be actualized requires the material (wood, nails, bricks, etc.) necessary for its actuality (i.e. being a fully functional house). However, this does not imply that a house has a soul. In regards to artifacts, the source of motion that is required for their full actualization is outside of themselves (for example, a builder builds a house). In natural beings, this source of motion is contained within the being itself. Aristotle elaborates on this point when he addresses the faculties of the soul.

The various faculties of the soul, such as nutrition, movement (peculiar to animals), reason (peculiar to humans), sensation (special, common, and incidental) and so forth, when exercised, constitute the "second" actuality, or fulfillment, of the capacity to be alive. For example, someone who falls asleep, as opposed to someone who falls dead, can wake up and live their life, while the latter can no longer do so.

Aristotle identified three hierarchical levels of natural beings: plants, animals, and people, having three different degrees of soul: "Bios" (life), "Zoë" (animate life), and "Psuchë" (self-conscious life). For these groups, he identified three corresponding levels of soul, or biological activity: the nutritive activity of growth, sustenance and reproduction which all life shares ("Bios"); the self-willed motive activity and sensory faculties, which only animals and people have in common ("Zoë"); and finally "reason", of which people alone are capable ("Pseuchë").

Aristotle's discussion of the soul is in his work, "De Anima" ("On the Soul"). Although mostly seen as opposing Plato in regard to the immortality of the soul, a controversy can be found in relation to the fifth chapter of the third book: In this text both interpretations can be argued for, soul as a whole can be deemed mortal, and a part called "active intellect" or "active mind" is immortal and eternal. Advocates exist for both sides of the controversy, but it has been understood that there will be permanent disagreement about its final conclusions, as no other Aristotelian text contains this specific point, and this part of "De Anima" is obscure. Further, Aristotle states that the soul helps humans find the truth and understanding the true purpose or role of the soul is extremely difficult.

Following Aristotle, Avicenna (Ibn Sina) and Ibn al-Nafis, an Arab physician, further elaborated upon the Aristotelian understanding of the soul and developed their own theories on the soul. They both made a distinction between the soul and the spirit, and the Avicennian doctrine on the nature of the soul was influential among the Scholastics. Some of Avicenna's views on the soul include the idea that the immortality of the soul is a consequence of its nature, and not a purpose for it to fulfill. In his theory of "The Ten Intellects", he viewed the human soul as the tenth and final intellect.

While he was imprisoned, Avicenna wrote his famous "Floating Man" thought experiment to demonstrate human self-awareness and the substantial nature of the soul. He told his readers to imagine themselves suspended in the air, isolated from all sensations, which includes no sensory contact with even their own bodies. He argues that in this scenario one would still have self-consciousness. He thus concludes that the idea of the self is not logically dependent on any physical thing, and that the soul should not be seen in relative terms, but as a primary given, a substance. This argument was later refined and simplified by René Descartes in epistemic terms, when he stated: "I can abstract from the supposition of all external things, but not from the supposition of my own consciousness."

Avicenna generally supported Aristotle's idea of the soul originating from the heart, whereas Ibn al-Nafis rejected this idea and instead argued that the soul "is related to the entirety and not to one or a few organs". He further criticized Aristotle's idea whereby every unique soul requires the existence of a unique source, in this case the heart. al-Nafis concluded that "the soul is related primarily neither to the spirit nor to any organ, but rather to the entire matter whose temperament is prepared to receive that soul," and he defined the soul as nothing other than "what a human indicates by saying "I".

Following Aristotle (whom he referred to as "the Philosopher") and Avicenna, Thomas Aquinas (1225–74) understood the soul to be the first actuality of the living body. Consequent to this, he distinguished three orders of life: plants, which feed and grow; animals, which add sensation to the operations of plants; and humans, which add intellect to the operations of animals.

Concerning the human soul, his epistemological theory required that, since the knower becomes what he knows, the soul is definitely not corporeal—if it is corporeal when it knows what some corporeal thing is, that thing would come to be within it. Therefore, the soul has an operation which does not rely on a body organ, and therefore the soul can exist without a body. Furthermore, since the rational soul of human beings is a subsistent form and not something made of matter and form, it cannot be destroyed in any natural process. The full argument for the immortality of the soul and Aquinas' elaboration of Aristotelian theory is found in Question 75 of the First Part of the Summa Theologica.

In his discussions of rational psychology, Immanuel Kant (1724–1804) identified the soul as the "I" in the strictest sense, and argued that the existence of inner experience can neither be proved nor disproved.

Gilbert Ryle's ghost in the machine argument, which is a rejection of Descartes' mind–body dualism, can provide a contemporary understanding of the soul/mind, and the problem concerning its connection to the brain/body.

Psychologist James Hillman's archetypal psychology is an attempt to restore the concept of the soul, which Hillman viewed as the "self-sustaining and imagining substrate" upon which consciousness rests. Hillman described the soul as that "which makes meaning possible, [deepens] events into experiences, is communicated in love, and has a religious concern", as well as "a special relation with death". Departing from the Cartesian dualism "between outer tangible reality and inner states of mind", Hillman takes the Neoplatonic stance that there is a "third, middle position" in which soul resides. Archetypal psychology acknowledges this third position by attuning to, and often accepting, the archetypes, dreams, myths, and even psychopathologies through which, in Hillman's view, soul expresses itself.

The current scientific consensus across all fields is that there is no evidence for the existence of any kind of soul in the traditional sense. Many modern scientists, such as Julien Musolino, hold that the mind is merely a complex machine that operates on the same physical laws as all other objects in the universe. According to Musolino, there is currently no scientific evidence whatsoever to support the existence of the soul; he claims there is also considerable evidence that seems to indicate that souls do not exist.

The search for the soul, however, is seen to have been instrumental in driving the understanding of the anatomy and physiology of the human body, particularly in the fields of cardiovascular and neurology. In the two dominant conflicting concepts of the soul – one seeing it to be spiritual and immortal, and the other seeing it to be material and mortal, both have described the soul as being located in a particular organ or as pervading the whole body.

Neuroscience as an interdisciplinary field, and its branch of cognitive neuroscience particularly, operates under the ontological assumption of physicalism. In other words, it assumes—in order to perform its science—that only the fundamental phenomena studied by physics exist. Thus, neuroscience seeks to understand mental phenomena within the framework according to which human thought and behavior are caused solely by physical processes taking place inside the brain, and it operates by the way of reductionism by seeking an explanation for the mind in terms of brain activity.

To study the mind in terms of the brain several methods of functional neuroimaging are used to study the neuroanatomical correlates of various cognitive processes that constitute the mind. The evidence from brain imaging indicates that all processes of the mind have physical correlates in brain function. However, such correlational studies cannot determine whether neural activity plays a causal role in the occurrence of these cognitive processes (correlation does not imply causation) and they cannot determine if the neural activity is either necessary or sufficient for such processes to occur. Identification of causation, and of necessary and sufficient conditions requires explicit experimental manipulation of that activity. If manipulation of brain activity changes consciousness, then a causal role for that brain activity can be inferred. Two of the most common types of manipulation experiments are loss-of-function and gain-of-function experiments. In a loss-of-function (also called "necessity") experiment, a part of the nervous system is diminished or removed in an attempt to determine if it is necessary for a certain process to occur, and in a gain-of-function (also called "sufficiency") experiment, an aspect of the nervous system is increased relative to normal. Manipulations of brain activity can be performed with direct electrical brain stimulation, magnetic brain stimulation using transcranial magnetic stimulation, psychopharmacological manipulation, optogenetic manipulation, and by studying the symptoms of brain damage (case studies) and lesions. In addition, neuroscientists are also investigating how the mind develops with the development of the brain.

Physicist Sean M. Carroll has written that the idea of a soul is incompatible with quantum field theory (QFT). He writes that for a soul to exist: "Not only is new physics required, but dramatically new physics. Within QFT, there can't be a new collection of 'spirit particles' and 'spirit forces' that interact with our regular atoms, because we would have detected them in existing experiments."

Some theorists have invoked quantum indeterminism as an explanatory mechanism for possible soul/brain interaction, but neuroscientist Peter Clarke found errors with this viewpoint, noting there is no evidence that such processes play a role in brain function; Clarke concluded that a Cartesian soul has no basis from quantum physics.

Some parapsychologists have attempted to establish, by scientific experiment, whether a soul separate from the brain exists, as is more commonly defined in religion rather than as a synonym of psyche or mind. Milbourne Christopher (1979) and Mary Roach (2010) have argued that none of the attempts by parapsychologists have yet succeeded.

In 1901 Duncan MacDougall conducted an experiment in which he made weight measurements of patients as they died. He claimed that there was weight loss of varying amounts at the time of death; he concluded the soul weighed 21 grams, based on measurements of a single patient and discarding conflicting results. The physicist Robert L. Park has written that MacDougall's experiments "are not regarded today as having any scientific merit" and the psychologist Bruce Hood wrote that "because the weight loss was not reliable or replicable, his findings were unscientific."




</doc>
<doc id="1865924" url="https://en.wikipedia.org/wiki?curid=1865924" title="Geist">
Geist

Geist () is a German noun with a degree of importance in German philosophy. Its semantic field corresponds to English ghost, spirit, mind, intellect. Some English translators resort to using "spirit/mind" or "spirit (mind)" to help convey the meaning of the term. 

"Geist" is also a central concept in Georg Wilhelm Friedrich Hegel's 1807 "The Phenomenology of Spirit" ("Phänomenologie des Geistes"). Notable compounds, all associated with Hegel's view of world history of the late 18th century, include Weltgeist "world-spirit", Volksgeist "national spirit" and Zeitgeist "spirit of the age".

German "Geist" (masculine gender) continues Old High German "geist", attested as the translation of Latin "spiritus". 
It is the direct cognate of English "ghost", from a West Germanic "gaistaz". Its derivation from a PIE root "g̑heis-" "to be agitated, frightened" suggests that the Germanic word originally referred to frightening (c.f. English ghastly) apparitions or ghosts, and may also have carried the connotation of "ecstatic agitation, "furor"" related to the cult of Germanic Mercury. 
As the translation of biblical Latin "spiritus" (Greek πνεῦμα) "spirit, breath" the Germanic word acquires a Christian meaning from an early time, notably in reference to the Holy Spirit (Old English "sē hālga gāst" "the Holy Ghost", OHG " ther heilago geist", Modern German "der Heilige Geist").
The English word is in competition with Latinate "spirit" from the Middle English period, but its broader meaning is preserved well into the early modern period.

The German noun much like English "spirit" could refer to spooks or ghostly apparitions of the dead, to the religious concept, as in the Holy Spirit, as well as to the "spirit of wine", i.e. ethanol. 
However, its special meaning of "mind, intellect" never shared by English "ghost" is acquired only in the 18th century, under the influence of French "esprit".
In this sense it became extremely productive in the German language of the 18th century in general as well as in 18th-century German philosophy.
"Geist" could now refer to the quality of intellectual brilliance, to wit, innovation, erudition, etc.
It is also in this time that the adjectival distinction of "geistlich" "spiritual, pertaining to religion" vs. "geistig" "intellectual, pertaining to the mind" begins to be made. Reference to spooks or ghosts is made by the adjective "geisterhaft" "ghostly, spectral".

Numerous compounds are formed in the 18th to 19th centuries, some of them loan translations of French expressions, such as "Geistesgegenwart" = "présence d'esprit" ("mental presence, acuity"), "Geistesabwesenheit" = "absence d’esprit" ("mental absence, distraction"), "geisteskrank" "mentally ill", "geistreich" "witty, intellectually brilliant", "geistlos" "unintelligent, unimaginative, vacuous" etc. 
It is from these developments that certain German compounds containing "-geist" have been loaned into English, such as "Zeitgeist".

German "Geist" in this particular sense of "mind, wit, erudition; intangible essence, spirit" has no precise English-language equivalent, for which reason translators sometimes retain "Geist" as a German loanword.

"Geist" is a central concept in Hegel's "The Phenomenology of Spirit" ("Phänomenologie des Geistes"). According to Hegel, the "Weltgeist" ("world spirit") is not an actual object or a transcendental, Godlike thing, but a means of philosophizing about history. "Weltgeist" is effected in history through the mediation of various "Volksgeister" ("national spirits"), the great men of history, such as Napoleon, are the "concrete universal".

This has led some to claim that Hegel favored the great man theory, although his philosophy of history, in particular concerning the role of the "universal state" ("Universalstaat", which means a universal "order" or "statute" rather than "state"), and of an "End of History" is much more complex.

For Hegel, the great hero is unwittingly utilized by "Geist" or "absolute spirit", by a "ruse of reason" as he puts it, and is irrelevant to history once his historic mission is accomplished; he is thus subjected to the teleological principle of history, a principle which allows Hegel to reread the history of philosophy as culminating in his philosophy of history.

"Weltgeist", the world spirit concept, designates an idealistic principle of world explanation, which can be found from the beginnings of philosophy up to more recent time. The concept of world spirit was already accepted by the idealistic schools of ancient Indian philosophy, whereby one explained objective reality as its product. (See metaphysical objectivism) In the early philosophy of Greek antiquity, Socrates, Plato and Aristotle all paid homage, amongst other things, to the concept of world spirit. Hegel later based his philosophy of history on it.

"Weltgeist" "world-spirit" is older than the 18th century, at first (16th century) in the sense of "secularism, impiety, irreligiosity" ("spiritus mundi"), in the 17th century also personalised in the sense of "man of the world", "mundane or secular person".
Also from the 17th century, "Weltgeist" acquired a philosophical or spiritual sense of "world-spirit" or "world-soul" ("anima mundi, spiritus universi") in the sense of Panentheism, a spiritual essence permeating all of nature, or the active principle animating the universe, including the physical sense, such as the attraction between magnet and iron or between Moon and tide.

This idea of "Weltgeist" in the sense of "anima mundi" became very influential in 18th-century German philosophy. In philosophical contexts, "der Geist" on its own could refer to this concept, as in Christian Thomasius, "Versuch vom Wesen des Geistes" (1709).
Belief in a "Weltgeist" as animating principle immanent to the universe became dominant in German thought due to the influence of Goethe, in the later part of the 18th century.

Already in the poetical language of Johann Ulrich von König (d. 1745), the "Weltgeist" 
appears as the active, masculine principle opposite the feminine principle of Nature.

"Weltgeist" in the sense of Goethe comes close to being a synonym of God and can be attributed agency and will. 
Herder, who tended to prefer the form "Weltengeist" (as it were "spirit of worlds"), pushes this to the point of composing prayers addressed to this world-spirit:
The term was notably embraced by Hegel and his followers in the early 19th century.
For the 19th century, the term as used by Hegel (1807) became prevalent, less in the sense of an animating principle of nature or the universe but as the invisible force advancing world history:

Hegel's description of Napoleon as "the world-soul on horseback" ("die Weltseele zu Pferde") became proverbial.
The phrase is a shortened paraphrase of Hegel's words in a letter written on 13 October 1806, the day before the Battle of Jena, to his friend Friedrich Immanuel Niethammer:
I saw the Emperor – this world-soul – riding out of the city on reconnaissance. It is indeed a wonderful sensation to see such an individual, who, concentrated here at a single point, astride a horse, reaches out over the world and masters it.

The letter was not published in Hegel's time, but the expression was attributed to Hegel anecdotally, appearing in print from 1859.
It is used without attribution by Meyer Kayserling in his "Sephardim" (1859:103), and is apparently not recognized as a reference to Hegel by the reviewer in "Göttingische gelehrte Anzeigen", who notes it disapprovingly, as one of Kayserling's "bad jokes" ("schlechte Witze").
The phrase become widely associated with Hegel later in the 19th century.

"Volksgeist" or "Nationalgeist" refers to a "spirit" of an individual people ("Volk"), its "national spirit" or "national character". The term "Nationalgeist" is used in the 1760s by Justus Möser and by Johann Gottfried Herder. The term "Nation" at this time is used in the sense of "natio" "nation, ethnic group, race", mostly replaced by the term "Volk" after 1800.
In the early 19th century, the term "Volksgeist" was used by Friedrich Carl von Savigny in order to express the "popular" sense of justice.
Savigniy explicitly referred to the concept of an "esprit des nations " used by Voltaire. and of the "esprit général" invoked by Montesquieu.

Hegel uses the term in his "Lectures on the Philosophy of History".
Based on the Hegelian use of the term, Wilhelm Wundt, Moritz Lazarus and Heymann Steinthal in the mid-19th-century established the field of "Völkerpsychologie" ("psychology of nations").

In Germany the concept of Volksgeist has developed and changed its meaning through eras and fields. The most important examples are: In the literary field, Schlegel and the Brothers Grimm. In the history of cultures, Herder. In the history of the State or political history, Hegel. In the field of law, Savigny and in the field of psychology Wundt. This means that the concept is ambiguous. Furthermore it is not limited to Romanticism as it is commonly known.
The concept of was also influential in American cultural anthropology. According to the historian of anthropology George W. Stocking, Jr., "… one may trace the later American anthropological idea of culture back through Bastian's Volkergedanken and the folk psychologist's Volksgeister to Wilhelm von Humboldt's Nationalcharakter -- and behind that, although not without a paradoxical and portentous residue of conceptual and ideological ambiguity, to the Herderian ideal of Volksgeist."

The compound "Zeitgeist" (;, "spirit of the age" or "spirit of the times") similarly to "Weltgeist" describes 
an invisible agent or force dominating the characteristics of a given epoch in world history.
The term is now mostly associated with Hegel, contrasting with Hegel's use of "Volksgeist" "national spirit" and "Weltgeist" "world-spirit", 
but its coinage and popularization precedes Hegel, and is mostly due to Herder and Goethe.
The term as used contemporarily may more pragmatically refer to a fashion or fad which prescribes what is acceptable or tasteful, e.g. in the field of architecture.

Hegel in "Phenomenology of the Spirit" (1807) uses both "Weltgeist" and "Volksgeist" but prefers the phrase "Geist der Zeiten" "spirit of the times" over the compound "Zeitgeist".

Hegel believed that culture and art reflected its time. Thus, he argued that it would be impossible to produce classical art in the modern world, as modernity is essentially a "free and ethical culture".

The term has also been used more widely in the sense of an intellectual or aesthetic fashion or fad.
For example, Charles Darwin's 1859 proposition that evolution occurs by natural selection has been cited as a case of the "zeitgeist" of the epoch, an idea "whose time had come", seeing that his contemporary, Alfred Russel Wallace, was outlining similar models during the same period. 
Similarly, intellectual fashions such as the emergence of logical positivism in the 1920s, leading to a focus on behaviorism and blank-slatism over the following decades, and later, during the 1950s to 1960s, the shift from behaviorism to post-modernism and critical theory can be argued to be an expression of the intellectual or academic "zeitgeist".
"Zeitgeist" in more recent usage has been used by Forsyth (2009) in reference to his "theory of leadership" and in other publications describing models of business or industry.
Malcolm Gladwell argued in his book "Outliers" that entrepreneurs who succeeded in the early stages of a nascent industry often share similar characteristics.





</doc>
<doc id="841441" url="https://en.wikipedia.org/wiki?curid=841441" title="Psychonautics">
Psychonautics

Psychonautics (from the Ancient Greek ' "soul, spirit, mind" and ' "sailor, navigator" – "a sailor of the soul") refers both to a methodology for describing and explaining the subjective effects of altered states of consciousness, especially an important subgroup called holotropic states, including those induced by meditation or mind-altering substances, and to a research cabal in which the researcher voluntarily immerses themselves into an altered mental state in order to explore the accompanying experiences.

The term has been applied diversely, to cover all activities by which altered states are induced and utilized for spiritual purposes or the exploration of the human condition, including shamanism, lamas of the Tibetan Buddhist tradition, sensory deprivation, and archaic/modern drug users who use entheogenic substances in order to gain deeper insights and spiritual experiences. A person who uses altered states for such exploration is known as a psychonaut.

The term "psychonautics" derives from the prior term "psychonaut", usually attributed to German author Ernst Jünger who used the term in describing Arthur Heffter in his 1970 essay on his own extensive drug experiences "Annäherungen: Drogen und Rausch" (literally: "Approaches: Drugs and Inebriation"). In this essay, Jünger draws many parallels between drug experience and physical exploration—for example, the danger of encountering hidden "reefs."

Peter J. Carroll made "Psychonaut" the title of a 1982 book on the experimental use of meditation, ritual and drugs in the experimental exploration of consciousness and of psychic phenomena, or "chaos magic". The term's first published use in a scholarly context is attributed to ethnobotanist Jonathan Ott, in 2001.

Clinical psychiatrist Jan Dirk Blom describes psychonautics as denoting "the exploration of the psyche by means of techniques such as lucid dreaming, brainwave entrainment, sensory deprivation, and the use of hallucinogenics or entheogens", and a psychonaut as one who "seeks to investigate their mind using intentionally induced altered states of consciousness" for spiritual, scientific, or research purposes.

Psychologist Dr. Elliot Cohen of Leeds Metropolitan University and the UK Institute of Psychosomanautics defines psychonautics as "the means to study and explore consciousness (including the unconscious) and altered states of consciousness; it rests on the realization that to study consciousness is to transform it." He associates it with a long tradition of historical cultures worldwide. Leeds Metropolitan University is currently the only university in the UK to offer a module in Psychonautics.

American Buddhist writer Robert Thurman depicts the Tibetan Buddhist master as a psychonaut, stating that "Tibetan lamas could be called psychonauts, since they journey across the frontiers of death into the in-between realm."

The aims and methods of psychonautics, when state-altering substances are involved, is commonly distinguished from recreational drug use by research sources. Psychonautics as a means of exploration need not involve drugs, and may take place in a religious context with an established history. Cohen considers psychonautics closer in association to wisdom traditions and other transpersonal and integral movements.

However, there is considerable overlap with modern drug use and due to its modern close association with psychedelics and other drugs, it is also studied in the context of drug abuse from a perspective of addiction, the drug abuse market and online psychology, and studies into existing and emerging drugs within toxicology.


These may be used in combination; for example, traditions such as shamanism may combine ritual, fasting, and hallucinogenic substances.

One of the best known psychonautical works is Aldous Huxley's "The Doors of Perception". In addition to Ernst Jünger, who coined the term, the American physician, neuroscientist, psychoanalyst, philosopher, writer and inventor John C. Lilly is another well-known psychonaut. Lilly was interested in the nature of consciousness and, amongst other techniques, he used isolation tanks in his research.

Philosophical- and Science-fiction author Philip K. Dick has also been described as a psychonaut for several of his works such as "The Three Stigmata of Palmer Eldritch". Another influential psychonaut is the psychologist and writer Timothy Leary. Leary is known for controversial talks and research on the subject; he wrote several books including "The Psychedelic Experience". Another widely known psychonaut is the American philosopher, ethnobotanist, lecturer, and author Terence McKenna. McKenna spoke and wrote about subjects including psychedelic drugs, plant-based entheogens, shamanism, metaphysics, alchemy, language, culture, technology, and the theoretical origins of human consciousness.



</doc>
<doc id="2075414" url="https://en.wikipedia.org/wiki?curid=2075414" title="Mental state">
Mental state

A mental state is a state of mind that an agent is in. Most simplistically, a mental state is a mental condition. It is a relation that connects the agent with a proposition. Several of these states are a combination of mental representations and propositional attitudes. There are several paradigmatic states of mind that an agent has: love, hate, pleasure and pain, and attitudes toward propositions such as: believing that, conceiving that, hoping and fearing that, etc.

Discussions about mental states can be found in many areas of study.

In cognitive psychology and the philosophy of mind, a mental state is a kind of hypothetical state that corresponds to thinking and feeling, and consists of a conglomeration of mental representations and propositional attitudes. Several theories in philosophy and psychology try to determine the relationship between the agent's mental state and a proposition.

Instead of looking into what a mental state is, in itself, clinical psychology and psychiatry determine a person's mental health through a mental status examination.

Mental states also include attitudes towards propositions, of which there are at least two—factive, non-factive, both of which entail the mental state of acquaintance. To be acquainted with a proposition is to understand its meaning and be able to entertain it. The proposition can be true or false, and acquaintance requires no specific attitude towards that truth or falsity. Factive attitudes include those mental states that are attached to the truth of the proposition—i.e. the proposition entails truth. Some factive mental states include "perceiving that", "remembering that", "regretting that", and (more controversially) "knowing that". Non-factive attitudes do not entail the truth of the propositions to which they are attached. That is, one can be in one of these mental states and the proposition can be false. An example of a non-factive attitude is believing—you can believe a false proposition and you can believe a true proposition. Since you have the possibility of both, such mental states do not entail truth, and therefore, are not factive. However, belief does entail an attitude of assent toward the presumed truth of the proposition (whether or not it's so), making it and other non-factive attitudes different than mere acquaintance.



</doc>
<doc id="27294113" url="https://en.wikipedia.org/wiki?curid=27294113" title="Oneironautics">
Oneironautics

Oneironautics [ə.neɪ.roʊ.nɔː.tɪks] refers to the ability to travel within a dream on a conscious basis. Such a traveler in a dream may be called an oneironaut.

A lucid dream is one in which the dreamer is aware of dreaming and may be able to exert some degree of control over the dream's characters, narrative or environment. Early references to the phenomenon are found in ancient Greek texts.

The idea of one person being able to consciously travel or interact within the dream of another person, known variously as "dream telepathy", "telepathic lucid dreaming" or "telepathic dreaming", has been explored in the realms of science and fantasy fiction; in recent works, such an interaction is often depicted as a computer-mediated psychotherapeutic action, as is the case in "The Cell", and "Paprika", as well as through the direct intervention of another sleeping person, as in "Inception", "Dreamscape" and "Waking Life". The concept is also included in the fantasy series "The Wheel of Time" as an ability "dreamwalkers" are able to use.

A trope in such works of fiction explores the ramifications of whether the sleeping protagonist should enter the sleeping brain of another as opposed to allowing another individual to enter one's own brain; the entering of another individual's brain often results in unpleasant surprises, depending upon the mental state of the host or the preparedness of the guest. Roger Zelazny's 1966 sci-fi novella "The Dream Master", which applies computer-mediated dream telepathy in a psychotherapeutic setting, focuses on the protagonist's growing struggle to keep his balance as he enters the brain of a fellow psychotherapist who is blind and subconsciously, destructively hungers for the visual stimuli upon which dreams largely depend.



</doc>
<doc id="2681589" url="https://en.wikipedia.org/wiki?curid=2681589" title="Noogenesis">
Noogenesis

Noogenesis is the emergence and evolution of intelligence.

Noo-, nous (, ), from the ancient Greek , is a term that currently encompasses the meanings: "mind, intelligence, intellect, reason; wisdom; insight, intuition, thought."

Noogenesis was first mentioned in the posthumously published in 1955 book "The Phenomenon of Man" by Pierre Teilhard de Chardin, an anthropologist and philosopher, in a few places:

The lack of any kind of definition of the term has led to a variety of interpretations reflected in the book, including "the contemporary period of evolution on Earth, signified by transformation of biosphere onto the sphere of intelligence—noosphere", "evolution run by human mind" etc.
The most widespread interpretation is thought to be "the emergence of mind, which follows geogenesis, biogenesis and anthropogenesis, forming a new sphere on Earth—noosphere".

In 2005 Alexey Eryomin in the monograph Noogenesis and Theory of Intellect proposed a new concept of noogenesis in understanding the evolution of intellectual systems, concepts of intellectual systems, information logistics, information speed, intellectual energy, intellectual potential, consolidated into a theory of the intellect which combines the biophysical parameters of intellectual energy—the amount of information, its acceleration (frequency, speed) and the distance it's being sent—into a formula.
According to the new concept—proposed hypothesis continue prognostic progressive evolution of the species "Homo sapiens", the analogy between the human brain with the enormous number of neural cells firing at the same time and a similarly functioning human society.
A new understanding of the term "noogenesis" as an evolution of the intellect was proposed by A. Eryomin. A hypothesis based on recapitulation theory links the evolution of the human brain to the development of human civilization. The parallel between the number of people living on Earth and the number of neurons becomes more and more obvious leading us to viewing global intelligence as an analogy for human brain.
All of the people living on this planet have undoubtedly inherited the amazing cultural treasures of the past, be it production, social and intellectual ones. We are genetically hardwired to be a sort of "live RAM" of the global intellectual system. Alexey Eryomin suggests that humanity is moving towards a unified self-contained informational and intellectual system. His research has shown the probability of Super Intellect realizing itself as Global Intelligence on Earth. We could get closer to understanding the most profound patterns and laws of the Universe if these kinds of research were given enough attention. Also, the resemblance between the individual human development and such of the whole human race has to be explored further if we are to face some of the threats of the future.

Therefore, generalizing and summarizing: 
The term "noogenesis" can be used in a variety of fields i.e. medicine, biophysics, semiotics, mathematics, information technology, psychology, theory of global evolution etc. thus making it a truly cross-disciplinary one. In astrobiology noogenesis concerns the origin of intelligent life and more specifically technological civilizations capable of communicating with humans and or traveling to Earth. The lack of evidence for the existence of such extraterrestrial life creates the Fermi paradox.

The emergence of the human mind is considered to be one of the five fundamental phenomenons of emergent evolution. 
To understand the mind, it is necessary to determine how human thinking differs from other thinking beings. Such differences include the ability to generate calculations, to combine dissimilar concepts, to use mental symbols, and to think abstractly.
The knowledge of the phenomenon of intelligent systems—the emergence of reason (noogenesis) boils down to:


Several published works which do not employ the term "noogenesis", however, address some patterns in the emergence and functioning of the human intelligence: working memory capacity ≥ 7, ability to predict, prognosis, hierarchical (6 layers neurons) system of information analysis, consciousness, memory, generated and consumed information properties etc. They also set the limits of several physiological aspects of human intelligence. Сonception of emergence of insight.

Historical evolutionary development and emergence of "H. sapiens" as species, include emergence of such concepts as anthropogenesis, phylogenesis, morphogenesis, cephalization, systemogenesis, cognition systems autonomy.

On the other hand, development of an individual's intellect deals with concepts of embryogenesis, ontogenesis, morphogenesis, neurogenesis, higher nervous function of I.P.Pavlov and his philosophy of mind.
Despite the fact that the morphofunctional maturity is usually reached by the age of 13, the definitive functioning of the brain structures is not complete until about 16–17 years of age.

The fields of Bioinformatics, genetic engineering, noopharmacology, cognitive load, brain stimulations, the efficient use of altered states of consciousness, use of non-human cognition, information technology (IT), artificial intelligence (AI) are all believed to be effective methods of intelligence advancement and may be the future of intelligence on earth and the galaxy.

In his 2019 book “The Syntellect Hypothesis: Five Paradigms of the Mind’s Evolution,” futurist and evolutionary cyberneticist Alex M. Vikoulov uses the term "noogenesis" as “The Origin of Mind”, the emergence and evolution of intelligence. The author refers to computation-based biological evolution as a process of Noogenesis. At the present day, the Noosphere is evolving towards the ‘Syntellect’ (Greek syn, with, together + intellect) which can be defined as the unified mind of civilization that integrates all individual natural and artificial minds through the mediation and accumulative effects of information networks. The Syntellect Hypothesis which is the main focus of his book, as its title suggests, refers to a phase transition of a complex intelligent system to self-awareness of a living, conscious superorganism when intellectual synergy of its components reaches threshold complexity to become one supermind. This metamorphosis is associated with emergence of higher-order self-awareness and dimensionality of a new consciousness structure. As a bio-species merging with its advanced technology, human-machine civilization is now on the verge of the 'Syntellect Emergence' – becoming one Global Mind – Gaia v.2.0.

The development of the human brain, perception, cognition, memory and neuroplasticity are unsolved problems in neuroscience. Several megaprojects are being carried out in: Blue Brain Project, Allen Brain Atlas, Human Connectome Project, Google Brain, - in attempt to better our understanding of the brain's functionality along with the intention to develop human cognitive performance in the future with artificial intelligence, informational, communication and cognitive technology. An International Brain Initiative currently integrated national-level brain research initiatives (American BRAIN Initiative, European Human Brain Project, China Brain Project, Japan Brain/MINDS, Canadian Brain Research Strategy, Australian Brain Alliance, Korea Brain Initiative) with goals support an interface between countries to enable synergistic interactions with interdisciplinary approaches arising from the latest research in neuroscience and brain-inspired artificial intelligence etc. According to the Russian National Strategy - fundamental scientific research should be aimed at creating universal artificial intelligence.


</doc>
<doc id="6880483" url="https://en.wikipedia.org/wiki?curid=6880483" title="Philosophy of mind">
Philosophy of mind

Philosophy of mind is a branch of philosophy that studies the ontology and nature of the mind and its relationship with the body. The mind–body problem is a paradigm issue in philosophy of mind, although other issues are addressed, such as the hard problem of consciousness, and the nature of particular mental states. Aspects of the mind that are studied include mental events, mental functions, mental properties, consciousness, the ontology of the mind, the nature of thought, and the relationship of the mind to the body.

Dualism and monism are the two central schools of thought on the mind–body problem, although nuanced views have arisen that do not fit one or the other category neatly.


Most modern philosophers of mind adopt either a reductive physicalist or non-reductive physicalist position, maintaining in their different ways that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, especially in the fields of sociobiology, computer science (specifically, artificial intelligence), evolutionary psychology and the various neurosciences. Reductive physicalists assert that all mental states and properties will eventually be explained by scientific accounts of physiological processes and states. Non-reductive physicalists argue that although the mind is not a separate substance, mental properties supervene on physical properties, or that the predicates and vocabulary used in mental descriptions and explanations are indispensable, and cannot be reduced to the language and lower-level explanations of physical science. Continued neuroscientific progress has helped to clarify some of these issues; however, they are far from being resolved. Modern philosophers of mind continue to ask how the subjective qualities and the intentionality of mental states and properties can be explained in naturalistic terms.

The mind–body problem concerns the explanation of the relationship that exists between minds, or mental processes, and bodily states or processes. The main aim of philosophers working in this area is to determine the nature of the mind and mental states/processes, and how—or even if—minds are affected by and can affect the body.

Our perceptual experiences depend on stimuli that arrive at our various sensory organs from the external world, and these stimuli cause changes in our mental states, ultimately causing us to feel a sensation, which may be pleasant or unpleasant. Someone's desire for a slice of pizza, for example, will tend to cause that person to move his or her body in a specific manner and in a specific direction to obtain what he or she wants. The question, then, is how it can be possible for conscious experiences to arise out of a lump of gray matter endowed with nothing but electrochemical properties.

A related problem is how someone's propositional attitudes (e.g. beliefs and desires) cause that individual's neurons to fire and muscles to contract. These comprise some of the puzzles that have confronted epistemologists and philosophers of mind from at least the time of René Descartes.

Dualism is a set of views about the relationship between mind and matter (or body). It begins with the claim that mental phenomena are, in some respects, non-physical. One of the earliest known formulations of mind–body dualism was expressed in the eastern Sankhya and Yoga schools of Hindu philosophy (c. 650 BCE), which divided the world into purusha (mind/spirit) and prakriti (material substance). Specifically, the Yoga Sutra of Patanjali presents an analytical approach to the nature of the mind.

In Western Philosophy, the earliest discussions of dualist ideas are in the writings of Plato who maintained that humans' "intelligence" (a faculty of the mind or soul) could not be identified with, or explained in terms of, their physical body. However, the best-known version of dualism is due to René Descartes (1641), and holds that the mind is a non-extended, non-physical substance, a "res cogitans". Descartes was the first to clearly identify the mind with consciousness and self-awareness, and to distinguish this from the brain, which was the seat of intelligence. He was therefore the first to formulate the mind–body problem in the form in which it still exists today.

The most frequently used argument in favor of dualism appeals to the common-sense intuition that conscious experience is distinct from inanimate matter. If asked what the mind is, the average person would usually respond by identifying it with their self, their personality, their soul, or another related entity. They would almost certainly deny that the mind simply is the brain, or vice versa, finding the idea that there is just one ontological entity at play to be too mechanistic, or simply unintelligible. Many modern philosophers of mind think that these intuitions are misleading and that we should use our critical faculties, along with empirical evidence from the sciences, to examine these assumptions to determine whether there is any real basis to them.

Another important argument in favor of dualism is that the mental and the physical seem to have quite different, and perhaps irreconcilable, properties. Mental events have a subjective quality, whereas physical events do not. So, for example, one can reasonably ask what a burnt finger feels like, or what a blue sky looks like, or what nice music sounds like to a person. But it is meaningless, or at least odd, to ask what a surge in the uptake of glutamate in the dorsolateral portion of the prefrontal cortex feels like.

Philosophers of mind call the subjective aspects of mental events "qualia" or "raw feels". There is something that it is like to feel pain, to see a familiar shade of blue, and so on. There are qualia involved in these mental events that seem particularly difficult to reduce to anything physical. David Chalmers explains this argument by stating that we could conceivably know all the objective information about something, such as the brain states and wavelengths of light involved with seeing the color red, but still not know something fundamental about the situation – what it is like to see the color red.

If consciousness (the mind) can exist independently of physical reality (the brain), one must explain how physical memories are created concerning consciousness. Dualism must therefore explain how consciousness affects physical reality. One possible explanation is that of a miracle, proposed by Arnold Geulincx and Nicolas Malebranche, where all mind–body interactions require the direct intervention of God.

Another possible argument that has been proposed by C. S. Lewis is the Argument from Reason: if, as monism implies, all of our thoughts are the effects of physical causes, then we have no reason for assuming that they are also the consequent of a reasonable ground. Knowledge, however, is apprehended by reasoning from ground to consequent. Therefore, if monism is correct, there would be no way of knowing this—or anything else—we could not even suppose it, except by a fluke.

The zombie argument is based on a thought experiment proposed by Todd Moody, and developed by David Chalmers in his book "The Conscious Mind". The basic idea is that one can imagine one's body, and therefore conceive the existence of one's body, without any conscious states being associated with this body. Chalmers' argument is that it seems possible that such a being could exist because all that is needed is that all and only the things that the physical sciences describe about a zombie must be true of it. Since none of the concepts involved in these sciences make reference to consciousness or other mental phenomena, and any physical entity can be by definition described scientifically via physics, the move from conceivability to possibility is not such a large one. Others such as Dennett have argued that the notion of a philosophical zombie is an incoherent, or unlikely, concept. It has been argued under physicalism that one must either believe that anyone including oneself might be a zombie, or that no one can be a zombie—following from the assertion that one's own conviction about being (or not being) a zombie is a product of the physical world and is therefore no different from anyone else's. This argument has been expressed by Dennett who argues that "Zombies think they are conscious, think they have qualia, think they suffer pains—they are just 'wrong' (according to this lamentable tradition) in ways that neither they nor we could ever discover!"
See also the problem of other minds.

Interactionist dualism, or simply interactionism, is the particular form of dualism first espoused by Descartes in the "Meditations". In the 20th century, its major defenders have been Karl Popper and John Carew Eccles. It is the view that mental states, such as beliefs and desires, causally interact with physical states.

Descartes' famous argument for this position can be summarized as follows: Seth has a clear and distinct idea of his mind as a thinking thing that has no spatial extension (i.e., it cannot be measured in terms of length, weight, height, and so on). He also has a clear and distinct idea of his body as something that is spatially extended, subject to quantification and not able to think. It follows that mind and body are not identical because they have radically different properties.

At the same time, however, it is clear that Seth's mental states (desires, beliefs, etc.) have causal effects on his body and vice versa: A child touches a hot stove (physical event) which causes pain (mental event) and makes her yell (physical event), this in turn provokes a sense of fear and protectiveness in the caregiver (mental event), and so on.

Descartes' argument crucially depends on the premise that what Seth believes to be "clear and distinct" ideas in his mind are necessarily true. Many contemporary philosophers doubt this. For example, Joseph Agassi suggests that several scientific discoveries made since the early 20th century have undermined the idea of privileged access to one's own ideas. Freud claimed that a psychologically-trained observer can understand a person's unconscious motivations better than the person himself does. Duhem has shown that a philosopher of science can know a person's methods of discovery better than that person herself does, while Malinowski has shown that an anthropologist can know a person's customs and habits better than the person whose customs and habits they are. He also asserts that modern psychological experiments that cause people to see things that are not there provide grounds for rejecting Descartes' argument, because scientists can describe a person's perceptions better than the person herself can.

Psychophysical parallelism, or simply parallelism, is the view that mind and body, while having distinct ontological statuses, do not causally influence one another. Instead, they run along parallel paths (mind events causally interact with mind events and brain events causally interact with brain events) and only seem to influence each other. This view was most prominently defended by Gottfried Leibniz. Although Leibniz was an ontological monist who believed that only one type of substance, the monad, exists in the universe, and that everything is reducible to it, he nonetheless maintained that there was an important distinction between "the mental" and "the physical" in terms of causation. He held that God had arranged things in advance so that minds and bodies would be in harmony with each other. This is known as the doctrine of pre-established harmony.

Occasionalism is the view espoused by Nicholas Malebranche as well as Islamic philosophers such as Abu Hamid Muhammad ibn Muhammad al-Ghazali that asserts that all supposedly causal relations between physical events, or between physical and mental events, are not really causal at all. While body and mind are different substances, causes (whether mental or physical) are related to their effects by an act of God's intervention on each specific occasion.

Property dualism is the view that the world is constituted of just one kind of substance – the physical kind – and there exist two distinct kinds of properties: physical properties and mental properties. In other words, it is the view that non-physical, mental properties (such as beliefs, desires and emotions) inhere in some physical bodies (at least, brains). How mental and physical properties relate causally depends on the variety of property dualism in question, and is not always a clear issue. Sub-varieties of property dualism include:


Dual aspect theory or dual-aspect monism is the view that the mental and the physical are two aspects of, or perspectives on, the same substance. (Thus it is a mixed position, which is monistic in some respects). In modern philosophical writings, the theory's relationship to neutral monism has become somewhat ill-defined, but one proffered distinction says that whereas neutral monism allows the context of a given group of neutral elements and the relationships into which they enter to determine whether the group can be thought of as mental, physical, both, or neither, dual-aspect theory suggests that the mental and the physical are manifestations (or aspects) of some underlying substance, entity or process that is itself neither mental nor physical as normally understood. Various formulations of dual-aspect monism also require the mental and the physical to be complementary, mutually irreducible and perhaps inseparable (though distinct).

This is a philosophy of mind that regards the degrees of freedom between mental and physical well-being as not necessarily synonymous thus implying an experiential dualism between body and mind. An example of these disparate degrees of freedom is given by Allan Wallace who notes that it is "experientially apparent that one may be physically uncomfortable—for instance, while engaging in a strenuous physical workout—while mentally cheerful; conversely, one may be mentally distraught while experiencing physical comfort". Experiential dualism notes that our subjective experience of merely seeing something in the physical world seems qualitatively different than mental processes like grief that comes from losing a loved one. This philosophy also is a proponent of causal dualism which is defined as the dual ability for mental states and physical states to affect one another. Mental states can cause changes in physical states and vice versa.

However, unlike cartesian dualism or some other systems, experiential dualism does not posit two fundamental substances in reality: mind and matter. Rather, experiential dualism is to be understood as a conceptual framework that gives credence to the qualitative difference between the experience of mental and physical states. Experiential dualism is accepted as the conceptual framework of Madhyamaka Buddhism.

Madhayamaka Buddhism goes even further, finding fault with the monist view of physicalist philosophies of mind as well in that these generally posit matter and energy as the fundamental substance of reality. Nonetheless, this does not imply that the cartesian dualist view is correct, rather Madhyamaka regards as error any affirming view of a fundamental substance to reality.In denying the independent self-existence of all the phenomena that make up the world of our experience, the Madhyamaka view departs from both the substance dualism of Descartes and the substance monism—namely, physicalism—that is characteristic of modern science. The physicalism propounded by many contemporary scientists seems to assert that the real world is composed of physical things-in-themselves, while all mental phenomena are regarded as mere appearances, devoid of any reality in and of themselves. Much is made of this difference between appearances and reality.
Indeed, physicalism, or the idea that matter is the only fundamental substance of reality, is explicitly rejected by Buddhism.In the Madhyamaka view, mental events are no more or less real than physical events. In terms of our common-sense experience, differences of kind do exist between physical and mental phenomena. While the former commonly have mass, location, velocity, shape, size, and numerous other physical attributes, these are not generally characteristic of mental phenomena. For example, we do not commonly conceive of the feeling of affection for another person as having mass or location. These physical attributes are no more appropriate to other mental events such as sadness, a recalled image from one's childhood, the visual perception of a rose, or consciousness of any sort. Mental phenomena are, therefore, not regarded as being physical, for the simple reason that they lack many of the attributes that are uniquely characteristic of physical phenomena. Thus, Buddhism has never adopted the physicalist principle that regards only physical things as real.

In contrast to dualism, monism does not accept any fundamental divisions. The fundamentally disparate nature of reality has been central to forms of eastern philosophies for over two millennia. In Indian and Chinese philosophy, monism is integral to how experience is understood. Today, the most common forms of monism in Western philosophy are physicalist. Physicalistic monism asserts that the only existing substance is physical, in some sense of that term to be clarified by our best science. However, a variety of formulations (see below) are possible. Another form of monism, idealism, states that the only existing substance is mental. Although pure idealism, such as that of George Berkeley, is uncommon in contemporary Western philosophy, a more sophisticated variant called panpsychism, according to which mental experience and properties may be at the foundation of physical experience and properties, has been espoused by some philosophers such as Alfred North Whitehead and David Ray Griffin.

Phenomenalism is the theory that representations (or sense data) of external objects are all that exist. Such a view was briefly adopted by Bertrand Russell and many of the logical positivists during the early 20th century. A third possibility is to accept the existence of a basic substance that is neither physical nor mental. The mental and physical would then both be properties of this neutral substance. Such a position was adopted by Baruch Spinoza and was popularized by Ernst Mach in the 19th century. This neutral monism, as it is called, resembles property dualism.

Behaviorism dominated philosophy of mind for much of the 20th century, especially the first half. In psychology, behaviorism developed as a reaction to the inadequacies of introspectionism. Introspective reports on one's own interior mental life are not subject to careful examination for accuracy and cannot be used to form predictive generalizations. Without generalizability and the possibility of third-person examination, the behaviorists argued, psychology cannot be scientific. The way out, therefore, was to eliminate the idea of an interior mental life (and hence an ontologically independent mind) altogether and focus instead on the description of observable behavior.

Parallel to these developments in psychology, a philosophical behaviorism (sometimes called logical behaviorism) was developed. This is characterized by a strong verificationism, which generally considers unverifiable statements about interior mental life pointless. For the behaviorist, mental states are not interior states on which one can make introspective reports. They are just descriptions of behavior or dispositions to behave in certain ways, made by third parties to explain and predict another's behavior.

Philosophical behaviorism has fallen out of favor since the latter half of the 20th century, coinciding with the rise of cognitivism.

Type physicalism (or type-identity theory) was developed by John Smart and Ullin Place as a direct reaction to the failure of behaviorism. These philosophers reasoned that, if mental states are something material, but not behavioral, then mental states are probably identical to internal states of the brain. In very simplified terms: a mental state "M" is nothing other than brain state "B". The mental state "desire for a cup of coffee" would thus be nothing more than the "firing of certain neurons in certain brain regions".
Despite its initial plausibility, the identity theory faces a strong challenge in the form of the thesis of multiple realizability, first formulated by Hilary Putnam. For example, not only humans, but many different species of animals can experience pain. However, it seems highly unlikely that all of these diverse organisms with the same pain experience are in the identical brain state. And if this is the case, then pain cannot be identical to a specific brain state. The identity theory is thus empirically unfounded.

On the other hand, even granted the above, it does not follow that identity theories of all types must be abandoned. According to token identity theories, the fact that a certain brain state is connected with only one mental state of a person does not have to mean that there is an absolute correlation between types of mental state and types of brain state. The type–token distinction can be illustrated by a simple example: the word "green" contains four types of letters (g, r, e, n) with two tokens (occurrences) of the letter "e" along with one each of the others.
The idea of token identity is that only particular occurrences of mental events are identical with particular occurrences or tokenings of physical events. Anomalous monism (see below) and most other non-reductive physicalisms are token-identity theories. Despite these problems, there is a renewed interest in the type identity theory today, primarily due to the influence of Jaegwon Kim.

Functionalism was formulated by Hilary Putnam and Jerry Fodor as a reaction to the inadequacies of the identity theory. Putnam and Fodor saw mental states in terms of an empirical computational theory of the mind. At about the same time or slightly after, D.M. Armstrong and David Kellogg Lewis formulated a version of functionalism that analyzed the mental concepts of folk psychology in terms of functional roles. Finally, Wittgenstein's idea of meaning as use led to a version of functionalism as a theory of meaning, further developed by Wilfrid Sellars and Gilbert Harman. Another one, psychofunctionalism, is an approach adopted by the naturalistic philosophy of mind associated with Jerry Fodor and Zenon Pylyshyn.

What all these different varieties of functionalism share in common is the thesis that mental states are characterized by their causal relations with other mental states and with sensory inputs and behavioral outputs. That is, functionalism abstracts away from the details of the physical implementation of a mental state by characterizing it in terms of non-mental functional properties. For example, a kidney is characterized scientifically by its functional role in filtering blood and maintaining certain chemical balances. From this point of view, it does not really matter whether the kidney be made up of organic tissue, plastic nanotubes or silicon chips: it is the role that it plays and its relations to other organs that define it as a kidney.

Non-reductionist philosophers hold firmly to two essential convictions with regard to mind–body relations: 1) Physicalism is true and mental states must be physical states, but 2) All reductionist proposals are unsatisfactory: mental states cannot be reduced to behavior, brain states or functional states. Hence, the question arises whether there can still be a non-reductive physicalism. Donald Davidson's anomalous monism is an attempt to formulate such a physicalism. He "thinks that when one runs across what are traditionally seen as absurdities of Reason, such as akrasia or self-deception, the personal psychology framework is not to be given up in favor of the subpersonal one, but rather must be enlarged or extended so that the rationality set out by the principle of charity can be found elsewhere."

Davidson uses the thesis of supervenience: mental states supervene on physical states, but are not reducible to them. "Supervenience" therefore describes a functional dependence: there can be no change in the mental without some change in the physical–causal reducibility between the mental and physical without ontological reducibility.

Because non-reductive physicalist theories attempt to both retain the ontological distinction between mind and body and try to solve the "surfeit of explanations puzzle" in some way; critics often see this as a paradox and point out the similarities to epiphenomenalism, in that it is the brain that is seen as the root "cause" not the mind, and the mind seems to be rendered inert.

Epiphenomenalism regards one or more mental states as the byproduct of physical brain states, having no influence on physical states. The interaction is one-way (solving the "surfeit of explanations puzzle") but leaving us with non-reducible mental states (as a byproduct of brain states) – causally reducible, but ontologically irreducible to physical states. Pain would be seen by epiphenomenalists as being caused by the brain state but as not having effects on other brain states, though it might have effects on other mental states (i.e. cause distress).

Weak emergentism is a form of "non-reductive physicalism" that involves a layered view of nature, with the layers arranged in terms of increasing complexity and each corresponding to its own special science. Some philosophers hold that emergent properties causally interact with more fundamental levels, while others maintain that higher-order properties simply supervene over lower levels without direct causal interaction. The latter group therefore holds a less strict, or "weaker", definition of emergentism, which can be rigorously stated as follows: a property P of composite object O is emergent if it is metaphysically impossible for another object to lack property P if that object is composed of parts with intrinsic properties identical to those in O and has those parts in an identical configuration.

Sometimes emergentists use the example of water having a new property when Hydrogen H and Oxygen O combine to form HO (water). In this example there "emerges" a new property of a transparent liquid that would not have been predicted by understanding hydrogen and oxygen as gases. This is analogous to physical properties of the brain giving rise to a mental state. Emergentists try to solve the notorious mind–body gap this way. One problem for emergentism is the idea of "causal closure" in the world that does not allow for a mind-to-body causation.

If one is a materialist and believes that all aspects of our common-sense psychology will find reduction to a mature cognitive neuroscience, and that non-reductive materialism is mistaken, then one can adopt a final, more radical position: eliminative materialism.

There are several varieties of eliminative materialism, but all maintain that our common-sense "folk psychology" badly misrepresents the nature of some aspect of cognition. Eliminativists such as Patricia and Paul Churchland argue that while folk psychology treats cognition as fundamentally sentence-like, the non-linguistic vector/matrix model of neural network theory or connectionism will prove to be a much more accurate account of how the brain works.

The Churchlands often invoke the fate of other, erroneous popular theories and ontologies that have arisen in the course of history. For example, Ptolemaic astronomy served to explain and roughly predict the motions of the planets for centuries, but eventually this model of the solar system was eliminated in favor of the Copernican model. The Churchlands believe the same eliminative fate awaits the "sentence-cruncher" model of the mind in which thought and behavior are the result of manipulating sentence-like states called "propositional attitudes".

Some philosophers take an epistemic approach and argue that the mind–body problem is currently unsolvable, and perhaps will always remain unsolvable to human beings. This is usually termed New mysterianism. Colin McGinn holds that human beings are cognitively closed in regards to their own minds. According to McGinn human minds lack the concept-forming procedures to fully grasp how mental properties such as consciousness arise from their causal basis. An example would be how an elephant is cognitively closed in regards to particle physics.

A more moderate conception has been expounded by Thomas Nagel, which holds that the mind–body problem is currently unsolvable at the present stage of scientific development and that it might take a future scientific paradigm shift or revolution to bridge the explanatory gap. Nagel posits that in the future a sort of "objective phenomenology" might be able to bridge the gap between subjective conscious experience and its physical basis.

Each attempt to answer the mind–body problem encounters substantial problems. Some philosophers argue that this is because there is an underlying conceptual confusion. These philosophers, such as Ludwig Wittgenstein and his followers in the tradition of linguistic criticism, therefore reject the problem as illusory. They argue that it is an error to ask how mental and biological states fit together. Rather it should simply be accepted that human experience can be described in different ways—for instance, in a mental and in a biological vocabulary. Illusory problems arise if one tries to describe the one in terms of the other's vocabulary or if the mental vocabulary is used in the wrong contexts. This is the case, for instance, if one searches for mental states of the brain. The brain is simply the wrong context for the use of mental vocabulary—the search for mental states of the brain is therefore a category error or a sort of fallacy of reasoning.

Today, such a position is often adopted by interpreters of Wittgenstein such as Peter Hacker. However, Hilary Putnam, the originator of functionalism, has also adopted the position that the mind–body problem is an illusory problem which should be dissolved according to the manner of Wittgenstein.

The thesis of physicalism is that the mind is part of the material (or physical) world. Such a position faces the problem that the mind has certain properties that no other material thing seems to possess. Physicalism must therefore explain how it is possible that these properties can nonetheless emerge from a material thing. The project of providing such an explanation is often referred to as the "naturalization of the mental". Some of the crucial problems that this project attempts to resolve include the existence of qualia and the nature of intentionality.

Many mental states seem to be experienced subjectively in different ways by different individuals. And it is characteristic of a mental state that it has some experiential "quality", e.g. of pain, that it hurts. However, the sensation of pain between two individuals may not be identical, since no one has a perfect way to measure how much something hurts or of describing exactly how it feels to hurt. Philosophers and scientists therefore ask where these experiences come from. The existence of cerebral events, in and of themselves, cannot explain why they are accompanied by these corresponding qualitative experiences. The puzzle of why many cerebral processes occur with an accompanying experiential aspect in consciousness seems impossible to explain.

Yet it also seems to many that science will eventually have to explain such experiences. This follows from an assumption about the possibility of reductive explanations. According to this view, if an attempt can be successfully made to explain a phenomenon reductively (e.g., water), then it can be explained why the phenomenon has all of its properties (e.g., fluidity, transparency). In the case of mental states, this means that there needs to be an explanation of why they have the property of being experienced in a certain way.

The 20th-century German philosopher Martin Heidegger criticized the ontological assumptions underpinning such a reductive model, and claimed that it was impossible to make sense of experience in these terms. This is because, according to Heidegger, the nature of our subjective experience and its "qualities" is impossible to understand in terms of Cartesian "substances" that bear "properties". Another way to put this is that the very concept of qualitative experience is incoherent in terms of—or is semantically incommensurable with the concept of—substances that bear properties.

This problem of explaining introspective first-person aspects of mental states and consciousness in general in terms of third-person quantitative neuroscience is called the explanatory gap. There are several different views of the nature of this gap among contemporary philosophers of mind. David Chalmers and the early Frank Jackson interpret the gap as ontological in nature; that is, they maintain that qualia can never be explained by science because physicalism is false. There are two separate categories involved and one cannot be reduced to the other. An alternative view is taken by philosophers such as Thomas Nagel and Colin McGinn. According to them, the gap is epistemological in nature. For Nagel, science is not yet able to explain subjective experience because it has not yet arrived at the level or kind of knowledge that is required. We are not even able to formulate the problem coherently. For McGinn, on other hand, the problem is one of permanent and inherent biological limitations. We are not able to resolve the explanatory gap because the realm of subjective experiences is cognitively closed to us in the same manner that quantum physics is cognitively closed to elephants. Other philosophers liquidate the gap as purely a semantic problem. This semantic problem, of course, led to the famous "Qualia Question", which is: "Does Red cause Redness"?

Intentionality is the capacity of mental states to be directed towards ("about") or be in relation with something in the external world. This property of mental states entails that they have contents and semantic referents and can therefore be assigned truth values. When one tries to reduce these states to natural processes there arises a problem: natural processes are not true or false, they simply happen. It would not make any sense to say that a natural process is true or false. But mental ideas or judgments are true or false, so how then can mental states (ideas or judgments) be natural processes? The possibility of assigning semantic value to ideas must mean that such ideas are about facts. Thus, for example, the idea that Herodotus was a historian refers to Herodotus and to the fact that he was a historian. If the fact is true, then the idea is true; otherwise, it is false. But where does this relation come from? In the brain, there are only electrochemical processes and these seem not to have anything to do with Herodotus.

Philosophy of perception is concerned with the nature of perceptual experience and the status of perceptual objects, in particular how perceptual experience relates to appearances and beliefs about the world. The main contemporary views within philosophy of perception include naive realism, enactivism and representational views.

Humans are corporeal beings and, as such, they are subject to examination and description by the natural sciences. Since mental processes are intimately related to bodily processes, the descriptions that the natural sciences furnish of human beings play an important role in the philosophy of mind. There are many scientific disciplines that study processes related to the mental. The list of such sciences includes: biology, computer science, cognitive science, cybernetics, linguistics, medicine, pharmacology, and psychology.

The theoretical background of biology, as is the case with modern natural sciences in general, is fundamentally materialistic. The objects of study are, in the first place, physical processes, which are considered to be the foundations of mental activity and behavior. The increasing success of biology in the explanation of mental phenomena can be seen by the absence of any empirical refutation of its fundamental presupposition: "there can be no change in the mental states of a person without a change in brain states."

Within the field of neurobiology, there are many subdisciplines that are concerned with the relations between mental and physical states and processes: Sensory neurophysiology investigates the relation between the processes of perception and stimulation. Cognitive neuroscience studies the correlations between mental processes and neural processes. Neuropsychology describes the dependence of mental faculties on specific anatomical regions of the brain. Lastly, evolutionary biology studies the origins and development of the human nervous system and, in as much as this is the basis of the mind, also describes the ontogenetic and phylogenetic development of mental phenomena beginning from their most primitive stages. Evolutionary biology furthermore places tight constraints on any philosophical theory of the mind, as the gene-based mechanism of natural selection does not allow any giant leaps in the development of neural complexity or neural software but only incremental steps over long time periods.

The methodological breakthroughs of the neurosciences, in particular the introduction of high-tech neuroimaging procedures, has propelled scientists toward the elaboration of increasingly ambitious research programs: one of the main goals is to describe and comprehend the neural processes which correspond to mental functions (see: neural correlate). Several groups are inspired by these advances.

Computer science concerns itself with the automatic processing of information (or at least with physical systems of symbols to which information is assigned) by means of such things as computers. From the beginning, computer programmers have been able to develop programs that permit computers to carry out tasks for which organic beings need a mind. A simple example is multiplication. It is not clear whether computers could be said to have a mind. Could they, someday, come to have what we call a mind? This question has been propelled into the forefront of much philosophical debate because of investigations in the field of artificial intelligence (AI).

Within AI, it is common to distinguish between a modest research program and a more ambitious one: this distinction was coined by John Searle in terms of a weak AI and strong AI. The exclusive objective of "weak AI", according to Searle, is the successful simulation of mental states, with no attempt to make computers become conscious or aware, etc. The objective of strong AI, on the contrary, is a computer with consciousness similar to that of human beings. The program of strong AI goes back to one of the pioneers of computation Alan Turing. As an answer to the question "Can computers think?", he formulated the famous Turing test. Turing believed that a computer could be said to "think" when, if placed in a room by itself next to another room that contained a human being and with the same questions being asked of both the computer and the human being by a third party human being, the computer's responses turned out to be indistinguishable from those of the human. Essentially, Turing's view of machine intelligence followed the behaviourist model of the mind—intelligence is as intelligence does. The Turing test has received many criticisms, among which the most famous is probably the Chinese room thought experiment formulated by Searle.

The question about the possible sensitivity (qualia) of computers or robots still remains open. Some computer scientists believe that the specialty of AI can still make new contributions to the resolution of the "mind–body problem". They suggest that based on the reciprocal influences between software and hardware that takes place in all computers, it is possible that someday theories can be discovered that help us to understand the reciprocal influences between the human mind and the brain (wetware).

Psychology is the science that investigates mental states directly. It uses generally empirical methods to investigate concrete mental states like joy, fear or obsessions. Psychology investigates the laws that bind these mental states to each other or with inputs and outputs to the human organism.

An example of this is the psychology of perception. Scientists working in this field have discovered general principles of the perception of forms. A law of the psychology of forms says that objects that move in the same direction are perceived as related to each other. This law describes a relation between visual input and mental perceptual states. However, it does not suggest anything about the nature of perceptual states. The laws discovered by psychology are compatible with all the answers to the mind–body problem already described.

Cognitive science is the interdisciplinary scientific study of the mind and its processes. It examines what cognition is, what it does, and how it works. It includes research on intelligence and behavior, especially focusing on how information is represented, processed, and transformed (in faculties such as perception, language, memory, reasoning, and emotion) within nervous systems (human or other animal) and machines (e.g. computers). Cognitive science consists of multiple research disciplines, including psychology, artificial intelligence, philosophy, neuroscience, linguistics, anthropology, sociology, and education. It spans many levels of analysis, from low-level learning and decision mechanisms to high-level logic and planning; from neural circuitry to modular brain organisation. Rowlands argues that cognition is enactive, embodied, embedded, affective and (potentially) extended. The position is taken that the "classical sandwich" of cognition sandwiched between perception and action is artificial; cognition has to be seen as a product of a strongly coupled interaction that cannot be divided this way.

Most of the discussion in this article has focused on one style or tradition of philosophy in modern Western culture, usually called analytic philosophy (sometimes described as Anglo-American philosophy). Many other schools of thought exist, however, which are sometimes subsumed under the broad (and vague) label of continental philosophy. In any case, though topics and methods here are numerous, in relation to the philosophy of mind the various schools that fall under this label (phenomenology, existentialism, etc.) can globally be seen to differ from the analytic school in that they focus less on language and logical analysis alone but also take in other forms of understanding human existence and experience. With reference specifically to the discussion of the mind, this tends to translate into attempts to grasp the concepts of thought and perceptual experience in some sense that does not merely involve the analysis of linguistic forms.

Immanuel Kant's "Critique of Pure Reason", first published in 1781 and presented again with major revisions in 1787, represents a significant intervention into what will later become known as the philosophy of mind. Kant's first critique is generally recognized as among the most significant works of modern philosophy in the West. Kant is a figure whose influence is marked in both continental and analytic/Anglo-American philosophy. Kant's work develops an in-depth study of transcendental consciousness, or the life of the mind as conceived through universal categories of consciousness.

In Georg Wilhelm Friedrich Hegel's "Philosophy of Mind" (frequently translated as "Philosophy of Spirit" or Geist), the third part of his "Encyclopedia of the Philosophical Sciences", Hegel discusses three distinct types of mind: the "subjective mind/spirit", the mind of an individual; the "objective mind/spirit", the mind of society and of the State; and the "Absolute mind/spirit", the position of religion, art, and philosophy. See also Hegel's "The Phenomenology of Spirit". Nonetheless, Hegel's work differs radically from the style of Anglo-American philosophy of mind.

In 1896, Henri Bergson made in "Matter and Memory" "Essay on the relation of body and spirit" a forceful case for the ontological difference of body and mind by reducing the problem to the more definite one of memory, thus allowing for a solution built on the "empirical test case" of aphasia.

In modern times, the two main schools that have developed in response or opposition to this Hegelian tradition are phenomenology and existentialism. Phenomenology, founded by Edmund Husserl, focuses on the contents of the human mind (see noema) and how processes shape our experiences. Existentialism, a school of thought founded upon the work of Søren Kierkegaard, focuses on Human predicament and how people deal with the situation of being alive. Existential-phenomenology represents a major branch of continental philosophy (they are not contradictory), rooted in the work of Husserl but expressed in its fullest forms in the work of Martin Heidegger, Jean-Paul Sartre, Simone de Beauvoir and Maurice Merleau-Ponty. See Heidegger's "Being and Time", Merleau-Ponty's "Phenomenology of Perception", Sartre's "Being and Nothingness", and Simone de Beauvoir's "The Second Sex".

There are countless subjects that are affected by the ideas developed in the philosophy of mind. Clear examples of this are the nature of death and its definitive character, the nature of emotion, of perception and of memory. Questions about what a person is and what his or her identity consists of also have much to do with the philosophy of mind. There are two subjects that, in connection with the philosophy of the mind, have aroused special attention: free will and the self.

In the context of philosophy of mind, the problem of free will takes on renewed intensity. This is certainly the case, at least, for materialistic determinists. According to this position, natural laws completely determine the course of the material world. Mental states, and therefore the will as well, would be material states, which means human behavior and decisions would be completely determined by natural laws. Some take this reasoning a step further: people cannot determine by themselves what they want and what they do. Consequently, they are not free.

This argumentation is rejected, on the one hand, by the compatibilists. Those who adopt this position suggest that the question "Are we free?" can only be answered once we have determined what the term "free" means. The opposite of "free" is not "caused" but "compelled" or "coerced". It is not appropriate to identify freedom with indetermination. A free act is one where the agent could have done otherwise if it had chosen otherwise. In this sense a person can be free even though determinism is true. The most important compatibilist in the history of the philosophy was David Hume. More recently, this position is defended, for example, by Daniel Dennett.

On the other hand, there are also many incompatibilists who reject the argument because they believe that the will is free in a stronger sense called libertarianism. These philosophers affirm the course of the world is either a) not completely determined by natural law where natural law is intercepted by physically independent agency, b) determined by indeterministic natural law only, or c) determined by indeterministic natural law in line with the subjective effort of physically non-reducible agency. Under Libertarianism, the will does not have to be deterministic and, therefore, it is potentially free. Critics of the second proposition (b) accuse the incompatibilists of using an incoherent concept of freedom. They argue as follows: if our will is not determined by anything, then we desire what we desire by pure chance. And if what we desire is purely accidental, we are not free. So if our will is not determined by anything, we are not free.

The philosophy of mind also has important consequences for the concept of "self". If by "self" or "I" one refers to an essential, immutable nucleus of the "person", some modern philosophers of mind, such as Daniel Dennett believe that no such thing exists. According to Dennett and other contemporaries, the self is considered an illusion. The idea of a self as an immutable essential nucleus derives from the idea of an immaterial soul. Such an idea is unacceptable to modern philosophers with physicalist orientations and their general skepticism of the concept of "self" as postulated by David Hume, who could never catch himself "not" doing, thinking or feeling anything. However, in the light of empirical results from developmental psychology, developmental biology and neuroscience, the idea of an essential inconstant, material nucleus—an integrated representational system distributed over changing patterns of synaptic connections—seems reasonable.



</doc>
<doc id="25754129" url="https://en.wikipedia.org/wiki?curid=25754129" title="Theory of forms">
Theory of forms

The theory of Forms or theory of Ideas is a philosophical theory, concept, or world-view, attributed to Plato, that the physical world is not as real or true as timeless, absolute, unchangeable ideas. According to this theory, ideas in this sense, often capitalized and translated as "Ideas" or "Forms", are the non-physical essences of all things, of which objects and matter in the physical world are merely imitations. Plato speaks of these entities only through the characters (primarily Socrates) of his dialogues who sometimes suggest that these Forms are the only objects of study that can provide knowledge. The theory itself is contested from within Plato's dialogues, and it is a general point of controversy in philosophy. Whether the theory represents Plato's own views is held in doubt by modern scholarship. Nonetheless the theory is considered to be a classical solution to the problem of universals.

The early Greek concept of form precedes attested philosophical usage and is represented by a number of words mainly having to do with vision, sight, and appearance. Plato uses these aspects of sight and appearance from the early Greek concept of the form in his dialogues to explain the Forms and the Good.

The meaning of the term ("eidos"), "visible form", and related terms μορφή ("morphē"), "shape", and φαινόμενα ("phainomena"), "appearances", from φαίνω ("phainō"), "shine", Indo-European ""*bʰeh₂-"" or "*bhā-" remained stable over the centuries until the beginning of philosophy, when they became equivocal, acquiring additional specialized philosophic meanings. The pre-Socratic philosophers, starting with Thales, noted that appearances change, and began to ask what the thing that changes "really" is. The answer was substance, which stands under the changes and is the actually existing thing being seen. The status of appearances now came into question. What is the form really and how is that related to substance?

The Forms are expounded upon in Plato's dialogues and general speech, in that every object or quality in reality has a form: dogs, human beings, mountains, colors, courage, love, and goodness. Form answers the question, "What is that?" Plato was going a step further and asking what Form itself is. He supposed that the object was essentially or "really" the Form and that the phenomena were mere shadows mimicking the Form; that is, momentary portrayals of the Form under different circumstances. The problem of universals – how can one thing in general be many things in particular – was solved by presuming that Form was a distinct singular thing but caused plural representations of itself in particular objects. For example, in the dialogue Parmenides, Socrates states: "Nor, again, if a person were to show that all is one by partaking of one, and at the same time many by partaking of many, would that be very astonishing. But if he were to show me that the absolute one was many, or the absolute many one, I should be truly amazed." Matter is considered particular in itself. For Plato, forms, such as beauty, are more real than any objects that imitate them. Though the forms are timeless and unchanging, physical things are in a constant change of existence. Where forms are unqualified perfection, physical things are qualified and conditioned.

These Forms are the essences of various objects: they are that without which a thing would not be the kind of thing it is. For example, there are countless tables in the world but the Form of tableness is at the core; it is the essence of all of them. Plato's Socrates held that the world of Forms is transcendent to our own world (the world of substances) and also is the essential basis of reality. Super-ordinate to matter, Forms are the most pure of all things. Furthermore, he believed that true knowledge/intelligence is the ability to grasp the world of Forms with one's mind.

A Form is "aspatial" (transcendent to space) and "atemporal" (transcendent to time). Atemporal means that it does not exist within any time period, rather it provides the formal basis for time. It therefore formally grounds beginning, persisting and ending. It is neither eternal in the sense of existing forever, nor mortal, of limited duration. It exists transcendent to time altogether. Forms are aspatial in that they have no spatial dimensions, and thus no orientation in space, nor do they even (like the point) have a location. They are non-physical, but they are not in the mind. Forms are extra-mental (i.e. real in the strictest sense of the word).

A Form is an objective "blueprint" of perfection. The Forms are perfect and unchanging representations of objects and qualities. For example, the Form of beauty or the Form of a triangle. For the form of a triangle say there is a triangle drawn on a blackboard. A triangle is a polygon with 3 sides. The triangle as it is on the blackboard is far from perfect. However, it is only the intelligibility of the Form "triangle" that allows us to know the drawing on the chalkboard is a triangle, and the Form "triangle" is perfect and unchanging. It is exactly the same whenever anyone chooses to consider it; however, time only effects the observer and not of the triangle. It follows that the same attributes would exist for the Form of beauty and for all Forms.

The words, εἶδος ("eidos") and ἰδέα ("idea") come from the Indo-European root or "*weid-" "see" (cognate with Sanskrit "vétti"). "Eidos" (though not "idea") is already attested in texts of the Homeric era, the earliest Greek literature. This transliteration and the translation tradition of German and Latin lead to the expression "theory of Ideas." The word is however not the English "idea," which is a mental concept only.

The theory of matter and form (today's hylomorphism) started with Plato and possibly germinal in some of the presocratic writings. The forms were considered as being "in" something else, which Plato called nature ("physis"). The latter seemed as carved "wood", ὕλη ("hyle") in Greek, corresponding to "materia" in Latin, from which the English word "matter" is derived, shaped by receiving (or exchanging) forms.

The English word "form" may be used to translate two distinct concepts that concerned Plato—the outward "form" or appearance of something, and "Form" in a new, technical nature, that never...assumes a form like that of any of the things which enter into her; ... But the forms which enter into and go out of her are the likenesses of real existences modelled after their patterns in a wonderful and inexplicable manner... The objects that are seen, according to Plato, are not real, but literally "mimic" the real Forms. In the Allegory of the Cave expressed in "Republic", the things that are ordinarily perceived in the world are characterized as shadows of the real things, which are not perceived directly. That which the observer understands when he views the world mimics the archetypes of the many types and properties (that is, of universals) of things observed.

Plato often invokes, particularly in his dialogues "Phaedo", "Republic" and "Phaedrus", poetic language to illustrate the mode in which the Forms are said to exist. Near the end of the "Phaedo", for example, Plato describes the world of Forms as a pristine region of the physical universe located above the surface of the Earth ("Phd." 109a-111c). In the "Phaedrus" the Forms are in a "place beyond heaven" ("huperouranios topos") ("Phdr." 247c ff); and in the "Republic" the sensible world is contrasted with the intelligible realm ("noēton topon") in the famous Allegory of the Cave.

It would be a mistake to take Plato's imagery as positing the intelligible world as a literal physical space apart from this one. Plato emphasizes that the Forms are not beings that extend in space (or time), but subsist apart from any physical space whatsoever. Thus we read in the "Symposium" of the Form of Beauty: "It is not anywhere in another thing, as in an animal, or in earth, or in heaven, or in anything else, but itself by itself with itself," (211b). And in the "Timaeus" Plato writes: "Since these things are so, we must agree that that which keeps its own form unchangingly, which has not been brought into being and is not destroyed, which neither receives into itself anything else from anywhere else, "nor itself enters into anything anywhere", is one thing," (52a, emphasis added).

According to Plato, Socrates postulated a world of ideal Forms, which he admitted were impossible to know. Nevertheless, he formulated a very specific description of that world, which did not match his metaphysical principles. Corresponding to the world of Forms is our world, that of the shadows, an imitation of the real one. Just as shadows exist only because of the light of a fire, our world exists as, "the offspring of the good". Our world is modeled after the patterns of the Forms. The function of humans in our world is therefore to imitate the ideal world as much as possible which, importantly, includes imitating the good, i.e. acting morally.

Plato lays out much of this theory in the "Republic" where, in an attempt to define Justice, he considers many topics including the constitution of the ideal state. While this state, and the Forms, do not exist on earth, because their imitations do, Plato says we are able to form certain well-founded opinions about them, through a theory called recollection.

The republic is a greater imitation of Justice:Our aim in founding the state was not the disproportional happiness of any one class, but the greatest happiness of the whole; we thought that in a state ordered with a view to the good of the whole we should be most likely to find justice.

The key to not know how such a state might come into existence is the word "founding" ("oikidzomen"), which is used of colonization. It was customary in such instances to receive a constitution from an elected or appointed lawgiver; however in Athens, lawgivers were appointed to reform the constitution from time to time (for example, Draco, Solon). In speaking of reform, Socrates uses the word "purge" ("diakathairountes") in the same sense that Forms exist purged of matter.

The purged society is a regulated one presided over by philosophers educated by the state, who maintain three non-hereditary classes as required: the tradesmen (including merchants and professionals), the guardians (militia and police) and the philosophers (legislators, administrators and the philosopher-king). Class is assigned at the end of education, when the state institutes individuals in their occupation. Socrates expects class to be hereditary but he allows for mobility according to natural ability. The criteria for selection by the academics is ability to perceive forms (the analog of English "intelligence") and martial spirit as well as predisposition or aptitude.

The views of Socrates on the proper order of society are certainly contrary to Athenian values of the time and must have produced a shock effect, intentional or not, accounting for the animosity against him. For example, reproduction is much too important to be left in the hands of untrained individuals: "... the possession of women and the procreation of children ... will ... follow the general principle that friends have all things in common, ..." The family is therefore to be abolished and the children – whatever their parentage – to be raised by the appointed mentors of the state.

Their genetic fitness is to be monitored by the physicians: "... he (Asclepius, a culture hero) did not want to lengthen out good-for-nothing lives, or have weak fathers begetting weaker sons – if a man was not able to live in the ordinary way he had no business to cure him ..." Physicians minister to the healthy rather than cure the sick: "... (Physicians) will minister to better natures, giving health both of soul and of body; but those who are diseased in their bodies they will leave to die, and the corrupt and incurable souls they will put an end to themselves." Nothing at all in Greek medicine so far as can be known supports the airy (in the Athenian view) propositions of Socrates. Yet it is hard to be sure of Socrates' real views considering that there are no works written by Socrates himself. There are two common ideas pertaining to the beliefs and character of Socrates: the first being the Mouthpiece Theory where writers use Socrates in dialogue as a mouthpiece to get their own views across. However, since most of what we know about Socrates comes from plays, most of the Platonic plays are accepted as the more accurate Socrates since Plato was a direct student of Socrates.

Perhaps the most important principle is that just as the Good must be supreme so must its image, the state, take precedence over individuals in everything. For example, guardians "... will have to be watched at every age in order that we may see whether they preserve their resolution and never, under the influence either of force or enchantment, forget or cast off their sense of duty to the state." This concept of requiring guardians of guardians perhaps suffers from the Third Man weakness (see below): guardians require guardians require guardians, ad infinitum. The ultimate trusty guardian is missing. Socrates does not hesitate to face governmental issues many later governors have found formidable: "Then if anyone at all is to have the privilege of lying, the rulers of the state should be the persons, and they ... may be allowed to lie for the public good."

Plato's conception of Forms actually differs from dialogue to dialogue, and in certain respects it is never fully explained, so many aspects of the theory are open to interpretation. Forms are first introduced in the Phaedo, but in that dialogue the concept is simply referred to as something the participants are already familiar with, and the theory itself is not developed. Similarly, in the Republic, Plato relies on the concept of Forms as the basis of many of his arguments but feels no need to argue for the validity of the theory itself or to explain precisely what Forms are. Commentators have been left with the task of explaining what Forms are and how visible objects participate in them, and there has been no shortage of disagreement. Some scholars advance the view that Forms are paradigms, perfect examples on which the imperfect world is modeled. Others interpret Forms as universals, so that the Form of Beauty, for example, is that quality that all beautiful things share. Yet others interpret Forms as "stuffs," the conglomeration of all instances of a quality in the visible world. Under this interpretation, we could say there is a little beauty in one person, a little beauty in another—all the beauty in the world put together is the Form of Beauty. Plato himself was aware of the ambiguities and inconsistencies in his Theory of Forms, as is evident from the incisive criticism he makes of his own theory in the Parmenides.

Plato's main evidence for the existence of Forms is intuitive only and is as follows.

We call both the sky and blue jeans by the same color, blue. However, clearly a pair of jeans and the sky are not the same color; moreover, the wavelengths of light reflected by the sky at every location and all the millions of blue jeans in every state of fading constantly change, and yet we somehow have a consensus of the basic form Blueness as it applies to them. Says Plato:But if the very nature of knowledge changes, at the time when the change occurs there will be no knowledge, and, according to this view, there will be no one to know and nothing to be known: but if that which knows and that which is known exist ever, and the beautiful and the good and every other thing also exist, then I do not think that they can resemble a process of flux, as we were just now supposing.

Plato believed that long before our bodies ever existed, our souls existed and inhabited heaven, where they became directly acquainted with the forms themselves. Real knowledge, to him, was knowledge of the forms. But knowledge of the forms cannot be gained through sensory experience because the forms are not in the physical world. Therefore, our real knowledge of the forms must be the memory of our initial acquaintance with the forms in heaven. Therefore, what we seem to learn is in fact just remembering.

No one has ever seen a perfect circle, nor a perfectly straight line, yet everyone knows what a circle and a straight line are. Plato utilizes the tool-maker's blueprint as evidence that Forms are real:... when a man has discovered the instrument which is naturally adapted to each work, he must express this natural form, and not others which he fancies, in the material ...

Perceived circles or lines are not exactly circular or straight, and true circles and lines could never be detected since by definition they are sets of infinitely small points. But if the perfect ones were not real, how could they direct the manufacturer?

Plato was well aware of the limitations of the theory, as he offered his own criticisms of it in his dialogue "Parmenides". There Socrates is portrayed as a young philosopher acting as junior counterfoil to aged Parmenides. To a certain extent it is tongue-in-cheek as the older Socrates will have solutions to some of the problems that are made to puzzle the younger.

The dialogue does present a very real difficulty with the Theory of Forms, which Plato most likely only viewed as problems for later thought. These criticisms were later emphasized by Aristotle in rejecting an independently existing world of Forms. It is worth noting that Aristotle was a pupil and then a junior colleague of Plato; it is entirely possible that the presentation of "Parmenides" "sets up" for Aristotle; that is, they agreed to disagree.

One difficulty lies in the conceptualization of the "participation" of an object in a form (or Form). The young Socrates conceives of his solution to the problem of the universals in another metaphor, which though wonderfully apt, remains to be elucidated:
Nay, but the idea may be like the day which is one and the same in many places at once, and yet continuous with itself; in this way each idea may be one and the same in all at the same time.

But exactly how is a Form like the day in being everywhere at once? The solution calls for a distinct form, in which the particular instances, which are not identical to the form, participate; i.e., the form is shared out somehow like the day to many places. The concept of "participate", represented in Greek by more than one word, is as obscure in Greek as it is in English. Plato hypothesized that distinctness meant existence as an independent being, thus opening himself to the famous third man argument of Parmenides, which proves that forms cannot independently exist and be participated.

If universal and particulars – say man or greatness – all exist and are the same then the Form is not one but is multiple. If they are only like each other then they contain a form that is the same and others that are different. Thus if we presume that the Form and a particular are alike then there must be another, or third Form, man or greatness by possession of which they are alike. An infinite regression would then result; that is, an endless series of third men. The ultimate participant, greatness, rendering the entire series great, is missing. Moreover, any Form is not unitary but is composed of infinite parts, none of which is the proper Form.

The young Socrates (some may say the young Plato) did not give up the Theory of Forms over the Third Man but took another tack, that the particulars do not exist as such. Whatever they are, they "mime" the Forms, appearing to be particulars. This is a clear dip into representationalism, that we cannot observe the objects as they are in themselves but only their representations. That view has the weakness that if only the mimes can be observed then the real Forms cannot be known at all and the observer can have no idea of what the representations are supposed to represent or that they are representations.

Socrates' later answer would be that men already know the Forms because they were in the world of Forms before birth. The mimes only recall these Forms to memory. The comedian Aristophanes wrote a play, "The Clouds", poking fun of Socrates with his head in the clouds.

The topic of Aristotle's criticism of Plato's Theory of Forms is a large one and continues to expand. Rather than quote Plato, Aristotle often summarized. Classical commentaries thus recommended Aristotle as an introduction to Plato. As a historian of prior thought, Aristotle was invaluable, however this was secondary to his own dialectic and in some cases he treats purported implications as if Plato had actually mentioned them, or even defended them. In examining Aristotle's criticism of The Forms, it is helpful to understand Aristotle's own hylomorphic forms, by which he intends to salvage much of Plato's theory.

In the summary passage quoted above Plato distinguishes between real and non-real "existing things", where the latter term is used of substance. The figures that the artificer places in the gold are not substance, but gold is. Aristotle stated that, for Plato, all things studied by the sciences have Form and asserted that Plato considered only substance to have Form. Uncharitably, this leads him to something like a contradiction: Forms existing as the objects of science, but not-existing as non-substance. Scottish philosopher W.D. Ross objects to this as a mischaracterization of Plato.

Plato did not claim to know where the line between Form and non-Form is to be drawn. As Cornford points out, those things about which the young Socrates (and Plato) asserted "I have often been puzzled about these things" (in reference to Man, Fire and Water), appear as Forms in later works. However, others do not, such as Hair, Mud, Dirt. Of these, Socrates is made to assert, "it would be too absurd to suppose that they have a Form."

Ross also objects to Aristotle's criticism that Form Otherness accounts for the differences between Forms and purportedly leads to contradictory forms: the Not-tall, the Not-beautiful, etc. That particulars participate in a Form is for Aristotle much too vague to permit analysis. By one way in which he unpacks the concept, the Forms would cease to be of one essence due to any multiple participation. As Ross indicates, Plato didn't make that leap from "A is not B" to "A is Not-B." Otherness would only apply to its own particulars and not to those of other Forms. For example, there is no Form Not-Greek, only "particulars" of Form Otherness that somehow "suppress" Form Greek.

Regardless of whether Socrates meant the particulars of Otherness yield Not-Greek, Not-tall, Not-beautiful, etc., the particulars would operate specifically rather than generally, each somehow yielding only one exclusion.

Plato had postulated that we know Forms through a remembrance of the soul's past lives and Aristotle's arguments against this treatment of epistemology are compelling. For Plato, particulars somehow do not exist, and, on the face of it, "that which is non-existent cannot be known". See "Metaphysics" III 3–4.

Nominalism (from Latin "nomen", "name") says that ideal universals are mere names, human creations; the blueness shared by sky and blue jeans is a shared concept, communicated by our word "blueness". Blueness is held not to have any existence beyond that which it has in instances of blue things. This concept arose in the Middle Ages, as part of Scholasticism.

Scholasticism was a highly multinational, polyglottal school of philosophy, and the nominalist argument may be more obvious if an example is given in more than one language. For instance, colour terms are strongly variable by language; some languages consider blue and green the same colour, others have monolexemic terms for several shades of blue, which are considered different; other, like the Mandarin "qing" denote both blue and black. The German word "Stift" means a pen or a pencil, and also anything of the same shape. English does not have such a word. The English "pencil" originally meant "small paintbrush"; the term later included the silver rod used for silverpoint. The German "Bleistift" and "Silberstift" can both be called "Stift", but this term also includes felt-tip pens, which are clearly not pencils.

The shifting and overlapping nature of these concepts makes it easy to imagine them as mere names, with meanings not rigidly defined, but specific enough to be useful for communication. Given a group of objects, how is one to decide if it contains only instances of a single Form, or several mutually-exclusive Forms?

The theory is presented in the following dialogues:





</doc>
<doc id="59350035" url="https://en.wikipedia.org/wiki?curid=59350035" title="Mind in eastern philosophy">
Mind in eastern philosophy

The study of the mind in Eastern philosophy has parallels to the Western study of the Philosophy of mind as a branch of philosophy that studies the nature of the mind. Dualism and monism are the two central schools of thought on the mind–body problem in the Western tradition, although nuanced views have arisen that do not fit one or the other category neatly. Dualism is found in both Eastern and Western traditions (in the Sankhya and Yoga schools of Hindu philosophy as well as Plato) but its entry into Western philosophy was thanks to René Descartes in the 17th century. This article on mind in eastern philosophy deals with this subject from the standpoint of eastern philosophy which is historically strongly separated from the Western tradition and its approach to the Western philosophy of mind.

Substance Dualism is a common feature of several orthodox Hindu schools including the Sāṅkhya, Nyāya, Yoga and Dvaita Vedanta. In these schools a clear difference is drawn between matter and a non-material soul, which is eternal and undergoes samsara, a cycle of death and rebirth. The Nyāya school argued that qualities such as cognition and desire are inherent qualities which are not possessed by anything solely material, and therefore by process of elimination must belong to a non-material self, the atman. Many of these schools see their spiritual goal as moksha, liberation from the cycle of reincarnation.

In the Advaita Vedanta of the 8th century Indian philosopher Śaṅkara, the mind, body and world are all held to be appearances of the same unchanging eternal conscious entity called Brahman. Advaita, which means non-dualism, holds the view that all that exists is pure absolute consciousness. The fact that the world seems to be made up of changing entities is an illusion, or Maya. The only thing that exists is Brahman, which is described as Satchitananda (Being, consciousness and bliss). Advaita Vedanta is best described by a verse which states "Brahman is alone True, and this world of plurality is an error; the individual self is not different from Brahman."

Another form of monistic Vedanta is Vishishtadvaita (qualified non-dualism) as posited by the eleventh century philosopher Ramanuja. Ramanuja criticized Advaita Vedanta by arguing that consciousness is always intentional and that it is also always a property of something. Ramanuja's Brahman is defined by a multiplicity of qualities and properties in a single monistic entity. This doctrine is called "samanadhikaranya" (several things in a common substrate).

Arguably the first exposition of empirical materialism in the history of philosophy is in the Cārvāka school (also called Lokāyata). The Cārvāka school rejected the existence of anything but matter (which they defined as being made up of the four elements), including God and the soul. Therefore, they held that even consciousness was nothing but a construct made up of atoms. A section of the Cārvāka school believed in a material soul made up of air or breath, but since this also was a form of matter, it was not said to survive death.

Buddhist teachings describe that the mind manifests moment-to-moment as sense impressions and mental phenomena that are continuously changing. The moment-by-moment manifestation of the mind-stream has been described as happening in every person all the time, even in a scientist who analyses various phenomena in the world, or analyses the material body including the organ brain. The manifestation of the mind-stream is also described as being influenced by physical laws, biological laws, psychological laws, volitional laws, and universal laws.

A salient feature of Buddhist philosophy which sets it apart from Indian orthodoxy is the centrality of the doctrine of not-self (Pāli. anatta, Skt. anātman). The Buddha's not-self doctrine sees humans as an impermanent composite of five psychological and physical aspects instead of a single fixed self. In this sense, what is called ego or the self is merely a convenient fiction, an illusion that does not apply to anything real but to an erroneous way of looking at the ever-changing stream of five interconnected aggregate factors. The relationship between these aggregates is said to be one of dependent-arising (pratītyasamutpāda). This means that all things, including mental events, arise co-dependently from a plurality of other causes and conditions. This seems to reject both causal determinist and epiphenomenalist conceptions of mind.

Three centuries after the death of the Buddha (c. 150 BCE) saw the growth of a large body of literature called the Abhidharma in several contending Buddhist schools. In the Abhidharmic analysis of mind, the ordinary thought is defined as prapañca ('conceptual proliferation'). According to this theory, perceptual experience is bound up in multiple conceptualizations (expectations, judgments and desires). This proliferation of conceptualizations form our illusory superimposition of concepts like self and other upon an ever-changing stream of aggregate phenomena.
In this conception of mind no strict distinction is made between the conscious faculty and the actual sense perception of various phenomena. Consciousness is instead said to be divided into six sense modalities, five for the five senses and sixth for perception of mental phenomena. The arising of cognitive awareness is said to depend on sense perception, awareness of the mental faculty itself which is termed mental or 'introspective awareness' ("manovijñāna") and attention ("āvartana"), the picking out of objects out of the constantly changing stream of sensory impressions.

Rejection of a permanent agent eventually led to the philosophical problems of the seeming continuity of mind and also of explaining how rebirth and karma continue to be relevant doctrines without an eternal mind. This challenge was met by the Theravāda school by introducing the concept of mind as a factor of existence. This "life-stream" (Bhavanga-sota) is an undercurrent forming the condition of being. The continuity of a karmic "person" is therefore assured in the form of a mindstream (citta-santana), a series of flowing mental moments arising from the subliminal life-continuum mind (Bhavanga-citta), mental content, and attention.

The Sautrāntika school held a form of phenomenalism that saw the world as imperceptible. It held that external objects exist only as a support for cognition, which can only apprehend mental representations. This influenced the later Yogācāra school of Mahayana Buddhism. The Yogācāra school is often called the mind-only school because of its internalist stance that consciousness is the ultimate existing reality. The works of Vasubandhu have often been interpreted as arguing for some form of Idealism. Vasubandhu uses the dream argument and a mereological refutation of atomism to attack the reality of external objects as anything other than mental entities. Scholarly interpretations of Vasubandhu's philosophy vary widely, and include phenomenalism, neutral monism and realist phenomenology.

The Indian Mahayana schools were divided on the issue of the possibility of reflexive awareness ("svasaṃvedana"). Dharmakīrti accepted the idea of reflexive awareness as expounded by the Yogācāra school, comparing it to a lamp that illuminates itself while also illuminating other objects. This was strictly rejected by Mādhyamika scholars like Candrakīrti. Since in the philosophy of the Mādhyamika all things and mental events are characterized by emptiness, they argued that consciousness could not be an inherently reflexive ultimate reality since that would mean it was self-validating and therefore not characterized by emptiness. These views were ultimately reconciled by the 8th century thinker Śāntarakṣita. In Śāntarakṣita's synthesis he adopts the idealist Yogācāra views of reflexive awareness as a conventional truth into the structure of the two truths doctrine. Thus he states: "By relying on the Mind-Only system, know that external entities do not exist. And by relying on this Middle Way system, know that no self exists at all, even in that [mind]." 

The Yogācāra school also developed the theory of the repository consciousness ("ālayavijñāna") to explain continuity of mind in rebirth and accumulation of karma. This repository consciousness acts as a storehouse for karmic seeds (bija) when all other senses are absent during the process of death and rebirth as well as being the causal potentiality of dharmic phenomena. Thus according to B. Alan Wallace: 
No constituents of the body—in the brain or elsewhere—transform into mental states and processes. Such subjective experiences do not emerge from the body, but neither do they emerge from nothing. Rather, all objective mental appearances arise from the substrate, and all subjective mental states and processes arise from the substrate consciousness.

Tibetan Buddhist theories of mind evolved directly from the Indian Mahayana views. Thus the founder of the Gelug school, Je Tsongkhapa discusses the Yogācāra system of the Eight Consciousnesses in his "Explanation of the Difficult Points". He would later come to repudiate Śāntarakṣita's pragmatic idealism. 
According to the 14th Dalai Lama the mind can be defined "as an entity that has the nature of mere experience, that is, 'clarity and knowing'. It is the knowing nature, or agency, that is called mind, and this is non-material." The simultaneously dual nature of mind is as follows:
The 14th Dalai Lama has also explicitly laid out his theory of mind as experiential dualism which is described above under the different types of dualism.

Because Tibetan philosophy of mind is ultimately soteriological, it focuses on meditative practices such as Dzogchen and Mahamudra that allow a practitioner to experience the true reflexive nature of their mind directly. This unobstructed knowledge of one's primordial, empty and non-dual Buddha nature is called rigpa. The mind's innermost nature is described among various schools as pure luminosity or "clear light" ('od gsal) and is often compared to a crystal ball or a mirror. Sogyal Rinpoche speaks of mind thus:
"Imagine a sky, empty, spacious, and pure from the beginning; its essence is like this. Imagine a sun, luminous, clear, unobstructed, and spontaneously present; its nature is like this."

The central issue in Chinese Zen philosophy of mind is in the difference between the pure and awakened mind and the defiled mind. Chinese Chan master Huangpo described the mind as without beginning and without form or limit while the defiled mind was that which was obscured by attachment to form and concepts. The pure Buddha-mind is thus able to see things "as they truly are", as absolute and non-dual "thusness" (Tathatā). This non-conceptual seeing also includes the paradoxical fact that there is no difference between a defiled and a pure mind, as well as no difference between samsara and nirvana.

In the Shobogenzo, the Japanese philosopher Dogen argued that body and mind are neither ontologically nor phenomenologically distinct but are characterized by a oneness called "shin jin" (bodymind). According to Dogen, "casting off body and mind" ("Shinjin datsuraku") in zazen will allow one to experience things-as-they-are ("genjokoan") which is the nature of original enlightenment ("hongaku").



</doc>
<doc id="48778" url="https://en.wikipedia.org/wiki?curid=48778" title="Action theory (philosophy)">
Action theory (philosophy)

Action theory (or theory of action) is an area in philosophy concerned with theories about the processes causing willful human bodily movements of a more or less complex kind. This area of thought involves epistemology, ethics, metaphysics, jurisprudence, and philosophy of mind, and has attracted the strong interest of philosophers ever since Aristotle's "Nicomachean Ethics" (Third Book). With the advent of psychology and later neuroscience, many theories of action are now subject to empirical testing.

Philosophical action theory, or the philosophy of action, should not be confused with sociological theories of social action, such as the action theory established by Talcott Parsons. Nor should it be confused with Activity Theory.

Basic action theory typically describes action as behavior caused by an "agent" in a particular "situation". The agent's "desires" and "beliefs" (e.g. me wanting a glass of water and believing the clear liquid in the cup in front of me is water) lead to bodily behavior (e.g. reaching over for the glass). In the simple theory (see Donald Davidson), the desire and belief jointly cause the action. Michael Bratman has raised problems for such a view and argued that we should take the concept of intention as basic and not analyzable into beliefs and desires.

In some theories a desire plus a belief about the means of satisfying that desire are always what is behind an action. Agents aim, in acting, to maximize the satisfaction of their desires. Such a theory of prospective rationality underlies much of economics and other social sciences within the more sophisticated framework of rational choice. However, many theories of action argue that rationality extends far beyond calculating the best means to achieve one's ends. For instance, a belief that I ought to do X, in some theories, can directly cause me to do X without my having to want to do X (i.e. have a desire to do X). Rationality, in such theories, also involves responding correctly to the reasons an agent perceives, not just acting on wants.

While action theorists generally employ the language of causality in their theories of what the nature of action is, the issue of what causal determination comes to has been central to controversies about the nature of free will.

Conceptual discussions also revolve around a precise definition of action in philosophy. Scholars may disagree on which bodily movements fall under this category, e.g. whether thinking should be analysed as action, and how complex actions involving several steps to be taken and diverse intended consequences are to be summarised or decomposed.




</doc>
<doc id="106238" url="https://en.wikipedia.org/wiki?curid=106238" title="Cognition">
Cognition

Cognition is "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses". It encompasses many aspects of intellectual functions and processes such as attention, the formation of knowledge, memory and working memory, judgment and evaluation, reasoning and "computation", problem solving and decision making, comprehension and production of language. Cognitive processes use existing knowledge and generate new knowledge.

Cognitive processes are analyzed from different perspectives within different contexts, notably in the fields of linguistics, anesthesia, neuroscience, psychiatry, psychology, education, philosophy, anthropology, biology, systemics, logic, and computer science. These and other different approaches to the analysis of cognition are synthesised in the developing field of cognitive science, a progressively autonomous academic discipline.

The word "cognition" comes from the Latin verb "cognosco" ("con", 'with', and "gnōscō", 'know'; itself a cognate of the Greek verb γι(γ)νώσκω, "gi(g)nόsko", meaning 'I know, perceive'), meaning 'to conceptualize' or 'to recognize'.

The word "cognition" dates back to the 15th century, when it meant "thinking and awareness". Attention to cognitive processes came about more than eighteen centuries earlier, however, beginning with Aristotle (384–322 BC) and his interest in the inner workings of the mind and how they affect the human experience. Aristotle focused on cognitive areas pertaining to memory, perception, and mental imagery. He placed great importance on ensuring that his studies were based on empirical evidence, that is, scientific information that is gathered through observation and conscientious experimentation. Two millennia later, the groundwork for modern concepts of cognition was laid during the Enlightenment by thinkers such as John Locke and Dugald Stewart who sought to develop a model of the mind in which ideas were acquired, remembered and manipulated.

During the early nineteenth century cognitive models were developed both in philosophy, particularly by authors writing about the philosophy of mind, and within medicine, especially by physicians seeking to understand how to cure madness. Within Britain these models were studied in the academy by scholars such as James Sully at University College London, and they were even used by politicians when considering the national education act of 1870.

As psychology emerged as a burgeoning field of study in Europe and then gained a following in America, other scientists like Wilhelm Wundt, Herman Ebbinghaus, Mary Whiton Calkins, and William James would offer their contributions to the study of human cognition.

Wilhelm Wundt (1832–1920) emphasized the notion of what he called</ref> introspection: examining the inner feelings of an individual. With introspection, the subject had to be careful to describe their feelings in the most objective manner possible in order for Wundt to find the information scientific. Though Wundt's contributions are by no means minimal, modern psychologists find his methods to be quite subjective and choose to rely on more objective procedures of experimentation to make conclusions about the human cognitive process.

Hermann Ebbinghaus (1850–1909) conducted cognitive studies that mainly examined the function and capacity of human memory. Ebbinghaus developed his own experiment in which he constructed over 2,000 syllables made out of nonexistent words, for instance EAS. He then examined his own personal ability to learn these non-words. He purposely chose non-words as opposed to real words to control for the influence of pre-existing experience on what the words might symbolize, thus enabling easier recollection of them. Ebbinghaus observed and hypothesized a number of variables that may have affected his ability to learn and recall the non-words he created. One of the reasons, he concluded, was the amount of time between the presentation of the list of stimuli and the recitation or recall of same. Ebbinghaus was the first to record and plot a "learning curve", and a "forgetting curve". His work heavily influenced the study of serial position and its effect on memory, discussed in subsequent sections.

Mary Whiton Calkins (1863–1930) was an influential American pioneer in the realm of psychology. Her work also focused on the human memory capacity. A common theory, called the recency effect, can be attributed to the studies that she conducted. The recency effect, also discussed in the subsequent experiment section, is the tendency for individuals to be able to accurately recollect the final items presented in a sequence of stimuli. Calkin's theory is closely related to the aforementioned study and conclusion of the memory experiments conducted by Hermann Ebbinghaus.

William James (1842–1910) is another pivotal figure in the history of cognitive science. James was quite discontent with Wundt's emphasis on introspection and Ebbinghaus' use of nonsense stimuli. He instead chose to focus on the human learning experience in everyday life and its importance to the study of cognition. James' most significant contribution to the study and theory of cognition was his textbook "Principles of Psychology" that preliminarily examines aspects of cognition such as perception, memory, reasoning, and attention.

René Descartes (1596-1650) was a seventeenth-century philosopher who came up with the phrase "Cogito, ergo sum." Which means "I think, therefore I am." He took a philosophical approach to the study of cognition and the mind, with his Meditations he wanted people to meditate along with him to come to the same conclusions as he did but in their own free cognition. 

In psychology, the term "cognition" is usually used within an information processing view of an individual's psychological functions (see cognitivism), and it is the same in cognitive engineering; in a branch of social psychology called social cognition, the term is used to explain attitudes, attribution, and group dynamics.

Human cognition is conscious and unconscious, concrete or abstract, as well as intuitive (like knowledge of a language) and conceptual (like a model of a language). It encompasses processes such as memory, association, concept formation, pattern recognition, language, attention, perception, action, problem solving and mental imagery. Traditionally, emotion was not thought of as a cognitive process, but now much research is being undertaken to examine the cognitive psychology of emotion; research is also focused on one's awareness of one's own strategies and methods of cognition, which is called metacognition.

While few people would deny that cognitive processes are a function of the brain, a cognitive theory will not necessarily make reference to the brain or to biological processes (compare neurocognitive). It may purely describe behavior in terms of information flow or function. Relatively recent fields of study such as neuropsychology aim to bridge this gap, using cognitive paradigms to understand how the brain implements the information-processing functions (see also cognitive neuroscience), or to understand how pure information-processing systems (e.g., computers) can simulate human cognition (see also artificial intelligence). The branch of psychology that studies brain injury to infer normal cognitive function is called cognitive neuropsychology. The links of cognition to evolutionary demands are studied through the investigation of animal cognition.

For years, sociologists and psychologists have conducted studies on cognitive development or the construction of human thought or mental processes.

Jean Piaget was one of the most important and influential people in the field of Developmental Psychology. He believed that humans are unique in comparison to animals because we have the capacity to do "abstract symbolic reasoning". His work can be compared to Lev Vygotsky, Sigmund Freud, and Erik Erikson who were also great contributors in the field of Developmental Psychology. Today, Piaget is known for studying the cognitive development in children. He studied his own three children and their intellectual development and came up with a theory that describes the stages children pass through during development.

Serial position

The serial position experiment is meant to test a theory of memory that states that when information is given in a serial manner, we tend to remember information in the beginning of the sequence, called the primacy effect, and information in the end of the sequence, called the recency effect. Consequently, information given in the middle of the sequence is typically forgotten, or not recalled as easily. This study predicts that the recency effect is stronger than the primacy effect, because the information that is most recently learned is still in working memory when asked to be recalled. Information that is learned first still has to go through a retrieval process. This experiment focuses on human memory processes.

Word superiority

The word superiority experiment presents a subject with a word, or a letter by itself, for a brief period of time, i.e. 40ms, and they are then asked to recall the letter that was in a particular location in the word. By theory, the subject should be better able to correctly recall the letter when it was presented in a word than when it was presented in isolation. This experiment focuses on human speech and language.

Brown-Peterson

In the Brown-Peterson experiment, participants are briefly presented with a trigram and in one particular version of the experiment, they are then given a distractor task, asking them to identify whether a sequence of words are in fact words, or non-words (due to being misspelled, etc.). After the distractor task, they are asked to recall the trigram from before the distractor task. In theory, the longer the distractor task, the harder it will be for participants to correctly recall the trigram. This experiment focuses on human short-term memory.

Memory span

During the memory span experiment, each subject is presented with a sequence of stimuli of the same kind; words depicting objects, numbers, letters that sound similar, and letters that sound dissimilar. After being presented with the stimuli, the subject is asked to recall the sequence of stimuli that they were given in the exact order in which it was given. In one particular version of the experiment, if the subject recalled a list correctly, the list length was increased by one for that type of material, and vice versa if it was recalled incorrectly. The theory is that people have a memory span of about seven items for numbers, the same for letters that sound dissimilar and short words. The memory span is projected to be shorter with letters that sound similar and with longer words.

Visual search

In one version of the visual search experiment, a participant is presented with a window that displays circles and squares scattered across it. The participant is to identify whether there is a green circle on the window. In the "featured" search, the subject is presented with several trial windows that have blue squares or circles and one green circle or no green circle in it at all. In the "conjunctive" search, the subject is presented with trial windows that have blue circles or green squares and a present or absent green circle whose presence the participant is asked to identify. What is expected is that in the feature searches, reaction time, that is the time it takes for a participant to identify whether a green circle is present or not, should not change as the number of distractors increases. Conjunctive searches where the target is absent should have a longer reaction time than the conjunctive searches where the target is present. The theory is that in feature searches, it is easy to spot the target, or if it is absent, because of the difference in color between the target and the distractors. In conjunctive searches where the target is absent, reaction time increases because the subject has to look at each shape to determine whether it is the target or not because some of the distractors if not all of them, are the same color as the target stimuli. Conjunctive searches where the target is present take less time because if the target is found, the search between each shape stops.

Knowledge representation

The semantic network of knowledge representation systems has been studied in various paradigms. One of the oldest paradigms is the leveling and sharpening of stories as they are repeated from memory studied by Bartlett. The semantic differential used factor analysis to determine the main meanings of words, finding that value or "goodness" of words is the first factor. More controlled experiments examine the categorical relationships of words in free recall. The hierarchical structure of words has been explicitly mapped in George Miller's Wordnet. More dynamic models of semantic networks have been created and tested with neural network experiments based on computational systems such as latent semantic analysis (LSA), Bayesian analysis, and multidimensional factor analysis. The semantics (meaning) of words is studied by all the disciplines of cognitive science.

An emergent field of research, referred to as "Team Cognition", is arising in military sciences. "Team cognition" indicates “an emergent property of teams that results from the interplay of individual cognition and team process behaviors [...] [Team cognition] underlies team performance” (Arizona State University East, 2005, Cooke NJ, 2005).





</doc>
<doc id="37080" url="https://en.wikipedia.org/wiki?curid=37080" title="Thought">
Thought

Thought encompasses an "aim-oriented flow of ideas and associations that can lead to a reality-oriented conclusion". Although thinking is an activity of an existential value for humans, there is still no consensus as to how it is adequately defined or understood.

Because thought underlies many human actions and interactions, understanding its physical and metaphysical origins and its effects has been a longstanding goal of many academic disciplines including philosophy, linguistics, psychology, neuroscience, artificial intelligence, biology, sociology and cognitive science.

Thinking allows humans to make sense of, interpret, represent or model the world they experience, and to make predictions about that world. It is therefore helpful to an organism with needs, objectives, and desires as it makes plans or otherwise attempts to accomplish those goals.

The word "thought" comes from Old English "þoht", or "geþoht", from stem of "þencan" "to conceive of in the mind, consider".

The word "thought" may mean:

Definitions may or may not require that thought

Definitions of thought may also be derived directly or indirectly from theories of thought.


The phenomenology movement in philosophy saw a radical change in the way in which we understand thought. Martin Heidegger's phenomenological analyses of the existential structure of man in "Being and Time" cast new light on the issue of thinking, unsettling traditional cognitive or rational interpretations of man which affect the way we understand thought. The notion of the fundamental role of non-cognitive understanding in rendering possible thematic consciousness informed the discussion surrounding artificial intelligence (AI) during the 1970s and 1980s.

Phenomenology, however, is not the only approach to thinking in modern Western philosophy. Philosophy of mind is a branch of philosophy that studies the nature of the mind, mental events, mental functions, mental properties, consciousness and their relationship to the physical body, particularly the brain. The mind–body problem, i.e. the relationship of the mind to the body, is commonly seen as the central issue in philosophy of mind, although there are other issues concerning the nature of the mind that do not involve its relation to the physical body.

The mind–body problem concerns the explanation of the relationship that exists between minds, or mental processes, and bodily states or processes. The main aim of philosophers working in this area is to determine the nature of the mind and mental states/processes, and how—or even if—minds are affected by and can affect the body.

Human perceptual experiences depend on stimuli which arrive at one's various sensory organs from the external world and these stimuli cause changes in one's mental state, ultimately causing one to feel a sensation, which may be pleasant or unpleasant. Someone's desire for a slice of pizza, for example, will tend to cause that person to move his or her body in a specific manner and in a specific direction to obtain what he or she wants. The question, then, is how it can be possible for conscious experiences to arise out of a lump of gray matter endowed with nothing but electrochemical properties. A related problem is to explain how someone's propositional attitudes (e.g. beliefs and desires) can cause that individual's neurons to fire and his muscles to contract in exactly the correct manner. These comprise some of the puzzles that have confronted epistemologists and philosophers of mind from at least the time of René Descartes.

The above reflects a classical, functional description of how we work as cognitive, thinking systems. However the apparently irresolvable mind–body problem is said to be overcome, and bypassed, by the embodied cognition approach, with its roots in the work of Heidegger, Piaget, Vygotsky, Merleau-Ponty and the pragmatist John Dewey.

This approach states that the classical approach of separating the mind and analysing its processes is misguided: instead, we should see that the mind, actions of an embodied agent, and the environment it perceives and envisions, are all parts of a whole which determine each other. Therefore, functional analysis of the mind alone will always leave us with the mind–body problem which cannot be solved.

A neuron (also known as a neurone or nerve cell) is an excitable cell in the nervous system that processes and transmits information by electrochemical signaling. Neurons are the core components of the brain, the vertebrate spinal cord, the invertebrate ventral nerve cord and the peripheral nerves. A number of specialized types of neurons exist: sensory neurons respond to touch, sound, light and numerous other stimuli affecting cells of the sensory organs that then send signals to the spinal cord and brain. Motor neurons receive signals from the brain and spinal cord that cause muscle contractions and affect glands. Interneurons connect neurons to other neurons within the brain and spinal cord. Neurons respond to stimuli, and communicate the presence of stimuli to the central nervous system, which processes that information and sends responses to other parts of the body for action. Neurons do not go through mitosis and usually cannot be replaced after being destroyed, although astrocytes have been observed to turn into neurons, as they are sometimes pluripotent.

Psychologists have concentrated on thinking as an intellectual exertion aimed at finding an answer to a question or the solution of a practical problem. Cognitive psychology is a branch of psychology that investigates internal mental processes such as problem solving, memory, and language. The school of thought arising from this approach is known as cognitivism, which is interested in how people mentally represent information processing. It had its foundations in the Gestalt psychology of Max Wertheimer, Wolfgang Köhler, and Kurt Koffka, and in the work of Jean Piaget, who provided a theory of stages/phases that describes children's cognitive development.

Cognitive psychologists use psychophysical and experimental approaches to understand, diagnose, and solve problems, concerning themselves with the mental processes which mediate between stimulus and response. They study various aspects of thinking, including the psychology of reasoning, and how people make decisions and choices, solve problems, as well as engage in creative discovery and imaginative thought. Cognitive theory contends that solutions to problems either take the form of algorithms: rules that are not necessarily understood but promise a solution, or of heuristics: rules that are understood but that do not always guarantee solutions. Cognitive science differs from cognitive psychology in that algorithms that are intended to simulate human behavior are implemented or implementable on a computer. In other instances, solutions may be found through insight, a sudden awareness of relationships.

In developmental psychology, Jean Piaget was a pioneer in the study of the development of thought from birth to maturity. In his theory of cognitive development, thought is based on actions on the environment. That is, Piaget suggests that the environment is understood through assimilations of objects in the available schemes of action and these accommodate to the objects to the extent that the available schemes fall short of the demands. As a result of this interplay between assimilation and accommodation, thought develops through a sequence of stages that differ qualitatively from each other in mode of representation and complexity of inference and understanding. That is, thought evolves from being based on perceptions and actions at the sensorimotor stage in the first two years of life to internal representations in early childhood. Subsequently, representations are gradually organized into logical structures which first operate on the concrete properties of the reality, in the stage of concrete operations, and then operate on abstract principles that organize concrete properties, in the stage of formal operations. In recent years, the Piagetian conception of thought was integrated with information processing conceptions. Thus, thought is considered as the result of mechanisms that are responsible for the representation and processing of information. In this conception, speed of processing, cognitive control, and working memory are the main functions underlying thought. In the neo-Piagetian theories of cognitive development, the development of thought is considered to come from increasing speed of processing, enhanced cognitive control, and increasing working memory.

Positive psychology emphasizes the positive aspects of human psychology as equally important as the focus on mood disorders and other negative symptoms. In "Character Strengths and Virtues", Peterson and Seligman list a series of positive characteristics. One person is not expected to have every strength, nor are they meant to fully capsulate that characteristic entirely. The list encourages positive thought that builds on a person's strengths, rather than how to "fix" their "symptoms".

The "id", "ego" and "super-ego" are the three parts of the "psychic apparatus" defined in Sigmund Freud's structural model of the psyche; they are the three theoretical constructs in terms of whose activity and interaction mental life is described. According to this model, the uncoordinated instinctual trends are encompassed by the "id", the organized realistic part of the psyche is the "ego", and the critical, moralizing function is the "super-ego".

For psychoanalysis, the unconscious does not include all that is not conscious, rather only what is actively repressed from conscious thought or what the person is averse to knowing consciously. In a sense this view places the self in relationship to their unconscious as an adversary, warring with itself to keep what is unconscious hidden. If a person feels pain, all he can think of is alleviating the pain. Any of his desires, to get rid of pain or enjoy something, command the mind what to do. For Freud, the unconscious was a repository for socially unacceptable ideas, wishes or desires, traumatic memories, and painful emotions put out of mind by the mechanism of psychological repression. However, the contents did not necessarily have to be solely negative. In the psychoanalytic view, the unconscious is a force that can only be recognized by its effects—it expresses itself in the symptom.

Social psychology is the study of how people and groups interact. Scholars in this interdisciplinary area are typically either psychologists or sociologists, though all social psychologists employ both the individual and the group as their units of analysis.

Despite their similarity, psychological and sociological researchers tend to differ in their goals, approaches, methods, and terminology. They also favor separate academic journals and professional societies. The greatest period of collaboration between sociologists and psychologists was during the years immediately following World War II. Although there has been increasing isolation and specialization in recent years, some degree of overlap and influence remains between the two disciplines.

The collective unconscious, sometimes known as collective subconscious, is a term of analytical psychology, coined by Carl Jung. It is a part of the unconscious mind, shared by a society, a people, or all humanity, in an interconnected system that is the product of all common experiences and contains such concepts as science, religion, and morality. While Freud did not distinguish between "individual psychology" and "collective psychology", Jung distinguished the collective unconscious from the personal subconscious particular to each human being. The collective unconscious is also known as "a reservoir of the experiences of our species".

In the "Definitions" chapter of Jung's seminal work "Psychological Types", under the definition of "collective" Jung references "representations collectives", a term coined by Lucien Lévy-Bruhl in his 1910 book "How Natives Think". Jung says this is what he describes as the collective unconscious. Freud, on the other hand, did not accept the idea of a collective unconscious.





</doc>
<doc id="317501" url="https://en.wikipedia.org/wiki?curid=317501" title="Introspection">
Introspection

Introspection is the examination of one's own conscious thoughts and feelings. In psychology, the process of introspection relies exclusively on observation of one's mental state, while in a spiritual context it may refer to the examination of one's soul. Introspection is closely related to human self-reflection and self-discovery and is contrasted with external observation.

Introspection generally provides a privileged access to one's own mental states, not mediated by other sources of knowledge, so that individual experience of the mind is unique. Introspection can determine any number of mental states including: sensory, bodily, cognitive, emotional and so forth.

Introspection has been a subject of philosophical discussion for thousands of years. The philosopher Plato asked, "…why should we not calmly and patiently review our own thoughts, and thoroughly examine and see what these appearances in us really are?" While introspection is applicable to many facets of philosophical thought it is perhaps best known for its role in epistemology; in this context introspection is often compared with perception, reason, memory, and testimony as a source of knowledge.

It has often been claimed that Wilhelm Wundt, the father of modern psychology, was the first to adopt introspection to experimental psychology though the methodological idea had been presented long before, as by 18th century German philosopher-psychologists such as Alexander Gottlieb Baumgarten or Johann Nicolaus Tetens. Also, Wundt's views on introspection must be approached with great care. Wundt was influenced by notable physiologists, such as Gustav Fechner, who used a kind of controlled introspection as a means to study human sensory organs. Building upon the pre-existing use of introspection in physiology, Wundt believed the method of introspection was the ability to observe an experience, not just the logical reflection or speculations which some others interpreted his meaning to be. Wundt imposed exacting control over the use of introspection in his experimental laboratory at the University of Leipzig, making it possible for other scientists to replicate his experiments elsewhere, a development that proved essential to the development of psychology as a modern, peer-reviewed scientific discipline. Such exact purism was typical of Wundt and he instructed all introspection observations be performed under these same instructions: "1) the Observer must, if possible, be in a position to determine beforehand the entrance of the process to be observed. 2) the introspectionist must, as far as possible, grasp the phenomenon in a state of strained attention and follow its course. 3) Every observation must, in order to make certain, be capable of being repeated several times under the same conditions and 4) the conditions under which the phenomenon appears must be found out by the variation of the attendant circumstances and when this was done the various coherent experiments must be varied according to a plan partly by eliminating certain stimuli and partly by grading their strength and quality".

Edward Titchener was an early pioneer in experimental psychology and student of Wilhelm Wundt. After earning his doctorate under the tutelage of Wundt at the University of Leipzig, he made his way to Cornell University, where he established his own laboratory and research. When Titchener arrived at Cornell in 1894, psychology was still a fledgling discipline, especially in the United States, and Titchener was a key figure in bringing Wundt's ideas to America. However, Titchener misrepresented some of Wundt's ideas to the American psychological establishment, especially in his account of introspection which, Titchener taught, only served a purpose in the qualitative analysis of consciousness into its various parts, while Wundt saw it as a means to quantitatively measure the whole of conscious experience. Titchener was exclusively interested in the individual components that comprise conscious experience, while Wundt, seeing little purpose in the analysis of individual components, focused on synthesis of these components. Ultimately, Titchener's ideas would form the basis of the short-lived psychological theory of structuralism.

American historiography of introspection, according to some authors, is dominated by three misconceptions. In particular, historians of psychology tend to argue 1) that introspection once was the dominant method of psychological inquiry, 2) that behaviorism, and in particular John B. Watson, is responsible for discrediting introspection as a valid method, and 3) that scientific psychology completely abandoned introspection as a result of those critiques. Yet, introspection has not been the dominant method. It is believed to be so because Edward Titchener's student Edwin G. Boring, in his influential historical accounts of experimental psychology, privileged Titchener's views while giving little credit to original sources. Introspection has been critiqued by many other psychologists, including Wilhelm Wundt, and Knight Dunlap who in his article ""The Case Against Introspection"", presents an argument against self-observation that is not primarily rooted in behaviorist epistemology. Introspection is still widely used in psychology, but under different names, such as self-report surveys, interviews and fMRIs. It is not the method but rather its name that has been dropped from the dominant psychological vocabulary.

Partly as a result of Titchener's misrepresentation, the use of introspection diminished after his death and the subsequent decline of structuralism. Later psychological movements, such as functionalism and behaviorism, rejected introspection for its lack of scientific reliability among other factors. Functionalism originally arose in direct opposition to structuralism, opposing its narrow focus on the elements of consciousness and emphasising the purpose of consciousness and other psychological behavior. Behaviorism's objection to introspection focused much more on its unreliability and subjectivity which conflicted with behaviorism's focus on measurable behavior.

The more recently established cognitive psychology movement has to some extent accepted introspection's usefulness in the study of psychological phenomena, though generally only in experiments pertaining to internal thought conducted under experimental conditions. For example, in the "think aloud protocol", investigators cue participants to speak their thoughts aloud in order to study an active thought process without forcing an individual to comment on the process itself.

Already in the 18th century authors had criticized the use of introspection, both for knowing one's own mind and as a method for psychology. David Hume pointed out that introspecting a mental state tends to alter the very state itself; a German author, Christian Gottfried Schütz, noted that introspection is often described as mere "inner sensation", but actually requires also attention, that introspection does not get at unconscious mental states, and that it cannot be used naively - one needs to know what to look for. Immanuel Kant added that, if they are understood too narrowly, introspective experiments are impossible. Introspection delivers, at best, hints about what goes on in the mind; it does not suffice to justify knowledge claims about the mind. Similarly, the idea continued to be discussed between John Stuart Mill and Auguste Comte. 
Recent psychological research on cognition and attribution has asked people to report on their mental processes, for instance to say why they made a particular choice or how they arrived at a judgment. In some situations, these reports are clearly confabulated. For example, people justify choices they have not in fact made. Such results undermine the idea that those verbal reports are based on direct introspective access to mental content. Instead, judgements about one's own mind seem to be inferences from overt behavior, similar to judgements made about another person. However, it is hard to assess whether these results only apply to unusual experimental situations, or if they reveal something about everyday introspection. The theory of the adaptive unconscious suggests that a very large proportion of mental processes, even "high-level" processes like goal-setting and decision-making, are inaccessible to introspection.
Indeed, it is questionable how confident researchers can be in their own introspections.

One of the central implications of dissociations between consciousness and meta-consciousness is that individuals, presumably including researchers, can misrepresent their experiences to themselves. Jack and Roepstorff assert, '...there is also a sense in which subjects simply cannot be wrong about their own experiential states.' Presumably they arrived at this conclusion by drawing on the seemingly self-evident quality of their own introspections, and assumed that it must equally apply to others. However, when we consider research on the topic, this conclusion seems less self-evident. If, for example, extensive introspection can cause people to make decisions that they later regret, then one very reasonable possibility is that the introspection caused them to 'lose touch with their feelings'. In short, empirical studies suggest that people can fail to appraise adequately (i.e. are wrong about) their own experiential states.

Another question in regards to the veracious accountability of introspection is if researchers lack the confidence in their own introspections and those of their participants, then how can it gain legitimacy? Three strategies are accountable: identifying behaviors that establish credibility, finding common ground that enables mutual understanding, and developing a trust that allows one to know when to give the benefit of the doubt.
That is to say, that words are only meaningful if validated by one's actions; When people report strategies, feelings or beliefs, their behaviors must correspond with these statements if they are to be believed.

Even when their introspections are uninformative, people still give confident descriptions of their mental processes, being "unaware of their unawareness". This phenomenon has been termed the "introspection illusion" and has been used to explain some cognitive biases and belief in some paranormal phenomena. When making judgements about themselves, subjects treat their own introspections as reliable, whereas they judge other people based on their behavior. This can lead to illusions of superiority. For example, people generally see themselves as less conformist than others, and this seems to be because they do not introspect any urge to conform. Another reliable finding is that people generally see themselves as less biased than everyone else, because they are not likely to introspect any biased thought processes. These introspections are misleading, however, because biases work unconsciously.

One experiment tried to give their subjects access to others' introspections. They made audio recordings of subjects who had been told to say whatever came into their heads as they answered a question about their own bias. Although subjects persuaded themselves they were unlikely to be biased, their introspective reports did not sway the assessments of observers. When subjects were explicitly told to avoid relying on introspection, their assessments of their own bias became more realistic.

The 2020 study distinguish various sources of errors that may accompany introspection and classify four main errors: attentional, attributional, conceptual, and expressional error. Furthermore, methodological recommendations for the possible elimination of these errors have been determined in the study: "1) a better focusing of the subject’s attention to their mental processes, 2) providing suitable stimuli, and 3) the sharing of introspective experience between subjects."

In Eastern Christianity some concepts addressing human needs, such as sober introspection "(nepsis"), require watchfulness of the human heart and the conflicts of the human "nous", heart or mind. Noetic understanding can not be achieved by rational or discursive thought (i.e. systemization).

Jains practise "pratikraman" (Sanskrit "introspection"), a process of repentance of wrongdoings during their daily life, and remind themselves to refrain from doing so again. Devout Jains often do Pratikraman at least twice a day.

Introspection is encouraged in schools such as Advaita Vedanta; in order for one to know their own true nature, they need to reflect and introspect on their true nature—which is what meditation is. Especially, Swami Chinmayananda emphasised the role of introspection in five stages, outlined in his book "Self Unfoldment."

Introspection (also referred to as Rufus dialogue, interior monologue, self-talk) is the fiction-writing mode used to convey a character's thoughts. As explained by Renni Browne and Dave King, "One of the great gifts of literature is that it allows for the expression of unexpressed thoughts…"

According to Nancy Kress, a character's thoughts can greatly enhance a story: deepening characterization, increasing tension, and widening the scope of a story. As outlined by Jack M. Bickham, thought plays a critical role in both scene and sequel.





</doc>
<doc id="309909" url="https://en.wikipedia.org/wiki?curid=309909" title="Aristotelianism">
Aristotelianism

Aristotelianism ( ) is a tradition of philosophy that takes its defining inspiration from the work of Aristotle. This school of thought, in the modern sense of philosophy, covers existence, ethics, mind and related subjects. In Aristotle's time, philosophy included natural philosophy, which preceded the advent of modern science during the Scientific Revolution. The works of Aristotle were initially defended by the members of the Peripatetic school and later on by the Neoplatonists, who produced many commentaries on Aristotle's writings. In the Islamic Golden Age, Avicenna and Averroes translated the works of Aristotle into Arabic and under them, along with philosophers such as Al-Kindi and Al-Farabi, Aristotelianism became a major part of early Islamic philosophy.

Moses Maimonides adopted Aristotelianism from the Islamic scholars and based his "Guide for the Perplexed" on it and that became the basis of Jewish scholastic philosophy. Although some of Aristotle's logical works were known to western Europe, it was not until the Latin translations of the 12th century that the works of Aristotle and his Arabic commentators became widely available. Scholars such as Albertus Magnus and Thomas Aquinas interpreted and systematized Aristotle's works in accordance with Catholic theology.

After retreating under criticism from modern natural philosophers, the distinctively Aristotelian idea of teleology was transmitted through Wolff and Kant to Hegel, who applied it to history as a totality. Although this project was criticized by Trendelenburg and Brentano as non-Aristotelian, Hegel's influence is now often said to be responsible for an important Aristotelian influence upon Marx. 

Recent Aristotelian ethical and "practical" philosophy, such as that of Gadamer and McDowell, is often premissed upon a rejection of Aristotelianism's traditional metaphysical or theoretical philosophy. From this viewpoint, the early modern tradition of political republicanism, which views the "res publica", public sphere or state as constituted by its citizens' virtuous activity, can appear thoroughly Aristotelian.

The most famous contemporary Aristotelian philosopher is Alasdair MacIntyre. Especially famous for helping to revive virtue ethics in his book "After Virtue", MacIntyre revises Aristotelianism with the argument that the highest temporal goods, which are internal to human beings, are actualized through participation in social practices. He juxtaposes Aristotelianism with the managerial institutions of capitalism and its state, and with rival traditions — including the philosophies of Hume and Nietzsche — that reject Aristotle's idea of essentially human goods and virtues and instead legitimate capitalism. Therefore, on MacIntyre's account, Aristotelianism is not identical with Western philosophy as a whole; rather, it is "the best theory so far, [including] the best theory so far about what makes a particular theory the best one." Politically and socially, it has been characterized as a newly "revolutionary Aristotelianism". This may be contrasted with the more conventional, apolitical and effectively conservative uses of Aristotle by, for example, Gadamer and McDowell. Other important contemporary Aristotelian theorists include Fred D. Miller, Jr. in politics and Rosalind Hursthouse in ethics.

The original followers of Aristotle were the members of the Peripatetic school. The most prominent members of the school after Aristotle were Theophrastus and Strato of Lampsacus, who both continued Aristotle's researches. During the Roman era the school concentrated on preserving and defending his work. The most important figure in this regard was Alexander of Aphrodisias who commentated on Aristotle's writings. With the rise of Neoplatonism in the 3rd century, Peripateticism as an independent philosophy came to an end, but the Neoplatonists sought to incorporate Aristotle's philosophy within their own system, and produced many commentaries on Aristotle.

Byzantine Aristotelianism emerged in the Byzantine Empire in the form of Aristotelian paraphrase: adaptations in which Aristotle's text is rephrased, reorganized, and pruned, in order to make it more easily understood. This genre was allegedly invented by Themistius in the mid-4th century, revived by Michael Psellos in the mid-11th century, and further developed by Sophonias in the late 13th to early 14th centuries.

Leo the Mathematician was appointed to the chair of philosophy at the Magnaura School in the mid-9th century to teach Aristotelian logic. The 11th and 12th centuries saw the emergence of twelfth-century Byzantine Aristotelianism. Before the 12th century, the whole Byzantine output of Aristotelian commentaries was focused on logic. However, the range of subjects covered by the Aristotelian commentaries produced in the two decades after 1118 is much greater due to the initiative of the princess Anna Comnena who commissioned a number of scholars to write commentaries on previously neglected works of Aristotle.

In the Abbasid Empire, many foreign works were translated into Arabic, large libraries were constructed, and scholars were welcomed. Under the caliphs Harun al-Rashid and his son Al-Ma'mun, the House of Wisdom in Baghdad flourished. Christian scholar Hunayn ibn Ishaq (809–873) was placed in charge of the translation work by the caliph. In his lifetime, Ishaq translated 116 writings, including works by Plato and Aristotle, into Syriac and Arabic.

With the founding of House of Wisdom, the entire corpus of Aristotelian works that had been preserved (excluding the "Eudemian Ethics", "Magna Moralia" and "Politics") became available, along with its Greek commentators; this corpus laid a uniform foundation for Islamic Aristotelianism.

Al-Kindi (801–873) was the first of the Muslim Peripatetic philosophers, and is known for his efforts to introduce Greek and Hellenistic philosophy to the Arab world. He incorporated Aristotelian and Neoplatonist thought into an Islamic philosophical framework. This was an important factor in the introduction and popularization of Greek philosophy in the Muslim intellectual world.

The philosopher Al-Farabi (872–950) had great influence on science and philosophy for several centuries, and in his time was widely thought second only to Aristotle in knowledge (alluded to by his title of "the Second Teacher"). His work, aimed at synthesis of philosophy and Sufism, paved the way for the work of Avicenna (980–1037). Avicenna was one of the main interpreters of Aristotle. The school of thought he founded became known as Avicennism, which was built on ingredients and conceptual building blocks that are largely Aristotelian and Neoplatonist.

At the western end of the Mediterranean Sea, during the reign of Al-Hakam II (961 to 976) in Córdoba, a massive translation effort was undertaken, and many books were translated into Arabic. Averroes (1126–1198), who spent much of his life in Cordoba and Seville, was especially distinguished as a commentator of Aristotle. He often wrote two or three different commentaries on the same work, and some 38 commentaries by Averroes on the works of Aristotle have been identified. Although his writings had only marginal impact in Islamic countries, his works would eventually have a huge impact in the Latin West, and would lead to the school of thought known as Averroism.

Although some knowledge of Aristotle seems to have lingered on in the ecclesiastical centres of western Europe after the fall of the Roman empire, by the ninth century nearly all that was known of Aristotle consisted of Boethius's commentaries on the "Organon", and a few abridgments made by Latin authors of the declining empire, Isidore of Seville and Martianus Capella. From that time until the end of the eleventh century, little progress is apparent in Aristotelian knowledge.

The renaissance of the 12th century saw a major search by European scholars for new learning. James of Venice, who probably spent some years in Constantinople, translated Aristotle's "Posterior Analytics" from Greek into Latin in the mid-twelfth century, thus making the complete Aristotelian logical corpus, the "Organon", available in Latin for the first time. Scholars travelled to areas of Europe that once had been under Muslim rule and still had substantial Arabic-speaking populations. From central Spain, which had returned to Christian rule in the eleventh century, scholars produced many of the Latin translations of the 12th century. The most productive of these translators was Gerard of Cremona, (c. 1114–1187), who translated 87 books, which included many of the works of Aristotle such as his "Posterior Analytics", "Physics", "On the Heavens", "On Generation and Corruption", and "Meteorology". Michael Scot (c. 1175–1232) translated Averroes' commentaries on the scientific works of Aristotle.

Aristotle's physical writings began to be discussed openly, and at a time when Aristotle's method was permeating all theology, these treatises were sufficient to cause his prohibition for heterodoxy in the Condemnations of 1210–1277. In the first of these, in Paris in 1210, it was stated that "neither the books of Aristotle on natural philosophy or their commentaries are to be read at Paris in public or secret, and this we forbid under penalty of excommunication." However, despite further attempts to restrict the teaching of Aristotle, by 1270 the ban on Aristotle's natural philosophy was ineffective.

William of Moerbeke (c. 1215–1286) undertook a complete translation of the works of Aristotle or, for some portions, a revision of existing translations. He was the first translator of the "Politics" (c. 1260) from Greek into Latin. Many copies of Aristotle in Latin then in circulation were assumed to have been influenced by Averroes, who was suspected of being a source of philosophical and theological errors found in the earlier translations of Aristotle. Such claims were without merit, however, as the "Alexandrian" Aristotelianism of Averroes followed "the strict study of the text of Aristotle, which was introduced by Avicenna, [because] a large amount of traditional Neoplatonism was incorporated with the body of traditional Aristotelianism".

Albertus Magnus (c. 1200–1280) was among the first medieval scholars to apply Aristotle's philosophy to Christian thought. He produced paraphrases of most of the works of Aristotle available to him. He digested, interpreted and systematized the whole of Aristotle's works, gleaned from the Latin translations and notes of the Arabian commentators, in accordance with Church doctrine. His efforts resulted in the formation of a Christian reception of Aristotle in the Western Europe. Magnus did not repudiate Plato. In that, he belonged to the dominant tradition of philosophy that preceded him, namely the "concordist tradition", which sought to harmonize Aristotle with Plato through interpretation (see for example Porphyry's "On Plato and Aristotle Being Adherents of the Same School"). Magnus famously wrote:

Thomas Aquinas (1225–1274), the pupil of Albertus Magnus, wrote a dozen commentaries on the works of Aristotle. Thomas was emphatically Aristotelian, he adopted Aristotle's analysis of physical objects, his view of place, time and motion, his proof of the prime mover, his cosmology, his account of sense perception and intellectual knowledge, and even parts of his moral philosophy. The philosophical school that arose as a legacy of the work of Thomas Aquinas was known as Thomism, and was especially influential among the Dominicans, and later, the Jesuits.

Using Albert's and Thomas's commentaries, as well as Marsilius of Padua's "Defensor pacis", 14th-century scholar Nicole Oresme translated Aristotle's moral works into French and wrote extensive comments on them.

After retreating under criticism from modern natural philosophers, the distinctively Aristotelian idea of teleology was transmitted through Wolff and Kant to Hegel, who applied it to history as a totality. Although this project was criticized by Trendelenburg and Brentano as un-Aristotelian, Hegel's influence is now often said to be responsible for an important Aristotelian influence upon Marx. Postmodernists, in contrast, reject Aristotelianism's claim to reveal important theoretical truths. In this, they follow Heidegger's critique of Aristotle as the greatest source of the entire tradition of Western philosophy.

Aristotelianism is understood by its proponents as critically developing Plato's theories. Recent Aristotelian ethical and 'practical' philosophy, such as that of Gadamer and McDowell, is often premised upon a rejection of Aristotelianism's traditional metaphysical or theoretical philosophy. From this viewpoint, the early modern tradition of political republicanism, which views the "res publica", public sphere or state as constituted by its citizens' virtuous activity, can appear thoroughly Aristotelian.

The contemporary Aristotelian philosopher Alasdair MacIntyre is specially famous for helping to revive virtue ethics in his book "After Virtue". MacIntyre revises Aristotelianism with the argument that the highest temporal goods, which are internal to human beings, are actualized through participation in social practices. He opposes Aristotelianism to the managerial institutions of capitalism and its state, and to rival traditions—including the philosophies of Hume, Kant, Kierkegaard, and Nietzsche—that reject its idea of essentially human goods and virtues and instead legitimize capitalism. Therefore, on MacIntyre's account, Aristotelianism is not identical with Western philosophy as a whole; rather, it is "the best theory so far, [including] the best theory so far about what makes a particular theory the best one." Politically and socially, it has been characterized as a newly 'revolutionary Aristotelianism'. This may be contrasted with the more conventional, apolitical and effectively conservative uses of Aristotle by, for example, Gadamer and McDowell. Other important contemporary Aristotelian theorists include Fred D. Miller, Jr. in politics and Rosalind Hursthouse in ethics.

In metaphysics, an Aristotelian realism about universals is defended by such philosophers as David Malet Armstrong and Stephen Mumford, and is applied to the philosophy of mathematics by James Franklin.

Bertrand Russell criticizes Aristotle's logic on the following points:


In addition, Russell ends his review of the Aristotelian logic with these words:
I conclude that the Aristotelian doctrines with which we have been concerned in this chapter are wholly false, with the exception of the formal theory of the syllogism, which is unimportant. Any person in the present day who wishes to learn logic will be wasting his time if he reads Aristotle or any of his disciples. Nonetheless, Aristotle's logical writings show great ability, and would have been useful to mankind if they had appeared at a time when intellectual originality was still active. Unfortunately, they appeared at the very end of the creative period of Greek thought, and therefore came to be accepted as authoritative. By the time that logical originality revived, a reign of two thousand years had made Aristotle very difficult to dethrone. Throughout modern times, practically every advance in science, in logic, or in philosophy has had to be made in the teeth of the opposition from Aristotle's disciples.



</doc>
<doc id="21402758" url="https://en.wikipedia.org/wiki?curid=21402758" title="Qualia">
Qualia

In philosophy and certain models of psychology, qualia ( or ; singular form: quale) are defined as individual instances of subjective, conscious experience. The term "qualia" derives from the Latin neuter plural form ("qualia") of the Latin adjective "quālis" () meaning "of what sort" or "of what kind" in a specific instance, such as "what it is like to taste a specific apple, this particular apple now".

Examples of qualia include the perceived sensation of "pain" of a headache, the "taste" of wine, as well as the "redness" of an evening sky. As qualitative characters of sensation, qualia stand in contrast to "propositional attitudes", where the focus is on beliefs about experience rather than what it is directly like to be experiencing.

Philosopher and cognitive scientist Daniel Dennett once suggested that "qualia" was "an unfamiliar term for something that could not be more familiar to each of us: the ways things seem to us".

Much of the debate over their importance hinges on the definition of the term, and various philosophers emphasize or deny the existence of certain features of qualia. Consequently, the nature and existence of various definitions of qualia remain controversial because they are not verifiable.

There are many definitions of qualia, which have changed over time. One of the simpler, broader definitions is: "The 'what it is like' character of mental states. The way it feels to have mental states such as pain, seeing red, smelling a rose, etc."

Charles Sanders Peirce introduced the term quale in philosophy in 1866. 

Clarence Irving Lewis, in his book "Mind and the World Order" (1929), was the first to use the term "qualia" in its generally agreed upon modern sense.
Frank Jackson later defined qualia as "...certain features of the bodily sensations especially, but also of certain perceptual experiences, which no amount of purely physical information includes". 

Daniel Dennett identifies four properties that are commonly ascribed to qualia. According to these, qualia are:

If qualia of this sort exist, then a normally sighted person who sees red would be unable to describe the experience of this perception in such a way that a listener who has never experienced color will be able to know everything there is to know about that experience. Though it is possible to make an analogy, such as "red looks hot", or to provide a description of the conditions under which the experience occurs, such as "it's the color you see when light of 700-nm wavelength is directed at you", supporters of this kind of qualia contend that such a description is incapable of providing a complete description of the experience.

Another way of defining qualia is as "raw feels". A "raw feel" is a perception in and of itself, considered entirely in isolation from any effect it might have on behavior and behavioral disposition. In contrast, a "cooked feel" is that perception seen as existing in terms of its effects. For example, the perception of the taste of wine is an ineffable, raw feel, while the experience of warmth or bitterness caused by that taste of wine would be a cooked feel. Cooked feels are not qualia.

According to an argument put forth by Saul Kripke in his paper "Identity and Necessity" (1971), one key consequence of the claim that such things as raw feels can be meaningfully discussed—that qualia exist—is that it leads to the logical possibility of two entities exhibiting identical behavior in all ways despite one of them entirely lacking qualia. While very few ever claim that such an entity, called a philosophical zombie, actually exists, the mere possibility is claimed to be sufficient to refute physicalism.

Arguably, the idea of hedonistic utilitarianism, where the ethical value of things is determined from the amount of subjective pleasure or pain they cause, is dependent on the existence of qualia.

Since it is by definition impossible to convey qualia verbally, it is also impossible to demonstrate them directly in an argument; so a more tangential approach is needed. Arguments for qualia generally come in the form of thought experiments designed to lead one to the conclusion that qualia exist.

Although it does not actually mention the word "qualia", Thomas Nagel's paper "What Is it Like to Be a Bat?" is often cited in debates over qualia. Nagel argues that consciousness has an essentially subjective character, a what-it-is-like aspect. He states that "an organism has conscious mental states if and only if there is something that it is like to "be" that organism—something it is like "for" the organism." Nagel also suggests that the subjective aspect of the mind may not ever be sufficiently accounted for by the objective methods of reductionistic science. He claims that "if we acknowledge that a physical theory of mind must account for the subjective character of experience, we must admit that no presently available conception gives us a clue about how this could be done." Furthermore, he states that "it seems unlikely that any physical theory of mind can be contemplated until more thought has been given to the general problem of subjective and objective."

The inverted spectrum thought experiment, originally developed by John Locke, invites us to imagine that we wake up one morning and find that for some unknown reason all the colors in the world have been inverted, i.e. swapped to the hue on the opposite side of a color wheel. Furthermore, we discover that no physical changes have occurred in our brains or bodies that would explain this phenomenon. Supporters of the existence of qualia argue that since we can imagine this happening without contradiction, it follows that we are imagining a change in a property that determines the way things look to us, but that has no physical basis. In more detail:

The argument thus claims that if we find the inverted spectrum plausible, we must admit that qualia exist (and are non-physical). Some philosophers find it absurd that an armchair argument can prove something to exist, and the detailed argument does involve a lot of assumptions about conceivability and possibility, which are open to criticism. Perhaps it is not possible for a given brain state to produce anything other than a given quale in our universe, and that is all that matters.

The idea that an inverted spectrum would be undetectable in practice is also open to criticism on more scientific grounds (see main article). There is an actual experiment—albeit somewhat obscure—that parallels the inverted spectrum argument. George M. Stratton, professor of psychology at the University of California, Berkeley, performed an experiment in which he wore special prism glasses that caused the external world to appear upside down. After a few days of continually wearing the glasses, an adaptation occurred and the external world appeared righted. When the glasses were removed, the external world again appeared inverted. After a similar period, perception of the external world returned to the "normal" perceptual state. If this argument provides indicia that qualia exist, it does not necessarily follow that they must be non-physical, because that distinction should be considered a separate epistemological issue.

A similar argument holds that it is conceivable (or not inconceivable) that there could be physical duplicates of people, called "philosophical zombies", without any qualia at all. These "zombies" would demonstrate outward behavior precisely similar to that of a normal human, but would not have a subjective phenomenology. It is worth noting that a necessary condition for the possibility of philosophical zombies is that there be no specific part or parts of the brain that directly give rise to qualia—the zombie can only exist if subjective consciousness is causally separate from the physical brain.

"Are zombies possible? They're not just possible, they're actual. We're all zombies. Nobody is conscious." - Daniel Dennett ("Consciousness Explained,1991")

Joseph Levine's paper "Conceivability, Identity, and the Explanatory Gap" takes up where the criticisms of conceivability arguments, such as the inverted spectrum argument and the zombie argument, leave off. Levine agrees that conceivability is flawed as a means of establishing metaphysical realities, but points out that even if we come to the "metaphysical" conclusion that qualia are physical, there is still an "explanatory" problem.
However, such an epistemological or explanatory problem might indicate an underlying metaphysical issue—the non-physicality of qualia, even if not proven by conceivability arguments is far from ruled out.

Frank Jackson offers what he calls the "knowledge argument" for qualia. One example runs as follows:
This thought experiment has two purposes. First, it is intended to show that qualia exist. If one agrees with the thought experiment, we believe that Mary gains something after she leaves the room—that she acquires knowledge of a particular thing that she did not possess before. That knowledge, Jackson argues, is knowledge of the quale that corresponds to the experience of seeing red, and it must thus be conceded that qualia are real properties, since there is a difference between a person who has access to a particular quale and one who does not.

The second purpose of this argument is to refute the physicalist account of the mind. Specifically, the knowledge argument is an attack on the physicalist claim about the completeness of physical truths. The challenge posed to physicalism by the knowledge argument runs as follows:

First Jackson argued that qualia are epiphenomenal: not causally efficacious with respect to the physical world. Jackson does not give a positive justification for this claim—rather, he seems to assert it simply because it defends qualia against the classic problem of dualism. Our natural assumption would be that qualia must be causally efficacious in the physical world, but some would ask how we could argue for their existence if they did not affect our brains. If qualia are to be non-physical properties (which they must be in order to constitute an argument against physicalism), some argue that it is almost impossible to imagine how they could have a causal effect on the physical world. By redefining qualia as epiphenomenal, Jackson attempts to protect them from the demand of playing a causal role.

Later, however, he rejected epiphenomenalism. This, he argues, is because when Mary first sees red, she says "wow", so it must be Mary's qualia that cause her to say "wow". This contradicts epiphenomenalism. Since the Mary's room thought experiment seems to create this contradiction, there must be something wrong with it. This is often referred to as the "there must be a reply" reply.

In "Consciousness Explained" (1991) and "Quining Qualia" (1988), Daniel Dennett offers an argument against qualia by claiming that the above definition breaks down when one tries to make a practical application of it. In a series of thought experiments, which he calls "intuition pumps", he brings qualia into the world of neurosurgery, clinical psychology, and psychological experimentation. His argument states that, once the concept of qualia is so imported, it turns out that we can either make no use of it in the situation in question, or that the questions posed by the introduction of qualia are unanswerable precisely because of the special properties defined for qualia.

In Dennett's updated version of the inverted spectrum thought experiment, "alternative neurosurgery", you again awake to find that your qualia have been inverted—grass appears red, the sky appears orange, etc. According to the original account, you should be immediately aware that something has gone horribly wrong. Dennett argues, however, that it is impossible to know whether the diabolical neurosurgeons have indeed inverted your qualia (by tampering with your optic nerve, say), or have simply inverted your connection to memories of past qualia. Since both operations would produce the same result, you would have no means on your own to tell which operation has actually been conducted, and you are thus in the odd position of not knowing whether there has been a change in your "immediately apprehensible" qualia.

Dennett's argument revolves around the central objection that, for qualia to be taken seriously as a component of experience—for them to even make sense as a discrete concept—it must be possible to show that

Dennett attempts to show that we cannot satisfy (a) either through introspection or through observation, and that qualia's very definition undermines its chances of satisfying (b).

Supporters of qualia could point out that in order for you to notice a change in qualia, you must compare your current qualia with your memories of past qualia. Arguably, such a comparison would involve immediate apprehension of your current qualia "and" your memories of past qualia, but not the past qualia "themselves". Furthermore, modern functional brain imaging has increasingly suggested that the memory of an experience is processed in similar ways and in similar zones of the brain as those originally involved in the original perception. This may mean that there would be asymmetry in outcomes between altering the mechanism of perception of qualia and altering their memories. If the diabolical neurosurgery altered the immediate perception of qualia, you might not even notice the inversion directly, since the brain zones which re-process the memories would themselves invert the qualia remembered. On the other hand, alteration of the qualia memories themselves would be processed without inversion, and thus you would perceive them as an inversion. Thus, you might know immediately if memory of your qualia had been altered, but might not know if immediate qualia were inverted or whether the diabolical neurosurgeons had done a sham procedure.

Dennett also has a response to the "Mary the color scientist" thought experiment. He argues that Mary would not, in fact, learn something new if she stepped out of her black and white room to see the color red. Dennett asserts that if she already truly knew "everything about color", that knowledge would include a deep understanding of why and how human neurology causes us to sense the "quale" of color. Mary would therefore already know exactly what to expect of seeing red, before ever leaving the room. Dennett argues that the misleading aspect of the story is that Mary is supposed to not merely be knowledgeable about color but to actually know "all" the physical facts about it, which would be a knowledge so deep that it exceeds what can be imagined, and twists our intuitions.

If Mary really does know everything physical there is to know about the experience of color, then this effectively grants her almost omniscient powers of knowledge. Using this, she will be able to deduce her own reaction, and figure out exactly what the experience of seeing red will feel like.

Dennett finds that many people find it difficult to see this, so he uses the case of RoboMary to further illustrate what it would be like for Mary to possess such a vast knowledge of the physical workings of the human brain and color vision. RoboMary is an intelligent robot who, instead of the ordinary color camera-eyes, has a software lock such that she is only able to perceive black and white and shades in-between.

RoboMary can examine the computer brain of similar non-color-locked robots when they look at a red tomato, and see exactly how they react and what kinds of impulses occur. RoboMary can also construct a simulation of her own brain, unlock the simulation's color-lock and, with reference to the other robots, simulate exactly how this simulation of herself reacts to seeing a red tomato. RoboMary naturally has control over all of her internal states except for the color-lock. With the knowledge of her simulation's internal states upon seeing a red tomato, RoboMary can put her own internal states directly into the states they would be in upon seeing a red tomato. In this way, without ever seeing a red tomato through her cameras, she will know exactly what it is like to see a red tomato.

Dennett uses this example to show us that Mary's all-encompassing physical knowledge makes her own internal states as transparent as those of a robot or computer, and it is almost straightforward for her to figure out exactly how it feels to see red.

Perhaps Mary's failure to learn exactly what seeing red feels like is simply a failure of language, or a failure of our ability to describe experiences. An alien race with a different method of communication or description might be perfectly able to teach their version of Mary exactly how seeing the color red would feel. Perhaps it is simply a uniquely human failing to communicate first-person experiences from a third-person perspective. Dennett suggests that the description might even be possible using English. He uses a simpler version of the Mary thought experiment to show how this might work. What if Mary was in a room without triangles and was prevented from seeing or making any triangles? An English-language description of just a few words would be sufficient for her to imagine what it is like to see a triangle—she can simply and directly visualize a triangle in her mind. Similarly, Dennett proposes, it is perfectly, logically possible that the quale of what it is like to see red could eventually be described in an English-language description of millions or billions of words.

In "Are we explaining consciousness yet?" (2001), Dennett approves of an account of qualia defined as the deep, rich collection of individual neural responses that are too fine-grained for language to capture. For instance, a person might have an alarming reaction to yellow because of a yellow car that hit her previously, and someone else might have a nostalgic reaction to a comfort food. These effects are too individual-specific to be captured by English words. "If one dubs this inevitable residue "qualia", then qualia are guaranteed to exist, but they are just more of the same, dispositional properties that have not yet been entered in the catalog [...]."

According to Paul Churchland, Mary might be considered to be like a feral child. Feral children have suffered extreme isolation during childhood. Technically when Mary leaves the room, she would not have the ability to see or know what the color red is. A brain has to learn and develop how to see colors. Patterns need to form in the V4 section of the visual cortex. These patterns are formed from exposure to wavelengths of light. This exposure is needed during the early stages of brain development. In Mary's case, the identifications and categorizations of color will only be in respect to representations of black and white.

In his book "Good and Real" (2006), Gary Drescher compares qualia with "gensyms" (generated symbols) in Common Lisp. These are objects that Lisp treats as having no properties or components and which can only be identified as equal or not equal to other objects. Drescher explains, "we have no introspective access to whatever internal properties make the "red" gensym recognizably distinct from the "green" [...] even though we know the sensation when we experience it." Under this interpretation of qualia, Drescher responds to the Mary thought experiment by noting that "knowing about red-related cognitive structures and the dispositions they engender—even if that knowledge were implausibly detailed and exhaustive—would not necessarily give someone who lacks prior color-experience the slightest clue whether the card now being shown is of the color called red." This does not, however, imply that our experience of red is non-mechanical; "on the contrary, gensyms are a routine feature of computer-programming languages".

David Lewis has an argument that introduces a new hypothesis about types of knowledge and their transmission in qualia cases. Lewis agrees that Mary cannot learn what red looks like through her monochrome physicalist studies. But he proposes that this doesn't matter. Learning transmits information, but experiencing qualia doesn't transmit information; instead it communicates abilities. When Mary sees red, she doesn't get any new information. She gains new abilities—now she can remember what red looks like, imagine what other red things might look like and recognize further instances of redness. Lewis states that Jackson's thought experiment uses the "Phenomenal Information Hypothesis"—that is, the new knowledge that Mary gains upon seeing red is phenomenal information. Lewis then proposes a different "Ability Hypothesis" that differentiates between two types of knowledge: knowledge that (information) and knowledge how (abilities). Normally the two are entangled; ordinary learning is also an experience of the subject concerned, and people both learn information (for instance, that Freud was a psychologist) and gain ability (to recognize images of Freud). However, in the thought experiment, Mary can only use ordinary learning to gain know-that knowledge. She is prevented from using experience to gain the know-how knowledge that would allow her to remember, imagine and recognize the color red.

We have the intuition that Mary has been deprived of some vital data to do with the experience of redness. It is also uncontroversial that some things cannot be learned inside the room; for example, we do not expect Mary to learn how to ski within the room. Lewis has articulated that information and ability are potentially different things. In this way, physicalism is still compatible with the conclusion that Mary gains new knowledge. It is also useful for considering other instances of qualia; "being a bat" is an ability, so it is know-how knowledge.

The artificial intelligence researcher Marvin Minsky thinks the problems posed by qualia are essentially issues of complexity, or rather of mistaking complexity for simplicity.

Michael Tye holds the opinion there are no qualia, no "veils of perception" between us and the referents of our thought. He describes our experience of an object in the world as "transparent". By this he means that no matter what private understandings and/or misunderstandings we may have of some public entity, it is still there before us in reality. The idea that qualia intervene between ourselves and their origins he regards as "a massive error"; as he says, "it is just not credible that visual experiences are systematically misleading in this way"; "the only objects of which you are aware are the external ones making up the scene before your eyes"; there are "no such things as the qualities of experiences" for "they are qualities of external surfaces (and volumes and films) if they are qualities of anything." This insistence permits him to take our experience as having a reliable base since there is no fear of losing contact with the realness of public objects.

In Tye's thought there is no question of qualia without information being contained within them; it is always "an awareness that", always "representational". He characterizes the perception of children as a misperception of referents that are undoubtedly as present for them as they are for grown-ups. As he puts it, they may not know that "the house is dilapidated", but there is no doubt about their seeing the house. After-images are dismissed as presenting no problem for the Transparency Theory because, as he puts it, after-images being illusory, there is nothing that one sees.

Tye proposes that phenomenal experience has five basic elements, for which he has coined the acronym PANIC—Poised, Abstract, Nonconceptual, Intentional Content. It is "Poised" in the sense that the phenomenal experience is always presented to the understanding, whether or not the agent is able to apply a concept to it. Tye adds that the experience is "maplike" in that, in most cases, it reaches through to the distribution of shapes, edges, volumes, etc. in the world—you may not be reading the "map" but, as with an actual map there is a reliable match with what it is mapping. It is "Abstract" because it is still an open question in a particular case whether you are in touch with a concrete object (someone may feel a pain in a "left leg" when that leg has actually been amputated). It is "Nonconceptual" because a phenomenon can exist although one does not have the concept by which to recognize it. Nevertheless, it is "Intentional" in the sense that it represents something, again whether or not the particular observer is taking advantage of that fact; this is why Tye calls his theory "representationalism". This last makes it plain that Tye believes that he has retained a direct contact with what produces the phenomena and is therefore not hampered by any trace of a "veil of perception".

Roger Scruton, whilst sceptical of the idea that neurobiology can tell us a great amount about consciousness, is of the opinion that the idea of qualia is incoherent, and that Wittgenstein's famous private language argument effectively disproves it. Scruton writes, The belief that these essentially private features of mental states exist, and that they form the introspectible essence of whatever possesses them, is grounded in a confusion, one that Wittgenstein tried to sweep away in his arguments against the possibility of a private language. When you judge that I am in pain, it is on the basis of my circumstances and behavior, and you could be wrong. When I ascribe a pain to myself, I don’t use any such evidence. I don’t find out that I am in pain by observation, nor can I be wrong. But that is not because there is some other fact about my pain, accessible only to me, which I consult in order to establish what I am feeling. For if there were this inner private quality, I could misperceive it; I could get it wrong, and I would have to find out whether I am in pain. To describe my inner state, I would also have to invent a language, intelligible only to me – and that, Wittgenstein plausibly argues, is impossible. The conclusion to draw is that I ascribe pain to myself not on the basis of some inner quale but on no basis at all.
In his book "On Human Nature", Scruton does pose a potential line of criticism to this, which is that whilst Wittgenstein's private language argument does disprove the concept of reference to qualia, or the idea that we can talk even to ourselves of their nature, it does not disprove its existence altogether. Scruton believes that this is a valid criticism, and this is why he stops short of actually saying that quales don't exist, and instead merely suggests that we should abandon them as a concept. However, he does give a quote by Wittgenstein as a response: "Whereof one cannot speak, thereof one must be silent."

David Chalmers formulated the hard problem of consciousness, raising the issue of qualia to a new level of importance and acceptance in the field. In his paper "Absent Qualia, Fading Qualia, Dancing Qualia", he also argued for what he called "the principle of organizational invariance". In this paper, he argues that if a system such as one of appropriately configured computer chips reproduces the functional organization of the brain, it will also reproduce the qualia associated with the brain.

E. J. Lowe, of Durham University, denies that holding to indirect realism (in which we have access only to sensory features internal to the brain) necessarily implies a Cartesian dualism. He agrees with Bertrand Russell that our "retinal images"—that is, the distributions across our retinas—are connected to "patterns of neural activity in the cortex" (Lowe 1986). He defends a version of the Causal Theory of Perception in which a causal path can be traced between the external object and the perception of it. He is careful to deny that we do any inferring from the sensory field, a view which he believes allows us to found an access to knowledge on that causal connection. In a later work he moves closer to the non-epistemic theory in that he postulates "a wholly non-conceptual component of perceptual experience", but he refrains from analyzing the relation between the perceptual and the "non-conceptual". Most recently he has drawn attention to the problems that hallucination raises for the direct realist and to their disinclination to enter the discussion on the topic.

John Barry Maund, an Australian philosopher of perception at the University of Western Australia, draws attention to a key distinction of qualia. Qualia are open to being described on two levels, a fact that he refers to as "dual coding". Using the Television Analogy (which, as the non-epistemic argument shows, can be shorn of its objectionable aspects), he points out that, if asked what we see on a television screen there are two answers that we might give:

The states of the screen during a football match are unquestionably different from those of the screen during a chess game, but there is no way available to us of describing the ways in which they are different except by reference to the play, moves and pieces in each game.

He has refined the explanation by shifting to the example of a "Movitype" screen, often used for advertisements and announcements in public places. A Movitype screen consists of a matrix—or "raster" as the neuroscientists prefer to call it (from the Latin "rastrum", a "rake"; think of the lines on a TV screen as "raked" across)—that is made up of an array of tiny light-sources. A computer-led input can excite these lights so as to give the impression of letters passing from right to left, or even, on the more advanced forms now commonly used in advertisements, to show moving pictures. Maund's point is as follows. It is obvious that there are two ways of describing what you are seeing. We could either adopt the everyday public language and say "I saw some sentences, followed by a picture of a 7-Up can." Although that is a perfectly adequate way of describing the sight, nevertheless, there is a scientific way of describing it which bears no relation whatsoever to this commonsense description. One could ask the electronics engineer to provide us with a computer print-out staged across the seconds that you were watching it of the point-states of the raster of lights. This would no doubt be a long and complex document, with the state of each tiny light-source given its place in the sequence. The interesting aspect of this list is that, although it would give a comprehensive and point-by-point-detailed description of the state of the screen, nowhere in that list would there be a mention of "English sentences" or "a 7-Up can".

What this makes clear is that there are two ways to describe such a screen, (1) the "commonsense" one, in which publicly recognizable objects are mentioned, and (2) an accurate point-by-point account of the actual state of the field, but makes no mention of what any passer-by would or would not make of it. This second description would be non-epistemic from the common sense point of view, since no objects are mentioned in the print-out, but perfectly acceptable from the engineer's point of view. Note that, if one carries this analysis across to human sensing and perceiving, this rules out Daniel Dennett's claim that all qualiaphiles must regard qualia as "ineffable", for at this second level they are in principle quite "effable"—indeed, it is not ruled out that some neurophysiologist of the future might be able to describe the neural detail of qualia at this level.

Maund has also extended his argument particularly with reference of color. Color he sees as a dispositional property, not an objective one, an approach which allows for the facts of difference between person and person, and also leaves aside the claim that external objects are colored. Colors are therefore "virtual properties", in that it is as if things possessed them; although the naïve view attributes them to objects, they are intrinsic, non-relational inner experiences.

In his book "Sensing the World", Moreland Perkins argues that qualia need not be identified with their objective sources: a smell, for instance, bears no direct resemblance to the molecular shape that gives rise to it, nor is a toothache actually in the tooth. He is also like Hobbes in being able to view the process of sensing as being something complete in itself; as he puts it, it is not like "kicking a football" where an external object is required—it is more like "kicking a kick", an explanation which entirely avoids the familiar Homunculus Objection, as adhered to, for example, by Gilbert Ryle. Ryle was quite unable even to entertain this possibility, protesting that "in effect it explained the having of sensations as the not having of sensations." However, A.J. Ayer in a rejoinder identified this objection as "very weak" as it betrayed an inability to detach the notion of eyes, indeed any sensory organ, from the neural sensory experience.

Vilayanur S. Ramachandran and William Hirstein proposed three laws of qualia (with a fourth later added), which are "functional criteria that need to be fulfilled in order for certain neural events to be associated with qualia" by philosophers of the mind:
They proposed that the phenomenal nature of qualia could be communicated (as in "oh "that" is what salt tastes like") if brains could be appropriately connected with a "cable of neurons". If this turned out to be possible this would scientifically prove or objectively demonstrate the existence and the nature of qualia.

Howard Robinson is a philosopher who has concentrated his research within the philosophy of mind. Taking what has been through the latter part of the last century an unfashionable stance, he has consistently argued against those explanations of sensory experience that would reduce them to physical origins. He has never regarded the theory of sense-data as refuted, but has set out to refute in turn the objections which so many have considered to be conclusive. The version of the theory of sense-data he defends takes what is before consciousness in perception to be qualia as mental presentations that are causally linked to external entities, but which are not physical in themselves. Unlike the philosophers so far mentioned, he is therefore a dualist, one who takes both matter and mind to have real and metaphysically distinct natures. In one of his most recent articles he takes the physicalist to task for ignoring the fact that sensory experience can be entirely free of representational character. He cites phosphenes as a stubborn example (phosphenes are flashes of neural light that result either from sudden pressure in the brain—as induced, for example, by intense coughing, or through direct physical pressure on the retina), and points out that it is grossly counter-intuitive to argue that these are not visual experiences on a par with open-eye seeing.

William Robinson (no relation) takes a very similar view to that of his namesake. In his most recent book, "Understanding Phenomenal Consciousness", he is unusual as a dualist in calling for research programs that investigate the relation of qualia to the brain. The problem is so stubborn, he says, that too many philosophers would prefer "to explain it away", but he would rather have it explained and does not see why the effort should not be made. However, he does not expect there to be a straightforward scientific reduction of phenomenal experience to neural architecture; on the contrary he regards this as a forlorn hope. The "Qualitative Event Realism" that Robinson espouses sees phenomenal consciousness as caused by brain events but not identical with them, being non-material events.

It is noteworthy that he refuses to set aside the vividness—and commonness—of mental images, both visual and aural, standing here in direct opposition to Daniel Dennett, who has difficulty in crediting the experience in others. He is similar to Moreland Perkins in keeping his investigation wide enough to apply to all the senses.

Edmond Wright is a philosopher who considers the intersubjective aspect of perception. From Locke onwards it had been normal to frame perception problems in terms of a single subject S looking at a single entity E with a property p. However, if we begin with the facts of the differences in sensory registration from person to person, coupled with the differences in the criteria we have learned for distinguishing what we together call "the same" things, then a problem arises of how two persons align their differences on these two levels so that they can still get a practical overlap on parts of the real about them—and, in particular, update each other about them.

Wright mentions being struck with the hearing difference between himself and his son, discovering that his son could hear sounds up to nearly 20 kilohertz while his range only reached to 14 kHz or so. This implies that a difference in qualia could emerge in human action (for example, the son could warn the father of a high-pitched escape of a dangerous gas kept under pressure, the sound-waves of which would be producing no qualia evidence at all for the father). The relevance for language thus becomes critical, for an informative statement can best be understood as an updating of a perception—and this may involve a radical re-selection from the qualia fields viewed as non-epistemic, even perhaps of the presumed singularity of "the" referent, a fortiori if that "referent" is the self. Here he distinguishes his view from that of Revonsuo, who too readily makes his "virtual space" "egocentric".

Wright's particular emphasis has been on what he asserts is a core feature of communication, that, in order for an updating to be set up and made possible, both speaker and hearer have to behave as if they have identified "the same singular thing", which, he notes, partakes of the structure of a joke or a story. Wright says that this systematic ambiguity seems to opponents of qualia to be a sign of fallacy in the argument (as ambiguity is in pure logic) whereas, on the contrary, it is sign—in talk about "what" is perceived—of something those speaking to each other have to learn to take advantage of. In extending this analysis, he has been led to argue for an important feature of human communication being the degree and character of the faith maintained by the participants in the dialogue, a faith that has priority over what has before been taken to be the key virtues of language, such as "sincerity", "truth", and "objectivity". Indeed, he considers that to prioritize them over faith is to move into superstition.

Erwin Schrödinger, a theoretical physicist and one of the leading pioneers of quantum mechanics, also published in the areas of colorimetry and color perception. In several of his philosophical writings, he defends the notion that qualia are not physical.
He continues on to remark that subjective experiences do not form a one-to-one correspondence with stimuli. For example, light of wavelength in the neighborhood of 590 nm produces the sensation of yellow, whereas exactly the same sensation is produced by mixing red light, with wavelength 760 nm, with green light, at 535 nm. From this he concludes that there is no "numerical connection with these physical, objective characteristics of the waves" and the sensations they produce.

Schrödinger concludes with a proposal of how it is that we might arrive at the mistaken belief that a satisfactory theoretical account of qualitative experience has been—or might ever be—achieved:

When looked at philosophically, qualia become a tipping point between physicality and the metaphysical, which polarizes the discussion, as we've seen above, into "Do they or do they not exist?" and "Are they physical or beyond the physical?" However, from a strictly neurological perspective, they can both exist, and be very important to the organism's survival, and be the result of strict neuronal oscillation, and still not rule out the metaphysical. A good example of this pro/con blending is in Rodolfo Llinás's "I of the Vortex" (MIT Press, 2002, pp. 202–207). Llinás argues that qualia are ancient and necessary for an organism's survival "and" a product of neuronal oscillation. Llinás gives the evidence of anesthesia of the brain and subsequent stimulation of limbs to demonstrate that qualia can be "turned off" with changing only the variable of neuronal oscillation (local brain electrical activity), while all other connections remain intact, arguing strongly for an oscillatory—electrical origin of qualia, or important aspects of them.

Roger Orpwood, an engineer with a strong background in studying neural mechanisms, proposed a neurobiological model that gives rise to qualia and ultimately, consciousness. As advancements in cognitive and computational neuroscience continue to grow, the need to study the mind, and qualia, from a scientific perspective follows. Orpwood does not deny the existence of qualia, nor does he intend to debate its physical or non-physical existence. Rather, he suggests that qualia are created through the neurobiological mechanism of re-entrant feedback in cortical systems 

Orpwood develops his mechanism by first addressing the issue of information. One unsolved aspect of qualia is the concept of the fundamental information involved in creating the experience. He does not address a position on the metaphysics of the information underlying the experience of qualia, nor does he state what information actually is. However, Orpwood does suggest that information in general is of two types: the information structure and information message. Information structures are defined by the physical vehicles and structural, biological patterns encoding information. That encoded information is the information message; a source describing "what" that information is. The neural mechanism or network receives input information structures, completes a designated instructional task (firing of the neuron or network), and outputs a modified information structure to downstream regions. The information message is the purpose and meaning of the information structure and causally exists as a result of that particular information structure. Modification of the information structure changes the meaning of the information message, but the message itself cannot be directly altered.

Local cortical networks have the capacity to receive feedback from their own output information structures. This form of local feedback continuously cycles part of the networks output structures as its next input information structure. Since the output structure must represent the information message derived from the input structure, each consecutive cycle that is fed-back will represent the output structure the network just generated. As the network of mechanisms cannot recognize the information message, but only the input information structure, the network is unaware that it is representing its own previous outputs. The neural mechanisms are merely completing their instructional tasks and outputting any recognizable information structures. Orpwood proposes that these local networks come into an attractor state that consistently outputs exactly the same information structure as the input structure. Instead of only representing the information message derived from the input structure, the network will now represent its own output and thereby its own information message. As the input structures are fed-back, the network identifies the previous information structure as being a previous representation of the information message. As Orpwood states, 

Representation of the networks own output structures, by which represents its own information message, is Orpwood's explanation that grounds the manifestation of qualia via neurobiological mechanisms. These mechanisms are particular to networks of pyramidal neurons. Although computational neuroscience still has much to investigate regarding pyramidal neurons, their complex circuitry is relatively unique. Research shows that the complexity of pyramidal neuron networks is directly related to the increase in the functional capabilities of a species. When human pyramidal networks are compared with other primate species and species with less intricate behavioral and social interactions, the complexity of these neural networks drastically decline. The complexity of these networks are also increased in frontal brain regions. These regions are often associated with conscious assessment and modification of one's immediate environment; often referred to as "executive functions". Sensory input is necessary to gain information from the environment, and perception of that input is necessary for navigating and modifying interactions with the environment. This suggests that frontal regions containing more complex pyramidal networks are associated with an increased perceptive capacity. As perception is necessary for conscious thought to occur, and since the experience of qualia is derived from consciously recognizing some perception, qualia may indeed be specific to the functional capacity of pyramidal networks. This derives Orpwood's notion that the mechanisms of re-entrant feedback may not only create qualia, but also be the foundation to consciousness.

It is possible to apply a criticism similar to Nietzsche's criticism of Kant's "thing in itself" to qualia: Qualia are unobservable in others and unquantifiable in us. We cannot possibly be sure, when discussing individual qualia, that we are even discussing the same phenomena. Thus, any discussion of them is of indeterminate value, as descriptions of qualia are necessarily of indeterminate accuracy. Qualia can be compared to "things in themselves" in that they have no publicly demonstrable properties; this, along with the impossibility of being sure that we are communicating about the same qualia, makes them of indeterminate value and definition in any philosophy in which proof relies upon precise definition. On the other hand, qualia could be considered akin to Kantian phenomena since they are held to be seemings of appearances. Revonsuo, however, considers that, within neurophysiological inquiry, a definition at the level of the fields may become possible (just as we can define a television picture at the level of liquid crystal pixels).

Whether or not qualia or consciousness can play any causal role in the physical world remains an open question, with epiphenomenalism acknowledging the existence of qualia while denying it any causal power. The position has been criticized by a number of philosophers, if only because our own consciousness seem to be causally active. In order to avoid epiphenomenalism, one who believes that qualia are nonphysical would need to embrace something like interactionist dualism; or perhaps emergentism, the claim that there are as yet unknown causal relations between the mental and physical. This in turn would imply that qualia can be detected by an external agency through their causal powers.

To illustrate: one might be tempted to give as examples of qualia "the pain of a headache, the taste of wine, or the redness of an evening sky". But this list of examples already prejudges a central issue in the current debate on qualia. An analogy might make this clearer. Suppose someone wants to know the nature of the liquid crystal pixels on a television screen, those tiny elements that provide all the distributions of color that go to make up the picture. It would not suffice as an answer to say that they are the "redness of an evening sky" as it appears on the screen. We would protest that their real character was being ignored. One can see that relying on the list above assumes that we must tie sensations not only to the notion of given objects in the world (the "head", "wine", "an evening sky"), but also to the properties with which we characterize the experiences themselves ("redness", for example).

Nor is it satisfactory to print a little red square as at the top of the article, for, since each person has a slightly different registration of the light-rays, it confusingly suggests that we all have the same response. Imagine in a television shop seeing "a red square" on twenty screens at once, each slightly different—something of vital importance would be overlooked if a single example were to be taken as defining them all.

Yet it has been argued whether or not identification with the external object should still be the core of a correct approach to sensation, for there are many who state the definition thus because they regard the link with external reality as crucial. If sensations are defined as "raw feels", there arises a palpable threat to the reliability of knowledge. The reason has been given that, if one sees them as neurophysiological happenings in the brain, it is difficult to understand how they could have any connection to entities, whether in the body or the external world. It has been declared, by John McDowell for example, that to countenance qualia as a "bare presence" prevents us ever gaining a certain ground for our knowledge. The issue is thus fundamentally an epistemological one: it would appear that access to knowledge is blocked if one allows the existence of qualia as fields in which only virtual constructs are before the mind.

His reason is that it puts the entities about which we require knowledge behind a "veil of perception", an occult field of "appearance" which leaves us ignorant of the reality presumed to be beyond it. He is convinced that such uncertainty propels into the dangerous regions of relativism and solipsism: relativism sees all truth as determined by the single observer; solipsism, in which the single observer is the only creator of and legislator for his or her own universe, carries the assumption that no one else exists. These accusations constitute a powerful ethical argument against qualia being something going on in the brain, and these implications are probably largely responsible for the fact that in the 20th century it was regarded as not only freakish, but also dangerously misguided to uphold the notion of sensations as going on inside the head. The argument was usually strengthened with mockery at the very idea of "redness" being in the brain: the question was—and still is—"How can there be red neurons in the brain?" which strikes one as a justifiable appeal to common sense.

To maintain a philosophical balance, the argument for "raw feels" needs to be set side by side with the claim above. Viewing sensations as "raw feels" implies that initially they have not yet—to carry on the metaphor—been "cooked", that is, unified into "things" and "persons", which is something the mind does after the sensation has responded to the blank input, that response driven by motivation, that is, initially by pain and pleasure, and subsequently, when memories have been implanted, by desire and fear. Such a "raw-feel" state has been more formally identified as "non-epistemic". In support of this view, the theorists cite a range of empirical facts. The following can be taken as representative. There are brain-damaged persons, known as "agnosics" (literally "not-knowing") who still have vivid visual sensations but are quite unable to identify any entity before them, including parts of their own body. There is also the similar predicament of persons, formerly blind, who are given sight for the first time—and consider what it is a newborn baby must experience. A German psychologist of the 19th century, Hermann von Helmholtz, proposed a simple experiment to demonstrate the non-epistemic nature of qualia: his instructions were to stand in front of a familiar landscape, turn your back on it, bend down and look at the landscape between your legs—you will find it difficult in the upside-down view to recognize what you found familiar before.

These examples suggest that a "bare presence"—that is, knowledgeless sensation that is no more than evidence—may really occur. Present supporters of the non-epistemic theory thus regard sensations as only data in the sense that they are "given" (Latin "datum", "given") and fundamentally involuntary, which is a good reason for not regarding them as basically mental. In the last century they were called "sense-data" by the proponents of qualia, but this led to the confusion that they carried with them reliable proofs of objective causal origins. For instance, one supporter of qualia was happy to speak of the redness and bulginess of a cricket ball as a typical "sense-datum", though not all of them were happy to define qualia by their relation to external entities (see Roy Wood Sellars). The modern argument, following Sellars' lead, centers on how we learn under the regime of motivation to interpret the sensory evidence in terms of "things", "persons", and "selves" through a continuing process of feedback.

The definition of qualia thus is governed by one's point of view, and that inevitably brings with it philosophical and neurophysiological presuppositions. The question, therefore, of what qualia can be raises profound issues in the philosophy of mind, since some materialists want to deny their existence altogether: on the other hand, if they are accepted, they cannot be easily accounted for as they raise the difficult problem of consciousness. There are committed dualists such as Richard L. Amoroso or John Hagelin who believe that the mental and the material are two distinct aspects of physical reality like the distinction between the classical and quantum regimes. In contrast, there are direct realists for whom the thought of qualia is unscientific as there appears to be no way of making them fit within the modern scientific picture; and there are committed proselytizers for a final truth who reject them as forcing knowledge out of reach.




</doc>
<doc id="1287236" url="https://en.wikipedia.org/wiki?curid=1287236" title="Innatism">
Innatism

Innatism is a philosophical and epistemological doctrine that holds that the mind is born with ideas/knowledge, and that therefore the mind is not a "blank slate" at birth, as early empiricists such as John Locke claimed. It asserts that not all knowledge is gained from experience and the senses. Plato and Descartes are prominent philosophers in the development of innatism and the notion that the mind is already born with ideas, knowledge and beliefs. Both philosophers emphasize that experiences are the key to unlocking this knowledge but not the source of the knowledge itself. Essentially, no knowledge is derived exclusively from one's experiences as empiricists like John Locke suggested.

In general usage, the terms "innatism" and "nativism" are synonymous as they both refer to notions of preexisting ideas present in the mind. However, more correctly, innatism refers to the philosophy of Plato and Descartes, who assumed that a God or a similar being or process placed innate ideas and principles in the human mind.

Nativism represents an adaptation of this, grounded in the fields of genetics, cognitive psychology, and psycholinguistics. Nativists hold that innate beliefs are in some way genetically programmed to arise in our mind—that innate beliefs are the phenotypes of certain genotypes that all humans share in common.

Nativism is a modern view rooted in innatism. The advocates of nativism are mainly philosophers who also work in the field of cognitive psychology or psycholinguistics: most notably Noam Chomsky and Jerry Fodor (although the latter has adopted a more critical attitude towards nativism in his later writings). The nativist's general objection against empiricism is still the same as was raised by the rationalists; the human mind of a newborn child is not a "tabula rasa", but equipped with an inborn structure.

In philosophy and psychology, an innate idea is a concept or item of knowledge which is said to be "universal to all humanity"—that is, something people are born with rather than something people have "learned" through experience.

The issue is controversial, and can be said to be an aspect of a long-running nature versus nurture debate, albeit one localized to the question of understanding human cognition.

Although individual human beings obviously vary due to cultural, racial, linguistic and era-specific influences, innate ideas are said to belong to a more fundamental level of human cognition. For example, the philosopher René Descartes theorized that knowledge of God is innate in everybody as a product of the faculty of faith.

Other philosophers, most notably the empiricists, were critical of the theory and denied the existence of any innate ideas, saying all human knowledge was founded on experience, rather than "a priori" reasoning.

Philosophically, the debate over innate ideas is central to the conflict between rationalist and empiricist epistemologies. While rationalists believe that certain ideas exist independently of experience, empiricism claims that all knowledge is derived from experience.

Immanuel Kant was a German philosopher who is regarded as having ended the impasse in modern philosophy between rationalists and empiricists, and is widely held to have synthesized these two early modern traditions in his thought.

Plato argues that if there are certain concepts that we know to be true but did not learn from experience then it must be because we have an innate knowledge of it and this knowledge must have been gained before birth. In Plato's "Meno", he recalls a situation in which Socrates, his mentor, questioned a slave boy about a geometry theorem. Though the slave boy had no previous experience with geometry, he was able to generate the right responses to the questions he was asked. Plato reasoned that this was possible as Socrates' questions sparked the innate knowledge of math the boy had had from birth.

Plato produced a tripartite to help explain what is contained within soul, which man is able to tap into for higher decision-making. Normally man is good but able to be confused and have the conscience become distorted with irrelevance’s or illogical reasoning. While Socrates believed no man does evil knowingly, Plato was skeptical. For we have to make conscious decisions with relation to other parts of our nature, that isn’t the same as reason. There are separations (which a person may have predominance of by nature or may rationally choose), each of course have a role to play and its up to our reason to rule, hence the title "Plato: The Rule of Reason". The tripartite may be categorized as:

"Individual material things are known by the senses, whereas forms are known by the intellect.". The forms have real independent existence.

Descartes conveys the idea that innate knowledge or ideas is something inborn such as one would say, that a certain disease might be 'innate' to signify that a person might be at risk of contracting such a disease. He suggests that something that is 'innate' is effectively present from birth and while it may not reveal itself then, is more than likely to present itself later in life. Descartes comparison of innate knowledge to an innate disease, whose symptoms may only show up later in life, unless prohibited by a factor like age or puberty, suggests that if an event occurs prohibiting someone from exhibiting an innate behaviour or knowledge, it doesn't mean the knowledge did not exist at all but rather it wasn't expressed – they were not able to acquire that knowledge. In other words, innate beliefs, ideas and knowledge require experiences to be triggered or they may never be expressed. Experiences are not the source of knowledge as proposed by John Locke, but catalysts to the uncovering of knowledge.

The main antagonist to the concept of innate ideas is John Locke, a contemporary of Leibniz. Locke argued that the mind is in fact devoid of all knowledge or ideas at birth; it is a blank sheet or "tabula rasa". He argued that all our ideas are constructed in the mind via a process of constant composition and decomposition of the input that we receive through our senses.

Locke, in "An Essay Concerning Human Understanding", suggests that the concept of universal assent in fact proves nothing, except perhaps that everyone is in agreement; in short universal assent proves that there is universal assent and nothing else. Moreover, Locke goes on to suggest that in fact there "is" no universal assent. Even a phrase such as "What is, is" is not universally assented to; infants and severely handicapped adults do not generally acknowledge this truism. Locke also attacks the idea that an innate idea can be imprinted on the mind without the owner realizing it. For Locke, such reasoning would allow one to conclude the absurd: “all the Truths a Man ever comes to know, will, by this account, be, every one of them, innate.” To return to the musical analogy, we may not be able to recall the entire melody until we hear the first few notes, but we were aware of the fact that we knew the melody and that upon hearing the first few notes we would be able to recall the rest.

Locke ends his attack upon innate ideas by suggesting that the mind is a "tabula rasa" or "blank slate", and that all ideas come from experience; all our knowledge is founded in sensory experience.

Essentially, the same knowledge thought to be "a priori" by Leibniz is in fact, according to Locke, the result of empirical knowledge, which has a lost origin [been forgotten] in respect to the inquirer. However, the inquirer is not cognizant of this fact; thus, he experiences what he believes to be "a priori" knowledge.

1) The theory of innate knowledge is excessive. Even innatists accept that most of our knowledge is learned through experience, but if that can be extended to account for all knowledge, we learn colour through seeing it, so therefore, there is no need for a theory about an innate understanding of colour.

2) No ideas are universally held. Do we all possess the idea of God? Do we all believe in justice and beauty? Do we all understand the law of identity? If not, it may not be the case that we have acquired these ideas through impressions/experience/social interaction (this is the children's and idiot's criticism).

3) Even if there are some universally agreed statements, it is just the ability of the human brain to organize learned ideas/words, that is, innate. An "ability to organize" is not the same as "possessing propositional knowledge" (e.g., a computer with no saved files has all the operations programmed in but has an empty memory).

Gottfried Wilhelm Leibniz suggested that we are born with certain innate ideas, the most identifiable of these being mathematical truisms. The idea that "1 + 1 = 2" is evident to us without the necessity for empirical evidence. Leibniz argues that empiricism can only show us that concepts are true in the present; the observation of one apple and then another in one instance, and in that instance only, leads to the conclusion that one and another equals two. However, the suggestion that one and another will always equal two require an innate idea, as that would be a suggestion of things unwitnessed.

Leibniz called such concepts as mathematical truisms "necessary truths". Another example of such may be the phrase, "what is, is" or "it is impossible for the same thing to be and not to be". Leibniz argues that such truisms are universally assented to (acknowledged by all to be true); this being the case, it must be due to their status as innate ideas. Often there are ideas that are acknowledged as necessarily true but are not universally assented to. Leibniz would suggest that this is simply because the person in question has not become aware of the innate idea, not because they do not possess it. Leibniz argues that empirical evidence can serve to bring to the surface certain principles that are already innately embedded in our minds. This is similar to needing to hear only the first few notes in order to recall the rest of the melody.

In his "Meno", Plato raises an important epistemological quandary: How is it that we have certain ideas which are not conclusively derivable from our environments? Noam Chomsky has taken this problem as a philosophical framework for the scientific enquiry into innatism. His linguistic theory, which derives from 18th century classical-liberal thinkers such as Wilhelm von Humboldt, attempts to explain in cognitive terms how we can develop knowledge of systems which are said, by supporters of innatism, to be too rich and complex to be derived from our environment. One such example is our linguistic faculty. Our linguistic systems contain a systemic complexity which supposedly could not be empirically derived: the environment seems too poor, variable and indeterminate, according to Chomsky, to explain the extraordinary ability to learn complex concepts possessed by very young children. Essentially, their accurate grammatical knowledge cannot have originated from their experiences as their experiences are not adequate. It follows that humans must be born with a universal innate grammar, which is determinate and has a highly organized directive component, and enables the language learner to ascertain and categorize language heard into a system. Chomsky states that the ability to learn how to properly construct sentences or know which sentences are grammatically incorrect is an ability gained from innate knowledge. Noam Chomsky cites as evidence for this theory, the apparent invariability, according to his views, of human languages at a fundamental level. In this way, linguistics may provide a window into the human mind, and establish scientific theories of innateness which otherwise would remain merely speculative.

One implication of Noam Chomsky's innatism, if correct, is that at least a part of human knowledge consists in cognitive predispositions, which are triggered and developed by the environment, but not determined by it. Chomsky suggests that we can look at how a belief is acquired as an input-output situation. He supports the doctrine of innatism as he states that human beliefs gathered from sensory experience are much richer and complex than the experience itself. He asserts that the extra information gathered is from the mind itself as it cannot solely be from experiences. Humans derive excess amount of information from their environment so some of that information must be pre-determined.

Parallels can then be drawn, on a purely speculative level, between our moral faculties and language, as has been done by sociobiologists such as E. O. Wilson and evolutionary psychologists such as Steven Pinker. The relative consistency of fundamental notions of morality across cultures seems to produce convincing evidence for these theories. In psychology, notions of archetypes such as those developed by Carl Jung, suggest determinate identity perceptions.

Evidence for innatism is being found by neuroscientists working on the Blue Brain Project. They discovered that neurons transmit signals despite an individual's experience. It had been previously assumed that neuronal circuits are made when the experience of an individual is imprinted in the brain, making memories. Researchers at Blue Brain discovered a network of about fifty neurons which they believed were building blocks of more complex knowledge but contained basic innate knowledge that could be combined in different more complex ways to give way to acquired knowledge, like memory.

Scientists ran tests on the neuronal circuits of several rats and ascertained that if the neuronal circuits had only been formed based on an individual's experience, the tests would bring about very different characteristics for each rat. However, the rats all displayed similar characteristics which suggests that their neuronal circuits must have been established previously to their experiences – it must be inborn and created prior to their experiences. The Blue Brain Project research suggests that some of the "building blocks" of knowledge are genetic and present at birth.

There are two ways in which animals can gain knowledge. The first of these two ways is learning. This is when an animal gathers information about its surrounding environment and then proceeds to use this information. For example, if an animal eats something that hurts its stomach, it has learned not to eat this again. The second way that an animal can acquire knowledge is through innate knowledge. This knowledge is genetically inherited. The animal automatically knows it without any prior experience. An example of this is when a horse is born and can immediately walk. The horse has not learned this behavior; it simply knows how to do it. In some scenarios, innate knowledge is more beneficial than learned knowledge. However, in other scenarios the opposite is true.

In a changing environment, an animal must constantly be gaining new information in order to survive. However, in a stable environment this same individual need only to gather the information it needs once and rely on it for the duration of its life. Therefore, there are different scenarios in which learning or innate knowledge is better suited.
Essentially, the cost of obtaining certain knowledge versus the benefit of having it determined whether an animal evolved to learn in a given situation or whether it innately knew the information. If the cost of gaining the knowledge outweighed the benefit of having it, then the individual would not have evolved to learn in this scenario; instead, non-learning would evolve. However, if the benefit of having certain information outweighed the cost of obtaining it, then the animal would be far more likely to evolve to have to learn this information.

Non-learning is more likely to evolve in two scenarios. If an environment is static and change does not or rarely occurs then learning would simply be unnecessary. Because there is no need for learning in this scenario – and because learning could prove to be disadvantageous due to the time it took to learn the information – non-learning evolves. However, if an environment were in a constant state of change then learning would also prove to be disadvantageous. Anything learned would immediately become irrelevant because of the changing environment. The learned information would no longer apply. Essentially, the animal would be just as successful if it took a guess as if it learned. In this situation, non-learning would evolve.

However, in environments where change occurs but is not constant, learning is more likely to evolve. Learning is beneficial in these scenarios because an animal can adapt to the new situation, but can still apply the knowledge that it learns for a somewhat extended period of time. Therefore, learning increases the chances of success as opposed to guessing and adapts to changes in the environment as opposed to innate knowledge.






</doc>
<doc id="252866" url="https://en.wikipedia.org/wiki?curid=252866" title="Altered state of consciousness">
Altered state of consciousness

An altered state of consciousness (ASC), also called altered state of mind or mind alteration, is any condition which is significantly different from a normal waking state. By 1892, the expression was in use in relation to hypnosis although an ongoing debate about hypnosis as an ASC based on modern definition exists. The next retrievable instance, by Dr Max Mailhouse from his 1904 presentation to conference, however, is unequivocally identified as such, as it was in relation to epilepsy, and is still used today. In academia, the expression was used as early as 1966 by Arnold M. Ludwig and brought into common usage from 1969 by Charles Tart. It describes induced changes in one's mental state, almost always temporary. A synonymous phrase is "altered state of awareness".

There is no general definition of an altered state of consciousness, as any definitional attempt would firstly have to rely on a definition of a normal state of consciousness. Attempts to define the term can however be found in Philosophy, Psychology and Neuroscience. There is no final consensus what the most accurate definition for what purpose is. In the following, the best established and latest definitions are provided:

"An altered state is any mental state(s), induced by various physiological, psychological, or pharmacological maneuvers or agents, which can be recognized subjectively by the individual himself (or by an objective observer of the individual) as representing a sufficient deviation in subjective experience of psychological functioning from certain general norms for that individual during alert, waking consciousness."

Starting from this Charles Tart focuses his definition on the subjective experience of a state of consciousness and its deviation from a normal waking state.

"Altered states of consciousness are alternate patterns or configurations of experience, which differ qualitatively from a baseline state."

Farthing's definition of an altered state of consciousness (ASC) is based on Charles Tart's terminology. Charles Tart described an altered state of consciousness as a profound change in the "overall pattern of subjective experiences". In order to define an ASC, Tart focuses on the importance of subjective experience.

Farthing adds to his definition that an ASC is short-termed or at least reversible and that it might not even be recognized as an ASC at that moment. His definition relies only on subjective experience, leaving aside behavioral changes and physiological response.

"An altered state of consciousness (ASC) may be defined as a temporary change in the overall pattern of subjective experience, such that the individual believes that his or her mental functioning is distinctly different from certain general norms for his or her normal waking state of consciousness". Farthing (1992, p. 205)

He lists fourteen dimensions of changed subjective experience. To account for an ASC, multiple dimensions need to be altered.

A recent working definition for empirical research is based on these previous definitions and provided by Schmidt.

[Translated from German]: 
As a working definition for neuroscientific research, it might suffice to presume that most people have a strong intuition concerning which variability in their everyday wakeful state feels normal to them. This variability of experience is considered as normal fluctuation, while any state that is experienced to diverge significantly from it can be called an ASC. From an experimental perspective, it is also reasonable to compare ASC conditions to a baseline state – a state subjectively judged as average, or normal. The comparison with a 'normal' baseline requires that the ASC under investigation is of relatively short duration (minutes to hours), which differentiates ASCs from most pathological conditions. Importantly, it has been emphasized that an ASC is not a mere quantitative change in a single cognitive function (e.g. elevated arousal). Instead, it is a multidimensional phenomenon. Thereby, the relative intensity of multiple consciousness aspects constitutes a 'phenomenological pattern' characterizing a particular state. Such 'patterns' have also been referred to as relative changes in the '(basic) dimensions of consciousness'. For empirical research, such patterns correspond to a multivariate combination of independent 'consciousness factors', which can be quantified via questionnaires. The 'phenomenological pattern' results from the factor structure of the applied psychometric assessment, i.e. the individual ratings, or factor scores, of a questionnaire.

Altered states of consciousness might have been employed by humans as early as 30,000 years ago. Mind-altering plants and/or excessive dancing were used to attain an ecstatic or mystic state. Examples of early religious use of altered states of consciousness are the rites of Dionysos and the Eleusinian Mysteries, as well as Yoga and Meditation. Followers of various shamanic traditions "enter altered states of consciousness in order to serve their community." Terence McKenna has suggested that the use of psychedelic mushrooms in prehistoric times has led to the "evolution of human language and symbol use". Some theorists propose that mind-altering substances, such as Soma, might have pushed the formation of some of the world's main religions.

Meditation in its various forms is being rediscovered by modern psychology because of its therapeutic potential and its ability to "enable the activity of the mind to settle down". In Psychotherapy techniques like hypnosis, meditation, support psychological processes.

Due to the behaviourist paradigm in psychology altered states of consciousness were dismissed as a field of scientific inquiry during the early 20th century. They were pathologized and merely seen as symptoms of intoxication or demonic possession.

Their return into psychology began with Wiliam James' interest into a variety of altered states, such as "mystical experiences and drug-induced states". James' investigations into first-person-subjective-experience contributed to the reconsideration of introspection as a valuable research method in the academic community.

The social change of the turbulent 1960s has decisively led to a change of the scientific perspective to the point that introspection as a scientific method and ASCs as valid realms of experience became more widely accepted. Foundations for the research have been laid out by various scientists such as Abraham Maslow, Walter N. Pahnke, Stanislav Grof and Charles Tart. They focused on seemingly beneficial aspects of ASCs such as their potential to "promote creativity or treat addiction". Rather oppressive states such as dissociation from trauma were neglected.

The findings of the famous Good Friday Experiment by Pahnke suggest that mystical experiences can be triggered by psilocybin. Later investigations by Rick Doblin found that participants valued those experiences as "spiritual high points of their lives".

In the midst of the rise of new-age subculture Stanislav Grof and others formed the new field of transpersonal psychology, which emphasized "the importance of individual human experience, validity of mystical and spiritual experience, interconnectedness of self with others and the world and potential of self-transformation".

Abraham Maslow's research on peak experiences, as moments of "highest happiness and fulfillment", further contributed to the depathologization of altered states.

A first summary of the existing literature was carried out by Charles T. Tart in his book "Altered the States of Consciousness", which led to a more common use of the term. Tart coined the key terms discrete and baseline states of consciousness and thought about a general classification system for ASCs. He also called for "state specific sciences" in which researchers should do science on ASCs from within such states.

A classification of Altered States of Consciousness is helpful if one wants to compare or differentiate between induced ASCs and other variations of consciousness. Various researchers have attempted the classification into a broader framework. The attempts of classification discussed in the following focus on slightly different aspects of ASCs. Several authors suggested classification schemata with regard to the genesis of altered states and with regard to the type of experiences:

A classification with five categories was suggested by Vaitl to distinguish ASCs according to how they were induced:


Vaitl further suggests four basic aspects of experiences: (1) activation (2) awareness span (3) self-awareness (4) sensory dynamics. Alternatively Roland Fischer suggests a classification along ergotropic (i.e., ecstasy) or trophotropic (i.e., meditation) properties. The work of Adolph Dittrich aimed to empirically determine common underlying dimensions of consciousness altererations induced by different methods, such as drugs or non-pharmacological methods. He suggested three basic dimensions, which were termed: (1) oceanic boundlessness (2) dread of ego dissolution (3) visionary restructuralization. Further, Ken Wilber proposes a multidimensional system and adds that the individual experience of an ASC is shaped by a person's unique psychological development.

Michael Winkelman identifies four different "modes of consciousness": (1) the waking mode (2) the deep sleep mode (3) the REM sleep / dreaming mode (4) the integrative mode. Within this framework, many ASCs (psychedelics, hypnosis, meditation, etc.) are defined as belonging to the integrative mode.

An altered state of consciousness may be defined as a short-term change in the general configuration of one's individual experience, such that the rational functioning is clearly altered from one's usual state of consciousness. There are many ways that one's consciousness can be altered, such as by using psychoactive drugs, which are defined as chemical substances that pass through the blood and disturb brain function, causing changes in awareness, attitude, consciousness, and behavior.

Cannabis is a psychoactive drug that is known to alter the state of consciousness. Cannabis alters mental activity, memory, and pain perception. One who is under the influence of cannabis may (or may not) experience degrees of paranoia, increased sensitivity, and delayed reactions not normal for their usual conscious state. A 2009 review of anxiety and cannabis studies concluded that “frequent cannabis users appear to have higher levels of anxiety than non-users,” and that “a considerable number of subjects developed anxiety disorders before the first symptoms of cannabis dependence.” That led researchers to believe that anxiety-prone people tend to use cannabis as a self-prescribed anxiety medicine, opposing the idea that cannabis is what's causing the anxiety.

MDMA (ecstasy) is a drug that also alters one's state of consciousness. The state of consciousness brought about by MDMA ingestion includes a rise in positive feelings and a reduction in negative feelings (Aldridge, D., & Fachner, J. ö. 2005). Users' emotions are increased and inhibitions lowered, often accompanied by a sensation of intimacy or connection with other people.

Opioids are a class of drugs that alter consciousness. Examples of opioids include heroin, morphine, hydrocodone, and oxycodone. Opioids produce analgesia and often feelings of euphoria in users. Opioid abuse may result in decreased production of endorphins in the brain, natural pain relievers whose effects may be heightened by drugs. If one takes a large dose of opioids to compensate for the lack of natural endorphins, the result may be death.

Cocaine alters one's state of consciousness. Cocaine affects the neurotransmitters that nerves use to communicate with each other. Cocaine inhibits the reuptake of norepinephrine, serotonin, dopamine, and other neurotransmitters in the synapse, resulting in an altered state of consciousness or a "high" (Aldridge, D., & Fachner, J. ö. 2005).

Lysergic acid diethylamide, or LSD, activates serotonin receptors (the amine transmitter of nerve urges) in brain matter. LSD acts on certain serotonin receptors, and its effects are most prominent in the cerebral cortex, an area involved in attitude, thought, and insight, which obtains sensory signs from all parts of the body. LSD's main effects are emotional and psychological. The ingester's feelings may alter quickly through a range from fear to ecstasy. (Humphrey, N. 2001) This may cause one to experience many levels of altered consciousness.

Alcohol alters consciousness by shifting levels of neurotransmitters. Neurotransmitters are endogenous chemicals that transmit signals across a synapse from one neuron (nerve cell) to another "target" cell (often another neuron). Neurotransmitters can cause inhibitory or excitatory effects on the "target" cell they are affecting. Alcohol increases the effect of the neurotransmitter GABA (gamma-Aminobutyric acid) in the brain. GABA causes slow actions and inaudible verbal communication that often occur in alcoholics. Alcohol also decreases the excitatory neurotransmitter glutamate. Suppressing this stimulant results in a similar type of physiological slowdown. In addition to increasing the GABA and decreasing the glutamate in the brain, alcohol increases the amount of the chemical dopamine in the brain, which is one of the addictive causes of alcoholism.

Altered states of consciousness may also be induced by:

Emotions influence behavior that alters the state of consciousness. Emotions can be influenced by various stimuli.

Pathological or accidental induction may refer to unforeseen events or illnesses. According to Dr. Jeffrey R. Avner, professor of clinical pediatrics, a crucial element to understanding accidental and pathological causes to altered states of consciousness (ASC) is that it begins with reduced self-awareness followed by reduced awareness in the environment (2006). Those with personal experience of conditions such as Depersonalisation often cite the opposite, that it is an increased awareness of the environment and the self that results in altered states of consciousness. When the reduction of self-awareness and environmental awareness take effect, they produce altered states of consciousness. The specific conditions below provide clarity on the types of conditions compromise accidental and pathological causes.

The first condition, traumatic experience, is defined as a lesion caused by an external force (Trauma. (n.d.) In Merriam-Webster Dictionary online, 2013). Examples include impact to the brain caused by blunt force (i.e., a car accident). The reason a traumatic experience causes altered states of consciousness is that it changes how the brain works. The external impact diverts the blood flow from the front of the brain to other areas. The front of the brain is known as the prefrontal cortex responsible for analytical thought (Kunsman, 2012). When the damage becomes uncontrollable, the patient experiences changes in behavior and impaired self-awareness. This is exactly when an altered state of consciousness is experienced.

Another common cause is epilepsy. According to Medlineplus epilepsy can be described as a brain disorder that causes seizures (2013). During the seizure it is said that the patient will experience hallucinations and loss of mental control causing temporary dissociation from reality. 
A study that was conducted with six epileptic patients and used the functional magnetic resonance imaging (fMRI) detected how the patients did indeed experience hallucinations while a seizure is occurring. This not only altered the patient's behavioral pattern but also made them dissociate from reality during that particular time frame.

The next item of interest is oxygen deficiency, questioning how oxygen deficiency impacts the brain is an important part of comprehending why ASC occurs when there is oxygen deprivation in an environment.

In addition to oxygen deprivation or deficiency, infections are a common pathological cause of ASC. A prime example of an infection includes meningitis. The medical website WEBMD states that meningitis is an infection that causes the coverings of the brain to swell. This particular infection occurs in children and young adults. This infection is primarily viral. Viral meningitis causes ASC and its symptoms include fevers and seizures (2010). The Impairment becomes visible the moment seizures begin to occur, this is when the patient enters the altered state of consciousness.

Sleep deprivation is also associated with ASC, and can provoke seizures due to fatigue. Sleep deprivation can be chronic or short-term depending on the severity of the patient's condition. Many patients report hallucinations because sleep deprivation impacts the brain. An MRI study conducted at Harvard Medical school in 2007, found that a sleep-deprived brain was not capable of being in control of its sensorimotor functions, leading to an impairment to the patient's self-awareness. Patients were also prone to be a lot clumsier than if had they not been experiencing sleep deprivation.

Coupled with deprivation of sleep and oxygen, another form of deprivation includes fasting. Fasting can occur because of religious purposes or from psychological conditions such as anorexia. Fasting refers to the ability to willingly refrain from food and possibly drinks as well. The dissociation caused by fasting is not only life-threatening but it is the reason why extended fasting periods can lead to ASC. Thus, the temporary dissociation from reality allows fasting to fall into the category of an ASC following the definition provided by Dr. Avner (2006).

Another pathological cause is psychosis, otherwise known as a psychotic episode. In order to comprehend psychosis, it is important to determine what symptoms it implies. Psychotic episodes often include delusions, paranoia, derealization, depersonalization, and hallucinations (Revonsuo et al., 2008). Studies have not been able to clearly identify when a person is reaching a higher level of risk for a psychotic episode (Schimmelmann, B., Walger, P., & Schultze-Lutter, F., 2013), but the earlier people are treated for psychosis the more likely they are to avoid the devastating consequences which could lead to a psychotic disorder (Schimmelmann, B., Walger, P., & Schultze-Lutter, F., 2013). Unfortunately, there are very few studies which have thoroughly investigated psychotic episodes, and the ability to predict this disorder remains unclear. (Schimmelmann, B., Walger, P., & Schultze-Lutter, F., 2013).

Reviewing the previous conditions for accidental and pathological causes, we can come to understand that all of these accidental or pathological causes share the component of reduced self-awareness. Therefore, ASCs cannot only be caused naturally but they can be induced intentionally with methods including hypnosis meditation, amongst others. There are also ASCs which are caused by less recreational purposes; people who utilize illegal substances, or heavy dosages of medications, as well as large amounts of alcohol, can indeed comply with the definition of an ASC (Revonsuo et al., 2008).

The entropic brain hypothesis by Robin L. Carhart-Harris (2014). refers to a theory which is informed by neuroimaging research that uses the hallucinogen induced neurological state to make inferences about other states of consciousness.
The expression "entropy" is applied here in the context of states of consciousness and their associated neurodynamics, while high entropy is synonymous with high disorder.
It is proposed that a general distinction can be made between two fundamentally different modes of cognition: Primary and secondary consciousness.

Primary consciousness is associated with unconstrained cognition and less ordered (higher-entropy) neurodynamics that preceded the development of modern, normal waking consciousness in adults. Examples include the psychedelic state, the rapid eye movement sleep (REM) state or the onset phase of psychosis.
Secondary consciousness is associated with constrained cognition and more ordered neurodynamics. Examples include normal waking consciousness, the anesthetized or the depressed state.

The theory further proposes that via pharmacological induction of psychedelic substances psilocybin, the brain is able to enter into the primary state of consciousness (the psychedelic state) from normal waking consciousness. This "phase transition" between these two fundamentally different poles of consciousness is facilitated by a collapse of the normally highly organized activity within the default mode network (DMN) and a decoupling between the DMN and the medial temporal lobes (MTLs), which are normally significantly coupled.
The DMN is closely associated with higher-order cognitive functions such as supporting the neurological basis for the self (e.g. self-reflection, subjectivity, introspection), thinking about others (e.g. theory of mind), remembering the past and thinking about the future (e.g. episodic memory). Task-positive networks are associated with the inverse of these things e.g., focus on and scrutiny of the external world.

The entropic brain hypothesis emphasizes the great research potential of the psychedelic state of mind for gaining more insight into general human consciousness.

Extensive scientific investigation on altered states of consciousness and their relationship to drug interactions with receptors in the brain have been performed. Particularly the study of the neurotransmitter serotonin and the effects of psychedelic drugs on the brain has been intensively researched over the past sixty years. It has been hypothesized that hallucinogens act either as an antagonist or an agonist at serotonin-2A receptors and will elicit a state that shares some common phenomenological features with early acute stages of the group of schizophrenia disorders.

Findings implicate that abnormalities of serotonin function and the serotonergic system could be responsible for psychiatric disorders such as the spectrum of schizophrenia (gating) disorders and therefore, that serotonin agonist or antagonists might be useful in the treatment of e.g. schizophrenia. To investigate the underlying causative neurotransmitter mechanisms of this phenomenon, the CSTC (cortico-striato-thalamo-cortical) loop model has been formulated based on empirical neurobiological work. 
It is indicated that the common hypofrontality (overactivation of frontal brain parts) and cortical activation pattern induced by serotonergic and glutamatergic hallucinogens is due to a common disruption of thalamic gating of sensory and cognitive information. The CSTC feedback loop plays a major role in gating or filtering out external and internal information to the cortex. Thereby it influences the regulation of the level of awareness and attention.

Disruption of the CSTC loop system is proposed to significantly influence information processing, e.g. the ability to screen out, inhibit filter or gate extraneous stimuli and to direct selective attention to salient features of the environment.
Failures of these attentional gating mechanisms might overload patients with the excessive processing of both sensory and cognitive stimuli, which could lead to a breakdown of cognitive integrity and difficulty in distinguishing self from non-self and failure to integrate an overwhelming flood of information. Descriptive elaboration of the mentioned effects can be found in the literature on schizophrenia as well as in descriptions of hallucinogenic drug action.

Despite strong evidence linking serotonin and psychosis, novel research indicates that some behavioral effects of drugs such as psilocybin appear to be independent of the classical 5-HT2A receptor-agonist actions, implying that the model described here is not the only underlying framework at play. Interdisciplinary research enterprises have set out to study the convergence of serotonergic and glutamatergic models of psychosis and dynamic neurotransmitter interactions, derived from the study of hallucinogenic drugs, in the future.




</doc>
<doc id="7973164" url="https://en.wikipedia.org/wiki?curid=7973164" title="Anomalous experiences">
Anomalous experiences

Anomalous experiences, such as so-called benign hallucinations, may occur in a person in a state of good mental and physical health, even in the apparent absence of a transient trigger factor such as fatigue, intoxication or sensory deprivation.

The evidence for this statement has been accumulating for more than a century. Studies of benign hallucinatory experiences go back to 1886 and the early work of the Society for Psychical Research, which suggested approximately 10% of the population had experienced at least one hallucinatory episode in the course of their life. More recent studies have validated these findings; the precise incidence found varies with the nature of the episode and the criteria of "hallucination" adopted, but the basic finding is now well-supported.

Of particular interest, for reasons to be discussed below, are those anomalous experiences which are characterised by extreme perceptual realism.

A common type of anomalous experience is the apparitional experience, which may be defined as one in which a subject seems to perceive some person or thing that is not physically present. Self-selected samples tend to report a predominance of human figures, but apparitions of animals, and even objects are also reported. Notably, the majority of the human figures reported in such samples are not recognised by the subject, and of those who are, not all are of deceased persons; apparitions of living persons have also been reported.

Out-of-body experiences (OBEs) have become to some extent conflated in the public mind with the concept of the near-death experience. However, the evidence suggests that the majority of out-of-body experiences do not occur near death, but in conditions of either very high or very low arousal. McCreery has suggested that this latter paradox may be explained by reference to the fact that sleep may be approached, not only by the conventional route of low arousal and deafferentation, but also by the less familiar route of extreme stress and hyper-arousal. On this model OBEs represent the intrusion of Stage 1 sleep processes into waking consciousness.

OBEs can be regarded as hallucinatory in the sense that they are perceptual or quasi-perceptual experiences in which by definition the ostensible viewpoint is not coincident with the physical body of the subject. Therefore, the normal sensory input, if any, that the subject is receiving during the experience cannot correspond exactly to the perceptual representation of the world in the subject's consciousness.

As with hallucinatory experiences in general, attempts to survey samples of the general population have suggested that such experiences are relatively common, incidence figures of between 15 and 25 percent being commonly reported. The variation is presumably to be accounted for by the different types of populations sampled and the different criteria of ‘out-of-body experience’ used.

A dream has been defined by some (e.g. "Encyclopædia Britannica") as a hallucinatory experience during sleep. 

A lucid dream may be defined as one in which the dreamer is aware that he or she is asleep and dreaming. The term ‘lucid dream’ was first used by the Dutch physician Frederik van Eeden, who studied his own dreams of this type. The word ‘lucid’ refers to the fact that the subject has achieved insight into his or her condition, rather than the perceptual quality of the experience. Nevertheless, it is one of the features of lucid dreams that they can have an extremely high quality of perceptual realism, to the extent that the dreamer may spend time examining and admiring the perceptual environment and the way it appears to imitate that of waking life.

Lucid dreams by definition occur during sleep, but they may be regarded as hallucinatory experiences in the same way as non-lucid dreams of a vivid perceptual nature may be regarded as hallucinatory, that is they are examples of 'an experience having the character of sense perception, but without relevant or adequate sensory stimulation […]' 

A false awakening is one in which the subject believes he/she has woken up, whether from a lucid or a non-lucid dream, but is in fact still asleep. Sometimes the experience is so realistic perceptually (the sleeper seeming to wake in his or her own bedroom, for example) that insight is not achieved at once, or even until the dreamer really wakes up and realises that what has occurred was hallucinatory. Such experiences seem particularly liable to occur to those who deliberately cultivate lucid dreams. However, they may also occur spontaneously and be associated with the experience of sleep paralysis.

Psychotic-like symptoms, such as hallucinations and unusual perceptual experience, involve gross alterations in the experience of reality. Normal perception is substantially constructive and what we perceive is strongly influenced by our prior experiences and expectancies. Healthy individuals prone to hallucinations, or scoring highly on psychometric measures of positive schizotypy, tend to show a bias toward reporting stimuli that did not occur under perceptually ambiguous experimental conditions. During visual detection of fast-moving words, undergraduate students scoring highly on positive schizotypy had significantly high rates of false perceptions of words (i.e. reported seeing words that were not included in the experimental trials). Positive schizotypal symptoms in healthy adults seem to predict false perceptions in laboratory tasks and certain environmental parameters such as perceptual load and frequency of visual targets are critical in the generation of false perceptions. When detection of events becomes either effortless or cognitively demanding, generation of such biases can be prevented.

Auditory hallucinations, and in particular the hearing of a voice, are thought of as particularly characteristic of people suffering from schizophrenia. However, normal subjects also report auditory hallucinations to a surprising extent. For example, Bentall and Slade found that as many as 15.4% of a population of 150 male students were prepared to endorse the statement "In the past I have had the experience of hearing a person's voice and then found that no one was there". They add: "no less than 17.5% of the [subjects] were prepared to score the item 'I often hear a voice speaking my thoughts aloud' as 'Certainly Applies'. This latter item is usually regarded as a first-rank symptom of schizophrenia ..."

Green and McCreery found that 14% of their 1800 self-selected subjects reported a purely auditory hallucination, and of these nearly half involved the hearing of articulate or inarticulate human speech sounds. An example of the former would be the case of an engineer facing a difficult professional decision, who, while sitting in a cinema, heard a voice saying, "loudly and distinctly": ‘You can't do it, you know". He adds: "It was so clear and resonant that I turned and looked at my companion who was gazing placidly at the screen ... I was amazed and somewhat relieved when it became apparent that I was the only person who had heard anything."

This case would be an example of what Posey and Losch call "hearing a comforting or advising voice that is not perceived as being one's own thoughts". They estimated that approximately 10% of their population of 375 American college students had this type of experience.

It has been suggested that auditory hallucinations are affected by culture, to the extent that when American subjects were examined they reported hearing stern authoritarian voices with violent or prohibitive suggestions, whereas voices heard in India and Africa tended to be playful and collaborative instead.

Hypnogogic and hypnopompic hallucinations occur in people without other symptoms and are considered non-pathological.

This is a paradoxical experience in which the person has a strong feeling of the presence of another person, sometimes recognised, sometimes unrecognised, but without any apparently justifying sensory stimulus.

The nineteenth-century American psychologist and philosopher William James described the experience thus: "From the way in which this experience is spoken of by those who have had it, it would appear to be an extremely definite and positive state of mind, coupled with a belief in the reality of its object quite as strong as any direct sensation ever gives. And yet no sensation seems to be connected with it at all ... The phenomenon would seem to be due to a pure conception becoming saturated with the sort of stinging urgency which ordinarily only sensations bring."

The following is an example of this type of experience: "My husband died in June 1945, and 26 years afterwards when I was at Church, I felt him standing beside me during the singing of a hymn. I felt I would see him if I turned my head. The feeling was so strong I was reduced to tears. I had not been thinking of him before I felt his presence. I had not had this feeling before that day, neither has it happened since then."

Experiences of this kind appear to meet all but one of the normal criteria of hallucination. For example, Slade and Bentall proposed the following working definition of a hallucination: "Any percept-like experience which (a) occurs in the absence of an appropriate stimulus, (b) has the full force or impact of the corresponding actual (real) perception, and (c) is not amenable to direct and voluntary control by the experiencer." 
The experience quoted above certainly meets the second and third of these three criteria. One might add that the "presence" in such a case is experienced as located in a definite position in external physical space. In this respect it may be said to be more hallucinatory than, for example, some hypnagogic imagery, which may be experienced as external to the subject but located in a mental "space" of its own. Other explanations for this phenomenon were discussed by the psychologist Graham Reed who wrote that such experiences may involve illusion, misinterpretation or suggestion. He noted that the experiences are usually reported at moments of fatigue, stress, or during the night.

The experience of sensing the presence of a deceased loved one is a commonly reported phenomenon in bereavement. It can take the form of a clearly sensory impression or can involve a quasi-sensory 'feeling' of presence. Rees conducted a study of 293 widowed people living in a particular area of mid-Wales. He found that 14% of those interviewed reported having had a visual hallucination of their deceased spouse, 13.3% an auditory one and 2.7% a tactile one. These categories overlapped to some extent as some people reported a hallucinatory experience in more than one modality. Of interest in light of the previous heading was the fact that 46.7% of the sample reported experiencing the presence of the deceased spouse. Other studies have similarly reported a frequency of approximately 50% in the bereaved population.

Sensing the presence of the deceased may be a cross-cultural phenomenon that is, however, interpreted differently depending on the cultural context in which it occurs. For example, one of the earliest studies of the phenomenon published in a Western peer-reviewed journal investigated the grief experiences of Japanese widows and found that 90% of them reported to have sensed the deceased. It was observed that, in contrast to Western interpretations, the widows were not concerned about their sanity and made sense of the experience in religious terms.

In the Western world, much of the bereavement literature of the 20th century had been influenced by psychoanalytic thinking and viewed these experiences as a form of denial, in the tradition of Freud's interpretation in "Mourning and Melancholia" of the bereaved person as 'clinging to the object through the medium of a hallucinatory wishful psychosis'. In recent decades, building on cross-cultural evidence about the adaptiveness of such experiences, the continuing bonds perspective as originated by Klass et al. (1996) has suggested that such experiences can be seen as normal and potentially adaptive in a Western context too. Since then, a number of qualitative studies have been published, describing the mainly beneficial effects of these experiences, especially when they are made sense of in spiritual or religious ways While most of these experiences tend to be reported as comforting to the perceiver, a small percentage of people have reported disturbing experiences, and there is ongoing research, for example by Field and others, to determine when continuing bonds experiences serve adjustment to bereavement and when they may be detrimental.

The main importance of anomalous experiences such as benign hallucinations to theoretical psychology lies in their relevance to the debate between the disease model versus the dimensional model of psychosis. According to the disease model, psychotic states such as those associated with schizophrenia and manic-depression, represent symptoms of an underlying disease process, which is dichotomous in nature; i.e. a given subject either does or does not have the disease, just as a person either does or does not have a physical disease such as tuberculosis. According to the dimensional model, by contrast, the population at large is ranged along a normally distributed continuum or dimension, which has been variously labelled as psychoticism (H.J.Eysenck), schizotypy (Gordon Claridge) or psychosis-proneness.

The occurrence of spontaneous hallucinatory experiences in persons who are enjoying good physical health at the time, and who are not drugged or in other unusual physical states of a transient nature such as extreme fatigue, would appear to provide support for the dimensional model. The alternative to this view requires one to posit some hidden or latent disease process, of which such experiences are a symptom or precursor, an explanation which would appear to beg the question.

The "argument from hallucination" has traditionally been one of those used by proponents of the philosophical theory of representationalism against direct realism. Representationalism holds that when perceiving the world we are not in direct contact with it, as common sense suggests, but only in direct contact with a representation of the world in consciousness. That representation may be a more or less accurate one depending on our circumstances, the state of our health, and so on. Direct realism, on the other hand, holds that the common sense or unthinking view of perception is correct, and that when perceiving the world we should be regarded as in direct contact with it, unmediated by any representation in consciousness.

Clearly, during an apparitional experience, for example, the correspondence between how the subject is perceiving the world and how the world really is at that moment is distinctly imperfect. At the same time the experience may present itself to the subject as indistinguishable from normal perception. McCreery has argued that such empirical phenomena strengthen the case for representationalism as against direct realism.




</doc>
<doc id="7664848" url="https://en.wikipedia.org/wiki?curid=7664848" title="World riddle">
World riddle

The term "world riddle" or "world-riddle" has been associated, for over 100 years, with Friedrich Nietzsche (who mentioned "Welträthsel" in several of his writings)
and with the biologist-philosopher Ernst Haeckel, who, as a professor of zoology at the University of Jena,
wrote the book "Die Welträthsel" in 1895–1899, in modern spelling "Die Welträtsel" (German "The World-riddles"), with the English version published under the title "The Riddle of the Universe", 1901.

The term "world riddle" concerns the nature of the universe and the meaning of life.

The question and answer of the World Riddle has also been examined as an inspiration or allegorical meaning within some musical compositions, such as the unresolved harmonic progression at the end of "Also sprach Zarathustra" (1896) by composer Richard Strauss, made famous in the film "".
Friedrich Nietzsche referred to the "World Riddle" ("Welträthsel") in several of his writings; however, his direct influence was limited to a few years, by his failing health.

Ernst Haeckel viewed the World Riddle as a dual-question of the form, "What is the nature of the physical universe and what is the nature of human thinking?" which he explained would have a single answer since humans and the universe were contained within one system, a mono-system, as Haeckel wrote in 1895:

Haeckel had written that human behavior and feeling could be explained, within the laws of the physical universe, as "mechanical work of the ganglion-cells" as stated.

The philosopher and psychologist William James has questioned the attitude of thinking that a single answer applies to everything or everyone. In his book "Pragmatism" (1907) he satirized the world-riddle as follows:

Emil du Bois-Reymond used the term "World Riddle" in 1880 for seven great questions of science, such as the ultimate nature of matter and the origin of simple sensations. In the lecture to the Berlin Academy of Sciences he declared that neither science nor philosophy could ever explain these riddles.




</doc>
<doc id="61571384" url="https://en.wikipedia.org/wiki?curid=61571384" title="Noocenosis">
Noocenosis

A Noocenosis (alternative spelling noocenose or noocoenosis) from Greek (noos) — "mind, thought, perception, sense" and (cenosis) – "common, mutual, shared, joint", is an artificial biological community (biocenosis) built upon a degraded ecosystem. A noocenosis is the result of structural improvements by man and differs from the original, evolutionarily constructed biological community. The concept noocenosis was first used in Russia (spelling: «нооценоз») in the 1990s by the ecologist Victor Vasilievich Petrashov (Петрашов, Виктор Васильевич). Academic use of the term noocenosis is still predominately limited to Post Soviet countries, with a few exceptions. Closely related terms include "Noobiocenosis" and "Noobiogeocenosis". The term "noobiogeocenosis was introduced in the 1970s by the ecologist Stanislav Semenovich Shwarts (Щварц, Станислав Семёнович).
The concept noocenosis belongs to the discipline Ecology and was established by the Russian professor and ecologist Victor Vasilievich Petrashov in his 1993 publication "Введение в нооценологи" (Introduction to Noocenology). Parallel to the ecological characterization based on Petrashov the term has been adopted in other disciplines, including Economics, Industrial Ecology, Agricultural Science, Philosophy and other Humanity studies. 

The interdisciplinary use of the term noocenosis, makes it difficult to provide one exclusive definition. The usages of the noocenosis concept can be split into two general groups: the conceptional use of the term in the field of ecology as a fully restored and improved biosphere of second order, and the usage as a social-informational unit in the fields of industrial ecology, agricultural sciences, economics. and humanity studies. The ecological conception of a noocenosis was introduced by Petrashov. In line with Petrashov's original definition; a fully restored and improved biosphere of second order, the ecologists Ekaterina Valerievna Shen and Alexander Edwinovich Wegosky have further developed the concept noocenosis, based on the experience gathered in their own organization, and the works of other naturalists, including Vladimir Ivanovich Vernadsky, Nikita Nikolayevich Moiseyev, and Nikolay Wladimirowitsch Timofeev-Ressovsky. The two characterizations of a noocenosis; as a restored natural complex and a social-informational unit, overlap one another. Wegosky, for instance, also incorporates the social-informational aspect of a noocenosis into the ecological characterization. A noocenosis, being a man-made natural complex, Wegosky further describes as a "nature-regenerating social system".

Viktor Vasilievich Petrashov, Professor at the State University for Agricultural Sciences – Moscow Timirjaev established the concept in the field of ecology. The various publications from Petrashov build the foundation for an ecological understanding of the noocenosis concept. 
According to Petrashov's definition, a noocenosis is a community of human and non-human organisms, which interact in an artificial habitat (biotope), following the destruction of the original habitat through the actions of man-kind. A noocenosis differs from other man-made, communities of natural organisms (cenoses), such as found in agricultural ecosystems or woodland plantations for economic use because of its complexity. The biodiversity, the territorial expansion of different species and the ecological structure of a noocenosis is comparable with that of the original biocenosis. Therefore, a noocenosis, is a system of "second-order", and has the same complexity as the evolutionary constructed ecosystem.

The level of complexity of a biocenosis can be measured as the sum of the biochemical systems it contains. The stability of an ecosystem is a direct result of the number of species and the number of ecological relationships between these species on their abiotic habitat. As such, an ecosystem in which the biochemical material circulation is constant and includes a great organic mass is generally more resilient. For example, in bio-diverse ecosystems the loss of one species is much more likely to be compensated by another. Thus, the creation of noocenoses includes not only the "repair" of anthropological damages to a natural community, but rather attempts to strengthen the resistance of said community against further disturbances.

Petrashov based the noocenosis conception on the theoretical words of Vladimir Ivanovich Vernadsky, founder of the Noosphere concept as well as Vladimir Nikolayevich Sukachev; the founder of the term "Biogeocenosis" and the correlating field of study. Next to the works of these authors, Petrashov used his own field research as an ecologist in the Russian Federation as the basis for the new concept. Petrashov was greatly influenced by Vladimir Ivanovich Vernadsky and his work on the Noosphere. Petrashov considered that the construction of noocenoses as the active implementation of the noosphere. Therefore, the entirety of all individual noocenosis make up the noosphere in the same way that many biocenoses make up the biosphere.
There is a distinct difference separating the noocenosis concept from other forms of nature restoration. A noocenosis differs fundamentally from the results of natural evolution, and therefore not synonymous with the concepts "Ecological Restoration" or "Conservation". In contrast, a noocenosis is a constructed and managed ecosystem which is based on the scientific observation of the previous biocenoses which have been destroyed by man.

Petrashov suggested that the terms "Noogeocenosis" and "Nooecocenosis" be used when the abiotic surroundings and the corresponding ecosystem of a noocenosis community are being studied. A noocenosis, according to Petrashov, is a community of organisms including humans, who take on the role of a "rational creator". When human kind compensates its own damage to the earth's natural ecosystems, giving back energy and substance for the repair of interrupted biochemical systems, it becomes an active member of a noocenosis.

The restoration of natural landscapes includes all processes which increase the biodiversity of a certain biotope. According to Petrashov, the restoration of the original, evolutionarily developed biotope is always preferable. The re-acclimatization of dominant species is, in most cases, easier because these species are already best adapted to the regional conditions. The constructed noocenosis should be, when and where possible, constructed so as to resemble the original biotope, which was destroyed by human-kind. Thus, a noocenosis may be created according to either 1) the type of destroyed biocenosis, 2) characteristics which best match the current phase of the biocenosis in the evolution of biotopes, or 3) the type of biocenosis, which best matches the resulting natural conditions, should the conditions have drastically changed from the original ones.

The Russian ecologist Stanislav Semenovich Shwarts used the term noobiogeocenosis in his 1974 publication, almost 20 years before Petrashov's publications on the noocenosis. Shwarts' conception has a few important parallels with Petrashov's, though the latter considered there to be no connection. Shwarts recognized that any attempt to restore biocenoses to their original form is prematurely condemned to failure. Therefore, he saw it to be necessary to rethink the idea of a "good biosphere". He suggested the term "noobiogeocenosis" to characterize newly constructed biocenoses which are equal to their naturally developed predecessors.
Further, Shwarts' conception vastly differs from Petrashov's. Shwarts characterizes the work of as an ecologist as following: to create a general scheme for the development of flourishing biogeocenoses in an urbanized environment. This scheme can be implemented as an industrial complex in conjunction with measures for the industrial development of the region and of the country. Schwarts sees a noobiogeocenosis as an aspect of "human ecology". The main task of this branch of science is to develop a general scheme for the development of a biogeocenotic cover of a single economic and geographical region.
Shwarts's definition of a noobiogeocenosis and the role of ecologists in its creation, is conceptually very close to the modern field of industrial ecology, which has been defined elsewhere as a "systems-based, multidisciplinary discourse that seeks to understand emergent behavior of complex integrated human/natural systems".
The field of industrial ecology also focuses on the economic and social impacts of the material exchange between industrial objects and their environment. 
In the field of industrial ecology, the term noocenosis has been used with reference to Shwarts' original charachterization of a noobiogeocenosis. According to recent literature in this discipline, a noocenosis is simply а human, technical and/or informational unit. The term is used to characterize the relationship between human production systems and the non-human environment. Furthermore, an important differentiation is that a noocenosis is seen as a separate instance from the biocenosis.

In contrast, a noocenosis is, for Petrashov, an ecological concept that focuses on the resilience of a biocenosis, the complexity of its ecological connections and its stage in the line of evolutionary development. All these factors are evaluated within the context of human-industrial degradation of biocenoses, the state which characterizes the modern world. Petrashov views human flourishing not as a goal per se, but rather as the bi-product of flourishing biocenoses. Social development is only possible through the rational construction and maintenance of biocenoses.

The Russian ecologists Ekaterina Valerievna Shen (Екатерина Валерьевна Шен) and Alexander Edwinovich Wegosky have been continuing and furthering Petrashov's ecological conception. They are practitioners of the new profession, which Petrashov first defined: Noocenology. The profession of a Noocenologist (Russ: Нооценолог) includes the construction of noocenoses. The corresponding specialization is Noocenology (Russ: Нооценология). The specialization is concerned with the construction of flourishing ecosystems of the second degree.
As a discipline, noocenology tries to compensate for damage caused by humans by transforming destroyed biocenoses that cannot be restored to their original state. Such destroyed biocenoses are converted into new systems based on the knowledge gained in the study of previous biocenoses. The irreversible character of ecological destruction appears as the central and defining principle of the field. The creation of noocoenoses is therefore a solution tailored to the reality of post-anthropogenic landscapes, which for various reasons can no longer be returned back to their pre-anthropogenic status. The profession of a noocenologist, as Petrashow emphasizes, is in light of the ecological crisis in which the planet finds itself, the only profession that could secure the future of humanity.

Wegosky and Shen have further developed the nocenosis concept as a practical discipline. Both ecologists are members of the Organization: Nature Revive Service – Nonprofit Partnership Service for Ecological Restoration of Degraded Landscapes, an independent and self-sustaining collective aimed at creating noocenoses - man-made socio-biogeosystems on postanthropogenic wastelands. Their definition of a noocenosis differs from that of Petrashov, and is as follows: A noocenosis is 

Wegosky and Shen's focus on the application of the concept noocenosis goes further than Petrashov's conception and encompasses the legal, economic, social and psychological aspects and barriers of creating noocenoses in a commercial society. In line with the ecologist Moiseyev, who contributed important developments to the Noosphere Theory (See Section The Noospheric era: Nikita Nikolaievich Moiseyev), Wegosky and Shen see systematic structures which inhibit the creation and growth of a new moral imperative, and thus the noospheric era. This is the central theoretical characteristic in which their theoretical conception differs from Petrashov's.

Wegosky's further contends, that consumer civilization hinders the creation of noocenoses as it lacks mature individuals, which act as edifiers and are able to create a rational nature complex based on scientific evidence of material flows and ecosystem development. Ideological limitations, specifically the fulfillment and creation of excess needs in a commercial society, result in production ecosystems, that block out all other options for organizing life, except for the constant improvement of schemes for absorbing resources and distributing the "benefits" of production.

As co-founder of the organization Nature Revive Service – Nonprofit Partnership Service for Ecological Restoration of Degraded Landscapes, active since 1992, Wegosky has developed with Shen the program practical noocenology which deals with the real world creation of noocenoses. These noocenose they refer to as "zones of life-thickening". These spaces are filled with as many protected ecological and social niches as possible.
Practical Noocenology draws on Nikolay Timofeev-Ressovsky's conception of biospheric rents. The focus is moved from fiscal profitability to ecological profitability, in which the use of raw material does not exceed the "rent" provided by the planetary substance. The biosphere according to Timofeev-Ressovsky, can be viewed as a giant combine, on which humanity is obliged to live with a certain percentage of 'green turnover'". The destruction of these biospheric rents has been jeopardizing the future existence of the human race for decades and has reached a critical threshold. Noocenology, the creation of noocenoses, should lay the formation of a natural information network, without which it is impossible to solve pressing evolutionary problems.

The term noocenosis is currently used in other disciplines other than ecology. This interdisciplinary use of the term differs from the original ecological characterization. For example, in the fields of Industrial Ecology and Agricultural Sciences a noocenosis is a social or economic complex (concretely this means the culmination of society, human labor and the material products of labor). This characterization is closer to Shwarts' concept: Noobiogeocenosis.
References to the concept noocenosis in the fields of Economics, Social Sciences and Humanity Studies have, up until now, been limited to references to the character of a noocenosis as an informational network of human thought. Such definitions do not share a foundation with Shwarts' conception of a noobiogeocenosis as the subject of human ecology, not are they related to Petrashov's original ecological characterization: a man-made nature-complex of second order.

The term noocenosis is used almost exclusively in the academic literature of Post-Soviet States. There have been few exceptions in which the concept was referenced by academics in other countries. A few university textbooks of ecology and industrial ecology, written in the Russian or Ukrainian Language, have used the term noocenosis.

The Kazakhstan National Agricultural University Almaty lists "noocenosis" as a foundational concept in the ecological education of students of agricultural sciences. Furthermore, the concept is being used in Ukrainian Universities and is referenced in textbooks in the Ukrainian Language.
Perhaps the most recent publication concerning the concept noocenosis is from Andre Kayukov (А.Н Каюков), Institute of Land Management, Cadastre- and Environmental Management in Krasnoyarsk. In his 2018 publication, Kazukov defines a Noobiogeocenosis as an artificial ecosystem. A Noobiogeocenosis differs from a biocenosis in that is contains another equally important community: a noocenosis. A noocenosis is made up of society, labor and the products of labor. By putting the needs and demands of the biocenosis at an equal level with those of society ( defined here as noocenosis) Kayukov's conception vastly differences from the ecological conception, in which the flourishing of a biocenosis is the primary goal of human interaction with nature.

With in the field of Industrial Ecology there are differing conceptions. For one, a noocenosis is generally defined as the social environment, and the degree of its change as a result of mineral and raw materials exchange.
On the other hand, the concept has a certain spatial character and is understood as the sphere of human kind in contrast to other spheres made up of plant and animal life and their abiotic habitats. It is a technical and scientific center of material exchange between industrial complexes and natural biocenoses.

A textbook of industrial ecology published by the University of Saratov prefers the term noobiogeocenosis, which it clarifies, is an elementary structure of the noosphere and one the possible systems of interaction between industry and nature. As a system, it includes territory, time, organization and scientific research. As a form of relationship between people and nature, it should enable the rational use of natural resources. The era of the noosphere must be preceded by a profound socio-economic restructuring of society, which is aimed at a careful attitude towards nature. 
A noobiogeocenosis is made up of even smaller units: a еcotop (here defined as all abiotic components of the environment), a biocenosis (all biotic components of the environment) and a noocenosis (the human- productive component in relationship to the environment). The noocenosis, it further describes as "a social-economic complex", made up of labor, society and the material products of labor. 
Furthermore a noocenosis is defined by the author as a community of rationalism (сообщества разума). Specifically, this means industrial production systems, that are based on rational principles about the living and non-living environment. The content of what makes a community rational is however, as further stated, determined by the general trajectory of social development. The principle of rationality, being determined by the trajectory of social development, once again separates the industrial ecology definition of a noocenosis from the ecological characterization based on Petrashov. Similarly to the reception in the field of agricultural science, this characterization allows the demands of human society to characterize the human relationship to natural objects. In contrast, Petrashov's conception considers the creation and maintenance of flourishing biocenoses to be the basis from which human relationships to nature must be constructed. Not the development of human society dictates the structure of a biocenosis, but the structure and ecological connections of the biocenosis determine the trajectory of social development.

Yan Vyacheslavovich Shokin (Ян Вячеславович Шокин ), Professor of Digital Economics and Management at the State University "Dubna", interpreted the concept noocenosis as the result of the natural convergence of economic units with coinciding economic interests. The essence of this interpretation is the a certain wavelength of economic interests, which are connected through a mutual exchange of motivation. Shokin reinterprets the term noocoenosis in the sense of the theory of economic interest to mean the overlap of certain interests in closed economic systems. The noocoenosis retains some loose connection to the biocenosis as it is a unit of the biosphere, in which "the economic activity clearly exceeds that of its surroundings". </ref> Шокин, Я.В. The concept is based on the noosphere concept in which man appears as a rational creator of his environment and has to act within the natural limits of his biosphere. Shokin reinterprets the term noocoenosis in the sense of the theory of economic interest and as an overlap of certain interests in closed economic systems. After all, there remains a noocoenosis as a unit of the biosphere, in which the economic activity clearly exceeds that of its surroundings.

The Russian-American philosopher, literary critic, cultural and linguist Mikhail Epstein introduced the term into the English language, with the following definition:

Epstein's definition identifies solely the noocenosis as an informational unit. Any mention of biocenoses are merely metaphorical. 

The Publication: „Global studies. International Interdisciplinary Encyclopedic Dictionary." provided a definition of a noocenosis along with four other terms that share the common root "noo": Noobiogeocenosis, Noosphere, and the noospheric, spiritual and environmental assembly of the world, noospheric worldview
(нообиогеоценоз, ноосфера, ноосферная духовно-экологическая ассамблея мира, ноосферное мировоззрение).

The concept noocenosis is an extension of the philosophical concept: noosphere. The noosphere has been primarily in Russia a topic of study and debate, but has also caught international academic attention in the past 30 years, inspiring many publications and even international conferences. 

The noosphere was predominantly defined by the Russian Biogeochemicist Vladimir Ivanovich Vernadsky as the sphere of (human) reason and the next planetary layer following the biosphere. Other important authors in the creation of the concept include Pierre Teilhard de Chardin and Edourd Le Roy. Vernadsky developed a theory about the trajectory of planetary development, derived from his observation that man-kind is the most relevant geological factor. The specific influence of man-kind on the earth is, he concluded, the result of scientific thought. The extension of communication systems, the creation of new materials, the colonization of the entire planet by the human race, and the geographic changes to planetary landscapes and systems are examples of the geological impact of man kind, and are predicated on man-kind's propensity to scientific thought. Vernadsky theorized that the culmination of evolution will be the onset of the noosphere, which is a scientific and socio-cultural transition of human society as well as a physical transition of the planet's biosphere. The ability for humankind as a species not only to shape the biosphere through scientific thought, work and technology, but also to consciously reflect on itself as a function of the biosphere together make up this new state of planetary organization.
Vernadsky saw in the 20th century the onset of the transition to the noosphere. Not only did he witness an onset of technological and scientific progress but Vernadsky claimed that conditions of his time enabled and forced man-kind to, for the first time, recognise himself as an inhabitant of the planet and to act from a planetary point of view. The transition to the noosphere thus encompasses a certain maturity of thought resulting not only in the proliferation of science and technological progress, but fore-mostly the appearance of human reason in relation to the species' planetary role. The development of a reasonable and reflected society would, for the first time, allow man-kind to consciously direct the co-evolution of human society and the biosphere. The noosphere is the active shaping of the planet through reflexive human thought, as far as it is a function of the earth's biogeochemical processes. Furthermore, the noosphere appears as the next planetary layer: the evolution of the biosphere. Vernadsky argued that science, human thought, human labor and technology are part of a greater evolutionary process, and are thus governed by the scientific laws governing all planetary processes. The „alien" environment; the biosphere, exerts stress upon technological progress, so that even the development of machines must follow the same planetary laws which determine the development of all organisms.

The concept "Noosphere", as a scientific concept, was further developed almost exclusively by Russian authors. A few exceptions can be found among humanities and philosophy academics in western countries. The most notable and influential author to pursue the scientific, Vernadskian conception of the noosphere has been Nikita Nikolaiavich Moiseyev with his publication "Человек и ноосфера" ("Man and the Noosphere") in 1990. Moiseyev identified an important contradiction in Vernadsky's theory. Vernadsky described the noosphere as an ultimately irreversible, teleological process, but also stressed the necessity of its active implementation by man-kind. Moiseyev concludes that the latter conception, that is, considering the noosphere as a "project" for human-kind requiring active implementation, best suits the reality emerging in the late 20th century. The epoch of the noosphere requires not only the active transformation of societal structure, but also the development of "a new morality". As such, creating the noosphere requires the radical restructuring of our entire being; the shift of standards and ideals. Moiseyev himself, attempted to contribute towards the active implementation of the noosphere through his participation in environmental and ecological advocacy internationally and within the Soviet Union. Moiseyev's publication was an important transition away from the Vernadskian conception of the Noosphere as an inevitable evolutionary stage, and towards the realization that the onset of noosphere is conditional upon human-kind's ability and willingness to actively implement it.


</doc>
<doc id="2208074" url="https://en.wikipedia.org/wiki?curid=2208074" title="Neurophilosophy">
Neurophilosophy

Neurophilosophy or philosophy of neuroscience is the interdisciplinary study of neuroscience and philosophy that explores the relevance of neuroscientific studies to the arguments traditionally categorized as philosophy of mind. The philosophy of neuroscience attempts to clarify neuroscientific methods and results using the conceptual rigor and methods of philosophy of science.

Below is a list of specific issues important to philosophy of neuroscience:

Many of the methods and techniques central to neuroscientific discovery rely on assumptions that can limit the interpretation of the data. Philosophers of neuroscience have discussed such assumptions in the use of functional magnetic resonance imaging, dissociation in cognitive neuropsychology, single unit recording, and computational neuroscience. Following are descriptions of many of the current controversies and debates about the methods employed in neuroscience.

Many fMRI studies rely heavily on the assumption of "localization of function" (same as functional specialization). Localization of function means that many cognitive functions can be localized to specific brain regions. A good example of functional localization comes from studies of the motor cortex. There seem to be different groups of cells in the motor cortex responsible for controlling different groups of muscles. Many philosophers of neuroscience criticize fMRI for relying too heavily on this assumption. Michael Anderson points out that subtraction method fMRI misses a lot of brain information that is important to the cognitive processes. Subtraction fMRI only shows the differences between the task activation and the control activation, but many of the brain areas activated in the control are obviously important for the task as well.

Some philosophers entirely reject any notion of localization of function and thus believe fMRI studies to be profoundly misguided. These philosophers maintain that brain processing acts holistically, that large sections of the brain are involved in processing most cognitive tasks (see holism in neurology and the modularity section below). One way to understand their objection to the idea of localization of function is the radio repair man thought experiment. In this thought experiment, a radio repair man opens up a radio and rips out a tube. The radio begins whistling loudly and the radio repair man declares that he must have ripped out the anti-whistling tube. There is no anti-whistling tube in the radio and the radio repair man has confounded function with effect. This criticism was originally targeted at the logic used by neuropsychological brain lesion experiments, but the criticism is still applicable to neuroimaging. These considerations are similar to Van Orden's and Paap's criticism of circularity in neuroimaging logic. According to them, neuroimagers assume that their theory of cognitive component parcellation is correct and that these components divide cleanly into feed-forward modules. These assumptions are necessary to justify their inference of brain localization. The logic is circular if the researcher then use the appearance of brain region activation as proof of the correctness of their cognitive theories.

A different problematic methodological assumption within fMRI research is the use of reverse inference A reverse inference is when the activation of a brain region is used to infer the presence of a given cognitive process. Poldrack points out that the strength of this inference depends critically on the likelihood that a given task employs a given cognitive process and the likelihood of that pattern of brain activation given that cognitive process. In other words, the strength of reverse inference is based upon the selectivity of the task used as well as the selectivity of the brain region activation. A 2011 article published in the NY times has been heavily criticized for misusing reverse inference. In the study, participants were shown pictures of their iPhones and the researchers measured activation of the insula. The researchers took insula activation as evidence of feelings of love and concluded that people loved their iPhones. Critics were quick to point out that the insula is not a very selective piece of cortex, and therefore not amenable to reverse inference.

The Neuropsychologist Max Coltheart took the problems with reverse inference a step further and challenged neuroimagers to give one instance in which neuroimaging had informed psychological theory Coltheart takes the burden of proof to be an instance where the brain imaging data is consistent with one theory but inconsistent with another theory. Roskies maintains that Coltheart's ultra cognitive position makes his challenge unwinnable. Since Coltheart maintains that the implementation of a cognitive state has no bearing on the function of that cognitive state, then it is impossible to find neuroimaging data that will be able to comment on psychological theories in the way Coltheart demands. Neuroimaging data will always be relegated to the lower level of implementation and be unable to selectively determine one or another cognitive theory. In a 2006 article, Richard Henson suggests that forward inference can be used to infer dissociation of function at the psychological level. He suggests that these kinds of inferences can be made when there is crossing activations between two task types in two brain regions and there is no change in activation in a mutual control region.

One final assumption worth mentioning is the assumption of pure insertion in fMRI. The assumption of pure insertion is the assumption that a single cognitive process can be inserted into another set of cognitive processes without affecting the functioning of the rest. For example, if you wanted to find the reading comprehension area of the brain, you might scan participants while they were presented with a word and while they were presented with a non-word (e.g. "Floob"). If you infer that the resulting difference in brain pattern represents the regions of the brain involved in reading comprehension, you have assumed that these changes are not reflective of changes in task difficulty or differential recruitment between tasks. The term pure insertion was coined by Donders as a criticism of reaction time methods.

Recently, researchers have begun using a new functional imaging technique called resting state functional connectivity MRI. Subjects' brains are scanned while the subject sits idly in the scanner. By looking at the natural fluctuations in the bold pattern while the subject is at rest, the researchers can see which brain regions co-vary in activation together. They can use the patterns of covariance to construct maps of functionally linked brain areas. The name "functional connectivity" is somewhat misleading since the data only indicates co-variation. Still, this is a powerful method for studying large networks throughout the brain. There are a couple of important methodological issues that need to be addressed. Firstly, there are many different possible brain mappings that could be used to define the brain regions for the network. The results could vary significantly depending on the brain region chosen. Secondly, what mathematical techniques are best about to characterize these brain regions?

The brain regions of interest are somewhat constrained by the size of the voxels. Rs-fcMRI uses voxels that are few millimeters cubed so the brain regions will have to be defined on a larger scale. Two of the statistical methods that are commonly applied to network analysis can work on the single voxel spatial scale, but graph theory methods are extremely sensitive to the way nodes are defined. Brains regions can be divided according to their cellular architectural, according to their connectivity, or according to physiological measures. Alternatively, you could take a theory neutral approach and randomly divide the cortex into partitions of the size of your choosing. As mentioned earlier, there are several approaches to network analysis once the your brain regions have been defined. Seed based analysis begins with an a priori defined seed region and finds all of the regions that are functionally connected to that region. Wig et al. caution that the resulting network structure will not give any information concerning the inter-connectivity of the identified regions or the relations of those regions to regions other than the seed region. Another approach is to use independent component analysis to create spatio-temporal component maps and the components are sorted by components that carry information of interest and those that are caused by noise. Wigs et al. once again warns that inference of functional brain region communities is difficult under ICA. ICA also has the issue of imposing orthogonality on the data. Graph theory uses a matrix to characterize covariance between regions which is then transformed into a network map. The problem with graph theory analysis is that network mapping is heavily influenced by a priori brain region and connectivity (nodes and edges), thus the researcher is at risk for cherry picking regions and connections according to their own theories. However, graph theory analysis is extremely valuable since it is the only method that gives pair-wise relationships between nodes. ICA has the added advantage of being a fairly principled method. It seems that using both methods will be important in uncovering the network connectivity of the brain. Mumford et al. hoped to avoid these issues and use a principled approach that could determine pair-wise relationships using a statistical technique adopted from analysis of gene co-expression networks.

Cognitive Neuropsychology studies brain damaged patients and uses the patterns of selective impairment in order to make inferences on the underlying cognitive structure. Dissociation between cognitive functions is taken to be evidence that these functions are independent. Theorists have identified several key assumptions that are needed to justify these inferences:
1) "Functional Modularity"- the mind is organized into functionally separate cognitive modules.
2). "Anatomical Modularity"- the brain is organized into functionally separate modules. This assumption is very similar to the assumption of functional localization. These assumptions differ from the assumption of functional modularity, because it is possible to have separable cognitive modules that are implemented by diffuse patterns of brain activation.
3)"Universality"- The basic organization of functional and anatomical modularity is the same for all normal humans. This assumption is needed if we are to make any claim about functional organization based on dissociation that extrapolates from the instance of a case study to the population.
4) "Transparency" / "Subtractivity"- the mind does not undergo substantial reorganization following brain damage. It is possible to remove one functional module without significantly altering the overall structure of the system. This assumption is necessary in order to justify using brain damaged patients in order to make inferences about the cognitive architecture of healthy people.

There are three principal types of evidence in cognitive neuropsychology: association, single dissociation and double dissociation. Association inferences observe that certain deficits are likely to co-occur. For example, there are many cases who have deficits in both abstract and concrete word comprehension following brain damage. Association studies are considered the weakest form of evidence, because the results could be accounted for by damage to neighboring brain regions and not damage to a single cognitive system. Single Dissociation inferences observe that one cognitive faculty can be spared while another can be damaged following brain damage. This pattern indicates that a) the two tasks employ different cognitive systems b) the two tasks occupy the same system and the damaged task is downstream from the spared task or c) that the spared task requires fewer cognitive resources than the damaged task. The "gold standard" for cognitive neuropsychology is the double dissociation. Double dissociation occurs when brain damage impairs task A in Patient1 but spares task B and brain damage spares task A in Patient 2 but damages task B. It is assumed that one instance of double dissociation is sufficient proof to infer separate cognitive modules in the performance of the tasks.

Many theorists criticize cognitive neuropsychology for its dependence on double dissociations. In one widely cited study, Joula and Plunkett used a model connectionist system to demonstrate that double dissociation behavioral patterns can occur through random lesions of a single module. They created a multilayer connectionist system trained to pronounce words. They repeatedly simulated random destruction of nodes and connections in the system and plotted the resulting performance on a scatter plot. The results showed deficits in irregular noun pronunciation with spared regular verb pronunciation in some cases and deficits in regular verb pronunciation with spared irregular noun pronunciation. These results suggest that a single instance of double dissociation is insufficient to justify inference to multiple systems.

Charter offers a theoretical case in which double dissociation logic can be faulty. If two tasks, task A and task B, use almost all of the same systems but differ by one mutually exclusive module apiece, then the selective lesioning of those two modules would seem to indicate that A and B use different systems. Charter uses the example of someone who is allergic to peanuts but not shrimp and someone who is allergic to shrimp and not peanuts. He argues that double dissociation logic leads one to infer that peanuts and shrimp are digested by different systems. John Dunn offers another objection to double dissociation. He claims that it is easy to demonstrate the existence of a true deficit but difficult to show that another function is truly spared. As more data is accumulated, the value of your results will converge on an effect size of zero, but there will always be a positive value greater than zero that has more statistical power than zero. Therefore, it is impossible to be fully confident that a given double dissociation actually exists.

On a different note, Alphonso Caramazza has given a principled reason for rejecting the use of group studies in cognitive neuropsychology. Studies of brain damaged patients can either take the form of a single case study, in which an individual's behavior is characterized and used as evidence, or group studies, in which a group of patients displaying the same deficit have their behavior characterized and averaged. In order to justify grouping a set of patient data together, the researcher must know that the group is homogenous, that their behavior is equivalent in every theoretically meaningful way. In brain damaged patients, this can only be accomplished "a posteriori" by analyzing the behavior patterns of all the individuals in the group. Thus according to Caramazza, any group study is either the equivalent of a set of single case studies or is theoretically unjustified. Newcombe and Marshall pointed out that there are some cases (they use Geschwind's syndrome as an example) and that group studies might still serve as a useful heuristic in cognitive neuropsychological studies.

It is commonly understood in neuroscience that information is encoded in the brain by the firing patterns of neurons. Many of the philosophical questions surrounding the neural code are related to questions about representation and computation that are discussed below. There are other methodological questions including whether neurons represent information through an average firing rate or whether there is information represented by the temporal dynamics. There are similar questions about whether neurons represent information individually or as a population.

Many of the philosophical controversies surrounding computational neuroscience involve the role of simulation and modeling as explanation. Carl Craver has been especially vocal about such interpretations. Jones and Love wrote an especially critical article targeted at Bayesian behavioral modeling that did not constrain the modeling parameters by psychological or neurological considerations
Eric Winsberg has written about the role of computer modeling and simulation in science generally, but his characterization is applicable to computational neuroscience.

The computational theory of mind has been widespread in neuroscience since the cognitive revolution in the 1960s. This section will begin with a historical overview of computational neuroscience and then discuss various competing theories and controversies within the field.

Computational neuroscience began in the 1930s and 1940s with two groups of researchers. The first group consisted of Alan Turing, Alonzo Church and John von Neumann, who were working to develop computing machines and the mathematical underpinnings of computer science. This work culminated in the theoretical development of so-called Turing machines and the Church–Turing thesis, which formalized the mathematics underlying computability theory. The second group consisted of Warren McCulloch and Walter Pitts who were working to develop the first artificial neural networks. McCulloch and Pitts were the first to hypothesize that neurons could be used to implement a logical calculus that could explain cognition. They used their toy neurons to develop logic gates that could make computations. However these developments failed to take hold in the psychological sciences and neuroscience until the mid-1950s and 1960s.
Behaviorism had dominated the psychology until the 1950s when new developments in a variety of fields overturned behaviorist theory in favor of a cognitive theory. From the beginning of the cognitive revolution, computational theory played a major role in theoretical developments. Minsky and McCarthy's work in artificial intelligence, Newell and Simon's computer simulations, and Noam Chomsky's importation of information theory into linguistics were all heavily reliant on computational assumptions. By the early 1960s, Hilary Putnam was arguing in favor of machine functionalism in which the brain instantiated Turing machines. By this point computational theories were firmly fixed in psychology and neuroscience.
By the mid-1980s, a group of researchers began using multilayer feed-forward analog neural networks that could be trained to perform a variety of tasks. The work by researchers like Sejnowski, Rosenberg, Rumelhart, and McClelland were labeled as connectionism, and the discipline has continued since then. The connectionist mindset was embraced by Paul and Patricia Churchland who then developed their "state space semantics" using concepts from connectionist theory. Connectionism was also condemned by researchers such as Fodor, Pylyshyn, and Pinker. The tension between the connectionists and the classicists is still being debated today.

One of the reasons that computational theories are appealing is that computers have the ability to manipulate representations to give meaningful output. Digital computers use strings of 1s and 0s in order to represent the content such as this Wikipedia page. Most cognitive scientists posit that our brains use some form of representational code that is carried in the firing patterns of neurons. Computational accounts seem to offer an easy way of explaining how our brains carry and manipulate the perceptions, thoughts, feelings, and actions that make up our everyday experience. While most theorists maintain that representation is an important part of cognition, the exact nature of that representation is highly debated. The two main arguments come from advocates of symbolic representations and advocates of associationist representations.

Symbolic representational accounts have been famously championed by Fodor and Pinker. Symbolic representation means that the objects are represented by symbols and are processed through rule governed manipulations that are sensation to the constitutive structure. The fact that symbolic representation is sensitive to the structure of the representations is a major part of its appeal. Fodor proposed the Language of Thought Hypothesis in which mental representations manipulated in the same way that language is syntactically manipulated in order to produce thought. According to Fodor, the language of thought hypothesis explains the systematicity and productivity seen in both language and thought. 

Associativist representations are most often described with connectionist systems. In connectionist systems, representations are distributed across all the nodes and connection weights of the system and thus are said to be sub symbolic. It is worth noting that a connectionist system is capable of implementing a symbolic system. There are several important aspects of neural nets that suggest that distributed parallel processing provides a better basis for cognitive functions than symbolic processing. Firstly, the inspiration for these systems came from the brain itself indicating biological relevance. Secondly, these systems are capable of storing content addressable memory, which is far more efficient than memory searches in symbolic systems. Thirdly, neural nets are resilient to damage while even minor damage can disable a symbolic system. Lastly, soft constraints and generalization when processing novel stimuli allow nets to behave more flexibly than symbolic systems.

The Churchlands described representation in a connectionist system in terms of state space. The content of the system is represented by an n-dimensional vector where the n= the number of nodes in the system and the direction of the vector is determined by the activation pattern of the nodes. Fodor rejected this method of representation on the grounds that two different connectionist systems could not have the same content. Further mathematical analysis of connectionist system relieved that connectionist systems that could contain similar content could be mapped graphically to reveal clusters of nodes that were important to representing the content. Unfortunately for the Churchlands, state space vector comparison was not amenable to this type of analysis. Recently, Nicholas Shea has offered his own account for content within connectionist systems that employs the concepts developed through cluster analysis.

Computationalism, a kind of functionalist philosophy of mind, is committed to the position that the brain is some sort of computer, but what does it mean to be a computer? The definition of a computation must be narrow enough so that we limit the number of objects that can be called computers. For example, it might seem problematic to have a definition wide enough to allow stomachs and weather systems to be involved in computations. However, it is also necessary to have a definition broad enough to allow all of the wide varieties of computational systems to compute. For example, if the definition of computation is limited to syntactic manipulation of symbolic representations, then most connectionist systems would not be able to compute. Rick Grush distinguishes between computation as a tool for simulation and computation as a theoretical stance in cognitive neuroscience. For the former, anything that can be computationally modeled counts as computing. In the latter case, the brain is a computing function that is distinct from systems like fluid dynamic systems and the planetary orbits in this regard. The challenge for any computational definition is to keep the two senses distinct.

Alternatively, some theorists choose to accept a narrow or wide definition for theoretical reasons. Pancomputationalism is the position that everything can be said to compute. This view has been criticized by Piccinini on the grounds that such a definition makes computation trivial to the point where it is robbed of its explanatory value.

The simplest definition of computations is that a system can be said to be computing when a computational description can be mapped onto the physical description. This is an extremely broad definition of computation and it ends up endorsing a form of pancomputationalism. Putnam and Searle, who are often credited with this view, maintain that computation is observer-related. In other words, if you want to view a system as computing then you can say that it is computing. Piccinini points out that, in this view, not only is everything computing, but also everything is computing in an indefinite number of ways. Since it is possible to apply an indefinite number of computational descriptions to a given system, the system ends up computing an indefinite number of tasks.

The most common view of computation is the semantic account of computation. Semantic approaches use a similar notion of computation as the mapping approaches with the added constraint that the system must manipulate representations with semantic content. Note from the earlier discussion of representation that both the Churchlands' connectionist systems and Fodor's symbolic systems use this notion of computation. In fact, Fodor is famously credited as saying "No computation without representation". Computational states can be individuated by an externalized appeal to content in a broad sense (i.e. the object in the external world) or by internalist appeal to the narrow sense content (content defined by the properties of the system). In order to fix the content of the representation, it is often necessary to appeal to the information contained within the system.
Grush provides a criticism of the semantic account. He points out that appeal to the informational content of a system to demonstrate representation by the system. He uses his coffee cup as an example of a system that contains information, such as the heat conductance of the coffee cup and the time since the coffee was poured, but is too mundane to compute in any robust sense. Semantic computationalists try to escape this criticism by appealing to the evolutionary history of system. This is called the biosemantic account. Grush uses the example of his feet, saying that by this account his feet would not be computing the amount of food he had eaten because their structure had not been evolutionarily selected for that purpose. Grush replies to the appeal to biosemantics with a thought experiment. Imagine that lightning strikes a swamp somewhere and creates an exact copy of you. According to the biosemantic account, this swamp-you would be incapable of computation because there is no evolutionary history with which to justify assigning representational content. The idea that for two physically identical structures one can be said to be computing while the other is not should be disturbing to any physicalist.

There are also syntactic or structural accounts for computation. These accounts do not need to rely on representation. However, it is possible to use both structure and representation as constrains on computational mapping. Shagrir identifies several philosophers of neuroscience who espouse structural accounts. According to him, Fodor and Pylyshyn require some sort of syntactic constraint on their theory of computation. This is consistent with their rejection of connectionist systems on the grounds of systematicity. He also identifies Piccinini as a structuralist quoting his 2008 paper: "the generation of output strings of digits from input strings of digits in accordance with a general rule that depends on the properties of the strings and (possibly) on the internal state of the system". Though Piccinini undoubtedly espouses structuralist views in that paper, he claims that mechanistic accounts of computation avoid reference to either syntax or representation. It is possible that Piccinini thinks that there are differences between syntactic and structural accounts of computation that Shagrir does not respect.

In his view of mechanistic computation, Piccinini asserts that functional mechanisms process vehicles in a manner sensitive to the differences between different portions of the vehicle, and thus can be said to generically compute. He claims that these vehicles are medium-independent, meaning that the mapping function will be the same regardless of the physical implementation. Computing systems can be differentiated based upon the vehicle structure and the mechanistic perspective can account for errors in computation.

Dynamical systems theory presents itself as an alternative to computational explanations of cognition. These theories are staunchly anti-computational and anti-representational. Dynamical systems are defined as systems that change over time in accordance with a mathematical equation. Dynamical systems theory claims that human cognition is a dynamical model in the same sense computationalists claim that the human mind is a computer. A common objection leveled at dynamical systems theory is that dynamical systems are computable and therefore a subset of computationalism. Van Gelder is quick to point out that there is a big difference between being a computer and being computable. Making the definition of computing wide enough to incorporate dynamical models would effectively embrace pancomputationalism.







</doc>
<doc id="189616" url="https://en.wikipedia.org/wiki?curid=189616" title="Eudaimonia">
Eudaimonia

Eudaimonia (Greek: ), sometimes anglicized as eudaemonia or eudemonia , is a Greek word commonly translated as happiness or welfare; however, "human flourishing or prosperity" and "blessedness" have been proposed as more accurate translations.

Etymologically, it consists of the words "eu" ("good") and "daimōn" ("spirit"). It is a central concept in Aristotelian ethics and subsequent Hellenistic philosophy, along with the terms "aretē" (most often translated as "virtue" or "excellence") and "phronesis" (often translated as "practical or ethical wisdom"). In Aristotle's works, eudaimonia (based on older Greek tradition) was used as the term for the highest human good, and so it is the aim of practical philosophy, including ethics and political philosophy, to consider (and also experience) what it really is, and how it can be achieved.

Discussion of the links between virtue of character ("ēthikē aretē") and happiness "(eudaimonia)" is one of the central concerns of ancient ethics, and a subject of much disagreement. As a result there are many varieties of eudaimonism. 

The "Definitions", a dictionary of Greek philosophical terms attributed to Plato himself but believed by modern scholars to have been written by his immediate followers in the Academy, provides the following definition of the word eudaimonia: "The good composed of all goods; an ability which suffices for living well; perfection in respect of virtue; resources sufficient for a living creature."

In his "Nicomachean Ethics" (§21; 1095a15–22), Aristotle says that everyone agrees that eudaimonia is the highest good for human beings, but that there is substantial disagreement on what sort of life counts as doing and living well; i.e. eudaimon:

Verbally there is a very general agreement; for both the general run of men and people of superior refinement say that it is [eudaimonia], and identify living well and faring well with being happy; but with regard to what [eudaimonia] is they differ, and the many do not give the same account as the wise. For the former think it is some plain and obvious thing like pleasure, wealth or honour… [1095a17]

So, as Aristotle points out, saying that eudaimon life is a life which is objectively desirable, and means living well, is not saying very much. Everyone wants to be eudaimon; and everyone agrees that being eudaimon is related to faring well and to an individual's well being. The really difficult question is to specify just what sort of activities enable one to live well. Aristotle presents various popular conceptions of the best life for human beings. The candidates that he mentions are a (1) life of pleasure, (2) a life of political activity and (3) a philosophical life.

One important move in Greek philosophy to answer the question of how to achieve eudaimonia is to bring in another important concept in ancient philosophy, "arete" ("virtue"). Aristotle says that the eudaimon life is one of "virtuous activity in accordance with reason" [1097b22–1098a20]. And even Epicurus who argues that the eudaimon life is the life of pleasure maintains that the life of pleasure coincides with the life of virtue. So the ancient ethical theorists tend to agree that virtue is closely bound up with happiness (areté is bound up with eudaimonia). However, they disagree on the way in which this is so. We shall consider the main theories in a moment, but first a warning about the proper translation of areté.

As already noted, the Greek word areté is usually translated into English as "virtue". One problem with this is that we are inclined to understand virtue in a moral sense, which is not always what the ancients had in mind. For a Greek, areté pertains to all sorts of qualities we would not regard as relevant to ethics, for example, physical beauty. So it is important to bear in mind that the sense of ‘virtue' operative in ancient ethics is not exclusively moral and includes more than states such as wisdom, courage and compassion. The sense of virtue which areté connotes would include saying something like "speed is a virtue in a horse", or "height is a virtue in a basketball player". Doing anything well requires virtue, and each characteristic activity (such as carpentry, flute playing, etc.) has its own set of virtues. The alternative translation "excellence" (or "a desirable quality") might be helpful in conveying this general meaning of the term. The moral virtues are simply a subset of the general sense in which a human being is capable of functioning well or excellently.

The Questionnaire for Eudaimonic Well-Being developed in Positive Psychology lists six dimensions of Eudaimonia: self-discovery, perceived development of one's best potentials, a sense of purpose and meaning in life, investment of significant effort in pursuit of excellence, intense involvement in activities and enjoyment of activities as personally expressive.

In terms of its etymology, eudaimonia is an abstract noun derived from "eu" meaning "well" and "daimon" (daemon), which refers to a minor deity or a guardian spirit.

Eudaimonia implies a positive and divine state of being that humanity is able to strive toward and possibly reach. A literal view of eudaimonia means achieving a state of being similar to benevolent deity, or being protected and looked after by a benevolent deity. As this would be considered the most positive state to be in, the word is often translated as 'happiness' although incorporating the divine nature of the word extends the meaning to also include the concepts of being fortunate, or blessed. Despite this etymology, however, discussions of eudaimonia in ancient Greek ethics are often conducted independently of any super-natural significance.

In his "Nicomachean Ethics," (1095a15–22) Aristotle says that eudaimonia means 'doing and living well'. It is significant that synonyms for eudaimonia are living well and doing well. On the standard English translation, this would be to say that ‘happiness is doing well and living well'. The word ‘happiness' does not entirely capture the meaning of the Greek word. One important difference is that happiness often connotes being or tending to be in a certain pleasant state of mind. For example, when we say that someone is "a very happy person", we usually mean that they seem subjectively contented with the way things are going in their life. We mean to imply that they feel good about the way things are going for them. In contrast, eudaimonia is a more encompassing notion than feeling happy since events that do not contribute to one's experience of feeling happy may affect one's eudaimonia.

Eudaimonia depends on all the things that would make us happy if we knew of their existence, but quite independently of whether we do know about them. Ascribing eudaimonia to a person, then, may include ascribing such things as being virtuous, being loved and having good friends. But these are all objective judgments about someone's life: they concern a person's really being virtuous, really being loved, and really having fine friends. This implies that a person who has evil sons and daughters will not be judged to be eudaimonic even if he or she does not know that they are evil and feels pleased and contented with the way they have turned out (happy). Conversely, being loved by your children would not count towards your happiness if you did not know that they loved you (and perhaps thought that they did not), but it would count towards your eudaimonia. So eudaimonia corresponds to the idea of having an objectively good or desirable life, to some extent independently of whether one knows that certain things exist or not. It includes conscious experiences of well being, success, and failure, but also a whole lot more. (See Aristotle's discussion: "Nicomachean Ethics," book 1.10–1.11.)

Because of this discrepancy between the meaning of eudaimonia and happiness, some alternative translations have been proposed. W.D. Ross suggests "well-being" and John Cooper proposes "flourishing". These translations may avoid some of the misleading associations carried by "happiness" although each tends to raise some problems of its own. In some modern texts therefore, the other alternative is to leave the term in an English form of the original Greek, as "eudaimonia".

What we know of Socrates' philosophy is almost entirely derived from Plato's writings. Scholars typically divide Plato's works into three periods: the early, middle, and late periods. They tend to agree also that Plato's earliest works quite faithfully represent the teachings of Socrates and that Plato's own views, which go beyond those of Socrates, appear for the first time in the middle works such as the "Phaedo" and the "Republic." This division will be employed here in dividing up the positions of Socrates and Plato on eudaimonia.

As with all other ancient ethical thinkers, Socrates thought that all human beings wanted eudaimonia more than anything else. (see Plato, "Apology" 30b, "Euthydemus" 280d–282d, "Meno" 87d–89a). However, Socrates adopted a quite radical form of eudaimonism (see above): he seems to have thought that virtue is both necessary and sufficient for eudaimonia. Socrates is convinced that virtues such as self-control, courage, justice, piety, wisdom and related qualities of mind and soul are absolutely crucial if a person is to lead a good and happy (eudaimon) life. Virtues guarantee a happy life eudaimonia. For example, in the "Meno", with respect to wisdom, he says: "everything the soul endeavours or endures under the guidance of wisdom ends in happiness" ["Meno" 88c].

In the "Apology," Socrates clearly presents his disagreement with those who think that the eudaimon life is the life of honour or pleasure, when he chastises the Athenians for caring more for riches and honour than the state of their souls.

Good Sir, you are an Athenian, a citizen of the greatest city with the greatest reputation for both wisdom and power; are you not ashamed of your eagerness to possess as much wealth, reputation, and honors as possible, while you do not care for nor give thought to wisdom or truth or the best possible state of your soul [29e].

... it does not seem like human nature for me to have neglected all my own affairs and to have tolerated this neglect for so many years while I was always concerned with you, approaching each one of you like a father or an elder brother to persuade you to care for "virtue". [31a–b; italics added]

It emerges a bit further on that this concern for one's soul, that one's soul might be in the best possible state, amounts to acquiring moral virtue. So Socrates' point that the Athenians should care for their souls means that they should care for their virtue, rather than pursuing honour or riches. Virtues are states of the soul. When a soul has been properly cared for and perfected it possesses the virtues. Moreover, according to Socrates, this state of the soul, moral virtue, is the most important good. The health of the soul is incomparably more important for eudaimonia than (e.g.) wealth and political power. Someone with a virtuous soul is better off than someone who is wealthy and honoured but whose soul is corrupted by unjust actions. This view is confirmed in the "Crito", where Socrates gets Crito to agree that the perfection of the soul, virtue, is the most important good:

And is life worth living for us with that part of us corrupted that unjust action harms and just action benefits? Or do we think that part of us, whatever it is, that is concerned with justice and injustice, is inferior to the body? Not at all. It is much more valuable…? Much more… (47e–48a)

Here Socrates argues that life is not worth living if the soul is ruined by wrongdoing. In summary, Socrates seems to think that virtue is both necessary and sufficient for eudaimonia. A person who is not virtuous cannot be happy, and a person with virtue cannot fail to be happy. We shall see later on that Stoic ethics takes its cue from this Socratic insight.

Plato's great work of the middle period, the "Republic", is devoted to answering a challenge made by the sophist Thrasymachus, that conventional morality, particularly the 'virtue' of justice, actually prevents the strong man from achieving eudaimonia. Thrasymachus's views are restatements of a position which Plato discusses earlier on in his writings, in the "Gorgias", through the mouthpiece of Callicles. The basic argument presented by Thrasymachus and Callicles is that justice (being just) hinders or prevents the achievement of eudaimonia because conventional morality requires that we control ourselves and hence live with un-satiated desires. This idea is vividly illustrated in book 2 of the "Republic" when Glaucon, taking up Thrasymachus' challenge, recounts a myth of the magical ring of Gyges. According to the myth, Gyges becomes king of Lydia when he stumbles upon a magical ring, which, when he turns it a particular way, makes him invisible, so that he can satisfy any desire he wishes without fear of punishment. When he discovers the power of the ring he kills the king, marries his wife and takes over the throne. The thrust of Glaucon's challenge is that no one would be just if he could escape the retribution he would normally encounter for fulfilling his desires at whim. But if eudaimonia is to be achieved through the satisfaction of desire, whereas being just or acting justly requires suppression of desire, then it is not in the interests of the strong man to act according to the dictates of conventional morality. (This general line of argument reoccurs much later in the philosophy of Nietzsche.) Throughout the rest of the "Republic", Plato aims to refute this claim by showing that the virtue of justice is necessary for eudaimonia.
The argument of the "Republic" is lengthy and complex. In brief, Plato argues that virtues are states of the soul, and that the just person is someone whose soul is ordered and harmonious, with all its parts functioning properly to the person's benefit. In contrast, Plato argues that the unjust man's soul, without the virtues, is chaotic and at war with itself, so that even if he were able to satisfy most of his desires, his lack of inner harmony and unity thwart any chance he has of achieving eudaimonia. Plato's ethical theory is eudaimonistic because it maintains that eudaimonia depends on virtue. On Plato's version of the relationship, virtue is depicted as the most crucial and the dominant constituent of eudaimonia.

Aristotle's account is articulated in the "Nicomachean Ethics" and the "Eudemian Ethics". In outline, for Aristotle, eudaimonia involves activity, exhibiting virtue ("aretē" sometimes translated as excellence) in accordance with reason. This conception of eudaimonia derives from Aristotle's essentialist understanding of human nature, the view that reason ("logos" sometimes translated as rationality) is unique to human beings and that the ideal function or work ("ergon") of a human being is the fullest or most perfect exercise of reason. Basically, well being (eudaimonia) is gained by proper development of one's highest and most human capabilities and human beings are "the rational animal". It follows that eudaimonia for a human being is the attainment of excellence ("areté") in reason.

According to Aristotle, eudaimonia actually requires activity, action, so that it is not sufficient for a person to possess a squandered ability or disposition. Eudaimonia requires not only good character but rational activity. Aristotle clearly maintains that to live in accordance with reason means achieving excellence thereby. Moreover, he claims this excellence cannot be isolated and so competencies are also required appropriate to related functions. For example, if being a truly outstanding scientist requires impressive math skills, one might say "doing mathematics well is necessary to be a first rate scientist". From this it follows that eudaimonia, living well, consists in activities exercising the rational part of the psyche in accordance with the virtues or excellency of reason [1097b22–1098a20]. Which is to say, to be fully engaged in the intellectually stimulating and fulfilling work at which one achieves well-earned success. The rest of the "Nicomachean Ethics" is devoted to filling out the claim that the best life for a human being is the life of excellence in accordance with reason. Since reason for Aristotle is not only theoretical but practical as well, he spends quite a bit of time discussing excellence of character, which enables a person to exercise his practical reason (i.e., reason relating to action) successfully.

Aristotle's ethical theory is eudaimonist because it maintains that eudaimonia depends on virtue. However, it is Aristotle's explicit view that virtue is necessary but not sufficient for eudaimonia. While emphasizing the importance of the rational aspect of the psyche, he does not ignore the importance of other ‘goods' such as friends, wealth, and power in a life that is eudaimonic. He doubts the likelihood of being eudaimonic if one lacks certain external goods such as ‘good birth, good children, and beauty'. So, a person who is hideously ugly or has "lost children or good friends through death" (1099b5–6), or who is isolated, is unlikely to be eudaimon. In this way, "dumb luck" (chance) can preempt one's attainment of eudaimonia.

Pyrrho was the founder of Pyrrhonism. A summary of his approach to eudaimonia was preserved by Eusebius, quoting Aristocles of Messene, quoting Timon of Phlius, in what is known as the "Aristocles passage."

"Whoever wants eudaimonia must consider these three questions: First, how are "pragmata" (ethical matters, affairs, topics) by nature? Secondly, what attitude should we adopt towards them? Thirdly, what will be the outcome for those who have this attitude?" Pyrrho's answer is that "As for "pragmata" they are all adiaphora (undifferentiated by a logical differentia), "astathmēta" (unstable, unbalanced, not measurable), and "anepikrita" (unjudged, unfixed, undecidable). Therefore, neither our sense-perceptions nor our "doxai" (views, theories, beliefs) tell us the truth or lie; so we certainly should not rely on them. Rather, we should be "adoxastoi" (without views), "aklineis" (uninclined toward this side or that), and "akradantoi" (unwavering in our refusal to choose), saying about every single one that it no more is than it is not or it both is and is not or it neither is nor is not.

With respect to aretē, the Pyrrhonist philosopher Sextus Empiricus said: 

If one defines a system as an attachment to a number of dogmas that agree with one another and with appearances, and defines a dogma as an assent to something non-evident, we shall say that the Pyrrhonist does not have a system. But if one says that a system is a way of life that, in accordance with appearances, follows a certain rationale, where that rationale shows how it is possible to seem to live rightly ("rightly" being taken, not as referring only to aretē, but in a more ordinary sense) and tends to produce the disposition to suspend judgment, then we say that he does have a system. 

Epicurus' ethical theory is hedonistic. (His view proved very influential on the founders and best proponents of utilitarianism, Jeremy Bentham and John Stuart Mill.) Hedonism is the view that pleasure is the only intrinsic good and that pain is the only intrinsic bad. An object, experience or state of affairs is intrinsically valuable if it is good simply because of what it is. Intrinsic value is to be contrasted with instrumental value. An object, experience or state of affairs is instrumentally valuable if it serves as a means to what is intrinsically valuable. To see this, consider the following example. Suppose a person spends their days and nights in an office, working at not entirely pleasant activities for the purpose of receiving money. Someone asks them "why do you want the money?", and they answer: "So, I can buy an apartment overlooking the ocean, and a red sports car." This answer expresses the point that money is instrumentally valuable because its value lies in what one obtains by means of it – in this case, the money is a means to getting an apartment and a sports car and the value of making this money dependent on the price of these commodities.

Epicurus identifies the good life with the life of pleasure. He understands eudaimonia as a more or less continuous experience of pleasure and, also, freedom from pain and distress. But it is important to notice that Epicurus does not advocate that one pursue any and every pleasure. Rather, he recommends a policy whereby pleasures are maximized "in the long run". In other words, Epicurus claims that some pleasures are not worth having because they lead to greater pains, and some pains are worthwhile when they lead to greater pleasures. The best strategy for attaining a maximal amount of pleasure overall is not to seek instant gratification but to work out a sensible long term policy.

Ancient Greek ethics is eudaimonist because it links virtue and eudaimonia, where eudaimonia refers to an individual's well being. Epicurus' doctrine can be considered eudaimonist since Epicurus argues that a life of pleasure will coincide with a life of virtue. He believes that we do and ought to seek virtue because virtue brings pleasure. Epicurus' basic doctrine is that a life of virtue is the life which generates the most amount of pleasure, and it is for this reason that we ought to be virtuous. This thesis—the eudaimon life is the pleasurable life—is not a tautology as "eudaimonia is the good life" would be: rather, it is the substantive and controversial claim that a life of pleasure and absence of pain is what eudaimonia consists in.

One important difference between Epicurus' eudaimonism and that of Plato and Aristotle is that for the latter virtue is a constituent of eudaimonia, whereas Epicurus makes virtue a means to happiness. To this difference, consider Aristotle's theory. Aristotle maintains that eudaimonia is what everyone wants (and Epicurus would agree). He also thinks that eudaimonia is best achieved by a life of virtuous activity in accordance with reason. The virtuous person takes pleasure in doing the right thing as a result of a proper training of moral and intellectual character (See e.g., "Nicomachean Ethics" 1099a5). However, Aristotle does not think that virtuous activity is pursued for the sake of pleasure. Pleasure is a byproduct of virtuous action: it does not enter at all into the reasons why virtuous action is virtuous. Aristotle does not think that we literally aim for eudaimonia. Rather, eudaimonia is what we achieve (assuming that we aren't particularly unfortunate in the possession of external goods) when we live according to the requirements of reason. Virtue is the largest constituent in a eudaimon life.
By contrast, Epicurus holds that virtue is the means to achieve happiness. His theory is eudaimonist in that he holds that virtue is indispensable to happiness; but virtue is not a constituent of a eudaimon life, and being virtuous is not (external goods aside) identical with being eudaimon. Rather, according to Epicurus, virtue is only instrumentally related to happiness. So whereas Aristotle would not say that one ought to aim for virtue in order to attain pleasure, Epicurus would endorse this claim.

Stoic philosophy begins with Zeno of Citium c.300 BC, and was developed by Cleanthes (331–232 BC) and Chrysippus (c.280–c.206 BC) into a formidable systematic unity. Zeno believed happiness was a "good flow of life"; Cleanthes suggested it was "living in agreement with nature", and Chrysippus believed it was "living in accordance with experience of what happens by nature". Stoic ethics is a particularly strong version of eudaimonism. According to the Stoics, virtue is necessary and sufficient for eudaimonia. (This thesis is generally regarded as stemming from the Socrates of Plato's earlier dialogues.) We saw earlier that the conventional Greek concept of arete is not quite the same as that denoted by "virtue", which has Christian connotations of charity, patience, and uprightness, since arete includes many non-moral virtues such as physical strength and beauty. However, the Stoic concept of arete is much nearer to the Christian conception of virtue, which refers to the moral virtues. However, unlike Christian understandings of virtue, righteousness or piety, the Stoic conception does not place as great an emphasis on mercy, forgiveness, self-abasement (i.e. the ritual process of declaring complete powerlessness and humility before God), charity and self-sacrificial love, though these behaviors/mentalities are not necessarily spurned by the Stoics (they are spurned by some other philosophers of Antiquity). Rather Stoicism emphasizes states such as justice, honesty, moderation, simplicity, self-discipline, resolve, fortitude, and courage (states which Christianity also encourages).

The Stoics make a radical claim that the eudaimon life is the morally virtuous life. Moral virtue is good, and moral vice is bad, and everything else, such as health, honour and riches, are merely "neutral". The Stoics therefore are committed to saying that external goods such as wealth and physical beauty are not really good at all. Moral virtue is both necessary and sufficient for eudaimonia. In this, they are akin to Cynic philosophers such as Antisthenes and Diogenes in denying the importance to eudaimonia of external goods and circumstances, such as were recognized by Aristotle, who thought that severe misfortune (such as the death of one's family and friends) could rob even the most virtuous person of eudaimonia. This Stoic doctrine re-emerges later in the history of ethical philosophy in the writings of Immanuel Kant, who argues that the possession of a "good will" is the only unconditional good. One difference is that whereas the Stoics regard external goods as neutral, as neither good nor bad, Kant's position seems to be that external goods are good, but only so far as they are a condition to achieving happiness.

Interest in the concept of eudaimonia and ancient ethical theory more generally enjoyed a revival in the twentieth century. G. E. M. Anscombe in her article "Modern Moral Philosophy" (1958) argued that duty-based conceptions of morality are conceptually incoherent for they are based on the idea of a "law without a lawgiver". She claims a system of morality conceived along the lines of the Ten Commandments depends on someone having made these rules. Anscombe recommends a return to the eudaimonistic ethical theories of the ancients, particularly Aristotle, which ground morality in the interests and well being of human moral agents, and can do so without appealing to any such lawgiver.

Julia Driver in the "Stanford Encyclopedia of Philosophy" explains:

Anscombe's article "Modern Moral Philosophy" stimulated the development of virtue ethics as an alternative to Utilitarianism, Kantian Ethics, and Social Contract theories. Her primary charge in the article is that, as secular approaches to moral theory, they are without foundation. They use concepts such as "morally ought", "morally obligated", "morally right", and so forth that are legalistic and require a legislator as the source of moral authority. In the past God occupied that role, but systems that dispense with God as part of the theory are lacking the proper foundation for meaningful employment of those concepts.

Models of eudaimonia in psychology emerged from early work on self-actualization and the means of its accomplishment by researchers such as Erik Erikson, Gordon Allport, and Abraham Maslow. 

Central theories are Diener's tripartite model of subjective well-being, Ryff's Six-factor Model of Psychological Well-being, Keyes work on flourishing, and Seligman's contributions to positive psychology and his theories on "authentic happiness" and P.E.R.M.A. Related concepts are happiness, flourishing, quality of life, contentment, and meaningful life.

The Japanese concept of Ikigai has been described as eudaimonic well-being, as it "entails actions of devoting oneself to pursuits one enjoys and is associated with feelings of accomplishment and fulfillment".





</doc>
<doc id="25140" url="https://en.wikipedia.org/wiki?curid=25140" title="Perception">
Perception

Perception (from the Latin "perceptio") is the organization, identification, and interpretation of sensory information in order to represent and understand the presented information or environment.

All perception involves signals that go through the nervous system, which in turn result from physical or chemical stimulation of the sensory system. For example, vision involves light striking the retina of the eye; smell is mediated by odor molecules; and hearing involves pressure waves.

Perception is not only the passive receipt of these signals, but it's also shaped by the recipient's learning, memory, expectation, and attention. Sensory input is a process that transforms this low-level information to higher-level information (e.g., extracts shapes for object recognition). The process that follows connects a person's concepts and expectations (or knowledge), restorative and selective mechanisms (such as attention) that influence perception.

Perception depends on complex functions of the nervous system, but subjectively seems mostly effortless because this processing happens outside conscious awareness.

Since the rise of experimental psychology in the 19th century, psychology's understanding of perception has progressed by combining a variety of techniques. Psychophysics quantitatively describes the relationships between the physical qualities of the sensory input and perception. Sensory neuroscience studies the neural mechanisms underlying perception. Perceptual systems can also be studied computationally, in terms of the information they process. Perceptual issues in philosophy include the extent to which sensory qualities such as sound, smell or color exist in objective reality rather than in the mind of the perceiver.

Although the senses were traditionally viewed as passive receptors, the study of illusions and ambiguous images has demonstrated that the brain's perceptual systems actively and pre-consciously attempt to make sense of their input. There is still active debate about the extent to which perception is an active process of hypothesis testing, analogous to science, or whether realistic sensory information is rich enough to make this process unnecessary.

The perceptual systems of the brain enable individuals to see the world around them as stable, even though the sensory information is typically incomplete and rapidly varying. Human and animal brains are structured in a modular way, with different areas processing different kinds of sensory information. Some of these modules take the form of sensory maps, mapping some aspect of the world across part of the brain's surface. These different modules are interconnected and influence each other. For instance, taste is strongly influenced by smell.

"Percept" is also a term used by Leibniz, Bergson, Deleuze, and Guattari to define perception independent from perceivers.

The process of perception begins with an object in the real world, known as the "distal stimulus" or "distal object". By means of light, sound, or another physical process, the object stimulates the body's sensory organs. These sensory organs transform the input energy into neural activity—a process called "transduction". This raw pattern of neural activity is called the "proximal stimulus". These neural signals are then transmitted to the brain and processed. The resulting mental re-creation of the distal stimulus is the "percept".

To explain the process of perception, an example could be an ordinary shoe. The shoe itself is the distal stimulus. When light from the shoe enters a person's eye and stimulates the retina, that stimulation is the proximal stimulus. The image of the shoe reconstructed by the brain of the person is the percept. Another example could be a ringing telephone. The ringing of the phone is the distal stimulus. The sound stimulating a person's auditory receptors is the proximal stimulus. The brain's interpretation of this as the "ringing of a telephone" is the percept. 

The different kinds of sensation (such as warmth, sound, and taste) are called "sensory modalities" or "stimulus modalities".

Psychologist Jerome Bruner developed a model of perception, in which people put "together the information contained in" a target and a situation to form "perceptions of ourselves and others based on social categories." This model is composed of three states:


According to Alan Saks and Gary Johns, there are three components to perception:


Stimuli are not necessarily translated into a percept and rarely does a single stimulus translate into a percept. An ambiguous stimulus may sometimes be transduced into one or more percepts, experienced randomly, one at a time, in a process termed ""multistable perception"." The same stimuli, or absence of them, may result in different percepts depending on subject's culture and previous experiences. 

Ambiguous figures demonstrate that a single stimulus can result in more than one percept. For example, the Rubin vase can be interpreted either as a vase or as two faces. The percept can bind sensations from multiple senses into a whole. A picture of a talking person on a television screen, for example, is bound to the sound of speech from speakers to form a percept of a talking person. 

In many ways, vision is the primary human sense. Light is taken in through each eye and focused in a way which sorts it on the retina according to direction of origin. A dense surface of photosensitive cells, including rods, cones, and intrinsically photosensitive retinal ganglion cells captures information about the intensity, color, and position of incoming light. Some processing of texture and movement occurs within the neurons on the retina before the information is sent to the brain. In total, about 15 differing types of information are then forwarded to the brain proper via the optic nerve.
Hearing (or "audition") is the ability to perceive sound by detecting vibrations (i.e., "sonic" detection). Frequencies capable of being heard by humans are called "audio" or "audible" "frequencies", the range of which is typically considered to be between 20 Hz and 20,000 Hz. Frequencies higher than audio are referred to as "ultrasonic", while frequencies below audio are referred to as "infrasonic". 

The auditory system includes the outer ears, which collect and filter sound waves; the middle ear, which transforms the sound pressure (impedance matching); and the inner ear, which produces neural signals in response to the sound. By the ascending auditory pathway these are led to the primary auditory cortex within the temporal lobe of the human brain, from where the auditory information then goes to the cerebral cortex for further processing.

Sound does not usually come from a single source: in real situations, sounds from multiple sources and directions are as they arrive at the ears. Hearing involves the computationally complex task of separating out sources of interest, identifying them and often estimating their distance and direction.

The process of recognizing objects through touch is known as "haptic perception". It involves a combination of somatosensory perception of patterns on the skin surface (e.g., edges, curvature, and texture) and proprioception of hand position and conformation. People can rapidly and accurately identify three-dimensional objects by touch. This involves exploratory procedures, such as moving the fingers over the outer surface of the object or holding the entire object in the hand. Haptic perception relies on the forces experienced during touch.

Gibson defined the haptic system as "the sensibility of the individual to the world adjacent to his body by use of his body." Gibson and others emphasized the close link between body movement and haptic perception, where the latter is "active exploration." 

The concept of haptic perception is related to the concept of extended physiological proprioception according to which, when using a tool such as a stick, perceptual experience is transparently transferred to the end of the tool.

Taste (formally known as "gustation") is the ability to perceive the flavor of substances, including, but not limited to, food. Humans receive tastes through sensory organs concentrated on the upper surface of the tongue, called "taste buds" or "gustatory calyculi". The human tongue has 100 to 150 taste receptor cells on each of its roughly-ten thousand taste buds. 

Traditionally, there have been four primary tastes: sweetness, bitterness, sourness, and saltiness. However, the recognition and awareness of umami, which is considered the fifth primary taste, is a relatively recent development in Western cuisine. Other tastes can be mimicked by combining these basic tastes, all of which contribute only partially to the sensation and flavor of food in the mouth. Other factors include smell, which is detected by the olfactory epithelium of the nose; texture, which is detected through a variety of mechanoreceptors, muscle nerves, etc.; and temperature, which is detected by thermoreceptors. All basic tastes are classified as either "appetitive" or "aversive", depending upon whether the things they sense are harmful or beneficial.

Smell is the process of absorbing molecules through olfactory organs, which are absorbed by humans through the nose. These molecules diffuse through a thick layer of mucus; come into contact with one of thousands of cilia that are projected from sensory neurons; and are then absorbed into a receptor (one of 347 or so). It is this process that causes humans to understand the concept of smell from a physical standpoint.

Smell is also a very interactive sense as scientists have begun to observe that olfaction comes into contact with the other sense in unexpected ways. It is also the most primal of the senses, as it is known to be the first indicator of safety or danger, therefore being the sense that drives the most basic of human survival skills. As such, it can be a catalyst for human behavior on a subconscious and instinctive level.

Social perception is the part of perception that allows people to understand the individuals and groups of their social world. Thus, it is an element of social cognition.

"Speech perception" is the process by which spoken language is heard, interpreted and understood. Research in this field seeks to understand how human listeners recognize the sound of speech (or "phonetics") and use such information to understand spoken language. 

Listeners manage to perceive words across a wide range of conditions, as the sound of a word can vary widely according to words that surround it and the tempo of the speech, as well as the physical characteristics, accent, tone, and mood of the speaker. Reverberation, signifying the persistence of sound after the sound is produced, can also have a considerable impact on perception. Experiments have shown that people automatically compensate for this effect when hearing speech.

The process of perceiving speech begins at the level of the sound within the auditory signal and the process of audition. The initial auditory signal is compared with visual information—primarily lip movement—to extract acoustic cues and phonetic information. It is possible other sensory modalities are integrated at this stage as well. This speech information can then be used for higher-level language processes, such as word recognition.

Speech perception is not necessarily uni-directional. Higher-level language processes connected with morphology, syntax, and/or semantics may also interact with basic speech perception processes to aid in recognition of speech sounds. It may be the case that it is not necessary (maybe not even possible) for a listener to recognize phonemes before recognizing higher units, such as words. In an experiment, Richard M. Warren replaced one phoneme of a word with a cough-like sound. His subjects restored the missing speech sound perceptually without any difficulty. Moreover, they were not able to accurately identify which phoneme had even been disturbed.

"Facial perception" refers to cognitive processes specialized in handling human faces (including perceiving the identity of an individual) and facial expressions (such as emotional cues.)

The "somatosensory cortex" is a part of the brain that receives and encodes sensory information from receptors of the entire body. 

Affective touch is a type of sensory information that elicits an emotional reaction and is usually social in nature. Such information is actually coded differently than other sensory information. Though the intensity of affective touch is still encoded in the primary somatosensory cortex, the feeling of pleasantness associated with affective touch is activated more in the anterior cingulate cortex. Increased blood oxygen level-dependent (BOLD) contrast imaging, identified during functional magnetic resonance imaging (fMRI), shows that signals in the anterior cingulate cortex, as well as the prefrontal cortex, are highly correlated with pleasantness scores of affective touch. Inhibitory transcranial magnetic stimulation (TMS) of the primary somatosensory cortex inhibits the perception of affective touch intensity, but not affective touch pleasantness. Therefore, the S1 is not directly involved in processing socially affective touch pleasantness, but still plays a role in discriminating touch location and intensity.

Multi-modal perception refers to concurrent stimulation in more than one sensory modality and the effect such has on the perception of events and objects in the world.

Chronoception refers to how the passage of time is perceived and experienced. Although the sense of time is not associated with a specific sensory system, the work of psychologists and neuroscientists indicates that human brains do have a system governing the perception of time, composed of a highly distributed system involving the cerebral cortex, cerebellum, and basal ganglia. One particular component of the brain, the suprachiasmatic nucleus, is responsible for the circadian rhythm (commonly known as one's "internal clock"), while other cell clusters appear to be capable of shorter-range timekeeping, known as an "ultradian rhythm".

One or more dopaminergic pathways in the central nervous system appear to have a strong modulatory influence on mental chronometry, particularly interval timing.

"Sense of agency" refers to the subjective feeling of having chosen a particular action. Some conditions, such as schizophrenia, can cause a loss of this sense, which may lead a person into delusions, such as feeling like a machine or like an outside source is controlling them. An opposite extreme can also occur, where people experience everything in their environment as though they had decided that it would happen.

Even in non-pathological cases, there is a measurable difference between the making of a decision and the feeling of agency. Through methods such as the Libet experiment, a gap of half a second or more can be detected from the time when there are detectable neurological signs of a decision having been made to the time when the subject actually becomes conscious of the decision.

There are also experiments in which an illusion of agency is induced in psychologically normal subjects. In 1999, psychologists Wegner and Wheatley gave subjects instructions to move a mouse around a scene and point to an image about once every thirty seconds. However, a second person—acting as a test subject but actually a confederate—had their hand on the mouse at the same time, and controlled some of the movement. Experimenters were able to arrange for subjects to perceive certain "forced stops" as if they were their own choice.

Recognition memory is sometimes divided into two functions by neuroscientists: "familiarity" and "recollection". A strong sense of familiarity can occur without any recollection, for example in cases of deja vu. 

The temporal lobe (specifically the perirhinal cortex) responds differently to stimuli that feel novel compared to stimuli that feel familiar. Firing rates in the perirhinal cortex are connected with the sense of familiarity in humans and other mammals. In tests, stimulating this area at 10–15 Hz caused animals to treat even novel images as familiar, and stimulation at 30–40 Hz caused novel images to be partially treated as familiar. In particular, stimulation at 30–40 Hz led to animals looking at a familiar image for longer periods, as they would for an unfamiliar one, though it did not lead to the same exploration behavior normally associated with novelty. 

Recent studies on lesions in the area concluded that rats with a damaged perirhinal cortex were still more interested in exploring when novel objects were present, but seemed unable to tell novel objects from familiar ones—they examined both equally. Thus, other brain regions are involved with noticing unfamiliarity, while the perirhinal cortex is needed to associate the feeling with a specific source.

Sexual stimulation is any stimulus (including bodily contact) that leads to, enhances, and maintains sexual arousal, possibly even leading to orgasm. Distinct from the general sense of touch, sexual stimulation is strongly tied to hormonal activity and chemical triggers in the body. Although sexual arousal may arise without physical stimulation, achieving orgasm usually requires physical sexual stimulation (stimulation of the Krause-Finger corpuscles found in erogenous zones of the body.)

Other senses enable perception of body balance, acceleration, gravity, position of body parts, temperature, and pain. They can also enable perception of internal senses, such as suffocation, gag reflex, abdominal distension, fullness of rectum and urinary bladder, and sensations felt in the throat and lungs.

In the case of visual perception, some people can actually see the percept shift in their mind's eye. Others, who are not picture thinkers, may not necessarily perceive the 'shape-shifting' as their world changes. This esemplastic nature has been demonstrated by an experiment that showed that ambiguous images have multiple interpretations on the perceptual level.

This confusing ambiguity of perception is exploited in human technologies such as camouflage and biological mimicry. For example, the wings of European peacock butterflies bear eyespots that birds respond to as though they were the eyes of a dangerous predator.

There is also evidence that the brain in some ways operates on a slight "delay" in order to allow nerve impulses from distant parts of the body to be integrated into simultaneous signals.

Perception is one of the oldest fields in psychology. The oldest quantitative laws in psychology are Weber's law, which states that the smallest noticeable difference in stimulus intensity is proportional to the intensity of the reference; and Fechner's law, which quantifies the relationship between the intensity of the physical stimulus and its perceptual counterpart (e.g., testing how much darker a computer screen can get before the viewer actually notices). The study of perception gave rise to the Gestalt School of Psychology, with an emphasis on holistic approach.

A "sensory system" is a part of the nervous system responsible for processing sensory information. A sensory system consists of sensory receptors, neural pathways, and parts of the brain involved in sensory perception. Commonly recognized sensory systems are those for vision, hearing, somatic sensation (touch), taste and olfaction (smell), as listed above. It has been suggested that the immune system is an overlooked sensory modality. In short, senses are transducers from the physical world to the realm of the mind.

The receptive field is the specific part of the world to which a receptor organ and receptor cells respond. For instance, the part of the world an eye can see, is its receptive field; the light that each rod or cone can see, is its receptive field. Receptive fields have been identified for the visual system, auditory system and somatosensory system, so far. Research attention is currently focused not only on external perception processes, but also to "interoception", considered as the process of receiving, accessing and appraising internal bodily signals. Maintaining desired physiological states is critical for an organism's well being and survival. Interoception is an iterative process, requiring the interplay between perception of body states and awareness of these states to generate proper self-regulation. Afferent sensory signals continuously interact with higher order cognitive representations of goals, history, and environment, shaping emotional experience and motivating regulatory behavior.

"Perceptual constancy" is the ability of perceptual systems to recognize the same object from widely varying sensory inputs. For example, individual people can be recognized from views, such as frontal and profile, which form very different shapes on the retina. A coin looked at face-on makes a circular image on the retina, but when held at angle it makes an elliptical image. In normal perception these are recognized as a single three-dimensional object. Without this correction process, an animal approaching from the distance would appear to gain in size. One kind of perceptual constancy is "color constancy": for example, a white piece of paper can be recognized as such under different colors and intensities of light. Another example is "roughness constancy": when a hand is drawn quickly across a surface, the touch nerves are stimulated more intensely. The brain compensates for this, so the speed of contact does not affect the perceived roughness. Other constancies include melody, odor, brightness and words. These constancies are not always total, but the variation in the percept is much less than the variation in the physical stimulus. The perceptual systems of the brain achieve perceptual constancy in a variety of ways, each specialized for the kind of information being processed, with phonemic restoration as a notable example from hearing.

The "principles of grouping" (or "Gestalt laws of grouping") are a set of principles in psychology, first proposed by Gestalt psychologists, to explain how humans naturally perceive objects as organized patterns and objects. Gestalt psychologists argued that these principles exist because the mind has an innate disposition to perceive patterns in the stimulus based on certain rules. These principles are organized into six categories:


Later research has identified additional grouping principles.

A common finding across many different kinds of perception is that the perceived qualities of an object can be affected by the qualities of context. If one object is extreme on some dimension, then neighboring objects are perceived as further away from that extreme. 

"Simultaneous contrast effect" is the term used when stimuli are presented at the same time, whereas "successive contrast" applies when stimuli are presented one after another.

The contrast effect was noted by the 17th Century philosopher John Locke, who observed that lukewarm water can feel hot or cold depending on whether the hand touching it was previously in hot or cold water. In the early 20th Century, Wilhelm Wundt identified contrast as a fundamental principle of perception, and since then the effect has been confirmed in many different areas. These effects shape not only visual qualities like color and brightness, but other kinds of perception, including how heavy an object feels. One experiment found that thinking of the name "Hitler" led to subjects rating a person as more hostile. Whether a piece of music is perceived as good or bad can depend on whether the music heard before it was pleasant or unpleasant. For the effect to work, the objects being compared need to be similar to each other: a television reporter can seem smaller when interviewing a tall basketball player, but not when standing next to a tall building. In the brain, brightness contrast exerts effects on both neuronal firing rates and neuronal synchrony.

Cognitive theories of perception assume there is a poverty of stimulus. This is the claim that sensations, by themselves, are unable to provide a unique description of the world. Sensations require 'enriching', which is the role of the mental model. 

The perceptual ecology approach was introduced by James J. Gibson, who rejected the assumption of a poverty of stimulus and the idea that perception is based upon sensations. Instead, Gibson investigated what information is actually presented to the perceptual systems. His theory "assumes the existence of stable, unbounded, and permanent stimulus-information in the ambient optic array. And it supposes that the visual system can explore and detect this information. The theory is information-based, not sensation-based." He and the psychologists who work within this paradigm detailed how the world could be specified to a mobile, exploring organism via the lawful projection of information about the world into energy arrays. "Specification" would be a 1:1 mapping of some aspect of the world into a perceptual array. Given such a mapping, no enrichment is required and perception is direct .

From Gibson's early work derived an ecological understanding of perception known as "perception-in-action," which argues that perception is a requisite property of animate action. It posits that, without perception, action would be unguided, and without action, perception would serve no purpose. Animate actions require both perception and motion, which can be described as "two sides of the same coin, the coin is action." Gibson works from the assumption that singular entities, which he calls "invariants," already exist in the real world and that all that the perception process does is home in upon them. 

The constructivist view, held by such philosophers as Ernst von Glasersfeld, regards the continual adjustment of perception and action to the external input as precisely what constitutes the "entity," which is therefore far from being invariant. Glasersfeld considers an "invariant" as a target to be homed in upon, and a pragmatic necessity to allow an initial measure of understanding to be established prior to the updating that a statement aims to achieve. The invariant does not, and need not, represent an actuality. Glasersfeld describes it as extremely unlikely that what is desired or feared by an organism will never suffer change as time goes on. This social constructionist theory thus allows for a needful evolutionary adjustment.

A mathematical theory of perception-in-action has been devised and investigated in many forms of controlled movement, and has been described in many different species of organism using the General Tau Theory. According to this theory, tau information, or time-to-goal information is the fundamental "percept" in perception.

Many philosophers, such as Jerry Fodor, write that the purpose of perception is knowledge. However, evolutionary psychologists hold that the primary purpose of perception is to guide action. They give the example of depth perception, which seems to have evolved not to help us know the distances to other objects but rather to help us move around in space. 

Evolutionary psychologists argue that animals ranging from fiddler crabs to humans use eyesight for collision avoidance, suggesting that vision is basically for directing action, not providing knowledge. Neuropsychologists showed that perception systems evolved along the specifics of animals' activities. This explains why bats and worms can perceive different frequency of auditory and visual systems than, for example, humans.

Building and maintaining sense organs is metabolically expensive. More than half the brain is devoted to processing sensory information, and the brain itself consumes roughly one-fourth of one's metabolic resources. Thus, such organs evolve only when they provide exceptional benefits to an organism's fitness. 

Scientists who study perception and sensation have long understood the human senses as adaptations. Depth perception consists of processing over half a dozen visual cues, each of which is based on a regularity of the physical world. Vision evolved to respond to the narrow range of electromagnetic energy that is plentiful and that does not pass through objects. Sound waves provide useful information about the sources of and distances to objects, with larger animals making and hearing lower-frequency sounds and smaller animals making and hearing higher-frequency sounds. Taste and smell respond to chemicals in the environment that were significant for fitness in the environment of evolutionary adaptedness. The sense of touch is actually many senses, including pressure, heat, cold, tickle, and pain. Pain, while unpleasant, is adaptive. An important adaptation for senses is range shifting, by which the organism becomes temporarily more or less sensitive to sensation. For example, one's eyes automatically adjust to dim or bright ambient light. Sensory abilities of different organisms often co-evolve, as is the case with the hearing of echolocating bats and that of the moths that have evolved to respond to the sounds that the bats make.

Evolutionary psychologists claim that perception demonstrates the principle of modularity, with specialized mechanisms handling particular perception tasks. For example, people with damage to a particular part of the brain suffer from the specific defect of not being able to recognize faces ("prosopagnosia"). EP suggests that this indicates a so-called face-reading module.

The theory of closed-loop perception proposes dynamic motor-sensory closed-loop process in which information flows through the environment and the brain in continuous loops. 


With experience, organisms can learn to make finer perceptual distinctions, and learn new kinds of categorization. Wine-tasting, the reading of X-ray images and music appreciation are applications of this process in the human sphere. Research has focused on the relation of this to other kinds of learning, and whether it takes place in peripheral sensory systems or in the brain's processing of sense information. Empirical research show that specific practices (such as yoga, mindfulness, Tai Chi, meditation, Daoshi and other mind-body disciplines) can modify human perceptual modality. Specifically, these practices enable perception skills to switch from the external (exteroceptive field) towards a higher ability to focus on internal signals ("proprioception"). Also, when asked to provide verticality judgments, highly self-transcendent yoga practitioners were significantly less influenced by a misleading visual context. Increasing self-transcendence may enable yoga practitioners to optimize verticality judgment tasks by relying more on internal (vestibular and proprioceptive) signals coming from their own body, rather than on exteroceptive, visual cues.

Past actions and events that transpire right before an encounter or any form of stimulation have a strong degree of influence on how sensory stimuli are processed and perceived. On a basic level, the information our senses receive is often ambiguous and incomplete. However, they are grouped together in order for us to be able to understand the physical world around us. But it is these various forms of stimulation, combined with our previous knowledge and experience that allows us to create our overall perception. For example, when engaging in conversation, we attempt to understand their message and words by not only paying attention to what we hear through our ears but also from the previous shapes we have seen our mouths make. Another example would be if we had a similar topic come up in another conversation, we would use our previous knowledge to guess the direction the conversation is headed in.

A "perceptual set", also called "perceptual expectancy" or just "set" is a predisposition to perceive things in a certain way. It is an example of how perception can be shaped by "top-down" processes such as drives and expectations. Perceptual sets occur in all the different senses. They can be long term, such as a special sensitivity to hearing one's own name in a crowded room, or short term, as in the ease with which hungry people notice the smell of food. A simple demonstration of the effect involved very brief presentations of non-words such as "sael". Subjects who were told to expect words about animals read it as "seal", but others who were expecting boat-related words read it as "sail".

Sets can be created by motivation and so can result in people interpreting ambiguous figures so that they see what they want to see. For instance, how someone perceives what unfolds during a sports game can be biased if they strongly support one of the teams. In one experiment, students were allocated to pleasant or unpleasant tasks by a computer. They were told that either a number or a letter would flash on the screen to say whether they were going to taste an orange juice drink or an unpleasant-tasting health drink. In fact, an ambiguous figure was flashed on screen, which could either be read as the letter B or the number 13. When the letters were associated with the pleasant task, subjects were more likely to perceive a letter B, and when letters were associated with the unpleasant task they tended to perceive a number 13.

Perceptual set has been demonstrated in many social contexts. People who are primed to think of someone as "warm" are more likely to perceive a variety of positive characteristics in them, than if the word "warm" is replaced by "cold". When someone has a reputation for being funny, an audience is more likely to find them amusing. Individual's perceptual sets reflect their own personality traits. For example, people with an aggressive personality are quicker to correctly identify aggressive words or situations.

One classic psychological experiment showed slower reaction times and less accurate answers when a deck of playing cards reversed the color of the suit symbol for some cards (e.g. red spades and black hearts).

Philosopher Andy Clark explains that perception, although it occurs quickly, is not simply a bottom-up process (where minute details are put together to form larger wholes). Instead, our brains use what he calls "predictive coding". It starts with very broad constraints and expectations for the state of the world, and as expectations are met, it makes more detailed predictions (errors lead to new predictions, or "learning processes)". Clark says this research has various implications; not only can there be no completely "unbiased, unfiltered" perception, but this means that there is a great deal of feedback between perception and expectation (perceptual experiences often shape our beliefs, but those perceptions were based on existing beliefs). Indeed, predictive coding provides an account where this type of feedback assists in stabilizing our inference-making process about the physical world, such as with perceptual constancy examples.




</doc>
<doc id="18731794" url="https://en.wikipedia.org/wiki?curid=18731794" title="Pure thought">
Pure thought

Pure thought is an English translation of an expression originally attributed to Kant and Hegel. Their usage of the German counterpart revolved around the question of whether pure thought could exist without an object or some material. Today, more popular uses exist. That "Pure Thought" could existent and is part of the evidentiary change. The usage here is that pure thought is simply a process, another term for thought experiment. 


</doc>
<doc id="21508" url="https://en.wikipedia.org/wiki?curid=21508" title="Noosphere">
Noosphere

The noosphere is a philosophical concept developed and popularized by the French philosopher and Jesuit priest Pierre Teilhard de Chardin and the biogeochemist Vladimir Vernadsky. Vernadsky defined the noosphere as the new state of the biosphere and described as the planetary "sphere of reason". The noosphere represents the highest stage of biospheric development, its defining factor being the development of humankind's rational activities.

The word is derived from the Greek νόος ("mind", "reason") and σφαῖρα (""), in lexical analogy to "atmosphere" and "biosphere". The concept, however, cannot be accredited to a single author. The founding authors Vladimir Ivanovich Vernadsky and Pierre Teilhard de Chardin developed two related but starkly different concepts, the former being grounded in the geological sciences and the latter, in theology. Both conceptions of the noosphere share the common thesis that together human reason and the scientific thought has and will continue to create the next evolutionary geological layer. This geological layer is part of the evolutionary chain. Second generation authors, predominantly of Russian origin, have further developed the Vernadskian concept, creating the related concepts: noocenosis and noocenology.

The term noosphere was first used in the publications of Pierre Teilhard de Chardin in 1922 in his "Cosmogenesis". Vernadsky was most likely introduced to the term by a common acquaintance, Édouard Le Roy, during a stay in Paris. Some sources claim Édouard Le Roy actually first proposed the term. Vernadsky himself wrote that he was first introduced to the concept by Le Roy in his 1927 lectures at the College of France, and that Le Roy had emphasized a mutual exploration of the concept with Teilhard de Chardin. According to Vernadsky's own letters, he took Le Roy’s ideas on the noosphere from Le Roys article "Les origines humaines et l’evolution de l’intelligence", part III: "La noosphere et l’hominisation", before reworking the concept within his own field, biogeochemistry. The historian Bailes concludes that Vernadsky and Teilhard de Chardin were mutual influences on each other, as Teilhard de Chardin also attended the Vernadsky's lectures on biogeochemistry, before creating the concept of the noosphere.

An account stated that Le Roy and Teilhard was not aware of the concept of biosphere in their noosphere concept and that it was Vernadsky who introduced them to this notion, which gave their conceptualization a grounding on natural sciences. Both Teilhard de Chardin and Vernadsky base their conceptions of the noosphere on the term 'biosphere', developed by Edward Suess in 1875. Despite the differing backgrounds, approaches and focuses of Teilhard and Vernadsky, they have a few fundamental themes in common. Both scientists overstepped the boundaries of natural science and attempted to create all-embracing theoretical constructions founded in philosophy, social sciences and authorized interpretations of the evolutionary theory. Moreover, both thinkers were convinced of the teleological character of evolution. They also argued that human activity becomes a geological power and that the manner by which it is directed can influence the environment. There are, however, fundamental differences in the two conceptions.

In the theory of Vernadsky, the noosphere is the third in a succession of phases of development of the Earth, after the geosphere (inanimate matter) and the biosphere (biological life). Just as the emergence of life fundamentally transformed the geosphere, the emergence of human cognition fundamentally transforms the biosphere. In contrast to the conceptions of the Gaia theorists, or the promoters of cyberspace, Vernadsky's noosphere emerges at the point where humankind, through the mastery of nuclear processes, begins to create resources through the transmutation of elements. It is also currently being researched as part of the Global Consciousness Project.

Teilhard perceived a directionality in evolution along an axis of increasing "Complexity/Consciousness". For Teilhard, the noosphere is the sphere of thought encircling the earth that has emerged through evolution as a consequence of this growth in complexity / consciousness. The noosphere is therefore as much part of nature as the barysphere, lithosphere, hydrosphere, atmosphere, and biosphere. As a result, Teilhard sees the "social phenomenon [as] the culmination of and not the attenuation of the biological phenomenon." These social phenomena are part of the noosphere and include, for example, legal, educational, religious, research, industrial and technological systems. In this sense, the noosphere emerges through and is constituted by the interaction of human minds. The noosphere thus grows in step with the organization of the human mass in relation to itself as it populates the earth. Teilhard argued the noosphere evolves towards ever greater personalisation, individuation and unification of its elements. He saw the Christian notion of love as being the principal driver of noogenesis. Evolution would culminate in the Omega Point—an apex of thought/consciousness—which he identified with the eschatological return of Christ.

One of the original aspects of the noosphere concept deals with evolution. Henri Bergson, with his "L'évolution créatrice" (1907), was one of the first to propose evolution is "creative" and cannot necessarily be explained solely by Darwinian natural selection. "L'évolution créatrice" is upheld, according to Bergson, by a constant vital force which animates life and fundamentally connects mind and body, an idea opposing the dualism of René Descartes. In 1923, C. Lloyd Morgan took this work further, elaborating on an "emergent evolution" which could explain increasing complexity (including the evolution of mind). Morgan found many of the most interesting changes in living things have been largely discontinuous with past evolution. Therefore, these living things did not necessarily evolve through a gradual process of natural selection. Rather, he posited, the process of evolution experiences jumps in complexity (such as the emergence of a self-reflective universe, or noosphere), in a sort of qualitative punctuated equilibrium. Finally, the complexification of human cultures, particularly language, facilitated a quickening of evolution in which cultural evolution occurs more rapidly than biological evolution. Recent understanding of human ecosystems and of human impact on the biosphere have led to a link between the notion of sustainability with the "co-evolution" and harmonization of cultural and biological evolution.





</doc>
<doc id="177648" url="https://en.wikipedia.org/wiki?curid=177648" title="Personality">
Personality

Personality is defined as the characteristic sets of behaviors, cognitions, and emotional patterns that evolve from biological and environmental factors. While there is no generally agreed upon definition of personality, most theories focus on motivation and psychological interactions with one's environment. Trait-based personality theories, such as those defined by Raymond Cattell, define personality as the traits that predict a person's behavior. On the other hand, more behaviorally-based approaches define personality through learning and habits. Nevertheless, most theories view personality as relatively stable.

The study of the psychology of personality, called personality psychology, attempts to explain the tendencies that underlie differences in behavior. Many approaches have been taken on to study personality, including biological, cognitive, learning and trait-based theories, as well as psychodynamic, and humanistic approaches. Personality psychology is divided among the first theorists, with a few influential theories being posited by Sigmund Freud, Alfred Adler, Gordon Allport, Hans Eysenck, Abraham Maslow, and Carl Rogers.

Personality can be determined through a variety of tests. Due to the fact that personality is a complex idea, the dimensions of personality and scales of personality tests vary and often are poorly defined. Two main tools to measure personality are objective tests and projective measures. Examples of such tests are the: Big Five Inventory (BFI), Minnesota Multiphasic Personality Inventory (MMPI-2), Rorschach Inkblot test, Neurotic Personality Questionnaire KON-2006, or Eysenck's Personality Questionnaire (EPQ-R). All of these tests are beneficial because they have both reliability and validity, two factors that make a test accurate. "Each item should be influenced to a degree by the underlying trait construct, giving rise to a pattern of positive intercorrelations so long as all items are oriented (worded) in the same direction." A recent, but not well-known, measuring tool that psychologists use is the 16PF. It measures personality based on Cattell's 16 factor theory of personality. Psychologists also use it as a clinical measuring tool to diagnose psychiatric disorders and help with prognosis and therapy planning. 

Personality is often broken into factors or dimensions, statistically extracted from large questionnaires through Factor analysis. When brought back to two dimensions, often the dimensions of introvert-extrovert and neuroticism (emotionally unstable-stable) are used as first proposed by Eysenck in the 1960s. 

Many factor analyses found what is called the Big Five, which are openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism (or emotional stability). These components are generally stable over time, and about half of the variance appears to be attributable to a person's genetics rather than the effects of one's environment. The advantage of the Big five is that it expands across different elemental factors in personality, allowing psychologists to have the most accurate information they can garner. The Big Five Inventory is the most used measuring tool . 

Some research has investigated whether the relationship between happiness and extraversion seen in adults can also be seen in children. The implications of these findings can help identify children that are more likely to experience episodes of depression and develop types of treatment that such children are likely to respond to. In both children and adults, research shows that genetics, as opposed to environmental factors, exert a greater influence on happiness levels. Personality is not stable over the course of a lifetime, but it changes much more quickly during childhood, so personality constructs in children are referred to as temperament. Temperament is regarded as the precursor to personality. Whereas McCrae and Costa's Big Five model assesses personality traits in adults, the EAS (emotionality, activity, and sociability) model is used to assess temperament in children. This model measures levels of emotionality, activity, sociability, and shyness in children. The personality theorists consider temperament EAS model similar to the Big Five model in adults; however, this might be due to a conflation of concepts of personality and temperament as described above. Findings show that high degrees of sociability and low degrees of shyness are equivalent to adult extraversion, and correlate with higher levels of life satisfaction in children.

Another interesting finding has been the link found between acting extraverted and positive affect. Extraverted behaviors include acting talkative, assertive, adventurous, and outgoing. For the purposes of this study, positive affect is defined as experiences of happy and enjoyable emotions. This study investigated the effects of acting in a way that is counter to a person's dispositional nature. In other words, the study focused on the benefits and drawbacks of introverts (people who are shy, socially inhibited and non-aggressive) acting extraverted, and of extraverts acting introverted. After acting extraverted, introverts' experience of positive affect increased whereas extraverts seemed to experience lower levels of positive affect and suffered from the phenomenon of ego depletion. Ego depletion, or cognitive fatigue, is the use of one's energy to overtly act in a way that is contrary to one's inner disposition. When people act in a contrary fashion, they divert most, if not all, (cognitive) energy toward regulating this foreign style of behavior and attitudes. Because all available energy is being used to maintain this contrary behavior, the result is an inability to use any energy to make important or difficult decisions, plan for the future, control or regulate emotions, or perform effectively on other cognitive tasks.

One question that has been posed is why extraverts tend to be happier than introverts. The two types of explanations attempt to account for this difference are instrumental theories and temperamental theories. The instrumental theory suggests that extraverts end up making choices that place them in more positive situations and they also react more strongly than introverts to positive situations. The temperamental theory suggests that extraverts have a disposition that generally leads them to experience a higher degree of positive affect. In their study of extraversion, Lucas and Baird found no statistically significant support for the instrumental theory but did, however, find that extraverts generally experience a higher level of positive affect.

Research has been done to uncover some of the mediators that are responsible for the correlation between extraversion and happiness. "Self-esteem" and "self-efficacy" are two such mediators.

Self-efficacy is one's belief about abilities to perform up to personal standards, the ability to produce desired results, and the feeling of having some ability to make important life decisions. Self-efficacy has been found to be related to the personality traits of extraversion and subjective well-being.

Self-efficacy, however, only partially mediates the relationship between extraversion (and neuroticism) and subjective happiness. This implies that there are most likely other factors that mediate the relationship between subjective happiness and personality traits. "Self-esteem" may be another similar factor. Individuals with a greater degree of confidence about themselves and their abilities seem to have both higher degrees of subjective well-being and higher levels of extraversion.

Other research has examined the phenomenon of "mood maintenance" as another possible mediator. "Mood maintenance" is the ability to maintain one's average level of happiness in the face of an ambiguous situation – meaning a situation that has the potential to engender either positive or negative emotions in different individuals. It has been found to be a stronger force in extraverts. This means that the happiness levels of extraverted individuals are less susceptible to the influence of external events. This finding implies that extraverts' positive moods last longer than those of introverts.

Trnka (2012) stated: "past empirical research has neglected the relationship of neuroticism and semantic perception of different emotions." And for this reason suggested in his research assess "the valence of 10 given negative emotions - disgust, anger, sadness, fear, contempt, hate, disappointment, jealousy, envy and guilt."

Modern conceptions of personality, such as the Temperament and Character Inventory have suggested four basic temperaments that are thought to reflect basic and automatic responses to danger and reward that rely on associative learning. The four temperaments, "harm avoidance", "reward dependence", "novelty seeking" and "persistence" are somewhat analogous to ancient conceptions of melancholic, sanguine, choleric, phlegmatic personality types, although the temperaments reflect dimensions rather than distance categories. While factor based approaches to personality have yielded models that account for significant variance, the developmental biological model has been argued to better reflect underlying biological processes. Distinct genetic, neurochemical and neuroanatomical correlates responsible for each temperamental trait have been observed, unlike with five factor models.

The harm avoidance trait has been associated with increased reactivity in insular and amygdala salience networks, as well as reduced 5-HT2 receptor binding peripherally, and reduced GABA concentrations. Novelty seeking has been associated with reduced activity in insular salience networks increased striatal connectivity. Novelty seeking correlates with dopamine synthesis capacity in the striatum, and reduced auto receptor availability in the midbrain. Reward dependence has been linked with the oxytocin system, with increased concentration of plasma oxytocin being observed, as well as increased volume in oxytocin related regions of the hypothalamus. Persistence has been associated with increased striatal-mPFC connectivity, increased activation of ventral striatal-orbitofrontal-anterior cingulate circuits, as well as increased salivary amylase levels indicative of increased noradrenergic tone.

It has been shown that personality traits are more malleable by environmental influences than researchers originally believed. Personality differences predict the occurrence of life experiences.

One study that has shown how the home environment, specifically the types of parents a person has, can affect and shape their personality. Mary Ainsworth's Strange Situation experiment showcased how babies reacted to having their mother leave them alone in a room with a stranger. The different styles of attachment, labelled by Ainsworth, were Secure, Ambivalent, avoidant, and disorganized. Children who were securely attached tend to be more trusting, sociable, and are confident in their day-to-day life. Children who were disorganized were reported to have higher levels of anxiety, anger, and risk-taking behavior.

Judith Rich Harris's group socialization theory postulates that an individual's peer groups, rather than parental figures, are the primary influence of personality and behavior in adulthood. Intra- and intergroup processes, not dyadic relationships such as parent-child relationships, are responsible for the transmission of culture and for environmental modification of children's personality characteristics. Thus, this theory points at the peer group representing the environmental influence on a child's personality rather than the parental style or home environment.

Tessuya Kawamoto's "Personality Change from Life Experiences: Moderation Effect of Attachment Security" talked about laboratory tests. The study mainly focused on the effects of life experiences on change in personality on and life experiences. The assessments suggested that "the accumulation of small daily experiences may work for the personality development of university students and that environmental influences may vary by individual susceptibility to experiences, like attachment security".

There has been some recent debate over the subject of studying personality in a different culture. Some people think that personality comes entirely from culture and therefore there can be no meaningful study in cross-culture study. On the other hand, many believe that some elements are shared by all cultures and an effort is being made to demonstrate the cross-cultural applicability of "the Big Five".

Cross-cultural assessment depends on the universality of personality traits, which is whether there are common traits among humans regardless of culture or other factors. If there is a common foundation of personality, then it can be studied on the basis of human traits rather than within certain cultures. This can be measured by comparing whether assessment tools are measuring similar constructs across countries or cultures. Two approaches to researching personality are looking at emic and etic traits. Emic traits are constructs unique to each culture, which are determined by local customs, thoughts, beliefs, and characteristics. Etic traits are considered universal constructs, which establish traits that are evident across cultures that represent a biological bases of human personality. If personality traits are unique to individual culture, then different traits should be apparent in different cultures. However, the idea that personality traits are universal across cultures is supported by establishing the Five Factor Model of personality across multiple translations of the NEO-PI-R, which is one of the most widely used personality measures. When administering the NEO-PI-R to 7,134 people across six languages, the results show a similar pattern of the same five underlying constructs that are found in the American factor structure.

Similar results were found using the Big Five Inventory (BFI), as it was administered in 56 nations across 28 languages. The five factors continued to be supported both conceptually and statistically across major regions of the world, suggesting that these underlying factors are common across cultures. There are some differences across culture but they may be a consequence of using a lexical approach to study personality structures, as language has limitations in translation and different cultures have unique words to describe emotion or situations. For example, the term "feeling blue" is used to describe sadness in more Westernized cultures, but does not translate to other languages. Differences across cultures could be due to real cultural differences, but they could also be consequences of poor translations, biased sampling, or differences in response styles across cultures. Examining personality questionnaires developed within a culture can also be useful evidence for the universality of traits across cultures, as the same underlying factors can still be found. Results from several European and Asian studies have found overlapping dimensions with the Five Factor Model as well as additional culture-unique dimensions. Finding similar factors across cultures provides support for the universality of personality trait structure, but more research is necessary to gain stronger support.

The modern sense of individual personality is a result of the shifts in culture originating in the Renaissance, an essential element in modernity. In contrast, the Medieval European's sense of self was linked to a network of social roles: "the household, the kinship network, the guild, the corporation – these were the building blocks of personhood". Stephen Greenblatt observes, in recounting the recovery (1417) and career of Lucretius' poem "De rerum natura": "at the core of the poem lay key principles of a modern understanding of the world." "Dependent on the family, the individual alone was nothing," Jacques Gélis observes. "The characteristic mark of the modern man has two parts: one internal, the other external; one dealing with his environment, the other with his attitudes, values, and feelings." Rather than being linked to a network of social roles, the modern man is largely influenced by the environmental factors such as: "urbanization, education, mass communication, industrialization, and politicization."

William James (1842–1910) argued that temperament explains a great deal of the controversies in the history of philosophy by arguing that it is a very influential premise in the arguments of philosophers. Despite seeking only impersonal reasons for their conclusions, James argued, the temperament of philosophers influenced their philosophy. Temperament thus conceived is tantamount to a bias. Such bias, James explained, was a consequence of the trust philosophers place in their own temperament. James thought the significance of his observation lay on the premise that in philosophy an objective measure of success is whether a philosophy is peculiar to its philosopher or not, and whether a philosopher is dissatisfied with any other way of seeing things or not.

James argued that temperament may be the basis of several divisions in academia, but focused on philosophy in his 1907 lectures on "Pragmatism". In fact, James' lecture of 1907 fashioned a sort of trait theory of the empiricist and rationalist camps of philosophy. As in most modern trait theories, the traits of each camp are described by James as distinct and opposite, and may be possessed in different proportions on a continuum, and thus characterize the personality of philosophers of each camp. The "mental make-up" (i.e. personality) of rationalist philosophers is described as "tender-minded" and "going by "principles," and that of empiricist philosophers is described as "tough-minded" and "going by "facts." James distinguishes each not only in terms of the philosophical claims they made in 1907, but by arguing that such claims are made primarily on the basis of temperament. Furthermore, such categorization was only incidental to James' purpose of explaining his pragmatist philosophy, and is not exhaustive.

According to James, the "temperament" of rationalist philosophers differed fundamentally from the "temperament" of empiricist philosophers of his day. The tendency of rationalist philosophers toward "refinement" and "superficiality" never satisfied an empiricist temper of mind. Rationalism leads to the creation of "closed systems", and such optimism is considered shallow by the fact-loving mind, for whom perfection is far off. Rationalism is regarded as "pretension", and a temperament most inclined to "abstraction". The temperament of rationalists, according to James, led to sticking with logic.

Empiricists, on the other hand, stick with the external senses rather than logic. British empiricist John Locke's (1632–1704) explanation of personal identity provides an example of what James referred to. Locke explains the identity of a person, i.e. personality, on the basis of a precise definition of identity, by which the meaning of identity differs according to what it is being applied to. The identity of a person, is quite distinct from the identity of a man, woman, or substance according to Locke. Locke concludes that consciousness is personality because it "always accompanies thinking, it is that which makes every one to be what he calls self," and remains constant in different places at different times. Thus his explanation of personal identity is in terms of experience as James indeed maintained is the case for most empiricists.

Rationalists conceived of the identity of persons differently than empiricists such as Locke who distinguished identity of substance, person, and life. According to Locke, Rene Descartes (1596–1650) agreed only insofar as he did not argue that one immaterial spirit is the basis of the person "for fear of making brutes thinking things too." According to James, Locke tolerated arguments that a soul was behind the consciousness of any person. However, Locke's successor David Hume (1711–1776), and empirical psychologists after him denied the soul except for being a term to describe the cohesion of inner lives. However, some research suggests Hume excluded personal identity from his opus An Inquiry Concerning Human Understanding because he thought his argument was sufficient but not compelling. Descartes himself distinguished active and passive faculties of mind, each contributing to thinking and consciousness in different ways. The passive faculty, Descartes argued, simply receives, whereas the active faculty produces and forms ideas, but does not presuppose thought, and thus cannot be within the thinking thing. The active faculty mustn't be within self because ideas are produced without any awareness of them, and are sometimes produced against one's will.

Rationalist philosopher Benedictus Spinoza (1632–1677) argued that ideas are the first element constituting the human mind, but existed only for actually existing things. In other words, ideas of non-existent things are without meaning for Spinoza, because an idea of a non-existent thing cannot exist. Further, Spinoza's rationalism argued that the mind does not know itself, except insofar as it perceives the "ideas of the modifications of body," in describing its external perceptions, or perceptions from without. On the contrary, from within, Spinoza argued, perceptions connect various ideas clearly and distinctly. The mind is not the free cause of its actions for Spinoza. Spinoza equates the will with the understanding, and explains the common distinction of these things as being two different things as error which results from the individual's misunderstanding of the nature of thinking.

The biological basis of personality is the theory that anatomical structures located in the brain contribute to personality traits. This stems from neuropsychology, which studies how the structure of the brain relates to various psychological processes and behaviors. For instance, in human beings, the frontal lobes are responsible for foresight and anticipation, and the occipital lobes are responsible for processing visual information. In addition, certain physiological functions such as hormone secretion also affect personality. For example, the hormone testosterone is important for sociability, affectivity, aggressiveness, and sexuality. Additionally, studies show that the expression of a personality trait depends on the volume of the brain cortex it is associated with.

There is also a confusion among some psychologists who conflate personality with temperament. Temperament traits that are based on weak neurochemical imbalances within neurotransmitter systems are much more stable, consistent in behavior and show up in early childhood; they can't be changed easily but can be compensated for in behavior. In contrast to that, personality traits and features are the product of the socio-cultural development of humans and can be learned and/or changed.

Personology confers a multidimensional, complex, and comprehensive approach to personality. According to Henry A. Murray, personology is "The branch of psychology which concerns itself with the study of human lives and the factors that influence their course which investigates individual differences and types of personality… the science of men, taken as gross units… encompassing “psychoanalysis” (Freud), “analytical psychology” (Jung), “individual psychology” (Adler) and other terms that stand for methods of inquiry or doctrines rather than realms of knowledge." From a holistic perspective, personology studies personality as a whole, as a system, but in the same time through all its components, levels and spheres.

One of the theories that falls under this approach is the psychodynamic theory. This theory, created by Sigmund Freud, says that three mental structures determine our personality. These structures are the id, ego, and superego. The id is responsible for impulses, the superego for the idealized self and our moral code, and the ego for rational thought. Basically, it is the ego's job to satisfy the impulses of the id but also stay within the moral code of the superego.

The ego uses defense mechanisms to protect one’s mind from the conflictual ideas of the id and superego. These defense mechanisms work at the unconscious level and help a person deal with threatening events. These defense styles vary in adaptive value. So, a defense style that doesn’t provide the appropriate change to the person so that they can deal with the threatening event usually suggests the repeated use of immature defenses, such as denial.

Psychiatry is the medical specialty devoted to the diagnosis, prevention and treatment of mental disorders. High neuroticism is an independent prospective predictor for the development of the common mental disorders. Interest in the history of psychiatry continues to grow, with an increasing emphasis on topics of current interest such as the history of psychopharmacology, electroconvulsive therapy, and the interplay between psychiatry and society.




</doc>
<doc id="488083" url="https://en.wikipedia.org/wiki?curid=488083" title="Theory of mind">
Theory of mind

Theory of mind is the ability to attribute mental states — beliefs, intents, desires, emotions, knowledge, etc. — to oneself, and to others, and to understand that others have beliefs, desires, intentions, and perspectives that are different from one's own. Theory of mind is crucial for everyday human social interactions and is used when analyzing, judging, and inferring others' behaviors. Deficits can occur in people with autism spectrum disorders, genetic-based eating disorders, schizophrenia, attention deficit hyperactivity disorder, cocaine addiction, and brain damage suffered from alcohol's neurotoxicity, but not opiate addiction after prolonged abstinence. Although philosophical approaches to this exist, the theory of mind as such is distinct from the philosophy of mind.

Theory of mind is a theory insofar as the output (thoughts, feelings, etc.) of the mind is the only thing being directly observed so the existence of a mind is inferred. The presumption that others have a mind is termed a theory of mind because each human can only intuit the existence of their own mind through introspection, and no one has direct access to the mind of another so its existence and how it works can only be inferred from observations of others. It is typically assumed that others have minds analogous to one's own, and this assumption is based on the reciprocal, social interaction, as observed in joint attention, the functional use of language, and the understanding of others' emotions and actions. Having theory of mind allows one to attribute thoughts, desires, and intentions to others, to predict or explain their actions, and to posit their intentions. As originally defined, it enables one to understand that mental states can be the cause of—and thus be used to explain and predict—the behavior of others. Being able to attribute mental states to others and understanding them as causes of behavior implies, in part, that one must be able to conceive of the mind as a "generator of representations". If a person does not have a complete theory of mind, it may be a sign of cognitive or developmental impairment.

Theory of mind appears to be an innate potential ability in humans that requires social and other experience over many years for its full development. Different people may develop more, or less, effective theory of mind. Neo-Piagetian theories of cognitive development maintain that theory of mind is a byproduct of a broader hypercognitive ability of the human mind to register, monitor, and represent its own functioning.

Empathy is a related concept, meaning the recognition and understanding of the states of mind of others, including their beliefs, desires and particularly emotions. This is often characterized as the ability to "put oneself into another's shoes". Recent neuro-ethological studies of animal behaviour suggest that even rodents may exhibit ethical or empathetic abilities. While empathy is known as emotional perspective-taking, theory of mind is defined as cognitive perspective-taking.

Research on theory of mind, in humans and animals, adults and children, normally and atypically developing, has grown rapidly in the 35 years since Premack and Guy Woodruff's paper, "Does the chimpanzee have a theory of mind?". The emerging field of social neuroscience has also begun to address this debate, by imaging the brains of humans while they perform tasks demanding the understanding of an intention, belief or other mental state in others.

An alternative account of theory of mind is given within operant psychology and provides significant empirical evidence for a functional account of both perspective-taking and empathy. The most developed operant approach is founded on research on derived relational responding and is subsumed within what is called relational frame theory. According to this view, empathy and perspective-taking comprise a complex set of derived relational abilities based on learning to discriminate and respond verbally to ever more complex relations between self, others, place, and time, and through established relations.

Contemporary discussions of Theory of Mind have their roots in philosophical debate—most broadly, from the time of Descartes' "Second Meditation", which set the groundwork for considering the science of the mind. Most prominent recently are two contrasting approaches in the philosophical literature, to theory of mind: theory-theory and simulation theory. The theory-theorist imagines a veritable theory—"folk psychology"—used to reason about others' minds. The theory is developed automatically and innately, though instantiated through social interactions. It is also closely related to person perception and attribution theory from social psychology.

The intuitive assumption that others are minded is an apparent tendency we all share. We anthropomorphize non-human animals, inanimate objects, and even natural phenomena. Daniel Dennett referred to this tendency as taking an "intentional stance" toward things: we assume they have intentions, to help predict future behavior. However, there is an important distinction between taking an "intentional stance" toward something and entering a "shared world" with it. The intentional stance is a detached and functional theory we resort to during interpersonal interactions. A shared world is directly perceived and its existence structures reality itself for the perceiver. It is not just automatically applied to perception; it in many ways constitutes perception.

The philosophical roots of the relational frame theory (RFT) account of Theory of Mind arise from contextual psychology and refer to the study of organisms (both human and non-human) interacting in and with a historical and current situational context. It is an approach based on contextualism, a philosophy in which any event is interpreted as an ongoing act inseparable from its current and historical context and in which a radically functional approach to truth and meaning is adopted. As a variant of contextualism, RFT focuses on the construction of practical, scientific knowledge. This scientific form of contextual psychology is virtually synonymous with the philosophy of operant psychology.

The study of which animals are capable of attributing knowledge and mental states to others, as well as the development of this ability in human ontogeny and phylogeny, has identified several behavioral precursors to theory of mind. Understanding attention, understanding of others' intentions, and imitative experience with other people are hallmarks of a theory of mind that may be observed early in the development of what later becomes a full-fledged theory. In studies with non-human animals and pre-verbal humans, in particular, researchers look to these behaviors preferentially in making inferences about mind.

Simon Baron-Cohen identified the infant's understanding of attention in others, a social skill found by 7 to 9 months of age, as a "critical precursor" to the development of theory of mind. Understanding attention involves understanding that seeing can be directed selectively as attention, that the looker assesses the seen object as "of interest", and that seeing can induce beliefs. Attention can be directed and shared by the act of pointing, a joint attention behavior that requires taking into account another person's mental state, particularly whether the person notices an object or finds it of interest. Baron-Cohen speculates that the inclination to spontaneously reference an object in the world as of interest ("protodeclarative pointing") and to likewise appreciate the directed attention and interests of another may be the underlying motive behind all human communication.

Understanding of others' intentions is another critical precursor to understanding other minds because intentionality, or "aboutness", is a fundamental feature of mental states and events. The "intentional stance" has been defined by Daniel Dennett as an understanding that others' actions are goal-directed and arise from particular beliefs or desires. Both 2- and 3-year-old children could discriminate when an experimenter intentionally vs. accidentally marked a box with stickers as baited. Even earlier in ontogeny, Andrew N. Meltzoff found that 18-month-old infants could perform target manipulations that adult experimenters attempted and failed, suggesting the infants could represent the object-manipulating behavior of adults as involving goals and intentions. While attribution of intention (the box-marking) and knowledge (false-belief tasks) is investigated in young humans and nonhuman animals to detect precursors to a theory of mind, Gagliardi et al. have pointed out that even adult humans do not always act in a way consistent with an attributional perspective. In the experiment, adult human subjects made choices about baited containers when guided by confederates who could not see (and therefore, not know) which container was baited.

Recent research in developmental psychology suggests that the infant's ability to imitate others lies at the origins of both theory of mind and other social-cognitive achievements like perspective-taking and empathy. According to Meltzoff, the infant's innate understanding that others are "like me" allows it to recognize the equivalence between the physical and mental states apparent in others and those felt by the self. For example, the infant uses his own experiences, orienting his head/eyes toward an object of interest to understand the movements of others who turn toward an object, that is, that they will generally attend to objects of interest or significance. Some researchers in comparative disciplines have hesitated to put a too-ponderous weight on imitation as a critical precursor to advanced human social-cognitive skills like mentalizing and empathizing, especially if true imitation is no longer employed by adults. A test of imitation by Alexandra Horowitz found that adult subjects imitated an experimenter demonstrating a novel task far less closely than children did. Horowitz points out that the precise psychological state underlying imitation is unclear and cannot, by itself, be used to draw conclusions about the mental states of humans.

While much research has been done on infants, theory of mind develops continuously throughout childhood and into late adolescence as the synapses (neuronal connections) in the prefrontal cortex develop. The prefrontal cortex is thought to be involved in planning and decision-making. Children seem to develop theory of mind skills sequentially. The first skill to develop is the ability to recognize that others have diverse desires. Children are able to recognize that others have diverse beliefs soon after. The next skill to develop is recognizing that others have access to different knowledge bases. Finally, children are able to understand that others may have false beliefs and that others are capable of hiding emotions. While this sequence represents the general trend in skill acquisition, it seems that more emphasis is placed on some skills in certain cultures, leading to more valued skills to develop before those that are considered not as important. For example, in individualistic cultures such as the United States, a greater emphasis is placed on the ability to recognize that others have different opinions and beliefs. In a collectivistic culture, such as China, this skill may not be as important and therefore may not develop until later.

There is evidence to believe that the development of theory of mind is closely intertwined with language development in humans. One meta-analysis showed a moderate to strong correlation ("r" = 0.43) between performance on theory of mind and language tasks. One might argue that this relationship is due solely to the fact that both language and theory of mind seem to begin to develop substantially around the same time in children (between ages 2–5). However, many other abilities develop during this same time period as well, and do not produce such high correlations with one another nor with theory of mind. There must be something else going on to explain the relationship between theory of mind and language.

Pragmatic theories of communication assume that infants must possess an understanding of beliefs and mental states of others to infer the communicative content that proficient language users intend to convey. Since a verbal utterance is often underdetermined, and therefore, it can have different meanings depending on the actual context theory of mind abilities can play a crucial role in understanding the communicative and informative intentions of others and inferring the meaning of words. Some empirical results suggest that even 13-month-old infants have an early capacity for communicative mind-reading that enables them to infer what relevant information is transferred between communicative partners, which implies that human language relies at least partially on theory of mind skills.

Carol A. Miller posed further possible explanations for this relationship. One idea was that the extent of verbal communication and conversation involving children in a family could explain theory of mind development. The belief is that this type of language exposure could help introduce a child to the different mental states and perspectives of others. This has been suggested empirically by findings indicating that participation in family discussion predict scores on theory of mind tasks, as well as findings showing that deaf children who have hearing parents and may not be able to communicate with their parents much during early years of development tend to score lower on theory of mind tasks.

Another explanation of the relationship between language and theory of mind development has to do with a child's understanding of mental state words such as ""think" and "believe"". Since a mental state is not something that one can observe from behavior, children must learn the meanings of words denoting mental states from verbal explanations alone, requiring knowledge of the syntactic rules, semantic systems, and pragmatics of a language. Studies have shown that understanding of these mental state words predicts theory of mind in four-year-olds.

A third hypothesis is that the ability to distinguish a whole sentence ("Jimmy thinks the world is flat") from its embedded complement ("the world is flat") and understand that one can be true while the other can be false is related to theory of mind development. Recognizing these sentential complements as being independent of one another is a relatively complex syntactic skill and has been shown to be related to increased scores on theory of mind tasks in children.

In addition to these hypotheses, there is also evidence that the neural networks between the areas of the brain responsible for language and theory of mind are closely connected. The temporoparietal junction has been shown to be involved in the ability to acquire new vocabulary, as well as perceive and reproduce words. The temporoparietal junction also contains areas that specialize in recognizing faces, voices, and biological motion, in addition to theory of mind. Since all of these areas are located so closely together, it is reasonable to conclude that they work together. Moreover, studies have reported an increase in activity in the TPJ when patients are absorbing information through reading or images regarding other peoples' beliefs but not while observing information about physical control stimuli.

Neurotypical adults have the theory of mind concepts that they developed as children (concepts such as belief, desire, knowledge and intention). A focal question is how they use these concepts to meet the diverse demands of social life, ranging from snap decisions about how to trick an opponent in a competitive game, to keeping up with who knows what in a fast-moving conversation, to judging the guilt or innocence of the accused in a court of law.

Boaz Keysar, Dale Barr and colleagues found that adults often failed to "use" their theory of mind abilities to interpret a speaker's message, even though they were perfectly well aware that the speaker lacked critical knowledge. Other studies converge in showing that adults are prone to “egocentric biases”, whereby they are influenced by their own beliefs, knowledge or preferences when judging those of other people, or else neglect other people's perspectives entirely. There is also evidence that adults with greater memory and inhibitory capacity and greater motivation are more likely to use their theory of mind abilities.

In contrast, evidence from tasks looking for indirect effects of thinking about other people's mental states suggests that adults may sometimes use their theory of mind automatically. Agnes Kovacs and colleagues measured the time it took adults to detect the presence of a ball as it was revealed from behind an occluder. They found that adults’ speed of response was influenced by whether or not an avatar in the scene thought there was a ball behind the occluder, even though adults were not asked to pay attention to what the avatar thought. Dana Samson and colleagues measured the time it took adults to judge the number of dots on the wall of a room. They found that adults responded more slowly when an avatar standing in the room happened to see fewer dots than they did, even when they had never been asked to pay attention to what the avatar could see. It has been questioned whether these “altercentric biases” truly reflect automatic processing of what another person is thinking or seeing, or whether they instead reflect attention and memory effects cued by the avatar, but not involving any representation of what they think or see.

Different theories have sought to explain these patterns of results. The idea that theory of mind is automatic is attractive because it would help explain how people keep up with the theory of mind demands of competitive games and fast-moving conversations. It might also explain evidence that human infants and some non-human species sometimes appear capable of theory of mind, despite their limited resources for memory and cognitive control. The idea that theory of mind is effortful and not automatic is attractive because it feels effortful to decide whether a defendant is guilty or innocent, or whether a negotiator is bluffing, and economy of effort would help explain why people sometimes neglect to use their theory of mind. Ian Apperly and Stephen Butterfill have suggested that people do in fact have “two systems” for theory of mind, in common with “two systems” accounts in many other areas of psychology. On this account, “system 1” is cognitively efficient and enables theory of mind for a limited but useful set of circumstances. “System 2” is cognitively effortful, but enables much more flexible theory of mind abilities. This account has been criticised by Peter Carruthers who suggests that the same core theory of mind abilities can be used in both simple and complex ways. The account has been criticised by Celia Heyes who suggests that “system 1” theory of mind abilities do not require representation of mental states of other people, and so are better thought of as “sub-mentalising”.

In older age, theory of mind capacities decline, irrespective of how exactly they are tested (e.g. stories, eyes, videos, false belief-video, false belief-other, faux pas). However, the decline in other cognitive functions is even stronger, suggesting that social cognition is somewhat preserved. In contrast to theory of mind, empathy shows no impairments in aging.

There are two kinds of theory of mind representations: cognitive (concerning the mental states, beliefs, thoughts, and intentions of others) and affective (concerning the emotions of others). Cognitive theory of mind is further separated into first order (e.g., I think she thinks that...) and second order (e.g., he thinks that she thinks that...). There is evidence that cognitive and affective theory of mind processes are functionally independent from one another. In studies of Alzheimer's disease, which typically occurs in older adults, the patients display impairment with second order cognitive theory of mind, but usually not with first order cognitive or affective theory of mind. However, it is difficult to discern a clear pattern of theory of mind variation due to age. There have been many discrepancies in the data collected thus far, likely due to small sample sizes and the use of different tasks that only explore one aspect of theory of mind. Many researchers suggest that the theory of mind impairment is simply due to the normal decline in cognitive function.

Researchers have proposed that five key aspects of theory of mind develop sequentially for all children between the ages of three to five. This five-step theory of mind scale consists of the development of diverse desires (DD), diverse beliefs (DB), knowledge access (KA), false beliefs (FB), and hidden emotions (HE). Australian, American and European children acquire theory of mind in this exact order, and studies with children in Canada, India, Peru, Samoa, and Thailand indicate that they all pass the false belief task at around the same time, suggesting that the children develop theory of mind consistently around the world.

However, children from Iran and China develop theory of mind in a slightly different order. Although they begin the development of theory of mind around the same time, toddlers from these countries understand knowledge access (KA) before Western children but take longer to understand diverse beliefs (DB). Researchers believe this swap in the developmental order is related to the culture of collectivism in Iran and China, which emphasizes interdependence and shared knowledge as opposed to the culture of individualism in Western countries, which promotes individuality and accepts differing opinions. Because of these different cultural values, Iranian and Chinese children might take longer to understand that other people have different beliefs and opinions. This suggests that the development of theory of mind is not universal and solely determined by innate brain processes but also influenced by social and cultural factors.

Whether children younger than 3 or 4 years old may have any theory of mind is a topic of debate among researchers. It is a challenging question, due to the difficulty of assessing what pre-linguistic children understand about others and the world. Tasks used in research into the development of Theory of Mind must take into account the "umwelt"—(the German word "Umwelt" means "environment" or "surrounding world")—of the pre-verbal child.

One of the most important milestones in theory of mind development is gaining the ability to attribute "false belief": that is, to recognize that others can have beliefs about the world that are diverging. To do this, it is suggested, one must understand how knowledge is formed, that people's beliefs are based on their knowledge, that mental states can differ from reality, and that people's behavior can be predicted by their mental states. Numerous versions of the false-belief task have been developed, based on the initial task done by Wimmer and Perner (1983).

In the most common version of the false-belief task (often called the "'Sally-Anne' test" or "'Sally-Anne' task"), children are told or shown a story involving two characters. For example, the child is shown two dolls, Sally and Anne, who have a basket and a box, respectively. Sally also has a marble, which she places into her basket, and then leaves the room. While she is out of the room, Anne takes the marble from the basket and puts it into the box. Sally returns, and the child is then asked where Sally will look for the marble. The child passes the task if she answers that Sally will look in the basket, where Sally put the marble; the child fails the task if she answers that Sally will look in the box, where the child knows the marble is hidden, even though Sally cannot know this, since she did not see it hidden there. To pass the task, the child must be able to understand that another's mental representation of the situation is different from their own, and the child must be able to predict behavior based on that understanding.

Another example is when a boy leaves chocolate on a shelf and then leaves the room. His mother puts it in the fridge. To pass the task, the child must understand that the boy, upon returning, holds the false belief that his chocolate is still on the shelf.

The results of research using false-belief tasks have been fairly consistent: most normally developing children are able to pass the tasks from around age four. Notably, while most children, including those with Down syndrome, are able to pass this test, in one study, 80% of children diagnosed with autism were "unable" to do so.

Adults may also experience problems with false beliefs. For instance, when they show hindsight bias, defined as: "the inclination to see events that have already happened as being more predictable than they were before they took place." In an experiment by Fischhoff in 1975, adult subjects who were asked for an independent assessment were unable to disregard information on actual outcome. Also in experiments with complicated situations, when assessing others' thinking, adults can be unable to disregard certain information that they have been given.

Other tasks have been developed to try to solve the problems inherent in the false-belief task. In the "Unexpected contents", or "Smarties" task, experimenters ask children what they believe to be the contents of a box that looks as though it holds a candy called "Smarties". After the child guesses (usually) "Smarties", it is shown that the box in fact contained pencils. The experimenter then re-closes the box and asks the child what she thinks another person, who has not been shown the true contents of the box, will think is inside. The child passes the task if he/she responds that another person will think that "Smarties" exist in the box, but fails the task if she responds that another person will think that the box contains pencils. Gopnik & Astington (1988) found that children pass this test at age four or five years.

The "false-photograph" task is another task that serves as a measure of theory of mind development. In this task, children must reason about what is represented in a photograph that differs from the current state of affairs. Within the false-photograph task, either a location or identity change exists. In the location-change task, the examiner puts an object in one location ("e.g.", chocolate in an open green cupboard), whereupon the child takes a Polaroid photograph of the scene. While the photograph is developing, the examiner moves the object to a different location ("e.g.", a blue cupboard), allowing the child to view the examiner's action. The examiner asks the child two control questions: "When we first took the picture, where was the object?" and "Where is the object now?". The subject is also asked a "false-photograph" question: "Where is the object in the picture?" The child passes the task if he/she correctly identifies the location of the object in the picture and the actual location of the object at the time of the question. However, the last question might be misinterpreted as: "Where in this room is the object that the picture depicts?" and therefore some examiners use an alternative phrasing.

To make it easier for animals, young children, and individuals with classical (Kanner-type) autism to understand and perform theory of mind tasks, researchers have developed tests in which verbal communication is de-emphasized: some whose administration does not involve verbal communication on the part of the examiner, some whose successful completion does not require verbal communication on the part of the subject, and some that meet both of the foregoing standards. One category of tasks uses a preferential looking paradigm, with looking time as the dependent variable. For instance, 9-month-old infants prefer looking at behaviors performed by a human hand over those made by an inanimate hand-like object. Other paradigms look at rates of imitative behavior, the ability to replicate and complete unfinished goal-directed acts, and rates of pretend play.

Recent research on the early precursors of theory of mind has looked at innovative ways at capturing preverbal infants' understanding of other people's mental states, including perception and beliefs. Using a variety of experimental procedures, studies have shown that infants from their first year of life have an implicit understanding of what other people see and what they know. A popular paradigm used to study infants' theory of mind is the violation of expectation procedure, which predicates on infants' tendency to look longer at unexpected and surprising events compared to familiar and expected events. Therefore, their looking-times measures would give researchers an indication of what infants might be inferring, or their implicit understanding of events. One recent study using this paradigm found that 16-month-olds tend to attribute beliefs to a person whose visual perception was previously witnessed as being "reliable", compared to someone whose visual perception was "unreliable". Specifically, 16-month-olds were trained to expect a person's excited vocalization and gaze into a container to be associated with finding a toy in the reliable-looker condition or an absence of a toy in the unreliable-looker condition. Following this training phase, infants witnessed, in an object-search task, the same persons either searching for a toy in the correct or incorrect location after they both witnessed the location of where the toy was hidden. Infants who experienced the reliable looker were surprised and therefore looked longer when the person searched for the toy in the incorrect location compared to the correct location. In contrast, the looking time for infants who experienced the unreliable looker did not differ for either search locations. These findings suggest that 16-month-old infants can differentially attribute beliefs about a toy's location based on the person's prior record of visual perception.

The theory of mind impairment describes a difficulty someone would have with perspective-taking. This is also sometimes referred to as "mind-blindness". This means that individuals with a theory of mind impairment would have a difficult time seeing phenomena from any other perspective than their own. Individuals who experience a theory of mind deficit have difficulty determining the intentions of others, lack understanding of how their behavior affects others, and have a difficult time with social reciprocity. Theory of Mind deficits have been observed in people with autism spectrum disorders, people with schizophrenia, people with nonverbal learning disorder, people with attention deficit disorder, persons under the influence of alcohol and narcotics, sleep-deprived persons, and persons who are experiencing severe emotional or physical pain. Theory of mind deficits have also been observed in deaf children who are late signers (i.e., are born to hearing parents), but the deficit is due to the delay in language learning, not any cognitive deficit, and therefore disappears once the child learns sign language.

In 1985 Simon Baron-Cohen, Alan M. Leslie and Uta Frith suggested that children with autism do not employ theory of mind and suggested that autistic children have particular difficulties with tasks requiring the child to understand another person's beliefs. These difficulties persist when children are matched for verbal skills and have been taken as a key feature of autism.

Many individuals classified as autistic have severe difficulty assigning mental states to others, and they seem to lack theory of mind capabilities. Researchers who study the relationship between autism and theory of mind attempt to explain the connection in a variety of ways. One account assumes that theory of mind plays a role in the attribution of mental states to others and in childhood pretend play. According to Leslie, theory of mind is the capacity to mentally represent thoughts, beliefs, and desires, regardless of whether or not the circumstances involved are real. This might explain why some autistic individuals show extreme deficits in both theory of mind and pretend play. However, Hobson proposes a social-affective justification, which suggests that with an autistic person, deficits in theory of mind result from a distortion in understanding and responding to emotions. He suggests that typically developing human beings, unlike autistic individuals, are born with a set of skills (such as social referencing ability) that later lets them comprehend and react to other people's feelings. Other scholars emphasize that autism involves a specific developmental delay, so that autistic children vary in their deficiencies, because they experience difficulty in different stages of growth. Very early setbacks can alter proper advancement of joint-attention behaviors, which may lead to a failure to form a full theory of mind.

It has been speculated that Theory of Mind exists on a continuum as opposed to the traditional view of a discrete presence or absence. While some research has suggested that some autistic populations are unable to attribute mental states to others, recent evidence points to the possibility of coping mechanisms that facilitate a spectrum of mindful behavior.
Tine et al. suggest that autistic children score substantially lower on measures of social theory of mind in comparison to children diagnosed with Asperger syndrome.

Generally, children with more advanced theory of mind abilities display more advanced social skills, greater adaptability to new situations, and greater cooperation with others. As a result, these children are typically well-liked. However, “children may use their mind-reading abilities to manipulate, outwit, tease, or trick their peers”. Individuals possessing inferior theory of mind skills, such as children with autism spectrum disorder, may be socially rejected by their peers since they are unable to communicate effectively. Social rejection has been proven to negatively impact a child's development and can put the child at greater risk of developing depressive symptoms.

Peer-mediated interventions (PMI) are a school-based treatment approach for children and adolescents with autism spectrum disorder in which peers are trained to be role models in order to promote social behavior. Laghi et al. studied if analysis of prosocial (nice) and antisocial (nasty) theory of mind behaviors could be used, in addition to teacher recommendations, to select appropriate candidates for PMI programs. Selecting children with advanced theory of mind skills who use them in prosocial ways will theoretically make the program more effective. While the results indicated that analyzing the social uses of theory of mind of possible candidates for a PMI program is invaluable, it may not be a good predictor of a candidate's performance as a role model.

Individuals with the diagnosis of schizophrenia can show deficits in theory of mind. Mirjam Sprong and colleagues investigated the impairment by examining 29 different studies, with a total of over 1500 participants. This meta-analysis showed significant and stable deficit of theory of mind in people with schizophrenia. They performed poorly on false-belief tasks, which test the ability to understand that others can hold false beliefs about events in the world, and also on intention-inference tasks, which assess the ability to infer a character's intention from reading a short story. Schizophrenia patients with negative symptoms, such as lack of emotion, motivation, or speech, have the most impairment in theory of mind and are unable to represent the mental states of themselves and of others. Paranoid schizophrenic patients also perform poorly because they have difficulty accurately interpreting others' intentions. The meta-analysis additionally showed that IQ, gender, and age of the participants do not significantly affect the performance of theory of mind tasks.

Current research suggests that impairment in theory of mind negatively affects clinical insight, the patient's awareness of their mental illness. Insight requires theory of mind—a patient must be able to adopt a third-person perspective and see the self as others do. A patient with good insight would be able to accurately self-represent, by comparing oneself with others and by viewing oneself from the perspective of others. Insight allows a patient to recognize and react appropriately to his symptoms; however, a patient who lacks insight would not realize that he has a mental illness, because of his inability to accurately self-represent. Therapies that teach patients perspective-taking and self-reflection skills can improve abilities in reading social cues and taking the perspective of another person.

The majority of the current literature supports the argument that the theory of mind deficit is a stable trait-characteristic rather than a state-characteristic of schizophrenia. The meta-analysis conducted by Sprong et al. showed that patients in remission still had impairment in theory of mind. The results indicate that the deficit is not merely a consequence of the active phase of schizophrenia.

Schizophrenic patients' deficit in theory of mind impairs their daily interactions with others. An example of a disrupted interaction is one between a schizophrenic parent and a child. Theory of mind is particularly important for parents, who must understand the thoughts and behaviors of their children and react accordingly. Dysfunctional parenting is associated with deficits in the first-order theory of mind, the ability to understand another person's thoughts, and the second-order theory of mind, the ability to infer what one person thinks about another person's thoughts. Compared with healthy mothers, mothers with schizophrenia are found to be more remote, quiet, self-absorbed, insensitive, unresponsive, and to have fewer satisfying interactions with their children. They also tend to misinterpret their children's emotional cues, and often misunderstand neutral faces as negative. Activities such as role-playing and individual or group-based sessions are effective interventions that help the parents improve on perspective-taking and theory of mind. Although there is a strong association between theory of mind deficit and parental role dysfunction, future studies could strengthen the relationship by possibly establishing a causal role of theory of mind on parenting abilities.

Impairments in theory of mind, as well as other social-cognitive deficits are commonly found in people suffering from alcoholism, due to the neurotoxic effects of alcohol on the brain, particularly the prefrontal cortex.

Individuals in a current major depressive episode, a disorder characterized by social impairment, show deficits in theory of mind decoding. Theory of mind decoding is the ability to use information available in the immediate environment (e.g., facial expression, tone of voice, body posture) to accurately label the mental states of others. The opposite pattern, enhanced theory of mind, is observed in individuals vulnerable to depression, including those individuals with past major depressive disorder (MDD), dysphoric individuals, and individuals with a maternal history of MDD.

Children diagnosed with developmental language disorder (DLD) exhibit much lower scores on reading and writing sections of standardized tests, yet have a normal nonverbal IQ. These language deficits can be any specific deficits in lexical semantics, syntax, or pragmatics, or a combination of multiple problems. They often exhibit poorer social skills than normally developing children, and seem to have problems decoding beliefs in others. A recent meta-analysis confirmed that children with DLD have substantially lower scores on theory of mind tasks compared to typically developing children.

Research on theory of mind in autism led to the view that mentalizing abilities are subserved by dedicated mechanisms that can - in some cases - be impaired while general cognitive function remains largely intact.

Neuroimaging research has supported this view, demonstrating specific brain regions consistently engaged during theory of mind tasks. PET research on theory of mind, using verbal and pictorial story comprehension tasks, has identified a set of brain regions including the medial prefrontal cortex (mPFC), and area around posterior superior temporal sulcus (pSTS), and sometimes precuneus and amygdala/temporopolar cortex. Subsequently, research on the neural basis of theory of mind has diversified, with separate lines of research focused on the understanding of beliefs, intentions, and more complex properties of minds such as psychological traits.

Studies from Rebecca Saxe's lab at MIT, using a false-belief versus false-photograph task contrast aimed at isolating the mentalizing component of the false-belief task, have very consistently found activation in mPFC, precuneus, and temporo-parietal junction (TPJ), right-lateralized. In particular, it has been proposed that the right TPJ (rTPJ) is selectively involved in representing the beliefs of others. However, some debate exists, as some scientists have noted that the same rTPJ region has been consistently activated during spatial reorienting of visual attention; Jean Decety from the University of Chicago and Jason Mitchell from Harvard have thus proposed that the rTPJ subserves a more general function involved in both false-belief understanding and attentional reorienting, rather than a mechanism specialized for social cognition. However, it is possible that the observation of overlapping regions for representing beliefs and attentional reorienting may simply be due to adjacent, but distinct, neuronal populations that code for each. The resolution of typical fMRI studies may not be good enough to show that distinct/adjacent neuronal populations code for each of these processes. In a study following Decety and Mitchell, Saxe and colleagues used higher-resolution fMRI and showed that the peak of activation for attentional reorienting is approximately 6-10mm above the peak for representing beliefs. Further corroborating that differing populations of neurons may code for each process, they found no similarity in the patterning of fMRI response across space.

Functional imaging has also been used to study the detection of mental state information in Heider-Simmel-esque animations of moving geometric shapes, which typical humans automatically perceive as social interactions laden with intention and emotion. Three studies found remarkably similar patterns of activation during the perception of such animations versus a random or deterministic motion control: mPFC, pSTS, fusiform face area (FFA), and amygdala were selectively engaged during the Theory of Mind condition. Another study presented subjects with an animation of two dots moving with a parameterized degree of intentionality (quantifying the extent to which the dots chased each other), and found that pSTS activation correlated with this parameter.

A separate body of research has implicated the posterior superior temporal sulcus in the perception of intentionality in human action; this area is also involved in perceiving biological motion, including body, eye, mouth, and point-light display motion. One study found increased pSTS activation while watching a human lift his hand versus having his hand pushed up by a piston (intentional versus unintentional action). Several studies have found increased pSTS activation when subjects perceive a human action that is incongruent with the action expected from the actor's context and inferred intention. Examples would be: a human performing a reach-to-grasp motion on empty space next to an object, versus grasping the object; a human shifting eye gaze toward empty space next to a checkerboard target versus shifting gaze toward the target; an unladen human turning on a light with his knee, versus turning on a light with his knee while carrying a pile of books; and a walking human pausing as he passes behind a bookshelf, versus walking at a constant speed. In these studies, actions in the "congruent" case have a straightforward goal, and are easy to explain in terms of the actor's intention. The incongruent actions, on the other hand, require further explanation (why would someone twist empty space next to a gear?), and then apparently would demand more processing in the STS. Note that this region is distinct from the temporo-parietal area activated during false belief tasks. Also note that pSTS activation in most of the above studies was largely right-lateralized, following the general trend in neuroimaging studies of social cognition and perception. Also right-lateralized are the TPJ activation during false belief tasks, the STS response to biological motion, and the FFA response to faces.

Neuropsychological evidence has provided support for neuroimaging results regarding the neural basis of theory of mind. Studies with patients suffering from a lesion of the frontal lobes and the temporoparietal junction of the brain (between the temporal lobe and parietal lobe) reported that they have difficulty with some theory of mind tasks. This shows that theory of mind abilities are associated with specific parts of the human brain. However, the fact that the medial prefrontal cortex and temporoparietal junction are necessary for theory of mind tasks does not imply that these regions are specific to that function. TPJ and mPFC may subserve more general functions necessary for Theory of Mind.

Research by Vittorio Gallese, Luciano Fadiga and Giacomo Rizzolatti (reviewed in) has shown that some sensorimotor neurons, which are referred to as mirror neurons, first discovered in the premotor cortex of rhesus monkeys, may be involved in action understanding. Single-electrode recording revealed that these neurons fired when a monkey performed an action, as well as when the monkey viewed another agent carrying out the same task. Similarly, fMRI studies with human participants have shown brain regions (assumed to contain mirror neurons) that are active when one person sees another person's goal-directed action. These data have led some authors to suggest that mirror neurons may provide the basis for theory of mind in the brain, and to support simulation theory of mind reading (see above).

There is also evidence against the link between mirror neurons and theory of mind. First, macaque monkeys have mirror neurons but do not seem to have a 'human-like' capacity to understand theory of mind and belief. Second, fMRI studies of theory of mind typically report activation in the mPFC, temporal poles and TPJ or STS, but these brain areas are not part of the mirror neuron system. Some investigators, like developmental psychologist Andrew Meltzoff and neuroscientist Jean Decety, believe that mirror neurons merely facilitate learning through imitation and may provide a precursor to the development of Theory of Mind. Others, like philosopher Shaun Gallagher, suggest that mirror-neuron activation, on a number of counts, fails to meet the definition of simulation as proposed by the simulation theory of mindreading.

In a recent paper, Keren Haroush and Ziv Williams outlined the case for a group of neurons in primates' brains that uniquely predicted the choice selection of their interacting partner. These primates' neurons, located in the anterior cingulate cortex of rhesus monkeys, were observed using single-unit recording while the monkeys played a variant of the iterative prisoner's dilemma game. By identifying cells that represent the yet unknown intentions of a game partner, Haroush & Williams' study supports the idea that theory of mind may be a fundamental and generalized process, and suggests that "anterior cingulate cortex" neurons may potentially act to complement the function of mirror neurons during social interchange.

Several neuroimaging studies have looked at the neural basis theory of mind impairment in subjects with Asperger syndrome and high-functioning autism (HFA). The first PET study of theory of mind in autism (also the first neuroimaging study using a task-induced activation paradigm in autism) replicated a prior study in normal individuals, which employed a story-comprehension task. This study found displaced and diminished mPFC activation in subjects with autism. However, because the study used only six subjects with autism, and because the spatial resolution of PET imaging is relatively poor, these results should be considered preliminary.

A subsequent fMRI study scanned normally developing adults and adults with HFA while performing a "reading the mind in the eyes" task: viewing a photo of a human's eyes and choosing which of two adjectives better describes the person's mental state, versus a gender discrimination control. The authors found activity in orbitofrontal cortex, STS, and amygdala in normal subjects, and found no amygdala activation and abnormal STS activation in subjects with autism.

A more recent PET study looked at brain activity in individuals with HFA and Asperger syndrome while viewing Heider-Simmel animations (see above) versus a random motion control. In contrast to normally developing subjects, those with autism showed no STS or FFA activation, and significantly less mPFC and amygdala activation. Activity in extrastriate regions V3 and LO was identical across the two groups, suggesting intact lower-level visual processing in the subjects with autism. The study also reported significantly less functional connectivity between STS and V3 in the autism group. Note, however, that decreased temporal correlation between activity in STS and V3 would be expected simply from the lack of an evoked response in STS to intent-laden animations in subjects with autism. A more informative analysis would be to compute functional connectivity after regressing out evoked responses from all-time series.

A subsequent study, using the incongruent/congruent gaze-shift paradigm described above, found that in high-functioning adults with autism, posterior STS (pSTS) activation was undifferentiated while they watched a human shift gaze toward a target and then toward adjacent empty space. The lack of additional STS processing in the incongruent state may suggest that these subjects fail to form an expectation of what the actor should do given contextual information, or that feedback about the violation of this expectation doesn't reach STS. Both explanations involve an impairment in the ability to link eye gaze shifts with intentional explanations. This study also found a significant anticorrelation between STS activation in the incongruent-congruent contrast and social subscale score on the Autism Diagnostic Interview-Revised, but not scores on the other subscales.

In 2011, an fMRI study demonstrated that the right temporoparietal junction (rTPJ) of higher-functioning adults with autism was not more selectively activated for mentalizing judgments when compared to physical judgments about self and other. rTPJ selectivity for mentalizing was also related to individual variation on clinical measures of social impairment: individuals whose rTPJ was increasingly more active for mentalizing compared to physical judgments were less socially impaired, while those who showed little to no difference in response to mentalizing or physical judgments were the most socially impaired. This evidence builds on work in typical development that suggests rTPJ is critical for representing mental state information, irrespective of whether it is about oneself or others. It also points to an explanation at the neural level for the pervasive mind-blindness difficulties in autism that are evident throughout the lifespan.

The brain regions associated with theory of mind include the superior temporal gyrus (STS), the temporoparietal junction (TPJ), the medial prefrontal cortex (MPFC), the precuneus, and the amygdala. The reduced activity in the MPFC of individuals with schizophrenia is associated with the Theory of mind deficit and may explain impairments in social function among people with schizophrenia. Increased neural activity in MPFC is related to better perspective-taking, emotion management, and increased social functioning. Disrupted brain activities in areas related to theory of mind may increase social stress or disinterest in social interaction, and contribute to the social dysfunction associated with schizophrenia.

Group member average scores of theory of mind abilities, measured with the Reading the Mind in the Eyes test (RME), are suggested as drivers of successful group performance. In particular, high group average scores on the RME are shown to be correlated with the collective intelligence factor "c" defined as a group's ability to perform a wide range of mental tasks, a group intelligence measure similar to the "g" factor for general individual intelligence. RME is a Theory of Mind test for adults that shows sufficient test-retest reliability and constantly differentiates control groups from individuals with functional autism or Asperger syndrome. It is one of the most widely accepted and well-validated tests for Theory of Mind abilities within adults.

The evolutionary origin of theory of mind remains obscure. While many theories make claims about its role in the development of human language and social cognition few of them specify in detail any evolutionary neurophysiological precursors. A recent theory claims that Theory of Mind has its roots in two defensive reactions, namely immobilization stress and tonic immobility, which are implicated in the handling of stressful encounters and also figure prominently in mammalian childrearing practices (Tsoukalas, 2018). Their combined effect seems capable of producing many of the hallmarks of theory of mind, e.g., eye-contact, gaze-following, inhibitory control and intentional attributions.

An open question is whether other animals besides humans have a genetic endowment and social environment that allows them to acquire a theory of mind in the same way that human children do. This is a contentious issue because of the problem of inferring from animal behavior the existence of thinking or of particular thoughts, or the existence of a concept of self or self-awareness, consciousness and qualia. One difficulty with non-human studies of theory of mind is the lack of sufficient numbers of naturalistic observations, giving insight into what the evolutionary pressures might be on a species' development of theory of mind.

Non-human research still has a major place in this field, however, and is especially useful in illuminating which nonverbal behaviors signify components of theory of mind, and in pointing to possible stepping points in the evolution of what many claim to be a uniquely human aspect of social cognition. While it is difficult to study human-like theory of mind and mental states in species whose potential mental states we have an incomplete understanding, researchers can focus on simpler components of more complex capabilities. For example, many researchers focus on animals' understanding of intention, gaze, perspective, or knowledge (or rather, what another being has seen). A study that looked at understanding of intention in orangutans, chimpanzees and children showed that all three species understood the difference between accidental and intentional acts. Part of the difficulty in this line of research is that observed phenomena can often be explained as simple stimulus-response learning, as it is in the nature of any theorizers of mind to have to extrapolate internal mental states from observable behavior. Recently, most non-human theory of mind research has focused on monkeys and great apes, who are of most interest in the study of the evolution of human social cognition. Other studies relevant to attributions theory of mind have been conducted using plovers and dogs, and have shown preliminary evidence of understanding attention—one precursor of theory of mind—in others.

There has been some controversy over the interpretation of evidence purporting to show theory of mind ability—or inability—in animals. Two examples serve as demonstration: first, Povinelli "et al." (1990) presented chimpanzees with the choice of two experimenters from whom to request food: one who had seen where food was hidden, and one who, by virtue of one of a variety of mechanisms (having a bucket or bag over his head; a blindfold over his eyes; or being turned away from the baiting) does not know, and can only guess. They found that the animals failed in most cases to differentially request food from the "knower". By contrast, Hare, Call, and Tomasello (2001) found that subordinate chimpanzees were able to use the knowledge state of dominant rival chimpanzees to determine which container of hidden food they approached. William Field and Sue Savage-Rumbaugh believe that bonobos have developed theory of mind, and cite their communications with a captive bonobo, Kanzi, as evidence.

In a 2016 experiment, ravens "Corvus corax" were shown to take into account visual access of unseen conspecifics. The researchers argued that "ravens can generalize from their own perceptual experience to infer the possibility of being seen".

A 2016 study published by evolutionary anthropologist Christopher Krupenye brings new light to the existence of Theory of Mind, and particularly false beliefs, in non-human primates.







</doc>
<doc id="20062" url="https://en.wikipedia.org/wiki?curid=20062" title="Meditation">
Meditation

Meditation is a practice where an individual uses a technique – such as mindfulness, or focusing the mind on a particular object, thought, or activity – to train attention and awareness, and achieve a mentally clear and emotionally calm and stable state. Scholars have found meditation difficult to define, as practices vary both between traditions and within them.

Meditation has been practiced since antiquity in numerous religious traditions, often as part of the path towards enlightenment and self realization. The earliest records of meditation (Dhyana), come from the Hindu traditions of Vedantism. Since the 19th century, Asian meditative techniques have spread to other cultures where they have also found application in non-spiritual contexts, such as business and health.

Meditation may be used with the aim of reducing stress, anxiety, depression, and pain, and increasing peace, perception, self-concept, and well-being. Meditation is under research to define its possible health (psychological, neurological, and cardiovascular) and other effects.

The English "meditation" is derived from Old French "meditacioun", in turn from Latin "meditatio" from a verb "meditari", meaning "to think, contemplate, devise, ponder". The use of the term "meditatio" as part of a formal, stepwise process of meditation goes back to the 12th century monk Guigo II.

Apart from its historical usage, the term "meditation" was introduced as a translation for Eastern spiritual practices, referred to as "dhyāna" in Hinduism and Buddhism and which comes from the Sanskrit root "dhyai", meaning to contemplate or meditate. The term "meditation" in English may also refer to practices from Islamic Sufism, or other traditions such as Jewish Kabbalah and Christian Hesychasm.

Meditation has proven difficult to define as it covers a wide range of dissimilar practices in different traditions. In popular usage, the word "meditation" and the phrase "meditative practice" are often used imprecisely to designate practices found across many cultures. These can include almost anything that is claimed to train the attention or to teach calm or compassion. There remains no definition of necessary and sufficient criteria for meditation that has achieved universal or widespread acceptance within the modern scientific community. In 1971, Claudio Naranjo noted that "The word 'meditation' has been used to designate a variety of practices that differ enough from one another so that we may find trouble in defining what "meditation" is." A 2009 study noted a "persistent lack of consensus in the literature" and a "seeming intractability of defining meditation".

Dictionaries give both the original Latin meaning of "think[ing] deeply about (something)"; as well as the popular usage of "to focus one's mind for a period of time," "the act of giving your attention to only one thing, either as a religious activity or as a way of becoming calm and relaxed," and "to engage in mental exercise (such as concentration on one's breathing or repetition of a mantra) for the purpose of reaching a heightened level of spiritual awareness."

In modern psychological research, meditation has been defined and characterized in a variety of ways. Many of these emphasize the role of attention. and characterize the practice of meditation as attempts to get beyond the reflexive, "discursive thinking" or "logic" mind to achieve a deeper, more devout, or more relaxed state.

Bond et al. (2009) identified criteria for defining a practice as meditation "for use in a comprehensive systematic review of the therapeutic use of meditation," using "a 5-round Delphi study with a panel of 7 experts in meditation research" who were also trained in diverse but empirically highly studied (Eastern-derived or clinical) forms of meditation;
Several other definitions of meditation have been used by influential modern reviews of research on meditation across multiple traditions:

Some of the difficulty in precisely defining meditation has been in recognizing the particularities of the many various traditions; and theories and practice can differ within a tradition. Taylor noted that even within a faith such as "Hindu" or "Buddhist", schools and individual teachers may teach distinct types of meditation.
Ornstein noted that "Most techniques of meditation do not exist as solitary practices but are only artificially separable from an entire system of practice and belief." For instance, while monks meditate as part of their everyday lives, they also engage the codified rules and live together in monasteries in specific cultural settings that go along with their meditative practices.

In the West, meditation techniques have sometimes been thought of in two broad categories: focused (or concentrative) meditation and open monitoring (or mindfulness) meditation.

"One style, Focused Attention (FA) meditation, entails the voluntary focusing of attention on a chosen object, breathing, image, or words. The other style, Open Monitoring (OM) meditation, involves non-reactive monitoring of the content of experience from moment to moment."
"Direction of mental attention... A practitioner can focus intensively on one particular object (so-called "concentrative meditation"), on all mental events that enter the field of awareness (so-called "mindfulness meditation"), or both specific focal points and the field of awareness."
Focused methods include paying attention to the breath, to an idea or feeling (such as mettā (loving-kindness)), to a kōan, or to a mantra (such as in transcendental meditation), and single point meditation.

Open monitoring methods include mindfulness, shikantaza and other awareness states.

Practices using both methods include vipassana (which uses anapanasati as a preparation), and samatha (calm-abiding).

In "No thought" methods, ""the practitioner is fully alert, aware, and in control of their faculties but does not experience any unwanted thought activity." This is in contrast to the common meditative approaches of being detached from, and non-judgmental of, thoughts, but not of aiming for thoughts to cease. In the meditation practice of the Sahaja yoga spiritual movement, the focus is on thoughts ceasing. Clear light yoga also aims at a state of no mental content, as does the no thought (wu nian) state taught by Huineng, and the teaching of Yaoshan Weiyan.

One proposal is that transcendental meditation and possibly other techniques be grouped as an 'automatic self-transcending' set of techniques. Other typologies include dividing meditation into concentrative, generative, receptive and reflective practices.

The Transcendental Meditation technique recommends practice of 20 minutes twice per day. Some techniques suggest less time, especially when starting meditation, and Richard Davidson has quoted research saying benefits can be achieved with a practice of only 8 minutes per day. Some meditators practice for much longer, particularly when on a course or retreat. Some meditators find practice best in the hours before dawn.

Asanas and positions such as the full-lotus, half-lotus, Burmese, Seiza, and kneeling positions are popular in Buddhism, Jainism and Hinduism, although other postures such as sitting, supine (lying), and standing are also used. Meditation is also sometimes done while walking, known as kinhin, while doing a simple task mindfully, known as samu or while lying down known as savasana.

Some religions have traditions of using prayer beads as tools in devotional meditation. Most prayer beads and Christian rosaries consist of pearls or beads linked together by a thread. The Roman Catholic rosary is a string of beads containing five sets with ten small beads. The Hindu japa mala has 108 beads (the figure 108 in itself having spiritual significance, as well as those used in Jainism and Buddhist prayer beads. Each bead is counted once as a person recites a mantra until the person has gone all the way around the mala. The Muslim misbaha has 99 beads.

The Buddhist literature has many stories of Enlightenment being attained through disciples being struck by their masters. According to T. Griffith Foulk professor of Religion at Sarah Lawrence College the encouragement stick was an integral part of the Zen practice:

Richard Davidson has expressed the view that having a narrative can help maintenance of daily practice. For instance he himself prostrates to the teachings, and meditates "not primarily for my benefit, but for the benefit of others."

There are many schools and styles of meditation within Hinduism. In pre-modern and traditional Hinduism, "Yoga" and "Dhyana" are practised to realize union of one's eternal self or soul, one's ātman. In Advaita Vedanta this is equated with the omnipresent and non-dual Brahman. In the dualistic Yoga school and Samkhya, the Self is called Purusha, a pure consciousness separate from matter. Depending on the tradition, the liberative event is named moksha, vimukti or kaivalya.

The earliest clear references to meditation in Hindu literature are in the middle Upanishads and the Mahabharata (including the Bhagavad Gita). According to Gavin Flood, the earlier Brihadaranyaka Upanishad is describing meditation when it states that "having become calm and concentrated, one perceives the self ("ātman") within oneself".

One of the most influential texts of classical Hindu Yoga is Patañjali's Yoga sutras (c. 400 CE), a text associated with Yoga and Samkhya, which outlines eight limbs leading to kaivalya ("aloneness"). These are ethical discipline (yamas), rules (niyamas), physical postures (āsanas), breath control (prāṇāyama), withdrawal from the senses (pratyāhāra), one-pointedness of mind (dhāraṇā), meditation (dhyāna), and finally samādhi.

Later developments in Hindu meditation include the compilation of Hatha Yoga (forceful yoga) compendiums like the Hatha Yoga Pradipika, the development of Bhakti yoga as a major form of meditation and Tantra. Another important Hindu yoga text is the Yoga Yajnavalkya, which makes use of Hatha Yoga and Vedanta Philosophy.

Jain meditation and spiritual practices system were referred to as salvation-path. It has three parts called the "Ratnatraya" "Three Jewels": right perception and faith, right knowledge and right conduct. Meditation in Jainism aims at realizing the self, attaining salvation, and taking the soul to complete freedom. It aims to reach and to remain in the pure state of soul which is believed to be pure consciousness, beyond any attachment or aversion. The practitioner strives to be just a knower-seer (Gyata-Drashta). Jain meditation can be broadly categorized to "Dharmya Dhyana" and "Shukla Dhyana".

Jainism uses meditation techniques such as "pindāstha-dhyāna, padāstha-dhyāna, rūpāstha-dhyāna, rūpātita-dhyāna, and savīrya-dhyāna". In "padāstha dhyāna" one focuses on a mantra. A mantra could be either a combination of core letters or words on deity or themes. There is a rich tradition of Mantra in Jainism. All Jain followers irrespective of their sect, whether Digambara or Svetambara, practice mantra. Mantra chanting is an important part of daily lives of Jain monks and followers. Mantra chanting can be done either loudly or silently in mind.

Contemplation is a very old and important meditation technique. The practitioner meditates deeply on subtle facts. In "agnya vichāya", one contemplates on seven facts – life and non-life, the inflow, bondage, stoppage and removal of "karmas", and the final accomplishment of liberation. In "apaya vichāya", one contemplates on the incorrect insights one indulges, which eventually develops right insight. In "vipaka vichāya", one reflects on the eight causes or basic types of "karma". In "sansathan vichāya", one thinks about the vastness of the universe and the loneliness of the soul.

Buddhist meditation refers to the meditative practices associated with the religion and philosophy of Buddhism. Core meditation techniques have been preserved in ancient Buddhist texts and have proliferated and diversified through teacher-student transmissions. Buddhists pursue meditation as part of the path toward awakening and nirvana. The closest words for meditation in the classical languages of Buddhism are "bhāvanā", "jhāna"/"dhyāna", and "vipassana".

Buddhist meditation techniques have become popular in the wider world, with many non-Buddhists taking them up. There is considerable homogeneity across meditative practices – such as breath meditation and various recollections ("anussati") – across Buddhist schools, as well as significant diversity. In the Theravāda tradition, there are over fifty methods for developing mindfulness and forty for developing concentration, while in the Tibetan tradition there are thousands of visualization meditations. Most classical and contemporary Buddhist meditation guides are school-specific.

According to the Theravada and Sarvastivada commentatorial traditions, and the Tibetan tradition, the Buddha identified two paramount mental qualities that arise from wholesome meditative practice:


Through the meditative development of serenity, one is able to weaken the obscuring hindrances and bring the mind to a collected, pliant and still state (samadhi). This quality of mind then supports the development of insight and wisdom (Prajñā) which is the quality of mind that can "clearly see" ("vi-passana") the nature of phenomena. What exactly is to be seen varies within the Buddhist traditions. In Theravada, all phenomena are to be seen as impermanent, suffering, not-self and empty. When this happens, one develops dispassion ("viraga") for all phenomena, including all negative qualities and hindrances and lets them go. It is through the release of the hindrances and ending of craving through the meditative development of insight that one gains liberation.

In the modern era, Buddhist meditation saw increasing popularity due to the influence of Buddhist modernism on Asian Buddhism, and western lay interest in Zen and the Vipassana movement. The spread of Buddhist meditation to the Western world paralleled the spread of Buddhism in the West. The modernized concept of mindfulness (based on the Buddhist term "sati") and related meditative practices have in turn led to mindfulness based therapies.

In Sikhism, simran (meditation) and good deeds are both necessary to achieve the devotee's Spiritual goals; without good deeds meditation is futile. When Sikhs meditate, they aim to feel God's presence and emerge in the divine light. It is only God's divine will or order that allows a devotee to desire to begin to meditate.Nām Japnā involves focusing one's attention on the names or great attributes of God.

Taoist meditation has developed techniques including concentration, visualization, "qi" cultivation, contemplation, and mindfulness meditations in its long history. Traditional Daoist meditative practices were influenced by Chinese Buddhism from around the 5th century, and influenced Traditional Chinese medicine and the Chinese martial arts.

Livia Kohn distinguishes three basic types of Taoist meditation: "concentrative", "insight", and "visualization". "Ding" 定 (literally means "decide; settle; stabilize") refers to "deep concentration", "intent contemplation", or "perfect absorption". "Guan" 觀 (lit. "watch; observe; view") meditation seeks to merge and attain unity with the Dao. It was developed by Tang Dynasty (618–907) Taoist masters based upon the "Tiantai" Buddhist practice of "Vipassanā" "insight" or "wisdom" meditation. "Cun" 存 (lit. "exist; be present; survive") has a sense of "to cause to exist; to make present" in the meditation techniques popularized by the Taoist Shangqing and Lingbao Schools. A meditator visualizes or actualizes solar and lunar essences, lights, and deities within their body, which supposedly results in health and longevity, even "xian" 仙/仚/僊, "immortality".

The (late 4th century BCE) "Guanzi" essay "Neiye" "Inward training" is the oldest received writing on the subject of "qi" cultivation and breath-control meditation techniques. For instance, "When you enlarge your mind and let go of it, when you relax your vital breath and expand it, when your body is calm and unmoving: And you can maintain the One and discard the myriad disturbances. ... This is called "revolving the vital breath": Your thoughts and deeds seem heavenly."

The (c. 3rd century BCE) Taoist "Zhuangzi" records "zuowang" or "sitting forgetting" meditation. Confucius asked his disciple Yan Hui to explain what "sit and forget" means: "I slough off my limbs and trunk, dim my intelligence, depart from my form, leave knowledge behind, and become identical with the Transformational Thoroughfare."

Taoist meditation practices are central to Chinese martial arts (and some Japanese martial arts), especially the "qi"-related "neijia" "internal martial arts". Some well-known examples are "daoyin" "guiding and pulling", "qigong" "life-energy exercises", "neigong" "internal exercises", "neidan" "internal alchemy", and "taijiquan" "great ultimate boxing", which is thought of as moving meditation. One common explanation contrasts "movement in stillness" referring to energetic visualization of "qi" circulation in "qigong" and "zuochan" "seated meditation", versus "stillness in movement" referring to a state of meditative calm in "taijiquan" forms.

Judaism has made use of meditative practices for thousands of years. For instance, in the Torah, the patriarch Isaac is described as going "לשוח" ("lasuach") in the field – a term understood by all commentators as some type of meditative practice (Genesis 24:63). Similarly, there are indications throughout the Tanakh (the Hebrew Bible) that the prophets meditated. In the Old Testament, there are two Hebrew words for meditation: "hāgâ" (), "to sigh" or "murmur", but also "to meditate", and "sîḥâ" (), "to muse", or "rehearse in one's mind".

Classical Jewish texts espouse a wide range of meditative practices, often associated with the cultivation of "kavanah" or intention. The first layer of rabbinic law, the Mishnah, describes ancient sages "waiting" for an hour before their prayers, "in order to direct their hearts to the Omnipresent One (Mishnah Berakhot 5:1). Other early rabbinic texts include instructions for visualizing the Divine Presence (B. Talmud Sanhedrin 22a) and breathing with conscious gratitude for every breath (Genesis Rabba 14:9).

One of the best known types of meditation in early Jewish mysticism was the work of the Merkabah, from the root /R-K-B/ meaning "chariot" (of God). Some meditative traditions have been encouraged in Kabbalah, and some Jews have described Kabbalah as an inherently meditative field of study. Kabbalistic meditation often involves the mental visualization of the supernal realms. Aryeh Kaplan has argued that the ultimate purpose of Kabbalistic meditation is to understand and cleave to the Divine.

Meditation has been of interest to a wide variety of modern Jews. In modern Jewish practice, one of the best known meditative practices is called "hitbodedut" ("התבודדות", alternatively transliterated as "hisbodedus"), and is explained in Kabbalistic, Hasidic, and Mussar writings, especially the Hasidic method of Rabbi Nachman of Breslav. The word derives from the Hebrew word "boded" (בודד), meaning the state of being alone. Another Hasidic system is the Habad method of "hisbonenus", related to the Sephirah of "Binah", Hebrew for understanding. This practice is the analytical reflective process of making oneself understand a mystical concept well, that follows and internalises its study in Hasidic writings. The Musar Movement, founded by Rabbi Israel Salanter in the middle of the nineteenth-century, emphasized meditative practices of introspection and visualization that could help to improve moral character. Conservative rabbi Alan Lew has emphasized meditation playing an important role in the process of "teshuvah" (repentance). Jewish Buddhists have adopted Buddhist styles of meditation.

Christian meditation is a term for a form of prayer in which a structured attempt is made to get in touch with and deliberately reflect upon the revelations of God. The word meditation comes from the Latin word "meditari", which means to concentrate. Christian meditation is the process of deliberately focusing on specific thoughts (e.g. a biblical scene involving Jesus and the Virgin Mary) and reflecting on their meaning in the context of the love of God. Christian meditation is sometimes taken to mean the middle level in a broad three stage characterization of prayer: it then involves more reflection than first level vocal prayer, but is more structured than the multiple layers of contemplation in Christianity.

The Rosary is a devotion for the meditation of the mysteries of Jesus and Mary. “The gentle repetition of its prayers makes it an excellent means to moving into deeper meditation. It gives us an opportunity to open ourselves to God’s word, to refine our interior gaze by turning our minds to the life of Christ. The first principle is that meditation is learned through practice. Many people who practice rosary meditation begin very simply and gradually develop a more sophisticated meditation. The meditator learns to hear an interior voice, the voice of God”.

According to Edmund P. Clowney, Christian meditation contrasts with Eastern forms of meditation as radically as the portrayal of God the Father in the Bible contrasts with depictions of Krishna or Brahman in Indian teachings. Unlike some Eastern styles, most styles of Christian meditation do not rely on the repeated use of mantras, and yet are also intended to stimulate thought and deepen meaning. Christian meditation aims to heighten the personal relationship based on the love of God that marks Christian communion. In "Aspects of Christian meditation", the Catholic Church warned of potential incompatibilities in mixing Christian and Eastern styles of meditation. In 2003, in "A Christian reflection on the New Age" the Vatican announced that the "Church avoids any concept that is close to those of the New Age".

Salah is a mandatory act of devotion performed by Muslims five times per day. The body goes through sets of different postures, as the mind attains a level of concentration called "khushu".

A second optional type of meditation, called dhikr, meaning remembering and mentioning God, is interpreted in different meditative techniques in Sufism or Islamic mysticism. This became one of the essential elements of Sufism as it was systematized traditionally. It is juxtaposed with "fikr" (thinking) which leads to knowledge. By the 12th century, the practice of Sufism included specific meditative techniques, and its followers practiced breathing controls and the repetition of holy words.

Sufism uses a meditative procedure like Buddhist concentration, involving high-intensity and sharply focused introspection. In the Oveyssi-Shahmaghsoudi Sufi order, for example, muraqaba takes the form of tamarkoz, "concentration" in Persian.

"Tafakkur" or "tadabbur" in Sufism literally means "reflection upon the universe": this is considered to permit access to a form of cognitive and emotional development that can emanate only from the higher level, i.e. from God. The sensation of receiving divine inspiration awakens and liberates both heart and intellect, permitting such inner growth that the apparently mundane actually takes on the quality of the infinite. Muslim teachings embrace life as a test of one's submission to God.

In the teachings of the Bahá'í Faith, meditation is a primary tool for spiritual development, involving reflection on the words of God. While prayer and meditation are linked, where meditation happens generally in a prayerful attitude, prayer is seen specifically as turning toward God, and meditation is seen as a communion with one's self where one focuses on the divine.

In Bahá'í teachings the purpose of meditation is to strengthen one's understanding of the words of God, and to make one's soul more susceptible to their potentially transformative power, more receptive to the need for both prayer and meditation to bring about and maintain a spiritual communion with God.

Bahá'u'lláh, the founder of the religion, never specified any particular form of meditation, and thus each person is free to choose their own form. However, he did state that Bahá'ís should read a passage of the Bahá'í writings twice a day, once in the morning, and once in the evening, and meditate on it. He also encouraged people to reflect on one's actions and worth at the end of each day. During the Nineteen Day Fast, a period of the year during which Bahá'ís adhere to a sunrise-to-sunset fast, they meditate and pray to reinvigorate their spiritual forces.

Movements which use magic, such as Wicca, Thelema, Neopaganism, and occultism, often require their adherents to meditate as a preliminary to the magical work. This is because magic is often thought to require a particular state of mind in order to make contact with spirits, or because one has to visualize one's goal or otherwise keep intent focused for a long period during the ritual in order to see the desired outcome. Meditation practice in these religions usually revolves around visualization, absorbing energy from the universe or higher self, directing one's internal energy, and inducing various trance states. Meditation and magic practice often overlap in these religions as meditation is often seen as merely a stepping stone to supernatural power, and the meditation sessions may be peppered with various chants and spells.

Mantra meditation, with the use of a japa mala and especially with focus on the Hare Krishna maha-mantra, is a central practice of the Gaudiya Vaishnava faith tradition and the International Society for Krishna Consciousness (ISKCON), also known as the Hare Krishna movement. Other popular New Religious Movements include the Ramakrishna Mission, Vedanta Society, Divine Light Mission, Chinmaya Mission, Osho, Sahaja Yoga, Transcendental Meditation, Oneness University, Brahma Kumaris and Vihangam Yoga.

New Age meditations are often influenced by Eastern philosophy, mysticism, yoga, Hinduism and Buddhism, yet may contain some degree of Western influence. In the West, meditation found its mainstream roots through the social revolution of the 1960s and 1970s, when many of the youth of the day rebelled against traditional religion as a reaction against what some perceived as the failure of Christianity to provide spiritual and ethical guidance.
New Age meditation as practised by the early hippies is regarded for its techniques of blanking out the mind and releasing oneself from conscious thinking. This is often aided by repetitive chanting of a mantra, or focusing on an object. New Age meditation evolved into a range of purposes and practices, from serenity and balance to access to other realms of consciousness to the concentration of energy in group meditation to the supreme goal of samadhi, as in the ancient yogic practice of meditation.

The US National Center for Complementary and Integrative Health states that "Meditation is a mind and body practice that has a long history of use for increasing calmness and physical relaxation, improving psychological balance, coping with illness, and enhancing overall health and well-being." A 2014 review found that practice of mindfulness meditation for two to six months by people undergoing long-term psychiatric or medical therapy could produce small improvements in anxiety, pain, or depression. In 2017, the American Heart Association issued a scientific statement that meditation may be a reasonable adjunct practice to help reduce the risk of cardiovascular diseases, with the qualification that meditation needs to be better defined in higher-quality clinical research of these disorders.

Low-quality evidence indicates that meditation may help with irritable bowel syndrome, insomnia, cognitive decline in the elderly, and post-traumatic stress disorder.

A 2010 review of the literature on spirituality and performance in organizations found an increase in corporate meditation programs.

As of 2016 around a quarter of U.S. employers were using stress reduction initiatives. The goal was to help reduce stress and improve reactions to stress. Aetna now offers its program to its customers. Google also implements mindfulness, offering more than a dozen meditation courses, with the most prominent one, "Search Inside Yourself", having been implemented since 2007. General Mills offers the Mindful Leadership Program Series, a course which uses a combination of mindfulness meditation, yoga and dialogue with the intention of developing the mind's capacity to pay attention.

Herbert Benson of Harvard Medical School conducted a series of clinical tests on meditators from various disciplines, including the Transcendental Meditation technique and Tibetan Buddhism. In 1975, Benson published a book titled "The Relaxation Response" where he outlined his own version of meditation for relaxation. Also in the 1970s, the American psychologist Patricia Carrington developed a similar technique called Clinically Standardized Meditation (CSM). In Norway, another sound-based method called Acem Meditation developed a psychology of meditation and has been the subject of several scientific studies.

Biofeedback has been used by many researchers since the 1950s in an effort to enter deeper states of mind.

The history of meditation is intimately bound up with the religious context within which it was practiced. Some authors have even suggested the hypothesis that the emergence of the capacity for focused attention, an element of many methods of meditation, may have contributed to the latest phases of human biological evolution. Some of the earliest references to meditation are found in the Hindu Vedas of India. Wilson translates the most famous Vedic mantra "Gayatri" as: "We meditate on that desirable light of the divine Savitri, who influences our pious rites" (Rigveda : Mandala-3, Sukta-62, Rcha-10). Around the 6th to 5th centuries BCE, other forms of meditation developed via Confucianism and Taoism in China as well as Hinduism, Jainism, and early Buddhism in India.

In the Roman Empire, by 20 BCE Philo of Alexandria had written on some form of "spiritual exercises" involving attention (prosoche) and concentration and by the 3rd century Plotinus had developed meditative techniques.

The Pāli Canon from the 1st century BCE considers Buddhist meditation as a step towards liberation. By the time Buddhism was spreading in China, the "Vimalakirti Sutra" which dates to 100 CE included a number of passages on meditation, clearly pointing to Zen (known as Chan in China, Thiền in Vietnam, and Seon in Korea). The Silk Road transmission of Buddhism introduced meditation to other Asian countries, and in 653 the first meditation hall was opened in Singapore. Returning from China around 1227, Dōgen wrote the instructions for zazen.

The Islamic practice of Dhikr had involved the repetition of the 99 Names of God since the 8th or 9th century. By the 12th century, the practice of Sufism included specific meditative techniques, and its followers practiced breathing controls and the repetition of holy words. Interactions with Indians or the Sufis may have influenced the Eastern Christian meditation approach to hesychasm, but this can not be proved. Between the 10th and 14th centuries, hesychasm was developed, particularly on Mount Athos in Greece, and involves the repetition of the Jesus prayer.

Western Christian meditation contrasts with most other approaches in that it does not involve the repetition of any phrase or action and requires no specific posture. Western Christian meditation progressed from the 6th century practice of Bible reading among Benedictine monks called Lectio Divina, i.e. divine reading. Its four formal steps as a "ladder" were defined by the monk Guigo II in the 12th century with the Latin terms "lectio", "meditatio", "oratio", and "contemplatio" (i.e. read, ponder, pray, contemplate). Western Christian meditation was further developed by saints such as Ignatius of Loyola and Teresa of Avila in the 16th century.

Meditation has spread in the West since the late 19th century, accompanying increased travel and communication among cultures worldwide. Most prominent has been the transmission of Asian-derived practices to the West. In addition, interest in some Western-based meditative practices has been revived, and these have been disseminated to a limited extent to Asian countries.

Ideas about Eastern meditation had begun "seeping into American popular culture even before the American Revolution through the various sects of European occult Christianity", and such ideas "came pouring in [to America] during the era of the transcendentalists, especially between the 1840s and the 1880s." The following decades saw further spread of these ideas to America:

More recently, in the 1960s, another surge in Western interest in meditative practices began. The rise of communist political power in Asia led to many Asian spiritual teachers taking refuge in Western countries, oftentimes as refugees. In addition to spiritual forms of meditation, secular forms of meditation have taken root. Rather than focusing on spiritual growth, secular meditation emphasizes stress reduction, relaxation and self-improvement.

Research on the processes and effects of meditation is a subfield of neurological research. Modern scientific techniques, such as fMRI and EEG, were used to observe neurological responses during meditation. Concerns have been raised on the quality of meditation research, including the particular characteristics of individuals who tend to participate.

Since the 1970s, clinical psychology and psychiatry have developed meditation techniques for numerous psychological conditions. Mindfulness practice is employed in psychology to alleviate mental and physical conditions, such as reducing depression, stress, and anxiety. Mindfulness is also used in the treatment of drug addiction, although the quality of research has been poor. Studies demonstrate that meditation has a moderate effect to reduce pain. There is insufficient evidence for any effect of meditation on positive mood, attention, eating habits, sleep, or body weight.

A 2017 systematic review and meta-analysis of the effects of meditation on empathy, compassion, and prosocial behaviors found that meditation practices had small to medium effects on self-reported and observable outcomes, concluding that such practices can "improve positive prosocial emotions and behaviors".

The 2012 US National Health Interview Survey (NHIS) (34,525 subjects) found 8% of US adults used meditation, with lifetime and 12-month prevalence of meditation use of 5.2% and 4.1% respectively. In the 2017 NHIS survey, meditation use among workers was 10% (up from 8% in 2002).

The psychologist Thomas Joiner argues that modern mindfulness meditation has been "corrupted" for commercial gain by self-help celebrities, and suggests that it encourages unhealthy narcissistic and self-obsessed mindsets.

Meditation has been correlated with unpleasant experiences in some people. 

In one study, published in 2019, of 1,232 regular meditators with at least two months of meditation experience, about a quarter reported having had particularly unpleasant meditation-related experiences (such as anxiety, fear, distorted emotions or thoughts, altered sense of self or the world), which they thought may have been caused by their meditation practice. Meditators with high levels of repetitive negative thinking and those who only engage in deconstructive meditation were more likely to report unpleasant side effects. Adverse effects were less frequently reported in women and religious meditators. 

Difficult experiences encountered in meditation are mentioned in traditional sources; and some may be considered to be just an expected part of the process: for example: seven stages of purification mentioned in Theravāda Buddhism, or possible “unwholesome or frightening visions” mentioned in a practical manual on vipassanā meditation.

Many major traditions in which meditation is practiced, such as Buddhism and Hinduism, advise members not to consume intoxicants, while others, such as the Rastafarian movements and Native American Church, view drugs as integral to their religious lifestyle.

The fifth of the five precepts of the Pancasila, the ethical code in the Theravada and Mahayana Buddhist traditions, states that adherents must: "abstain from fermented and distilled beverages that cause heedlessness."

On the other hand, the ingestion of psychoactives has been a central feature in the rituals of many religions, in order to produce altered states of consciousness. In several traditional shamanistic ceremonies, drugs are used as agents of ritual. In the Rastafari movement, cannabis is believed to be a gift from Jah and a sacred herb to be used regularly, while alcohol is considered to debase man. Native Americans use peyote, as part of religious ceremony, continuing today.




</doc>
<doc id="1005874" url="https://en.wikipedia.org/wiki?curid=1005874" title="Principle">
Principle

A principle is a proposition or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 "Principles of Operation".

Examples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.

In common English, it is a substantive and collective term referring to rule governance, the absence of which, being "unprincipled", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only "in principle" but not in fact.

A principle represents values that orient and rule the conduct of persons in a particular society. To "act on principle" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.

It represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.

Archimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the "population principle", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.

The principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:

However, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that "the Earth is round", corresponds to a direct relation between the subject and the predicate.

According to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.

The principle of the excluding third or "principium tertium exclusum" is a principle of the traditional logic formulated canonically by Leibniz as: either "A" is "B" or "A" isn't "B". It is read the following way: either "P" is true, or its denial ¬"P" is.
It is also known as "tertium non datur" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).



</doc>
<doc id="11081176" url="https://en.wikipedia.org/wiki?curid=11081176" title="Mind–body problem">
Mind–body problem

The mind–body problem is a debate concerning the relationship between thought and consciousness in the human mind, and the brain as part of the physical body. It is distinct from the question of how mind and body function chemically and physiologically, as that question presupposes an interactionist account of mind–body relations. This question arises when mind and body are considered as distinct, based on the premise that the mind and the body are fundamentally different in nature.

The problem was addressed by René Descartes in the 17th century, resulting in Cartesian dualism, and by pre-Aristotelian philosophers, in Avicennian philosophy, and in earlier Asian traditions. A variety of approaches have been proposed. Most are either dualist or monist. Dualism maintains a rigid distinction between the realms of mind and matter. Monism maintains that there is only one unifying reality, substance or essence, in terms of which everything can be explained.

Each of these categories contains numerous variants. The two main forms of dualism are substance dualism, which holds that the mind is formed of a distinct type of substance not governed by the laws of physics, and property dualism, which holds that mental properties involving conscious experience are fundamental properties, alongside the fundamental properties identified by a completed physics. The three main forms of monism are physicalism, which holds that the mind consists of matter organized in a particular way; idealism, which holds that only thought truly exists and matter is merely an illusion; and neutral monism, which holds that both mind and matter are aspects of a distinct essence that is itself identical to neither of them. Psychophysical parallelism is a third possible alternative regarding the relation between mind and body, between interaction (dualism) and one-sided action (monism).

Several philosophical perspectives have been developed which reject the mind–body dichotomy. The historical materialism of Karl Marx and subsequent writers, itself a form of physicalism, held that consciousness was engendered by the material contingencies of one's environment. An explicit rejection of the dichotomy is found in French structuralism, and is a position that generally characterized post-war Continental philosophy.

The absence of an empirically identifiable meeting point between the non-physical mind (if there is such a thing) and its physical extension (if there is such a thing) has proven problematic to dualism, and many modern philosophers of mind maintain that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, particularly in the fields of sociobiology, computer science, evolutionary psychology, and the neurosciences.

An ancient model of the mind known as the Five-Aggregate Model, described in the Buddhist teachings, explains the mind as continuously changing sense impressions and mental phenomena. Considering this model, it is possible to understand that it is the constantly changing sense impressions and mental phenomena (i.e., the mind) that experiences/analyzes all external phenomena in the world as well as all internal phenomena including the body anatomy, the nervous system as well as the organ brain. This conceptualization leads to two levels of analyses: (i) analyses conducted from a third-person perspective on how the brain works, and (ii) analyzing the moment-to-moment manifestation of an individual's mind-stream (analyses conducted from a first-person perspective). Considering the latter, the manifestation of the mind-stream is described as happening in every person all the time, even in a scientist who analyses various phenomena in the world, including analyzing and hypothesizing about the organ brain.

Philosophers David L. Robb and John F. Heil introduce mental causation in terms of the mind–body problem of interaction:
Contemporary neurophilosopher Georg Northoff suggests that mental causation is compatible with classical formal and final causality.

Biologist, theoretical neuroscientist and philosopher, Walter J. Freeman, suggests that explaining mind–body interaction in terms of "circular causation" is more relevant than linear causation.

In neuroscience, much has been learned about correlations between brain activity and subjective, conscious experiences. Many suggest that neuroscience will ultimately explain consciousness:
"...consciousness is a biological process that will eventually be explained in terms of molecular signaling pathways used by interacting populations of nerve cells..." However, this view has been criticized because "consciousness" has yet to be shown to be a "process", and the "hard problem" of relating consciousness directly to brain activity remains elusive.

The "neural correlates of consciousness" "are the smallest set of brain mechanisms and events sufficient for some specific conscious feeling, as elemental as the color red or as complex as the sensual, mysterious, and primeval sensation evoked when looking at [a] jungle scene..." Neuroscientists use empirical approaches to discover neural correlates of subjective phenomena.

A science of consciousness must explain the exact relationship between subjective conscious mental states and brain states formed by electrochemical interactions in the body, the so-called hard problem of consciousness. 
Neurobiology studies the connection scientifically, as do neuropsychology and neuropsychiatry. "Neurophilosophy" is the interdisciplinary study of neuroscience and philosophy of mind. In this pursuit, neurophilosophers, such as Patricia Churchland, 
Paul Churchland 
and Daniel Dennett, 
have focused primarily on the body rather than the mind. In this context, neuronal correlates may be viewed as causing consciousness, where consciousness can be thought of as an undefined property that depends upon this complex, adaptive, and highly interconnected biological system. However, it's unknown if discovering and characterizing neural correlates may eventually provide a theory of consciousness that can explain the first-person experience of these "systems", and determine whether other systems of equal complexity lack such features.

The massive parallelism of neural networks allows redundant populations of neurons to mediate the same or similar percepts. Nonetheless, it is assumed that every subjective state will have associated neural correlates, which can be manipulated to artificially inhibit or induce the subject's experience of that conscious state. The growing ability of neuroscientists to manipulate neurons using methods from molecular biology in combination with optical tools was achieved by the development of behavioral and organic models that are amenable to large-scale genomic analysis and manipulation. Non-human analysis such as this, in combination with imaging of the human brain, have contributed to a robust and increasingly predictive theoretical framework.

There are two common but distinct dimensions of the term "consciousness", one involving "arousal" and "states of consciousness" and the other involving "content of consciousness" and "conscious states". To be conscious "of" something, the brain must be in a relatively high state of arousal (sometimes called "vigilance"), whether awake or in REM sleep. Brain arousal level fluctuates in a circadian rhythm but these natural cycles may be influenced by lack of sleep, alcohol and other drugs, physical exertion, etc. Arousal can be measured behaviorally by the signal amplitude required to trigger a given reaction (for example, the sound level that causes a subject to turn and look toward the source). High arousal states involve conscious states that feature specific perceptual content, planning and recollection or even fantasy. Clinicians use scoring systems such as the Glasgow Coma Scale to assess the level of arousal in patients with "impaired states of consciousness" such as the comatose state, the persistent vegetative state, and the minimally conscious state. Here, "state" refers to different amounts of externalized, physical consciousness: ranging from a total absence in coma, persistent vegetative state and general anesthesia, to a fluctuating, minimally conscious state, such as sleep walking and epileptic seizure.

Many nuclei with distinct chemical signatures in the thalamus, midbrain and pons must function for a subject to be in a sufficient state of brain arousal to experience anything at all. These nuclei therefore belong to the enabling factors for consciousness. Conversely it is likely that the specific content of any particular conscious sensation is mediated by particular neurons in the cortex and their associated satellite structures, including the amygdala, thalamus, claustrum and the basal ganglia.

The following is a very brief account of some contributions to the mind–body problem.

The Buddha (480–400 B.C.E), founder of Buddhism, described the mind and the body as depending on each other in a way that two sheaves of reeds were to stand leaning against one another and taught that the world consists of mind and matter which work together, interdependently. Buddhist teachings describe the mind as manifesting from moment to moment, one thought moment at a time as a fast flowing stream. The components that make up the mind are known as the five aggregates (i.e., material form, feelings, perception, volition, and sensory consciousness), which arise and pass away continuously. The arising and passing of these aggregates in the present moment is described as being influenced by five causal laws: biological laws, psychological laws, physical laws, volitional laws, and universal laws. The Buddhist practice of mindfulness involves attending to this constantly changing mind-stream.

Ultimately, the Buddha's philosophy is that both mind and forms are conditionally arising qualities of an ever-changing universe in which, when nirvāna is attained, all phenomenal experience ceases to exist. According to the anattā doctrine of the Buddha, the conceptual self is a mere mental construct of an individual entity and is basically an impermanent illusion, sustained by form, sensation, perception, thought and consciousness. The Buddha argued that mentally clinging to any views will result in delusion and stress, since, according to the Buddha, a real self (conceptual self, being the basis of standpoints and views) cannot be found when the mind has clarity.

Plato (429–347 B.C.E.) believed that the material world is a shadow of a higher reality that consists of concepts he called Forms. According to Plato, objects in our everyday world "participate in" these Forms, which confer identity and meaning to material objects. For example, a circle drawn in the sand would be a circle only because it participates in the concept of an ideal circle that exists somewhere in the world of Forms. He argued that, as the body is from the material world, the soul is from the world of Forms and is thus immortal. He believed the soul was temporarily united with the body and would only be separated at death, when it would return to the world of Forms. Since the soul does not exist in time and space, as the body does, it can access universal truths. For Plato, ideas (or Forms) are the true reality, and are experienced by the soul. The body is for Plato empty in that it can not access the abstract reality of the world; it can only experience shadows. This is determined by Plato's essentially rationalistic epistemology.

For Aristotle (384–322 BC) "mind" is a faculty of the "soul". Regarding the soul, he said:
In the end, Aristotle saw the relation between soul and body as uncomplicated, in the same way that it is uncomplicated that a cubical shape is a property of a toy building block. The soul is a property exhibited by the body, one among many. Moreover, Aristotle proposed that when the body perishes, so does the soul, just as the shape of a building block disappears with destruction of the block.

In religious philosophy of Eastern monotheism, dualism denotes a binary opposition of an idea that contains two essential parts. The first formal concept of a "mind-body" split may be found in the "divinity - secularity" dualism of the ancient Persian religion of Zoroastrianism around the mid-fifth century BC. Gnosticism is a modern name for a variety of ancient dualistic ideas inspired by Judaism popular in the first and second century AD. These ideas later seem to have been incorporated into Galen's "tripartite soul" that led into both the Christian sentiments expressed in the later Augustinian theodicy and Avicenna’s Platonism in Islamic Philosophy.

Like Aristotle, St. Thomas Aquinas (1225–1274) believed that the mind and the body are one, like the seal and the wax are one, and it is therefore pointless to ask whether they are one. However, (referring to "mind" as "the soul") he asserted that the soul persists after the death of the body in spite of their unity, calling the soul "this particular thing". Since his view was primarily theological rather than philosophical, it is impossible to fit it neatly within either the category of physicalist or dualist.

René Descartes (1596–1650) believed that mind exerted control over the brain "via" the pineal gland:
His posited relation between mind and body is called Cartesian dualism or substance dualism. He held that "mind" was distinct from "matter", but could influence matter. How such an interaction could be exerted remains a contentious issue.

For Kant (1724–1804) beyond "mind" and "matter" there exists a world of "a priori" forms, which are seen as necessary preconditions for understanding. Some of these forms, space and time being examples, today seem to be pre-programmed in the brain.

Kant views the mind–body interaction as taking place through forces that may be of different kinds for mind and body.

For Huxley (1825–1895) the conscious mind was a by-product of the brain that has no influence upon the brain, a so-called epiphenomenon.

A. N. Whitehead advocated a sophisticated form of panpsychism that has been called by David Ray Griffin "panexperientialism".

For Popper (1902–1994) there are "three" aspects of the mind–body problem: the worlds of matter, mind, and of the creations of the mind, such as mathematics. In his view, the third-world creations of the mind could be interpreted by the second-world mind and used to affect the first-world of matter. An example might be radio, an example of the interpretation of the third-world (Maxwell's electromagnetic theory) by the second-world mind to suggest modifications of the external first world.

For Searle (b. 1932) the mind–body problem is a false dichotomy; that is, mind is a perfectly ordinary aspect of the brain. Searle proposed Biological naturalism in 1980. 





</doc>
<doc id="175456" url="https://en.wikipedia.org/wiki?curid=175456" title="Mind–body dualism">
Mind–body dualism

Mind–body dualism is the view in the philosophy of mind that mental phenomena are non-physical, or that the mind and body are distinct and separable. Thus, it encompasses a set of views about the relationship between mind and matter, and between subject and object, and is contrasted with other positions, such as physicalism and enactivism, in the mind–body problem.

Aristotle shared Plato's view of multiple souls and further elaborated a hierarchical arrangement, corresponding to the distinctive functions of plants, animals, and people: a nutritive soul of growth and metabolism that all three share; a perceptive soul of pain, pleasure, and desire that only people and other animals share; and the faculty of reason that is unique to people only. In this view, a soul is the hylomorphic form of a viable organism, wherein each level of the hierarchy formally supervenes upon the substance of the preceding level. For Aristotle, the first two souls, based on the body, perish when the living organism dies, whereas remains an immortal and perpetual intellective part of mind. For Plato however, the soul was not dependent on the physical body; he believed in metempsychosis, the migration of the soul to a new physical body.

Dualism is closely associated with the thought of René Descartes (1641), which holds that the mind is a nonphysical—and therefore, non-spatial—substance. Descartes clearly identified the mind with consciousness and self-awareness and distinguished this from the brain as the seat of intelligence. Hence, he was the first to formulate the mind–body problem in the form in which it exists today. Dualism is contrasted with various kinds of monism. Substance dualism is contrasted with all forms of materialism, but property dualism may be considered a form of emergent materialism or non-reductive physicalism in some sense.

Ontological dualism makes dual commitments about the nature of existence as it relates to mind and matter, and can be divided into three different types:


Substance dualism is a type of dualism most famously defended by René Descartes, which states that there are two kinds of foundation: mental and physical. This philosophy states that the mental can exist outside of the body, and the body cannot think. Substance dualism is important historically for having given rise to much thought regarding the famous mind–body problem. Substance dualism is a philosophical position compatible with most theologies which claim that immortal souls occupy an independent realm of existence distinct from that of the physical world. In contemporary discussions of substance dualism, philosophers posit dualist positions that are significantly less radical than Descartes's: for instance, a position defended by William Hasker called Emergent Dualism seems, to some philosophers, more intuitively attractive than Descartes's substance dualism in virtue of its being in line with ("inter alia)" evolutionary biology.

Property dualism asserts that an ontological distinction lies in the differences between properties of mind and matter, and that consciousness is ontologically irreducible to neurobiology and physics. It asserts that when matter is organized in the appropriate way (i.e., in the way that living human bodies are organized), mental properties emerge. Hence, it is a sub-branch of emergent materialism. What views properly fall under the "property dualism" rubric is itself a matter of dispute. There are different versions of property dualism, some of which claim independent categorisation.

Non-reductive physicalism is a form of property dualism in which it is asserted that all mental states are causally reducible to physical states. One argument for this has been made in the form of anomalous monism expressed by Donald Davidson, where it is argued that mental events are identical to physical events, and there can be strict law-governed causal relationships. Another argument for this has been expressed by John Searle, who is the advocate of a distinctive form of physicalism he calls biological naturalism. His view is that although mental states are ontologically irreducible to physical states, they are causally reducible (see causality). He has acknowledged that "to many people" his views and those of property dualists look a lot alike. But he thinks the comparison is misleading.

Epiphenomenalism is a form of property dualism, in which it is asserted that one or more mental states do not have any influence on physical states (both ontologically and causally irreducible). It asserts that while material causes give rise to sensations, volitions, ideas, etc., such mental phenomena themselves cause nothing further: they are causal dead-ends. This can be contrasted to interactionism, on the other hand, in which mental causes can produce material effects, and vice versa.

Predicate dualism is a view espoused by nonreductive physicalists such as Donald Davidson and Jerry Fodor, who maintain that while there is only one ontological category of substances and properties of substances (usually physical), the predicates that we use to describe mental events cannot be redescribed in terms of (or reduced to) physical predicates of natural languages. If we characterize "predicate monism" as the view subscribed to by eliminative materialists, who maintain that such intentional predicates as believe, desire, think, feel, etc., will eventually be eliminated from both the language of science and from ordinary language because the entities to which they refer do not exist, then "predicate dualism" is most easily defined as the negation of this position. Predicate dualists believe that so-called "folk psychology", with all of its propositional attitude ascriptions, is an ineliminable part of the enterprise of describing, explaining and understanding human mental states and behavior.

Davidson, for example, subscribes to Anomalous Monism, according to which there can be no strict psychophysical laws which connect mental and physical events under their descriptions as "mental" and "physical" events. However, all mental events also have physical descriptions. It is in terms of the latter that such events can be connected in law-like relations with other physical events. Mental predicates are irreducibly different in character (rational, holistic and necessary) from physical predicates (contingent, atomic and causal).

This part is about causation between properties and states of the thing under study, not its substances or predicates. Here a state is the set of all properties of what's being studied. Thus each state describes only one point in time.

Interactionism is the view that mental states, such as beliefs and desires, causally interact with physical states. This is a position which is very appealing to common-sense intuitions, notwithstanding the fact that it is very difficult to establish its validity or correctness by way of logical argumentation or empirical proof. It seems to appeal to common-sense because we are surrounded by such everyday occurrences as a child's touching a hot stove (physical event) which causes him to feel pain (mental event) and then yell and scream (physical event) which causes his parents to experience a sensation of fear and protectiveness (mental event) and so on.

Non-reductive physicalism is the idea that while mental states are physical they are not reducible to physical properties, in that an ontological distinction lies in the differences between the properties of mind and matter. According to non-reductive physicalism all mental states are causally reducible to physical states where mental properties map to physical properties and vice versa. A prominent form of non-reductive physicalism called anomalous monism was first proposed by Donald Davidson in his 1970 paper "Mental events", where it is claimed that mental events are identical with physical events, and that the mental is anomalous, i.e. under their mental descriptions these mental events are not regulated by strict physical laws.

Epiphenomenalism states that all mental events are caused by a physical event and have no physical consequences, and that one or more mental states do not have any influence on physical states. So, the mental event of deciding to pick up a rock (""M1") is caused by the firing of specific neurons in the brain ("P1"). When the arm and hand move to pick up the rock ("P2"") this is not caused by the preceding mental event "M1", nor by "M1" and "P1" together, but only by "P1". The physical causes are in principle reducible to fundamental physics, and therefore mental causes are eliminated using this reductionist explanation. If P1 causes both "M1" and "P2", there is no overdetermination in the explanation for "P2".

The idea that even if the animal were conscious nothing would be added to the production of behavior, even in animals of the human type, was first voiced by La Mettrie (1745), and then by Cabanis (1802), and was further explicated by Hodgson (1870) and Huxley (1874). Jackson gave a subjective argument for epiphenomenalism, but later refuted it and embraced physicalism.

Psycho-physical parallelism is a very unusual view about the interaction between mental and physical events which was most prominently, and perhaps "only" truly, advocated by Gottfried Wilhelm von Leibniz. Like Malebranche and others before him, Leibniz recognized the weaknesses of Descartes' account of causal interaction taking place in a physical location in the brain. Malebranche decided that such a material basis of interaction between material and immaterial was impossible and therefore formulated his doctrine of occasionalism, stating that the interactions were really caused by the intervention of God on each individual occasion. Leibniz's idea is that God has created a pre-established harmony such that it only seems "as if" physical and mental events cause, and are caused by, one another. In reality, mental causes only have mental effects and physical causes only have physical effects. Hence the term "parallelism" is used to describe this view.

Occasionalism is a philosophical doctrine about causation which says that created substances cannot be efficient causes of events. Instead, all events are taken to be caused directly by God itself. The theory states that the illusion of efficient causation between mundane events arises out of a constant conjunction that God had instituted, such that every instance where the cause is present will constitute an "occasion" for the effect to occur as an expression of the aforementioned power. This "occasioning" relation, however, falls short of efficient causation. In this view, it is not the case that the first event causes God to cause the second event: rather, God first caused one and then caused the other, but chose to regulate such behaviour in accordance with general laws of nature. Some of its most prominent historical exponents have been Al-Ghazali, Louis de la Forge, Arnold Geulincx, and Nicholas Malebranche.

According to Immanuel Kant's philosophy, there is a distinction between actions done by desire and the ones performed by liberty (categorical imperative). Thus, not all physical actions are caused by either matter or freedom. Some actions are purely animal in nature, while others are the result of mental action on matter.

In the dialogue "Phaedo", Plato formulated his famous Theory of Forms as distinct and immaterial substances of which the objects and other phenomena that we perceive in the world are nothing more than mere shadows.

Plato makes it clear, in the "Phaedo", that the Forms are the "universalia ante res", i.e. they are ideal universals, by which we are able to understand the world. In his allegory of the cave Plato likens the achievement of philosophical understanding to emerging into the sun from a dark cave, where only vague shadows of what lies beyond that prison are cast dimly upon the wall. Plato's forms are non-physical and non-mental. They exist nowhere in time or space, but neither do they exist in the mind, nor in the pleroma of matter; rather, matter is said to "participate" in form (μεθεξις "methexis"). It remained unclear however, even to Aristotle, exactly what Plato intended by that.

Aristotle argued at length against many aspects of Plato's forms, creating his own doctrine of hylomorphism wherein form and matter coexist. Ultimately however, Aristotle's aim was to perfect a theory of forms, rather than to reject it. Although Aristotle strongly rejected the independent existence Plato attributed to forms, his metaphysics do agree with Plato's "a priori" considerations quite often. For example, Aristotle argues that changeless, eternal substantial form is necessarily immaterial. Because matter provides a stable substratum for a change in form, matter always has the potential to change. Thus, if given an eternity in which to do so, it "will", necessarily, exercise that potential.

Part of Aristotle's "psychology", the study of the soul, is his account of the ability of humans to reason and the ability of animals to perceive. In both cases, perfect copies of forms are acquired, either by direct impression of environmental forms, in the case of perception, or else by virtue of contemplation, understanding and recollection. He believed the mind can literally assume any form being contemplated or experienced, and it was unique in its ability to become a blank slate, having no essential form. As thoughts of earth are not heavy, any more than thoughts of fire are causally efficient, they provide an immaterial complement for the formless mind.

The philosophical school of Neoplatonism, most active in Late Antiquity, claimed that the physical and the spiritual are both emanations of "the One". Neoplatonism exerted a considerable influence on Christianity, as did the philosophy of Aristotle via scholasticism.

In the scholastic tradition of Saint Thomas Aquinas, a number of whose doctrines have been incorporated into Roman Catholic dogma, the soul is the substantial form of a human being. Aquinas held the "Quaestiones disputate de anima", or "Disputed questions on the soul", at the Roman "studium provinciale" of the Dominican Order at Santa Sabina, the forerunner of the Pontifical University of Saint Thomas Aquinas, "Angelicum" during the academic year 1265–66. By 1268 Aquinas had written at least the first book of the "Sententia Libri De anima", Aquinas' commentary on Aristotle's "De anima", the translation of which from the Greek was completed by Aquinas' Dominican associate at Viterbo William of Moerbeke in 1267. Like Aristotle, Aquinas held that the human being was a unified composite substance of two substantial principles: form and matter. The soul is the substantial form and so the first actuality of a material organic body with the potentiality for life. While Aquinas defended the unity of human nature as a composite substance constituted by these two inextricable principles of form and matter, he also argued for the incorruptibility of the intellectual soul, in contrast to the corruptibility of the vegetative and sensitive animation of plants and animals. His argument for the subsistence and incorruptibility of the intellectual soul takes its point of departure from the metaphysical principle that operation follows upon being ("agiture sequitur esse"), i.e., the activity of a thing reveals the mode of being and existence it depends upon. Since the intellectual soul exercises its own "per se" intellectual operations without employing material faculties, i.e. intellectual operations are immaterial, the intellect itself and the intellectual soul, must likewise be immaterial and so incorruptible. Even though the intellectual soul of man is able to subsist upon the death of the human being, Aquinas does not hold that the human person is able to remain integrated at death. The separated intellectual soul is neither a man nor a human person. The intellectual soul "by itself" is "not" a human person (i.e., an individual "supposit" of a rational nature). Hence, Aquinas held that "soul of St. Peter pray for us" would be more appropriate than "St. Peter pray for us", because all things connected with his person, including memories, ended with his corporeal life.

The Catholic doctrine of the resurrection of the body does nor subscribe that, sees body and soul as forming a whole and states that at the second coming, the souls of the departed will be reunited with their bodies as a whole person (substance) and witness to the apocalypse. The thorough consistency between dogma and contemporary science was maintained here in part from a serious attendance to the principle that there can be only one truth. Consistency with science, logic, philosophy, and faith remained a high priority for centuries, and a university doctorate in theology generally included the entire science curriculum as a prerequisite. This doctrine is not universally accepted by Christians today. Many believe that one's immortal soul goes directly to Heaven upon death of the body.

In his "Meditations on First Philosophy", Descartes embarked upon a quest in which he called all his previous beliefs into doubt, in order to find out what he could be certain of. In so doing, he discovered that he could doubt whether he had a body (it could be that he was dreaming of it or that it was an illusion created by an evil demon), but he could not doubt whether he had a mind. This gave Descartes his first inkling that the mind and body were different things. The mind, according to Descartes, was a "thinking thing" (), and an immaterial substance. This "thing" was the essence of himself, that which doubts, believes, hopes, and thinks. The body, "the thing that exists" (), regulates normal bodily functions (such as heart and liver). According to Descartes, animals only had a body and not a soul (which distinguishes humans from animals). The distinction between mind and body is argued in "Meditation" VI as follows: I have a clear and distinct idea of myself as a thinking, non-extended thing, and a clear and distinct idea of body as an extended and non-thinking thing. Whatever I can conceive clearly and distinctly, God can so create.

The central claim of what is often called "Cartesian dualism", in honor of Descartes, is that the immaterial mind and the material body, while being ontologically distinct substances, causally interact. This is an idea that continues to feature prominently in many non-European philosophies. Mental events cause physical events, and vice versa. But this leads to a substantial problem for Cartesian dualism: How can an immaterial mind cause anything in a material body, and vice versa? This has often been called the "problem of interactionism."

Descartes himself struggled to come up with a feasible answer to this problem. In his letter to Elisabeth of Bohemia, Princess Palatine, he suggested that spirits interacted with the body through the pineal gland, a small gland in the centre of the brain, between the two hemispheres. The term "Cartesian dualism" is also often associated with this more specific notion of causal interaction through the pineal gland. However, this explanation was not satisfactory: "how" can an immaterial mind interact with the physical pineal gland? Because Descartes' was such a difficult theory to defend, some of his disciples, such as Arnold Geulincx and Nicholas Malebranche, proposed a different explanation: That all mind–body interactions required the direct intervention of God. According to these philosophers, the appropriate states of mind and body were only the "occasions" for such intervention, not real causes. These occasionalists maintained the strong thesis that all causation was directly dependent on God, instead of holding that all causation was natural except for that between mind and body.

In addition to already discussed theories of dualism (particularly the Christian and Cartesian models) there are new theories in the defense of dualism. Naturalistic dualism comes from Australian philosopher, David Chalmers (born 1966) who argues there is an explanatory gap between objective and subjective experience that cannot be bridged by reductionism because consciousness is, at least, logically autonomous of the physical properties upon which it supervenes. According to Chalmers, a naturalistic account of property dualism requires a new fundamental category of properties described by new laws of supervenience; the challenge being analogous to that of understanding electricity based on the mechanistic and Newtonian models of materialism prior to Maxwell's equations.

A similar defense comes from Australian philosopher Frank Jackson (born 1943) who revived the theory of epiphenomenalism which argues that mental states do not play a role in physical states. Jackson argues that there are two kinds of dualism. The first is "substance dualism" that assumes there is second, non-corporeal form of reality. In this form, body and soul are two different substances. The second form is "property dualism" that says that body and soul are different "properties" of the same body. He claims that functions of the mind/soul are internal, very private experiences that are not accessible to observation by others, and therefore not accessible by science (at least not yet). We can know everything, for example, about a bat's facility for echolocation, but we will never know how the bat experiences that phenomenon.

An important fact is that minds perceive intramental states differently from sensory phenomena, and this cognitive difference results in mental and physical phenomena having seemingly disparate properties. The subjective argument holds that these properties are irreconcilable under a physical mind.

Mental events have a certain "subjective" quality to them, whereas physical ones seem not to. So, for example, one may ask what a burned finger feels like, or what the blueness of the sky looks like, or what nice music sounds like. Philosophers of mind call the subjective aspects of mental events "qualia." There is something "that it's like" to feel pain, to see a familiar shade of blue, and so on. There are "qualia" involved in these mental events. And the claim is that qualia cannot be reduced to anything physical.

Thomas Nagel first characterized the problem of qualia for physicalistic monism in his article, "What Is It Like to Be a Bat?". Nagel argued that even if we knew everything there was to know from a third-person, scientific perspective about a bat's sonar system, we still wouldn't know what it is like to "be" a bat. However, others argue that "qualia" are consequent of the same neurological processes that engender the bat's mind, and will be fully understood as the science develops.

Frank Jackson formulated his well-known "knowledge argument" based upon similar considerations. In this thought experiment, known as Mary's room, he asks us to consider a neuroscientist, Mary, who was born, and has lived all of her life, in a black and white room with a black and white television and computer monitor where she collects all the scientific data she possibly can on the nature of colours. Jackson asserts that as soon as Mary leaves the room, she will come to have new knowledge which she did not possess before: the knowledge of the experience of colours (i.e., what they are like). Although Mary knows everything there is to know about colours from an objective, third-person perspective, she has never known, according to Jackson, what it was like to see red, orange, or green. If Mary really learns something new, it must be knowledge of something non-physical, since she already knew everything about the physical aspects of colour.

However, Jackson later rejected his argument and embraced physicalism. He notes that Mary obtains knowledge not of color, but of a new intramental state, "seeing color". Also, he notes that Mary might say "wow," and as a mental state affecting the physical, this clashed with his former view of epiphenomenalism. David Lewis' response to this argument, now known as the "ability" argument, is that what Mary really came to know was simply the ability to recognize and identify color sensations to which she had previously not been exposed. Daniel Dennett and others also provide arguments against this notion (see Objections).

The zombie argument is based on a thought experiment proposed by David Chalmers. The basic idea is that one can imagine, and, therefore, conceive the existence of, an apparently functioning human being/body without any conscious states being associated with it.

Chalmers' argument is that it seems plausible that such a being could exist because all that is needed is that all and only the things that the physical sciences describe and observe about a human being must be true of the zombie. None of the concepts involved in these sciences make reference to consciousness or other mental phenomena, and any physical entity can be described scientifically via physics whether it is conscious or not. The mere logical possibility of a p-zombie demonstrates that consciousness is a natural phenomenon beyond the current unsatisfactory explanations. Chalmers states that one probably could not build a living p-zombie because living things seem to require a level of consciousness. However (unconscious?) robots built to simulate humans may become the first real p-zombies. Hence Chalmers half-joking calls for the need to build a "consciousness meter" to ascertain if any given entity, human or robot, is conscious or not.

Others such as Dennett have argued that the notion of a philosophical zombie is an incoherent, or unlikely, concept. In particular, nothing proves that an entity (e.g., a computer or robot) which would perfectly mimic human beings, and especially perfectly mimic expressions of feelings (like joy, fear, anger, ...), would not indeed experience them, thus having similar states of consciousness to what a real human would have. It is argued that under physicalism, one must either believe that anyone including oneself might be a zombie, or that no one can be a zombie—following from the assertion that one's own conviction about being (or not being) a zombie is a product of the physical world and is therefore no different from anyone else's.

Robinson argues that, if predicate dualism is correct, then there are "special sciences" that are irreducible to physics. These allegedly irreducible subjects, which contain irreducible predicates, differ from hard sciences in that they are interest-relative. Here, interest-relative fields depend on the existence of minds that can have interested perspectives. Psychology is one such science; it completely depends on and presupposes the existence of the mind.

Physics is the general analysis of nature, conducted in order to understand how the universe behaves. On the other hand, the study of meteorological weather patterns or human behavior is only of interest to humans themselves. The point is that having a perspective on the world is a psychological state. Therefore, the special sciences presuppose the existence of minds which can have these states. If one is to avoid ontological dualism, then the mind that "has" a perspective must be part of the physical reality to which it "applies" its perspective. If this is the case, then in order to perceive the physical world as psychological, the mind must have a perspective on the physical. This, in turn, presupposes the existence of mind.

However, cognitive science and psychology do not require the mind to be irreducible, and operate on the assumption that it has physical basis. In fact, it is common in science to presuppose a complex system; while fields such as chemistry, biology, or geology could be verbosely expressed in terms of quantum field theory, it is convenient to use levels of abstraction like molecules, cells, or the mantle. It is often difficult to decompose these levels without heavy analysis and computation. Sober has also advanced philosophical arguments against the notion of irreducibility.

This argument concerns the differences between the applicability of counterfactual conditionals to physical objects, on the one hand, and to conscious, personal agents on the other. In the case of any material object, e.g. a printer, we can formulate a series of counterfactuals in the following manner:


Somewhere along the way from the printer's being made up exactly of the parts and materials which actually constitute it to the printer's being made up of some different matter at, say, 20%, the question of whether this printer is the same printer becomes a matter of arbitrary convention.

Imagine the case of a person, Frederick, who has a counterpart born from the same egg and a slightly genetically modified sperm. Imagine a series of counterfactual cases corresponding to the examples applied to the printer. Somewhere along the way, one is no longer sure about the identity of Frederick. In this latter case, it has been claimed, "overlap of constitution" cannot be applied to the identity of mind. As Madell puts it:

If the counterpart of Frederick, Frederickus, is 70% constituted of the same physical substance as Frederick, does this mean that it is also 70% mentally identical with Frederick? Does it make sense to say that something is mentally 70% Frederick? A possible solution to this dilemma is that of open individualism.

Richard Swinburne, in his book "The Existence of God", put forward an argument for mind-body dualism based upon personal identity. He states that the brain is composed of two hemispheres and a cord linking the two and that, as modern science has shown, either of these can be removed without the person losing any memories or mental capacities.

He then cites a thought-experiment for the reader, asking what would happen if each of the two hemispheres of one person were placed inside two different people. Either, Swinburne claims, one of the two is me or neither is- and there is no way of telling which, as each will have similar memories and mental capacities to the other. In fact, Swinburne claims, even if one's mental capacities and memories are far more similar to the original person than the others' are, they still may not be him.

From here, he deduces that even if we know what has happened to every single atom inside a person's brain, we still do not know what has happened to 'them' as an identity. From here it follows that a part of our mind, or our soul, is immaterial, and, as a consequence, that mind-body dualism is true.

Philosophers and scientists such as Victor Reppert, William Hasker, and Alvin Plantinga have developed an argument for dualism dubbed the "argument from reason". They credit C.S. Lewis with first bringing the argument to light in his book "Miracles"; Lewis called the argument "The Cardinal Difficulty of Naturalism", which was the title of chapter three of "Miracles".

The argument postulates that if, as naturalism entails, all of our thoughts are the effect of a physical cause, then we have no reason for assuming that they are also the consequent of a reasonable ground. However, knowledge is apprehended by reasoning from ground to consequent. Therefore, if naturalism were true, there would be no way of knowing it (or anything else), except by a fluke.

Through this logic, the statement "I have reason to believe naturalism is valid" is inconsistent in the same manner as "I never tell the truth." That is, to conclude its truth would eliminate the grounds from which to reach it. To summarize the argument in the book, Lewis quotes J. B. S. Haldane, who appeals to a similar line of reasoning:

In his essay "Is Theology Poetry?", Lewis himself summarises the argument in a similar fashion when he writes:

But Lewis later agreed with Elizabeth Anscombe's response to his "Miracles" argument. She showed that an argument could be valid and ground-consequent even if its propositions were generated via physical cause and effect by non-rational factors. Similar to Anscombe, Richard Carrier and John Beversluis have written extensive objections to the argument from reason on the untenability of its first postulate.

Descartes puts forward two main arguments for dualism in "Meditations": firstly, the "modal argument", or the "clear and distinct perception argument", and secondly the "indivisibility" or "divisibility" argument. 

The "modal argument" can be summarised as follows:
therefore
therefore
therefore

The argument is distinguished from the zombie argument because it establishes that the mind could continue to exist without the body, rather than that the unaltered body could exist without the mind. 

Alvin Plantinga, J. P. Moreland and Edward Feser have both supported the argument, although Feser and Moreland think that it must be carefully reformulated in order to be effective. 

The "indivisibility argument" for dualism was phrased by Descartes as follows: "there is a great difference between a mind and a body, because the body, by its very nature, is something divisible, whereas the mind is plainly indivisible. . . insofar as I am only a thing that thinks, I cannot distinguish any parts in me. . . . Although the whole mind seems to be united to the whole body, nevertheless, were a foot or an arm or any other bodily part amputated, I know that nothing would be taken away from the mind. . ."

The argument relies upon Leibniz' principle of the identity of indiscernibles, which states that two things are the same if and only if they share all their properties. 

A counterargument is the idea that matter is not infinitely divisible, and thus that the mind could be identified with material things that cannot be divided, or potentially Leibnizian monads.

One argument against Dualism is with regard to causal interaction. If consciousness (the mind) can exist independently of physical reality (the brain), one must explain how physical memories are created concerning consciousness. Dualism must therefore explain how consciousness affects physical reality. One of the main objections to dualistic interactionism is lack of explanation of how the material and immaterial are able to interact. Varieties of dualism according to which an immaterial mind causally affects the material body and vice versa have come under strenuous attack from different quarters, especially in the 20th century. Critics of dualism have often asked how something totally immaterial can affect something totally material—this is the basic "problem of causal interaction".

First, it is not clear "where" the interaction would take place. For example, burning one's finger causes pain. Apparently there is some chain of events, leading from the burning of skin, to the stimulation of nerve endings, to something happening in the peripheral nerves of one's body that lead to one's brain, to something happening in a particular part of one's brain, and finally resulting in the sensation of pain. But pain is not supposed to be spatially locatable. It might be responded that the pain "takes place in the brain." But evidently, the pain is in the finger. This may not be a devastating criticism.

However, there is a second problem about the interaction. Namely, the question of "how" the interaction takes place, where in dualism "the mind" is assumed to be non-physical and by definition outside of the realm of science. The "mechanism" which explains the connection between the mental and the physical would therefore be a philosophical proposition as compared to a scientific theory. For example, compare such a mechanism to a physical mechanism that "is" well understood. Take a very simple causal relation, such as when a cue ball strikes an eight ball and causes it to go into the pocket. What happens in this case is that the cue ball has a certain amount of momentum as its mass moves across the pool table with a certain velocity, and then that momentum is transferred to the eight ball, which then heads toward the pocket. Compare this to the situation in the brain, where one wants to say that a decision causes some neurons to fire and thus causes a body to move across the room. The intention to "cross the room now" is a mental event and, as such, it does not have physical properties such as force. If it has no force, then it would seem that it could not possibly cause any neuron to fire. However, with Dualism, an explanation is required of how something without any physical properties has physical "effects".

Alfred North Whitehead and, later, David Ray Griffin framed a new ontology (process philosophy) seeking precisely to avoid the pitfalls of ontological dualism.

The explanation given by Arnold Geulincx and Nicholas Malebranche is that of occasionalism, where all mind–body interactions require the direct intervention of God.

At the time C. S. Lewis wrote "Miracles", quantum mechanics (and physical indeterminism) was only in the initial stages of acceptance, but still Lewis stated the logical possibility that, if the physical world was proved to be indeterministic, this would provide an entry (interaction) point into the traditionally viewed closed system, where a scientifically described physically probable/improbable event could be philosophically described as an action of a non-physical entity on physical reality. He states, however, that none of the arguments in his book will rely on this. Although some interpretations of quantum mechanics consider wave function collapse to be indeterminate, in others this event is defined and deterministic.

The argument from physics is closely related to the argument from causal interaction. Many physicists and consciousness researchers have argued that any action of a nonphysical mind on the brain would entail the violation of physical laws, such as the conservation of energy.

By assuming a deterministic physical universe, the objection can be formulated more precisely. When a person decides to walk across a room, it is generally understood that the decision to do so, a mental event, immediately causes a group of neurons in that person's brain to fire, a physical event, which ultimately results in his walking across the room. The problem is that if there is something totally nonphysical "causing" a bunch of neurons to fire, then there is no "physical" event which causes the firing. This means that some physical energy is required to be generated against the physical laws of the deterministic universe—this is by definition a miracle and there can be no scientific explanation of (repeatable experiment performed regarding) where the "physical" energy for the firing came from. Such interactions would violate the fundamental laws of physics. In particular, if some external source of energy is responsible for the interactions, then this would violate the law of the conservation of energy. Dualistic interactionism has therefore been criticized for violating a general heuristic principle of science: the causal closure of the physical world.

The "Stanford Encyclopedia of Philosophy" and the "New Catholic Encyclopedia" give two possible replies to the above objections. The first reply is that the mind may influence the "distribution" of energy, without altering its quantity. The second possibility is to deny that the human body is causally closed, as the conservation of energy applies only to closed systems. However, physicalists object that no evidence exists for the causal non-closure of the human body. Robin Collins responds that energy conservation objections misunderstand the role of energy conservation in physics. Well understood scenarios in general relativity violate energy conservation and quantum mechanics provides precedent for causal interactions, or correlation without energy or momentum exchange. However, this does not mean the mind spends energy and, despite that, it still doesn't exclude the supernatural.

Another reply is akin to parallelism—Mills holds that behavioral events are causally overdetermined, and can be explained by either physical or mental causes alone. An overdetermined event is fully accounted for by multiple causes at once. However, J. J. C. Smart and Paul Churchland have pointed out that if physical phenomena fully determine behavioral events, then by Occam's razor an unphysical mind is unnecessary.

Robinson suggests that the interaction may involve dark energy, dark matter or some other currently unknown scientific process. However, such processes would necessarily be physical, and in this case dualism is replaced with physicalism, or the interaction point is left for study at a later time when these physical processes are understood.

Another reply is that the interaction taking place in the human body may not be described by "billiard ball" classical mechanics. If a nondeterministic interpretation of quantum mechanics is correct then microscopic events are indeterminate, where the degree of determinism increases with the scale of the system (see Quantum decoherence). Philosophers Karl Popper and John Eccles and physicist Henry Stapp have theorized that such indeterminacy may apply at the macroscopic scale. However, Max Tegmark has argued that classical and quantum calculations show that quantum decoherence effects do not play a role in brain activity. Indeed, macroscopic quantum states have only ever been observed in superconductors near absolute zero.

Yet another reply to the interaction problem is to note that it doesn't seem that there is an interaction problem for all forms of substance dualism. For instance, Thomistic dualism (the dualism of Thomas Aquinas) doesn't obviously face any issue with regards to interaction.

This argument has been formulated by Paul Churchland, among others. The point is that, in instances of some sort of brain damage (e.g. caused by automobile accidents, drug abuse, pathological diseases, etc.), it is always the case that the mental substance and/or properties of the person are significantly changed or compromised. If the mind were a completely separate substance from the brain, how could it be possible that every single time the brain is injured, the mind is also injured? Indeed, it is very frequently the case that one can even predict and explain the kind of mental or psychological deterioration or change that human beings will undergo when specific parts of their brains are damaged. So the question for the dualist to try to confront is how can all of this be explained if the mind is a separate and immaterial substance from, or if its properties are ontologically independent of, the brain.

Property dualism and William Hasker's "emergent dualism" seek to avoid this problem. They assert that the mind is a property or substance that emerges from the appropriate arrangement of physical matter, and therefore could be affected by any rearrangement of matter.

Phineas Gage, who suffered destruction of one or both frontal lobes by a projectile iron rod, is often cited as an example illustrating that the brain causes mind. Gage certainly exhibited some mental changes after his accident. This physical event, the destruction of part of his brain, therefore caused some kind of change in his mind, suggesting a correlation between brain states and mental states. Similar examples abound; neuroscientist David Eagleman describes the case of another individual who exhibited escalating pedophilic tendencies at two different times, and in each case was found to have tumors growing in a particular part of his brain.

Case studies aside, modern experiments have demonstrated that the relation between brain and mind is much more than simple correlation. By damaging, or manipulating, specific areas of the brain repeatedly under controlled conditions (e.g. in monkeys) and reliably obtaining the same results in measures of mental state and abilities, neuroscientists have shown that the relation between damage to the brain and mental deterioration is likely causal. This conclusion is further supported by data from the effects of neuro-active chemicals (such as those affecting neurotransmitters) on mental functions, but also from research on neurostimulation (direct electrical stimulation of the brain, including transcranial magnetic stimulation).

Another common argument against dualism consists in the idea that since human beings (both phylogenetically and ontogenetically) begin their existence as entirely physical or material entities and since nothing outside of the domain of the physical is added later on in the course of development, then we must necessarily end up being fully developed material beings. There is nothing non-material or "mentalistic" involved in conception, the formation of the blastula, the gastrula, and so on. The postulation of a non-physical mind would seem superfluous.

In some contexts, the decisions that a person makes can be detected up to 10 seconds in advance by means of scanning their brain activity. Subjective experiences and covert attitudes can be detected, as can mental imagery. This is strong empirical evidence that cognitive processes have a physical basis in the brain.

The argument from simplicity is probably the simplest and also the most common form of argument against dualism of the mental. The dualist is always faced with the question of why anyone should find it necessary to believe in the existence of two, ontologically distinct, entities (mind and brain), when it seems possible and would make for a simpler thesis to test against scientific evidence, to explain the same events and properties in terms of one. It is a heuristic principle in science and philosophy not to assume the existence of more entities than is necessary for clear explanation and prediction (see Occam's razor).

This argument was criticized by Peter Glassen in a debate with J. J. C. Smart in the pages of "Philosophy" in the late 1970s and early 1980s. Glassen argued that, because it is not a physical entity, Occam's Razor cannot consistently be appealed to by a physicalist or materialist as a justification of mental states or events, such as the belief that dualism is false. The idea is that Occam's razor may not be as "unrestricted" as it is normally described (applying to all qualitative postulates, even abstract ones) but instead concrete (only applies to physical objects). If one applies Occam's Razor unrestrictedly, then it recommends monism until pluralism either receives more support or is disproved. If one applies Occam's Razor only concretely, then it may not be used on abstract concepts (this route, however, has serious consequences for selecting between hypotheses "about" the abstract).





</doc>
<doc id="7344" url="https://en.wikipedia.org/wiki?curid=7344" title="Cogito, ergo sum">
Cogito, ergo sum

This proposition became a fundamental element of Western philosophy, as it purported to form a secure foundation for knowledge in the face of radical doubt. While other knowledge could be a figment of imagination, deception, or mistake, Descartes asserted that the very act of doubting one's own existence served—at minimum—as proof of the reality of one's own mind; there must be a thinking entity—in this case the self—for there to be a thought.

The critique against the proposition is the presupposition of an "I" doing the thinking, so that the most Descartes was entitled to say was: "thinking is occurring".

Descartes first wrote the phrase in French in his 1637 "Discourse on the Method". He referred to it in Latin without explicitly stating the familiar form of the phrase in his 1641 "Meditations on First Philosophy". The earliest written record of the phrase in Latin is in his 1644 "Principles of Philosophy", where, in a margin note (see below), he provides a clear explanation of his intent: "[W]e cannot doubt of our existence while we doubt". Fuller forms of the phrase are attributable to other authors.

The phrase first appeared (in French) in Descartes's 1637 "Discourse on the Method" in the first paragraph of its fourth part:

In 1641, Descartes published (in Latin) "Meditations on first philosophy" in which he referred to the proposition, though not explicitly as "cogito, ergo sum" in Meditation II:

In 1644, Descartes published (in Latin) his "Principles of Philosophy" where the phrase "ego cogito, ergo sum" appears in Part 1, article 7:

Descartes's margin note for the above paragraph is:

Descartes, in a lesser-known posthumously published work dated as written ca. 1647 and titled ("The Search for Truth by Natural Light"), wrote:

The proposition is sometimes given as . This fuller form was penned by the eloquent French literary critic, Antoine Léonard Thomas, in an award-winning 1765 essay in praise of Descartes, where it appeared as In English, this is "Since I doubt, I think; since I think, I exist"; with rearrangement and compaction, "I doubt, therefore I think, therefore I am", or in Latin, "dubito, ergo cogito, ergo sum".

A further expansion, ("…—a thinking thing") extends the "cogito" with Descartes's statement in the subsequent Meditation, , or, in English, "I am a thinking (conscious) thing, that is, a being who doubts, affirms, denies, knows a few objects, and is ignorant of many …". This has been referred to as "the expanded "cogito"".

Neither nor indicate whether the verb form corresponds to the English simple present or progressive aspect. Translation needs a larger context to determine aspect.

Following John Lyons (1982), Vladimir Žegarac notes, "The temptation to use the simple present is said to arise from the lack of progressive forms in Latin and French, and from a misinterpretation of the meaning of "cogito" as habitual or generic." (Cf. gnomic aspect.) Ann Banfield writes (also following Lyons), "In order for the statement on which Descartes's argument depends to represent certain knowledge, … its tense must be a true present—in English, a progressive, … not as 'I think' but as 'I am thinking, in conformity with the general translation of the Latin or French present tense in such nongeneric, nonstative contexts." Or in the words of Simon Blackburn, "Descartes’s premise is not ‘I think’ in the sense of ‘I ski’, which can be true even if you are not at the moment skiing. It is supposed to be parallel to ‘I am skiing’."

Fumitaka Suzuki (2012) writes "Taking consideration of Cartesian theory of continuous creation, which theory was developed especially in the Meditations and in the Principles, we would assure that 'I am thinking, therefore I am/exist' is the most appropriate English translation of 'ego cogito, ergo sum'."

The similar translation “I am thinking, therefore I exist” of Descartes's correspondence in French (“, ”) appears in "The Philosophical Writings of Descartes" by Cottingham et al. (1988).

The earliest known translation as "I am thinking, therefore I am" is from 1872 by Charles Porterfield Krauth.

As put succinctly by Krauth (1872), "That cannot doubt which does not think, and that cannot think which does not exist. I doubt, I think, I exist."

The phrase "cogito, ergo sum" is not used in Descartes's "Meditations on First Philosophy" but the term "the "cogito"" is used to refer to an argument from it. In the "Meditations", Descartes phrases the conclusion of the argument as "that the proposition, "I am, I exist," is necessarily true whenever it is put forward by me or conceived in my mind." ("Meditation" II)

At the beginning of the second meditation, having reached what he considers to be the ultimate level of doubt—his argument from the existence of a deceiving god—Descartes examines his beliefs to see if any have survived the doubt. In his belief in his own existence, he finds that it is impossible to doubt that he exists. Even if there were a deceiving god (or an evil demon), one's belief in their own existence would be secure, for there is no way one could be deceived unless one existed in order to be deceived.

But I have convinced myself that there is absolutely nothing in the world, no sky, no earth, no minds, no bodies. Does it now follow that I, too, do not exist? No. If I convinced myself of something [or thought anything at all], then I certainly existed. But there is a deceiver of supreme power and cunning who deliberately and constantly deceives me. In that case, I, too, undoubtedly exist, if he deceives me; and let him deceive me as much as he can, he will never bring it about that I am nothing, so long as I think that I am something. So, after considering everything very thoroughly, I must finally conclude that the proposition, "I am, I exist," is necessarily true whenever it is put forward by me or conceived in my mind. (AT VII 25; CSM II 16–17)

There are three important notes to keep in mind here. First, he claims only the certainty of "his own" existence from the first-person point of view — he has not proved the existence of other minds at this point. This is something that has to be thought through by each of us for ourselves, as we follow the course of the meditations. Second, he does not say that his existence is necessary; he says that "if he thinks", then necessarily he exists (see the instantiation principle). Third, this proposition "I am, I exist" is held true not based on a deduction (as mentioned above) or on empirical induction but on the clarity and self-evidence of the proposition. Descartes does not use this first certainty, the "cogito", as a foundation upon which to build further knowledge; rather, it is the firm ground upon which he can stand as he works to discover further truths. As he puts it:

Archimedes used to demand just one firm and immovable point in order to shift the entire earth; so I too can hope for great things if I manage to find just one thing, however slight, that is certain and unshakable. (AT VII 24; CSM II 16)

According to many Descartes specialists, including Étienne Gilson, the goal of Descartes in establishing this first truth is to demonstrate the capacity of his criterion — the immediate clarity and distinctiveness of self-evident propositions — to establish true and justified propositions despite having adopted a method of generalized doubt. As a consequence of this demonstration, Descartes considers science and mathematics to be justified to the extent that their proposals are established on a similarly immediate clarity, distinctiveness, and self-evidence that presents itself to the mind. The originality of Descartes's thinking, therefore, is not so much in expressing the "cogito" — a feat accomplished by other predecessors, as we shall see — but on using the "cogito" as demonstrating the most fundamental epistemological principle, that science and mathematics are justified by relying on clarity, distinctiveness, and self-evidence.
Baruch Spinoza in "Principia philosophiae cartesianae" at its "Prolegomenon" identified "cogito ergo sum" the "ego sum cogitans" (I am a thinking being) as the thinking substance with his ontological interpretation.

Although the idea expressed in "cogito, ergo sum" is widely attributed to Descartes, he was not the first to mention it. Plato spoke about the "knowledge of knowledge" (Greek νόησις νοήσεως "nóesis noéseos") and Aristotle explains the idea in full length:
But if life itself is good and pleasant (...) and if one who sees is conscious that he sees, one who hears that he hears, one who walks that he walks and similarly for all the other human activities there is a faculty that is conscious of their exercise, so that whenever we perceive, we are conscious that we perceive, and whenever we think, we are conscious that we think, and to be conscious that we are perceiving or thinking is to be conscious that we exist... ("Nicomachean Ethics", 1170a25 ff.)

In the late sixth or early fifth century BC, Parmenides is quoted as saying "For to be aware and to be are the same" (B3). Augustine of Hippo in "De Civitate Dei" writes "Si […] fallor, sum" ("If I am mistaken, I am") (book XI, 26), and also anticipates modern refutations of the concept. Furthermore, in the "Enchiridion" Augustine attempts to refute skepticism by stating, "[B]y not positively affirming that they are alive, the skeptics ward off the appearance of error in themselves, yet they do make errors simply by showing themselves alive; one cannot err who is not alive. That we live is therefore not only true, but it is altogether certain as well" (Chapter 7 section 20). In 1640 correspondence, Descartes thanked two colleagues for drawing his attention to Augustine and notes similarity and difference. (See CSMK III 159, 161.)

Another predecessor was Avicenna's "Floating Man" thought experiment on human self-awareness and self-consciousness.

The 8th century Hindu philosopher Adi Shankara wrote in a similar fashion, No one thinks, 'I am not', arguing that one's existence cannot be doubted, as there must be someone there to doubt. The central idea of "cogito, ergo sum" is also the topic of Mandukya Upanishad.

Spanish philosopher Gómez Pereira in his 1554 work "De Inmortalitate Animae", published in 1749, wrote "nosco me aliquid noscere, & quidquid noscit, est, ergo ego sum" ("I know that I know something, anyone who knows exists, then I exist").

In "Descartes, The Project of Pure Enquiry", Bernard Williams provides a history and full evaluation of this issue. The first to raise the "I" problem was Pierre Gassendi. He "points out that recognition that one has a set of thoughts does not imply that one is a particular thinker or another. Were we to move from the observation that there is thinking occurring to the attribution of this thinking to a particular agent, we would simply assume what we set out to prove, namely, that there exists a particular person endowed with the capacity for thought". In other words, "the only claim that is indubitable here is the agent-independent claim that there is cognitive activity present". The objection, as presented by Georg Lichtenberg, is that rather than supposing an entity that is thinking, Descartes should have said: "thinking is occurring." That is, whatever the force of the "cogito", Descartes draws too much from it; the existence of a thinking thing, the reference of the "I," is more than the "cogito" can justify. Friedrich Nietzsche criticized the phrase in that it presupposes that there is an "I", that there is such an activity as "thinking", and that "I" know what "thinking" is. He suggested a more appropriate phrase would be "it thinks" wherein the "it" could be an impersonal subject as in the sentence "It is raining."

The Danish philosopher Søren Kierkegaard calls the phrase a tautology in his "Concluding Unscientific Postscript". He argues that the "cogito" already presupposes the existence of "I", and therefore concluding with existence is logically trivial. Kierkegaard's argument can be made clearer if one extracts the premise "I think" into the premises "'x' thinks" and "I am that 'x'", where "x" is used as a placeholder in order to disambiguate the "I" from the thinking thing.

Here, the "cogito" has already assumed the "I"'s existence as that which thinks. For Kierkegaard, Descartes is merely "developing the content of a concept", namely that the "I", which already exists, thinks. As Kierkegaard argues, the proper logical flow of argument is that existence is already assumed or presupposed in order for thinking to occur, not that existence is concluded from that thinking.

Bernard Williams claims that what we are dealing with when we talk of thought, or when we say "I am thinking," is something conceivable from a third-person perspective; namely objective "thought-events" in the former case, and an objective thinker in the latter. He argues, first, that it is impossible to make sense of "there is thinking" without relativizing it to "something." However, this something cannot be Cartesian egos, because it is impossible to differentiate objectively between things just on the basis of the pure content of consciousness. The obvious problem is that, through introspection, or our experience of consciousness, we have no way of moving to conclude the existence of any third-personal fact, to conceive of which would require something above and beyond just the purely subjective contents of the mind.

As a critic of Cartesian subjectivity, Heidegger sought to ground human subjectivity in death as that certainty which individualizes and authenticates our being. As he wrote in 1927:

"This certainty, that "I myself am in that I will die," is the basic certainty of Dasein itself. It is a genuine statement of Dasein, while "cogito sum" is only the semblance of such a statement. If such pointed formulations mean anything at all, then the appropriate statement pertaining to Dasein in its being would have to be "sum moribundus" [I am in dying], "moribundus" not as someone gravely ill or wounded, but insofar as I am, I am "moribundus". The "MORIBUNDUS" first gives the "SUM" its sense."

The Scottish philosopher John Macmurray rejects the "cogito" outright in order to place action at the center of a philosophical system he entitles the Form of the Personal. "We must reject this, both as standpoint and as method. If this be philosophy, then philosophy is a bubble floating in an atmosphere of unreality." The reliance on thought creates an irreconcilable dualism between thought and action in which the unity of experience is lost, thus dissolving the integrity of our selves, and destroying any connection with reality. In order to formulate a more adequate "cogito", Macmurray proposes the substitution of "I do" for "I think", ultimately leading to a belief in God as an agent to whom all persons stand in relation.




</doc>
<doc id="430976" url="https://en.wikipedia.org/wiki?curid=430976" title="Uncanny valley">
Uncanny valley

In aesthetics, the uncanny valley is a hypothesized relationship between the degree of an object's resemblance to a human being and the emotional response to such an object. The concept of the uncanny valley suggests that humanoid objects which imperfectly resemble actual human beings provoke uncanny or strangely familiar feelings of eeriness and revulsion in observers. "Valley" denotes a dip in the human observer's affinity for the replica, a relation that otherwise increases with the replica's human likeness.

Examples can be found in robotics, 3D computer animations, and lifelike dolls among others. With the increasing prevalence of virtual reality, augmented reality, and photorealistic computer animation, the "valley" has been cited in the popular press in reaction to the verisimilitude of the creation as it approaches indistinguishability from reality. The uncanny valley hypothesis predicts that an entity appearing almost human will risk eliciting cold, eerie feelings in viewers.

The concept was identified by the robotics professor Masahiro Mori as "bukimi no tani genshō" () in 1970. The term was first translated as "uncanny valley" in the 1978 book "Robots: Fact, Fiction, and Prediction", written by Jasia Reichardt, thus forging an unintended link to Ernst Jentsch's concept of the "uncanny", introduced in a 1906 essay entitled "On the Psychology of the Uncanny". Jentsch's conception was elaborated by Sigmund Freud in a 1919 essay entitled "The Uncanny" ("Das Unheimliche").

Mori's original hypothesis states that as the appearance of a robot is made more human, some observers' emotional response to the robot becomes increasingly positive and empathetic, until it reaches a point beyond which the response quickly becomes strong revulsion. However, as the robot's appearance continues to become less distinguishable from a human being, the emotional response becomes positive once again and approaches human-to-human empathy levels.

This area of repulsive response aroused by a robot with appearance and motion between a "barely human" and "fully human" entity is the uncanny valley. The name captures the idea that an almost human-looking robot seems overly "strange" to some human beings, produces a feeling of uncanniness, and thus fails to evoke the empathic response required for productive human–robot interaction.

A number of theories have been proposed to explain the cognitive mechanism underlying the phenomenon:

A series of studies experimentally investigated whether uncanny valley effects exist for static images of robot faces. Mathur MB & Reichling DB used two complementary sets of stimuli spanning the range from very mechanical to very human-like: first, a sample of 80 objectively chosen robot face images from Internet searches, and second, a morphometrically and graphically controlled 6-face series set of faces. They asked subjects to explicitly rate the likability of each face. To measure trust toward each face, subjects completed a one-shot investment game to indirectly measure how much money they were willing to "wager" on a robot's trustworthiness. Both stimulus sets showed a robust uncanny valley effect on explicitly-rated likability and a more context-dependent uncanny valley on implicitly-rated trust. Their exploratory analysis of one proposed mechanism for the uncanny valley, perceptual confusion at a category boundary, found that category confusion occurs in the uncanny valley but does not mediate the effect on social and emotional responses.

One study conducted in 2009 examined the evolutionary mechanism behind the aversion associated with the uncanny valley. A group of five monkeys were shown three images: two different 3D monkey faces (realistic, unrealistic), and a real photo of a monkey's face. The monkeys' eye-gaze was used as a proxy for preference or aversion. Since the realistic 3D monkey face was looked at less than either the real photo, or the unrealistic 3D monkey face, this was interpreted as an indication that the monkey participants found the realistic 3D face aversive, or otherwise preferred the other two images. As one would expect with the uncanny valley, more realism can lead to less positive reactions, and this study demonstrated that neither human-specific cognitive processes, nor human culture explain the uncanny valley. In other words, this aversive reaction to realism can be said to be evolutionary in origin.

As of 2011, researchers at University of California, San Diego and California Institute for Telecommunications and Information Technology are measuring human brain activations related to the uncanny valley. In one study using fMRI, a group of cognitive scientists and roboticists found the biggest differences in brain responses for uncanny robots in parietal cortex, on both sides of the brain, specifically in the areas that connect the part of the brain's visual cortex that processes bodily movements with the section of the motor cortex thought to contain mirror neurons. The researchers say they saw, in essence, evidence of mismatch or perceptual conflict. The brain "lit up" when the human-like appearance of the android and its robotic motion "didn’t compute". Ayşe Pınar Saygın, an assistant professor from UCSD, says "The brain doesn’t seem selectively tuned to either biological appearance or biological motion per se. What it seems to be doing is looking for its expectations to be met – for appearance and motion to be congruent."

Viewer perception of facial expression and speech and the uncanny valley in realistic, human-like characters intended for video games and film is being investigated by Tinwell et al., 2011. Consideration is also given by Tinwell et al. (2010) as to how the uncanny may be exaggerated for antipathetic characters in survival horror games. Building on the body of work already undertaken in android science, this research intends to build a conceptual framework of the uncanny valley using 3D characters generated in a real-time gaming engine. The goal is to analyze how cross-modal factors of facial expression and speech can exaggerate the uncanny. Tinwell et al., 2011 have also introduced the notion of an "unscalable" uncanny wall that suggests that a viewer's discernment for detecting imperfections in realism will keep pace with new technologies in simulating realism. A summary of Angela Tinwell's research on the uncanny valley, psychological reasons behind the uncanny valley and how designers may overcome the uncanny in human-like virtual characters is provided in her book, "The Uncanny Valley in Games and Animation" by CRC Press.

A number of design principles have been proposed for avoiding the uncanny valley:

A number of criticisms have been raised concerning whether the uncanny valley exists as a unified phenomenon amenable to scientific scrutiny:

An effect similar to the uncanny valley was noted by Charles Darwin in 1839:
A similar "uncanny valley" effect could, according to the ethical-futurist writer Jamais Cascio, show up when humans begin modifying themselves with transhuman enhancements (cf. body modification), which aim to improve the abilities of the human body beyond what would normally be possible, be it eyesight, muscle strength, or cognition. So long as these enhancements remain within a perceived norm of human behavior, a negative reaction is unlikely, but once individuals supplant normal human variety, revulsion can be expected. However, according to this theory, once such technologies gain further distance from human norms, "transhuman" individuals would cease to be judged on human levels and instead be regarded as separate entities altogether (this point is what has been dubbed "posthuman"), and it is here that acceptance would rise once again out of the uncanny valley. Another example comes from "pageant retouching" photos, especially of children, which some find disturbingly doll-like.

Due to rapid advancements in the areas of artificial intelligence and affective computing, cognitive scientists have also suggested the possibility of an "Uncanny Valley of Mind". Accordingly, people might experience strong feelings of aversion if they encounter highly advanced, emotion-sensitive technology. Among the possible explanations for this phenomenon, both a perceived loss of human uniqueness and expectations of immediate physical harm are discussed by contemporary research.

A number of films that use computer-generated imagery to show characters have been described by reviewers as giving a feeling of revulsion or "creepiness" as a result of the characters looking too realistic. Examples include the following:


An increasingly common practice is to feature virtual actors in films: CGI likenesses of real actors used because the original actor either looks too old for the part or is deceased. Sometimes a virtual actor is created with involvement from the original actor (who may contribute motion capture, audio, etc.), while at other times the actor has no involvement. Reviewers have often criticized the use of virtual actors for its uncanny valley effect, saying it adds an eerie feeling to the movie. Examples of virtual actors that have received such criticism include replicas of Arnold Schwarzenegger in "Terminator Salvation" (2009), Jeff Bridges in "" (2010), Schwarzenegger again in "Terminator Genisys" (2015), Peter Cushing and Carrie Fisher in "Rogue One" (2016), and Will Smith in "Gemini Man" (2019).

The use of virtual actors is in contrast with digital de-aging, which can involve simply removing wrinkles from actors' faces. This practice has generally not faced uncanny valley criticism. One exception is the 2019 film "The Irishman", in which Robert De Niro, Al Pacino and Joe Pesci were all de-aged to try to make them look up to 50 years younger: one reviewer wrote that the actors' "hunched and stiff" body language stood in marked contrast to their facial appearance, while another wrote that when De Niro's character was in his 30s, he looked like he was 50.

Deepfake software, which first began to be widely used in 2017, uses machine learning to graft one person's appearance onto another's facial expressions, thus providing an alternate approach to both creating virtual actors and digital de-aging. Various individuals have created web videos that use deepfake software to re-create some of the notable previous uses of virtual actors and de-aging in film. Journalists have tended to praise these deepfake imitations, calling them "more naturalistic" and "objectively better" than the originals.

In the 2008 "30 Rock" episode "Succession", Frank Rossitano explains the uncanny valley concept, using a graph and "Star Wars" examples, to try to convince Tracy Jordan that his dream of creating a pornographic video game is impossible. He also references the computer-animated film "The Polar Express."




</doc>
