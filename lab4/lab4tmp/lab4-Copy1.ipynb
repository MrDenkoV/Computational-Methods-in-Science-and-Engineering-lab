{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare a lot of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'AA/'\n",
    "files = [\"wiki_0\"+str(i) for i in range(9)] + [\"wiki_\"+str(i) for i in range(10, 14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep():\n",
    "    count = 0\n",
    "    for file in files:\n",
    "        with open(path+file) as fp: \n",
    "            while True:\n",
    "                markers=0\n",
    "                empty=True\n",
    "                with open('done/page'+str(count)+'.txt', 'a') as the_file:\n",
    "                    while True:\n",
    "                        line = fp.readline() \n",
    "\n",
    "                        if not line:\n",
    "                            break\n",
    "                        if line[0]=='<':\n",
    "                            markers+=1\n",
    "                            if markers==2:\n",
    "                                break\n",
    "                            continue\n",
    "                        if empty and not line.isspace() and 'http' not in line:\n",
    "                            empty = False\n",
    "                        the_file.write(line)\n",
    "                if empty:\n",
    "                    os.remove('done/page'+str(count)+'.txt')\n",
    "                    count-=1\n",
    "                count+=1\n",
    "                if not line:\n",
    "                    break\n",
    "# prep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded a lot of pages from wikipedia xml export.\\\n",
    "Extracted xml to plain text using wiki extractor https://github.com/attardi/wikiextractor .\\\n",
    "Seperated into many files the function prep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare Bag-Of-Words\n",
    "###### In our case union of all words\n",
    "We will use Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install num2words\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import num2words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'done/'\n",
    "files = ['page'+str(i)+'.txt' for i in range(1388)]\n",
    "bagOfWords = dict()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"_\",\"\")\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub('  +',' ',text)\n",
    "    text = re.sub(r'½','.5',text) # who tf uses this character?\n",
    "    text = re.sub(r'¼','.25',text) # who tf uses this character?\n",
    "    \n",
    "    text_list = text.split()\n",
    "    text = \"\"\n",
    "    \n",
    "    for word in text_list:\n",
    "\n",
    "        word = lemmatizer.lemmatize(word, pos=\"v\")\n",
    "\n",
    "#         if word.isnumeric():\n",
    "#             word = num2words.num2words(float(word))\n",
    "        \n",
    "        if word in stop_words: continue\n",
    "\n",
    "        text += word + \" \"\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r'[0-9]','',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare Function - prepares given text for out bag-of-words, reducing words to their basic dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 princesa\n",
      "20000 obviously\n",
      "30000 costcutting\n",
      "40000 sponge\n",
      "50000 wannabe\n",
      "60000 gizona\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "bagOfWords = dict() # slows down really quick, so remember to clear dict\n",
    "for file in files:\n",
    "    with open(path+file) as fp:\n",
    "        for fileline in fp:\n",
    "            line = prepare(fileline)\n",
    "            for word in line.split():\n",
    "                if word not in bagOfWords:\n",
    "                    bagOfWords[word.lower()]=count\n",
    "                    count+=1\n",
    "                    if count % 1e4 == 0:\n",
    "                        print(count, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to rather slow execution, print to check if everything is in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. For each document prepare bag-of-words and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000 coefficients\n",
      "600000 since\n",
      "900000 products\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "wordBags = [dict() for file in files]\n",
    "for ix, file in enumerate(files):\n",
    "    with open(path+file) as fp:\n",
    "        for fileline in fp:\n",
    "#             line = re.sub(\"[^\\w]\", \" \", fileline) \n",
    "            line = prepare(fileline)\n",
    "            for word in line.split():\n",
    "                if word.lower() not in bagOfWords:\n",
    "                    continue\n",
    "                count+=1\n",
    "                if count % 3e5 == 0:\n",
    "                    print(count, word)\n",
    "                if word not in wordBags[ix]:\n",
    "                    wordBags[ix][word.lower()]=1\n",
    "                else:\n",
    "                    wordBags[ix][word.lower()]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print to assure process not hang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create sparse term-by-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sparse.lil_matrix((len(bagOfWords), len(wordBags))) # rows and cols\n",
    "for ix, bag in enumerate(wordBags):\n",
    "    for word in bag:\n",
    "        A[bagOfWords[word], ix] = bag[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A - Matrix with the number of bag-of-words rows and number of documents columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Calulate Inverse Document Frequency and multiply each matrix element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want no amplify the strength of rare words / topic related words appearing in specific documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF=0\n",
    "N=len(files)\n",
    "nw=1\n",
    "counted = A.getnnz(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "japan\n",
      "subterranean\n",
      "argument\n",
      "yasuo\n",
      "singlecelled\n",
      "gangsta\n",
      "manthesingabs\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for word in bagOfWords:\n",
    "    nw=counted[bagOfWords[word]]\n",
    "    IDF = math.log(N/nw)\n",
    "#     A[bagOfWords[word]]*=IDF\n",
    "    A[bagOfWords[word], :]*=IDF\n",
    "    if count%1e4==0:\n",
    "        print(word)\n",
    "    count+=1\n",
    "#     print(nw, IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print to ensure process not hang\\\n",
    "Remember to clear A matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Query\n",
    "Input words and number of closest documents to display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate similarity based on:\n",
    "q - inputed bag-of-words\\\n",
    "$$ cos\\theta_j = \\frac{q^Td_j}{||q||*||d_j||} = \\frac{q^TAe_j}{||q||*||Ae_j||} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Japan and Japanese are super interesting and the best!\"\n",
    "k = 10\n",
    "# query = \"Black people in the Japan\"\n",
    "# k = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66262 1388\n"
     ]
    }
   ],
   "source": [
    "querybag = np.array([0 for i in bagOfWords])\n",
    "text = prepare(query).split()\n",
    "\n",
    "\n",
    "for word in text:\n",
    "    if word not in bagOfWords:\n",
    "        text.remove(word)\n",
    "    querybag[bagOfWords[word]]+=1\n",
    "print(len(bagOfWords), len(wordBags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "[(array([0.50857295]), 0), (array([0.41311798]), 2), (array([0.25316546]), 8), (array([0.24275078]), 6), (array([0.19001917]), 1065), (array([0.14164975]), 3), (array([0.1381964]), 337), (array([0.11915394]), 338), (array([0.11407905]), 112), (array([0.10533123]), 354)]\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "mod = np.linalg.norm(querybag)\n",
    "\n",
    "colA = A.tocsc()\n",
    "for i in range(len(wordBags)): # number of columns/files \n",
    "    modj = sparse.linalg.norm(colA[:,i])\n",
    "    \n",
    "    res.append(((querybag.T@colA[:,i])/(mod*modj) if modj!=0 else 0, i))\n",
    "        \n",
    "    if i%1e2==0:\n",
    "        print(i)\n",
    "\n",
    "res.sort(key=lambda x: (-x[0], -x[1]))\n",
    "# res.sort()\n",
    "res = res[:k]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For query \"Japan and Japanese are super interesting and the best!\", with key words ['japan', 'japanese', 'super', 'interest', 'best'], the best 10 results are:\n",
      "\n",
      "1: file page0.txt titled:\n",
      "Japan\n",
      "\n",
      "2: file page2.txt titled:\n",
      "Outline of Japan\n",
      "\n",
      "3: file page8.txt titled:\n",
      "Japan and World Bank\n",
      "\n",
      "4: file page6.txt titled:\n",
      "Black people in Japan\n",
      "\n",
      "5: file page1065.txt titled:\n",
      "Interest articulation\n",
      "\n",
      "6: file page3.txt titled:\n",
      "Blakiston's Line\n",
      "\n",
      "7: file page337.txt titled:\n",
      "History of anime\n",
      "\n",
      "8: file page338.txt titled:\n",
      "Anime\n",
      "\n",
      "9: file page112.txt titled:\n",
      "Tibet Interest Group\n",
      "\n",
      "10: file page354.txt titled:\n",
      "Japan Media Arts Festival\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"For query \\\"{query}\\\", with key words {text}, the best {k} results are:\\n\")\n",
    "for ix, i in enumerate(res):\n",
    "    print(f\"{ix+1}: file page{i[1]}.txt titled:\")\n",
    "    with open(\"done/page\"+str(i[1])+\".txt\", \"r\") as f:\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Normalize vectors $d_j$ and $q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then aforementioned equation is simplified to\n",
    "$$ |q^TA| = [|cos\\theta_1|,|cos\\theta_2|,\\dots,|cos\\theta_n|] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Japan and Japanese are super interesting and the best!\"\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66262 1388\n"
     ]
    }
   ],
   "source": [
    "colA = A.tocsc()\n",
    "querybag = np.array([0 for i in bagOfWords])\n",
    "text = prepare(query).split()\n",
    "\n",
    "\n",
    "for word in text:\n",
    "    if word not in bagOfWords:\n",
    "        text.remove(word)\n",
    "    querybag[bagOfWords[word]]+=1\n",
    "print(len(bagOfWords), len(wordBags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 834.4732328418058\n",
      "100 276.64221464214324\n",
      "200 119.45020182290006\n",
      "300 61.74315362409992\n",
      "400 68.05379464717565\n",
      "500 105.94348595433729\n",
      "600 39.84752023403267\n",
      "700 471.6939809301491\n",
      "800 30.434632017645008\n",
      "900 28.24674182487699\n",
      "1000 70.67316282378326\n",
      "1100 122.6489685848564\n",
      "1200 273.0646039960876\n",
      "1300 202.8068188279269\n",
      "[(0.5085729483167328, 0), (0.4131179764654834, 2), (0.2531654597053116, 8), (0.24275077689544935, 6), (0.19001917387558614, 1065), (0.14164974838755934, 3), (0.1381963986947751, 337), (0.11915394089685416, 338), (0.11407904859180991, 112), (0.10533122744361345, 354)]\n"
     ]
    }
   ],
   "source": [
    "querybag = querybag/np.linalg.norm(querybag) #remember to clean!\n",
    "\n",
    "for i in range(len(wordBags)):\n",
    "    norm = sparse.linalg.norm(colA[:,i])\n",
    "    colA[:,i] = colA[:,i]/norm\n",
    "    if i%1e2 == 0:\n",
    "        print(i, norm)\n",
    "\n",
    "res = list(querybag.T@colA)\n",
    "for i in range(len(res)):\n",
    "    res[i]=(res[i], i)\n",
    "res.sort(key=lambda x: (-x[0], -x[1]))\n",
    "# res.sort()\n",
    "res = res[:k]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For query Japan and Japanese are super interesting and the best!, with key words ['japan', 'japanese', 'super', 'interest', 'best'], the best 10 results are:\n",
      "1: file page0.txt titled:\n",
      "Japan\n",
      "\n",
      "2: file page2.txt titled:\n",
      "Outline of Japan\n",
      "\n",
      "3: file page8.txt titled:\n",
      "Japan and World Bank\n",
      "\n",
      "4: file page6.txt titled:\n",
      "Black people in Japan\n",
      "\n",
      "5: file page1065.txt titled:\n",
      "Interest articulation\n",
      "\n",
      "6: file page3.txt titled:\n",
      "Blakiston's Line\n",
      "\n",
      "7: file page337.txt titled:\n",
      "History of anime\n",
      "\n",
      "8: file page338.txt titled:\n",
      "Anime\n",
      "\n",
      "9: file page112.txt titled:\n",
      "Tibet Interest Group\n",
      "\n",
      "10: file page354.txt titled:\n",
      "Japan Media Arts Festival\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"For query {query}, with key words {text}, the best {k} results are:\")\n",
    "for ix, i in enumerate(res):\n",
    "    print(f\"{ix+1}: file page{i[1]}.txt titled:\")\n",
    "    with open(\"done/page\"+str(i[1])+\".txt\", \"r\") as f:\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Noise reduction using SVD and low rank approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A\\simeq A_k = U_kD_kV^T_k = \\sum_{i=1}^{k}\\sigma_iu_iv_i^T $$\n",
    "\n",
    "And probability measure being\n",
    "\n",
    "$$ cos\\phi_j = \\frac{q^TA_ke_j}{||q||*||A_ke_j||} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Compare reuslts with and without noise reduction. Find optimal k and test IDF's impact on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test IDF's impact on results I omitted the cells in the 5th point.\\\n",
    "It resulted in less in less accurate outcome.\\\n",
    "IDF has huge impact on search resuts, it devalues popular words, making other (rarer, unique) have bigger influence on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
