Randomized rounding

Within computer science and operations research,
many combinatorial optimization problems are computationally intractable to solve exactly (to optimality).
Many such problems do admit fast (polynomial time) approximation algorithms—that is, algorithms that are guaranteed to return an approximately optimal solution given any input.

Randomized rounding

is a widely used approach for designing and analyzing such approximation algorithms. 
The basic idea is to use the probabilistic method
to convert an optimal solution of a relaxation
of the problem into an approximately optimal solution to the original problem.

The basic approach has three steps:

(Although the approach is most commonly applied with linear programs,
other kinds of relaxations are sometimes used.
For example, see Goeman's and Williamson's semi-definite programming-based
Max-Cut approximation algorithm.)

The challenge in the first step is to choose a suitable integer linear program.
Familiarity with linear programming is required, in particular, familiarity with
how to model problems using linear programs and integer linear programs.
But, for many problems, there is a natural integer linear program that works well,
such as in the Set Cover example below. (The integer linear program should have a small
integrality gap;
indeed randomized rounding is often used to prove bounds on integrality gaps.)

In the second step, the optimal fractional solution can typically be computed
in polynomial time
using any standard linear programming algorithm.

In the third step, the fractional solution must be converted into an integer solution
(and thus a solution to the original problem).
This is called "rounding" the fractional solution.
The resulting integer solution should (provably) have cost
not much larger than the cost of the fractional solution.
This will ensure that the cost of the integer solution
is not much larger than the cost of the optimal integer solution.

The main technique used to do the third step (rounding) is to use randomization,
and then to use probabilistic arguments to bound the increase in cost due to the rounding
(following the probabilistic method from combinatorics).
There, probabilistic arguments are used to show the existence of discrete structures with
desired properties. In this context, one uses such arguments to show the following:

Finally, to make the third step computationally efficient,
one either shows that formula_3 approximates formula_1
with high probability (so that the step can remain randomized)
or one derandomizes the rounding step,
typically using the method of conditional probabilities.
The latter method converts the randomized rounding process
into an efficient deterministic process that is guaranteed
to reach a good outcome.

The randomized rounding step differs from most applications of the probabilistic method in two respects:

The following example illustrates how randomized rounding can be used to design an approximation algorithm for the Set Cover problem.

Fix any instance formula_14 of set cover over a universe formula_15.

For step 1, let IP be the standard integer linear program for set cover for this instance.

For step 2, let LP be the linear programming relaxation of IP,
and compute an optimal solution formula_16 to LP
using any standard linear programming algorithm.

(The feasible solutions to LP are the vectors formula_1
that assign each set formula_18
a non-negative weight formula_19,
such that, for each element formula_20,
formula_3 "covers" formula_22
-- the total weight assigned to the sets containing formula_22
is at least 1, that is,
The optimal solution formula_16
is a feasible solution whose cost
is as small as possible.)
Note that any set cover formula_27 for formula_28
gives a feasible solution formula_1
(where formula_30 for formula_31,
formula_32 otherwise).
The cost of this formula_27 equals the cost of formula_1, that is,
In other words, the linear program LP is a relaxation
of the given set-cover problem.

Since formula_16 has minimum cost among feasible solutions to the LP,
"the cost of formula_16 is a lower bound on the cost of the optimal set cover".

Here is a description of the third step—the rounding step,
which must convert the minimum-cost fractional set cover formula_16
into a feasible integer solution formula_3 (corresponding to a true set cover).

The rounding step should produce an formula_3 that, with positive probability,
has cost within a small factor of the cost of formula_16.
Then (since the cost of formula_16 is a lower bound on the cost of the optimal set cover),
the cost of formula_3 will be within a small factor of the optimal cost.

As a starting point, consider the most natural rounding scheme:

With this rounding scheme,
the expected cost of the chosen sets is at most formula_48,
the cost of the fractional cover.
This is good. Unfortunately the coverage is not good.
When the variables formula_49 are small,
the probability that an element formula_22 is not covered is about

So only a constant fraction of the elements will be covered in expectation.

To make formula_3 cover every element with high probability,
the standard rounding scheme
first "scales up" the rounding probabilities
by an appropriate factor formula_53.
Here is the standard rounding scheme:

Scaling the probabilities up by formula_59
increases the expected cost by formula_59,
but makes coverage of all elements likely.
The idea is to choose formula_59 as small
as possible so that all elements are provably
covered with non-zero probability.
Here is a detailed analysis.

(Note: with care the formula_65
can be reduced to formula_67.)

The output formula_3 of the random rounding scheme has the desired properties
as long as none of the following "bad" events occur:

The expectation of each formula_75 is at most formula_76.
By linearity of expectation,
the expectation of formula_69
is at most formula_78.
Thus, by Markov's inequality, the probability of the first bad event
above is at most formula_79.

For the remaining bad events (one for each element formula_22), note that,
since formula_81 for any given element formula_22,
the probability that formula_22 is not covered is

(This uses the inequality formula_85,
which is strict for formula_86.)

Thus, for each of the formula_87 elements,
the probability that the element is not covered is less than formula_88.

By the naive union bound,
the probability that one of the formula_89 bad events happens
is less than formula_90.
Thus, with positive probability there are no bad events
and formula_3 is a set cover of cost at most formula_71.
QED

The lemma above shows the "existence" of a set cover
of cost formula_93).
In this context our goal is an efficient approximation algorithm,
not just an existence proof, so we are not done.

One approach would be to increase formula_59
a little bit, then show that the probability of success is at least, say, 1/4.
With this modification, repeating the random rounding step a few times
is enough to ensure a successful outcome with high probability.

That approach weakens the approximation ratio.
We next describe a different approach that yields
a deterministic algorithm that is guaranteed to
match the approximation ratio of the existence proof above.
The approach is called the method of conditional probabilities.

The deterministic algorithm emulates the randomized rounding scheme:
it considers each set formula_44 in turn,
and chooses formula_96.
But instead of making each choice "randomly" based on formula_16,
it makes the choice "deterministically", so as to
"keep the conditional probability of failure, given the choices so far, below 1".

We want to be able to set each variable formula_75 in turn
so as to keep the conditional probability of failure below 1.
To do this, we need a good bound on the conditional probability of failure.
The bound will come by refining the original existence proof.
That proof implicitly bounds the probability of failure
by the expectation of the random variable
where
is the set of elements left uncovered at the end.

The random variable formula_101 may appear a bit mysterious,
but it mirrors the probabilistic proof in a systematic way.
The first term in formula_101 comes from applying Markov's inequality
to bound the probability of the first bad event (the cost is too high).
It contributes at least 1 to formula_101 if the cost of formula_3 is too high.
The second term
counts the number of bad events of the second kind (uncovered elements).
It contributes at least 1 to formula_101 if formula_3 leaves any element uncovered.
Thus, in any outcome where formula_101 is less than 1,
formula_3 must cover all the elements
and have cost meeting the desired bound from the lemma.
In short, if the rounding step fails, then formula_109.
This implies (by Markov's inequality) that
"formula_110 is an upper bound on the probability of failure."
Note that the argument above is implicit already in the proof of the lemma,
which also shows by calculation that formula_111.

To apply the method of conditional probabilities,
we need to extend the argument to bound the "conditional" probability of failure
as the rounding step proceeds.
Usually, this can be done in a systematic way,
although it can be technically tedious.

So, what about the "conditional" probability of failure as the rounding step iterates through the sets?
Since formula_109 in any outcome where the rounding step fails,
by Markov's inequality, the "conditional" probability of failure
is at most the "conditional" expectation of formula_101.

Next we calculate the conditional expectation of formula_101,
much as we calculated the unconditioned expectation of formula_101 in the original proof.
Consider the state of the rounding process at the end of some iteration formula_116.
Let formula_117 denote the sets considered so far
(the first formula_116 sets in formula_28).
Let formula_120 denote the (partially assigned) vector formula_3
(so formula_122 is determined only if formula_123).
For each set formula_124,
let formula_125
denote the probability with which formula_75 will be set to 1.
Let formula_127 contain the not-yet-covered elements.
Then the conditional expectation of formula_101,
given the choices made so far, that is, given formula_120, is

Note that formula_131 is determined only after iteration formula_116.

To keep the conditional probability of failure below 1,
it suffices to keep the conditional expectation of formula_101 below 1.
To do this, it suffices to keep the conditional expectation of formula_101 from increasing.
This is what the algorithm will do.
It will set formula_75 in each iteration to ensure that
(where formula_137).

In the formula_116th iteration,
how can the algorithm set formula_139
to ensure that formula_140?
It turns out that it can simply set formula_139
so as to "minimize" the resulting value of formula_142.

To see why, focus on the point in time when iteration formula_116 starts.
At that time, formula_144 is determined,
but formula_142 is not yet determined
--- it can take two possible values depending on how formula_139
is set in iteration formula_116.
Let formula_148 denote the value of formula_149.
Let formula_150 and formula_151,
denote the two possible values of formula_142,
depending on whether formula_139 is set to 0, or 1, respectively.
By the definition of conditional expectation,
Since a weighted average of two quantities
is always at least the minimum of those two quantities,
it follows that
Thus, setting formula_139
so as to minimize the resulting value of
formula_131
will guarantee that
formula_158.
This is what the algorithm will do.

In detail, what does this mean?
Considered as a function of formula_139
formula_131
is a linear function of formula_139,
and the coefficient of formula_139 in that function is

Thus, the algorithm should set formula_139 to 0 if this expression is positive,
and 1 otherwise. This gives the following algorithm.

input: set system formula_28, universe formula_15, cost vector formula_167

output: set cover formula_3 (a solution to the standard integer linear program for set cover)

The algorithm ensures that the conditional expectation of formula_101,
formula_185, does not increase at each iteration.
Since this conditional expectation is initially less than 1 (as shown previously),
the algorithm ensures that the conditional expectation stays below 1.
Since the conditional probability of failure
is at most the conditional expectation of formula_101,
in this way the algorithm
ensures that the conditional probability of failure stays below 1.
Thus, at the end, when all choices are determined,
the algorithm reaches a successful outcome.
That is, the algorithm above returns a set cover formula_3
of cost at most formula_183 times
the minimum cost of any (fractional) set cover.

In the example above, the algorithm was guided by the conditional expectation of a random variable formula_101.
In some cases, instead of an exact conditional expectation,
an "upper bound" (or sometimes a lower bound)
on some conditional expectation is used instead.
This is called a pessimistic estimator.





