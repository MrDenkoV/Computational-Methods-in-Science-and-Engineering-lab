{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare a lot of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'AA/'\n",
    "files = [\"wiki_0\"+str(i) for i in range(9)] + [\"wiki_\"+str(i) for i in range(10, 14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep():\n",
    "    count = 0\n",
    "    for file in files:\n",
    "        with open(path+file) as fp: \n",
    "            while True:\n",
    "                markers=0\n",
    "                empty=True\n",
    "                with open('done/page'+str(count)+'.txt', 'a') as the_file:\n",
    "                    while True:\n",
    "                        line = fp.readline() \n",
    "\n",
    "                        if not line:\n",
    "                            break\n",
    "                        if line[0]=='<':\n",
    "                            markers+=1\n",
    "                            if markers==2:\n",
    "                                break\n",
    "                            continue\n",
    "                        if empty and not line.isspace() and 'http' not in line:\n",
    "                            empty = False\n",
    "                        the_file.write(line)\n",
    "                if empty:\n",
    "                    os.remove('done/page'+str(count)+'.txt')\n",
    "                    count-=1\n",
    "                count+=1\n",
    "                if not line:\n",
    "                    break\n",
    "# prep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded a lot of pages from wikipedia xml export.\\\n",
    "Extracted xml to plain text using wiki extractor https://github.com/attardi/wikiextractor .\\\n",
    "Seperated into many files the function prep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare Bag-Of-Words\n",
    "###### In our case union of all words\n",
    "We will use Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install num2words\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import num2words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'done/'\n",
    "files = ['page'+str(i)+'.txt' for i in range(1388)]\n",
    "bagOfWords = dict()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"_\",\"\")\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub('  +',' ',text)\n",
    "    text = re.sub(r'½','.5',text) # who tf uses this character?\n",
    "    text = re.sub(r'¼','.25',text) # who tf uses this character?\n",
    "    \n",
    "    text_list = text.split()\n",
    "    text = \"\"\n",
    "    \n",
    "    for word in text_list:\n",
    "\n",
    "        word = lemmatizer.lemmatize(word, pos=\"v\")\n",
    "\n",
    "#         if word.isnumeric():\n",
    "#             word = num2words.num2words(float(word))\n",
    "        \n",
    "        if word in stop_words: continue\n",
    "\n",
    "        text += word + \" \"\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r'[0-9]','',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare Function - prepares given text for out bag-of-words, reducing words to their basic dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 princesa\n",
      "20000 obviously\n",
      "30000 costcutting\n",
      "40000 sponge\n",
      "50000 wannabe\n",
      "60000 gizona\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "bagOfWords = dict() # slows down really quick, so remember to clear dict\n",
    "for file in files:\n",
    "    with open(path+file) as fp:\n",
    "        for fileline in fp:\n",
    "            line = prepare(fileline)\n",
    "            for word in line.split():\n",
    "                if word not in bagOfWords:\n",
    "                    bagOfWords[word.lower()]=count\n",
    "                    count+=1\n",
    "                    if count % 1e4 == 0:\n",
    "                        print(count, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to rather slow execution, print to check if everything is in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. For each document prepare bag-of-words and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000 coefficients\n",
      "600000 since\n",
      "900000 products\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "wordBags = [dict() for file in files]\n",
    "for ix, file in enumerate(files):\n",
    "    with open(path+file) as fp:\n",
    "        for fileline in fp:\n",
    "#             line = re.sub(\"[^\\w]\", \" \", fileline) \n",
    "            line = prepare(fileline)\n",
    "            for word in line.split():\n",
    "                if word.lower() not in bagOfWords:\n",
    "                    continue\n",
    "                count+=1\n",
    "                if count % 3e5 == 0:\n",
    "                    print(count, word)\n",
    "                if word not in wordBags[ix]:\n",
    "                    wordBags[ix][word.lower()]=1\n",
    "                else:\n",
    "                    wordBags[ix][word.lower()]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print to assure process not hang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create sparse term-by-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sparse.lil_matrix((len(bagOfWords), len(wordBags))) # rows and cols\n",
    "for ix, bag in enumerate(wordBags):\n",
    "    for word in bag:\n",
    "        A[bagOfWords[word], ix] = bag[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A - Matrix with the number of bag-of-words rows and number of documents columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Calulate Inverse Document Frequency and multiply each matrix element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want no amplify the strength of rare words / topic related words appearing in specific documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF=0\n",
    "N=len(files)\n",
    "nw=1\n",
    "counted = A.getnnz(axis=1)\n",
    "AIDF = sparse.lil_matrix((len(bagOfWords), len(wordBags))) # rows and cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "japan\n",
      "subterranean\n",
      "argument\n",
      "yasuo\n",
      "singlecelled\n",
      "gangsta\n",
      "manthesingabs\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for word in bagOfWords:\n",
    "    nw=counted[bagOfWords[word]]\n",
    "    IDF = math.log(N/nw)\n",
    "#     A[bagOfWords[word]]*=IDF\n",
    "    AIDF[bagOfWords[word], :]=A[bagOfWords[word]]*IDF\n",
    "    if count%1e4==0:\n",
    "        print(word)\n",
    "    count+=1\n",
    "#     print(nw, IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print to ensure process not hang\\\n",
    "Remember to clear A matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Query\n",
    "Input words and number of closest documents to display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate similarity based on:\n",
    "q - inputed bag-of-words\\\n",
    "$$ cos\\theta_j = \\frac{q^Td_j}{||q||*||d_j||} = \\frac{q^TAe_j}{||q||*||Ae_j||} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Japan, Japanese and manga are super interesting and the best!\"\n",
    "k = 10\n",
    "# query = \"Black people in the Japan\"\n",
    "# k = 20\n",
    "wantidf = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66262 1388\n"
     ]
    }
   ],
   "source": [
    "querybag = np.array([0 for i in bagOfWords])\n",
    "text = prepare(query).split()\n",
    "\n",
    "\n",
    "for word in text:\n",
    "    if word not in bagOfWords:\n",
    "        text.remove(word)\n",
    "    else:\n",
    "        querybag[bagOfWords[word]]+=1\n",
    "print(len(bagOfWords), len(wordBags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "[(array([0.46862835]), 0), (array([0.42606222]), 355), (array([0.40809564]), 360), (array([0.39063166]), 357), (array([0.37712339]), 2), (array([0.23110739]), 8), (array([0.22160013]), 6), (array([0.18880402]), 352), (array([0.17346298]), 1065), (array([0.15318436]), 354)]\n"
     ]
    }
   ],
   "source": [
    "if wantidf:\n",
    "    colA = AIDF.tocsc()\n",
    "else:\n",
    "    colA = A.tocsc()\n",
    "\n",
    "res = []\n",
    "mod = np.linalg.norm(querybag)\n",
    "\n",
    "for i in range(len(wordBags)): # number of columns/files \n",
    "    modj = sparse.linalg.norm(colA[:,i])\n",
    "    \n",
    "    res.append(((querybag.T@colA[:,i])/(mod*modj) if modj!=0 else 0, i))\n",
    "        \n",
    "    if i%1e2==0:\n",
    "        print(i)\n",
    "\n",
    "res.sort(key=lambda x: (-x[0], -x[1]))\n",
    "# res.sort()\n",
    "res = res[:k]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For query \"Japan, Japanese and manga are super interesting and the best!\", with key words ['japan', 'japanese', 'manga', 'super', 'interest', 'best'], the best 10 results are:\n",
      "\n",
      "1: file page0.txt titled:\n",
      "Japan\n",
      "\n",
      "2: file page355.txt titled:\n",
      "Manga\n",
      "\n",
      "3: file page360.txt titled:\n",
      "Manga outside Japan\n",
      "\n",
      "4: file page357.txt titled:\n",
      "History of manga\n",
      "\n",
      "5: file page2.txt titled:\n",
      "Outline of Japan\n",
      "\n",
      "6: file page8.txt titled:\n",
      "Japan and World Bank\n",
      "\n",
      "7: file page6.txt titled:\n",
      "Black people in Japan\n",
      "\n",
      "8: file page352.txt titled:\n",
      "Hokusai Manga\n",
      "\n",
      "9: file page1065.txt titled:\n",
      "Interest articulation\n",
      "\n",
      "10: file page354.txt titled:\n",
      "Japan Media Arts Festival\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"For query \\\"{query}\\\", with key words {text}, the best {k} results are:\\n\")\n",
    "for ix, i in enumerate(res):\n",
    "    print(f\"{ix+1}: file page{i[1]}.txt titled:\")\n",
    "    with open(\"done/page\"+str(i[1])+\".txt\", \"r\") as f:\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Normalize vectors $d_j$ and $q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then aforementioned equation is simplified to\n",
    "$$ |q^TA| = [|cos\\theta_1|,|cos\\theta_2|,\\dots,|cos\\theta_n|] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Japan, Japanese and manga are super interesting and the best!\"\n",
    "k = 10\n",
    "wantidf = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66262 1388\n"
     ]
    }
   ],
   "source": [
    "if wantidf:\n",
    "    colA = AIDF.tocsc()\n",
    "else:\n",
    "    colA = A.tocsc()\n",
    "querybag = np.array([0 for i in bagOfWords])\n",
    "text = prepare(query).split()\n",
    "\n",
    "\n",
    "for word in text:\n",
    "    if word not in bagOfWords:\n",
    "        text.remove(word)\n",
    "    else:\n",
    "        querybag[bagOfWords[word]]+=1\n",
    "print(len(bagOfWords), len(wordBags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 834.4732328418058\n",
      "100 276.64221464214324\n",
      "200 119.45020182290006\n",
      "300 61.74315362409992\n",
      "400 68.05379464717565\n",
      "500 105.94348595433729\n",
      "600 39.84752023403267\n",
      "700 471.6939809301491\n",
      "800 30.434632017645008\n",
      "900 28.24674182487699\n",
      "1000 70.67316282378326\n",
      "1100 122.6489685848564\n",
      "1200 273.0646039960876\n",
      "1300 202.8068188279269\n",
      "[(0.4686283454035747, 0), (0.4260622202042567, 355), (0.4080956408580059, 360), (0.39063166044005054, 357), (0.3771233910350561, 2), (0.23110738843627396, 8), (0.22160012726256922, 6), (0.18880402291936035, 352), (0.17346297981692485, 1065), (0.15318436118771325, 354)]\n"
     ]
    }
   ],
   "source": [
    "querybag = querybag/np.linalg.norm(querybag) #remember to clean!\n",
    "\n",
    "for i in range(len(wordBags)):\n",
    "    norm = sparse.linalg.norm(colA[:,i])\n",
    "    colA[:,i] = colA[:,i]/norm\n",
    "    if i%1e2 == 0:\n",
    "        print(i, norm)\n",
    "\n",
    "res = list(querybag.T@colA)\n",
    "for i in range(len(res)):\n",
    "    res[i]=(res[i], i)\n",
    "res.sort(key=lambda x: (-x[0], -x[1]))\n",
    "# res.sort()\n",
    "res = res[:k]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For query Japan, Japanese and manga are super interesting and the best!, with key words ['japan', 'japanese', 'manga', 'super', 'interest', 'best'], the best 10 results are:\n",
      "1: file page0.txt titled:\n",
      "Japan\n",
      "\n",
      "2: file page355.txt titled:\n",
      "Manga\n",
      "\n",
      "3: file page360.txt titled:\n",
      "Manga outside Japan\n",
      "\n",
      "4: file page357.txt titled:\n",
      "History of manga\n",
      "\n",
      "5: file page2.txt titled:\n",
      "Outline of Japan\n",
      "\n",
      "6: file page8.txt titled:\n",
      "Japan and World Bank\n",
      "\n",
      "7: file page6.txt titled:\n",
      "Black people in Japan\n",
      "\n",
      "8: file page352.txt titled:\n",
      "Hokusai Manga\n",
      "\n",
      "9: file page1065.txt titled:\n",
      "Interest articulation\n",
      "\n",
      "10: file page354.txt titled:\n",
      "Japan Media Arts Festival\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"For query {query}, with key words {text}, the best {k} results are:\")\n",
    "for ix, i in enumerate(res):\n",
    "    print(f\"{ix+1}: file page{i[1]}.txt titled:\")\n",
    "    with open(\"done/page\"+str(i[1])+\".txt\", \"r\") as f:\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Noise reduction using SVD and low rank approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A\\simeq A_k = U_kD_kV^T_k = \\sum_{i=1}^{k}\\sigma_iu_iv_i^T $$\n",
    "\n",
    "And probability measure being\n",
    "\n",
    "$$ cos\\phi_j = \\frac{q^TA_ke_j}{||q||*||A_ke_j||} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowRankApp(k, A=AIDF):\n",
    "    U,S,V = sparse.linalg.svds(A, k)\n",
    "    return sparse.csc_matrix(U @ np.diag(S) @ V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<66262x1388 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 91971656 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowRankApp(150, A.tocsc()) # rather slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(query, k=10, frm=1, to=-1, stp=50, A=AIDF, noiseCancel=True, printf=False):\n",
    "    colA = A.tocsc()\n",
    "#     colA=A\n",
    "    querybag = np.array([0 for i in bagOfWords])\n",
    "    text = prepare(query).split()\n",
    "\n",
    "\n",
    "    for word in text:\n",
    "        if word not in bagOfWords:\n",
    "            text.remove(word)\n",
    "        else:\n",
    "            querybag[bagOfWords[word]]+=1\n",
    "            \n",
    "        ### normalization\n",
    "    querybag = querybag/np.linalg.norm(querybag) #remember to clean!\n",
    "    for i in range(len(wordBags)):\n",
    "        norm = sparse.linalg.norm(colA[:,i])\n",
    "        colA[:,i] = colA[:,i]/norm\n",
    "\n",
    "\n",
    "    for x in range(frm, min(colA.shape) if to==-1 else to, stp):\n",
    "        if noiseCancel:\n",
    "            APP = lowRankApp(x, colA)\n",
    "        else:\n",
    "            APP=colA\n",
    "        res = list(querybag.T@APP)\n",
    "        for i in range(len(res)):\n",
    "            res[i]=(res[i], i)\n",
    "        res.sort(key=lambda x: (-x[0], -x[1]))\n",
    "        # res.sort()\n",
    "        res = res[:k]\n",
    "        res = [i[1] for i in res]\n",
    "        print(f\"For k={x} Top {k} results are: {res}\")\n",
    "    \n",
    "    if printf:\n",
    "        print(f\"For query {query}, with key words {text}, the best {k} results are:\")\n",
    "        for ix, i in enumerate(res):\n",
    "            print(f\"{ix+1}: file page{i}.txt titled:\")\n",
    "            with open(\"done/page\"+str(i)+\".txt\", \"r\") as f:\n",
    "                print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Japan, Japanese and manga are super interesting and the best!\"\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suggest using custom test(query, from=, to=, stp=) - stp=step fr k, the line below takes some time :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=150 Top 10 results are: [355, 360, 357, 0, 2, 8, 352, 6, 354, 359]\n"
     ]
    }
   ],
   "source": [
    "test(query, frm=150, to=170, stp=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Compare reuslts with and without noise reduction. Find optimal k and test IDF's impact on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Japan, Japanese and manga are super interesting and the best!\"\n",
    "k = 10 # top k results are displayed - different k than the one is svd!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets see the results without noise cancell.\\\n",
    "The \"For k=xx \" deosn't matter in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=10 Top 10 results are: [0, 355, 360, 357, 2, 8, 6, 352, 1065, 354]\n"
     ]
    }
   ],
   "source": [
    "test(query, k=k, frm=10, to=20, stp=30, noiseCancel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets test many k values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=1 Top 10 results are: [862, 1218, 1277, 916, 1216, 1321, 1184, 930, 934, 1187]\n",
      "For k=51 Top 10 results are: [355, 360, 357, 352, 338, 0, 337, 359, 2, 353]\n",
      "For k=101 Top 10 results are: [355, 360, 357, 0, 2, 8, 352, 354, 6, 359]\n",
      "For k=151 Top 10 results are: [355, 360, 357, 0, 2, 8, 352, 6, 354, 359]\n",
      "For k=201 Top 10 results are: [355, 360, 0, 357, 2, 8, 6, 352, 354, 359]\n",
      "For k=251 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 354]\n",
      "For k=301 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 354]\n",
      "For k=351 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 354]\n",
      "For k=401 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 353]\n",
      "For k=451 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 353]\n",
      "For k=501 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 354]\n",
      "For k=551 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 354]\n",
      "For k=601 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 354]\n",
      "For k=651 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 354]\n",
      "For k=701 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 354]\n",
      "For k=751 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 354]\n",
      "For k=801 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 354]\n",
      "For k=851 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 359, 1065]\n",
      "For k=901 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 1065, 359]\n",
      "For k=951 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 1065, 354]\n",
      "For k=1001 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 1065, 354]\n",
      "For k=1051 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 1065, 354]\n",
      "For k=1101 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 1065, 354]\n",
      "For k=1151 Top 10 results are: [355, 0, 360, 357, 2, 8, 6, 352, 1065, 354]\n",
      "For k=1201 Top 10 results are: [355, 0, 360, 2, 357, 8, 6, 352, 1065, 354]\n",
      "For k=1251 Top 10 results are: [0, 355, 2, 357, 360, 8, 6, 352, 1065, 354]\n",
      "For k=1301 Top 10 results are: [0, 355, 2, 357, 360, 8, 6, 352, 1065, 359]\n",
      "For k=1351 Top 10 results are: [0, 355, 360, 357, 2, 8, 6, 352, 1065, 354]\n"
     ]
    }
   ],
   "source": [
    "test(query, k=k, stp=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjectively the best results are obtained for k ~ 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For cells below, compile the Final version cell, that is further below, first!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets test without IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For query Japan, Japanese and manga are super interesting and the best!, with key words ['japan', 'japanese', 'manga', 'super', 'interest', 'best'], the best 10 results are:\n",
      "1: file page355.txt titled:\n",
      "Manga\n",
      "\n",
      "2: file page0.txt titled:\n",
      "Japan\n",
      "\n",
      "3: file page2.txt titled:\n",
      "Outline of Japan\n",
      "\n",
      "4: file page357.txt titled:\n",
      "History of manga\n",
      "\n",
      "5: file page360.txt titled:\n",
      "Manga outside Japan\n",
      "\n",
      "6: file page6.txt titled:\n",
      "Black people in Japan\n",
      "\n",
      "7: file page8.txt titled:\n",
      "Japan and World Bank\n",
      "\n",
      "8: file page359.txt titled:\n",
      "Kono Manga ga Sugoi!\n",
      "\n",
      "9: file page352.txt titled:\n",
      "Hokusai Manga\n",
      "\n",
      "10: file page338.txt titled:\n",
      "Anime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colA = A.tocsc()\n",
    "### normalization\n",
    "for i in range(len(wordBags)):\n",
    "    norm = sparse.linalg.norm(colA[:,i])\n",
    "    colA[:,i] = colA[:,i]/norm\n",
    "    \n",
    "APP=lowRankApp(150, colA.tocsc())\n",
    "find(query, k, APP, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets see outcomes of applying the proper way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For query Japan, Japanese and manga are super interesting and the best!, with key words ['japan', 'japanese', 'manga', 'super', 'interest', 'best'], the best 10 results are:\n",
      "1: file page355.txt titled:\n",
      "Manga\n",
      "\n",
      "2: file page360.txt titled:\n",
      "Manga outside Japan\n",
      "\n",
      "3: file page357.txt titled:\n",
      "History of manga\n",
      "\n",
      "4: file page0.txt titled:\n",
      "Japan\n",
      "\n",
      "5: file page2.txt titled:\n",
      "Outline of Japan\n",
      "\n",
      "6: file page8.txt titled:\n",
      "Japan and World Bank\n",
      "\n",
      "7: file page352.txt titled:\n",
      "Hokusai Manga\n",
      "\n",
      "8: file page6.txt titled:\n",
      "Black people in Japan\n",
      "\n",
      "9: file page354.txt titled:\n",
      "Japan Media Arts Festival\n",
      "\n",
      "10: file page359.txt titled:\n",
      "Kono Manga ga Sugoi!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colA = AIDF.tocsc()\n",
    "### normalization\n",
    "for i in range(len(wordBags)):\n",
    "    norm = sparse.linalg.norm(colA[:,i])\n",
    "    colA[:,i] = colA[:,i]/norm\n",
    "    \n",
    "APP=lowRankApp(150, colA.tocsc())\n",
    "find(query, k, APP, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not using IDF has resulted in different and less accurate outcome.\\\n",
    "IDF has impact on search results, it devalues popular words, making other (rarer, unique) have greater influence on the result.\n",
    "###### IDF has huge impact, when we try to reduce noise using low rank approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to slow SVD calculations, apply query, number of top results, already low rank approximated matrix and whether you want to print the titles of found articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(query, k=10, A=AIDF, verbose=False):\n",
    "#     colA = A.tocsc()\n",
    "#     colA=A\n",
    "    querybag = np.array([0 for i in bagOfWords])\n",
    "    text = prepare(query).split()\n",
    "\n",
    "\n",
    "    for word in text:\n",
    "        if word not in bagOfWords:\n",
    "            text.remove(word)\n",
    "        else:\n",
    "            querybag[bagOfWords[word]]+=1\n",
    "            \n",
    "        ### normalization\n",
    "    querybag = querybag/np.linalg.norm(querybag) #remember to clean!\n",
    "    for i in range(len(wordBags)):\n",
    "        norm = sparse.linalg.norm(colA[:,i])\n",
    "        colA[:,i] = colA[:,i]/norm\n",
    "\n",
    "    res = list(querybag.T@APP)\n",
    "    for i in range(len(res)):\n",
    "        res[i]=(res[i], i)\n",
    "    res.sort(key=lambda x: (-x[0], -x[1]))\n",
    "    # res.sort()\n",
    "    res = res[:k]\n",
    "    res = [i[1] for i in res]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"For query {query}, with key words {text}, the best {k} results are:\")\n",
    "        for ix, i in enumerate(res):\n",
    "            print(f\"{ix+1}: file page{i}.txt titled:\")\n",
    "            with open(\"done/page\"+str(i)+\".txt\", \"r\") as f:\n",
    "                print(f.readline())\n",
    "    else:\n",
    "        print(f\"For k={x} Top {k} results are: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "colA = AIDF.tocsc()\n",
    "### normalization\n",
    "for i in range(len(wordBags)):\n",
    "    norm = sparse.linalg.norm(colA[:,i])\n",
    "    colA[:,i] = colA[:,i]/norm\n",
    "    \n",
    "APP=lowRankApp(150, colA.tocsc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For query Japan, Japanese and manga are super interesting and the best!, with key words ['japan', 'japanese', 'manga', 'super', 'interest', 'best'], the best 10 results are:\n",
      "1: file page355.txt titled:\n",
      "Manga\n",
      "\n",
      "2: file page360.txt titled:\n",
      "Manga outside Japan\n",
      "\n",
      "3: file page357.txt titled:\n",
      "History of manga\n",
      "\n",
      "4: file page0.txt titled:\n",
      "Japan\n",
      "\n",
      "5: file page2.txt titled:\n",
      "Outline of Japan\n",
      "\n",
      "6: file page8.txt titled:\n",
      "Japan and World Bank\n",
      "\n",
      "7: file page352.txt titled:\n",
      "Hokusai Manga\n",
      "\n",
      "8: file page6.txt titled:\n",
      "Black people in Japan\n",
      "\n",
      "9: file page354.txt titled:\n",
      "Japan Media Arts Festival\n",
      "\n",
      "10: file page359.txt titled:\n",
      "Kono Manga ga Sugoi!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Japan, Japanese and manga are super interesting and the best!\"\n",
    "k = 10\n",
    "find(query, k=k, A=APP, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change query and k and run a cell above for different searches - it works rather fast"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
